

=== Page 6 ===
Contents
Preface to the Second Edition
ix
Preface
xi
Audience
xi
Teaching strategy
xii
How to use this book
xii
Installing the rethinking R package
xvi
Acknowledgments
xvi
Chapter 1.
The Golem of Prague
1
1.1.
Statistical golems
1
1.2.
Statistical rethinking
4
1.3.
Tools for golem engineering
10
1.4.
Summary
17
Chapter 2.
Small Worlds and Large Worlds
19
2.1.
The garden of forking data
20
2.2.
Building a model
28
2.3.
Components of the model
32
2.4.
Making the model go
36
2.5.
Summary
46
2.6.
Practice
46
Chapter 3.
Sampling the Imaginary
49
3.1.
Sampling from a grid-approximate posterior
52
3.2.
Sampling to summarize
53
3.3.
Sampling to simulate prediction
61
3.4.
Summary
68
3.5.
Practice
68
Chapter 4.
Geocentric Models
71
4.1.
Why normal distributions are normal
72
4.2.
A language for describing models
77
4.3.
Gaussian model of height
78
4.4.
Linear prediction
91
4.5.
Curves from lines
110
4.6.
Summary
120
4.7.
Practice
120
Chapter 5.
The Many Variables & The Spurious Waffles
123
5.1.
Spurious association
125
5.2.
Masked relationship
144
v


=== Page 7 ===
vi
CONTENTS
5.3.
Categorical variables
153
5.4.
Summary
158
5.5.
Practice
159
Chapter 6.
The Haunted DAG & The Causal Terror
161
6.1.
Multicollinearity
163
6.2.
Post-treatment bias
170
6.3.
Collider bias
176
6.4.
Confronting confounding
183
6.5.
Summary
189
6.6.
Practice
189
Chapter 7.
Ulysses’ Compass
191
7.1.
The problem with parameters
193
7.2.
Entropy and accuracy
202
7.3.
Golem taming: regularization
214
7.4.
Predicting predictive accuracy
217
7.5.
Model comparison
225
7.6.
Summary
235
7.7.
Practice
235
Chapter 8.
Conditional Manatees
237
8.1.
Building an interaction
239
8.2.
Symmetry of interactions
250
8.3.
Continuous interactions
252
8.4.
Summary
260
8.5.
Practice
260
Chapter 9.
Markov Chain Monte Carlo
263
9.1.
Good King Markov and his island kingdom
264
9.2.
Metropolis algorithms
267
9.3.
Hamiltonian Monte Carlo
270
9.4.
Easy HMC: ulam
279
9.5.
Care and feeding of your Markov chain
287
9.6.
Summary
296
9.7.
Practice
296
Chapter 10.
Big Entropy and the Generalized Linear Model
299
10.1.
Maximum entropy
300
10.2.
Generalized linear models
312
10.3.
Maximum entropy priors
321
10.4.
Summary
321
Chapter 11.
God Spiked the Integers
323
11.1.
Binomial regression
324
11.2.
Poisson regression
345
11.3.
Multinomial and categorical models
359
11.4.
Summary
365
11.5.
Practice
366
Chapter 12.
Monsters and Mixtures
369
12.1.
Over-dispersed counts
369
12.2.
Zero-inflated outcomes
376


=== Page 8 ===
CONTENTS
vii
12.3.
Ordered categorical outcomes
380
12.4.
Ordered categorical predictors
391
12.5.
Summary
397
12.6.
Practice
397
Chapter 13.
Models With Memory
399
13.1.
Example: Multilevel tadpoles
401
13.2.
Varying effects and the underfitting/overfitting trade-off
408
13.3.
More than one type of cluster
415
13.4.
Divergent transitions and non-centered priors
420
13.5.
Multilevel posterior predictions
426
13.6.
Summary
431
13.7.
Practice
431
Chapter 14.
Adventures in Covariance
435
14.1.
Varying slopes by construction
437
14.2.
Advanced varying slopes
447
14.3.
Instruments and causal designs
455
14.4.
Social relations as correlated varying effects
462
14.5.
Continuous categories and the Gaussian process
467
14.6.
Summary
485
14.7.
Practice
485
Chapter 15.
Missing Data and Other Opportunities
489
15.1.
Measurement error
491
15.2.
Missing data
499
15.3.
Categorical errors and discrete absences
516
15.4.
Summary
521
15.5.
Practice
521
Chapter 16.
Generalized Linear Madness
525
16.1.
Geometric people
526
16.2.
Hidden minds and observed behavior
531
16.3.
Ordinary differential nut cracking
536
16.4.
Population dynamics
541
16.5.
Summary
550
16.6.
Practice
550
Chapter 17.
Horoscopes
553
Endnotes
557
Bibliography
573
Citation index
585
Topic index
589


=== Page 10 ===
Preface to the Second Edition
It came as a complete surprise to me that I wrote a statistics book. It is even more sur-
prising how popular the book has become. But I had set out to write the statistics book that
I wish I could have had in graduate school. No one should have to learn this stuff the way I
did. I am glad there is an audience to benefit from the book.
It consumed five years to write it. There was an initial set of course notes, melted down
and hammered into a first 200-page manuscript. I discarded that first manuscript. But it
taught me the outline of the book I really wanted to write. Then, several years of teaching
with the manuscript further refined it.
Really, I could have continued refining it every year. Going to press carries the penalty of
freezing a dynamic process of both learning how to teach the material and keeping up with
changes in the material. As time goes on, I see more elements of the book that I wish I had
done differently. I’ve also received a lot of feedback on the book, and that feedback has given
me ideas for improving it.
So in the second edition, I put those ideas into action. The major changes are:
The R package has some new tools. The map tool from the first edition is still here, but
now it is named quap. This renaming is to avoid misunderstanding. We just used it to get
a quadratic approximation to the posterior. So now it is named as such. A bigger change is
that map2stan has been replaced by ulam. The new ulam is very similar to map2stan, and
in many cases can be used identically. But it is also much more flexible, mainly because it
does not make any assumptions about GLM structure and allows explicit variable types. All
the map2stan code is still in the package and will continue to work. But now ulam allows for
much more, especially in later chapters. Both of these tools allow sampling from the prior
distribution, using extract.prior, as well as the posterior. This helps with the next change.
Much more prior predictive simulation. A prior predictive simulation means simulating
predictions from a model, using only the prior distribution instead of the posterior distri-
bution. This is very useful for understanding the implications of a prior. There was only a
vestigial amount of this in the first edition. Now many modeling examples have some prior
predictive simulation. I think this is one of the most useful additions to the second edition,
since it helps so much with understanding not only priors but also the model itself.
More emphasis on the distinction between prediction and inference. Chapter 5, the chap-
ter on multiple regression, has been split into two chapters. The first chapter focuses on
helpful aspects of regression; the second focuses on ways that it can mislead. This allows as
well a more direct discussion of causal inference. This means that DAGs—directed acyclic
ix


=== Page 11 ===
x
PREFACE TO THE SECOND EDITION
graphs—make an appearance. The chapter on overfitting, Chapter 7 now, is also more di-
rect in cautioning about the predictive nature of information criteria and cross-validation.
Cross-validation and importance sampling approximations of it are now discussed explicitly.
New model types. Chapter 4 now presents simple splines. Chapter 7 introduces one kind
or robust regression. Chapter 12 explains how to use ordered categorical predictor variables.
Chapter 13 presents a very simple type of social network model, the social relations model.
Chapter 14 has an example of a phylogenetic regression, with a somewhat critical and hetero-
dox presentation. And there is an entirely new chapter, Chapter 16, that focuses on models
that are not easily conceived of as GLMMs, including ordinary differential equation models.
Some new data examples. There are some new data examples, including the Japanese cherry
blossoms time series on the cover and a larger primate evolution data set with 300 species
and a matching phylogeny.
More presentation of raw Stan models. There are many more places now where raw Stan
model code is explained. I hope this makes a transition to working directly in Stan easier.
But most of the time, working directly in Stan is still optional.
Kindness and persistence. As in the first edition, I have tried to make the material as kind as
possible. None of this stuff is easy, and the journey into understanding is long and haunted.
It is important that readers expect that confusion is normal. This is also the reason that I
have not changed the basic modeling strategy in the book.
First, I force the reader to explicitly specify every assumption of the model. Some readers
of the first edition lobbied me to use simplified formula tools like brms or rstanarm. Those
are fantastic packages, and graduating to use them after this book is recommended. But
I don’t see how a person can come to understand the model when using those tools. The
priors being hidden isn’t the most limiting part. Instead, since linear model formulas like
y ~ (1|x) + z don’t show the parameters, nor even all of the terms, it is not easy to see
how the mathematical model relates to the code. It is ultimately kinder to be a bit cruel and
require more work. So the formula lists remain. You’ll thank me later.
Second, half the book goes by before MCMC appears. Some readers of the first edi-
tion wanted me to start instead with MCMC. I do not do this because Bayes is not about
MCMC. We seek the posterior distribution, but there are many legitimate approximations
of it. MCMC is just one set of strategies. Using quadratic approximation in the first half also
allows a clearer tie to non-Bayesian algorithms. And since finding the quadratic approxima-
tion is fast, it means readers don’t have to struggle with too many things at once.
Thanks. Many readers and colleagues contributed comments that improved upon the first
edition. There are too many to name individually. Several anonymous reviewers provided
many pages of constructive criticism. Bret Beheim and Aki Vehtari commented on multi-
ple chapters. My colleagues at the Max Planck Institute for Evolutionary Anthropology in
Leipzig made the largest contributions, by working through draft chapters and being relent-
lessly honest.
Richard McElreath
Leipzig, 14 December 2019
