{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9f12e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### This extract pages number (starting from actual page 1) and bundles it with page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49993dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import warnings\n",
    "\n",
    "def extract_page_data_fitz(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts page numbers and text from a PDF file using PyMuPDF.\n",
    "    The function looks for page numbers in the top and bottom 15% of each page.\n",
    "    It returns a list of dictionaries, each containing the page index, page number,\n",
    "    and the full text of the page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_data = []\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        height = page.rect.height\n",
    "        width = page.rect.width\n",
    "\n",
    "        top_rect = fitz.Rect(0, 0, width, height * 0.15)\n",
    "        bottom_rect = fitz.Rect(0, height * 0.85, width, height)\n",
    "\n",
    "        top_text = page.get_text(\"text\", clip=top_rect).split()\n",
    "        bottom_text = page.get_text(\"text\", clip=bottom_rect).split()\n",
    "\n",
    "        found_number = None\n",
    "        for text in top_text + bottom_text:\n",
    "            if text.isdigit():\n",
    "                found_number = int(text)\n",
    "                break\n",
    "\n",
    "        full_text = page.get_text(\"text\")\n",
    "\n",
    "        pages_data.append({\n",
    "            \"index\": i,\n",
    "            \"number\": found_number,\n",
    "            \"content\": full_text\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "    return pages_data\n",
    "\n",
    "\n",
    "def correct_page_numbers(pages_data, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Corrects the page numbers in the extracted data.\n",
    "    It looks for a sequence of consecutive page numbers and fills in the gaps.\n",
    "    The function also handles the case where page numbers are not in a sequential order\n",
    "    by correcting them based on the first found sequence of consecutive page numbers.\n",
    "    The function also sets page numbers less than 1 to None.\n",
    "    If no sequence is found, it returns None.\n",
    "    The function returns the index of the first page with number 1.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find first sequence of 'sequence_length' consecutive page numbers\n",
    "        seen = [(i, d[\"number\"]) for i, d in enumerate(pages_data) if isinstance(d[\"number\"], int)]\n",
    "\n",
    "        for start in range(len(seen) - sequence_length + 1):\n",
    "            valid = True\n",
    "            for j in range(sequence_length):\n",
    "                if seen[start + j][1] != seen[start][1] + j:\n",
    "                    valid = False\n",
    "                    break\n",
    "            if valid:\n",
    "                base_index, base_number = seen[start]\n",
    "                break\n",
    "        else:\n",
    "            # No sequence found\n",
    "            return None\n",
    "\n",
    "        # Forward fill from base_index\n",
    "        for offset, page in enumerate(pages_data[base_index:], start=0):\n",
    "            page[\"number\"] = base_number + offset\n",
    "\n",
    "        # Backward fill before base_index\n",
    "        for offset in range(1, base_index + 1):\n",
    "            page = pages_data[base_index - offset]\n",
    "            page[\"number\"] = base_number - offset\n",
    "\n",
    "        # Set pages < 1 == None\n",
    "        for page in pages_data:\n",
    "            if page[\"number\"] < 1:\n",
    "                page[\"number\"] = None\n",
    "\n",
    "        # Find index of first page with number 1\n",
    "        start_chapter = next((page['index'] for page in pages_data if page[\"number\"] == 1), None)\n",
    "\n",
    "        return start_chapter\n",
    "\n",
    "    except Exception:\n",
    "        # Catch any unexpected errors and return None\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_text(pdf_path, start_chapter=None):\n",
    "    \"\"\"\n",
    "    Extracts the text from a PDF file using PyMuPDF.\n",
    "    It returns the text of the book starting from the specified page index.\n",
    "    If no start_chapter is provided, it extracts the text from the entire PDF.\n",
    "    \"\"\"\n",
    "    if start_chapter:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        all_pages_text = []\n",
    "        for page_range in range(start_chapter, len(doc)):\n",
    "            page_text = doc[page_range].get_text(\"text\")\n",
    "            all_pages_text.append(page_text)\n",
    "        doc.close()\n",
    "        whole_text = \"\\n\".join(all_pages_text)\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            \"start_chapter is None: extracting text from the entire PDF.\",\n",
    "            UserWarning\n",
    "        )\n",
    "        doc = fitz.open(pdf_path)\n",
    "        whole_text = \"\"\n",
    "        for page in doc:\n",
    "            page_text = page.get_text(\"text\")\n",
    "            whole_text += page_text\n",
    "        doc.close()\n",
    "    \n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d8e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Probability Theory\n",
      "Probability theory provides the basis for mathematical statistics.\n",
      "Probability theory has two distinct elements. One is just a special case\n",
      "of measure theory and can be approached in that way. For this aspect, the\n",
      "presentation in this chapter assumes familiarity with the material in\n",
      "Section 0.1 beginning on page 692. This aspect is “pure” mathematics. The\n",
      "other aspect of probability theory is essentially built on a gedanken experiment\n",
      "involving drawing balls from an urn that contains balls of diﬀerent colors, and\n",
      "noting the colors of the balls drawn. In this aspect of probability theory, we\n",
      "may treat “probability” as a primitive (that is, undeﬁned) concept. In this line\n",
      "of development, we relate “probability” informally to some notion of long-term\n",
      "frequency or to expectations or beliefs relating to the types of balls that will\n",
      "be drawn. Following some additional axiomatic developments, however, this\n",
      "aspect of probability theory is also essentially “pure” mathematics.\n",
      "Because it is just mathematics, in probability theory per se, we do not ask\n",
      "“what do you think is the probability that ...?” Given an axiomatic framework,\n",
      "one’s beliefs are irrelevant, whether probability is a measure or is a primitive\n",
      "concept. In statistical theory or applications, however, we may ask questions\n",
      "about “beliefs”, and the answer(s) may depend on deep philosophical consid-\n",
      "erations in connecting the mathematical concepts of probability theory with\n",
      "decisions about the “real world”. This may lead to a diﬀerent deﬁnition of\n",
      "probability. For example, Lindley and Phillips (1976), page 115, state “Proba-\n",
      "bility is a relation between you and the external world, expressing your opinion\n",
      "of some aspect of that world...” I am sure that an intellectually interesting\n",
      "theory could be developed based on ways of “expressing your opinion[s]”, but I\n",
      "will not use “probability” in this way; rather, throughout this book, I will use\n",
      "the term probability as a measure (see Deﬁnition 0.1.10, page 704). For spe-\n",
      "ciﬁc events in a given application, of course, certain values of the probability\n",
      "measure may be assigned based on “opinions”, “beliefs”, or whatever.\n",
      "Another useful view of “probability” is expressed by Gnedenko and Kolmogorov\n",
      "(1954) (page 1): “The very concept of mathematical probability [their empha-\n",
      "sis] would be fruitless if it did not ﬁnd its realization in the frequency [their\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2\n",
      "1 Probability Theory\n",
      "emphasis] of occurrence of events under large-scale repetition of uniform con-\n",
      "ditions.” (See a more complete quotation on page 143.)\n",
      "Ranks of Mathematical Objects\n",
      "In probability theory we deal with various types of mathematical objects.\n",
      "We would like to develop concepts and identify properties that are indepen-\n",
      "dent of the type of the underlying objects, but that is not always possible.\n",
      "Occasionally we will ﬁnd it necessary to discuss scalar objects, rank one ob-\n",
      "jects (vectors), and rank two objects (matrices) separately. In general, most\n",
      "degree-one properties, such as expectations of linear functions, can be consid-\n",
      "ered uniformly across the diﬀerent types of mathematical objects. Degree-two\n",
      "properties, such as variances, however, must usually be considered separately\n",
      "for scalars, vectors, and matrices.\n",
      "Overview of Chapter\n",
      "This chapter covers important topics in probability theory at a fairly fast\n",
      "pace. Some of the material in this chapter, such as the properties of certain\n",
      "families of distributions, is often considered part of “mathematical statistics”,\n",
      "rather than a part of probability theory. Unless the interest is in use of data\n",
      "for describing a distribution or for making inferences about the distribution,\n",
      "however, the study of properties of the distribution is part of probability\n",
      "theory, rather than statistics.\n",
      "We begin in Section 1.1 with statements of deﬁnitions and some basic\n",
      "properties. The initial development of this section parallels the ﬁrst few sub-\n",
      "sections of Section 0.1 for more general measures, and then the development\n",
      "of expectations depends on the results of Section 0.1.6 for integration.\n",
      "Sections 1.3 and 1.4 are concerned with sequences of independent random\n",
      "variables. The limiting properties of such sequences are important. Many of\n",
      "the limiting properties can be studied using expansions in power series, which\n",
      "is the topic of Section 1.2.\n",
      "In Section 1.5 we do a fairly careful development of the concept of condi-\n",
      "tioning. We do not take conditional probability to be a fundamental concept,\n",
      "as we take (unconditional) probability itself to be. Conditional probability,\n",
      "rather, is based on conditional expectation as the fundamental concept, so we\n",
      "begin that discussion by considering conditional expectation. This provides a\n",
      "more general foundation for conditional probability than we would have if we\n",
      "deﬁned it more directly in terms of a measurable space. Conditional probabil-\n",
      "ity plays an important role in sequences that lack a simplifying assumption\n",
      "of independence. We discuss sequences that lack independence in Section 1.6.\n",
      "Many interesting sequences also do not have identical marginal distributions,\n",
      "but rather follow some kind of evolving model whose form depends on, but is\n",
      "not necessarily determined by, previous variates in the sequence.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "3\n",
      "In the next chapter, beginning on page 155, I identify and describe useful\n",
      "classes of probability distributions. These classes are important because they\n",
      "are good models of observable random phenomena, and because they are\n",
      "easy to work with. The properties of various statistical methods discussed in\n",
      "subsequent chapters depend on the underlying probability model, and some of\n",
      "the properties of the statistical methods can be worked out easily for particular\n",
      "models discussed in Chapter 2.\n",
      "1.1 Some Important Probability Deﬁnitions and Facts\n",
      "A probability distribution is built from a measure space in which the measure\n",
      "is a probability measure.\n",
      "Deﬁnition 1.1 (probability measure)\n",
      "A measure ν whose domain is a σ-ﬁeld deﬁned on the sample space Ωwith\n",
      "the property that ν(Ω) = 1 is called a probability measure.\n",
      "We often use P to denote a probability measure, just as we often use λ, µ, or\n",
      "ν to denote a general measure.\n",
      "Properties of the distribution and statistical inferences regarding it are\n",
      "derived and evaluated in the context of the “probability triple”,\n",
      "(Ω, F, P ).\n",
      "(1.1)\n",
      "Deﬁnition 1.2 (probability space)\n",
      "If P in the measure space (Ω, F, P ) is a probability measure, the triple\n",
      "(Ω, F, P ) is called a probability space.\n",
      "Probability spaces are the basic structures we will consider in this chapter. In\n",
      "a probability space (Ω, F, P ), a set A ∈F is called an “event”.\n",
      "The full σ-ﬁeld F in the probability space (Ω, F, P ) may not be necessary\n",
      "to deﬁne the space.\n",
      "Deﬁnition 1.3 (determining class)\n",
      "If P and Q are probability measures deﬁned on the measurable space (Ω, F),\n",
      "a collection of sets C ⊆F is called a determining class of P and Q, iﬀ\n",
      "P (A) = Q(A) ∀A ∈C =⇒P (B) = Q(B) ∀B ∈F.\n",
      "For measures P and Q deﬁned on the measurable space (Ω, F), the condition\n",
      "P (B) = Q(B) ∀B ∈F, of course, is the same as the condition P = Q.\n",
      "Notice that the determining class is not necessarily a sub-σ-ﬁeld. If it is,\n",
      "however, a probability measure on the measurable space of the determining\n",
      "class results in a probability space that is essentially the same as that formed\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4\n",
      "1 Probability Theory\n",
      "by the probability measure on the original measurable space. That is, in the\n",
      "notation of Deﬁnition 1.3, if C is a determining class, then the probability\n",
      "space (Ω, σ(C), P ) is essentially equivalent to (Ω, F, P ) in so far as properties\n",
      "of the probability measure are concerned.\n",
      "We now deﬁne complete probability measures and spaces as special cases of\n",
      "complete measures and complete measure spaces (Deﬁnitions 0.1.16 and 0.1.21\n",
      "on pages 707 and 709). Completeness is often necessary in order to ensure\n",
      "convergence of sequences of probabilities.\n",
      "Deﬁnition 1.4 (complete probability space)\n",
      "A probability measure P deﬁned on the σ-ﬁeld F is said to be complete if\n",
      "A1 ⊆A ∈F and P (A) = 0 implies A1 ∈F. If the probability measure P in\n",
      "the probability space (Ω, F, P ) is complete, we also say that the probability\n",
      "space is a complete probability space.\n",
      "An event A such that P (A) = 0 is called a negligible event or negligible\n",
      "set. For a set A1 that is a subset of a negligible set, as in Deﬁnition 1.4, it is\n",
      "clear that A1 is also negligible.\n",
      "Deﬁnition 1.5 (almost surely (a.s.))\n",
      "Given a probability space (Ω, F, P ), a property that holds for all elements of\n",
      "F with positive probability is said to hold almost surely, or a.s.\n",
      "This is the same as almost everywhere for general measures, and there is no\n",
      "essential diﬀerence in “almost everywhere” and “almost surely”.\n",
      "1.1.1 Probability and Probability Distributions\n",
      "The elements in the probability space can be any kind of objects. They do not\n",
      "need to be numbers. Later, on page 9, we will deﬁne a real-valued measurable\n",
      "function (to be called a “random variable”), and consider the measure on IR\n",
      "induced or “pushed forward” by this function. (See page 712 for deﬁnition of\n",
      "an induced measure.) This induced measure, which is usually based either on\n",
      "the counting measure (deﬁned on countable sets as their cardinality) or on\n",
      "the Lebesgue measure (the length of intervals), is also a probability measure.\n",
      "First, however, we continue with some deﬁnitions that do not involve ran-\n",
      "dom variables.\n",
      "Probability Measures on Events; Independence and\n",
      "Exchangeability\n",
      "Deﬁnition 1.6 (probability of an event)\n",
      "In a probability space (Ω, F, P ), the probability of the event A is P (A). This\n",
      "is also written as Pr(A).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "5\n",
      "In the probability space (Ω, F, P ), for A ∈F, we have\n",
      "Pr(A) = P (A) =\n",
      "Z\n",
      "A\n",
      "dP.\n",
      "(1.2)\n",
      "We use notation such as “Pr(·)”, “E(·)”, “V(·)”, and so on (to be intro-\n",
      "duced later) as generic symbols to represent speciﬁc quantities within the\n",
      "context of a given probability space. Whenever we discuss more than one\n",
      "probability space, it may be necessary to qualify the generic notation or else\n",
      "use an alternative notation for the same concept. For example, when dealing\n",
      "with the probability spaces (Ω, F, P ) and (Λ, G, Q), we may use notation of\n",
      "the form “PrP (·)” or “PrQ(·)”; but in this case, of course, the notation “P (·)”\n",
      "or “Q(·)”is simpler.\n",
      "One of the most important properties that involves more than one event or\n",
      "more than one function or more than one measurable function is independence.\n",
      "We deﬁne independence in a probability space in three steps.\n",
      "Deﬁnition 1.7 (independence)\n",
      "Let (Ω, F, P ) be a probability space.\n",
      "1. Independence of events (within a collection of events).\n",
      "Let C be a collection of events; that is, a collection of subsets of F. The\n",
      "events in C are independent iﬀfor a positive integer n and distinct events\n",
      "A1, . . ., An in C,\n",
      "P (A1 ∩· · · ∩An) = P (A1) · · · P (An).\n",
      "(1.3)\n",
      "2. Independence of collections of events (and, hence, of σ-ﬁelds).\n",
      "For any index set I, let Ci be a collection of sets with Ci ⊆F. The\n",
      "collections Ci are independent iﬀthe events in any union of the Ci are\n",
      "independent; that is, {Ai ∈Ci : i ∈I} are independent events.\n",
      "3. Independence of Borel functions (and, hence, of random variables,\n",
      "which are special functions deﬁned below).\n",
      "For i in some index set I, the Borel-measurable functions Xi are indepen-\n",
      "dent iﬀσ(Xi) for i ∈I are independent.\n",
      "While we have deﬁned independence in terms of a single probability measure\n",
      "(which gives meaning to the left side of equation (1.3)), we could deﬁne the\n",
      "concept over diﬀerent probability spaces in the obvious way that requires the\n",
      "probability of all events simultaneously to be the product of the probabilities\n",
      "of the individual events.\n",
      "Notice that Deﬁnition 1.7 provides meaning to mixed phrases such as “the\n",
      "event A is independent of the σ-ﬁeld F” or “the random variable X (deﬁned\n",
      "below) is independent of the event A”.\n",
      "We will often consider a sequence or a process of events, σ-ﬁelds, and so\n",
      "on. In this case, the collection C in Deﬁnition 1.7 is a sequence. For events C =\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6\n",
      "1 Probability Theory\n",
      "A1, A2, . . ., which we may write as {An}, we say the sequence is a sequence of\n",
      "independent events. We also may abuse the terminology slightly and say that\n",
      "“the sequence is independent”. Similarly, we speak of independent sequences\n",
      "of collections of events or of Borel functions.\n",
      "Notice that each pair of events within a collection of events may be inde-\n",
      "pendent, but the collection itself is not independent.\n",
      "Example 1.1 pairwise independence\n",
      "Consider an experiment of tossing a coin twice. Let\n",
      "A be “heads on the ﬁrst toss”\n",
      "B be “heads on the second toss”\n",
      "C be “exactly one head and one tail on the two tosses”\n",
      "We see immediately that any pair is independent, but that the three events\n",
      "are not independent; in fact, the intersection is ∅.\n",
      "We refer to this situation as “pairwise independent”. The phrase “mutually\n",
      "independent”, is ambiguous, and hence, is best avoided. Sometimes people use\n",
      "the phrase “mutually independent” to try to emphasize that we are referring to\n",
      "independence of all events, but the phrase can also be interpreted as “pairwise\n",
      "independent”.\n",
      "Notice that an event is independent of itself if its probability is 0 or 1.\n",
      "If collections of sets that are independent are closed wrt intersection, then\n",
      "the σ-ﬁelds generated by those collections are independent, as the following\n",
      "theorem asserts.\n",
      "Theorem 1.1\n",
      "Let (Ω, F, P ) be a probability space and suppose Ci ⊆F for i ∈I are inde-\n",
      "pendent collections of events. If ∀i ∈I, A, B ∈Ci ⇒A ∩B ∈Ci, then σ(Ci)\n",
      "for i ∈I are independent.\n",
      "Proof. Exercise.\n",
      "Independence also applies to the complement of a set, as we see next.\n",
      "Theorem 1.2\n",
      "Let (Ω, F, P ) be a probability space. Suppose A, B ∈F are independent. Then\n",
      "A and Bc are independent.\n",
      "Proof. We have\n",
      "P (A) = P (A ∩B) + P (A ∩Bc),\n",
      "hence,\n",
      "P (A ∩Bc) = P (A)(1 −P (B))\n",
      "= P (A)P (Bc),\n",
      "and so A and Bc are independent.\n",
      "In the interesting cases in which the events have equal probability, a con-\n",
      "cept closely related to independence is exchangeability. We deﬁne exchange-\n",
      "ability in a probability space in three steps, similar to those in the deﬁnition\n",
      "of independence.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "7\n",
      "Deﬁnition 1.8 (exchangeability)\n",
      "Let (Ω, F, P ) be a probability space.\n",
      "1. Exchangeability of events within a collection of events.\n",
      "Let C = {Ai : i ∈I}} for some index set I be a collection of events; that\n",
      "is, a collection of subsets of F. Let n be any positive integer (less than\n",
      "or equal to #(C) if #(C) < ∞) and let {i1, . . ., in} and {j1, . . ., jn} each\n",
      "be sets of distinct positive integers in I. The events in C are exchangeable\n",
      "iﬀfor any positive integer n and distinct events Ai1, . . ., Ain and distinct\n",
      "events Aj1, . . ., Ajn in C,\n",
      "P (∪n\n",
      "k=1Aik) = P (∪n\n",
      "k=1Ajk).\n",
      "(1.4)\n",
      "2. Exchangeability of collections of events (and, hence, of σ-ﬁelds).\n",
      "For any index set I, let Ci be a collection of sets with Ci ⊆F. The\n",
      "collections Ci are exchangeable iﬀthe events in any collection of the form\n",
      "{Ai ∈Ci : i ∈I} are exchangeable.\n",
      "3. Exchangeability of Borel functions (and, hence, of random variables,\n",
      "which are special functions deﬁned below).\n",
      "For i in some index set I, the Borel-measurable functions Xi are exchange-\n",
      "able iﬀσ(Xi) for i ∈I are exchangeable.\n",
      "(This also deﬁnes exchangeability of any generators of σ-ﬁelds.)\n",
      "Notice that events being exchangeable requires that they have equal proba-\n",
      "bilities.\n",
      "As mentioned following Deﬁnition 1.7, we will often consider a sequence\n",
      "or a process of events, σ-ﬁelds, and so on. In this case, the collection C in\n",
      "Deﬁnition 1.8 is a sequence, and we may say the sequence {An} is a sequence\n",
      "of exchangeable events. Similarly, we speak of exchangeable sequences of col-\n",
      "lections of events or of Borel functions. As with independence, we also may\n",
      "abuse the terminology slightly and say that “the sequence is exchangeable”.\n",
      "For events with equal probabilities, independence implies exchangeability,\n",
      "but exchangeability does not imply independence.\n",
      "Theorem 1.3\n",
      "Let (Ω, F, P ) be a probability space and suppose C ⊆F is a collection of inde-\n",
      "pendent events with equal probabilities. Then C is an exchangeable collection\n",
      "of events.\n",
      "Proof. Exercise.\n",
      "The next example shows that exchangeability does not imply indepen-\n",
      "dence.\n",
      "Example 1.2 independence and exchangeability\n",
      "A simple urn example may illustrate the diﬀerence in exchangeability and\n",
      "independence. Suppose an urn contains 3 balls, 2 of which are red and 1 of\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8\n",
      "1 Probability Theory\n",
      "which is not red. We “randomly” draw balls from the urn without replacing\n",
      "them (that is, if there are n balls to draw from, the probability that any\n",
      "speciﬁc one is drawn is 1/n).\n",
      "Let Ri be the event that a red ball is drawn on the ith draw, and Rc\n",
      "i be\n",
      "the event that a non-red ball is drawn. We see the following\n",
      "Pr(R1) = Pr(R2) = Pr(R3) = 2/3\n",
      "and\n",
      "Pr(Rc\n",
      "1) = Pr(Rc\n",
      "2) = Pr(Rc\n",
      "3) = 1/3.\n",
      "Now\n",
      "Pr(R1 ∩R2) = 1/3;\n",
      "hence, R1 and R2 are not independent. Similarly, we can see that R1 and R3\n",
      "are not independent and that R2 and R3 are not independent. Hence, the col-\n",
      "lection {R1, R2, R3} is certainly not independent (in fact, Pr(R1 ∩R2 ∩R3) =\n",
      "0). The events R1, R2, and R3 are exchangeable, however. The probabilities\n",
      "of singletons are equal and of course the probability of the full set is equal to\n",
      "itself however it is ordered, so all we need to check are the probabilities of the\n",
      "doubletons:\n",
      "Pr(R1 ∪R2) = Pr(R1 ∪R3) = Pr(R2 ∪R3) = 1.\n",
      "Using a binomial tree, we could extend the computations in the preceding\n",
      "example for an urn containing n balls m of which are red, with the events\n",
      "Ri deﬁned as before, to see that the elements of any subset of the m Ris is\n",
      "exchangeable, but that they are not independent.\n",
      "While, of course, checking deﬁnitions explicitly is necessary, it is useful to\n",
      "develop an intuition for such properties as independence and exchangeabil-\n",
      "ity. A little simple reasoning about the urn problem of Example 1.2 should\n",
      "provide heuristic justiﬁcation for exchangeability as well as for the lack of\n",
      "independence.\n",
      "1.1.2 Random Variables\n",
      "In many applications of probability concepts, we deﬁne a measurable function\n",
      "X, called a random variable, from (Ω, F) to (IRd, Bd):\n",
      "X : (Ω, F) 7→\n",
      "\u0010\n",
      "IRd, Bd\u0011\n",
      ".\n",
      "(1.5)\n",
      "The random variable, together with a probability measure, P , on the measur-\n",
      "able space (Ω, F) determines a new probability space (IRd, Bd, P ◦X−1).\n",
      "We can study the properties of the probability space (Ω, F, P ) through\n",
      "the random variable and the probability space (IRd, Bd, P ◦X−1), which is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "9\n",
      "easier to work with because the sample space, X[Ω], is IRd or some subset of\n",
      "it, rather than some abstract set Ω. In most applications, it is more natural\n",
      "to begin with Ωas some subset, X , of IRd, to develop a vague notion of some\n",
      "σ-ﬁeld on X , and to deﬁne a random variable that relates in a meaningful way\n",
      "to the problem being studied.\n",
      "The mapping of the random variable allows us to assign meaning to the\n",
      "elements of Ωconsistent with the application of interest. The properties of\n",
      "one space carry over to the other one, subject to the random variable, and we\n",
      "may refer to objects of either space equivalently. Random variables allow us\n",
      "to develop a theory of probability that is useful in statistical applications.\n",
      "Deﬁnition 1.9 (random variable)\n",
      "A measurable function, X(ω) or just X, from a measurable space (Ω, F) to\n",
      "the measurable space (IRd, Bd) is called a random variable, or, to be more\n",
      "speciﬁc, a d-variate random variable.\n",
      "This deﬁnition means that “Borel function” (see page 719) and “random\n",
      "variable” are synonymous. Notice that the words “random” and “variable” do\n",
      "not carry any separate meaning.\n",
      "Many authors deﬁne a random variable only for the case d = 1, and for\n",
      "the case of d ≥1, call the Borel function a “random vector”. I see no reason\n",
      "for this distinction. Recall that I use “real” to refer to an element of IRd for\n",
      "any positive integer d. My usage is diﬀerent from an alternate usage in which\n",
      "“real” means what I call a “real scalar”; in that alternate usage, a random\n",
      "variable takes values only in IR.\n",
      "We often denote the image of X, that is, X[Ω], as X . If B ∈B(X ), then\n",
      "X−1[B] ∈F.\n",
      "Although we deﬁne the random variable X to be real, we could form a\n",
      "theory of probability and statistics that allowed X to be a function into a\n",
      "general ﬁeld. Complex-valued random variables are often useful, especially,\n",
      "for example, in harmonic analysis of such things as electrical signals, but we\n",
      "will not consider them in any detail in this text.\n",
      "Notice that a random variable is ﬁnite a.s. If this were not the case, certain\n",
      "problems would arise in the deﬁnitions of some useful functions of the random\n",
      "variable that we will discuss below.\n",
      "A random variable could perhaps more appropriately be deﬁned as an\n",
      "equivalence class of real-valued measurable functions that are equal almost\n",
      "surely; that is, a class in which if X\n",
      "a.s.\n",
      "= Y , then X and Y are the same\n",
      "random variable.\n",
      "Note that a real constant is a random variable. If c is a real constant and\n",
      "if X a.s.\n",
      "= c, then we call X a degenerate random variable; that is, any constant\n",
      "c is a degenerate random variable. We call a random variable that is not a\n",
      "degenerate random variable a nondegenerate random variable.\n",
      "Another comment on a.s. may be in order here. The expression “X ̸= c\n",
      "a.s.” means the measure of Ωc = {ω : X(ω) = c} is 0. (That is, the expression\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "10\n",
      "1 Probability Theory\n",
      "does not mean the there is some set with positive measure on which X(ω) ̸= c.)\n",
      "Similar interpretations apply to other expressions such as “X > c a.s.”.\n",
      "Simple Random Variables\n",
      "Some useful random variables assume only a ﬁnite number of diﬀerent values;\n",
      "these are called simple random variables because they are simple functions.\n",
      "Deﬁnition 1.10 (simple random variable)\n",
      "A random variable that has a ﬁnitely-countable range is called a simple ran-\n",
      "dom variable.\n",
      "This is just the same as a simple function, Deﬁnition 0.1.28 on page 719.\n",
      "We will also speak of “discrete” random variables. A discrete random vari-\n",
      "able has a countable range. A simple random variable is discrete, but a discrete\n",
      "random variable is not necessarily simple.\n",
      "σ-Fields Generated by Random Variables\n",
      "A random variable deﬁned on (Ω, F) determines a useful sub-σ-ﬁeld of F.\n",
      "First, we establish that a certain collection of sets related to a measurable\n",
      "function is a σ-ﬁeld.\n",
      "Theorem 1.4\n",
      "If X : Ω7→X ⊆IRd is a random variable, then σ(X−1[B(X )]) is a sub-σ-ﬁeld\n",
      "of F.\n",
      "Proof. Exercise. (Note that instead of B(X ) we could write Bd.)\n",
      "Now we give a name to that collection of sets.\n",
      "Deﬁnition 1.11 (σ-ﬁeld generated by a random variable)\n",
      "Let X : Ω7→IRd be a random variable. We call σ(X−1[Bd]) the σ-ﬁeld gener-\n",
      "ated by X and denote it as σ(X).\n",
      "Theorem 1.4 ensures that σ(X) is a σ-ﬁeld and in fact a sub-σ-ﬁeld of F.\n",
      "If X and Y are random variables deﬁned on the same measurable space,\n",
      "we may write σ(X, Y ), with the obvious meaning (see equation (0.1.5) on\n",
      "page 704). As with σ-ﬁelds generated by sets or functions discussed in Sec-\n",
      "tions 0.1.1 and 0.1.2, it is clear that σ(X) ⊆σ(X, Y ). This idea of sub-σ-ﬁelds\n",
      "generated by random variables is important in the analysis of a sequence of\n",
      "random variables. (It leads to the ideas of a ﬁltration; see page 125.)\n",
      "Random Variables and Probability Distributions\n",
      "Notice that a random variable is deﬁned in terms only of a measurable space\n",
      "(Ω, F) and a measurable space deﬁned on the reals (X , Bd). No associated\n",
      "probability measure is necessary for the deﬁnition, but for meaningful appli-\n",
      "cations of a random variable, we need some probability measure. For a random\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "11\n",
      "variable X deﬁned on (Ω, F) in the probability space (Ω, F, P ), the probability\n",
      "measure of X is P ◦X−1. (This is a pushforward measure; see page 712. In\n",
      "Exercise 1.9, you are asked to show that it is a probability measure.)\n",
      "A probability space is also called a population, a probability distribution,\n",
      "a distribution, or a law. The probability measure itself is the ﬁnal component\n",
      "that makes a measurable space a probability space, so we associate the distri-\n",
      "bution most closely with the measure. Thus, “P ” may be used to denote both\n",
      "a population and the associated probability measure. We use this notational\n",
      "convention throughout this book.\n",
      "For a given random variable X, a probability distribution determines\n",
      "Pr(X ∈B) for B ⊆X . The underlying probability measure P of course\n",
      "determines Pr(X−1 ∈A) for A ∈F.\n",
      "Quantiles\n",
      "Because the values of random variables are real, we can deﬁne various special\n",
      "values that would have no meaning in an abstract sample space. As we develop\n",
      "more structure on a probability space characterized by a random variable, we\n",
      "will deﬁne a number of special values relating to the random variable. Without\n",
      "any further structure, at this point we can deﬁne a useful value of a random\n",
      "variable that just relies on the ordering of the real numbers.\n",
      "For the random variable X ∈IR and given π ∈]0, 1[, the quantity xπ\n",
      "deﬁned as\n",
      "xπ = inf{x, s.t. Pr(X ≤x) ≥π}\n",
      "(1.6)\n",
      "is called the π quantile of X.\n",
      "For the random variable X ∈IRd, there are two ways we can interpret\n",
      "the quantiles. If the probability associated with the quantile, π, is a scalar,\n",
      "then the quantile is a level curve or contour in X ∈IRd. Such a quantile is\n",
      "obviously much more complicated, and hence, less useful, than a quantile in a\n",
      "univariate distribution. If π is a d-vector, then the deﬁnition in equation (1.6)\n",
      "applies to each element of X and the quantile is a point in IRd.\n",
      "Multiple Random Variables on the Same Probability Space\n",
      "If two random variables X and Y have the same distribution, we write\n",
      "X d= Y.\n",
      "(1.7)\n",
      "We say that they are identically distributed. Note the diﬀerence in this and\n",
      "the case in which we say X and Y are the same random variable. If X and Y\n",
      "are the same random variable, then X a.s.\n",
      "= Y . It is clear that\n",
      "X\n",
      "a.s.\n",
      "= Y =⇒X\n",
      "d= Y,\n",
      "(1.8)\n",
      "but the implication does not go the other way. (A simple example, using\n",
      "notation to be developed later, is the following. Let X ∼U(0, 1), and let\n",
      "Y = 1 −X. Then X d= Y but clearly it is not the case that X a.s.\n",
      "= Y .)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "12\n",
      "1 Probability Theory\n",
      "Support of a Random Variable\n",
      "Deﬁnition 1.12 (support of a distribution or of a random variable)\n",
      "The support of the distribution (or of the random variable) is the smallest\n",
      "closed set XS in the image of X such that P (X−1[XS]) = 1.\n",
      "We have seen that a useful deﬁnition of the support of a general measure\n",
      "requires some structure on the measure space (see page 710). Because the\n",
      "range of a random variable has suﬃcient structure (it is a metric space), in\n",
      "Deﬁnition 1.12, we arrive at a useful concept, while avoiding the ambiguities\n",
      "of a general probability space.\n",
      "Product Distribution\n",
      "If X1 and X2 are independent random variables with distributions P1 and P2,\n",
      "we call the joint distribution of (X1, X2) the product distribution of P1 and\n",
      "P2.\n",
      "Parameters, Parameter Spaces, and Parametric Families\n",
      "We often restrict our attention to a probability family or a family of distribu-\n",
      "tions, P = {Pθ}, where θ is some convenient index.\n",
      "Deﬁnition 1.13 (parametric family of probability distributions)\n",
      "A family of distributions on a measurable space (Ω, F) with probability mea-\n",
      "sures Pθ for θ ∈Θ is called a parametric family if Θ ⊆IRk for some ﬁxed\n",
      "positive integer k and θ fully determines the measure. We call θ the parame-\n",
      "ter and Θ the parameter space.\n",
      "If the dimension of Θ is large (there is no precise meaning of “large”\n",
      "here), we may refrain from calling θ a parameter, because we want to refer\n",
      "to some statistical methods as “nonparametric”. (In nonparametric methods,\n",
      "our analysis usually results in some general description of the distribution,\n",
      "rather than in a speciﬁcation of the distribution.)\n",
      "We assume that every parametric family is identiﬁable; that is, P =\n",
      "{Pθ, θ ∈Θ} is an identiﬁable parametric family if it is a parametric fam-\n",
      "ily and for θ1, θ2 ∈Θ if θ1 ̸= θ2 then Pθ1 ̸= Pθ2.\n",
      "A family that cannot be indexed in this way might be called a nonpara-\n",
      "metric family. The term “nonparametric” is most commonly used to refer to\n",
      "a statistical procedure, rather than to a family, however. In general terms, a\n",
      "nonparametric procedure is one that does not depend on strict assumptions\n",
      "about a parametric family.\n",
      "Example 1.3 a parametric family\n",
      "An example of a parametric family of distributions for the measurable space\n",
      "(Ω= {0, 1}, F = 2Ω) is that formed from the class of the probability measures\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "13\n",
      "Pπ({1}) = π and Pπ({0}) = 1 −π. This is a parametric family, namely, the\n",
      "Bernoulli distributions. The index of the family, π, is called the parameter of\n",
      "the distribution. The measures are dominated by the counting measure.\n",
      "Example 1.4 a nonparametric family\n",
      "An example of a nonparametric family over a measurable space (IR, B) is\n",
      "Pc = {P : P ≪ν}, where ν is the Lebesgue measure. This family contains\n",
      "all of the parametric families of Tables A.2 through A.6 of Appendix A as\n",
      "well as many other families.\n",
      "There are a number of useful parametric distributions to which we give\n",
      "names. For example, the normal or Gaussian distribution, the binomial dis-\n",
      "tribution, the chi-squared, and so on. Each of these distributions is actually a\n",
      "family of distributions. A speciﬁc member of the family is speciﬁed by speci-\n",
      "fying the value of each parameter associated with the family of distributions.\n",
      "For a few distributions, we introduce special symbols to denote the dis-\n",
      "tribution. We use N(µ, σ2) to denote a univariate normal distribution with\n",
      "parameters µ and σ2 (the mean and variance). To indicate that a random\n",
      "variable has a normal distribution, we use notation of the form\n",
      "X ∼N(µ, σ2),\n",
      "which here means that the random variable X has a normal distribution with\n",
      "parameters µ and σ2. We use\n",
      "Nd(µ, Σ)\n",
      "to denote a d-variate normal distribution with parameters µ and Σ\n",
      "We use\n",
      "U(θ1, θ2)\n",
      "to denote a uniform distribution with support [θ1, θ2]. The most common\n",
      "uniform distribution that we will use is U(0, 1).\n",
      "In some cases, we also use special symbols to denote random variables with\n",
      "particular distributions. For example, we often use χ2\n",
      "ν to denote a random\n",
      "variable with a chi-squared distribution with ν degrees of freedom.\n",
      "In Chapter 2 I discuss types of families of probability distributions, and\n",
      "in Tables A.1 through A.6 beginning on page 838 of Appendix A we give\n",
      "descriptions of some parametric families.\n",
      "The Cumulative Distribution Function (CDF)\n",
      "The cumulative distribution function provides an alternative expression of\n",
      "a probability measure on IRd. This function gives a clearer picture of the\n",
      "probability distribution, and also provides the basis for deﬁning other useful\n",
      "functions for studying a distribution.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "14\n",
      "1 Probability Theory\n",
      "Deﬁnition 1.14 (cumulative distribution function (CDF))\n",
      "If (IRd, Bd, P ) is a probability space, and F is deﬁned by\n",
      "F (x) = P (] −∞, x])\n",
      "∀x ∈IRd,\n",
      "(1.9)\n",
      "then F is called a cumulative distribution function, or CDF.\n",
      "The CDF is also called the distribution function, or DF.\n",
      "There are various forms of notation used for CDFs. The CDF of a given\n",
      "random variable X is often denoted as FX. A CDF in a parametric family Pθ\n",
      "is often denoted as Fθ, or as F (x; θ).\n",
      "If the probability measure P is dominated by the measure ν, then we also\n",
      "say that the associated CDF F is dominated by ν.\n",
      "The probability space completely determines F , and likewise, F completely\n",
      "determines P a.s.; hence, we often use the CDF and the probability measure\n",
      "interchangeably. More generally, given the probability space (Ω, F, P ) and the\n",
      "random variable X deﬁned on that space, if F is the CDF of X, the basic\n",
      "probability statement for an event A ∈F given in equation (1.2) can be\n",
      "written as\n",
      "P (A) =\n",
      "Z\n",
      "A\n",
      "dP =\n",
      "Z\n",
      "X[A]\n",
      "dF.\n",
      "(1.10)\n",
      "If the random variable is assumed to be in a family of distributions indexed\n",
      "by θ, we may use the notation Fθ(x) or F (x; θ).\n",
      "For a given random variable X, F (x) = Pr(X ≤x). We sometimes use the\n",
      "notation FX(x) to refer to the CDF of the random variable X.\n",
      "For a given CDF F , we deﬁne F called the tail CDF by\n",
      "F(x) = 1 −F (x).\n",
      "(1.11)\n",
      "This function, which is also denoted by F C, is particularly interesting for\n",
      "random variables whose support is IR+.\n",
      "The CDF is particularly useful in the case d = 1. (If X is a vector-valued\n",
      "random variable, and x is a vector of the same order, X ≤x is interpreted to\n",
      "mean that Xi ≤xi for each respective element.)\n",
      "Theorem 1.5 (properties of a CDF)\n",
      "If F is a CDF then\n",
      "1. limx↓−∞F (x) = 0.\n",
      "2. limx↑∞F (x) = 1.\n",
      "3. F (x1) ≤F (x2) if x1 < x2.\n",
      "4. limϵ↓0 F (x + ϵ) = F (x). (A CDF is continuous from the right.)\n",
      "Proof. Each property is an immediate consequence of the deﬁnition.\n",
      "These four properties characterize a CDF, as we see in the next theorem.\n",
      "Theorem 1.6\n",
      "If F is a function deﬁned on IRd that satisﬁes the properties of Theorem 1.5,\n",
      "then F is a CDF (for some probability space).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "15\n",
      "Proof. Exercise. (Hint: Given (IRd, Bd) and a function F deﬁned on IRd sat-\n",
      "isfying the properties of Theorem 1.5, deﬁne P as\n",
      "P (] −∞, x]) = F (x)\n",
      "∀x ∈IRd\n",
      "and show that P is a probability measure.)\n",
      "Because the four properties of Theorem 1.5 characterize a CDF, they can\n",
      "serve as an alternate deﬁnition of a CDF, without reference to a probability\n",
      "distribution. Notice, for example, that the Cantor function (see Section 0.1.5)\n",
      "is a CDF if we extend its deﬁnition to be 0 on ] −∞, 0[ and to be 1 on ]1, ∞[.\n",
      "The distribution associated with this CDF has some interesting properties;\n",
      "see Exercise 1.12.\n",
      "One of the most useful facts about an absolutely continuous CDF is its\n",
      "relation to a U(0, 1) distribution.\n",
      "Theorem 1.7\n",
      "If X is a random variable with absolutely continuous CDF F then F (X) ∼\n",
      "U(0, 1).\n",
      "Proof. If X is a random variable with CDF F then\n",
      "Pr(F (X) ≤t) =\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "t < 0\n",
      "t\n",
      "0 ≤t < 1\n",
      "1\n",
      "1 ≤t.\n",
      "This set of probabilities characterize the U(0, 1) distribution.\n",
      "Although Theorem 1.7 applies to continuous random variables, a discrete\n",
      "random variable has a similar property when we “spread out” the probability\n",
      "between the mass points.\n",
      "The Quantile Function: The Inverse of the CDF\n",
      "Although as I indicated above, quantiles can be deﬁned for random variables\n",
      "in IRd for general positive integer d, they are more useful for d = 1. I now\n",
      "deﬁne a useful function for that case. (The function could be generalized, but,\n",
      "again, the generalizations are not as useful.)\n",
      "Deﬁnition 1.15 (quantile function)\n",
      "If (IR, B, P ) is a probability space with CDF F , and F −1 is deﬁned on ]0, 1[\n",
      "by\n",
      "F −1(p) = inf{x, s.t. F (x) ≥p},\n",
      "(1.12)\n",
      "then F −1 is called a quantile function.\n",
      "Notice that if F is strictly increasing, the quantile function is the ordinary\n",
      "inverse of the cumulative distribution function. If F is not strictly increasing,\n",
      "the quantile function can be interpreted as a generalized inverse of the cumu-\n",
      "lative distribution function. This deﬁnition is reasonable (at the expense of\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "16\n",
      "1 Probability Theory\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "x\n",
      "CDF\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "p\n",
      "Quantile\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "x\n",
      "CDF\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "p\n",
      "Quantile\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "x\n",
      "CDF\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "p\n",
      "Quantile\n",
      "Figure 1.1.\n",
      "CDFs and Quantile Functions\n",
      "overloading the notation “·−1”) because, while a CDF may not be an invertible\n",
      "function, it is monotonic nondecreasing.\n",
      "Notice that for the random variable X with CDF F , if\n",
      "xπ = F −1(π),\n",
      "(1.13)\n",
      "then xπ is the π quantile of X as deﬁned in equation (1.6). Equation (1.13)\n",
      "is usually taken as the deﬁnition of the π quantile.\n",
      "The quantile function, just as the CDF, fully determines a probability\n",
      "distribution.\n",
      "Theorem 1.8 (properties of a quantile function)\n",
      "If F −1 is a quantile function and F is the associated CDF,\n",
      "1. F −1(F (x)) ≤x.\n",
      "2. F (F −1(p)) ≥p.\n",
      "3. F −1(p) ≤x ⇐⇒p ≤F (x).\n",
      "4. F −1(p1) ≤F −1(p2) if p1 ≤p2.\n",
      "5. limϵ↓0 F −1(p −ϵ) = F −1(p).\n",
      "(A quantile function is continuous from the left.)\n",
      "6. If U is a random variable distributed uniformly over ]0, 1[, then X =\n",
      "F −1(U) has CDF F .\n",
      "Proof. Exercise.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "17\n",
      "The ﬁrst ﬁve properties of a quantile function given in Theorem 1.8 char-\n",
      "acterize a quantile function, as stated in the following theorem.\n",
      "Theorem 1.9\n",
      "Let F be a CDF and let G be function such that\n",
      "1. G(F (x)) ≤x,\n",
      "2. F (G(p)) ≥p,\n",
      "3. G(p) ≤x ⇐⇒p ≤F (x),\n",
      "4. G(p1) ≤G(p2) if p1 ≤p2, and\n",
      "5. limϵ↓0 G(p −ϵ) = G(p).\n",
      "Then G is the quantile function associated with F , that is, G = F −1.\n",
      "Proof. Exercise. (The deﬁnitions of a CDF and a quantile function are suﬃ-\n",
      "cient.)\n",
      "As we might expect, the quantile function has many applications that\n",
      "parallel those of the CDF. For example, we have an immediate corollary to\n",
      "Theorem 1.7.\n",
      "Corollary 1.7.1\n",
      "If F is a CDF and U ∼U(0, 1), then F −1(U) is a random variable with CDF\n",
      "F .\n",
      "Corollary 1.7.1 is actually somewhat stronger than Theorem 1.7 because no\n",
      "modiﬁcation is needed for discrete distributions. One of the most common\n",
      "applications of this fact is in random number generation, because the basic\n",
      "pseudorandom variable that we can simulate has a U(0, 1) distribution.\n",
      "The Probability Density Function: The Derivative of the CDF\n",
      "Another function that may be very useful in describing a probability distribu-\n",
      "tion is the probability density function. This function also provides a basis for\n",
      "straightforward deﬁnitions of meaningful characteristics of the distribution.\n",
      "Deﬁnition 1.16 (probability density function (PDF))\n",
      "The derivative of a CDF (or, equivalently, of the probability measure) with\n",
      "respect to an appropriate measure, if it exists, is called the probability density\n",
      "function, PDF.\n",
      "The PDF is also called the density function.\n",
      "There are various forms of notation used for PDFs. As I mentioned on\n",
      "page 14, common forms of notation for CDFs are FX, Fθ, and F (x; θ). Of\n",
      "course, instead of “F ”, other upper case letters such as G and H are often\n",
      "used similarly. The common notation for the associated PDF parallels that of\n",
      "the CDF and uses the corresponding lower case letter, for example, fX, fθ, and\n",
      "f(x; θ). I will use the standard forms of mathematical notation for functions\n",
      "for denoting CDFs and PDFs. (Other statisticians often use a sloppy notation,\n",
      "called “generic notation”; see page 21.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "18\n",
      "1 Probability Theory\n",
      "Theorem 1.10 (properties of a PDF)\n",
      "Let F be a CDF deﬁned on IRd dominated by the measure ν. Let f be the PDF\n",
      "deﬁned as\n",
      "f(x) = dF (x)\n",
      "dν\n",
      ".\n",
      "Then over IRd\n",
      "f(x) ≥0\n",
      "a.e. ν,\n",
      "(1.14)\n",
      "f(x) < ∞\n",
      "a.e. ν,\n",
      "(1.15)\n",
      "and\n",
      "Z\n",
      "IRd fdν = 1.\n",
      "(1.16)\n",
      "If XS is the support of the distribution, then\n",
      "0 < f(x) < ∞\n",
      "∀x ∈XS.\n",
      "Proof. Exercise.\n",
      "A characteristic of some distributions that is easily deﬁned in terms of the\n",
      "PDF is the mode.\n",
      "Deﬁnition 1.17 (mode of a probability distribution)\n",
      "If x0 is a point in the support XS of a distribution with PDF f such that\n",
      "f(x0) ≥f(x),\n",
      "∀x ∈XS,\n",
      "then x0 is called a mode of the distribution.\n",
      "If the mode exists it may or may not be unique.\n",
      "Dominating Measures\n",
      "Although we use the term “PDF” and its synonyms for either discrete random\n",
      "variables and the counting measure or for absolutely continuous random vari-\n",
      "ables and Lebesgue measure, there are some diﬀerences in the interpretation\n",
      "of a PDF of a discrete random variable and a continuous random variable. In\n",
      "the case of a discrete random variable X, the value of the PDF at the point x is\n",
      "the probability that X = x; but this interpretation does not hold for a contin-\n",
      "uous random variable. For this reason, the PDF of a discrete random variable\n",
      "is often called a probability mass function, or just probability function. There\n",
      "are some concepts deﬁned in terms of a PDF, such as self-information, that\n",
      "depend on the PDF being a probability, as it would be in the case of discrete\n",
      "random variables.\n",
      "The general meaning of the term “discrete random variable” is that the\n",
      "probability measure is dominated by the counting measure; and likewise for\n",
      "a “absolutely continuous random variable” the general meaning is that the\n",
      "probability measure is dominated by Lebesgue measure. Any simple CDF\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "19\n",
      "has a PDF wrt the counting measure, but not every continuous CDF has a\n",
      "PDF wrt Lebesgue measure (the Cantor function, see page 723, is a classic\n",
      "counterexample – see Exercise 1.12b), but every absolutely continuous CDF\n",
      "does have a PDF wrt Lebesgue measure.\n",
      "The “appropriate measure” in the deﬁnition of PDF above must be σ-\n",
      "ﬁnite and must dominate the CDF. For a random variable X = (X1, . . ., Xd)\n",
      "with CDF FX(x) dominated by Lebesgue measure, the PDF, if it exists, is\n",
      "∂dFX(x)/∂x1 · · ·∂xd.\n",
      "In most distributions of interest there will be a PDF wrt a σ-ﬁnite measure.\n",
      "Many of our deﬁnitions and theorems will begin with a statement that includes\n",
      "a phrase similar to “a distribution with a PDF wrt a σ-ﬁnite measure”. In\n",
      "the case of a discrete random variable, that σ-ﬁnite measure is the counting\n",
      "measure (Deﬁnition 0.1.20 on page 708), and in the case of an absolutely\n",
      "continuous random variable, it is the Lebesgue measure (see page 717).\n",
      "Parameters and PDFs\n",
      "In addition to being functions of points x in the support, the PDFs of the\n",
      "distributions within a given parametric family Pθ, are also functions of θ.\n",
      "There may also be other constants in the PDF that can be separated from\n",
      "the functional dependence on x. It is often of interest to focus on the PDF\n",
      "solely as a function of x. (This may be because in applications, the “inverse”\n",
      "problem of deciding on the form of the PDF is simpler if we consider only the\n",
      "role of the observable x.) Given a PDF fθ(x), a useful decomposition is\n",
      "fθ(x) = g(θ)k(x),\n",
      "(1.17)\n",
      "where 0 < g and 0 ≤k, and k(x) encodes all of the dependence of fθ(x) on\n",
      "x. In this decomposition, we call k(x) the “kernel”.\n",
      "From equation (1.16), we have\n",
      "1\n",
      "g(θ) =\n",
      "Z\n",
      "IRd k(x)dν(x).\n",
      "(1.18)\n",
      "The function (g(θ))−1 is called the “normalizing function” or the “partition\n",
      "function”. (The latter name comes from statistical physics, and in many areas\n",
      "of application, the partition function is a meaningful characteristic of the\n",
      "problem. Both or either of these names is sometimes applied to g(θ).)\n",
      "The Likelihood; A Function of the Parameters\n",
      "It is often useful to consider the PDF (or CDF) as a function of the param-\n",
      "eter instead of the range of the random variable. We call such a function, a\n",
      "likelihood function, and for the parametric PDF fθ(x) with support XS and\n",
      "parameter space Θ, we denote the corresponding likelihood function as L(θ; x):\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "20\n",
      "1 Probability Theory\n",
      "L(θ; x) = fθ(x),\n",
      "∀θ ∈Θ, ∀x ∈XS.\n",
      "(1.19)\n",
      "(Actually, any positive scalar multiple of L(θ; x) in (1.19), is called a likelihood\n",
      "function corresponding to fθ(x). I will discuss likelihood functions later in\n",
      "more detail, particularly in Chapter 6, where their use in statistical inference\n",
      "is discussed.)\n",
      "While the likelihood and the PDF are the same, at any two ﬁxed points x\n",
      "and θ, they diﬀer fundamentally as functions.\n",
      "Example 1.5 likelihood in the exponential family\n",
      "Consider the exponential family of distributions with parameter θ. The PDF\n",
      "is\n",
      "pX(x ; θ) = θ−1e−x/θI¯IR+(x),\n",
      "(1.20)\n",
      "for θ ∈IR+. Plots of this PDF for θ = 1 and θ = 5 are shown on the left side\n",
      "of Figure 1.2.\n",
      "Given a single observation x, the likelihood is\n",
      "L(θ ; x) = θ−1e−x/θIIR+(θ).\n",
      "(1.21)\n",
      "Plots of this likelihood for x = 1 and x = 5 are shown on the right side of\n",
      "Figure 1.2.\n",
      "PDF   pX(x;θ)\n",
      ".5\n",
      "0\n",
      "x\n",
      "θ=1\n",
      "θ=5\n",
      "Likelihood   L(θ;x)\n",
      ".5\n",
      "0\n",
      "θ\n",
      "x=1\n",
      "x=5\n",
      "Figure 1.2.\n",
      "PDF and Likelihood for Exponential Distribution\n",
      "The log of the likelihood, called the log-likelihood function,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "21\n",
      "lL(θ ; x) = log L(θ ; x),\n",
      "(1.22)\n",
      "is also useful. We often denote the log-likelihood without the “L” subscript.\n",
      "(The notation for the likelihood and the log-likelihood varies with authors. My\n",
      "own choice of an uppercase “L” for the likelihood and a lowercase “l” for the\n",
      "log-likelihood is long-standing, and not based on any notational optimality\n",
      "consideration. Because of the variation in the notation for the log-likelihood,\n",
      "I will often use the “lL” notation because this expression is suggestive of the\n",
      "meaning.)\n",
      "In cases when the likelihood or the log-likelihood is diﬀerentiable wrt the\n",
      "parameter, the derivative is of interest because it indicates the sensitivity of\n",
      "the parametric family to the parameter when it is considered to be a variable.\n",
      "The most useful derivative is that of the log-likelihood, ∂lL(θ; x)/∂θ. The\n",
      "expectation of the square of this appears in a useful quantity in statistics, the\n",
      "Cram´er-Rao lower bound, inequality (3.39) on page 234.\n",
      "Parametric Families\n",
      "As mentioned above, if a speciﬁc CDF is Fθ, we often write the corresponding\n",
      "PDF as fθ:\n",
      "fθ = dFθ\n",
      "dν .\n",
      "(1.23)\n",
      "There may be some ambiguity in the use of such subscripts, however, because\n",
      "when we have deﬁned a speciﬁc random variable, we may use the symbol\n",
      "for the random variable as the identiﬁer of the CDF or PDF. The CDF and\n",
      "the PDF corresponding to a given random variable X are often denoted,\n",
      "respectively, as FX and fX. Adding to this confusion is the common usage by\n",
      "statisticians of the “generic notation”, that is, for given random variables X\n",
      "and Y , the notation f(x) may refer to a diﬀerent function than the notation\n",
      "f(y). I will not use the “generic” notation for CDFs and PDFs.\n",
      "We assume that every parametric family is identiﬁable; that is, if Fθ and\n",
      "fθ are the CDF and PDF for distributions within the family and these are\n",
      "dominated by the measure ν, then over the parameter space Θ for θ1 ̸= θ2,\n",
      "ν({x : Fθ1(x) ̸= Fθ2(x)}) > 0\n",
      "(1.24)\n",
      "and\n",
      "ν({x : fθ1(x) ̸= fθ2(x)}) > 0.\n",
      "(1.25)\n",
      "Dominating Measures\n",
      "The dominating measure for a given probability distribution is not unique,\n",
      "but use of a diﬀerent dominating measure may change the representation of\n",
      "the distribution. For example, suppose that the support of a distribution is\n",
      "S, and so we write the PDF as\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "22\n",
      "1 Probability Theory\n",
      "dFθ(x)\n",
      "dν\n",
      "= gθ(x)IS(x).\n",
      "(1.26)\n",
      "If we deﬁne a measure λ by\n",
      "λ(A) =\n",
      "Z\n",
      "A\n",
      "ISdν\n",
      "∀A ∈F,\n",
      "(1.27)\n",
      "then we could write the PDF as\n",
      "dFθ\n",
      "dλ = gθ.\n",
      "(1.28)\n",
      "Mixtures of Distributions\n",
      "If F1, F2, . . . are CDFs and π1, π2, . . . ≥0 are real constants such that P\n",
      "i πi =\n",
      "1, then\n",
      "F =\n",
      "X\n",
      "i\n",
      "πiFi\n",
      "(1.29)\n",
      "is a CDF (exercise). If each Fi in equation (1.29) is dominated by Lebesgue\n",
      "measure, then F is dominated by Lebesgue measure. Likewise, if each Fi is\n",
      "dominated by the counting measure, then F is dominated by the counting\n",
      "measure.\n",
      "The PDF corresponding to F is also the same linear combination of the\n",
      "corresponding PDFs.\n",
      "It is often useful to form mixtures of distributions of continuous random\n",
      "variables with distributions of discrete random variables. The ϵ-mixture dis-\n",
      "tribution, whose CDF is given in equation (2.45) on page 194, is an example.\n",
      "A mixture distribution can also be thought of as a random variable X\n",
      "whose distribution, randomly, is the same as that of some other random vari-\n",
      "able X1, X2, . . .; that is,\n",
      "X d= XI,\n",
      "where I is a random variable taking values in the index set of X1, X2, . . .. In\n",
      "terms of the πi in equation (1.29), we can deﬁne the random variable of the\n",
      "mixture X by\n",
      "X d= Xi\n",
      "with probability πi.\n",
      "We must be careful not to think of the linear combination of the CDFs or\n",
      "PDFs as applying to the random variables; the random variable of the mixture\n",
      "is not a linear combination of the constituent random variables.\n",
      "Joint and Marginal Distributions\n",
      "For a random variable consisting of two components, (X1, X2), we speak of its\n",
      "distribution as a “joint distribution”, and we call the separate distributions the\n",
      "“marginal distributions”. We might denote the PDF of the joint distribution\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "23\n",
      "as fX1,X2 and the PDF for X1 alone as fX1. We call fX1,X2 a joint PDF and\n",
      "fX1 a marginal PDF.\n",
      "We have the simple relationship\n",
      "fX1(x1) =\n",
      "Z\n",
      "fX1,X2(x1, x2) dx2.\n",
      "(1.30)\n",
      "Independence and Exchangeability of Random Variables\n",
      "We have deﬁned independence and exchangeability in general. We now give\n",
      "equivalent deﬁnitions for random variables.\n",
      "Deﬁnition 1.18 (independence of random variables)\n",
      "The random variables X1, . . ., Xk on (Ω, F, P ) are said to be independent iﬀ\n",
      "for any sets B1, . . ., Bk in the σ-ﬁeld of the image space,\n",
      "P (X1 ∈B1, . . ., Xk ∈Bk) =\n",
      "k\n",
      "Y\n",
      "i=1\n",
      "P (Xi ∈Bi).\n",
      "Notice that this deﬁnition is essentially the same as Deﬁnition 1.7.3 on\n",
      "page 5, and it corresponds to the deﬁnition of independence of events; that is,\n",
      "independence of X−1\n",
      "i\n",
      "[Bi] (Deﬁnition 1.7.1). These are not just separate deﬁ-\n",
      "nitions of independence of various objects. The following theorem (which we\n",
      "could have stated analogously following Deﬁnition 1.7) relates Deﬁnition 1.18\n",
      "above to Deﬁnition 1.7.2.\n",
      "Theorem 1.11\n",
      "The random variables X1, . . ., Xk on (Ω, F, P ) are independent iﬀthe σ-ﬁelds\n",
      "σ(X1), . . ., σ(Xk) are independent.\n",
      "Proof. Exercise.\n",
      "The following factorization theorem is often useful in establishing inde-\n",
      "pendence of random variables.\n",
      "Theorem 1.12\n",
      "If X1 and X2 are random variables with joint PDF fX1,X2 and marginal PDFs\n",
      "fX1 and fX2, then X1 and X2 are independent iﬀ\n",
      "fX1,X2(x1, x2) = fX1(x1)fX2(x2).\n",
      "(1.31)\n",
      "Proof. Exercise.\n",
      "We are often interested in random variables that have the same distribu-\n",
      "tions. In that case, exchangeability of the random variables is of interest.\n",
      "The following deﬁnition of exchangeability is essentially the same as Def-\n",
      "inition 1.8.3, and similar comments relating to exchangeability of random\n",
      "variables, sets, and σ-ﬁelds as made above relating to independence hold.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "24\n",
      "1 Probability Theory\n",
      "Deﬁnition 1.19 (exchangeability of random variables)\n",
      "The random variables X1, . . ., Xk on (Ω, F, P ) are said to be exchangeable\n",
      "iﬀthe joint distribution of X1, . . ., Xk is the same as the joint distribution\n",
      "of Π({X1, . . ., Xk}), for any Π, where Π(A) denotes a permutation of the\n",
      "elements of the set A.\n",
      "As we have seen, exchangeability requires identical distributions, but, given\n",
      "that, it is a weaker property than independence.\n",
      "Example 1.6 Polya’s urn process\n",
      "Consider an urn that initially contains r red and b blue balls. One ball is\n",
      "chosen randomly from the urn, and its color noted. The ball is then put back\n",
      "into the urn together with c balls of the same color. (Hence, the number of\n",
      "total balls in the urn changes. We can allow c = −1, in which case, the drawn\n",
      "ball is not returned to the urn.) Now deﬁne a binary random variable Ri = 1\n",
      "if a red ball is drawn and Ri = 0 if a blue ball is drawn. (The random variable\n",
      "Ri in this example is the indicator function for the event Ri in Example 1.2.)\n",
      "The sequence R1, R2, . . . is exchangeable, but not independent.\n",
      "An interesting fact about inﬁnite sequences of exchangeable binary random\n",
      "variables, such as those in Example 1.6 with c ≥0, is that they are mixtures\n",
      "of independent Bernoulli sequences; see page 75. This provides a link between\n",
      "exchangeability and independence. This connection between exchangeability\n",
      "and independence does not necessarily hold in ﬁnite sequences, as in the urn\n",
      "process of Example 1.2.\n",
      "Random Samples\n",
      "If the random variables X1, . . ., Xk are independent and\n",
      "X1\n",
      "d= · · · d= Xk,\n",
      "we say X1, . . ., Xk are identically and independently distributed, which we\n",
      "denote as iid. A set of iid random variables is called a simple random sample,\n",
      "and the cardinality of the set is called the “size” of the simple random sample.\n",
      "The common distribution of the variables is called the parent distribution of\n",
      "the simple random sample. We also often use the phrase “excangeable random\n",
      "sample”, with the obvious meaning.\n",
      "There are many situations in which a sample is generated randomly, but\n",
      "the sample is not a simple random sample or even an exchangeable random\n",
      "sample. Two of the most common such situations are in ﬁnite population\n",
      "sampling (see Section 5.5.2) and a process with a stopping rule that depends\n",
      "on the realizations of the random variable, such as sampling from an urn until\n",
      "a ball of a certain color is drawn or sampling from a binary (Bernoulli) process\n",
      "until a speciﬁed number of 1s have occurred (see Example 3.12).\n",
      "Despite the more general meaning of random sample, we often call a simple\n",
      "random sample just a “random sample”.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "25\n",
      "In statistical applications we often form functions of a random sample,\n",
      "and use known or assumed distributions of those functions to make inferences\n",
      "abput the parent distribution. Two of the most common functions of a random\n",
      "sample are the sample mean and the sample variance. Given a random sample\n",
      "X1, . . ., Xn, the sample mean is deﬁned as\n",
      "X =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "Xi/n,\n",
      "(1.32)\n",
      "and if n ≥2, the sample variance is deﬁned as\n",
      "S2 =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(Xi −X)2/(n −1).\n",
      "(1.33)\n",
      "(Notice that the divisor in the sample variance is n −1.)\n",
      "If the parent distribution is N(µ, σ2), the sample mean and sample variance\n",
      "have simple and useful distributions (see page 187).\n",
      "For functions of a random sample such as the sample mean and variance,\n",
      "we often include the sample size in the notation for the function, as Xn or\n",
      "S2\n",
      "n, and we may be interested in the properties of these functions as n gets\n",
      "larger (see Example 1.23).\n",
      "The Empirical Distribution Function\n",
      "Given a random sample X1, . . ., Xn, we can form a conditional discrete dis-\n",
      "tribution, dominated by a counting measure on {1, . . ., n}, with CDF\n",
      "Fn(x) = #{Xi | Xi ≤x}/n,\n",
      "for i = 1, . . ., n.\n",
      "(1.34)\n",
      "The simple CDF Fn is called the empirical cumulative distribution function\n",
      "(ECDF) for the sample. It has at most n + 1 distinct values.\n",
      "The ECDF has wide-ranging applications in statistics.\n",
      "1.1.3 Deﬁnitions and Properties of Expected Values\n",
      "First we deﬁne the expected value of a random variable.\n",
      "Deﬁnition 1.20 (expected value of a random variable)\n",
      "Given a probability space (Ω, F, P ) and a d-variate random variable X deﬁned\n",
      "on F, we deﬁne the expected value of X with respect to P , which we denote\n",
      "by E(X) or for clarity by EP (X), as\n",
      "E(X) =\n",
      "Z\n",
      "Ω\n",
      "X dP,\n",
      "(1.35)\n",
      "if this integral exists.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "26\n",
      "1 Probability Theory\n",
      "For the random variable X, E(X), if it exists, is called the ﬁrst moment of X.\n",
      "Although by properties following immediately from the deﬁnition, Pr(−∞<\n",
      "X < ∞) = 1, it could happen that\n",
      "R\n",
      "IRd X dP = ∞. In that case we say the ex-\n",
      "pected value, or ﬁrst moment, is inﬁnite. It could also happen that\n",
      "R\n",
      "IRd X dP\n",
      "does not exist (see Deﬁnition 0.1.41 on page 728), and in that case we say the\n",
      "expected value does not exist. Recall, however, what we mean by the expres-\n",
      "sion “integrable function”. This carries over to a random variable; we say the\n",
      "random variable X is integrable (or L1 integrable) iﬀ−∞<\n",
      "R\n",
      "IRd X dP < ∞.\n",
      "Example 1.7 existence and ﬁniteness of expectations\n",
      "Let the random variable X have the PDF\n",
      "fX(x) =\n",
      "1\n",
      "(1 + x)2 IIR+(x).\n",
      "(1.36)\n",
      "We have\n",
      "E(X) =\n",
      "Z ∞\n",
      "0\n",
      "x\n",
      "(1 + x)2 dx\n",
      "=\n",
      "\u0012\n",
      "log(1 + x) +\n",
      "1\n",
      "1 + x\n",
      "\u0013\f\f\f\f\n",
      "∞\n",
      "0\n",
      "= ∞.\n",
      "That is, E(X) is inﬁnite. (Although E(X) /∈IR, and some people would say\n",
      "that it does not exist in this case, we consider E(X) “to exist”, just as we speak\n",
      "of the existence of inﬁnite integrals (page 727). Just as we identify conditions\n",
      "in which integrals do not exist because of indeterminancy (page 728), however,\n",
      "we likewise will identify situations in which the expectations do not exist.)\n",
      "Let the random variable Y have the PDF\n",
      "fY (y) =\n",
      "1\n",
      "π(1 + y2).\n",
      "(1.37)\n",
      "We have\n",
      "E(Y ) =\n",
      "Z ∞\n",
      "−∞\n",
      "x\n",
      "π(1 + x2)dx\n",
      "=\n",
      "1\n",
      "2π log(1 + x2)\n",
      "\f\f\f\f\n",
      "∞\n",
      "−∞\n",
      ".\n",
      "That is, E(Y ) is not inﬁnite; it does not exist.\n",
      "The random variable X has the same distribution as the ratio of two\n",
      "standard exponential random variables, and the random variable Y has the\n",
      "same distribution as the ratio of two standard normal random variables, called\n",
      "a Cauchy distribution. (It is an exercise to show these facts.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "27\n",
      "It is clear that E is a linear operator; that is, for random variables X and\n",
      "Y deﬁned on the same probability space, and constant a,\n",
      "E(aX + Y ) = aE(X) + E(Y ),\n",
      "(1.38)\n",
      "if E(X) and E(Y ) are ﬁnite.\n",
      "Look carefully at the integral (1.35). It is the integral of a function, X,\n",
      "over Ωwith respect to a measure, P , over the σ-ﬁeld that together with Ω\n",
      "forms the measurable space. To emphasize the meaning more precisely, we\n",
      "could write the integral in the deﬁnition as\n",
      "E(X) =\n",
      "Z\n",
      "Ω\n",
      "X(ω) dP (ω).\n",
      "The integral (1.35) is over an abstract domain Ω. We can also write the\n",
      "expectation over the real range of the random variable and an equivalent\n",
      "measure on that range. If the CDF of the random variable is F , we have, in\n",
      "the abbreviated form of the ﬁrst expression given in the deﬁnition,\n",
      "E(X) =\n",
      "Z\n",
      "IRd x dF,\n",
      "(1.39)\n",
      "or in the more precise form,\n",
      "E(X) =\n",
      "Z\n",
      "IRd x dF (x).\n",
      "If the PDF exists and is f, we also have\n",
      "E(X) =\n",
      "Z\n",
      "IRd xf(x) dx.\n",
      "An important and useful fact about expected values is given in the next\n",
      "theorem.\n",
      "Theorem 1.13\n",
      "Let X be a random variable in IRd such that E(∥X∥2) < ∞. Then\n",
      "E(X) = arg min\n",
      "a∈IRd E(∥X −a∥2).\n",
      "(1.40)\n",
      "Proof. Exercise. Also, see equation (0.0.101).\n",
      "In statistical applications this result states that E(X) is the best prediction\n",
      "of X given a quadratic loss function in the absence of additional information\n",
      "about X.\n",
      "We deﬁne the expected value of a Borel function of a random variable in\n",
      "the same way as above for a random variable.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "28\n",
      "1 Probability Theory\n",
      "Deﬁnition 1.21 (expected value of a Borel function)\n",
      "If g is a Borel function of the random variable X with CDF F , then the\n",
      "expected value of g(X) is deﬁned as\n",
      "E(g(X)) =\n",
      "Z\n",
      "IRd g(x) dF (x).\n",
      "(1.41)\n",
      "Theorem 1.14\n",
      "If X and Y are random variables deﬁned over the same probability space and\n",
      "g is a Borel function of the random variable X, then\n",
      "X ≥Y a.s.\n",
      "⇒\n",
      "E(X) ≥E(Y ),\n",
      "(1.42)\n",
      "and\n",
      "g(X) ≥0 a.s.\n",
      "⇒\n",
      "E(g(X)) ≥0.\n",
      "(1.43)\n",
      "Proof. Each property is an immediate consequence of the deﬁnition.\n",
      "Expected Values of Probability Density Functions\n",
      "Expected values of PDFs or of functionals of PDFs are useful in applications\n",
      "of probability theory. The expectation of the negative of the log of a PDF,\n",
      "−log(f(X)), is the entropy (Deﬁnition 1.26 on page 41), which is related to\n",
      "the concept of “information”. We have also mentioned the expectation of the\n",
      "square of ∂log(f(X; θ))/∂θ, which appears in the Cram´er-Rao lower bound.\n",
      "The expectation of ∂log(f(X; θ))/∂θ is zero (see page 230).\n",
      "There are several interesting expectations of the PDFs of two diﬀerent\n",
      "distributions, some of which we will mention on page 36.\n",
      "Expected Values and Quantile Functions of Univariate Random\n",
      "Variables\n",
      "As we mentioned earlier, the quantile function has many applications that\n",
      "parallel those of the CDF.\n",
      "If X is a univariate random variable with CDF F , then the expected value\n",
      "of X is\n",
      "E(X) =\n",
      "Z 1\n",
      "0\n",
      "F −1(x)dx.\n",
      "(1.44)\n",
      "See Exercise 1.25.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "29\n",
      "Expected Value and Probability\n",
      "There are many interesting relationships between expected values and proba-\n",
      "bilities, such as the following.\n",
      "Theorem 1.15\n",
      "If X is a random variable such that X > 0 a.s., then\n",
      "E(X) =\n",
      "Z ∞\n",
      "0\n",
      "Pr(X > t)dt.\n",
      "(1.45)\n",
      "Proof. This is a simple application of Fubini’s theorem, using the CDF F of\n",
      "X:\n",
      "E(X) =\n",
      "Z ∞\n",
      "0\n",
      "x dF (x)\n",
      "=\n",
      "Z ∞\n",
      "0\n",
      "Z\n",
      "]0,x[\n",
      "dt dF (x)\n",
      "=\n",
      "Z ∞\n",
      "0\n",
      "Z\n",
      "]t,∞[\n",
      "dF (x)dt\n",
      "=\n",
      "Z ∞\n",
      "0\n",
      "(1 −F (t))dt\n",
      "=\n",
      "Z ∞\n",
      "0\n",
      "Pr(X > t)dt\n",
      "Theorem 1.15 leads in general to the following useful property for any\n",
      "given random variable X for which E(X) exists:\n",
      "E(X) =\n",
      "Z ∞\n",
      "0\n",
      "(1 −F (t))dt −\n",
      "Z 0\n",
      "−∞\n",
      "F (t)dt.\n",
      "(1.46)\n",
      "It is an exercise to write a proof of this statement.\n",
      "Another useful fact in applications involving the Bernoulli distribution\n",
      "with parameter π is the relationship\n",
      "E(X) = Pr(X = 1) = π.\n",
      "Expected Value of the Indicator Function\n",
      "We deﬁne the indicator function, IA(x), as\n",
      "IA(x) =\n",
      "\u001a 1 if x ∈A\n",
      "0 otherwise.\n",
      "(1.47)\n",
      "(This is also sometimes called the “characteristic function”, but we use that\n",
      "term to refer to something else.) If X is an integrable random variable over\n",
      "A, then IA(X) is an integrable random variable, and\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "30\n",
      "1 Probability Theory\n",
      "Pr(A) = E(IA(X)).\n",
      "(1.48)\n",
      "It is an exercise to write a proof of this statement. When it is clear from the\n",
      "context, we may omit the X, and merely write E(IA).\n",
      "Expected Value over a Measurable Set\n",
      "The expected value of an integrable random variable over a measurable set\n",
      "A ⊆IRd is\n",
      "E(XIA(X)) =\n",
      "Z\n",
      "A\n",
      "X dP.\n",
      "(1.49)\n",
      "It is an exercise to write a proof of this statement. We often denote this as\n",
      "E(XIA).\n",
      "Expected Value of General Measurable Functions\n",
      "A real-valued measurable function g of a random variable X is itself a random\n",
      "variable, possibly with a diﬀerent probability measure. Its expected value is\n",
      "deﬁned in exactly the same way as above. If the probability triple associated\n",
      "with the random variable X is (Ω, F, P ) and Y = g(X), we could identify a\n",
      "probability triple associated with Y . Being measurable, the relevant measur-\n",
      "able space of g(X) is (Ω, F), but the probability measure is not necessarily\n",
      "P . If we denote the probability triple associated with the random variable Y\n",
      "is (Ω, F, Q), we may distinguish the deﬁning integrals with respect to dP and\n",
      "dQ by EP and EQ.\n",
      "We can also write the expected value of Y in terms of the CDF of the origi-\n",
      "nal random variable. The expected value of a real-valued measurable function\n",
      "g of a random variable X with CDF F is E(g(X)) = R g(x)dF (x).\n",
      "Moments of Scalar Random Variables\n",
      "The higher-order moments are the expected values of positive integral powers\n",
      "of the random variable. If X is a scalar-valued random variable, the rth raw\n",
      "moment of X, if it exists, is E(Xr). We often denote the rth raw moment as\n",
      "µ′\n",
      "r. There is no requirement, except notational convenience, to require that r\n",
      "be an integer, and we will often allow it non-integral values, although “ﬁrst\n",
      "moment”, “second moment”, and so on refer to the integral values.\n",
      "For r ≥2, central moments or moments about E(X) are often more useful.\n",
      "The rth central moment of the univariate random variable X, denoted as µr,\n",
      "is E((X −E(X))r):\n",
      "µr =\n",
      "Z\n",
      "(x −µ)rdF (x).\n",
      "(1.50)\n",
      "if it exists. We also take µ1 to be µ, which is also µ′\n",
      "1. For a discrete distribution,\n",
      "this expression can be interpreted as a sum of the values at the mass points\n",
      "times their associated probabilities.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "31\n",
      "Notice that the moment may or may not exist, and if it exists, it may or\n",
      "may not be ﬁnite. (As usual, whenever I speak of some property of a moment,\n",
      "such as that it is ﬁnite, I am assuming that the moment exists, even though\n",
      "I may not make a statement to that eﬀect.)\n",
      "The ﬁrst two central moments are usually the most important; µ1 is called\n",
      "the mean and µ2 is called the variance. The variance of X is denoted by\n",
      "V(·). Because (X −E(X))2 ≥0 a.s., we see that the variance is nonnegative.\n",
      "Further, unless X\n",
      "a.s.\n",
      "= E(X), the variance is positive.\n",
      "The square root of the variance is called the standard deviation.\n",
      "E(|X|) is called the ﬁrst absolute moment of X; and generally, E(|X|r) is\n",
      "called the rth absolute moment.\n",
      "Theorem 1.16\n",
      "If the rth absolute moment of a scalar random variable is ﬁnite, then the\n",
      "absolute moments of order 1, 2, . . ., r −1 are ﬁnite.\n",
      "Proof. For s ≤r, |x|s ≤1 + |x|r.\n",
      "Example 1.8 moments of random variables in the t family\n",
      "Consider the t family of distributions (page 841) with PDF\n",
      "f(y) = Γ((ν + 1)/2)\n",
      "Γ(ν/2)√νπ (1 + y2/ν)−(ν+1)/2,\n",
      "in which the parameter ν is called the degrees of freedom. By direct integra-\n",
      "tion, it is easy to see that the rth absolute moment moment exists iﬀr ≤ν −1.\n",
      "We deﬁne the rth standardized moment as\n",
      "ηr = µr/µr/2\n",
      "2\n",
      ".\n",
      "(1.51)\n",
      "The ﬁrst raw moment or the mean, is an indicator of the general “location”\n",
      "of the distribution. The second central moment or the variance, denoted as\n",
      "µ2 or σ2 is a measure of the “spread” of the distribution. The nonnegative\n",
      "square root, σ, is sometimes called the “scale” of the distribution. The third\n",
      "standardized moment, η3, is an indicator of whether the distribution is skewed;\n",
      "it is called the skewness coeﬃcient. If η3 ̸= 0, the distribution is asymmetric,\n",
      "but η3 = 0 does not mean that the distribution is symmetric.\n",
      "The fourth standardized moment, η4 is called the kurtosis coeﬃcient. It\n",
      "is an indicator of how “peaked” the distribution is, or how “heavy” the tails\n",
      "of the distribution are. (Actually, exactly what this standardized moment\n",
      "measures cannot be described simply. Because, for the random variable X, we\n",
      "have\n",
      "η4 = V\n",
      "\u0012(X −µ)2\n",
      "σ2\n",
      "\u0013\n",
      "+ 1,\n",
      "it can be seen that the minimum value for η4 is attained for a discrete dis-\n",
      "tribution with mass points −σ and σ. We might therefore interpret η4 as a\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "32\n",
      "1 Probability Theory\n",
      "measure of variation about the two points −σ and σ. This, of course, leads\n",
      "to the two characteristics mentioned above: peakedness and heaviness in the\n",
      "tails.)\n",
      "The sequence of (raw) moments is very useful in characterizing a distri-\n",
      "bution of a scalar random variable, but often the central moments are to be\n",
      "preferred because the second and higher central moments are invariant to\n",
      "change in the ﬁrst moment (the “location”).\n",
      "Uniqueness of Moments of Scalar Random Variables\n",
      "An interesting question is whether the full set of moments fully determines a\n",
      "distribution of a scalar random variable.\n",
      "It seems reasonable to guess that this would not be the case if not all\n",
      "moments of all orders exist; and, indeed, it is a simple matter to construct\n",
      "two diﬀerent distributions whose moments beyond the kth are inﬁnite but\n",
      "whose ﬁrst k moments are the same. The question of interest, therefore, is\n",
      "whether the full set of moments fully determine a distribution, given that\n",
      "moments of all orders are ﬁnite. In general, they do not.\n",
      "Example 1.9 diﬀerent distributions with equal ﬁnite moments of all\n",
      "orders\n",
      "Consider the complex integral related to the gamma function,\n",
      "Z ∞\n",
      "0\n",
      "tα−1e−t/βdt = βαΓ(α),\n",
      "where α > 0 and β = 1/(γ + iξ) with γ > 0. Now, make a change of variable,\n",
      "xρ = t for 0 < ρ < 1/2; and for a nonnegative integer k, choose α = (k +1)/ρ,\n",
      "and ξ/γ = tan(ρπ). Noting that (1 + i tan(ρπ))(k+1)/ρ is real, we have that\n",
      "the imaginary part of the integral after substitution is 0:\n",
      "Z ∞\n",
      "0\n",
      "xke−γxρ sin(ξxρ)dx = 0.\n",
      "(1.52)\n",
      "Hence, for all |α| ≤1, the distributions over ¯IR+ with PDF\n",
      "p(x) = ce−γxρ(1 + α sin(ξxρ))I¯IR+(x),\n",
      "(1.53)\n",
      "where γ > 0 and 0 < ρ < 1/2 have the same moments of all orders k =\n",
      "0, 1, 2, . . ..\n",
      "We could in a similar way develop a family of distributions over IR that\n",
      "have the same moments. (In that case, the ρ is required to be in the range\n",
      "] −1, 1[.) The essential characteristic of these examples is that there exists x0,\n",
      "such that for γ > 0,\n",
      "p(x) > e−γ|x|ρ\n",
      "for x > x0,\n",
      "(1.54)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "33\n",
      "because we can add a function all of whose moments are 0. (Such a function\n",
      "would necessarily take on negative values, but not suﬃciently small to make\n",
      "p(x) plus that function be negative.)\n",
      "A distribution all of whose moments are the same as some other distribu-\n",
      "tion is called a moment-indeterminant distribution. The distributions within a\n",
      "family of distributions all of which have the same moments are called moment-\n",
      "equivalent distributions. In Exercise 1.28, you are asked to show that there\n",
      "are other distributions moment-equivalent to the lognormal family of distri-\n",
      "butions.\n",
      "So can we identify conditions that are suﬃcient for the moments to char-\n",
      "acterize a probability distribution? The answer is yes; in fact, there are several\n",
      "criteria that ensure that the moments uniquely determine a distribution. For\n",
      "scalar random variables, one criterion is given in the following theorem.\n",
      "Theorem 1.17\n",
      "Let ν0, ν1, . . . ∈IR be the moments of some probability distribution. (The mo-\n",
      "ments can be about any origin.) The probability distribution is uniquely deter-\n",
      "mined by those moments if\n",
      "∞\n",
      "X\n",
      "j=0\n",
      "νjtj\n",
      "j!\n",
      "(1.55)\n",
      "converges for some real nonzero t.\n",
      "A simple proof of this theorem is based on the uniqueness of the character-\n",
      "istic function, so we defer its proof to page 49, after we have discussed the\n",
      "characteristic function.\n",
      "Corollary 1.17.1 The moments of a probability distribution with ﬁnite sup-\n",
      "port uniquely determine the distribution.\n",
      "The following theorem, which we state without proof, tells us that the\n",
      "moments determine the distribution if the central moments are close enough\n",
      "to zero.\n",
      "Theorem 1.18\n",
      "Let µ0, µ1, . . . be the central moments of some probability distribution. If the\n",
      "support of the probability distribution is IR, it is uniquely determined by those\n",
      "moments if\n",
      "∞\n",
      "X\n",
      "j=0\n",
      "1\n",
      "µ1/2j\n",
      "2j\n",
      "(1.56)\n",
      "diverges.\n",
      "If the support of the probability distribution is ¯IR+, it is uniquely deter-\n",
      "mined by those moments if\n",
      "∞\n",
      "X\n",
      "j=0\n",
      "1\n",
      "µ1/2j\n",
      "j\n",
      "(1.57)\n",
      "diverges.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "34\n",
      "1 Probability Theory\n",
      "Corollary 1.18.1\n",
      "If a probability distribution has a PDF p(x) with support IR, then the moments\n",
      "uniquely determine p(x) if\n",
      "p(x) < M|x|λ−1e−γ|x|ρ\n",
      "for x > x0,\n",
      "(1.58)\n",
      "where M, λ, γ > 0 and ρ ≥1.\n",
      "If a probability distribution has a PDF p(x) with support ¯IR+, then the\n",
      "moments uniquely determine p(x) if\n",
      "p(x) < Mxλ−1e−γxρ\n",
      "for x > 0,\n",
      "(1.59)\n",
      "where M, λ, γ > 0 and ρ ≥1/2.\n",
      "The conditions in Theorem 1.18 are called the Carleman criteria (after Torsten\n",
      "Carleman). Compare the conditions in the corollary with inequality (1.54).\n",
      "Within a speciﬁc family of distributions, the full set of moments can usu-\n",
      "ally be expected to identify the distribution. For narrowly-deﬁned families of\n",
      "distributions, such as the normal or gamma families, often only one or two\n",
      "moments completely identify the family.\n",
      "Cumulants of Scalar Random Variables\n",
      "Another useful sequence of constants for describing the distribution of a scalar\n",
      "random variables are called cumulants, if they exist. Cumulants, except for\n",
      "the ﬁrst, are also invariant to change in the ﬁrst moment. The cumulants and\n",
      "the moments are closely related, and cumulants can be deﬁned in terms of\n",
      "raw moments, if they exist. For the ﬁrst few cumulants, κ1, κ2, . . ., and raw\n",
      "moments, µ′\n",
      "1, µ′\n",
      "2, . . ., of a scalar random variable, for example,\n",
      "µ′\n",
      "1 = κ1\n",
      "µ′\n",
      "2 = κ2 + κ2\n",
      "1\n",
      "µ′\n",
      "3 = κ3 + 3κ2κ1 + κ3\n",
      "1.\n",
      "(1.60)\n",
      "The expressions for the cumulants are a little more complicated, but can be\n",
      "obtained easily from the triangular system (1.60).\n",
      "Factorial Moments of Discrete Scalar Random Variables\n",
      "A discrete distribution with support x1, x2, . . . ∈IR is equivalent to a discrete\n",
      "distribution with support 0, 1, . . ., and for such a distribution, another kind\n",
      "of moment is sometimes useful. It is the factorial moment, related to the rth\n",
      "factorial of the real number y:\n",
      "y[r] = y(y −1) · · ·(y −(r −1)).\n",
      "(1.61)\n",
      "(We see that y[y] = y!. It is, of course, not necessary that y be an integer, but\n",
      "factorials are generally more useful in the context of nonnegative integers.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "35\n",
      "The rth factorial moment of the random variable X above is\n",
      "µ′\n",
      "[r] =\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "x[r]\n",
      "i pi.\n",
      "(1.62)\n",
      "We see that µ′\n",
      "[1] = µ′\n",
      "1 = µ1.\n",
      "The rth central factorial moment, denoted µ[r] is the rth factorial moment\n",
      "about µ.\n",
      "1.1.4 Relations among Random Variables\n",
      "In many applications there are two or more random variables of interest. For\n",
      "a given probability space (Ω, F, P ), there may be a collection of random vari-\n",
      "ables, W. If the random variables have some common properties, for example,\n",
      "if either all are discrete or all are continuous and if all have the same structure,\n",
      "we may identify the collection as a “space”.\n",
      "If the random variables have some relationship to each other, that is, if\n",
      "they are not independent, we seek useful measures of their dependence. The\n",
      "appropriate measure depends on the nature of their dependence. If they are\n",
      "quadratically related, a measure that is appropriate for linear relationships\n",
      "may be inappropriate, and vice versa.\n",
      "We will consider two ways of studying the relationships among random\n",
      "variables. The ﬁrst is based on second-degree moments, called covariances,\n",
      "between pairs of variables, and the other is based on functions that relate the\n",
      "CDF of one variable or one set of variables to the CDFs of other variables.\n",
      "These functions, called copulas, can involve more than single pairs of variables.\n",
      "Random Variable Spaces\n",
      "As with any function space, W may have interesting and useful properties.\n",
      "For example, W may be a linear space; that is, for X, Y ∈W and a ∈IR,\n",
      "aX + Y ∈W.\n",
      "The concept of Lp random variable spaces follows immediately from the\n",
      "general property of function spaces, discussed on page 741. for random vari-\n",
      "ables in an Lp random variable space, the pth absolute is ﬁnite.\n",
      "The closure of random variable spaces is often of interest. We deﬁne various\n",
      "forms of closure depending on types of convergence of a sequence Xn in the\n",
      "space. For example, given any sequence Xn ∈W if Xn\n",
      "Lr\n",
      "→X implies X ∈W\n",
      "then W is closed for the rth moment.\n",
      "Expectations\n",
      "We may take expectations of functions of random variables in terms of their\n",
      "joint distribution or in terms of their marginal distributions. To indicate the\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "36\n",
      "1 Probability Theory\n",
      "distribution used in an expectation, we may use notation for the expectation\n",
      "operator similar to that we use on the individual distribution, as described\n",
      "on page 23. Given the random variables X1 and X2, we use the notation EX1\n",
      "to indicate an expectation taken with respect to the marginal distribution of\n",
      "X1.\n",
      "We often denote the expectation taken with respect to the joint distribu-\n",
      "tion as simply E, but for emphasis, we may use the notation EX1,X2.\n",
      "We also use notation of the form EP , where P denotes the relevant proba-\n",
      "bility distribution of whatever form, or Eθ in a parametric family of probability\n",
      "distributions.\n",
      "Expectations of PDFs and of Likelihoods\n",
      "If the marginal PDFs of the random variables X1 and X2 are fX1 and fX2,\n",
      "we have the equalities\n",
      "EX1\n",
      "\u0012fX2(X1)\n",
      "fX1(X1)\n",
      "\u0013\n",
      "= EX2\n",
      "\u0012fX1(X2)\n",
      "fX2(X2)\n",
      "\u0013\n",
      "= 1.\n",
      "(1.63)\n",
      "On the other hand,\n",
      "EX1(−log(fX1(X1))) ≤EX1(−log(fX2(X1))),\n",
      "(1.64)\n",
      "with equality only if fX1(x) = fX2(x) a.e. (see page 41).\n",
      "When the distributions are in the same parametric family, we may write\n",
      "fθ with diﬀerent values of θ instead of fX1 and fX2. In that case, it is more\n",
      "natural to think of the functions as likelihoods since the parameter is the\n",
      "variable. From equation (1.63), for example, we have for the likelihood ratio,\n",
      "Eθ1\n",
      "\u0012L(θ2; X)\n",
      "L(θ1; X)\n",
      "\u0013\n",
      "= 1.\n",
      "(1.65)\n",
      "Covariance and Correlation\n",
      "Expectations are also used to deﬁne relationships among random variables.\n",
      "We will ﬁrst consider expectations of scalar random variables, and then discuss\n",
      "expectations of vector and matrix random variables.\n",
      "For two scalar random variables, X and Y , useful measures of a linear\n",
      "relationship between them are the covariance and correlation. The covariance\n",
      "of X and X, if it exists, is denoted by Cov(X, Y ), and is deﬁned as\n",
      "Cov(X, Y ) = E ((X −E(X))(Y −E(Y )))\n",
      "(1.66)\n",
      "From the Cauchy-Schwarz inequality (B.21) (see page 853), we see that\n",
      "(Cov(X, Y ))2 ≤V(X)V(Y ).\n",
      "(1.67)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "37\n",
      "The correlation of X and Y , written Cor(X, Y ), is deﬁned as\n",
      "Cor(X, Y ) = Cov(X, Y )\n",
      ".p\n",
      "V(X)V(Y ) .\n",
      "(1.68)\n",
      "The correlation is also called the correlation coeﬃcient and is often written\n",
      "as ρX,Y .\n",
      "From inequality (1.67), we see that the correlation coeﬃcient is in [−1, 1].\n",
      "If X and Y are independent, then Cov(X, Y ) = Cor(X, Y ) = 0 (exercise).\n",
      "Structure of Random Variables\n",
      "Random variables may consist of individual IR elements arrayed in some struc-\n",
      "ture, such as a vector so that the random variable itself is in IRd or as a matrix\n",
      "so that the random variable is in IRd×m. Many of the properties of random\n",
      "variables are essentially the same whatever their structure, except of course\n",
      "those properties may have structures dependent on that of the random vari-\n",
      "able.\n",
      "Multiplication is an operation that depends very strongly on the structure\n",
      "of the operand. If x is a scalar, x2 is a scalar. If x is a is vector, however, there\n",
      "are various operations that could be interpreted as extensions of a squaring\n",
      "operation. First, of course, is elementwise squaring. In this interpretation x2\n",
      "has the same structure as x. Salient relationships among the individual ele-\n",
      "ments of x may be lost by this operation, however. Other interpretations are\n",
      "xTx, which preserves none of the structure of x, and xxT, which is in IRd×d.\n",
      "The point of this is that what can reasonably be done in the analysis of ran-\n",
      "dom variables depends on the structure of the random variables, and such\n",
      "relatively simple concepts as moments require some careful consideration. In\n",
      "many cases, a third-order or higher-order moment is not useful because of its\n",
      "complexity.\n",
      "Structural Moments\n",
      "For random variables that have a structure such as a vector or matrix, the\n",
      "elementwise moments are the same as those for a scalar-valued random vari-\n",
      "able as described above, and hence, the ﬁrst moment, the mean, has the same\n",
      "structure as the random variable itself.\n",
      "Higher order moments of vectors and matrices present some problems be-\n",
      "cause the number of individual scalar moments is greater than the number of\n",
      "elements in the random object itself. For multivariate distributions, the higher-\n",
      "order marginal moments are generally more useful than the higher-order joint\n",
      "moments. We deﬁne the second-order moments (variances and covariances)\n",
      "for random vectors and for random matrices below.\n",
      "Deﬁnition 1.22 (variance-covariance of a random vector)\n",
      "The variance-covariance of a vector-valued random variable X is the expec-\n",
      "tation of the outer product,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "38\n",
      "1 Probability Theory\n",
      "V(X) = E \u0000(X −E(X))(X −E(X))T\u0001 ,\n",
      "(1.69)\n",
      "if it exists.\n",
      "For a constant vector, the rank of an outer product is no greater than 1,\n",
      "but unless X a.s.\n",
      "= E(X), V(X) is nonnegative deﬁnite. We see this by forming\n",
      "the scalar random variable Y = cTX for any c ̸= 0, and writing\n",
      "0 ≤V(Y )\n",
      "= E((cTX −cTE(X))2)\n",
      "= E((cT(X −E(X))(X −E(X))c)\n",
      "= cTV(X)c.\n",
      "(If X a.s.\n",
      "= E(X), then V(X) = 0, and while it is true that cT0c = 0 ≥0, we do\n",
      "not say that the 0 matrix is nonnegative deﬁnite. Recall further that whenever\n",
      "I write a term such as V(X), I am implicitly assuming its existence.)\n",
      "Furthermore, if it is not the case that X\n",
      "a.s.\n",
      "= E(X), unless some element\n",
      "Xi of a vector X is such that\n",
      "Xi\n",
      "a.s.\n",
      "=\n",
      "X\n",
      "j̸=i\n",
      "(aj + bjXj),\n",
      "then V(X) is positive deﬁnite a.s. To show this, we show that V(X) is full\n",
      "rank a.s. (exercise).\n",
      "The elements of V(X) are the bivariate moments of the respective elements\n",
      "of X; the (i, j) element of V(X) is the covariance of Xi and Xj, Cov(Xi, Xj).\n",
      "If V(X) is nonsingular, then the correlation matrix of X, written Cor(X)\n",
      "is\n",
      "Cor(X) =\n",
      "\u0000E(X −E(X))T\u0001\n",
      "(V(X))−1 E(X −E(X)).\n",
      "(1.70)\n",
      "The (i, j) element of Cor(X) is the correlation of Xi and Xj, and so the\n",
      "diagonal elements are all 1.\n",
      "Deﬁnition 1.23 (variance-covariance of a random matrix)\n",
      "The variance-covariance of a matrix random variable X is deﬁned as the\n",
      "variance-covariance of vec(X):\n",
      "V(X) = V(vec(X)) = E\n",
      "\u0000vec(X −E(X))(vec(X −E(X)))T\u0001\n",
      ",\n",
      "(1.71)\n",
      "if it exists.\n",
      "Linearity of Moments\n",
      "The linearity property of the expectation operator yields some simple linearity\n",
      "properties for moments of ﬁrst or second degree of random variables over the\n",
      "same probability space.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "39\n",
      "For random variables X, Y , and Z with ﬁnite variances and constants a,\n",
      "b, and c, we have\n",
      "V(aX + Y + c) = a2V(X) + V(Y ) + 2aCov(X, Y );\n",
      "(1.72)\n",
      "that is, V(·) is not a linear operator (but it simpliﬁes nicely), and\n",
      "Cov(aX +bY +c, X +Z) = aV(X)+aCov(X, Z)+bCov(X, Y )+bCov(Y, Z);\n",
      "(1.73)\n",
      "that is, Cov(·, ·) is a bilinear operator. Proofs of these two facts are left as\n",
      "exercises.\n",
      "Copulas\n",
      "A copula is a function that relates a multivariate CDF to lower dimensional\n",
      "marginal CDFs. The basic ideas of copulas can all be explored in the context\n",
      "of a bivariate distribution and the two associated univariate distributions, and\n",
      "the ideas extend in a natural way to higher dimensions.\n",
      "Deﬁnition 1.24 (two-dimensional copula)\n",
      "A two-dimensional copula is a function C that maps [0, 1]2 onto [0, 1] with the\n",
      "following properties:\n",
      "1. for every u ∈[0, 1],\n",
      "C(0, u) = C(u, 0) = 0,\n",
      "(1.74)\n",
      "and\n",
      "C(1, u) = C(u, 1) = u,\n",
      "(1.75)\n",
      "2. for every (u1, u2), (v1, v2) ∈[0, 1]2 with u1 ≤v1 and u2 ≤v2,\n",
      "C(u1, u2) −C(u1, v2) −C(v1, u2) + C(v1, v2) ≥0.\n",
      "(1.76)\n",
      "A two-dimensional copula is also called a 2-copula.\n",
      "The arguments to a copula C are often taken to be CDFs, which of course\n",
      "take values in [0, 1].\n",
      "The usefulness of copulas derive from Sklar’s theorem, which we state\n",
      "without proof.\n",
      "Theorem 1.19 (Sklar’s theorem)\n",
      "Let PXY be a bivariate CDF with marginal CDFs PX and PY . Then there\n",
      "exists a copula C such that for every x, y ∈IR,\n",
      "PXY (x, y) = C(PX(x), PY (y)).\n",
      "(1.77)\n",
      "If PX and PY are continuous everywhere, then C is unique; otherwise C is\n",
      "unique over the support of the distributions deﬁned by PX and PY .\n",
      "Conversely, if C is a copula and PX and PY are CDFs, then the function\n",
      "PXY (x, y) deﬁned by equation (1.77) is a CDF with marginal CDFs PX(x)\n",
      "and PY (y).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "40\n",
      "1 Probability Theory\n",
      "Thus, a copula is a joint CDF of random variables with U(0, 1) marginals.\n",
      "The proof of the ﬁrst part of the theorem is given in Nelsen (2006), among\n",
      "other places. The proof of the converse portion is straightforward and is left\n",
      "as an exercise.\n",
      "For many bivariate distributions the copula is the most useful way to\n",
      "relate the joint distribution to the marginals, because it provides a separate\n",
      "description of the individual distributions and their association with each\n",
      "other.\n",
      "One of the most important uses of copulas is to combine two marginal\n",
      "distributions to form a joint distribution with known bivariate characteristics.\n",
      "Certain standard copulas are useful in speciﬁc applications. The copula\n",
      "that corresponds to a bivariate normal distribution with correlation coeﬃcient\n",
      "ρ is\n",
      "CNρ(u, v) =\n",
      "Z Φ−1(u)\n",
      "−∞\n",
      "Z Φ−1(v)\n",
      "−∞\n",
      "φρ(t1, t2) dt2dt1,\n",
      "(1.78)\n",
      "where Φ(·) is the standard (univariate) normal CDF, and φρ(·, ·) is the bi-\n",
      "variate normal PDF with means 0, variances 1, and correlation coeﬃcient ρ.\n",
      "This copula is usually called the Gaussian copula and has been widely used\n",
      "in ﬁnancial applications.\n",
      "The association determined by a copula is not the same as that determined\n",
      "by a correlation; that is, two pairs of random variables may have the same\n",
      "copula but diﬀerent correlations.\n",
      "1.1.5 Entropy\n",
      "Probability theory is developed from models that characterize uncertainty\n",
      "inherent in random events. Information theory is developed in terms of the\n",
      "information revealed by random events. The premise is that the occurrence\n",
      "of an event with low probability is more informative than the occurrence of\n",
      "an event of high probability. For a discrete random variable we can eﬀec-\n",
      "tively associate a value of the random variable with an event, and we quantify\n",
      "information in such a way that the information revealed by a particular out-\n",
      "come decreases as the probability increases. Thus, there is more information\n",
      "revealed by a rare event than by a common event.\n",
      "Deﬁnition 1.25 (self-information)\n",
      "Let X be a discrete random variable with probability mass function pX. The\n",
      "self-information of X = x is −log2(pX(x)).\n",
      "Self-information is also called Shannon information. The logarithm to the base\n",
      "2 comes from the basic representation of information in base 2, but we can\n",
      "equivalently use any base, and it is common to use the natural log in the\n",
      "deﬁnition of self-information.\n",
      "The logarithm of the PDF is an important function in studying random\n",
      "variables. It is used to deﬁne logconcave families (see page 165). The negative\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "41\n",
      "of its expected value is called “entropy”, or sometimes, “Shannon entropy” to\n",
      "distinguish it from other measures of entropy.\n",
      "Deﬁnition 1.26 (entropy)\n",
      "Let X be a random variable with PDF pX with respect to a σ-ﬁnite measure.\n",
      "The entropy of the random variable X is\n",
      "E(−log(pX(X))).\n",
      "(1.79)\n",
      "The expected value of the derivative of the logarithm of the PDF with\n",
      "respect to a parameter of a distributional family yields another deﬁnition of\n",
      "“information” (see Section 1.1.6), and it appears in an important inequality\n",
      "in statistical theory (see pages 399 and 854).\n",
      "The expected value in expression (1.79) is smallest when the it is taken\n",
      "wrt the distribution with PDF pX, as we see in the following theorem, known\n",
      "as the Gibbs lemma.\n",
      "Theorem 1.20 (Gibbs lemma)\n",
      "Let P1 and P2 be probability distributions with PDFs p1 and p2 respectively.\n",
      "Then\n",
      "EP1(−log(p1(X))) ≤EP1(−log(p2(X))),\n",
      "(1.80)\n",
      "with equality only if p1(x) = p2(x) a.e.\n",
      "Proof. I give a proof for distributions with PDFs dominated by Lebesgue\n",
      "measure, but it is clear that a similar argument would hold for other measures.\n",
      "Let p1 and p2 be PDFs dominated by Lebesgue measure. Let X be the set\n",
      "of x such that p1(x) > 0. Over X\n",
      "log\n",
      "\u0012p2(x)\n",
      "p1(x)\n",
      "\u0013\n",
      "≤p2(x)\n",
      "p1(x) −1,\n",
      "with inequality iﬀp2(x) = p1(x). So we have\n",
      "p1(x) log\n",
      "\u0012p2(x)\n",
      "p1(x)\n",
      "\u0013\n",
      "≤p2(x) −p1(x).\n",
      "Now\n",
      "EP1(−log(p1(X))) −EP1(−log(p2(X))) = −\n",
      "Z\n",
      "X\n",
      "p1(x) log(p1(x))dx\n",
      "+\n",
      "Z\n",
      "X\n",
      "p1(x) log(p2(x))dx\n",
      "≤\n",
      "Z\n",
      "X\n",
      "p2(x)dx −\n",
      "Z\n",
      "X\n",
      "p1(x)dx\n",
      "≤\n",
      "Z\n",
      "IRd p2(x)dx −\n",
      "Z\n",
      "IRd p1(x)dx\n",
      "= 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "42\n",
      "1 Probability Theory\n",
      "The deﬁnitions of information theory are generally given in the context of\n",
      "a countable sample space, and in that case, we can see that the entropy is the\n",
      "expected value of the self-information, and equation (1.79) becomes\n",
      "H(X) = −\n",
      "X\n",
      "x\n",
      "pX(x) log(pX(x)),\n",
      "(1.81)\n",
      "which is the more familiar form in information theory.\n",
      "We can likewise deﬁne the joint entropy H(X, Y ) in terms of the joint\n",
      "PDF pX,Y .\n",
      "We can see that the entropy is maximized if all outcomes are equally\n",
      "probable. In the case of a discrete random variable with two outcomes with\n",
      "probabilities π and 1−π (a Bernoulli random variable with parameter π), the\n",
      "entropy is\n",
      "−π log(π) −(1 −π) log(1 −π).\n",
      "We note that it is maximized when π = 1/2.\n",
      "1.1.6 Fisher Information\n",
      "The concept of “information” is important in applications of probability the-\n",
      "ory. We have given one formal meaning of the term in Section 1.1.5. We will\n",
      "give another formal deﬁnition in this section, and in Section 1.6.1, we will give\n",
      "an informal meaning of the term in the context of evolution of σ-Fields.\n",
      "If a random variable X has a PDF f(x; θ) wrt a σ-ﬁnite measure that is\n",
      "diﬀerentiable in θ, the rate of change of the PDF at a given x with respect to\n",
      "diﬀerent values of θ intuitively is an indication of the amount of information\n",
      "x provides. For such distributions, we deﬁne the “information” (or “Fisher\n",
      "information”) that X contains about θ as\n",
      "I(θ) = Eθ\n",
      " \u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013 \u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013T!\n",
      ".\n",
      "(1.82)\n",
      "1.1.7 Generating Functions\n",
      "There are various functionals of the PDF or CDF that are useful for de-\n",
      "termining properties of a distribution. One important type are “generating\n",
      "functions”. These are functions whose derivatives evaluated at speciﬁc points\n",
      "yield important quantities that describe the distribution, such as moments, or\n",
      "as in the case of discrete distributions, the probabilities of the points in the\n",
      "support.\n",
      "We will describe these functions in terms of d-variate random variables,\n",
      "although, as we discussed on pages 30 through 39, the higher-order moments\n",
      "of d-variate random variables have structures that depends on d.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "43\n",
      "Moment-Generating Functions\n",
      "Deﬁnition 1.27 (moment-generating function)\n",
      "For the random d-variate X, the moment-generating function (MGF) is\n",
      "ψX(t) = E\n",
      "\u0010\n",
      "etTX\u0011\n",
      ",\n",
      "t ∈Nϵ(0) ⊆IRd,\n",
      "(1.83)\n",
      "if this expectation is ﬁnite for some ϵ > 0.\n",
      "The moment-generating function is obviously nonnegative.\n",
      "Many common distributions do not have moment-generating functions.\n",
      "Two examples are the Cauchy distribution (exercise) and the lognormal dis-\n",
      "tribution (Example 1.10).\n",
      "For a scalar random variable X, the MGF yields the (raw) moments di-\n",
      "rectly (assuming existence of all quantities):\n",
      "dkψX(t)\n",
      "dtk\n",
      "\f\f\f\f\n",
      "t=0\n",
      "= E(Xk).\n",
      "(1.84)\n",
      "For vector-valued random variables, the moments become tensors, but the\n",
      "ﬁrst two moments are very simple:\n",
      "∇ψX(t)|t=0 = E(X)\n",
      "(1.85)\n",
      "and\n",
      "∇∇ψX(t)|t=0 = E(XTX).\n",
      "(1.86)\n",
      "Because of the foregoing and the fact that for ∀k ≥1, |t| > 0, ∃x0 ∋|x| ≥\n",
      "x0 ⇒etx > xk, we have the following fact.\n",
      "Theorem 1.21\n",
      "If for the scalar random variable X the MGF ψX(t) exists, then E(|X|k) < ∞\n",
      "and E(Xk) = ψ(k)\n",
      "X (0).\n",
      "The theorem also holds for vector-valued random variables, with the appro-\n",
      "priate deﬁnition of Xk.\n",
      "The converse does not hold, as the following example shows.\n",
      "Example 1.10 a distribution whose moments exist but whose moment-\n",
      "generating function does not exist\n",
      "The lognormal distribution provides some interesting examples with regard\n",
      "to moments. First, recall from page 33 that the moments do not uniquely\n",
      "determine a lognormal distribution (see also Exercise 1.28).\n",
      "The standard lognormal distribution is related to the standard normal\n",
      "distribution. If Y has a standard normal distribution, then X = eY has a\n",
      "lognormal distribution (that is, the log of a lognormal random variable has a\n",
      "normal distribution).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "44\n",
      "1 Probability Theory\n",
      "Let Y be a standard normal variable and let X = eY . We see that the\n",
      "moments of the lognormal random variable X exist for all orders k = 1, 2, . . .:\n",
      "E \u0000Xk\u0001 = E \u0000ekY \u0001\n",
      "=\n",
      "1\n",
      "√\n",
      "2π\n",
      "Z ∞\n",
      "−∞\n",
      "ekye−y2/2dy\n",
      "= ek2/2.\n",
      "However, for t > 0,\n",
      "E \u0000etX\u0001 = E\n",
      "\u0010\n",
      "eteY \u0011\n",
      "=\n",
      "1\n",
      "√\n",
      "2π\n",
      "Z ∞\n",
      "−∞\n",
      "etey−y2/2dy.\n",
      "By expanding ey in the exponent of the integrand as 1+y+y2/2+y3/3!+· · ·,\n",
      "we see that the integrand is greater than exp(t(1 + y + y3/3!)). However,\n",
      "Z ∞\n",
      "−∞\n",
      "exp(t(1 + y + y3/3!))dy = ∞;\n",
      "hence, the MGF does not exist.\n",
      "When it exists, the MGF is very similar to the characteristic function,\n",
      "which we will deﬁne and discuss in Section 1.1.8 beginning on page 45.\n",
      "Generating Functions for Discrete Distributions\n",
      "Frequency-generating functions or probability-generating functions (the terms\n",
      "are synonymous) are useful for discrete random variables.\n",
      "Deﬁnition 1.28 (probability-generating function)\n",
      "For the discrete random variable X taking values x1, x2, . . . with probabili-\n",
      "ties 0 < p1, p2, . . ., the frequency-generating function or probability-generating\n",
      "function is the polynomial\n",
      "P (t) =\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "pi+1ti.\n",
      "(1.87)\n",
      "The probability of xr is\n",
      "dr+1\n",
      "dtr+1 P (t) |t=0\n",
      "(1.88)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "45\n",
      "The probability-generating function for the binomial distribution with pa-\n",
      "rameters π and n, for example, is\n",
      "P (t) = (πt + (1 −π))n.\n",
      "For a discrete distribution, there is also a generating function for the fac-\n",
      "torial moments. We see immediately that the factorial-moment-generating\n",
      "function is the same as the probability-generating function evaluated at t+1:\n",
      "P (t + 1) =\n",
      "∞\n",
      "X\n",
      "j=0\n",
      "pj+1(t + 1)j\n",
      "=\n",
      "∞\n",
      "X\n",
      "j=0\n",
      "pj+1\n",
      "j\n",
      "X\n",
      "i=1\n",
      "\u0012j\n",
      "i\n",
      "\u0013\n",
      "ti\n",
      "=\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "ti\n",
      "i!\n",
      "∞\n",
      "X\n",
      "j=0\n",
      "(pj+1j(j −1) · · ·(j −i + 1))\n",
      "=\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "ti\n",
      "i! µ′\n",
      "[i].\n",
      "(1.89)\n",
      "1.1.8 Characteristic Functions\n",
      "One of the most useful functions determining a probability distribution is the\n",
      "characteristic function, or CF. The CF is also a generating function for the\n",
      "moments.\n",
      "Deﬁnition and Properties\n",
      "Deﬁnition 1.29 (characteristic function)\n",
      "For the random d-variate variable X, the characteristic function (CF), with\n",
      "t ∈IRd, is\n",
      "ϕX(t) = E\n",
      "\u0010\n",
      "eitTX\u0011\n",
      "(1.90)\n",
      "where i = √−1.\n",
      "The characteristic function is the Fourier transform of the density with argu-\n",
      "ment −t/(2π).\n",
      "The function ez has many useful properties. There are several relationships\n",
      "to trigonometric functions, series expansions, limits, and inequalities involving\n",
      "this function that are useful in working with characteristic functions.\n",
      "We see that the integral in equation (1.83) exists (as opposed to the in-\n",
      "tegral in equation (1.90) deﬁning the MGF) by use of Euler’s formula (equa-\n",
      "tion (0.0.67)) to observe that\n",
      "\f\f\feitTx\f\f\f =\n",
      "\f\fcos\n",
      "\u0000tTx\n",
      "\u0001\n",
      "+ i sin\n",
      "\u0000tTx\n",
      "\u0001\f\f =\n",
      "q\n",
      "cos2 (tTx) + sin2 (tTx) = 1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "46\n",
      "1 Probability Theory\n",
      "Euler’s formula also provides an alternate expression for the CF that is\n",
      "sometimes useful:\n",
      "ϕX(t) = E \u0000cos \u0000tTX\u0001\u0001 + iE \u0000sin \u0000tTX\u0001\u0001 .\n",
      "(1.91)\n",
      "Although the CF always exists, it may not have an explicit representation.\n",
      "The CF for the lognormal distribution, for example, cannot be represented\n",
      "explicitly, but can be approximated to any tolerance by a divergent series\n",
      "(exercise).\n",
      "Note that the integration in the expectation operator deﬁning the CF is\n",
      "not complex integration; we interpret it as ordinary Lebesgue integration in\n",
      "the real dummy variable in the PDF (or dF ). Hence, if the MGF is ﬁnite\n",
      "for all t such that |t| < ϵ for some ϵ > 0, then the CF can be obtained by\n",
      "replacing t in ψX(t) by it. Note also that the MGF may be deﬁned only in a\n",
      "neighborhood of 0, but the CF is deﬁned over IR.\n",
      "There are some properties of the characteristic function that are immediate\n",
      "from the deﬁnition:\n",
      "ϕX(−t) = ϕX(t)\n",
      "(1.92)\n",
      "and\n",
      "ϕX(0) = 1.\n",
      "(1.93)\n",
      "The CF is real if the distribution is symmetric about 0. We see this from\n",
      "equation (1.91).\n",
      "The CF is bounded:\n",
      "|ϕX(t)| ≤1.\n",
      "(1.94)\n",
      "We see this property by ﬁrst observing that\n",
      "\f\f\fE\n",
      "\u0010\n",
      "eitTX\u0011\f\f\f ≤E\n",
      "\u0010\f\f\feitTX\f\f\f\n",
      "\u0011\n",
      ",\n",
      "and then by using Euler’s formula on\n",
      "\f\f\feitTx\f\f\f.\n",
      "The CF and the moments of the distribution if they exist are closely re-\n",
      "lated, as we will see below. Another useful property provides a bound on the\n",
      "diﬀerence of the CF at any point and a partial series in the moments in terms\n",
      "of expected absolute moments.\n",
      "\f\f\f\f\fϕX(t) −\n",
      "n\n",
      "X\n",
      "k=0\n",
      "(it)k\n",
      "k! E(Xk)\n",
      "\f\f\f\f\f ≤E\n",
      "\u0012\n",
      "min\n",
      "\u00122|tX|n\n",
      "n!\n",
      ", |tX|n+1\n",
      "(n + 1)!\n",
      "\u0013\u0013\n",
      ",\n",
      "(1.95)\n",
      "This property follows immediately from inequality 0.0.71 on page 663.\n",
      "Another slightly less obvious fact is\n",
      "Theorem 1.22\n",
      "The CF is uniformly continuous on IR.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "47\n",
      "Proof. We want to show that for any t ∈IR, |ϕX(t + h) −ϕX(t)| →0 as\n",
      "h →0.\n",
      "|ϕX(t + h) −ϕX(t)| =\n",
      "\f\f\f\f\n",
      "Z\n",
      "eitTX \u0010\n",
      "eihTX −1\n",
      "\u0011\n",
      "dF\n",
      "\f\f\f\f\n",
      "≤\n",
      "Z \f\f\feihTX −1\n",
      "\f\f\f dF\n",
      "By the bounded convergence theorem (Corollary 0.1.25.1 on page 734), the\n",
      "last integral goes to 0 as h →0, so we have the desired result.\n",
      "Moments and the CF\n",
      "As with the MGF, the (raw) moments of X, if they exist, can be obtained\n",
      "from the derivatives of the CF evaluated at 0. For a scalar random variable\n",
      "X, if the kth derivative of the CF exists in a neighborhood of 0, then\n",
      "dkϕX(t)\n",
      "dtk\n",
      "\f\f\f\f\n",
      "t=0\n",
      "= (−1)k/2E(Xk).\n",
      "(1.96)\n",
      "For vector-valued random variables, the moments become tensors of course,\n",
      "but the ﬁrst two moments are very simple:\n",
      "∇ϕX(t)|t=0 = iE(X)\n",
      "(1.97)\n",
      "and\n",
      "∇∇ϕX(t)|t=0 = −E(XTX).\n",
      "(1.98)\n",
      "Note that these derivatives are wrt a real variable. This means, for example,\n",
      "that existence of the ﬁrst derivative in a neighborhood does not imply the\n",
      "existence of all derivatives in that neighborhood.\n",
      "There is a type of converse to the statement that includes equation (1.96).\n",
      "Theorem 1.23\n",
      "Let X be a scalar random variable. If E(Xk) exists and is ﬁnite, then the kth\n",
      "derivative of the CF of X exists and satisﬁes equation (1.96).\n",
      "We will not give a formal proof here; But note that we have the existence\n",
      "of the derivative of the CF because we can interchange the diﬀerentiation\n",
      "and integration operators. For example, for the ﬁrst moment (and the ﬁrst\n",
      "derivative) we have\n",
      "∞>\n",
      "\f\f\f\f\n",
      "Z\n",
      "xdF\n",
      "\f\f\f\f =\n",
      "\f\f\f\f\n",
      "Z\n",
      "d\n",
      "dt eitx\f\f\n",
      "t=0 dF\n",
      "\f\f\f\f =\n",
      "\f\f\f\f\n",
      "d\n",
      "dt\n",
      "Z\n",
      "eitxdF\n",
      "\f\f\n",
      "t=0\n",
      "\f\f\f\f .\n",
      "This is very diﬀerent from the situation for MGFs; we saw in Example 1.10\n",
      "that the moment may exist but the MGF, let alone its derivative, may not\n",
      "exist.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "48\n",
      "1 Probability Theory\n",
      "The converse of Theorem 1.23 is true for even moments (see Chung (2000)\n",
      "or Gut (2005), for example); that is, if the CF has a ﬁnite even derivative of\n",
      "even order k at 0, then the random variable has a ﬁnite moment of order k.\n",
      "Perhaps surprisingly, however, the converse does not hold for odd moments\n",
      "(see Exercise 1.36).\n",
      "CF and MGF of Multiples and Sums\n",
      "The CF or MGF for scalar multiples of a random variable or for sums of iid\n",
      "random variables is easily obtained from the CF or MGF of the underlying\n",
      "random variable(s). It is easy to see from the deﬁnition that if the random\n",
      "variable X has CF ϕX(t) and Z = aX, for a ﬁxed scalar a, then\n",
      "ϕZ(t) = ϕX(at).\n",
      "(1.99)\n",
      "Likewise, if X1, . . ., Xn are iid with CF ϕX(t), and Y = X1 + · · · + Xn, then\n",
      "the CF or MGF is just the nth power of the CF or MGF of Xi:\n",
      "ϕY (t) = (ϕX(t))n.\n",
      "(1.100)\n",
      "Combining these and generalizing (for independent but not necessarily\n",
      "identically distributed Xi), for W = P\n",
      "i aiXi, we have\n",
      "ϕW (t) =\n",
      "Y\n",
      "i\n",
      "ϕXi(ait).\n",
      "(1.101)\n",
      "On page 59, we discuss the use of CFs and MGFs in studying general\n",
      "transformations of random variables.\n",
      "Uniqueness\n",
      "The importance of the characteristic function or of the moment-generating\n",
      "function if it exists is that it is unique to a given distribution. This fact is\n",
      "asserted formally in the following theorem.\n",
      "Theorem 1.24 (inversion theorem)\n",
      "The CF (or MGF if it exists) completely determines the distribution.\n",
      "We will not prove this theorem here; its proof can be found in a number of\n",
      "places, for example, Billingsley (1995), page 346. It is essentially the same\n",
      "theorem as the one often used in working with Fourier transforms.\n",
      "The limit of a sequence of CFs or MGFs also determines the limiting\n",
      "distribution if it exists, as we will see in Theorem 1.37. This fact, of course,\n",
      "also depends on the uniqueness of CFs or MGFs, if they exist.\n",
      "We now illustrate an application of the CF by proving Theorem 1.17 stated\n",
      "on page 33. This is a standard result, and its method of proof using analytic\n",
      "continuation is standard.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "49\n",
      "Proof. (Theorem 1.17)\n",
      "We are given the ﬁnite scalar moments ν0, ν1, . . . (about any origin) of some\n",
      "probability distribution, and the condition that\n",
      "∞\n",
      "X\n",
      "j=0\n",
      "νjtj\n",
      "j!\n",
      "converges for some real nonzero t. We want to show that the moments uniquely\n",
      "determine the distribution. We will do this by showing that the moments\n",
      "uniquely determine the CF.\n",
      "Because the moments exist, the characteristic function ϕ(t) is continuous\n",
      "and its derivatives exist at t = 0. We have for t in a neighborhood of 0,\n",
      "ϕ(t) =\n",
      "r\n",
      "X\n",
      "j=0\n",
      "(it)jµj\n",
      "j!\n",
      "+ Rr,\n",
      "(1.102)\n",
      "where |Rr| < νr+1|t|r+1/(r + 1)!.\n",
      "Now because P νjtj/j! converges, νjtj/j! goes to 0 and hence the right\n",
      "hand side of equation (1.102) is the inﬁnite series P∞\n",
      "j=0\n",
      "(it)jµj\n",
      "j!\n",
      "if it con-\n",
      "verges. This series does indeed converge because it is dominated termwise\n",
      "by P νjtj/j! which converges. Thus, ϕ(t) is uniquely determined within a\n",
      "neighborhood of t = 0. This is not suﬃcient, however, to say that ϕ(t) is\n",
      "uniquely determined a.e.\n",
      "We must extend the region of convergence to IR. We do this by analytic\n",
      "continuation. Let t0 be arbitrary, and consider a neighborhood of t = t0.\n",
      "Analogous to equation (1.102), we have\n",
      "ϕ(t) =\n",
      "r\n",
      "X\n",
      "j=0\n",
      "ij(t −t0)j\n",
      "j!\n",
      "Z ∞\n",
      "−∞\n",
      "xjeit0xdF + eRr.\n",
      "Now, the modulus of the coeﬃcient of (t −t0)j/j! is less than or equal to νj;\n",
      "hence, in a neighborhood of t0, ϕ(t) can be represented as a convergent Taylor\n",
      "series. But t0 was arbitrary, and so ϕ(t) can be extended through any ﬁnite\n",
      "interval by taking it as the convergent series. Hence, ϕ(t) is uniquely deﬁned\n",
      "over IR in terms of the moments; and therefore the distribution function is\n",
      "uniquely determined.\n",
      "Characteristic Functions for Functions of Random Variables;\n",
      "Joint and Marginal Distributions\n",
      "If X ∈IRd is a random variable and for a Borel function g, g(X) ∈IRm, the\n",
      "characteristic function of g(X) is\n",
      "ϕg(X)(t) = EX\n",
      "\u0010\n",
      "eitTg(X)\u0011\n",
      ",\n",
      "t ∈IRm,\n",
      "(1.103)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "50\n",
      "1 Probability Theory\n",
      "where EX represents expectation wrt the distribution of X. Other generating\n",
      "functions for g(X) are deﬁned similarly.\n",
      "In some cases of interest, the Borel function may just be a projection. For a\n",
      "random variable X consisting of two components, (X1, X2), either component\n",
      "is just a projection of X. In this case, we can factor the generating functions\n",
      "to correspond to the two components.\n",
      "If ϕX(t) is the CF of X, and if we decompose the vector t to be conformable\n",
      "to X = (X1, X2), then we have the CF of X1 as ϕX1(t1) = ϕX(t1, 0).\n",
      "Note that ϕX1(t1) is not (necessarily) the CF of the marginal distribution\n",
      "of X1. The expectation is taken with respect to the joint distribution.\n",
      "Following equation (1.31), we see immediately that X1 and X2 are inde-\n",
      "pendent iﬀ\n",
      "ϕX(t) = ϕX1(t1)ϕX2(t2).\n",
      "(1.104)\n",
      "Cumulant-Generating Function\n",
      "The cumulant-generating function, deﬁned in terms of the characteristic func-\n",
      "tion, can be used to generate the cumulants if they exist.\n",
      "Deﬁnition 1.30 (cumulant-generating function)\n",
      "For the random variable X with characteristic function ϕ(t) the cumulant-\n",
      "generating function is\n",
      "K(t) = log(ϕ(t)).\n",
      "(1.105)\n",
      "(The “K” in the notation for the cumulant-generating function is the Greek\n",
      "letter kappa.) The cumulant-generating function is often called the “second\n",
      "characteristic function”.\n",
      "The derivatives of the cumulant-generating function can be used to evalu-\n",
      "ate the cumulants, similarly to the use of the CF to generate the raw moments,\n",
      "as in equation (1.96).\n",
      "If Z = X + Y , given the random variables X and Y , we see that\n",
      "KZ(t) = KX(t) + KY (t).\n",
      "(1.106)\n",
      "The cumulant-generating function has useful properties for working with\n",
      "random variables of a form such as\n",
      "Yn =\n",
      "\u0010X\n",
      "Xi −nµ\n",
      "\u0011\n",
      "/√nσ,\n",
      "that appeared in the central limit theorem above. If X1, . . ., Xn are iid with\n",
      "cumulant-generating function KX(t), mean µ, and variance 0 < σ2 < ∞, then\n",
      "the cumulant-generating function of Yn is\n",
      "KYn(t) = −\n",
      "√nµt\n",
      "σ\n",
      "+ nKX\n",
      "\u0012\n",
      "t\n",
      "√nσ\n",
      "\u0013\n",
      ".\n",
      "(1.107)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "51\n",
      "A Taylor series expansion of this gives\n",
      "KYn(t) = −1\n",
      "2t2 + K′′′(0)\n",
      "6√nσ3 t3 + · · · ,\n",
      "(1.108)\n",
      "from which we see, as n →∞, that KYn(t) converges to −t2/2, which is the\n",
      "cumulant-generating function of the standard normal distribution.\n",
      "1.1.9 Functionals of the CDF; Distribution “Measures”\n",
      "We often use the term “functional” to mean a function whose arguments\n",
      "are functions. The value of a functional may be any kind of object, a real\n",
      "number or another function, for example. The domain of a functional is a set\n",
      "of functions. I will use notation of the following form: for the functional, a\n",
      "capital Greek or Latin letter, Υ, M, etc.; for the domain, a calligraphic Latin\n",
      "letter, F, G, etc.; for a function, an italic letter, g, F , G, etc.; and for the\n",
      "value, the usual notation for functions, Υ(G) where G ∈G, for example.\n",
      "Parameters of distributions as well as other interesting characteristics of\n",
      "distributions can often be deﬁned in terms of functionals of the CDF. For ex-\n",
      "ample, the mean of a distribution, if it exists, may be written as the functional\n",
      "M of the CDF F :\n",
      "M(F ) =\n",
      "Z\n",
      "y dF (y).\n",
      "(1.109)\n",
      "Viewing this mean functional as a Riemann–Stieltjes integral, for a discrete\n",
      "distribution, it reduces to a sum of the mass points times their associated\n",
      "probabilities.\n",
      "A functional operating on a CDF is called a statistical functional or sta-\n",
      "tistical function. (This is because they are often applied to the ECDF, and\n",
      "in that case are “statistics”.) I will refer to the values of such functionals as\n",
      "distributional measures. (Although the distinction is not important, “M” in\n",
      "equation (1.109) is a capital Greek letter mu. I usually—but not always—\n",
      "will use upper-case Greek letters to denote functionals, especially functionals\n",
      "of CDFs and in those cases, I usually will use the corresponding lower-case\n",
      "letters to represent the measures deﬁned by the functionals.)\n",
      "Many statistical functions, such as M(F ) above, are expectations; but not\n",
      "all are expectations. For example, the quantile functional in equation (1.113)\n",
      "below cannot be written as an expectation. (This was shown by Bickel and Lehmann\n",
      "(1969).)\n",
      "Linear functionals are often of interest. The statistical function M in equa-\n",
      "tion (1.109), for example, is linear over the distribution function space of CDFs\n",
      "for which the integral exists.\n",
      "It is important to recognize that a given functional may not exist at a\n",
      "given CDF. For example, if\n",
      "F (y) = 1/2 + tan−1((y −α)/β))/π\n",
      "(1.110)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "52\n",
      "1 Probability Theory\n",
      "(that is, the distribution is Cauchy), then M(F ) does not exist. (Recall that I\n",
      "follow the convention that when I write an expression such as M(F ) or Υ(F ),\n",
      "I generally imply the existence of the functional for the given F . That is, I do\n",
      "not always use a phrase about existence of something that I implicitly assume\n",
      "exists.)\n",
      "Also, for some parametric distributions, such as the family of beta distri-\n",
      "butions, there may not be a “nice” functional that yields the parameter.\n",
      "A functional of a CDF is generally a function of any parameters associated\n",
      "with the distribution, and in fact we often deﬁne a parameter as a statistical\n",
      "function. For example, if µ and σ are parameters of a distribution with CDF\n",
      "F (y ; µ, σ) and Υ is some functional, we have\n",
      "Υ(F (y ; µ, σ)) = g(µ, σ),\n",
      "for some function g. If, for example, the M in equation (1.109) above is Υ and\n",
      "the F is the normal CDF F (y ; µ, σ), then Υ(F (y ; µ, σ)) = µ.\n",
      "Moments\n",
      "For a univariate distribution with CDF F , the rth central moment from equa-\n",
      "tion (1.50), if it exists, is the functional\n",
      "µr = Mr(F )\n",
      "= R (y −µ)rdF (y).\n",
      "(1.111)\n",
      "For general random vectors or random variables with more complicated\n",
      "structures, this expression may be rather complicated. For r = 2, the matrix\n",
      "of joint moments for a random vector, as given in equation (1.69), is the\n",
      "functional\n",
      "Σ(F ) =\n",
      "Z\n",
      "(y −µ)(y −µ)T dF (y).\n",
      "(1.112)\n",
      "Quantiles\n",
      "Another set of useful distributional measures for describing a univariate dis-\n",
      "tribution with CDF F are the quantiles. For π ∈]0, 1[, the π quantile is given\n",
      "by the functional Ξπ(F ):\n",
      "Ξπ(F ) = inf{y, s.t. F (y) ≥π}.\n",
      "(1.113)\n",
      "This functional is the same as the quantile function or the generalized inverse\n",
      "CDF,\n",
      "Ξπ(F ) = F −1(π),\n",
      "(1.114)\n",
      "as given in Deﬁnition 1.15.\n",
      "The 0.5 quantile is an important one; it is called the median. For the\n",
      "Cauchy distribution, for example, the moment functionals do not exist, but\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "53\n",
      "the median does. An important functional for the Cauchy distribution is,\n",
      "therefore, Ξ0.5(F ) because that is the location of the “middle” of the distri-\n",
      "bution.\n",
      "Quantiles can be used for measures of scale and of characteristics of the\n",
      "shape of a distribution. A measure of the scale of a distribution, for example,\n",
      "is the interquartile range:\n",
      "Ξ0.75 −Ξ0.25.\n",
      "(1.115)\n",
      "Various measures of skewness can be deﬁned as\n",
      "(Ξ1−π −Ξ0.5) −(Ξ0.5 −Ξπ)\n",
      "Ξ1−π −Ξπ\n",
      ",\n",
      "(1.116)\n",
      "for 0 < π < 0.5. For π = 0.25, this is called the quartile skewness or the\n",
      "Bowley coeﬃcient. For π = 0.125, it is called the octile skewness. These can\n",
      "be especially useful with the measures based on moments do not exist. The\n",
      "extent of the peakedness and tail weight can be indicated by the ratio of\n",
      "interquantile ranges:\n",
      "Ξ1−π1 −Ξπ1\n",
      "Ξ1−π2 −Ξπ2\n",
      ".\n",
      "(1.117)\n",
      "These measures can be more useful than the kurtosis coeﬃcient based on the\n",
      "fourth moment, because diﬀerent choices of π1 and π2 emphasize diﬀerent\n",
      "aspects of the distribution. In expression (1.117), π1 = 0.025 and π2 = 0.125\n",
      "yield a good measure of tail weight, and π1 = 0.125 and π2 = 0.25 in expres-\n",
      "sion (1.117) yield a good measure of peakedness.\n",
      "LJ Functionals\n",
      "Various modiﬁcations of the mean functional M in equation (1.109) are often\n",
      "useful, especially in robust statistics. A functional of the form\n",
      "LJ(F ) =\n",
      "Z\n",
      "yJ(y) dF (y),\n",
      "(1.118)\n",
      "for some given function J, is called an LJ functional. If J ≡1, this is the\n",
      "mean functional. Often J is deﬁned as a function of F (y).\n",
      "A “trimmed mean”, for example, is deﬁned by an LJ functional with\n",
      "J(y) = (β −α)−1I]α,β[(F (y)), for constants 0 ≤α < β ≤1 and where I\n",
      "is the indicator function. In this case, the LJ functional is often denoted as\n",
      "Tα,β. Often β is taken to be 1−α, so the trimming is symmetric in probability\n",
      "content.\n",
      "Mρ Functionals\n",
      "Another family of functionals that generalize the mean functional are deﬁned\n",
      "as a solution to the minimization problem\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "54\n",
      "1 Probability Theory\n",
      "Z\n",
      "ρ(y, Mρ(F )) dF (y) = min\n",
      "θ∈Θ\n",
      "Z\n",
      "ρ(y, θ) dF (y),\n",
      "(1.119)\n",
      "for some function ρ and where Θ is some open subset of IRd. A functional de-\n",
      "ﬁned as the solution to this optimization problem is called an Mρ functional.\n",
      "(Note the similarity in names and notation: we call the M in equation (1.109)\n",
      "the mean functional; and we call the Mρ in equation (1.119) the Mρ func-\n",
      "tional.)\n",
      "Two related functions that play important roles in the analysis of Mρ\n",
      "functionals are\n",
      "ψ(y, t) = ∂ρ(y, t)\n",
      "∂t\n",
      ",\n",
      "(1.120)\n",
      "and\n",
      "λF (t) =\n",
      "Z\n",
      "ψ(y, t)dF (y) = ∂\n",
      "∂t\n",
      "Z\n",
      "ρ(y, t)dF (y)\n",
      "(1.121)\n",
      "If y is a scalar and ρ(y, θ) = (y −θ)2 then Mρ(F ) is the mean functional\n",
      "from equation (1.109). Other common functionals also yield solutions to the\n",
      "optimization problem (1.119); for example, for ρ(y, θ) = |y −θ|, Ξ0.5(F ) from\n",
      "equation (1.113) is an Mρ functional (possibly nonunique).\n",
      "We often choose the ρ in an Mρ functional to be a function of y −θ, and\n",
      "to be convex and diﬀerentiable. In this case, the Mρ functional is the solution\n",
      "to\n",
      "E(ψ(Y −θ)) = 0,\n",
      "(1.122)\n",
      "where\n",
      "ψ(y −θ) = dρ(y −θ)/dθ,\n",
      "if that solution is in the interior of Θ.\n",
      "1.1.10 Transformations of Random Variables\n",
      "We often need to determine the distribution of some transformation of a given\n",
      "random variable or a set of random variables. In the simplest case, we have\n",
      "a random variable X with known distribution and we want to determine the\n",
      "distribution of Y = h(X), where h is a full-rank transformation; that is, there\n",
      "is a function h−1 such that X = h−1(Y ). In other cases, the function may not\n",
      "be full-rank, for example, X may be an n-vector, and Y = Pn\n",
      "i=1 Xi.\n",
      "There are three general approaches to the problem: the method of CDFs;\n",
      "the method of direct change of variables, including convolutions; and the\n",
      "method of CFs or MGFs. Sometimes one method works best, and other times\n",
      "some other method works best.\n",
      "Method of CDFs\n",
      "Given X with known CDF FX and Y = h(X) is invertible as above, we can\n",
      "write the CDF FY of Y as\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "55\n",
      "FY (y) = Pr(Y ≤y)\n",
      "= Pr(h(X) ≤y)\n",
      "= Pr(X ≤h−1(y))\n",
      "= FX(h−1(y)).\n",
      "Example 1.11 distribution of the minimum order statistic in a two-\n",
      "parameter exponential distribution\n",
      "Consider a shifted version of the exponential family of distributions, called\n",
      "the two-parameter exponential with parameter (α, θ). Suppose the random\n",
      "variables X1, . . ., Xn are iid with Lebesgue PDF\n",
      "pα,θ(x) = θ−1e−(x−α)/θI]α,∞[(x).\n",
      "We want to ﬁnd the distribution of the minimum of {X1, . . ., Xn}. Let us\n",
      "denote that minimum by Y . (This is an order statistic, for which we use a\n",
      "general notation, Y = X(1). We discuss distributions of order statistics more\n",
      "fully in Section 1.1.12.) We have\n",
      "Pr(Y ≤t) = 1 −Pr(Y > t)\n",
      "= 1 −Pr(Xi > t for i = 1, . . ., n)\n",
      "= 1 −(Pr(Xi > t ∀Xi))n\n",
      "= 1 −(1 −Pr(Xi ≤t ∀Xi))n\n",
      "= 1 −(e−(t−α)/θ)n\n",
      "= 1 −e−n(t−α)/θ.\n",
      "This is the CDF for a two-parameter exponential distribution with param-\n",
      "eters α and θ/n. If instead of a two-parameter exponential distribution, we\n",
      "began with the more common one-parameter exponential distribution with\n",
      "parameter θ, the distribution of Y would be the one-parameter exponential\n",
      "distribution with parameter θ/n.\n",
      "Example 1.12 distribution of the square of a continuous random\n",
      "variable\n",
      "Given X with CDF FX and Lebesgue PDF fX. Let Y = X2. For x < 0,\n",
      "Y −1[]−∞, x]] = ∅, and Y −1[]−∞, x]] = X−1[[−√x, √x]], otherwise. Therefore\n",
      "the CDF FY of Y is\n",
      "FY (x) = Pr(Y −1[] −∞, x]])\n",
      "= Pr(X−1[[−√x, √x]])\n",
      "= (FX(√x) −FX(−√x))I¯IR+(x).\n",
      "Diﬀerentiating, we have the Lebesgue PDF of Y :\n",
      "fY (x) =\n",
      "1\n",
      "2√x(fX(√x) + fX(−√x))I¯IR+(x).\n",
      "(1.123)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "56\n",
      "1 Probability Theory\n",
      "Method of Change of Variables\n",
      "If X has density pX(x|α, θ) and Y = h(X), where h is a full-rank transforma-\n",
      "tion (that is, there is a function h−1 such that X = h−1(Y )), then the density\n",
      "of Y is\n",
      "pY (y|α, θ) = pX\n",
      "\u0000h−1(y)|α, θ\n",
      "\u0001\n",
      "|Jh−1(y)|,\n",
      "(1.124)\n",
      "where Jh−1(y) is the Jacobian of the inverse transformation, and | · | is the\n",
      "determinant.\n",
      "Constant linear transformations are particularly simple. If X is an n-vector\n",
      "random variable with PDF fX and A is an n×n constant matrix of full rank,\n",
      "the PDF of Y = AX is fX|det(A−1)|.\n",
      "In the change of variable method, we think of h as a mapping of the range\n",
      "X of the random variable X to the range Y of the random variable Y , and\n",
      "the method works by expressing the probability content of small regions in Y\n",
      "in terms of the probability content of the pre-image of those regions in X .\n",
      "For a given function h, we often must decompose X into disjoint sets over\n",
      "each of which h is one-to-one.\n",
      "Example 1.13 distribution of the square of a standard normal ran-\n",
      "dom variable\n",
      "Suppose X ∼N(0, 1), and let Y = h(X) = X2. The function h is one-to-one\n",
      "over ] −∞, 0[ and, separately, one-to-one over [0, −∞[. We could, of course,\n",
      "determine the PDF of Y using equation (1.123), but we will use the change-\n",
      "of-variables technique.\n",
      "The absolute value of the Jacobian of the inverse over both regions is\n",
      "x−1/2. The Lebesgue PDF of X is\n",
      "fX(x) =\n",
      "1\n",
      "√\n",
      "2πe−x2/2;\n",
      "hence, the Lebesgue PDF of Y is\n",
      "fY (y) =\n",
      "1\n",
      "√\n",
      "2π y−1/2e−y/2I¯IR+(y).\n",
      "(1.125)\n",
      "This is the PDF of a chi-squared random variable with one degree of freedom\n",
      "χ2\n",
      "1 (see Table A.3).\n",
      "Sums\n",
      "A simple application of the change of variables method is in the common\n",
      "situation of ﬁnding the distribution of the sum of two scalar random variables\n",
      "that are independent but not necessarily identically distributed.\n",
      "Suppose X is a d-variate random variable with PDF fX, Y is a d-variate\n",
      "random variable with PDF fY , and X and Y are independent. We want the\n",
      "density of U = X + Y . We form another variable V = Y and the matrix\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "57\n",
      "A =\n",
      "\u0012 Id Id\n",
      "0 Id\n",
      "\u0013\n",
      ",\n",
      "so that we have a full-rank transformation, (U, V )T = A(X, Y )T The inverse\n",
      "of the transformation matrix is\n",
      "A−1 =\n",
      "\u0012 Id −Id\n",
      "0\n",
      "Id\n",
      "\u0013\n",
      ",\n",
      "and the Jacobian is 1. Because X and Y are independent, their joint PDF\n",
      "is fXY (x, y) = fX(x)fY (y), and the joint PDF of U and V is fUV (u, v) =\n",
      "fX(u −v)fY (v); hence, the PDF of U is\n",
      "fU(u) = R\n",
      "IRd fX(u −v)fY (v)dv\n",
      "=\n",
      "R\n",
      "IRd fY (u −v)fX(v)dv.\n",
      "(1.126)\n",
      "We call fU the convolution of fX and fY . The commutative operation of\n",
      "convolution occurs often in applied mathematics, and we denote it by fU =\n",
      "fX ⋆fY . We often denote the convolution of a function f with itself by f(2);\n",
      "hence, the PDF of X1 + X2 where X1, X2 are iid with PDF fX is f(2)\n",
      "X . From\n",
      "equation (1.126), we see that the CDF of U is the convolution of the CDF of\n",
      "one of the summands with the PDF of the other:\n",
      "FU = FX ⋆fY = FY ⋆fX.\n",
      "(1.127)\n",
      "In the literature, this operation is often referred to as the convolution of the\n",
      "two CDFs, and instead of as in equation (1.127), may be written as\n",
      "FU = FX ⋆FY .\n",
      "Note the inconsistency in notation. The symbol “⋆” is overloaded. Following\n",
      "the latter notation, we also denote the convolution of the CDF F with itself\n",
      "as F (2).\n",
      "Example 1.14 sum of two independent Poissons\n",
      "Suppose X1 is distributed as Poisson(θ1) and X2 is distributed independently\n",
      "as Poisson(θ2). By equation (1.126), we have the probability function of the\n",
      "sum U = X1 + X2 to be\n",
      "fU(u) =\n",
      "u\n",
      "X\n",
      "v=0\n",
      "θu−v\n",
      "1\n",
      "eθ1\n",
      "(u −v)!\n",
      "θv\n",
      "2eθ2\n",
      "v!\n",
      "= 1\n",
      "u!eθ1+θ2(θ1 + θ2)u.\n",
      "The sum of the two independent Poissons is distributed as Poisson(θ1 +θ2).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "58\n",
      "1 Probability Theory\n",
      "Table 1.1. Distributions of the Sums of Independent Random Variables\n",
      "Distributions of Xi\n",
      "for i = 1, . . . , k\n",
      "Distribution of PXi\n",
      "Poisson(θi)\n",
      "Poisson(P θi)\n",
      "Bernoulli(π)\n",
      "binomial(k, π)\n",
      "binomial(ni, π)\n",
      "binomial(P ni, π)\n",
      "geometric(π)\n",
      "negative binomial(k, π)\n",
      "negative binomial(ni, π) negative binomial(P ni, π)\n",
      "normal(µi, σ2\n",
      "i )\n",
      "normal(P µi, Pσ2\n",
      "i )\n",
      "exponential(β)\n",
      "gamma(k, β)\n",
      "gamma(αi, β)\n",
      "gamma(P αi, β)\n",
      "The property shown in Example 1.14 obviously extends to k independent\n",
      "Poissons. Other common distributions also have this kind of property for sums,\n",
      "as shown in Table 1.1. For some families of distributions such as binomial,\n",
      "negative binomial, and gamma, the general case is the sum of special cases.\n",
      "The additive property of the gamma distribution carries over to the special\n",
      "cases: the sum of k iid exponentials with parameter θ is gamma(k, θ) and the\n",
      "sum of independent chi-squared variates with ν1, . . ., νk degrees of freedom is\n",
      "distributed as χ2P νi.\n",
      "The are other distributions that could be included in Table 1.1 if the\n",
      "parameters met certain restrictions, such as being equal; that is, the random\n",
      "variables in the sum are iid.\n",
      "In the case of the inverse Gaussian(µ, λ) distribution, a slightly weaker\n",
      "restriction than iid allows a useful result on the distribution of the sum so long\n",
      "as the parameters have a ﬁxed relationship. If X1, . . ., Xk are independent and\n",
      "Xi is distributed as inverse Gaussian(µ0αi, λ0α2\n",
      "i ), then PXi is distributed as\n",
      "inverse Gaussian(µ0\n",
      "P αi, λ0(P αi)2).\n",
      "Products\n",
      "Another simple application of the change of variables method is for ﬁnding\n",
      "the distribution of the product or the quotient of two scalar random variables\n",
      "that are independent but not necessarily identically distributed.\n",
      "Suppose X is a random variable with PDF fX and Y is a random variable\n",
      "with PDF fY and X and Y are independent, and we want the density of\n",
      "the product U = XY . As for the case with sums, we form another variable\n",
      "V = Y , form the joint distribution of U and V using the Jacobian of the inverse\n",
      "transformation, and ﬁnally integrate out V . Analogous to equation (1.126),\n",
      "we have\n",
      "fU(u) =\n",
      "Z ∞\n",
      "−∞\n",
      "fX(u/v)fY (v)v−1dv,\n",
      "(1.128)\n",
      "and for the quotient W = X/Y , we have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "59\n",
      "fW (w) =\n",
      "Z ∞\n",
      "−∞\n",
      "fX(wv)fY (v)vdv.\n",
      "(1.129)\n",
      "Example 1.15 the F family of distributions\n",
      "Suppose Y1 and Y2 are independent chi-squared random variables with ν1 and\n",
      "ν2 degrees of freedom respectively. We want to ﬁnd the distribution of W =\n",
      "Y1/Y2. Along with the PDFs of chi-squared random variables, equation (1.129)\n",
      "yields\n",
      "fW (w) ∝wν1/2−1\n",
      "Z ∞\n",
      "0\n",
      "v(ν1+ν2)/2−1e−(w+1)v/2dv.\n",
      "This integral can be evaluated by making the change of variables z = (w+1)v.\n",
      "After separating out the factors involving w, the remaining integrand is the\n",
      "PDF of a chi-squared random variable with ν1 + ν2 −1 degrees of freedom.\n",
      "Finally, we make one more change of variables: F = Wν2/ν1. This yields\n",
      "fF(f) ∝\n",
      "fν1/2−1\n",
      "(ν2 + ν1f)(ν1+ν2)/2 .\n",
      "(1.130)\n",
      "This is the PDF of an F random variable with ν1 and ν2 degrees of freedom\n",
      "(see Table A.3). It is interesting to note that the mean of such a random\n",
      "variable depends only on ν2.\n",
      "Method of MGFs or CFs\n",
      "In this method, for the transformation Y = h(X) we write the MGF of Y as\n",
      "E\n",
      "\u0010\n",
      "etTY \u0011\n",
      "= E\n",
      "\u0010\n",
      "etTh(X)\u0011\n",
      ", or we write the CF in a similar way. If we can work\n",
      "out the expectation (with respect to the known distribution of X), we have\n",
      "the MGF or CF of Y , which determines its distribution.\n",
      "The MGF or CF technique is particularly useful in the case when Y is the\n",
      "sum from a simple random sample. If\n",
      "Y = X1 + · · · + Xn,\n",
      "where X1, . . ., Xn are iid with CF ϕX(t), we see from the linearity of the\n",
      "expectation operator that the CF of Y is\n",
      "ϕY (t) = (ϕX(t))n.\n",
      "(1.131)\n",
      "We use this approach in the proof of the simple CLT, Theorem 1.38 on page 87.\n",
      "The MGF or CF for a linear transformation of a random variable has a\n",
      "simple relationship to the MGF or CF of the random variable itself, as we\n",
      "can easily see from the deﬁnition. Let X be a random variable in IRd, A be a\n",
      "d × m matrix of constants, and b be a constant m-vector. Now let\n",
      "Y = ATX + b.\n",
      "Then\n",
      "ϕY (t) = eibTtϕX(At),\n",
      "t ∈IRm.\n",
      "(1.132)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "60\n",
      "1 Probability Theory\n",
      "Example 1.16 distribution of the sum of squares of independent\n",
      "standard normal random variables\n",
      "Suppose X1, . . ., Xn\n",
      "iid\n",
      "∼N(0, 1), and let Y = PX2\n",
      "i . In Example 1.13, we saw\n",
      "that Yi = X2\n",
      "i\n",
      "d= χ2\n",
      "1. Because the Xi are iid, the Yi are iid. Now the MGF of\n",
      "a χ2\n",
      "1 is\n",
      "E\n",
      "\u0000etYi\u0001\n",
      "=\n",
      "Z ∞\n",
      "0\n",
      "1\n",
      "√\n",
      "2π y−1/2e−y(1−2t)/2dy\n",
      "= (1 −2t)−1/2\n",
      "for\n",
      "t < 1\n",
      "2.\n",
      "Hence, the MGF of Y is (1 −2t)n/2 for t < 1/2, which is seen to be the MGF\n",
      "of a chi-squared random variable with n degrees of freedom.\n",
      "This is a very important result for applications in statistics.\n",
      "1.1.11 Decomposition of Random Variables\n",
      "We are often interested in the sum of random numbers,\n",
      "Sk = X1 + · · · + Xk.\n",
      "(1.133)\n",
      "Because the sum may grow unwieldy as k increases, we may work with normed\n",
      "sums of the form Sk/ak.\n",
      "In order to develop interesting properties of Sk, there must be some com-\n",
      "monality among the individual Xi. The most restrictive condition is that the\n",
      "Xi be iid. Another condition for which we can develop meaningful results is\n",
      "that the Xi be independent, but diﬀerent subsequences of them may have\n",
      "diﬀerent distributions.\n",
      "The ﬁnite sums that we consider in this section have relevance in the limit\n",
      "theorems discussed in Sections 1.4.1 and 1.4.2.\n",
      "Inﬁnitely Divisible Distributions\n",
      "Instead of beginning with the Xi and forming their sum, we can think of\n",
      "the problem as beginning with a random variable X and decomposing it into\n",
      "additive components. A property of a random variable that allows a particular\n",
      "kind of additive decomposition is called divisibility.\n",
      "Deﬁnition 1.31 (n-divisibility of random variables)\n",
      "Given a random variable X and an integer n ≥2, we say X is n-divisible if\n",
      "there exist iid random variables X1, . . ., Xn such that\n",
      "X\n",
      "d= X1 + · · · + Xn.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "61\n",
      "Notice that (n + 1)-divisibility does not imply n-divisibility. We can see\n",
      "this in the example of a random variable X with a binomial (3, π) distribution,\n",
      "which is clearly 3-divisible (as the sum of three Bernoullis). We see, however,\n",
      "that X is not 2-divisible; that is, there cannot be iid X1 and X2 such that\n",
      "X = X1 + X2, because if there were, each Xi would take values in [0, 3/2]\n",
      "with Pr(Xi = 0) > 0 and Pr(Xi = 0) > 0, but in that case Pr(X = 3/2) > 0.\n",
      "Rather than divisibility for some ﬁxed n, a more useful form of divisibility\n",
      "is inﬁnite divisibility; that is, divisibility for any n ≥2.\n",
      "Deﬁnition 1.32 (inﬁnite divisibility of random variables)\n",
      "A nondegenerate random variable X is said to be inﬁnitely divisible if for\n",
      "every positive integer n, there are iid random variables Xn1, . . ., Xnn, such\n",
      "that\n",
      "X d= Xn1 + · · · + Xnn.\n",
      "(1.134)\n",
      "Note that the distributions of the Xns may be diﬀerent from each other,\n",
      "but for a given n, Xn1, . . ., Xnn are identically distributed.\n",
      "The random variables Xn1, . . ., Xnn in the deﬁnition above are a special\n",
      "case of a triangular array in which for given n, the Xn1, . . ., Xnn are iid. We\n",
      "encounter similar, more general triangular arrays in some of the limit theorems\n",
      "of Section 1.4.2 (on page 106).\n",
      "Because inﬁnite divisibility is associated with the distribution of the ran-\n",
      "dom variable, we will refer to other characteristics of an inﬁnitely divisible\n",
      "random variable, such as the PDF or CDF, as being inﬁnitely divisible. In\n",
      "the same form as equation (1.131), the characteristic function of an inﬁnitely\n",
      "divisible random variable for any positive integer n can be written as\n",
      "ϕX(t) = (ϕXn(t))n,\n",
      "for some characteristic function ϕXn(t). Furthermore, if the characteristic\n",
      "function of a random variable X can be expressed in this way for any n,\n",
      "then X is inﬁnitely divisible.\n",
      "The normal, the Cauchy, and the Poisson families of distributions are all\n",
      "inﬁnitely divisible (exercise).\n",
      "Divisibility properties are particularly useful in stochastic processes.\n",
      "Stable Distributions\n",
      "Another type of decomposition leads to the concept of stability. In this setup\n",
      "we require that the full set of random variables be iid.\n",
      "Deﬁnition 1.33 (stable random variables) ˜\n",
      "Let X, X1, . . ., Xn be iid nondegenerate random variables. If for each n there\n",
      "exist numbers dn and positive numbers cn, such that\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "62\n",
      "1 Probability Theory\n",
      "X1 + · · · + Xn\n",
      "d= cnX + dn,\n",
      "(1.135)\n",
      "then X is said to have a stable distribution (or to be a stable random variable).\n",
      "This family of distributions is symmetric. We see this by noting that for the\n",
      "variables X1 and X2 in Deﬁnition 1.33 or in equations (1.135) or (1.136),\n",
      "Y = X1 −X2 has a stable distribution that is symmetric about 0 (exercise).\n",
      "Because there is a generalization of the stable family of distributions that is\n",
      "not symmetric (see page 184), the family deﬁned here is sometimes called the\n",
      "symmetric stable family.\n",
      "Deﬁnition 1.33 is equivalent to the requirement that there be three iid\n",
      "nondegenerate random variables, X, X1, and X2, such that for arbitrary con-\n",
      "stants a1 and a2, there exists constants c and d such that\n",
      "a1X1 + a2X2\n",
      "d= cX + d.\n",
      "(1.136)\n",
      "In this case, X is has a stable distribution (or is a stable random variable). It\n",
      "is an exercise to show that this is the case.\n",
      "If X is a stable random variable as deﬁned in equation (1.135), then there\n",
      "exists a number α ∈]0, 2] such that\n",
      "cα = n1/α.\n",
      "(1.137)\n",
      "(See Feller (1971), Section VI.1, for a proof.) The number α is called the index\n",
      "of stability or the characteristic exponent, and the random variable with that\n",
      "index is said to be α-stable. The normal family of distributions is stable with\n",
      "characteristic exponent of 2 and the Cauchy family is stable with characteristic\n",
      "exponent of 1 (exercise).\n",
      "An inﬁnitely divisible family of distributions is stable (exercise). The Pois-\n",
      "son family is an example of a family of distributions that is inﬁnitely divisible,\n",
      "but not stable (exercise).\n",
      "1.1.12 Order Statistics\n",
      "In a set of iid random variables X1, . . ., Xn, it is often of interest to consider\n",
      "the ranked values Xi1 ≤· · · ≤Xin. These are called the order statistics and\n",
      "are denoted as X(1:n), . . ., X(n:n). For 1 ≤k ≤n, we refer to X(k:n) as the kth\n",
      "order statistic. We often use the simpler notation X(k), assuming that n is\n",
      "some ﬁxed and known value. Also, many other authors drop the parentheses\n",
      "in the other representation, Xk:n.\n",
      "Theorem 1.25\n",
      "In a random sample of size n from a distribution with PDF f dominated by a\n",
      "σ-ﬁnite measure and CDF F given the kth order statistic X(k) = a, the k −1\n",
      "random variables less than X(k) are iid with PDF\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.1 Some Important Probability Facts\n",
      "63\n",
      "fright(x) =\n",
      "1\n",
      "F (a)f(x)I]−∞,a[(x),\n",
      "and the n −k random variables greater than X(k) are iid with PDF\n",
      "fleft(x) =\n",
      "1\n",
      "1 −F (a)f(x)I]a,∞[(x).\n",
      "Proof. Exercise.\n",
      "Using Theorem 1.25, forming the joint density, and integrating out all\n",
      "variables except the kth order statistic, we can easily work out the density of\n",
      "the kth order statistic as\n",
      "fX(k)(x) =\n",
      "\u0012n\n",
      "k\n",
      "\u0013\u0010\n",
      "F (x)\n",
      "\u0011k−1\u0010\n",
      "1 −F (x)\n",
      "\u0011n−k\n",
      "f(x).\n",
      "(1.138)\n",
      "Example 1.17 distribution of order statistics from U(0, 1)\n",
      "The distribution of the kth order statistic from a U(0, 1) is the beta distri-\n",
      "bution with parameters k and n −k + 1, as we see from equation (1.138).\n",
      "From equation (1.138), the CDF of the kth order statistic in a sample\n",
      "from a distribution with CDF F can be expressed in terms of the regularized\n",
      "incomplete beta function (equation (C.10) on page 866) as\n",
      "FX(k)(x) = IF(x)(k, n −k + 1).\n",
      "(1.139)\n",
      "The joint density of all order statistics is\n",
      "n!\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "f(x(i))Ix(1)≤···≤x(n)(x(1), . . ., x(n))\n",
      "(1.140)\n",
      "The joint density of the ith and jth (i < j) order statistics is\n",
      "n!\n",
      "(i −1)!(j −i −1)!(n −j)! ·\n",
      "\u0010\n",
      "F (x(i))\n",
      "\u0011i−1\u0010\n",
      "F (x(j)) −F (x(i))\n",
      "\u0011j−i−1\u0010\n",
      "1 −F (x(j))\n",
      "\u0011n−j\n",
      "f(x(i))f(x(j)).\n",
      "(1.141)\n",
      "Although order statistics are obviously not independent, diﬀerences of or-\n",
      "der statistics or functions of those diﬀerences are sometimes independent, as\n",
      "we see in the case of the exponential family a uniform family in the following\n",
      "examples. These functions of diﬀerences of order statistics often are useful in\n",
      "statistical applications.\n",
      "Example 1.18 another distribution from a two-parameter exponen-\n",
      "tial distribution (continuation of Example 1.11)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "64\n",
      "1 Probability Theory\n",
      "Suppose the random variables X1, X2, . . ., Xn\n",
      "iid\n",
      "∼exponential(α, θ). (Note that\n",
      "n ≥2.) Let Y = X(1) as before and let Y1 = P(Xi −X(1)). We want to ﬁnd\n",
      "the distribution of Y1.\n",
      "Note that\n",
      "Y1 =\n",
      "X \u0000Xi −X(1)\n",
      "\u0001\n",
      "=\n",
      "X\n",
      "Xi −nX(1)\n",
      "=\n",
      "X \u0000X(i) −X(1)\n",
      "\u0001 .\n",
      "Now, for k = 2, . . ., n, let\n",
      "Yk = (n −k + 1) \u0000X(k) −X(k−1)\n",
      "\u0001 .\n",
      "Using the change-of-variables technique, we see that Yk\n",
      "iid\n",
      "∼exponential(0, θ),\n",
      "and are independent of X(i). We have independence because the resulting\n",
      "joint density function factorizes. (This is the well-known exponential spacings\n",
      "property.)\n",
      "Now, Pn\n",
      "k=2 Yk = P(X(i) −X(1)) = Y1, and the distribution of the sum of\n",
      "n −1 iid exponentials with parameters 0 and θ, multiplied by θ, is a gamma\n",
      "with parameters n −1 and 1.\n",
      "Example 1.19 distribution of order statistics from U(θ1, θ2)\n",
      "Let X(1), . . ., X(n) be the order statistics in a sample of size n from the\n",
      "U(θ1, θ2) distribution. While it is clear that X(1) and X(n) are not indepen-\n",
      "dent, the random variables\n",
      "Yi = X(i) −X(1)\n",
      "X(n) −X(1)\n",
      ",\n",
      "for i = 2, . . ., n −1,\n",
      "are independent of both X(1) and X(n). (Proving this is Exercise 1.47.)\n",
      "Quantiles and Expected Values of Order Statistics\n",
      "We would expect the kth order statistic in an iid sample of size n to have\n",
      "some relationship to the k/n quantile of the underlying distribution. Because\n",
      "of the limited set of values that k/n can take on for any given n, there are\n",
      "obvious problems in determining a direct relationship between quantiles and\n",
      "order statistics. For given 0 < π < 1, we call the (⌈nπ⌉)th order statistic,\n",
      "X(⌈nπ⌉:n), the sample quantile of order π.\n",
      "The expected value of the kth order statistic in a sample of size n, if it\n",
      "exists, should be approximately the same as the k/n quantile of the underlying\n",
      "distribution; and indeed, this is the case. We will consider this issue for large\n",
      "n in Theorem 1.48, but for now, the ﬁrst question is whether E(X(k:n)) exists.\n",
      "For simplicity in the following, let µ(k:n) = E(X(k:n)). It is easy to see that\n",
      "µ(k:n) exists and is ﬁnite if E(X) exists and is ﬁnite, where the distribution of\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.2 Series Expansions\n",
      "65\n",
      "X is the underlying distribution of the sample (exercise). The converse of this\n",
      "statement is not true, and there are important cases in which µ(k:n) exists and\n",
      "is ﬁnite, but the E(X) does not exist. For example, in the Cauchy distribution,\n",
      "µ(k:n) exists and is ﬁnite unless k = 1 or n (see Exercise 1.48b).\n",
      "For the iid random variables X1, X2, . . . if the expectations µ(k:n) exist, we\n",
      "have the following relationship:\n",
      "(n −k)µ(k:n) + kµ(k+1:n) = nµ(k:n−1).\n",
      "(1.142)\n",
      "(See Exercise 1.49.)\n",
      "1.2 Series Expansions\n",
      "Series expansions are useful in studying properties of probability distributions.\n",
      "We may represent a CDF or PDF as a series of some basis functions. For\n",
      "example, the density may be written as\n",
      "f(x) =\n",
      "∞\n",
      "X\n",
      "r=0\n",
      "crgr(x).\n",
      "In such an expansion, the basis functions are often chosen as derivatives of\n",
      "the normal distribution function, as in the Edgeworth expansions discussed\n",
      "in Section 1.2.4.\n",
      "1.2.1 Asymptotic Properties of Functions\n",
      "***The development of the series representation allows us to investigate the\n",
      "rate of convergence of the moments.\n",
      "*** Consider a set of iid random variables, X1, . . ., Xn, and a function\n",
      "Tn of those random variables. Suppose Tn converges in distribution to the\n",
      "distribution of the normal variate Z. A simple example of such a Tn is the\n",
      "standardized sample mean n1/2(Xn −µ)/σ from a sample of size n. If\n",
      "Tn\n",
      "d→Z,\n",
      "then the characteristic function converges:\n",
      "ϕTn(t) = E(eitTn) →E(eitZ).\n",
      "***The question is how fast does it converge. At what rate (in n) do the\n",
      "moments of Tn approach those of Z?\n",
      "***ﬁx Taylor series expansion *** equation (1.108) κ3 = K′′′(0)\n",
      "KYn(t) = 1\n",
      "2t2 +\n",
      "κ3\n",
      "6√nσ3 t3 + · · · ,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "66\n",
      "1 Probability Theory\n",
      "Thus, if the characteristic function for a random variable Tn, ϕTn(t), can\n",
      "be written as a series in terms involving n, it may be possible to determine\n",
      "the order of convergence of the moments (because the derivatives of the cf\n",
      "yield the moments).\n",
      "expansion of statistical functionals\n",
      "1.2.2 Expansion of the Characteristic Function\n",
      "The characteristic function\n",
      "ϕX(t) = E(eitX)\n",
      "is useful for determining moments or cumulants of the random variable X, for\n",
      "example, ϕ′(0) = iE(X) and ϕ′′(0) = −E(X2).\n",
      "If Tn = n1/2(Xn−µ)/σ, it has an asymptotic standard normal distribution,\n",
      "and\n",
      "ϕTn(t) = E(eitTn)\n",
      "= E(eitn1/2Z)\n",
      "=\n",
      "\u0010\n",
      "ϕZ(t/n1/2)\n",
      "\u0011n\n",
      ",\n",
      "where Z is standard normal.\n",
      "Now the jth cumulant, κj, of Z is the coeﬃcient of (it)j/j! in a power\n",
      "series expansion of the log of the characteristic function, so\n",
      "ϕZ(t) = exp\n",
      "\u0012\n",
      "κ1it + 1\n",
      "2κ2(it)2 + . . . + 1\n",
      "j!κj(it)j + . . .\n",
      "\u0013\n",
      ",\n",
      "but also\n",
      "ϕZ(t) = 1 + E(Z)it + 1\n",
      "2E(Z2)(it)2 + . . . + 1\n",
      "j!E(Zj)(it)j + . . ..\n",
      "1.2.3 Cumulants and Expected Values\n",
      "With some algebra, we can write the cumulants in terms of the expected\n",
      "values as\n",
      "X\n",
      "j≥1\n",
      "1\n",
      "j!κj(it)j = log\n",
      "\n",
      "1 +\n",
      "X\n",
      "j≥1\n",
      "1\n",
      "j!E(Zj)(it)j\n",
      "\n",
      "\n",
      "=\n",
      "X\n",
      "k≥1\n",
      "(−1)k+1 1\n",
      "k\n",
      "\n",
      "X\n",
      "j≥1\n",
      "1\n",
      "j!E(Zj)(it)j\n",
      "\n",
      "\n",
      "k\n",
      ".\n",
      "Equating coeﬃcients, we get\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.2 Series Expansions\n",
      "67\n",
      "κ1 = E(Z)\n",
      "κ2 = E(Z2) −(E(Z))2 = V(Z)\n",
      "κ3 = E((Z −E(Z))3)\n",
      "κ4 = E((Z −E(Z))4) −3(V(Z))2\n",
      "and so on. (There’s a lot of algebra here!)\n",
      "One thing to notice is that the cumulant κj is a homogeneous polynomial\n",
      "of degree j in the moments (i.e., each term is of degree j).\n",
      "Now, back to the characteristic function of Tn: ϕTn(t) = (ϕZ(t/n1/2))n.\n",
      "Using the fact that Z is standard normal (so κ1 = 0 and κ2 = 1), we can\n",
      "write (using the series expansion of the exponential function the last step),\n",
      "ϕTn(t) = exp\n",
      "\u00121\n",
      "2(it)2 + n−1/2 1\n",
      "3!κ3(it)3 + . . . +\n",
      "n−(j−2)/2 1\n",
      "j!κj(it)j + . . .\n",
      "\u0013\n",
      ",\n",
      "= e−t2/2 exp\n",
      "\u0012\n",
      "n−1/2 1\n",
      "3!κ3(it)3 + . . . +\n",
      "n−(j−2)/2 1\n",
      "j!κj(it)j + . . .\n",
      "\u0013\n",
      ",\n",
      "= e−t2/2 \u0010\n",
      "1 + n−1/2r1(it)n−1r2(it) + . . .+\n",
      "n−j/2rj(it) + . . .\n",
      "\u0011\n",
      ",\n",
      "where rj is a polynomial of degree 3j, with real coeﬃcients, and depends on\n",
      "the cumulants κ3, . . ., κj+2, but does not depend on n.\n",
      "r1(x) = 1\n",
      "6κ3x3\n",
      "and\n",
      "r2(x) = 1\n",
      "24κ4x4 + 1\n",
      "72κ2\n",
      "3x6\n",
      "for example. Note that rj is an even polynomial when j is even and is odd\n",
      "when j is odd.\n",
      "The relevance of this is that it indicates the rate of convergence of the\n",
      "moments.\n",
      "1.2.4 Edgeworth Expansions in Hermite Polynomials\n",
      "An Edgeworth expansion represents a distribution function as a series in\n",
      "derivatives of the normal distribution function, or of the density, φ(x). Tak-\n",
      "ing successive derivatives of the normal distribution function yields a series of\n",
      "polynomials that are orthogonal with respect to the normal density.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "68\n",
      "1 Probability Theory\n",
      "If D =\n",
      "d\n",
      "dx, we have,\n",
      "Dφ(x) = −xφ(x),\n",
      "D2φ(x) = (x2 −1)φ(x),\n",
      "D3φ(x) = (−x3 + 3x)φ(x)\n",
      "· · · ... · · ·\n",
      "Letting pi(x) = D(i)φ(x), we see that the pi(x) are orthogonal with respect\n",
      "to the normal density, that is,\n",
      "Z ∞\n",
      "−∞\n",
      "pi(x)pj(x)φ(x) dx = 0,\n",
      "when i ̸= j.\n",
      "From the polynomial factors of *****************, we identify the Her-\n",
      "mite polynomials\n",
      "H0 = 1\n",
      "H1 = x\n",
      "H2 = x2 −1\n",
      "H3 = x3 −3x\n",
      "· · · ... · · ·\n",
      "which as we discuss on page 753, is a series in Hermite polynomials times φ(x)\n",
      "and in the cumulants (or moments). A series using these Hermite polynomials\n",
      "is often called a Gram-Charlier series Edgeworth series.\n",
      "The Edgeworth expansion for a given CDF FX is\n",
      "FX(x) =\n",
      "X\n",
      "crHr(x)φ(x).\n",
      "The series representation is developed by equating terms in the characteristic\n",
      "functions.\n",
      "The adequacy of the series representation depends on how “close” the\n",
      "distribution of the random variable is to normal. If the random variable is\n",
      "asymptotically normal, the adequacy of the series representation of course\n",
      "depends on the rate of convergence of the distribution of the random variable\n",
      "to a normality.\n",
      "1.2.5 The Edgeworth Expansion\n",
      "Inverting the characteristic function transform yields a power series for the\n",
      "distribution function, FTn. After some more algebra, we can get this series in\n",
      "the form\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "69\n",
      "FTn(x) = Φ(x) + n−1/2p1(x)φ(x) + n−1p2(x)φ(x) + . . .,\n",
      "where the pj’s are polynomials containing the cumulants and simple combi-\n",
      "nations of Hermite polynomials.\n",
      "This is the Edgeworth expansion of the distribution function of Tn.\n",
      "The degree of pj is 3j −1, and is even for odd j, and odd for even j. This\n",
      "is one of the most important properties of this representation.\n",
      "In this expansion, heuristically, the term of order n−1/2 corrects the ap-\n",
      "proximation for the eﬀect of skewness, and the term of order n−1 corrects the\n",
      "approximation for the eﬀect of kurtosis.\n",
      "The ﬁrst few Hermite polynomials are shown in equation (0.1.98) on\n",
      "page 753.\n",
      "This is an instance of the more general method of representing a given\n",
      "function in terms of basis functions, as we discuss beginning on page 749.\n",
      "1.3 Sequences of Spaces, Events, and Random Variables\n",
      "Countably inﬁnite sequences play the main role in the deﬁnition of the basic\n",
      "concept of a σ-ﬁeld, and consequently, in the development of a theory of prob-\n",
      "ability. Sequences of sets correspond to sequences of events and, consequently,\n",
      "to sequences of random variables. Unions, intersections, and complements of\n",
      "sequences of sets are important for studying sequences of random variables.\n",
      "The material in this section depends heavily on the properties of sequences of\n",
      "sets discussed on page 626 and the following pages.\n",
      "At the most general level, we could consider a sequence of sample spaces,\n",
      "{(Ωn, Fn, Pn)}, but this level of generality is not often useful. Sequences of\n",
      "σ-ﬁelds and probability measures over a ﬁxed sample space, or sequences of\n",
      "probability measures over a ﬁxed sample space and ﬁxed σ-ﬁeld are often of\n",
      "interest, however.\n",
      "Sequences of σ-Fields and Associated Probability Measures\n",
      "Given a sample space Ω, we may be interested in a sequence of probability\n",
      "spaces, {(Ω, Fn, Pn)}. Such a sequence could arise naturally from a sequence\n",
      "of events {An} that generates a sequence of σ-ﬁelds. Beginning with some\n",
      "base σ-ﬁeld F0, we have the increasing sequence\n",
      "F0 ⊆F1 = σ(F0 ∪{A1}) ⊆F2 = σ(F1 ∪{A2}) ⊆· · · .\n",
      "(1.143)\n",
      "We have a sequence of probability spaces,\n",
      "{(Ω, Fn, Pn) | F0 ⊆F1 ⊆· · · ⊆F},\n",
      "(1.144)\n",
      "where the domains of the measures Pn are evolving, but otherwise the sample\n",
      "space and the other characteristics of P are not changing. This evolving se-\n",
      "quence is the underlying paradigm of some stochastic processes, particularly\n",
      "martingales, that we discuss in Section 1.6.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "70\n",
      "1 Probability Theory\n",
      "Such a sequence of σ-ﬁelds could of course equivalently be generated by a\n",
      "sequence of random variables, instead of by a sequence of sets.\n",
      "The sequence of probability measures exists and the measures are unique,\n",
      "as the Carath´eodory extension theorem ensures (see page 712).\n",
      "Sequences of Probability Measures\n",
      "It is also of interest to consider a sequence of probability measures {Pn} over\n",
      "a ﬁxed measurable space (Ω, F). Such sequences have important applications\n",
      "in statistics. Convergent sequences of probability measures form the basis\n",
      "for asymptotic inference. We will consider the basic properties beginning on\n",
      "page 78, and then consider asymptotic inference in Section 3.8 beginning on\n",
      "page 306.\n",
      "Sequences of Events; lim sup and lim inf\n",
      "In most of our discussion of sequences of events and of random variables, we\n",
      "will assume that we have a single probability space, (Ω, F, P ). This assump-\n",
      "tion is implicit in a phrase such as “a sequence of events and an associated\n",
      "probability measure”.\n",
      "We begin by recalling a basic fact about a sequence of events and an\n",
      "associated probability measure:\n",
      "limP (Ai) ≤P (limAi),\n",
      "(1.145)\n",
      "and it is possible that\n",
      "lim P (Ai) ̸= P (limAi).\n",
      "Compare this with the related fact about a useful sequence of intervals\n",
      "(page 647):\n",
      "lim\n",
      "n→∞\n",
      "n\n",
      "[\n",
      "i=1\n",
      "\u0014\n",
      "a + 1\n",
      "i , b −1\n",
      "i\n",
      "\u0015\n",
      "̸=\n",
      "[\n",
      "lim\n",
      "i→∞\n",
      "\u0014\n",
      "a + 1\n",
      "i , b −1\n",
      "i\n",
      "\u0015\n",
      ".\n",
      "Consider a sequence of probabilities deﬁned in terms of a sequence of\n",
      "events, {P (An)}. Two important types of limits of such sequences of proba-\n",
      "bilities are, similar to the analogous limits for sets deﬁned on page 626,\n",
      "lim sup\n",
      "n\n",
      "P (An) def\n",
      "= inf\n",
      "n sup\n",
      "i≥n\n",
      "P (Ai)\n",
      "(1.146)\n",
      "lim inf\n",
      "n\n",
      "P (An)\n",
      "def\n",
      "= sup\n",
      "n inf\n",
      "i≥n P (Ai).\n",
      "(1.147)\n",
      "Similarly to the corresponding relationship between unions and intersec-\n",
      "tions of sequences of sets, we have the relationships in the following theorem.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "71\n",
      "Theorem 1.26\n",
      "Let {An} be a sequence of events in a probability space. Then\n",
      "P (lim sup\n",
      "n\n",
      "An) ≤lim sup\n",
      "n\n",
      "P (An)\n",
      "(1.148)\n",
      "and\n",
      "P (lim inf\n",
      "n\n",
      "An) ≥lim inf\n",
      "n\n",
      "P (An).\n",
      "(1.149)\n",
      "Proof.\n",
      "Consider\n",
      "Bn = ∪∞\n",
      "i=nAi\n",
      "and\n",
      "Cn = ∩∞\n",
      "i=nAi.\n",
      "We see\n",
      "Bn ↘lim sup\n",
      "n\n",
      "An,\n",
      "and likewise\n",
      "Cn ↗lim inf\n",
      "n\n",
      "An.\n",
      "Now we use the continuity of the measure to get\n",
      "P (An) ≤P (Bn) →P (lim sup\n",
      "n\n",
      "An)\n",
      "and\n",
      "P (An) ≥P (Cn) →P (lim inf\n",
      "n\n",
      "An).\n",
      "For a sequence of sets {An}, we recall the intuitive interpretations of\n",
      "lim supn An and lim infn An:\n",
      "•\n",
      "An element ω is in lim supn An iﬀfor each n, there is some i ≥n for which\n",
      "ω ∈Ai. This means that ω must lie in inﬁnitely many of the An.\n",
      "•\n",
      "An element ω is in lim infn An iﬀthere is some n such that for all i ≥n,\n",
      "ω ∈Ai. This means that ω must lie in all but ﬁnitely many of the An.\n",
      "In applications of probability theory, the sets correspond to events, and\n",
      "generally we are more interested in those events that occur inﬁnitely often;\n",
      "that is, we are more interested in lim supn An. We often denote this as “i.o.”,\n",
      "and we deﬁne\n",
      "{An i.o.} = lim sup\n",
      "n\n",
      "An.\n",
      "(1.150)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "72\n",
      "1 Probability Theory\n",
      "Sequences of Random Variables; lim sup and lim inf\n",
      "The lim sup and lim inf of a sequence of random variables {Xn} mean the lim\n",
      "sup and lim inf of a sequence of functions, which we deﬁned on page 725:\n",
      "lim sup\n",
      "n\n",
      "Xn\n",
      "def\n",
      "= inf\n",
      "n sup\n",
      "i≥n\n",
      "Xi\n",
      "(1.151)\n",
      "and\n",
      "lim inf\n",
      "n\n",
      "Xn\n",
      "def\n",
      "= sup\n",
      "n inf\n",
      "i≥n Xi.\n",
      "(1.152)\n",
      "1.3.1 The Borel-Cantelli Lemmas\n",
      "The analysis of any ﬁnite sequence is straightforward, so the interesting be-\n",
      "havior of a sequence is determined by what happens as n gets large.\n",
      "Tail Events and the Kolmogorov Zero-One Law\n",
      "As n gets large, our interest will be in the “tail” of the sequence. In the fol-\n",
      "lowing, we consider sequences of σ-ﬁelds that are not necessarily increasing\n",
      "or increasing, as were the collections of events used in discussing inequali-\n",
      "ties (1.148) and (1.149).\n",
      "Deﬁnition 1.34 (tail σ-ﬁeld; tail event)\n",
      "Let {Fn} be a sequence of σ-ﬁelds. The σ-ﬁeld\n",
      "T = ∩∞\n",
      "n=1Fn\n",
      "is called the tail σ-ﬁeld of the sequence.\n",
      "An event A ∈T is called a tail event of the sequence.\n",
      "A tail event occurs inﬁnitely often (exercise).\n",
      "We are often interested in tail σ-ﬁelds of sequences of σ-ﬁelds generated by\n",
      "given sequences of events or sequences of random variables. Given the sequence\n",
      "of events {An}, we are interested in the σ-ﬁelds Fi = σ(Ai, Ai+1, . . .). A tail\n",
      "event in the sequence {Fi} is also called a tail event of the sequence of events\n",
      "{An}.\n",
      "Given the sequence of random variables {Xn}, we are also often interested\n",
      "in the σ-ﬁelds σ(Xi, Xi+1, . . .).\n",
      "If the events, σ-ﬁelds, or random variables in the sequence are independent,\n",
      "the independence carries over to subsequences and sub-σ-ﬁelds in a useful way.\n",
      "We will focus on sequences of random variables that deﬁne tail events, but\n",
      "the generating sequence could also be of events or of σ-ﬁelds.\n",
      "Lemma 1.27.1\n",
      "Let {Xn} be a sequence of independent random variables and let T\n",
      "be\n",
      "the tail σ-ﬁeld, ∩∞\n",
      "i=1σ(Xi, Xi+2, . . .). Then the events A ∈T\n",
      "and B ∈\n",
      "σ(X1, . . ., Xi−1) are independent for each i. Furthermore, A is independent\n",
      "of σ(X1, X2, . . .).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "73\n",
      "Proof.\n",
      "Because the random variables X1, X2, . . . are independent, T and σ(X1, . . ., Xi−1)\n",
      "are independent and hence the events A ∈T and B ∈σ(X1, . . ., Xi−1) are\n",
      "independent. (This is the same reasoning as in the proof of Theorem 1.11,\n",
      "which applies to random.) Therefore, A is independent of σ(X1, . . ., Xi−1)\n",
      "and hence, also independent of F0 = ∪∞\n",
      "i=1σ(X1, . . ., Xi). By Theorem 1.1\n",
      "then, A is independent of σ(X1, X2, . . .).\n",
      "Lemma 1.27.1 has an interesting implication. Tail events in sequences gen-\n",
      "erated by independent events or random variables are independent of them-\n",
      "selves.\n",
      "Theorem 1.27 (Kolmogorov’s Zero-One Law)\n",
      "Let {Xn} be a sequence of independent random variables and A be an event\n",
      "in the tail σ-ﬁeld of the sequence of σ-ﬁelds generated by {Xn}. Then P (A)\n",
      "is either zero or one.\n",
      "Proof. An event A be an event in the tail σ-ﬁeld is independent of itself;\n",
      "hence\n",
      "P (A) = P (A ∪A) = P (A)P (A),\n",
      "and so P (A) must have a probability of 0 or 1.\n",
      "For a sequence of events in a given probability space, the Borel-Cantelli\n",
      "lemmas address the question implied by the Kolmogorov zero-one law. These\n",
      "lemmas tell us the probability of the lim sup. Under one condition, we get a\n",
      "probability of 0 without requiring independence. Under the other condition,\n",
      "with a requirement of independence, we get a probability of 1.\n",
      "Theorem 1.28 (Borel-Cantelli Lemma I)\n",
      "Let {An} be a sequence of events and P be a probability measure. Then\n",
      "∞\n",
      "X\n",
      "n=1\n",
      "P (An) < ∞\n",
      "=⇒\n",
      "P (lim sup\n",
      "n\n",
      "An) = 0.\n",
      "(1.153)\n",
      "Proof. First, notice that P (∪∞\n",
      "i=nAi) can be arbitrarily small if n is large\n",
      "enough. From lim supn An ⊆∪∞\n",
      "i=nAi, we have\n",
      "P (lim sup\n",
      "n\n",
      "An) ≤P (∪∞\n",
      "i=nAi)\n",
      "≤\n",
      "∞\n",
      "X\n",
      "i=n\n",
      "P (Ai)\n",
      "→0\n",
      "as n →∞\n",
      "because\n",
      "∞\n",
      "X\n",
      "n=1\n",
      "P (An) < ∞.\n",
      "The requirement that P∞\n",
      "n=1 P (An) < ∞in Theorem 1.28 means that\n",
      "An must be approaching sets with ever-smaller probability. If that is not\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "74\n",
      "1 Probability Theory\n",
      "the case, say for example, if An is a constant set with positive probability,\n",
      "then of course P∞\n",
      "n=1 P (An) = ∞, but we cannot say much about about\n",
      "P (lim supn An). However, if the An are disjoint, then P∞\n",
      "n=1 P (An) = ∞may\n",
      "have a meaningful implication. The second Borel-Cantelli requires that the\n",
      "sets be independent.\n",
      "Theorem 1.29 (Borel-Cantelli Lemma II)\n",
      "Let {An} be a sequence of independent events and P be a probability measure.\n",
      "Then\n",
      "∞\n",
      "X\n",
      "n=1\n",
      "P (An) = ∞\n",
      "=⇒\n",
      "P (lim sup\n",
      "n\n",
      "An) = 1.\n",
      "(1.154)\n",
      "Proof. Applying de Morgan’s law (equation (0.0.21)), we just need to show\n",
      "that P (lim infn Ac\n",
      "n) = 0. We use the fact that for x ≥0\n",
      "1 −x ≤e−x\n",
      "to get, for any n and j,\n",
      "P\n",
      "\u0010\n",
      "∩n+j\n",
      "k=nAc\n",
      "k\n",
      "\u0011\n",
      "=\n",
      "n+j\n",
      "Y\n",
      "k=n\n",
      "(1 −P (Ak))\n",
      "≤exp\n",
      " \n",
      "−\n",
      "n+j\n",
      "X\n",
      "k=n\n",
      "P (Ak)\n",
      "!\n",
      ".\n",
      "Since P∞\n",
      "n=1 P (An) diverges, the last expression goes to 0 as j →∞, and so\n",
      "P (∩∞\n",
      "k=nAc\n",
      "k) = lim\n",
      "j→∞P\n",
      "\u0010\n",
      "∩n+j\n",
      "k=nAc\n",
      "k\n",
      "\u0011\n",
      "= 0.\n",
      "1.3.2 Exchangeability and Independence of Sequences\n",
      "The sequences that we have discussed may or may not be exchangeable or\n",
      "independent. The sequences in Theorem 1.29 are independent, for example,\n",
      "but most sequences that we consider are not necessarily independent. When\n",
      "we discuss limit theorems in Section 1.4, we will generally require indepen-\n",
      "dence. In Section 1.6, we will relax the requirement of independence, but\n",
      "will require some common properties of the elements of the sequence. Before\n",
      "proceeding, however, in this general section on sequences of random vari-\n",
      "ables, we will brieﬂy consider exchangeable sequences and state without proof\n",
      "de Finetti’s representation theorem, which provides a certain connection be-\n",
      "tween exchangeability and independence. This theorem tells us that inﬁnite\n",
      "sequences of exchangeable binary random variables are mixtures of indepen-\n",
      "dent Bernoulli sequences.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "75\n",
      "Theorem 1.30 (de Finetti’s representation theorem)\n",
      "Let {Xi}∞\n",
      "i=1 be an inﬁnite sequence of binary random variables such that for\n",
      "any n, {Xi}n\n",
      "i=1 is exchangeable. Then there is a unique probability measure P\n",
      "on [0, 1] such that for each ﬁxed sequence of zeros and ones {ei}n\n",
      "i=1,\n",
      "Pr(X1 = e1, . . ., Xn = en) =\n",
      "Z 1\n",
      "0\n",
      "πk(1 −π)n−kdµ(π),\n",
      "where k = Pn\n",
      "i=1 ei.\n",
      "The converse clearly holds; that is, if a P as speciﬁed in the theorem exists,\n",
      "then the sequence is exchangeable.\n",
      "A proof of a more general version of de Finetti’s representation the-\n",
      "orem (for random variables that are not necessarily binary) is given by\n",
      "Hewitt and Stromberg (1965) and in Schervish (1995).\n",
      "1.3.3 Types of Convergence\n",
      "The ﬁrst important point to understand about asymptotic theory is that there\n",
      "are diﬀerent kinds of convergence of a sequence of random variables, {Xn}.\n",
      "Three of these kinds of convergence have analogues in convergence of gen-\n",
      "eral measurable functions (see Appendix 0.1) and a fourth type applies to\n",
      "convergence of the measures themselves. Diﬀerent types of convergence apply\n",
      "to\n",
      "•\n",
      "a function, that is, directly to the random variable (Deﬁnition 1.35). This\n",
      "is the convergence that is ordinarily called “strong convergence”.\n",
      "•\n",
      "expected values of powers of the random variable (Deﬁnition 1.36). This\n",
      "is also a type of strong convergence.\n",
      "•\n",
      "probabilities of the random variable being within a range of another ran-\n",
      "dom variable (Deﬁnition 1.37). This is a weak convergence.\n",
      "•\n",
      "the distribution of the random variable (Deﬁnition 1.39, stated in terms\n",
      "of weak convergence of probability measures, Deﬁnition 1.38). This is the\n",
      "convergence that is ordinarily called “weak convergence”.\n",
      "In statistics, we are interested in various types of convergence of proce-\n",
      "dures of statistical inference. Depending on the kind of inference, one type\n",
      "of convergence may be more relevant than another. We will discuss these in\n",
      "later chapters. At this point, however, it is appropriate to point out that an\n",
      "important property of point estimators is consistency, and the various types\n",
      "of consistency of point estimators, which we will discuss in Section 3.8.1, cor-\n",
      "respond directly to the types of convergence of sequences of random variables\n",
      "we discuss below.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "76\n",
      "1 Probability Theory\n",
      "Almost Sure Convergence\n",
      "Deﬁnition 1.35 (almost sure (a.s.) convergence)\n",
      "We say that {Xn} converges almost surely to X if\n",
      "lim\n",
      "n→∞Xn = X a.s.\n",
      "(1.155)\n",
      "We write\n",
      "Xn\n",
      "a.s.\n",
      "→X.\n",
      "Writing this deﬁnition in the form of Deﬁnition 0.1.38 on page 726, with Xn\n",
      "and X deﬁned on the probability space (Ω, F, P ), we have\n",
      "P ({ω :\n",
      "lim\n",
      "n→∞Xn(ω) = X(ω)}) = 1.\n",
      "(1.156)\n",
      "This expression provides a very useful heuristic for distinguishing a.s. conver-\n",
      "gence from other types of convergence.\n",
      "Almost sure convergence is equivalent to\n",
      "lim\n",
      "n→∞Pr (∪∞\n",
      "m=n∥Xm −X∥> ϵ) = 0,\n",
      "(1.157)\n",
      "for every ϵ > 0 (exercise).\n",
      "Almost sure convergence is also called “almost certain” convergence, and\n",
      "written as Xn\n",
      "a.c.\n",
      "→X.\n",
      "The condition (1.155) can also be written as\n",
      "Pr\n",
      "\u0010\n",
      "lim\n",
      "n→∞∥Xn −X∥< ϵ\n",
      "\u0011\n",
      "= 1,\n",
      "(1.158)\n",
      "for every ϵ > 0. For this reason, almost sure convergence is also called conver-\n",
      "gence with probability 1, and may be indicated by writing Xn\n",
      "wp1\n",
      "→X. Hence,\n",
      "we may encounter three equivalent expressions:\n",
      "a.s.\n",
      "→\n",
      "≡\n",
      "a.c.\n",
      "→\n",
      "≡\n",
      "wp1\n",
      "→.\n",
      "Almost sure convergence of a sequence of random variables {Xn} to a\n",
      "constant c implies lim supn Xn = lim infn Xn = c, and implies {Xn = c i.o.};\n",
      "by itself, however, {Xn = c i.o.} does not imply any kind of convergence of\n",
      "{Xn}.\n",
      "Convergence in rth Moment\n",
      "Deﬁnition 1.36 (convergence in rth moment (convergence in Lr))\n",
      "For ﬁxed r > 0, we say that {Xn} converges in rth moment to X if\n",
      "lim\n",
      "n→∞E(∥Xn −X∥r\n",
      "r) = 0.\n",
      "(1.159)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "77\n",
      "We write\n",
      "Xn\n",
      "Lr\n",
      "→X.\n",
      "(Compare Deﬁnition 0.1.50 on page 748.)\n",
      "Convergence in rth moment requires that E(∥Xn∥r\n",
      "r) < ∞for each n. Con-\n",
      "vergence in rth moment implies convergence in sth moment for s ≤r (and, of\n",
      "course, it implies that E(∥Xn∥s\n",
      "s) < ∞for each n). (See Theorem 1.16, which\n",
      "was stated only for scalar random variables.)\n",
      "For r = 1, convergence in rth moment is called convergence in absolute\n",
      "mean. For r = 2, it is called convergence in mean square or convergence in\n",
      "second moment, and of course, it implies convergence in mean. (Recall our\n",
      "notational convention: ∥Xn −X∥= ∥Xn −X∥2.)\n",
      "The Cauchy criterion (see Exercise 0.0.6d on page 689) is often useful for\n",
      "proving convergence in mean or convergence in mean square, without speci-\n",
      "fying the limit of the sequence. The sequence {Xn} converges in mean square\n",
      "(to some real number) iﬀ\n",
      "lim\n",
      "n,m→∞E(∥Xn −Xm∥) = 0.\n",
      "(1.160)\n",
      "Convergence in Probability\n",
      "Deﬁnition 1.37 (convergence in probability)\n",
      "We say that {Xn} converges in probability to X if for every ϵ > 0,\n",
      "lim\n",
      "n→∞Pr(∥Xn −X∥> ϵ) = 0.\n",
      "(1.161)\n",
      "We write\n",
      "Xn\n",
      "p→X.\n",
      "(Compare Deﬁnition 0.1.51 on page 748 for general measures.)\n",
      "Notice the diﬀerence in convergence in probability and convergence in\n",
      "rth moment. Convergence in probability together with uniform integrability\n",
      "implies convergence in mean, but not in higher rth moments. It is easy to\n",
      "construct examples of sequences that converge in probability but that do not\n",
      "converge in second moment (exercise).\n",
      "Notice the diﬀerence in convergence in probability and almost sure con-\n",
      "vergence; in the former case the limit of probabilities is taken, in the lat-\n",
      "ter the case a probability of a limit is evaluated; compare equations (1.157)\n",
      "and (1.161). It is easy to construct examples of sequences that converge in\n",
      "probability but that do not converge almost surely (exercise).\n",
      "Although convergence in probability does not imply almost sure converge,\n",
      "it does imply the existence of a subsequence that does converge almost surely,\n",
      "as stated in the following theorem.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "78\n",
      "1 Probability Theory\n",
      "Theorem 1.31\n",
      "Suppose {Xn} converges in probability to X. Then there exists a subsequence\n",
      "{Xni} that converges almost surely to X.\n",
      "Stated another way, this theorem says that if {Xn} converges in probability\n",
      "to X, then there is an increasing sequence {ni} of positive integers such that\n",
      "lim\n",
      "i→∞Xni\n",
      "a.s.\n",
      "= X.\n",
      "Proof. The proof is an exercise. You could ﬁrst show that there is an increas-\n",
      "ing sequence {ni} such that\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "Pr(|Xni −X| > 1/i) < ∞,\n",
      "and from this conclude that Xni\n",
      "a.s.\n",
      "→X.\n",
      "Weak Convergence\n",
      "There is another type of convergence that is very important in statistical\n",
      "applications; in fact, it is the basis for asymptotic statistical inference. This\n",
      "convergence is deﬁned in terms of pointwise convergence of the sequence of\n",
      "CDFs; hence it is a weak convergence. We will give the deﬁnition in terms of\n",
      "the sequence of CDFs or, equivalently, of probability measures, and then state\n",
      "the deﬁnition in terms of a sequence of random variables.\n",
      "Deﬁnition 1.38 (weak convergence of probability measures)\n",
      "Let {Pn} be a sequence of probability measures and {Fn} be the sequence\n",
      "of corresponding CDFs, and let F be a CDF with corresponding probability\n",
      "measure P . If at each point of continuity t of F ,\n",
      "lim\n",
      "n→∞Fn(t) = F (t),\n",
      "(1.162)\n",
      "we say that the sequence of CDFs {Fn} converges weakly to F , and, equiva-\n",
      "lently, we say that the sequence of probability measures {Pn} converges weakly\n",
      "to P . We write\n",
      "Fn\n",
      "w→F\n",
      "or\n",
      "Pn\n",
      "w→P\n",
      "Deﬁnition 1.39 (convergence in distribution (in law))\n",
      "If {Xn} have CDFs {Fn} and X has CDF F , we say that {Xn} converges in\n",
      "distribution or in law to X iﬀFn\n",
      "w→F . We write\n",
      "Xn\n",
      "d→X.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "79\n",
      "Because convergence in distribution is not precisely a convergence of the\n",
      "random variables themselves, it may be preferable to use a notation of the\n",
      "form\n",
      "L(Xn) →L(X),\n",
      "where the symbol L(·) refers to the distribution or the “law” of the random\n",
      "variable.\n",
      "When a random variable converges in distribution to a distribution for\n",
      "which we have adopted a symbol such as N(µ, σ2), for example, we may use\n",
      "notation of the form\n",
      "Xn\n",
      "∼→N(µ, σ2).\n",
      "Because this notation only applies in this kind of situation, we often write it\n",
      "more simply as just\n",
      "Xn →N(µ, σ2),\n",
      "or in the “law” notation, L(Xn) →N(µ, σ2)\n",
      "For certain distributions we have special symbols to represent a random\n",
      "variable. In such cases, we may use notation of the form\n",
      "Xn\n",
      "d→χ2\n",
      "ν,\n",
      "which in this case indicates that the sequence {Xn} converges in distribution\n",
      "to a random variable with a chi-squared distribution with ν degrees of freedom.\n",
      "The “law” notation for this would be L(Xn) →L(χ2\n",
      "ν).\n",
      "Determining Classes\n",
      "In the case of multiple probability measures over a measurable space, we\n",
      "may be interested in how these measures behave over diﬀerent sub-σ-ﬁelds,\n",
      "in particular, whether there is a determining class smaller than the σ-ﬁeld of\n",
      "the given measurable space. For convergent sequences of probability measures,\n",
      "the determining classes of interest are those that preserve convergence of the\n",
      "measures for all sets in the σ-ﬁeld of the given measurable space.\n",
      "Deﬁnition 1.40 (convergence-determining class)\n",
      "Let {Pn} be a sequence of probability measures deﬁned on the measurable\n",
      "space (Ω, F) that converges (weakly) to P , also a probability measure deﬁned\n",
      "on (Ω, F). A collection of subsets C ⊆F is called a convergence-determining\n",
      "class of the sequence, iﬀ\n",
      "Pn(A) →P (A) ∀A ∈C ∋P (∂A) = 0 =⇒Pn(B) →P (B) ∀B ∈F.\n",
      "It is easy to see that a convergence-determining class is a determining\n",
      "class (exercise), but the converse is not true, as the following example from\n",
      "Romano and Siegel (1986) shows.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "80\n",
      "1 Probability Theory\n",
      "Example 1.20 a determining class that is not a convergence-deter-\n",
      "mining class\n",
      "For this example, we use the familiar measurable space (IR, B), and construct\n",
      "a determining class C whose sets exclude exactly one point, and then deﬁne\n",
      "a probability measure P that puts mass one at that point. All that is then\n",
      "required is to deﬁne a sequence {Pn} that converges to P . The example given\n",
      "by Romano and Siegel (1986) is the collection C of all ﬁnite open intervals\n",
      "that do not include the single mass point of P . (It is an exercise to show\n",
      "that this is a determining class.) For deﬁniteness, let that special point be\n",
      "0, and let Pn be the probability measure that puts mass one at n. Then, for\n",
      "any A ∈C, Pn(A) →0 = P (A), but for any interval (a, b) where a < 0 and\n",
      "0 < b < 1, Pn((a, b)) = 0 but P ((a, b)) = 1.\n",
      "Both convergence in probability and convergence in distribution are weak\n",
      "types of convergence. Convergence in probability, however, means that the\n",
      "probability is high that the two random variables are close to each other,\n",
      "while convergence in distribution means that two random variables have the\n",
      "same distribution. That does not mean that they are very close to each other.\n",
      "The term “weak convergence” is often used speciﬁcally for convergence\n",
      "in distribution because this type of convergence has so many applications in\n",
      "asymptotic statistical inference. In many interesting cases the limiting dis-\n",
      "tribution of a sequence {Xn} may be degenerate, but for some sequence of\n",
      "constants an, the limiting distribution of {anXn} may not be degenerate and\n",
      "in fact may be very useful in statistical applications. The limiting distribution\n",
      "of {anXn} for a reasonable choice of a sequence of normalizing constants {an}\n",
      "is called the asymptotic distribution of {Xn}. After some consideration of the\n",
      "relationships among the various types of convergence, in Section 1.3.7, we will\n",
      "consider the “reasonable” choice of normalizing constants and other proper-\n",
      "ties of weak convergence in distribution in more detail. The relevance of the\n",
      "limiting distribution of {anXn} will become more apparent in the statistical\n",
      "applications in Section 3.8.2 and later sections.\n",
      "Relationships among Types of Convergence\n",
      "Almost sure convergence and convergence in rth moment are both strong types\n",
      "of convergence, but they are not closely related to each other. We have the\n",
      "logical relations shown in Figure 1.3.\n",
      "The directions of the arrows in Figure 1.3 correspond to theorems with\n",
      "straightforward proofs. Where there are no arrows, as between Lr and a.s.,\n",
      "we can ﬁnd examples that satisfy one condition but not the other (see Ex-\n",
      "amples 1.21 and 1.22 below). For relations in the opposite direction of the\n",
      "arrows, we can construct counterexamples, as for example, the reader is asked\n",
      "to do in Exercises 1.54a and 1.54b.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "81\n",
      "Lr QQQQQQQ\n",
      "s\n",
      "?\n",
      "L1 PPPPPPP\n",
      "q\n",
      "P\n",
      "P\n",
      "P\n",
      "P\n",
      "P\n",
      "P\n",
      "P\n",
      "i\n",
      "uniformly\n",
      "integrable\n",
      "a.s\n",
      "\u0011\n",
      "\u0011\n",
      "\u0011\n",
      "\u0011\n",
      "\u0011\n",
      "\u0011\n",
      "\u0011\n",
      "+\n",
      "?\n",
      "a.s\n",
      "\u0010\u0010\u0010\u0010\u0010\u0010\u0010\n",
      "1\n",
      "\u0010\n",
      "\u0010\n",
      "\u0010\n",
      "\u0010\n",
      "\u0010\n",
      "\u0010\n",
      "\u0010\n",
      ") subsequence\n",
      "p\n",
      "?\n",
      "d (or w)\n",
      "Figure 1.3.\n",
      "Relationships of Convergence Types\n",
      "Useful Sequences for Studying Types of Convergence\n",
      "Just as for working with limits of unions and intersections of sets where we\n",
      "ﬁnd it useful to identify sequences of sets that behave in some simple way\n",
      "(such as the intervals [a + 1/n, b −1/n] on page 646), it is also useful to\n",
      "identify sequences of random variables that behave in interesting but simple\n",
      "ways.\n",
      "One useful sequence begins with {Un}, where Un ∼U(0, 1/n). We deﬁne\n",
      "Xn = nUn.\n",
      "(1.163)\n",
      "This sequence can be used to show that an a.s. convergent sequence may not\n",
      "converge in L1.\n",
      "Example 1.21 converges a.s. but not in mean\n",
      "Let {Xn} be the sequence deﬁned in equation (1.163). Since Pr(limn→∞Xn =\n",
      "0) = 1, Xn\n",
      "a.s.\n",
      "→0. The mean and in fact the rth moment (for r > 0) is 0.\n",
      "However,\n",
      "E(|Xn −0|r) =\n",
      "Z 1/n\n",
      "0\n",
      "nrdu = nr−1.\n",
      "For r = 1, this does not converge to the mean of 0, and for r > 1, it diverges;\n",
      "hence {Xn} does not converge to 0 in rth moment for any r ≥1. (It does\n",
      "converge to the correct rth moment for 0 < r < 1, however.)\n",
      "This example is also an example of a sequence that converges in probability\n",
      "(since a.s. convergence implies that), but does not converge in rth moment.\n",
      "Other kinds of interesting sequences can be constructed as indicators of\n",
      "events; that is, 0-1 random variables. One such simple sequence is the Bernoulli\n",
      "random variables {Xn} with probability that Xn = 1 being 1/n. This sequence\n",
      "can be used to show that a sequence that converges to X in probability does\n",
      "not necessarily converge to X a.s.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "82\n",
      "1 Probability Theory\n",
      "Other ways of deﬁning 0-1 random variables involve breaking a U(0, 1)\n",
      "distribution into uniform distributions on partitions of ]0, 1[. For example, for\n",
      "a positive integer k, we may form 2k subintervals of ]0, 1[ for j = 1, . . ., 2k as\n",
      "\u0015j −1\n",
      "2k ,\n",
      "j\n",
      "2k\n",
      "\u0014\n",
      ".\n",
      "As k gets larger, the Lebesgue measure of these subintervals approaches 0\n",
      "rapidly. Romano and Siegel (1986) build an indicator sequence using random\n",
      "variables on these subintervals for various counterexamples. This sequence can\n",
      "be used to show that an L2 convergent sequence may not converge a.s., as in\n",
      "the following example.\n",
      "Example 1.22 converges in second moment but not a.s.\n",
      "Let U ∼U(0, 1) and deﬁne\n",
      "Xn =\n",
      "(\n",
      "1\n",
      "if\n",
      "jn −1\n",
      "2kn\n",
      "< U < jn\n",
      "2kn\n",
      "0\n",
      "otherwise,\n",
      "where jn = 1, . . ., 2kn and kn →∞as n →∞. We see that\n",
      "E((Xn −0)2) = 1/(2kn),\n",
      "hence {Xn} converges in quadratic mean (or in mean square) to 0. We see,\n",
      "however, that limn→∞Xn does not exist (since for any value of U, Xn takes on\n",
      "each of the values 0 and 1 inﬁnitely often). Therefore, {Xn} cannot converge\n",
      "a.s. (to anything!).\n",
      "This is another example of a sequence that converges in probability (since\n",
      "convergence in rth moment implies that), but does not converge a.s.\n",
      "Convergence of PDFs\n",
      "The weak convergence of a sequence of CDFs {Fn} is the basis for most\n",
      "asymptotic statistical inference. The convergence of a sequence of PDFs {fn}\n",
      "is a stronger form of convergence because it implies uniform convergence of\n",
      "probability on any given Borel set.\n",
      "Theorem 1.32 (Scheﬀ´e)\n",
      "Let {fn} be a sequence of PDFs that converge pointwise to a PDF f; that is,\n",
      "at each x\n",
      "lim\n",
      "n→∞fn(x) = f(x).\n",
      "Then\n",
      "lim\n",
      "n→∞\n",
      "Z\n",
      "B\n",
      "|fn(x) −f(x)|dx = 0\n",
      "(1.164)\n",
      "uniformly for any Borel set B.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "83\n",
      "For a proof see Scheﬀ´e (1947).\n",
      "Hettmansperger and Klimko (1974) showed that if a weakly convergent\n",
      "sequence of CDFs {Fn} has an associated sequence of PDFs {fn}, and if\n",
      "these PDFs are unimodal at a given point, then on any closed interval that\n",
      "does not contain the modal point the sequence of PDFs converge uniformly\n",
      "to a PDF.\n",
      "Big O and Little o Almost Surely\n",
      "We are often interested in nature of the convergence or the rate of convergence\n",
      "of a sequence of random variables to another sequence of random variables.\n",
      "As in general spaces of real numbers that we consider in Section 0.0.5 on\n",
      "page 652, we distinguish two types of limiting behavior by big O and little\n",
      "o. These are involve the asymptotic ratio of the elements of one sequence to\n",
      "the elements of a given sequence {an}. We deﬁned two order classes, O(an)\n",
      "and o(an). In this section we begin with a given sequence of random variables\n",
      "{Yn} and deﬁne four diﬀerent order classes, O(Yn) a.s., o(Yn) a.s., OP(Yn),\n",
      "and oP(Yn), based on whether or not the ratio is approaching 0 (that is, big\n",
      "O or little o) and on whether the converge is almost sure or in probability.\n",
      "For sequences of random variables {Xn} and {Yn} deﬁned on a common\n",
      "probability space, we identify diﬀerent types of convergence, either almost\n",
      "sure or in probability.\n",
      "•\n",
      "Big O almost surely, written O(Yn) a.s.\n",
      "Xn ∈O(Yn) a.s. iﬀPr (∥Xn∥∈O(∥Yn∥)) = 1\n",
      "•\n",
      "Little o almost surely, written o(Yn) a.s.\n",
      "Xn ∈o(Yn) a.s. iﬀ∥Xn∥/∥Yn∥a.s.\n",
      "→0.\n",
      "Compare Xn/Yn\n",
      "a.s.\n",
      "→0 for Xn ∈IRm and Yn ∈IR.\n",
      "Big O and Little o Weakly\n",
      "We also have relationships in which one sequence converges to another in\n",
      "probability.\n",
      "•\n",
      "Big O in probability, written OP(Yn).\n",
      "Xn ∈OP(Yn) iﬀ∀ϵ > 0 ∃constant Cϵ > 0 ∋sup\n",
      "n Pr(∥Xn∥≥Cϵ∥Yn∥) < ϵ.\n",
      "If Xn ∈OP(1), Xn is said to be bounded in probability.\n",
      "If Xn\n",
      "d→X for any random variable X, then Xn ∈OP(1). (Exercise.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "84\n",
      "1 Probability Theory\n",
      "•\n",
      "Little o in probability, written oP(Yn).\n",
      "Xn ∈oP(Yn) iﬀ∥Xn∥/∥Yn∥\n",
      "p→0.\n",
      "If Xn ∈oP(1), then Xn converges in probability to 0, and conversely.\n",
      "If Xn ∈oP(1), then also Xn ∈OP(1). (Exercise.)\n",
      "Instead of a deﬁning sequence {Yn} of random variables, the sequence of\n",
      "interest may be a sequence of constants {an}.\n",
      "Some useful properties are the following, in which {Xn}, {Yn}, and {Zn}\n",
      "are random variables deﬁned on a common probability space, and {an} and\n",
      "{bn} are sequences of constants.\n",
      "Xn ∈op(an) =⇒Xn ∈Op(an)\n",
      "(1.165)\n",
      "Xn ∈op(1) ⇐⇒Xn →0.\n",
      "(1.166)\n",
      "Xn ∈Op(1/an),\n",
      "limbn/an < ∞=⇒Xn ∈Op(mn).\n",
      "(1.167)\n",
      "Xn ∈Op(an) =⇒XnYn ∈Op(anYn).\n",
      "(1.168)\n",
      "Xn ∈Op(an), Yn ∈Op(bn) =⇒XnYn ∈Op(anbn).\n",
      "(1.169)\n",
      "Xn ∈Op(an), Yn ∈Op(bn) =⇒Xn + Yn ∈Op(∥an∥+ ∥bn∥).\n",
      "(1.170)\n",
      "Xn ∈Op(Zn), Yn ∈Op(Zn) =⇒Xn + Yn ∈Op(Zn).\n",
      "(1.171)\n",
      "Xn ∈op(an), Yn ∈op(bn) =⇒XnYn ∈op(anbn).\n",
      "(1.172)\n",
      "Xn ∈op(an), Yn ∈op(bn) =⇒Xn + Yn ∈op(∥an∥+ ∥bn∥).\n",
      "(1.173)\n",
      "Xn ∈op(an), Yn ∈Op(bn) =⇒XnYn ∈op(anbn).\n",
      "(1.174)\n",
      "You are asked to prove these statements in Exercise 1.61. There are, of course,\n",
      "other variations on these relationships. The order of convergence of sequence\n",
      "of absolute expectations can be related to order of convergence in probability:\n",
      "an ∈IR+, E(|Xn|) ∈O(an) =⇒Xn ∈Op(an).\n",
      "(1.175)\n",
      "Almost sure convergence implies that the sup is bounded in probability. For\n",
      "any random variable X (recall that a random variable is ﬁnite a.s.),\n",
      "Xn\n",
      "a.s.\n",
      "→X =⇒sup |Xn| ∈Op(1).\n",
      "(1.176)\n",
      "You are asked to prove these statements in Exercise 1.62.\n",
      "The deﬁning sequence of interest is often an expression in n; for examples,\n",
      "OP(n−1), OP(n−1/2), and so on. For such orders of convergence, we have\n",
      "relationships similar to those given in statement (0.0.59) for nonstochastic\n",
      "convergence.\n",
      "OP(n−1) ⊆OP(n−1/2)\n",
      "etc.\n",
      "(1.177)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "85\n",
      "Example 1.23 order in probability of the sample mean\n",
      "Suppose Xn is the sample mean (equation (1.32)) from a random sample\n",
      "X1, . . ., Xn from a distribution with ﬁnite mean, µ, and ﬁnite variance, σ2.\n",
      "We ﬁrst note that E(Xn) = µ and V(Xn) = σ2/n. By Chebyshev’s inequality\n",
      "(page 848), we have for ϵ > 0,\n",
      "Pr(|Xn −µ| ≥ϵ) ≤σ2/n\n",
      "ϵ2\n",
      ",\n",
      "which goes to 0 as n →∞. Hence, Xn\n",
      "p→µ or Xn −µ ∈oP(1).\n",
      "Now, rewriting the inequality above, we have, for δ(ϵ) > 0,\n",
      "Pr(√n|Xn −µ| ≥δ(ϵ)) ≤\n",
      "σ2/n\n",
      "δ(ϵ)2/n.\n",
      "Now letting δ(ϵ) = σ/√ϵ, we have Xn −µ ∈OP(n−1/2).\n",
      "1.3.4 Weak Convergence in Distribution\n",
      "Convergence in distribution, sometimes just called “weak convergence”, plays\n",
      "a fundamental role in statistical inference. It is the type of convergence in\n",
      "the central limits (see Section 1.4.2) and it is the basis for the deﬁnition of\n",
      "asymptotic expectation (see Section 1.3.8), which, in turn is the basis for most\n",
      "of the concepts of asymptotic inference. (Asymptotic inference is not based on\n",
      "the limits of the properties of the statistics in a sequence, and in Section 3.8.3,\n",
      "beginning on page 311, we will consider some diﬀerences between “aysmptotic”\n",
      "properties and “limiting” properties.)\n",
      "In studying the properties of a sequence of random variables {Xn}, the holy\n",
      "grail often is to establish that anXn →N(µ, σ2) for some sequence {an}, and\n",
      "to determine reasonable estimates of µ and σ2. In this section we will show how\n",
      "this is sometimes possible, and we will consider it further in Section 1.3.7, and\n",
      "later in Section 3.8, where we will emphasize the statistical applications. Weak\n",
      "convergence to normality under less rigorous assumptions will be discussed in\n",
      "Section 1.4.\n",
      "Convergence in distribution of a sequence of random variables is deﬁned in\n",
      "terms of convergence of a sequence of CDFs. For a sequence that converges to\n",
      "a continuous CDF F , the Chebyshev norm of the diﬀerence between a function\n",
      "in the sequence and F goes to zero, as stated in the following theorem.\n",
      "Theorem 1.33 (Polya’s theorem)\n",
      "If Fn\n",
      "w→F and F is continuous in IRk, then\n",
      "lim\n",
      "n→∞sup\n",
      "t∈IRk |Fn(t) −F (t)| = 0.\n",
      "Proof. The proof proceeds directly by use of the δ-ϵ deﬁnition of continuity.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "86\n",
      "1 Probability Theory\n",
      "Theorem 1.34\n",
      "Let {Fn} be a sequence of CDFs on IR. Let\n",
      "Gn(x) = Fn(bgnx + agn)\n",
      "and\n",
      "Hn(x) = Fn(bhnx + ahn),\n",
      "where {bdn} and {bhn} are sequences of positive real numbers and {agn} and\n",
      "{ahn} are sequences of real numbers. Suppose\n",
      "Gn\n",
      "w→G\n",
      "and\n",
      "Hn\n",
      "w→H,\n",
      "where G and H are nondegenerate CDFs. Then\n",
      "bgn/bhn →b > 0,\n",
      "(agn −ahn)/bgn →a ∈IR,\n",
      "and\n",
      "H(bx + a) = G(x)\n",
      "∀x ∈IR.\n",
      "Proof. ** ﬁx\n",
      "The distributions in Theorem 1.34 are in a location-scale family (see Sec-\n",
      "tion 2.6, beginning on page 178).\n",
      "There are several necessary and suﬃcient conditions for convergence in\n",
      "distribution. A set of such conditions is given in the following “portmanteau”\n",
      "theorem.\n",
      "Theorem 1.35 (characterizations of convergence in distribution;\n",
      "“portmanteau” theorem)\n",
      "Given the sequence of random variables Xn and the random variable X, all de-\n",
      "ﬁned on a common probability space, then each of the following is a necessary\n",
      "and suﬃcient condition that Xn\n",
      "d→X.\n",
      "(i) E(g(Xn)) →E(g(X)) for all real bounded continuous functions g.\n",
      "(ii) E(g(Xn)) →E(g(X)) for all real functions g such that g(x) →0 as |x| →\n",
      "∞.\n",
      "(iii) Pr(Xn ∈B) →Pr(X ∈B) for all Borel sets B such that Pr(X ∈∂B) = 0.\n",
      "(iv) lim inf Pr(Xn ∈S) ≥Pr(X ∈S) for all open sets S.\n",
      "(v) lim supPr(Xn ∈T) ≤Pr(X ∈T) for all closed sets T.\n",
      "Proof. The proofs of the various parts of this theorem are in Billingsley\n",
      "(1995), among other resources.\n",
      "Although convergence in distribution does not imply a.s. convergence, con-\n",
      "vergence in distribution does allow us to construct an a.s. convergent sequence.\n",
      "This is stated in Skorokhod’s representation theorem.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "87\n",
      "Theorem 1.36 (Skorokhod’s representation theorem)\n",
      "If for the random variables (vectors!) X1, X2, . . ., we have Xn\n",
      "d→X, then\n",
      "there exist random variables Y1\n",
      "d= X1, Y2\n",
      "d= X2, . . ., and Y\n",
      "d= X, such that\n",
      "Yn\n",
      "a.s.\n",
      "→Y .\n",
      "Proof. Exercise.\n",
      "Theorem 1.37 (continuity theorem)\n",
      "Let X1, X2, · · · be a sequence of random variables (not necessarily indepen-\n",
      "dent) with characteristic functions ϕX1, ϕX2, · · · and let X be a random vari-\n",
      "able with characteristic function ϕX. Then\n",
      "Xn\n",
      "d→X\n",
      "⇐⇒\n",
      "ϕXn(t) →ϕX(t) ∀t.\n",
      "Proof. Exercise.\n",
      "The ⇐= part of the continuity theorem is called the L´evy-Cram´er theorem\n",
      "and the =⇒part is sometimes called the ﬁrst limit theorem.\n",
      "The continuity theorem also applies to MGFs if they exist for all Xn.\n",
      "A nice use of the continuity theorem is in the proof of a simple form\n",
      "of the central limit theorem, or CLT. Here I will give the proof for scalar\n",
      "random variables. There are other forms of the CLT, and other important\n",
      "limit theorems, which will be the topic of Section 1.4. Another reason for\n",
      "introducing this simple CLT now is so we can use it for some other results\n",
      "that we discuss before Section 1.4.\n",
      "Theorem 1.38 (central limit theorem)\n",
      "If X1, . . ., Xn are iid with mean µ and variance 0 < σ2 < ∞, then Yn =\n",
      "(PXi −nµ)/√nσ has limiting distribution N(0, 1).\n",
      "Proof. It will be convenient to deﬁne a function related to the CF: let h(t) =\n",
      "eµtϕX(t); hence h(0) = 1, h′(0) = 0, and h′′(0) = σ2. Now expand h in a\n",
      "Taylor series about 0:\n",
      "h(t) = h(0) + h′(0)it −1\n",
      "2h′′(ξ)t2,\n",
      "for some ξ between 0 and t. Substituting for h(0) and h′(0), and adding and\n",
      "subtracting σ2t/2 to this, we have\n",
      "h(t) = 1 −σ2t2\n",
      "2\n",
      "−(h′′(ξ) −σ2)t2\n",
      "2\n",
      ".\n",
      "This is the form we will ﬁnd useful. Now, consider the CF of Yn:\n",
      "ϕYn(t) = E\n",
      "\u0012\n",
      "exp\n",
      "\u0012\n",
      "it\n",
      "\u0012P Xi −nµ)\n",
      "√nσ\n",
      "\u0013\u0013\u0013\n",
      "=\n",
      "\u0012\n",
      "E\n",
      "\u0012\n",
      "exp\n",
      "\u0012\n",
      "it\n",
      "\u0012X −µ)\n",
      "√nσ\n",
      "\u0013\u0013\u0013\u0013n\n",
      "=\n",
      "\u0012\n",
      "h\n",
      "\u0012\n",
      "it\n",
      "√nσ\n",
      "\u0013\u0013n\n",
      ".\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "88\n",
      "1 Probability Theory\n",
      "From the expansion of h, we have\n",
      "h\n",
      "\u0012\n",
      "it\n",
      "√nσ\n",
      "\u0013\n",
      "= 1 −t2\n",
      "2n −(h′′(ξ) −σ2)t2\n",
      "2nσ2\n",
      ".\n",
      "So,\n",
      "ϕYn(t) =\n",
      "\u0012\n",
      "1 −t2\n",
      "2n −(h′′(ξ) −σ2)t2\n",
      "2nσ2\n",
      "\u0013n\n",
      ".\n",
      "Now we need a well-known (but maybe forgotten) result (see page 652): If\n",
      "limn→∞f(n) = 0, then\n",
      "lim\n",
      "n→∞\n",
      "\u0012\n",
      "1 + a\n",
      "n + f(n)\n",
      "n\n",
      "\u0013b\n",
      "n = eab.\n",
      "Therefore, because limn→∞h′′(ξ) = h′′(0) = σ2, limn→∞ϕYn(t) = e−t2/2,\n",
      "which is the CF of the N(0, 1) distribution. (Actually, the conclusion relies\n",
      "on the L´evy-Cram´er theorem, the ⇐= part of the continuity theorem, The-\n",
      "orem 1.37 on page 87; that is, while we know that the CF determines the\n",
      "distribution, we must also know that the convergent of a sequence of CFs\n",
      "determines a convergent distribution.)\n",
      "An important CLT has a weaker hypothesis than the simple one above;\n",
      "instead of iid random variables, we only require that they be independent (and\n",
      "have ﬁnite ﬁrst and second moments, of course). In Section 1.6, we relax the\n",
      "hypothesis in the other direction; that is, we allow dependence in the random\n",
      "variables. (In that case, we must impose some conditions of similarity of the\n",
      "distributions of the random variables.)\n",
      "Tightness of Sequences\n",
      "In a convergent sequence of probability measures on a metric space, we may\n",
      "be interested in how concentrated the measures in the sequence are. (If the\n",
      "space does not have a metric, this question would not make sense.) We refer\n",
      "to this as “tightness” of the sequence, and we will deﬁne it only on the metric\n",
      "space IRd.\n",
      "Deﬁnition 1.41 (tightness of a sequence of probability measures)\n",
      "Let {Pn} be a sequence of probability measures on (IRd, Bd). The sequence is\n",
      "said to be tight iﬀfor every ϵ > 0, there is a compact (bounded and closed)\n",
      "set C ∈Bd such that\n",
      "inf\n",
      "n Pn(C) > 1 −ϵ.\n",
      "Notice that this deﬁnition does not require that {Pn} be convergent, but of\n",
      "course, we are interested primarily in sequences that converge. The following\n",
      "theorem, whose proof can be found in Billingsley (1995) on page 336, among\n",
      "other places, connects tightness to convergence.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "89\n",
      "Theorem 1.39\n",
      "Let {Pn} be a sequence of probability measures on (IRd, Bd).\n",
      "(i) The sequence {Pn} is tight iﬀfor every subsequence {Pni} there exists a\n",
      "further subsequence {Pnj} ⊆{Pni} and a probability measure P on (IRd, Bd)\n",
      "such that\n",
      "Pnj\n",
      "w→P, as j →∞.\n",
      "(ii) If {Pn} is tight and each weakly convergent subsequence converges to the\n",
      "same measure P , then Pn\n",
      "w→P .\n",
      "Tightness of a sequence of random variables is deﬁned in terms of tightness\n",
      "of their associated probability measures.\n",
      "Deﬁnition 1.42 (tightness of a sequence of random variables)\n",
      "Let {Xn} be a sequence of random variables, with associated probability mea-\n",
      "sures {Pn}. The sequence {Xn} is said to be tight iﬀ\n",
      "∀ϵ > 0 ∃M < ∞∋sup\n",
      "n Pn(|Xn| > M) < ϵ.\n",
      "1.3.5 Expectations of Sequences; Sequences of Expectations\n",
      "The monotonicity of the expectation operator (1.42) of course carries over to\n",
      "sequences.\n",
      "The three theorems that relate to the interchange of a Lebesgue integration\n",
      "operation and a limit operation stated on page 733 (monotone convergence,\n",
      "Fatou’s lemma, and Lebesgue’s dominated convergence) apply immediately\n",
      "to expectations:\n",
      "•\n",
      "monotone convergence\n",
      "For 0 ≤X1 ≤X2 · · · a.s.\n",
      "Xn\n",
      "a.s.\n",
      "→X\n",
      "⇒\n",
      "E(Xn) →E(X)\n",
      "(1.178)\n",
      "•\n",
      "Fatou’s lemma\n",
      "0 ≤Xn a.s. ∀n\n",
      "⇒\n",
      "E(lim\n",
      "n inf Xn) ≤lim\n",
      "n inf E(Xn)\n",
      "(1.179)\n",
      "•\n",
      "dominated convergence\n",
      "Given a ﬁxed Y with E(Y ) < ∞,\n",
      "|Xn| ≤Y ∀n and Xn\n",
      "a.s.\n",
      "→X\n",
      "⇒\n",
      "E(Xn) →E(X).\n",
      "(1.180)\n",
      "These results require a.s. properties. Skorokhod’s Theorem 1.36, however,\n",
      "often allows us to extend results based on a.s. convergence to sequences that\n",
      "converge in distribution. Skorokhod’s theorem is the main tool used in the\n",
      "proofs of the following theorems, which we state without proof.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "90\n",
      "1 Probability Theory\n",
      "Theorem 1.40\n",
      "If for the random variables X1, X2, . . ., we have Xn\n",
      "d→X and for each k > 0\n",
      "E(|Xn|k) < ∞and E(|X|k) < ∞, then\n",
      "E(|Xn|k) →E(|X|k).\n",
      "(1.181)\n",
      "With additional conditions, we have a useful converse. It requires a limiting\n",
      "distribution that is not moment-indeterminant (see page 33). In that case, the\n",
      "converse says that the moments determine the limiting distribution.\n",
      "Theorem 1.41 Let X be a random variable that does not have a moment-\n",
      "indeterminant distribution, and let X1, X2, . . . be random variables. If for each\n",
      "k > 0 E(|Xn|k) < ∞and E(|X|k) < ∞, and if E(|Xn|k) →E(|X|k), then\n",
      "Xn\n",
      "d→X.\n",
      "Another useful convergence result for expectations is the Helly-Bray the-\n",
      "orem (or just the Helly theorem):\n",
      "Theorem 1.42 (Helly-Bray theorem)\n",
      "If g is a bounded and continuous Borel function over the support of {Xn},\n",
      "then\n",
      "Xn\n",
      "d→X ⇔E(g(Xn)) →E(g(X)).\n",
      "(1.182)\n",
      "With additional conditions there is also a converse of Theorem 1.42.\n",
      "The properties we have considered so far are all “nice”, “positive” results.\n",
      "We now consider an unhappy fact: in general,\n",
      "lim\n",
      "n→∞E(Xn) ̸= E( lim\n",
      "n→∞Xn),\n",
      "(1.183)\n",
      "as we see in the following example.\n",
      "Example 1.24 gambler’s ruin\n",
      "Let Y1, Y2, . . . be a sequence of iid random variables with\n",
      "Pr(Yi = 0) = Pr(Yi = 2) = 1\n",
      "2\n",
      "∀i = 1, 2, . . ..\n",
      "Now, let\n",
      "Xn =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "Yi.\n",
      "(1.184)\n",
      "It is intuitive that some Yk will eventually be 0, and in that case Xn = 0 for\n",
      "any n ≥k.\n",
      "*** ﬁnish: show that E(Xn) = 1 and limn→∞Xn = 0 a.s.; hence,\n",
      "limn→∞E(Xn) = 1 and E(limn→∞Xn) = 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "91\n",
      "1.3.6 Convergence of Functions\n",
      "In working with sequences of random variables we often encounter a situation\n",
      "in which members of the sequence may be represented as sums or products\n",
      "of elements one of which converges to a constant. Slutsky’s theorem provides\n",
      "very useful results concerning the convergence of such sequences.\n",
      "Theorem 1.43 (Slutsky’s theorem)\n",
      "Let X, {Xn}, {Bn}, and {Cn} be random variables on a common probability\n",
      "space, and let b, c ∈IRk. Suppose\n",
      "Xn\n",
      "d→X\n",
      "and\n",
      "Bn\n",
      "p→b\n",
      "and\n",
      "Cn\n",
      "p→c.\n",
      "Then\n",
      "BT\n",
      "n Xn + Cn\n",
      "d→bTX + c\n",
      "(1.185)\n",
      "Proof. Exercise.\n",
      "Slutsky’s theorem is one of the most useful results for showing conver-\n",
      "gence in distribution, and you should quickly recognize some special cases of\n",
      "Slutsky’s theorem. If Xn\n",
      "d→X, and Yn\n",
      "p→c, then\n",
      "Xn + Yn\n",
      "d→X + c\n",
      "(1.186)\n",
      "Y T\n",
      "n Xn\n",
      "d→cTX\n",
      "(1.187)\n",
      "and, if Yn ∈IR (that is, Yn is a scalar), then\n",
      "Xn/Yn\n",
      "d→X/c\n",
      "if\n",
      "c ̸= 0.\n",
      "(1.188)\n",
      "More General Functions\n",
      "The next issue has to do with functions of convergent sequences. We consider\n",
      "a sequence X1, X2, . . . in IRk. The ﬁrst function we consider is a simple linear\n",
      "projection, tTXn for t ∈IRk.\n",
      "Theorem 1.44 (Cram´er-Wold “device”)\n",
      "Let X1, X2, · · · be a sequence of random variables in IRk and let X be a random\n",
      "variable in IRk.\n",
      "Xn\n",
      "d→X\n",
      "⇐⇒\n",
      "tTXn\n",
      "d→tTX ∀t ∈IRk.\n",
      "Proof. Follows from the continuity theorem and equation (1.132).\n",
      "Now consider a general function g from (IRk, Bk) to (IRm, Bm). Given\n",
      "convergence of {Xn}, we consider the convergence of {g(Xn)}. Given the\n",
      "limiting distribution of a sequence {Xn}, the convergence of {g(Xn)} for a\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "92\n",
      "1 Probability Theory\n",
      "general function g is not assured. In the following we will consider the se-\n",
      "quence {g(Xn)} for the case that g is a continuous Borel function. (To speak\n",
      "about continuity of a function of random variables, we must add some kind of\n",
      "qualiﬁer, such as a.s., which, of course, assumes a probability measure.) The\n",
      "simple facts are given in Theorem 1.45.\n",
      "Theorem 1.45\n",
      "Let X and {Xn} be random variables (k-vectors) and let g be a continuous\n",
      "Borel function from IRk to IRk.\n",
      "Xn\n",
      "a.s.\n",
      "→X ⇒g(Xn) a.s.\n",
      "→g(X)\n",
      "(1.189)\n",
      "Xn\n",
      "p→X ⇒g(Xn)\n",
      "p→g(X)\n",
      "(1.190)\n",
      "Xn\n",
      "d→X ⇒g(Xn)\n",
      "d→g(X)\n",
      "(1.191)\n",
      "Proof. Exercise.\n",
      "Theorem 1.45 together with Slutsky’s theorem provide conditions under\n",
      "which we may say that g(Xn, Yn) converges to g(X, c).\n",
      "In the following we will consider normalizing constants that may make\n",
      "a sequence more useful. We may wish consider, for example, the asymptotic\n",
      "variance of a sequence whose limiting variance is zero.\n",
      "1.3.7 Asymptotic Distributions\n",
      "We will now resume the consideration of weak convergence of distributions\n",
      "that we began in Section 1.3.4. Asymptotic distributions are the basis for the\n",
      "concept of asymptotic expectation, discussed in Section 1.3.8 below.\n",
      "In many interesting cases, the limiting distribution of a sequence {Xn}\n",
      "is degenerate. The fact that {Xn} converges in probability to some given\n",
      "constant may be of interest, but in statistical applications, we are likely to\n",
      "be interested in how fast it converges, and what are the characteristics the\n",
      "sequence of the probability distribution that can be used for “large samples”.\n",
      "In this section we discuss how to modify a sequence so that the convergence\n",
      "is not degenerate. Statistical applications are discussed in Section 3.8.\n",
      "Normalizing Constants\n",
      "Three common types of sequences {Xn} of interest are iid sequences, sequences\n",
      "of partial sums, and sequences of order statistics. Rather than focusing on the\n",
      "sequence {Xn}, it may be more useful to to consider a sequence of linear\n",
      "transformations of Xn, {Xn −bn}, where the form of bn is generally diﬀerent\n",
      "for iid sequences, sequences of partial sums, and sequences of order statistics.\n",
      "Given a sequence of constants bn, if Xn −bn\n",
      "d→0, we may be interested\n",
      "in the rate of convergence, or other properties of the sequence as n becomes\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "93\n",
      "large. It may be useful to magnify the diﬀerence Xn −bn by use of some\n",
      "normalizing sequence of constants an:\n",
      "Yn = an(Xn −bn).\n",
      "(1.192)\n",
      "While the distribution of the sequence {Xn −bn} may be degenerate, the\n",
      "sequence {an(Xn −bn)} may have a distribution that is nondegenerate, and\n",
      "this asymptotic distribution may be useful in statistical inference. (This ap-\n",
      "proach is called “asymptotic inference”.) We may note that even though we\n",
      "are using the asymptotic distribution of {an(Xn−bn)}, for a reasonable choice\n",
      "of a sequence of normalizing constants {an}, we sometimes refer to it as the\n",
      "asymptotic distribution of {Xn} itself, but we must remember that it is the\n",
      "distribution of the normalized sequence, {an(Xn −bn)}.\n",
      "The shift constants generally serve to center the distribution, especially\n",
      "if the limiting distribution is symmetric. Although linear transformations are\n",
      "often most useful, we could consider sequences of more general transforma-\n",
      "tions of Xn; instead of {an(Xn −bn)}, we might consider {hn(Xn)}, for some\n",
      "sequence of functions {hn}.\n",
      "The Asymptotic Distribution of {g(Xn)}\n",
      "Applications often involve a diﬀerentiable Borel scalar function g, and we may\n",
      "be interested in the convergence of {g(Xn)}. (The same general ideas apply\n",
      "when g is a vector function, but the higher-order derivatives quickly become\n",
      "almost unmanageable.) When we have {Xn} converging in distribution to\n",
      "X + b, what we can say about the convergence of {g(Xn)} depends on the\n",
      "diﬀerentiability of g at b.\n",
      "Theorem 1.46\n",
      "Let X and {Xn} be random variables (k-vectors) such that\n",
      "an(Xn −bn)\n",
      "d→X,\n",
      "(1.193)\n",
      "where b1, b2, . . . is a sequence of constants such that limn→∞bn = b < ∞, and\n",
      "a1, a2, . . . is a sequence of constant scalars such that limn→∞an = ∞or such\n",
      "that limn→∞an = a > 0. Now let g be a Borel function from IRk to IR that is\n",
      "continuously diﬀerentiable at each bn. Then\n",
      "an(g(Xn) −g(bn)) d→(∇g(b))TX.\n",
      "(1.194)\n",
      "Proof. This follows from a Taylor series expansion of g(Xn) and Slutsky’s\n",
      "theorem.\n",
      "A common application of Theorem 1.46 arises from the simple corollary for\n",
      "the case when X in expression (1.193) has the multivariate normal distribution\n",
      "Nk(0, Σ) and ∇g(b) ̸= 0:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "94\n",
      "1 Probability Theory\n",
      "an(g(Xn) −g(bn))\n",
      "d→Y,\n",
      "(1.195)\n",
      "where Y ∼Nk(0, (∇g(b))TΣ∇g(b)).\n",
      "One reason limit theorems such as Theorem 1.46 are important is that they\n",
      "can provide approximations useful in statistical inference. For example, we\n",
      "often get the convergence of expression (1.193) from the central limit theorem,\n",
      "and then the convergence of the sequence {g(Xn)} provides a method for\n",
      "determining approximate conﬁdence sets using the normal distribution, so\n",
      "long as ∇g(b) ̸= 0. This method in asymptotic inference is called the delta\n",
      "method, and is illustrated in Example 1.25 below. It is particularly applicable\n",
      "when the asymptotic distribution is normal.\n",
      "The Case of ∇g(b) = 0\n",
      "Suppose ∇g(b) = 0 in equation (1.194). In this case the convergence in distri-\n",
      "bution is to a degenerate random variable, which may not be very useful. If,\n",
      "however, Hg(b) ̸= 0 (where Hg is the Hessian of g), then we can use a second\n",
      "order the Taylor series expansion and get something useful:\n",
      "2a2\n",
      "n(g(Xn) −g(bn))\n",
      "d→XTHg(b)X,\n",
      "(1.196)\n",
      "where we are using the notation and assuming the conditions of Theorem 1.46.\n",
      "Note that while an(g(Xn)−g(bn)) may have a degenerate limiting distribution\n",
      "at 0, a2\n",
      "n(g(Xn)−g(bn)) may have a nondegenerate distribution. (Recalling that\n",
      "limn→∞an = ∞, we see that this is plausible.) Equation (1.196) allows us also\n",
      "to get the asymptotic covariance for the pairs of individual elements of Xn.\n",
      "Use of expression (1.196) is called a second order delta method, and is\n",
      "illustrated in Example 1.25.\n",
      "Example 1.25 an asymptotic distribution in a Bernoulli family\n",
      "Consider the Bernoulli family of distributions with parameter π. The variance\n",
      "of a random variable distributed as Bernoulli(π) is g(π) = π(1 −π). Now,\n",
      "suppose X1, X2, . . .\n",
      "iid\n",
      "∼Bernoulli(π). Since E(Xn) = π, we may be interested\n",
      "in the distribution of Tn = g(Xn) = Xn(1 −Xn).\n",
      "From the central limit theorem (Theorem 1.38),\n",
      "√n(Xn −π) →N(0, π(1 −π)),\n",
      "(1.197)\n",
      "and so if π ̸= 1/2, g′(π) ̸= 0, we can use the delta method from expres-\n",
      "sion (1.194) to get\n",
      "√n(Tn −g(π)) →N(0, π(1 −π)(1 −2π)2).\n",
      "(1.198)\n",
      "If π = 1/2, g′(π) = 0 and this is a degenerate distribution, so we cannot\n",
      "use the delta method. Let’s use expression (1.196). The Hessian is particularly\n",
      "simple.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "95\n",
      "First, we note that in this case, the CLT yields √n(X −1/2) →N \u00000, 1\n",
      "4\n",
      "\u0001.\n",
      "Hence, if we scale and square, we get 4n(X −1\n",
      "2)2\n",
      "d→χ2\n",
      "1, or\n",
      "4n(Tn −g(π))\n",
      "d→χ2\n",
      "1.\n",
      "We can summarize the previous discussion and the special results of Ex-\n",
      "ample 1.25 as follows (assuming all of the conditions on the objects involved),\n",
      "√n(Tn −bn) →N(0, σ2)\n",
      "g′(b) = 0\n",
      "g′′(b) ̸= 0\n",
      "\n",
      "\n",
      "=⇒2n(g(Tn) −g(bn))2\n",
      "σ2g′′(b)\n",
      "d→χ2\n",
      "1.\n",
      "(1.199)\n",
      "Higher Order Expansions\n",
      "Suppose the second derivatives of g(b) are zero. We can easily extend this to\n",
      "higher order Taylor expansions in Theorem 1.47 below. (Note that because\n",
      "higher order Taylor expansions of vector expressions can become quite messy,\n",
      "in Theorem 1.47 we use Y = (Y1, . . ., Yk) in place of X as the limiting random\n",
      "variable.)\n",
      "Theorem 1.47\n",
      "Let Y and {Xn} be random variables (k-vectors) such that\n",
      "an(Xn −bn)\n",
      "d→Y,\n",
      "where bn is a constant sequence and a1, a2, . . . is a sequence of constant scalars\n",
      "such that limn→∞an = ∞. Now let g be a Borel function from IRk to IR whose\n",
      "mth order partial derivatives exist and are continuous in a neighborhood of bn,\n",
      "and whose jth, for 1 ≤j ≤m −1, order partial derivatives vanish at b. Then\n",
      "m!am\n",
      "n (g(Xn) −g(bn))\n",
      "d→\n",
      "k\n",
      "X\n",
      "i1=1\n",
      "· · ·\n",
      "k\n",
      "X\n",
      "im=1\n",
      "∂mg\n",
      "∂xi1 · · ·∂xim\n",
      "\f\f\f\f\f\n",
      "x=b\n",
      "Yi1 · · ·Yim.\n",
      "(1.200)\n",
      "Expansion of Statistical Functions\n",
      "*** refer to functional derivatives, Sections 0.1.13 and 0.1.13.\n",
      "Variance Stabilizing Transformations\n",
      "The fact that the variance in the asymptotic distribution in expression (1.198)\n",
      "depends on π may complicate our study of Tn and its relationship to π. Of\n",
      "course, this dependence results initially from the variance π(1 −π) in the\n",
      "asymptotic distribution in expression (1.197). If g(π) were chosen so that\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "96\n",
      "1 Probability Theory\n",
      "(g(π)′)2 = (π(1−π)−1), the variance in an expression similar to (1.198) would\n",
      "be constant (in fact, it would be 1).\n",
      "Instead of g(π) = π(1 −π) as in Example 1.25, we can use a solution to\n",
      "the diﬀerential equation\n",
      "g′(π) = π(1 −π)−1/2.\n",
      "One solution is g(t) = 2 arcsin(\n",
      "√\n",
      "t), and following the same procedure\n",
      "in Example 1.25 but using this function for the transformations, we have\n",
      "2√n\n",
      "\u0000arcsin(√Xn) −arcsin(√π)\n",
      "\u0001 d→N(0, 1).\n",
      "A transformation such as this is called a variance stabilizing transformation\n",
      "for obvious reasons.\n",
      "Example 1.26 variance stabilizing transformation in a normal fam-\n",
      "ily\n",
      "Consider the normal family of distributions with known mean 0 and variance\n",
      "σ2, and suppose X1, X2, . . . iid\n",
      "∼N(0, σ2). Since E(X2\n",
      "i ) = σ2, we may be inter-\n",
      "ested in the distribution of Tn = P X2\n",
      "i /n. We note that V(X2\n",
      "i ) = 2σ4, hence,\n",
      "the central limit theorem gives\n",
      "√n\n",
      " \n",
      "1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "X2\n",
      "i −σ2\n",
      "!\n",
      "→N(0, 2σ4).\n",
      "Following the ideas above, we seek a transformation g(σ2) such that\n",
      "(g′(σ2))2σ4 is constant wrt σ2. A solution to the diﬀerential equation that\n",
      "expresses this relationship is g(t) = log(t), and as above, we have\n",
      "√n\n",
      " \n",
      "log\n",
      " \n",
      "1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "X2\n",
      "i\n",
      "!\n",
      "−log \u0000σ2\u0001\n",
      "!\n",
      "→N(0, 2).\n",
      "(1.201)\n",
      "Order Statistics and Quantiles\n",
      "The asymptotic distributions of order statistics Xkn:n are often of interest. The\n",
      "asymptotic properties of “central” order statistics are diﬀerent from those of\n",
      "“extreme” order statistics.\n",
      "A sequence of central order statistics {X(k:n)} is one such that for given π ∈\n",
      "]0, 1[, k/n →π as n →∞. (Notice that k depends on n, but we will generally\n",
      "not use the notation kn.) As we suggested on page 64, the expected value of the\n",
      "kth order statistic in a sample of size n, if it exists, should be approximately the\n",
      "same as the k/n quantile of the underlying distribution. Under mild regularity\n",
      "conditions, a sequence of asymptotic central order statistics can be shown to\n",
      "converge in expectation to xπ, the π quantile.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "97\n",
      "Sample quantiles (deﬁned on page 64) are the ordinary quantiles in the\n",
      "sense of equation (1.13) of the discrete distribution deﬁned by the sample,\n",
      "X1, . . ., Xn, which has CDF Fn(x), the ECDF, as deﬁned in equation (1.34);\n",
      "that is, the π sample quantile is\n",
      "xπ = F −1\n",
      "n (π).\n",
      "(1.202)\n",
      "Properties of quantiles, of course, are diﬀerent for discrete and continuous\n",
      "distributions. In the following, for 0 < π < 1 we will assume that F (xπ)\n",
      "is twice diﬀerentiable in some neighborhood of xπ and F ′′ is bounded and\n",
      "F ′(xπ) > 0 in that neighborhood. Denote F ′(x) as f(x), and let Fn(x) be the\n",
      "ECDF. Now, write the kth order statistic as\n",
      "X(k:n) = xπ −Fn(xπ) −π\n",
      "f(xπ)\n",
      "+ Rn(π).\n",
      "(1.203)\n",
      "This is called the Bahadur representation, after Bahadur (1966), who showed\n",
      "that Rn(π) →0 as n →∞. Kiefer (1967) determined the exact order of Rn(π),\n",
      "so equation (1.203) is sometimes called the Bahadur-Kiefer representation.\n",
      "The Bahadur representation is useful in studying asymptotic properties of\n",
      "central order statistics.\n",
      "There is some indeterminacy in relating order statistics to quantiles. In\n",
      "the Bahadur representation, for example, the details are slightly diﬀerent if\n",
      "nπ happens to be an integer. (The results are the same, however.) Consider\n",
      "a slightly diﬀerent formulation for a set of m order statistics. The following\n",
      "result is due to Ghosh (1971).\n",
      "Theorem 1.48\n",
      "Let X1, . . ., Xn be iid random variables with PDF f. For k = n1, . . ., nm ≤n,\n",
      "let λk ∈]0, 1[ be such that nk = ⌈nλk⌉+1. Now suppose 0 < λ1 < · · · < λm < 1\n",
      "and for each k, f(xλk) > 0. Then the asymptotic distribution of the random\n",
      "m-vector\n",
      "\u0010\n",
      "n1/2(X(n1:n) −xλ1), . . ., n1/2(X(nm:n) −xλm)\n",
      "\u0011\n",
      "is m-variate normal with mean of 0, and covariance matrix whose i, j element\n",
      "is\n",
      "\u0012 λi(1 −λj)\n",
      "f(xλi)f(xλj)\n",
      "\u0013\n",
      ".\n",
      "For a proof of this theorem, see David and Nagaraja (2003).\n",
      "A sequence of extreme order statistics {X(k:n)} is one such that k/n →0 or\n",
      "k/n →1 as n →∞. Sequences of extreme order statistics from a distribution\n",
      "with bounded support generally converge to a degenerate distribution, while\n",
      "those from a distribution with unbounded support do not have a meaningful\n",
      "distribution unless the sequence is normalized in some way. We will consider\n",
      "asymptotic distributions of extreme order statistics in Section 1.4.3.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "98\n",
      "1 Probability Theory\n",
      "We now consider some examples of sequences of order statistics. In Ex-\n",
      "amples 1.27 and 1.28 below, we obtain degenerate distributions unless we\n",
      "introduce a normalizing factor. In Example 1.29, it is necessary to introduce\n",
      "a sequence of constant shifts.\n",
      "Example 1.27 asymptotic distribution of min or max order statis-\n",
      "tics from U(0, 1)\n",
      "Suppose X1, . . ., Xn are iid U(0, 1). The CDFs of the min and max, X(1:n)\n",
      "and X(n:n), are easy to work out. For x ∈[0, 1],\n",
      "FX(1:n)(x) = 1 −Pr(X1 > x, . . ., Xn > x)\n",
      "= 1 −(1 −x)n\n",
      "and\n",
      "FX(n:n)(x) = xn.\n",
      "Notice that these are beta distributions, as we saw in Example 1.17.\n",
      "Both of these extreme order statistics have degenerate distributions. For\n",
      "X(1:n), we have X(1:n)\n",
      "d→0 and\n",
      "E\n",
      "\u0000X(1:n)\n",
      "\u0001\n",
      "=\n",
      "1\n",
      "n + 1\n",
      "and so\n",
      "lim\n",
      "n→∞E\n",
      "\u0000X(1:n)\n",
      "\u0001\n",
      "= 0.\n",
      "This suggests the normalization nX(1:n). We have\n",
      "Pr \u0000nX(1:n) ≤x\u0001 = 1 −\n",
      "\u0010\n",
      "1 −x\n",
      "n\n",
      "\u0011n\n",
      "→1 −e−x\n",
      "x > 0.\n",
      "(1.204)\n",
      "This is the CDF of a standard exponential distribution. The distribution of\n",
      "nX(1:n) is more interesting than that of X(1:n).\n",
      "For X(n:n), we have X(n:n)\n",
      "d→1 and\n",
      "E\n",
      "\u0000X(n:n)\n",
      "\u0001\n",
      "=\n",
      "n\n",
      "n + 1\n",
      "and so\n",
      "lim\n",
      "n→∞E\n",
      "\u0000X(n:n)\n",
      "\u0001\n",
      "= 1,\n",
      "and there is no normalization to yield a nondegenerate distribution.\n",
      "Now consider the asymptotic distribution of central order statistics from\n",
      "U(0, 1).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.3 Sequences of Events and of Random Variables\n",
      "99\n",
      "Example 1.28 asymptotic distribution of a central order statistic\n",
      "from U(0, 1)\n",
      "Let X(k:n) be the kth order statistic from a random sample of size n from\n",
      "U(0, 1) and let\n",
      "Y = nX(k:n).\n",
      "Using Equation (1.138) with the CDF of U(0, 1), we have\n",
      "fY (y) =\n",
      "\u0012n\n",
      "k\n",
      "\u0013\u0010y\n",
      "n\n",
      "\u0011k−1\u0010\n",
      "1 −y\n",
      "n\n",
      "\u0011n−k\n",
      "I[0,1](y).\n",
      "Observing that\n",
      "lim\n",
      "n→∞\n",
      "\u0012n\n",
      "k\n",
      "\u0013\n",
      "= nk−1\n",
      "k! ,\n",
      "we have for ﬁxed k,\n",
      "lim\n",
      "n→∞fY (y) =\n",
      "1\n",
      "Γ(k)yk−1e−yI[0,∞[(y),\n",
      "(1.205)\n",
      "that is, the limiting distribution of {nX(k:n)} is gamma with scale parameter\n",
      "1 and shape parameter k. (Note, of course, k! = kΓ(k).) For ﬁnite values\n",
      "of n, the asymptotic distribution provides better approximations when k/n\n",
      "is relatively small. When k is large, n must be much larger in order for the\n",
      "asymptotic distribution to approximate the true distribution closely.\n",
      "If k = 1, the PDF in equation (1.205) is the exponential distribution, as\n",
      "shown in Example 1.27. For k →n, however, we must apply a limit similar to\n",
      "what is done in equation (1.204).\n",
      "While the min and max of the uniform distribution considered in Exam-\n",
      "ple 1.27 are “extreme” values, the more interesting extremes are those from\n",
      "distributions with inﬁnite support. In the next example, we consider an ex-\n",
      "treme value that has no bound. In such a case, in addition to any normaliza-\n",
      "tion, we must do a shift.\n",
      "Example 1.29 extreme value distribution from an exponential dis-\n",
      "tribution\n",
      "Let X(n:n) be the largest order statistic from a random sample of size n from\n",
      "an exponential distribution with PDF e−xI¯IR+(x) and let\n",
      "Y = X(n:n) −log(n).\n",
      "We have\n",
      "lim\n",
      "n→∞Pr(Y ≤y) = e−e−y\n",
      "(1.206)\n",
      "(Exercise 1.65). The distribution with CDF given in equation (1.206) is called\n",
      "an extreme value distribution. There are two other classes of “extreme value\n",
      "distributions”, which we will discuss in Section 1.4.3. The one in this example,\n",
      "which is the most common one, is called a type 1 extreme value distribution\n",
      "or a Gumbel distribution.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "100\n",
      "1 Probability Theory\n",
      "1.3.8 Asymptotic Expectation\n",
      "The properties of the asymptotic distribution, such as its mean or variance,\n",
      "are the asymptotic values of the corresponding properties of Tn. Let {Tn}\n",
      "be a sequence of random variables with E(|Tn|) < ∞and Tn\n",
      "d→T, with\n",
      "E(|T|) < ∞. Theorem 1.40 (on page 90) tells us that\n",
      "E(|Tn|k) →E(|T|k).\n",
      "When Tn is a normalized statistic, such as X, with variance of the form\n",
      "σ2/n, the limiting value of some properties of Tn may not be very use-\n",
      "ful in statistical inference. We need an “asymptotic variance” diﬀerent from\n",
      "limn→∞σ2/n.\n",
      "Because {Tn} may converge to a degenerate random variable, it may be\n",
      "more useful to generalize the deﬁnition of asymptotic expectation slightly. We\n",
      "will deﬁne “an asymptotic expectation”, and distinguish it from the “limiting\n",
      "expectation”. We will consider a sequence of the form {anTn}.\n",
      "Deﬁnition 1.43 (asymptotic expectation)\n",
      "Let {Tn} be a sequence of random variables, and let {an} be a sequence of\n",
      "positive constants with limn→∞an = ∞or with limn→∞an = a > 0, and\n",
      "such that anTn\n",
      "d→T, with E(|T|) < ∞. An asymptotic expectation of {Tn} is\n",
      "E(T/an).\n",
      "Notice that an asymptotic expectation many include an n; that is, the order\n",
      "of an asymptotic expression may be expressed in the asymptotic expectation.\n",
      "For example, the asymptotic variance of a sequence of estimators √nTn(X)\n",
      "may be of the form V(T/n); that is, the order of the asymptotic variance is\n",
      "n−1.\n",
      "We refer to limn→∞E(Sn) as the limiting expectation. It is important to\n",
      "recognize the diﬀerence in limiting expectation and asymptotic expectation.\n",
      "The limiting variance of a sequence of estimators √nTn(X) may be 0, while\n",
      "the asymptotic variance is of the form V(T/n).\n",
      "Asymptotic expectation has a certain arbitrariness associated with the\n",
      "choice of {an}. The range of possibilities for “an” asymptotic expectation,\n",
      "however, is limited, as the following theorem shows.\n",
      "Theorem 1.49\n",
      "Let {Tn} be a sequence of random variables, and let {cn} be a sequence of\n",
      "positive constants with limn→∞cn = ∞or with limn→∞cn = c > 0, and such\n",
      "that cnTn\n",
      "d→R, with E(|R|) < ∞. Likewise, let {dn} be a sequence of positive\n",
      "constants with limn→∞dn = ∞or with limn→∞dn = d > 0, and such that\n",
      "dnTn\n",
      "d→S, with E(|S|) < ∞. (This means that both E(R/cn) and E(S/dn)\n",
      "are asymptotic expectations of Tn.) Then it must be the case that either\n",
      "(i) E(R) = E(S) = 0,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.4 Limit Theorems\n",
      "101\n",
      "(ii) either E(R) ̸= 0, E(S) = 0, and dn/cn →0 or E(R) = 0, E(S) ̸= 0, and\n",
      "cn/dn →0,\n",
      "or\n",
      "(iii) E(R) ̸= 0, E(S) ̸= 0, and E(R/cn)/E(S/dn) →1.\n",
      "Proof. Exercise. (Use Theorem 1.34 on page 86.)\n",
      "Multivariate Asymptotic Expectation\n",
      "The multivariate generalization of asymptotic expectation is straightforward:\n",
      "Let {Xn} be a sequence of random k-vectors, and let {An} be a sequence of\n",
      "k×k positive deﬁnite matrices such that either limn→∞An diverges (that is, in\n",
      "the limit has no negative diagonal elements and some diagonal elements that\n",
      "are positively inﬁnite) or else limn→∞An = A, where A is positive deﬁnite\n",
      "and such that AnXn\n",
      "d→X, with E(|X|) < ∞. Then an asymptotic expectation\n",
      "of {Xn} is E(A−1\n",
      "n X).\n",
      "If the asymptotic expectation of {Xn} is B(n)µ for some matrix B(n), and\n",
      "g is a Borel function from IRk to IRk that is diﬀerentiable at µ, then by Theo-\n",
      "rem 1.46 on page 93 the asymptotic expectation of {g(Xn)} is B(n)Jg(µ))Tµ.\n",
      "1.4 Limit Theorems\n",
      "We are interested in functions of a sequence of random variables {Xi | i =\n",
      "1, . . ., n}, as n increases without bound. The functions of interest involve either\n",
      "sums or extreme order statistics. There are three general types of important\n",
      "limit theorems: laws of large numbers, central limit theorems, and extreme\n",
      "value theorems.\n",
      "Laws of large numbers give limits for probabilities or for expectations of\n",
      "sequences of random variables. The convergence to the limits may be weak or\n",
      "strong.\n",
      "Historically, the ﬁrst versions of both laws of large numbers and central\n",
      "limit theorems applied to sequences of binomial random variables.\n",
      "Central limit theorems and extreme value theorems provide weak conver-\n",
      "gence results, but they do even more; they specify a limiting distribution.\n",
      "Central limit theorems specify a limiting inﬁnitely divisible distribution, often\n",
      "a normal distribution; and extreme value theorems specify a limiting extreme\n",
      "value distribution, one of which we encountered in Example 1.29.\n",
      "The functions of the sequence of interest are of the form\n",
      "an\n",
      " n\n",
      "X\n",
      "i=1\n",
      "Xi −bn\n",
      "!\n",
      "(1.207)\n",
      "or\n",
      "an\n",
      "\u0000X(n:n) −bn\n",
      "\u0001 ,\n",
      "(1.208)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "102\n",
      "1 Probability Theory\n",
      "where {an} is a sequence of positive real constants and {bn} is a sequence\n",
      "of real constants. The sequence of normalizing constants {an} for either case\n",
      "often have the form an = n−p for some ﬁxed p > 0.\n",
      "For both laws of large numbers and central limit theorems, we will be\n",
      "interested in a function of the form of expression (1.207), whereas for the\n",
      "extreme value theorems, we will be interested in a function of the form of\n",
      "expression (1.208). An extreme value theorem, of course, may involve X(1:n)\n",
      "instead of X(n:n). The simplest version of a central limit theorem applies to\n",
      "sequences of iid random variables with ﬁnite variance, as in Theorem 1.38.\n",
      "The simplest version of the extreme value theorem applies to sequences of\n",
      "exponential random variables, as in Example 1.29.\n",
      "For the laws of large numbers and the central limit theorems, we will ﬁnd\n",
      "it convenient to deﬁne\n",
      "Sn =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "Xi.\n",
      "(1.209)\n",
      "We distinguish diﬀerent types of sequences of random variables based on\n",
      "the distributions of the individual terms in the sequence and on the corre-\n",
      "lational structure of the terms. Most of the results discussed in Section 1.3\n",
      "did not place any restrictions on the distributions or on their correlational\n",
      "structure. The limit theorems often require identical distributions (or at least\n",
      "distributions within the same family and which vary in a systematic way).\n",
      "Even when diﬀerent distributions are allowed, the limit theorems that we dis-\n",
      "cuss in this section require that the terms in the sequence be independent.\n",
      "We will consider sequences of correlated random variables in Section 1.6.\n",
      "1.4.1 Laws of Large Numbers\n",
      "The ﬁrst law of large numbers was Bernoulli’s (Jakob Bernoulli’s) theorem. In\n",
      "this case Sn is the sum of n iid Bernoullis, so it has a binomial distribution.\n",
      "Theorem 1.50 (Bernoulli’s theorem (binomial random variables))\n",
      "If Sn has a binomial distribution with parameters n and π, then\n",
      "1\n",
      "nSn\n",
      "p→π.\n",
      "(1.210)\n",
      "Proof. This follows from\n",
      "R\n",
      "Ω(Sn/n −π)2dP = π(1 −π)/n, which means Sn/n\n",
      "converges in mean square to π, which in turn means that it converges in\n",
      "probability to π.\n",
      "This is a weak law because the convergence is in probability.\n",
      "Bernoulli’s theorem applies to binomial random variables. We now state\n",
      "without proof four theorems about large numbers. Proofs can be found in\n",
      "Petrov (1995), for example. The ﬁrst two apply to iid random numbers and\n",
      "the second two require only independence and ﬁnite expectations. Two are\n",
      "weak laws (WLLN) and two are strong (SLLN). For applications in statistics,\n",
      "the weak laws are generally more useful than the strong laws.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.4 Limit Theorems\n",
      "103\n",
      "A generalization of Bernoulli’s theorem is the weak law of large numbers\n",
      "(WLLN) for iid random variables:\n",
      "Theorem 1.51 (WLLN for iid random variables)\n",
      "Let X1, X2, . . . be a sequence of iid random variables (and Sn = Pn\n",
      "i=1 Xi).\n",
      "There exists a sequence of real numbers a1, a2, . . . such that ∀i\n",
      "nPr(|Xi| > n) →0\n",
      "⇐⇒\n",
      "1\n",
      "nSn −bn\n",
      "p→0\n",
      "(1.211)\n",
      "The bn can be chosen so that bn ≤n, as bn = E\n",
      "\u0000XiI{|Xi|≤n}\n",
      "\u0001\n",
      ", for example.\n",
      "Theorem 1.52 (SLLN for iid random variables)\n",
      "Let X1, X2, . . . be a sequence of iid random variables such that ∀i E(|Xi|) =\n",
      "µ < ∞. Then\n",
      "1\n",
      "nSn\n",
      "a.s.\n",
      "→µ.\n",
      "(1.212)\n",
      "A slight generalization is the alternate conclusion\n",
      "1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "ai(Xi −E(X1))\n",
      "a.s.\n",
      "→0,\n",
      "for any bounded sequence of real numbers a1, a2, . . ..\n",
      "We can generalize these two limit theorems to the case of independence\n",
      "but not necessarily identical distributions, by putting limits on normalized pth\n",
      "moments.\n",
      "Theorem 1.53 (WLLN for independent random variables)\n",
      "Let X1, X2, . . . be a sequence of independent random variables such for some\n",
      "constant p ∈[1, 2],\n",
      "lim\n",
      "n→∞\n",
      "1\n",
      "np\n",
      "n\n",
      "X\n",
      "i=1\n",
      "E(|Xi|p) = 0.\n",
      "Then\n",
      "1\n",
      "n\n",
      " \n",
      "Sn −\n",
      "n\n",
      "X\n",
      "i=1\n",
      "E(Xi)\n",
      "!\n",
      "p→0.\n",
      "(1.213)\n",
      "Theorem 1.54 (SLLN for independent random variables)\n",
      "Let X1, X2, . . . be a sequence of independent random variables such for some\n",
      "constant p ∈[1, 2],\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "E(|Xi|p)\n",
      "ip\n",
      "< ∞.\n",
      "Then\n",
      "1\n",
      "n\n",
      " \n",
      "Sn −\n",
      "n\n",
      "X\n",
      "i=1\n",
      "E(Xi)\n",
      "!\n",
      "a.s.\n",
      "→0.\n",
      "(1.214)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "104\n",
      "1 Probability Theory\n",
      "We notice that the normalizing term in all of the laws of large numbers\n",
      "has been n−1. We recall that the normalizing term in the simple central limit\n",
      "theorem 1.38 (and in the central limit theorems we will consider in the next\n",
      "section) is n−1/2. Since the central limit theorems give convergence to a non-\n",
      "degenerate distribution, when the normalizing factor is as small as n−1/2, we\n",
      "cannot expect convergence in probability, and so certainly not almost sure con-\n",
      "vergence. We might ask if there is some sequence an with n−1/2 < an < n−1,\n",
      "such that when an is used as a normalizing factor, we have convergence in\n",
      "probability and possibly almost sure convergence. The “Law of the Iterated\n",
      "Logarithm (LIL)” provides a very interesting answer for iid random variables\n",
      "with ﬁnite variance. The sequence involves the iterated logarithm, log(log(n));\n",
      "speciﬁcally, an = (n log(log(n)))−1/2.\n",
      "Without loss of generality we will assume the random variables have mean\n",
      "0.\n",
      "Theorem 1.55 (Law of the Iterated Logarithm)\n",
      "Let X1, X2, . . . be a sequence of independent and identically distributed random\n",
      "variables with E(Xi) = 0 and V(Xi) = σ2 < ∞. Then\n",
      "1\n",
      "σ\n",
      "√\n",
      "2\n",
      "1\n",
      "p\n",
      "n log(log(n))\n",
      "Sn\n",
      "p→0\n",
      "(1.215)\n",
      "and\n",
      "lim sup\n",
      "1\n",
      "σ\n",
      "√\n",
      "2\n",
      "1\n",
      "p\n",
      "n log(log(n))\n",
      "Sn\n",
      "a.s.\n",
      "= 1.\n",
      "(1.216)\n",
      "Proof. See Billingsley (1995).\n",
      "Further generalizations of laws of large numbers apply to sequences of\n",
      "random variables in triangular arrays, as in the deﬁnition of inﬁnite divisibility,\n",
      "Deﬁnition 1.32. We will use triangular arrays in an important central limit\n",
      "theorem in the next section.\n",
      "1.4.2 Central Limit Theorems for Independent Sequences\n",
      "Central limit theorems give conditions that imply that certain standardized\n",
      "sequences converge to a normal distribution. We will be interested in sequences\n",
      "of the form of equation (1.207):\n",
      "an\n",
      " n\n",
      "X\n",
      "i=1\n",
      "Xi −bn\n",
      "!\n",
      ".\n",
      "where {an} is a sequence of positive real constants and {bn} is a sequence of\n",
      "real constants.\n",
      "The simplest central limit theorems apply to iid random variables. More\n",
      "complicated ones apply to independent random variables that are not neces-\n",
      "sarily identically distributed and/or that are not necessarily independent. In\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.4 Limit Theorems\n",
      "105\n",
      "this section we consider central limit theorems for independent sequences. On\n",
      "page 134 we will consider a central limit theorem in which the sequences are\n",
      "not necessarily independent.\n",
      "The central limit theorems require ﬁnite second moments.\n",
      "The de Moivre Laplace Central Limit Theorem\n",
      "The ﬁrst central limit theorem, called the de Moivre Laplace central limit\n",
      "theorem followed soon after Bernoulli’s theorem, and like Bernoulli’s theorem,\n",
      "it applies to Sn that has a binomial distribution with parameters n and π\n",
      "(because it is the sum of n iid Bernoullis with parameter π.\n",
      "Theorem 1.56 (De Moivre Laplace Central Limit Theorem)\n",
      "If Sn has a binomial distribution with parameters n and π, then\n",
      "1\n",
      "p\n",
      "π(1 −π)\n",
      "1\n",
      "√n(Sn −nπ)\n",
      "d→N(0, 1).\n",
      "(1.217)\n",
      "This central limit theorem is a special case of the classical central limit\n",
      "theorem for iid random variables with ﬁnite mean and variance.\n",
      "Notice that Bernoulli’s theorem and the de Moivre Laplace central limit\n",
      "theorem, which are stated in terms of binomial random variables, apply to\n",
      "normalized limits of sums of Bernoulli random variables. This is the usual\n",
      "form of these kinds of limit theorems; that is, they apply to normalized lim-\n",
      "its of sums of random variables. The ﬁrst generalizations apply to sums of\n",
      "iid random variables, and then further generalizations apply to sums of just\n",
      "independent random variables.\n",
      "The Central Limit Theorem for iid Scalar Random Variables with\n",
      "Finite Mean and Variance\n",
      "Theorem 1.57\n",
      "Let X1, X2, . . . be a sequence of independent random variables that are iden-\n",
      "tically distributed with mean µ and variance σ2 > 0. Then\n",
      "1\n",
      "σ\n",
      "1\n",
      "√n\n",
      " n\n",
      "X\n",
      "i=1\n",
      "Xi −nµ\n",
      "!\n",
      "d→N(0, 1).\n",
      "(1.218)\n",
      "A proof of this uses a limit of a characteristic function and the uniqueness\n",
      "of the characteristic function (see page 87).\n",
      "Independent but Not Identical; Triangular Arrays\n",
      "The more general central limit theorems apply to a triangular array; that is,\n",
      "to a sequence of ﬁnite subsequences. The variances of the sums of the subse-\n",
      "quences is what is used to standardize the sequence so that it is convergent.\n",
      "We deﬁne the sequence and the subsequences as follows.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "106\n",
      "1 Probability Theory\n",
      "Let {Xnj, j = 1, 2, . . ., kn} be independent random variables with kn →∞\n",
      "as n →∞. We let\n",
      "Rn =\n",
      "kn\n",
      "X\n",
      "j=1\n",
      "Xnj\n",
      "represent “row sums”, as we visualize the sequence in an array:\n",
      "X11, X12, . . .X1k1\n",
      "R1\n",
      "X21, X22, . . .. . .X2k2\n",
      "R2\n",
      "X31, X32, . . .. . .. . .X3k3\n",
      "R3\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "(1.219)\n",
      "There is no requirement that kj > ki when j > i, but since kn →∞as n →∞,\n",
      "that is certainly the trend. Note that within a row of this triangular array, the\n",
      "elements are independent, but between rows, there is no such requirement.\n",
      "Let σ2\n",
      "n = V(Rn), and assume 0 < σ2\n",
      "n.\n",
      "Notice that aside from the ﬁnite variance, the only assumption is that\n",
      "within a row, the elements are independent. There is no assumption regard-\n",
      "ing diﬀerent rows; they may be independent and they may or may not have\n",
      "identical distributions.\n",
      "In order to say anything about the asymptotic distribution of some func-\n",
      "tion of {Xnj} we need to impose conditions on the moments. There are three\n",
      "standard conditions that we consider for sequences satisfying the general con-\n",
      "ditions on kn and σ2\n",
      "n.\n",
      "•\n",
      "Lyapunov’s condition. Lyapunov’s condition applies uniformly to the\n",
      "sum of central moments of order (2 + δ). Lyapunov’s condition is\n",
      "kn\n",
      "X\n",
      "j=1\n",
      "E \u0000|Xnj −E(Xnj)|2+δ\u0001 ∈o(σ2+δ\n",
      "n\n",
      ")\n",
      "for some δ > 0.\n",
      "(1.220)\n",
      "•\n",
      "Lindeberg’s condition. Lindeberg’s condition is\n",
      "kn\n",
      "X\n",
      "j=1\n",
      "E\n",
      "\u0000(Xnj −E(Xnj))2I{|Xnj−EXnj|>ϵσn}(Xnj)\n",
      "\u0001\n",
      "∈o(σ2\n",
      "n) ∀ϵ > 0,\n",
      "(1.221)\n",
      "Instead of a strong uniform condition on a power in terms of a positive\n",
      "addition δ to 2, as in Lyapunov’s condition, Lindeberg’s condition applies\n",
      "to a ﬁxed power of 2 over an interval controlled by ϵ. Lindeberg’s con-\n",
      "dition requires that the sum of the second central moments over the full\n",
      "support minus the squared central diﬀerences near the mean is ultimately\n",
      "dominated by the variance of the sum. (That is to say, the sum of the\n",
      "tail components of the variance is dominated by the variance of the sum.\n",
      "This means that the distributions cannot be too heavy-tailed.) The re-\n",
      "quirement is in terms of an ϵ that tells how much of the central region to\n",
      "remove before computing the individual central moments.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.4 Limit Theorems\n",
      "107\n",
      "Clearly, Lyapunov’s condition implies Lindeberg’s condition.\n",
      "Although Lyapunov’s condition is more stringent than Lindeberg’s condi-\n",
      "tion, it is sometimes easier to establish Lyapunov’s condition than Linde-\n",
      "berg’s condition.\n",
      "•\n",
      "Feller’s condition. Lindeberg’s condition (or Lyapunov’s condition, of\n",
      "course) implies Feller’s condition, which is:\n",
      "lim\n",
      "n→∞max\n",
      "j≤kn\n",
      "σ2\n",
      "nj\n",
      "σ2n\n",
      "= 0.\n",
      "(1.222)\n",
      "This condition comes up in the proof of Lindeberg’s central limit theorem.\n",
      "A Central Limit Theorem for Independent Scalar Random\n",
      "Variables with Finite Mean and Variance\n",
      "A more general central limit theorem is called Lindeberg’s central limit theo-\n",
      "rem. It is stated in terms of a sequence of the ﬁnite subsequences of a triangular\n",
      "array, as we encountered in the deﬁnition of inﬁnite divisibility on page 61.\n",
      "Theorem 1.58 (Lindeberg’s Central Limit Theorem)\n",
      "For given n, let {Xnj, j = 1, 2, . . ., kn} be independent random variables with\n",
      "0 < σ2\n",
      "n, where σ2\n",
      "n = V(Pkn\n",
      "j=1 Xnj) and kn →∞as n →∞. If the Lindeberg\n",
      "condition (1.221) holds, then\n",
      "1\n",
      "σn\n",
      "kn\n",
      "X\n",
      "j=1\n",
      "(Xnj −E(Xnj)) d→N(0, 1).\n",
      "(1.223)\n",
      "Proof. *** From inequality (1.95), we have for each Xnj,\n",
      "\f\fϕXnj (t) −(−t2V(Xnj)/2)\n",
      "\f\f ≤E \u0000min\u0000|tXnj|2, |tXnj|3/6\u0001\u0001 .\n",
      "***\n",
      "Multivariate Central Limit Theorems for Independent Random\n",
      "Variables with Finite Mean and Variance\n",
      "The central limit theorems stated above have multivariate extensions that\n",
      "are relatively straightforward. The complications arise from the variance-\n",
      "covariance matrices, which must replace the simple scalars σ2\n",
      "n.\n",
      "The simplest situation is the iid case where each member of the sequence\n",
      "{Xn} of random k-vectors has the ﬁnite variance-covariance matrix Σ. In that\n",
      "case, similar to equation (1.218) for iid scalar random variables, we have\n",
      "Pn\n",
      "i=1(Xi −E(Xi))\n",
      "√n\n",
      "d→Nk(0, Σ).\n",
      "(1.224)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "108\n",
      "1 Probability Theory\n",
      "Another type of multivariate central limit theorem can be formed by think-\n",
      "ing of the subsequences in equation (1.223) as multivariate random variables.\n",
      "Let {kn} be a sequence of constants such that kn →∞as n →∞. Let\n",
      "Xni ∈IRmi, where mi ≤m for some ﬁxed integer m and for i = 1, . . ., kn, be\n",
      "independent with\n",
      "inf\n",
      "i,n λni > 0,\n",
      "where λni is the smallest eigenvalue of V(Xni). (Note that this is saying that\n",
      "variance-covariance matrix is positive deﬁnite for every n and i; but it’s saying\n",
      "a little more than that.) Also suppose that for some δ > 0, we have\n",
      "sup\n",
      "i,n\n",
      "V(∥Xni∥2+δ) < ∞.\n",
      "Now, let cni be a sequence in IRmi with the property that it is diﬀuse:\n",
      "lim\n",
      "n→∞\n",
      " \n",
      "max\n",
      "1≤i≤kn ∥cni∥2\n",
      ", kn\n",
      "X\n",
      "i=1\n",
      "∥cni∥2\n",
      "!\n",
      "= 0.\n",
      "Then we have something similar to equation (1.223):\n",
      "kn\n",
      "X\n",
      "j=1\n",
      "cT\n",
      "ni (Xnj −E(Xnj))\n",
      ",\n",
      "\n",
      "kn\n",
      "X\n",
      "j=1\n",
      "V(cT\n",
      "niXnj)\n",
      "\n",
      "\n",
      "1/2\n",
      "d→N(0, 1).\n",
      "(1.225)\n",
      "1.4.3 Extreme Value Distributions\n",
      "In Theorem 1.48 we saw that the asymptotic joint distribution of a set of\n",
      "central order statistics obeys the central limit theorem. The asymptotic dis-\n",
      "tribution of extreme order statistics, however, is not normal. We have already\n",
      "considered asymptotic distributions of extreme order statistics in special cases\n",
      "in Examples 1.27 and 1.28 for random variables with bounded ranges and in\n",
      "Example 1.29 for a random variable with unbounded range. The latter is the\n",
      "more interesting case, of course.\n",
      "Given a sequence of random variables {Xi | i = 1, . . ., n}, we are interested\n",
      "in the limiting distribution of functions of the form in expression (1.208), that\n",
      "is,\n",
      "an\n",
      "\u0000X(n:n) −bn\n",
      "\u0001 .\n",
      "The ﬁrst question, of course, is what conditions on an and bn will yield a limit-\n",
      "ing distribution that is nondegenerate. These conditions clearly must depend\n",
      "on the distributions of the Xi, and must take into account any dependencies\n",
      "within the sequence. We will consider only the simple case; that is, we will\n",
      "assume that the Xi are iid. Let F be the CDF of each Xi. The problem now\n",
      "is to ﬁnd a CDF G for a nondegenerate distribution, such that for each point\n",
      "of continuity x of G,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.4 Limit Theorems\n",
      "109\n",
      "lim\n",
      "n→∞F n(x/an + bn) = G(x).\n",
      "(1.226)\n",
      "Fisher and Tippett (1928) The most general answer to this is given in the\n",
      "following theorem.\n",
      "Theorem 1.59 (extreme value distribution)\n",
      "A CDF satisfying equation (1.226) must have one of the following three forms\n",
      "in which α > 0:\n",
      "•\n",
      "G(x) = 0,\n",
      "x ≤0\n",
      "exp(−x−α), x > 0;\n",
      "(1.227)\n",
      "•\n",
      "G(x) = exp(−e−x);\n",
      "(1.228)\n",
      "•\n",
      "G(x) = exp(−(x)α), x < 0\n",
      "1,\n",
      "x ≥0.\n",
      "(1.229)\n",
      "The proof of a version of this theorem was given by Fisher and Tippett\n",
      "(1928), and a more careful statement along with a proof was given by\n",
      "Gnedenko (1943). See de Haan and Ferreira (2006) for a proof, and see\n",
      "David and Nagaraja (2003) for further discussion.\n",
      "*** Combine these in one express and introduce the extreme value index\n",
      "*** Give names to the three classes.\n",
      "domain of attraction\n",
      "1.4.4 Other Limiting Distributions\n",
      "Asymptotic distributions are very important in statistical applications be-\n",
      "cause, while the exact distribution of a function of a ﬁnite set of random\n",
      "variables may be very complicated, often the asymptotic distribution is un-\n",
      "complicated. Often, as we have seen, the limiting distribution is normal if the\n",
      "sequence is properly normalized. If a normal distribution can be used, even as\n",
      "an approximation, there is a wealth of statistical theory that can be applied\n",
      "to the problem.\n",
      "The random variables of interest are often functions g(X) of simpler ran-\n",
      "dom variables X. If we know the limiting distribution of {Xn} we can often\n",
      "work out the limiting distribution of {g(Xn)}, depending on the nature of the\n",
      "function g. A simple example of this is equation (1.195) for the delta method.\n",
      "In this case we start with {Xn} that has a limiting normal distribution and\n",
      "we get that the limiting distribution of {g(Xn)} is also normal.\n",
      "We also can often get useful limiting distributions from the central limit\n",
      "theorem and the distributions of functions of normal random variables such\n",
      "as chi-squared, t, or F, as discussed in Section 2.9.2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "110\n",
      "1 Probability Theory\n",
      "1.5 Conditional Probability\n",
      "The concept of conditional distributions provides the basis for the analysis of\n",
      "relationships among variables.\n",
      "A simple way of developing the ideas begins by deﬁning the conditional\n",
      "probability of event A, given event B. If Pr(B) ̸= 0, the conditional probability\n",
      "of event A given event B is\n",
      "Pr(A|B) = Pr(A ∩B)\n",
      "Pr(B)\n",
      ",\n",
      "(1.230)\n",
      "which leads to the useful multiplication rule\n",
      "Pr(A ∩B) = Pr(B)Pr(A|B).\n",
      "(1.231)\n",
      "We see from this that if A and B are independent\n",
      "Pr(A|B) = Pr(A).\n",
      "If we interpret all of this in the context of the probability space (Ω, F, P ),\n",
      "we can deﬁne a new “conditioned” probability space, (Ω, F, PB), where we\n",
      "deﬁne PB by\n",
      "PB(A) = Pr(A ∩B),\n",
      "for any A ∈F. From this conditional probability space we could then proceed\n",
      "to develop “conditional” versions of the concepts discussed in the previous\n",
      "sections.\n",
      "This approach, however, is not entirely satisfactory because of the require-\n",
      "ment that Pr(B) ̸= 0. More importantly, this approach in terms of events\n",
      "does not provide a basis for the development of conditional probability den-\n",
      "sity functions.\n",
      "Another approach is to make use of a concept of conditional expectation,\n",
      "and that is what we will proceed to do. In this approach, we develop sev-\n",
      "eral basic ideas before we ﬁnally speak of distributions of conditional random\n",
      "variables in Section 1.5.4.\n",
      "1.5.1 Conditional Expectation: Deﬁnition and Properties\n",
      "The deﬁnition of conditional expectation of one random variable given an-\n",
      "other random variable is developed in two stages. First, we deﬁne conditional\n",
      "expectation over a sub-σ-ﬁeld and consider some of its properties, and then\n",
      "we deﬁne conditional expectation with respect to another measurable function\n",
      "(a random variable, for example) in terms of the conditional expectation over\n",
      "the sub-σ-ﬁeld generated by the inverse image of the function.\n",
      "A major diﬀerence in conditional expectations and unconditional expecta-\n",
      "tions is that conditional expectations may be nondegenerate random variables.\n",
      "When the expectation is conditioned on a random variable, relations involv-\n",
      "ing the conditional expectations must be qualiﬁed as holding in probability,\n",
      "or holding with probability 1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.5 Conditional Probability\n",
      "111\n",
      "Conditional Expectation over a Sub-σ-Field\n",
      "Deﬁnition 1.44 (conditional expectation over a sub-σ-ﬁeld)\n",
      "Let (Ω, F, P ) be a probability space, let A be a sub-σ-ﬁeld of F, and let X be\n",
      "an integrable random variable over Ω. The conditional expectation of X given\n",
      "A, denoted by E(X|A), is an A-measurable function from (Ω, F) to (IRd, Bd)\n",
      "such that\n",
      "Z\n",
      "A\n",
      "E(X|A) dP =\n",
      "Z\n",
      "A\n",
      "X dP,\n",
      "∀A ∈A.\n",
      "(1.232)\n",
      "Clearly, if A = F, then E(X|A) = E(X)\n",
      "Being a real A-measurable function, the conditional expectation is a ran-\n",
      "dom variable from the space (Ω, A, P ). Such a random variable exists and is\n",
      "a.s. unique, as we will see below (Theorem 1.60).\n",
      "Equation (1.232) in terms of an indicator function is\n",
      "Z\n",
      "A\n",
      "E(X|A) dP = E(XIA), ∀A ∈A.\n",
      "(1.233)\n",
      "Another equivalent condition, in terms of bounded A-measurable func-\n",
      "tions, is\n",
      "E(E((X|A)Y )) = E(XY )\n",
      "(1.234)\n",
      "for all bounded and A-measurable Y for which XY is integrable.\n",
      "Theorem 1.60\n",
      "Let (Ω, F, P ) be a probability space, let A be a sub-σ-ﬁeld of F, and let X be\n",
      "an integrable random variable from Ωinto IRd. Then there is an a.s. unique\n",
      "d-variate random variable Y on (Ω, A, PA) such that\n",
      "Z\n",
      "A\n",
      "Y dPA =\n",
      "Z\n",
      "A\n",
      "X dP,\n",
      "∀A ∈A.\n",
      "Proof. Exercise. (Use the Radon-Nikodym theorem 0.1.30, on page 739 to\n",
      "show the existence. For a.s. uniqueness, assume the A-measurable functions\n",
      "Y1 and Y2 are such that R\n",
      "A Y1 dP = R\n",
      "A Y2 dP ∀A ∈A and show that Y1 = Y2\n",
      "a.s. A and P .)\n",
      "Conditional Expectation with Respect to a Measurable Function\n",
      "Deﬁnition 1.45 (with respect to another measurable function)\n",
      "Let (Ω, F, P ) be a probability space, let A be a sub-σ-ﬁeld of F, let X be an\n",
      "integrable random variable over Ω, and let Y be a measurable function from\n",
      "(Ω, F, P ) to any measurable space (Λ, G). Then the conditional expectation of\n",
      "X given Y , denoted by E(X|Y ), is deﬁned as the conditional expectation of\n",
      "X given the sub-σ-ﬁeld generated by Y , that is,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "112\n",
      "1 Probability Theory\n",
      "E(X|Y ) = E(X|σ(Y )).\n",
      "(1.235)\n",
      "Deﬁnition 1.44 provides meaning for the expression E(X|σ(Y )) in equa-\n",
      "tion (1.235).\n",
      "Sub-σ-ﬁelds generated by random variables, such as σ(Y ), play an im-\n",
      "portant role in statistics. We can think of σ(Y ) as being the “information\n",
      "provided by Y ”. In an important type of time series, Y1, Y2, . . ., we encounter\n",
      "a sequence σ(Y1) ⊆σ(Y2) ⊆· · · and we think of each random variable in the\n",
      "series as providing additional information.\n",
      "Another view of conditional expectations in statistical applications is as\n",
      "approximations or predictions; see Section 1.5.3.\n",
      "1.5.2 Some Properties of Conditional Expectations\n",
      "Although the deﬁnition above may appear rather abstract, it is not too dif-\n",
      "ﬁcult to work with, and it yields the properties of conditional expectation\n",
      "that we have come to expect based on the limited deﬁnitions of elementary\n",
      "probability.\n",
      "For example, we have the simple relationship with the unconditional ex-\n",
      "pectation:\n",
      "E(E(X|A)) = E(X).\n",
      "(1.236)\n",
      "Also, if the individual conditional expectations exist, the conditional ex-\n",
      "pectation is a linear operator:\n",
      "∀a ∈IR, E(aX + Y |A) = aE(X|A) + E(Y |A) a.s.\n",
      "(1.237)\n",
      "This fact follows immediately from the deﬁnition. For any A ∈A\n",
      "E(aX + Y |A) =\n",
      "Z\n",
      "A\n",
      "aX + Y dP\n",
      "= a\n",
      "Z\n",
      "A\n",
      "X dP +\n",
      "Z\n",
      "A\n",
      "Y dP\n",
      "= aE(X|A) + E(Y |A)\n",
      "As with unconditional expectations, we have immediately from the deﬁni-\n",
      "tion:\n",
      "X ≤Y a.s.\n",
      "⇒\n",
      "E(X|A) ≤E(Y |A) a.s..\n",
      "(1.238)\n",
      "We can establish conditional versions of the three theorems stated on\n",
      "page 89 that relate to the interchange of an integration operation and a\n",
      "limit operation (monotone convergence, Fatou’s lemma, and dominated con-\n",
      "vergence). These extensions are fairly straightforward.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.5 Conditional Probability\n",
      "113\n",
      "•\n",
      "monotone convergence:\n",
      "for 0 ≤X1 ≤X2 · · · a.s.\n",
      "Xn\n",
      "a.s.\n",
      "→X\n",
      "⇒\n",
      "E(Xn|A)\n",
      "a.s.\n",
      "→E(X|A).\n",
      "(1.239)\n",
      "•\n",
      "Fatou’s lemma:\n",
      "0 ≤Xn ∀n\n",
      "⇒\n",
      "E(lim\n",
      "n inf Xn|A) ≤lim\n",
      "n inf E(Xn|A) a.s..\n",
      "(1.240)\n",
      "•\n",
      "dominated convergence:\n",
      "given a ﬁxed Y with E(Y |A) < ∞,\n",
      "|Xn| ≤Y ∀n and Xn\n",
      "a.s.\n",
      "→X\n",
      "⇒\n",
      "E(Xn|A) a.s.\n",
      "→E(X|A).\n",
      "(1.241)\n",
      "Another useful fact is that if Y is A-measurable and |XY | and |X| are inte-\n",
      "grable (notice this latter is stronger than what is required to deﬁne E(X|A)),\n",
      "then\n",
      "E(XY |A) = Y E(X|A) a.s.\n",
      "(1.242)\n",
      "Some Useful Conditional Expectations\n",
      "There are some conditional expectations that arise often, and which we should\n",
      "immediately recognize. The simplest one is\n",
      "E\u0000E(Y |X)\u0001 = E(Y ).\n",
      "(1.243)\n",
      "Note that the expectation operator is based on a probability distribution,\n",
      "and so anytime we see “E”, we need to ask “with respect to what probability\n",
      "distribution?” In notation such as that above, the distributions are implicit\n",
      "and all relate to the same probability space. The inner expectation on the left\n",
      "is with respect to the conditional distribution of Y given X, and so is a func-\n",
      "tion of X. The outer expectation is with respect to the marginal distribution\n",
      "of X.\n",
      "Approaching this slightly diﬀerently, we consider a random variable Z that\n",
      "is a function of the random variables X and Y :\n",
      "Z = f(X, Y ).\n",
      "We have\n",
      "E(f(X, Y )) = EY\n",
      "\u0000EX|Y (f(X, Y )|Y )\u0001 = EX\n",
      "\u0000EY |X(f(X, Y )|X)\u0001 .\n",
      "(1.244)\n",
      "Another useful conditional expectation relates adjusted variances to “to-\n",
      "tal” variances:\n",
      "V(Y ) = V\n",
      "\u0000E(Y |X)\n",
      "\u0001\n",
      "+ E\n",
      "\u0000V(Y |X)\n",
      "\u0001\n",
      ".\n",
      "(1.245)\n",
      "This is intuitive, although you should be able to prove it formally. The intuitive\n",
      "explanation is: the total variation in Y is the sum of the variation of its mean\n",
      "given X and its average variation about X (or given X). (Think of SST =\n",
      "SSR + SSE in regression analysis.)\n",
      "This equality implies the Rao-Blackwell inequality (drop the second term\n",
      "on the right).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "114\n",
      "1 Probability Theory\n",
      "Exchangeability, Conditioning, and Independence\n",
      "De Finetti’s representation theorem (Theorem 1.30 on page 75) requires an\n",
      "inﬁnite sequence, and does not hold for ﬁnite sequences. For example, consider\n",
      "an urn containing one red ball and one blue ball from which we draw the balls\n",
      "without replacement. Let Ri = 1 if a red ball is drawn on the ith draw and\n",
      "Ri = 0 otherwise. (This is the Polya’s urn of Example 1.6 on page 24 with\n",
      "r = b = 1 and c = −1.) Clearly, the sequence R1, R2 is exchangeable. Because\n",
      "Pr(R1 = 1, R2 = 1) = 0,\n",
      "if there were a measure µ as in de Finetti’s representation theorem, then we\n",
      "would have\n",
      "0 =\n",
      "Z 1\n",
      "0\n",
      "π2dµ(π),\n",
      "which means that µ must put mass 1 at the point 0. But also\n",
      "Pr(R1 = 0, R2 = 0) = 0,\n",
      "which would mean that\n",
      "0 =\n",
      "Z 1\n",
      "0\n",
      "(1 −π)2dµ(π).\n",
      "That would not be possible if µ satisﬁes the previous requirement. There are,\n",
      "however, ﬁnite versions of de Finetti’s theorem; see, for example, Diaconis\n",
      "(1977) or Schervish (1995).\n",
      "An alternate statement of de Finetti’s theorem identiﬁes a random variable\n",
      "with the distribution P , and in that way provides a more direct connection\n",
      "to its use in statistical inference.\n",
      "Theorem 1.61 (de Finetti’s representation theorem (alternate))\n",
      "The sequence {Xi}∞\n",
      "i=1 of binary random variables is exchangeable iﬀthere\n",
      "is a random variable Π such that, conditional on Π = π, the {Xi}∞\n",
      "i=1 are\n",
      "iid Bernoulli random variables with parameter π. Furthermore, if {Xi}∞\n",
      "i=1\n",
      "is exchangeable, then the distribution of Π is unique and Xn = Pn\n",
      "i=1 Xi/n\n",
      "converges to Π almost surely.\n",
      "Example 1.30 exchangeable Bernoulli random variables that are\n",
      "conditionally iid Bernoullis (Schervish, 1995)\n",
      "Suppose {Xn}∞\n",
      "n=1 are exchangeable Bernoulli random variables such that for\n",
      "each n and for k = 0, 1, . . ., n,\n",
      "Pr\n",
      " n\n",
      "X\n",
      "i=1\n",
      "= k\n",
      "!\n",
      "=\n",
      "1\n",
      "n + 1.\n",
      "Now Xn\n",
      "a.s.\n",
      "→Π, where Π is as in Theorem 1.61, and so Xn\n",
      "d→Π. To determine\n",
      "the distribution of Π, we write the CDF of Xn as\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.5 Conditional Probability\n",
      "115\n",
      "Fn(t) = ⌊nt⌋+ 1\n",
      "n + 1 ;\n",
      "hence, limFn(t) = t, which is the CDF of Π. Therefore, Π has a U(0, 1)\n",
      "distribution. The Xi are conditionally iid Bernoulli(π) for Π = π.\n",
      "The distributions in this example will be used in Examples 4.2 and 4.6 in\n",
      "Chapter 4 to illustrate methods in Bayesian data analysis.\n",
      "Conditional expectations also are important in approximations of one ran-\n",
      "dom variable by another random variable, and in “predicting” one random\n",
      "variable using another, as we see in the next section.\n",
      "1.5.3 Projections\n",
      "Use of one distribution or one random variable as an approximation of another\n",
      "distribution or random variable is a very useful technique in probability and\n",
      "statistics. It is a basic technique for establishing asymptotic results, and it\n",
      "underlies statistical applications involving regression analysis and prediction.\n",
      "Given two scalar random variables X and Y , consider the question of\n",
      "what Borel function g is such that g(X) is closest to Y in some sense. A\n",
      "common way to deﬁne closeness of random variables is by use of the expected\n",
      "squared distance, E((Y −g(X))2). This leads to the least squares criterion for\n",
      "determining the optimal g(X).\n",
      "First, we must consider whether or under what conditions, the problem\n",
      "has a solution under this criterion.\n",
      "Theorem 1.62\n",
      "Let X and Y be scalar random variables over a common measurable space and\n",
      "assume E(Y 2) < ∞. Then there exists a Borel measurable function g0 with\n",
      "E((g0(X))2) < ∞such that\n",
      "E((Y −g0(X))2) = inf{E((Y −g(X))2) | g(X) ∈G0},\n",
      "(1.246)\n",
      "where G0 = {g(X) | g : IR 7→IR is Borel measurable and E((g0(X))2) < ∞}.\n",
      "Proof. ***ﬁx\n",
      "Although Theorem 1.62 is stated in terms of scalar random variables,\n",
      "a similar result holds for vector-valued random variables. The next theorem\n",
      "identiﬁes a g0 that minimizes the L2 norm for vector-valued random variables.\n",
      "Theorem 1.63\n",
      "Let Y be a d-variate random variable such that E(∥Y ∥2) < ∞and let G be the\n",
      "set of all Borel measurable functions from IRk into IRd. Let X be a k-variate\n",
      "random variable such that E(∥E(Y |X)∥2) < ∞. Let g0(X) = E(Y |X). Then\n",
      "g0(X) = arg min\n",
      "g∈G\n",
      "E(∥Y −g(X)∥2).\n",
      "(1.247)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "116\n",
      "1 Probability Theory\n",
      "Proof. Exercise. Compare this with Theorem 1.13 on page 27, in which the\n",
      "corresponding solution is g0(X) = E(Y |a) = E(Y ).\n",
      "By the general deﬁnition of projection (Deﬁnition 0.0.9 on page 637), we\n",
      "see that conditional expectation can be viewed as a projection in a linear space\n",
      "deﬁned by the square-integrable random variables over a given probability\n",
      "space and the inner product ⟨Y, X⟩= E(Y X) and its induced norm. (In fact,\n",
      "some people deﬁne conditional expectation this way instead of the way we\n",
      "have in Deﬁnitions 1.44 and 1.45.)\n",
      "In regression applications in statistics using least squares, as we discuss on\n",
      "page 438, “bY ”, or the “predicted” Y given X, that is, E(Y |X) is the projection\n",
      "of Y onto X. For given ﬁxed values of Y and X the predicted Y given X is\n",
      "the vector projection, in the sense of Deﬁnition 0.0.9.\n",
      "We now formally deﬁne projection for random variables in a manner anal-\n",
      "ogous to Deﬁnition 0.0.9. Note that the random variable space is the range of\n",
      "the functions in G in Theorem 1.63.\n",
      "Deﬁnition 1.46 (projection of a random variable onto a space of random variables)\n",
      "Let Y be a random variable and let X be a random variable space deﬁned on\n",
      "the same probability space. A random variable Xp ∈X such that\n",
      "E(∥Y −Xp∥2) ≤E(∥Y −X∥2) ∀X ∈X\n",
      "(1.248)\n",
      "is called a projection of Y onto X .\n",
      "The most interesting random variable spaces are linear spaces, and in the\n",
      "following we will assume that X is a linear space, and hence the norm arises\n",
      "from an inner product so that the terms in inequality (1.248) involve variances\n",
      "and covariances.\n",
      "*** existence, closure of space in second norm (see page 35).\n",
      "*** treat vector variables diﬀerently: E(∥Y −E(Y )∥2) is not the vari-\n",
      "ance**** make this distinction earlier\n",
      "When X is a linear space, we have the following result for projections.\n",
      "Theorem 1.64\n",
      "Let X be a linear space of random variables with ﬁnite second moments. Then\n",
      "Xp is a projection of Y onto X iﬀXp ∈X and\n",
      "E\n",
      "\u0000(Y −Xp)TX\n",
      "\u0001\n",
      "= 0 ∀X ∈X .\n",
      "(1.249)\n",
      "Proof.\n",
      "For any X, Xp ∈X we have\n",
      "E((Y −X)T(Y −X)) = E((Y −Xp)T(Y −Xp))\n",
      "+2E((Y −Xp)T(Xp −X))\n",
      "+E((Xp −X)T(Xp −X))\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.5 Conditional Probability\n",
      "117\n",
      "If equation (1.249) holds then the middle term is zero and so E((Y −X)T(Y −\n",
      "X)) ≥E((Y −Xp)T(Y −Xp)) ∀X ∈X ; that is, Xp is a projection of Y onto\n",
      "X .\n",
      "Now, for any real scalar a and any X, Xp ∈X , we have\n",
      "E((Y −Xp −aX)T(Y −Xp −aX)) −E((Y −Xp)T(Y −Xp)) =\n",
      "−2aE((Y −Xp)TX) + a2E(XTX).\n",
      "If Xp is a projection of Y onto X , the term on the left side of the equation is\n",
      "nonnegative for every a. But the term on the right side of the equation can be\n",
      "nonnegative for every a only if the orthogonality condition of equation (1.249)\n",
      "holds; hence, we conclude that that is the case.\n",
      "Because a linear space contains the constants, we have the following corol-\n",
      "lary.\n",
      "Corollary 1.64.1\n",
      "Let X be a linear space of random variables with ﬁnite second moments. and\n",
      "let Xp be a projection of the random variable Y onto X . Then,\n",
      "E(Xp) = E(Y ),\n",
      "(1.250)\n",
      "Cov(Y −Xp, X) = 0 ∀X ∈X ,\n",
      "(1.251)\n",
      "and\n",
      "Cov(Y, X) = Cov(Xp, X) ∀X ∈X .\n",
      "(1.252)\n",
      "***ﬁx ** add uniqueness etc. E(Y ) = E(Xp) and Cov(Y −Xp, X) =\n",
      "0 ∀X ∈X .\n",
      "Deﬁnition 1.47 (projection of a function of random variables)\n",
      "Let Y1, . . ., Yn be a set of random variables. The projection of the statistic\n",
      "Tn(Y1, . . ., Yn) onto the kn random variables X1, . . ., Xkn is\n",
      "eTn = E(Tn) +\n",
      "kn\n",
      "X\n",
      "i=1\n",
      "(E(Tn|Xi) −E(Tn)) .\n",
      "(1.253)\n",
      "An interesting projection is one in which the Y1, . . ., Ykn in Deﬁnition 1.47\n",
      "are the same as X1, . . ., Xn. In that case, if Tn is a symmetric function of the\n",
      "X1, . . ., Xn (for example, the X1, . . ., Xn are iid), then the E(Tn|Xi) are iid\n",
      "with mean E(Tn). The residual, Tn −eTn, is often of interest. Writing\n",
      "Tn −eTn = Tn −E(Tn) −\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(E(Tn|Xi) −E(Tn)),\n",
      "we see that E(Tn −eTn) = 0. Hence, we have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "118\n",
      "1 Probability Theory\n",
      "E( eTn) = E(Tn),\n",
      "(1.254)\n",
      "and if V(Tn) < ∞\n",
      "V( eTn) = nV(E(Tn|Xi))\n",
      "(1.255)\n",
      "(exercise).\n",
      "If V(E(Tn|Xi)) > 0, by the central limit theorem, we have\n",
      "1\n",
      "p\n",
      "nV(E(Tn|Xi))\n",
      "( eTn −E(Tn))\n",
      "d→N(0, 1).\n",
      "We also have an interesting relationship between the variances of Tn and\n",
      "eTn, that is, V( eTn) ≤V(Tn), as the next theorem shows.\n",
      "Theorem 1.65\n",
      "If Tn is symmetric and V(Tn) < ∞for every n, and eTn is the projection of\n",
      "Tn onto X1, . . ., Xn, then\n",
      "E((Tn −eTn)2) = V(Tn) −V( eTn).\n",
      "Proof. Because E(Tn) = E( eTn), we have\n",
      "E((Tn −eTn)2) = V(Tn) + V( eTn) −2Cov(Tn, eTn).\n",
      "(1.256)\n",
      "But\n",
      "Cov(Tn, eTn) = E(Tn eTn) −(E(Tn))2\n",
      "= E(TnE(Tn)) + E\n",
      " \n",
      "Tn\n",
      "n\n",
      "X\n",
      "i=1\n",
      "E(Tn|Xi)\n",
      "!\n",
      "−nE(TnE(Tn))) −(E(Tn))2\n",
      "= nE (TnE(Tn|Xi)) −n(E(Tn))2\n",
      "= nV(E(Tn|Xi))\n",
      "= V( eTn),\n",
      "and the desired result follows from equation (1.256) above.\n",
      "The relevance of these facts, if we can show that eTn →Tn in some appro-\n",
      "priate way, then we can work out the asymptotic distribution of Tn. (The use\n",
      "of projections of U-statistics beginning on page 413 is an example.)\n",
      "Partial Correlations\n",
      "***ﬁx\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.5 Conditional Probability\n",
      "119\n",
      "1.5.4 Conditional Probability and Probability Distributions\n",
      "We now are in a position to deﬁne conditional probability. It is based on a\n",
      "conditional expectation.\n",
      "Deﬁnition 1.48 (conditional probability given a sub-σ-ﬁeld)\n",
      "Let (Ω, F, P ) be a probability space, let A be a sub-σ-ﬁeld of F, and let\n",
      "B ∈F. The conditional probability of B given A, denoted by Pr(B|A), is\n",
      "deﬁned as E(IB|A).\n",
      "The concept of conditional probability given a sub-σ-ﬁeld immediately\n",
      "yields the concepts of conditional probability given an event and conditional\n",
      "probability given a random variable. For a probability space (Ω, F, P ) with\n",
      "A, B ∈F, the conditional probability of B given A, denoted by Pr(B|A), is\n",
      "E(IB|σ(A)).\n",
      "Furthermore, if X is a random variable deﬁned on (Ω, F, P ), the condi-\n",
      "tional probability of B given X, denoted by Pr(B|A), is E(IB|σ(X)). This gives\n",
      "meaning to the concept of a conditional distribution of one random variable\n",
      "given another random variable.\n",
      "Conditional Distributions\n",
      "We start with a probability space (IRm, Bm, P1) and deﬁne a probability mea-\n",
      "sure on the measurable space (IRn × IRm, σ(Bn × Bm). We ﬁrst need the\n",
      "existence of such a probability measure (proved in Billingsley (1995), page\n",
      "439).\n",
      "For a random variable Y in IRm, its (marginal) distribution is determined\n",
      "by P1, which we denote as PY (y). For B ∈Bn and C ∈Bm, the condi-\n",
      "tional distribution is deﬁned by identifying a probability measure, denoted as\n",
      "PX|Y (·|y), on (IRn, σ(Bn)) for any ﬁxed y ∈IRm.\n",
      "The joint probability measure of (X, Y ) over IRn × IRm is deﬁned as\n",
      "PXY =\n",
      "Z\n",
      "C\n",
      "PX|Y (·|y)dPY (y),\n",
      "where C ∈Bm.\n",
      "For distributions with PDFs the conditional, joint, and marginal PDFs\n",
      "have the simple relationship\n",
      "fX|Y (x|y) = fXY (x, y)\n",
      "fY (y)\n",
      ",\n",
      "so long as fY (y) > 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "120\n",
      "1 Probability Theory\n",
      "Independence\n",
      "Theorem 1.66\n",
      "The random variables X and Y are independent iﬀthe conditional distribution\n",
      "equals the marginal distribution; that is, for the d-variate random variable X,\n",
      "iﬀ\n",
      "∀A ⊆IRd, Pr(X ∈A|Y ) = Pr(X ∈A).\n",
      "Proof. Exercise.\n",
      "This theorem means that we can factor the joint PDF or CDF of indepen-\n",
      "dent random variables.\n",
      "Conditional Independence\n",
      "The ideas of conditional distributions also lead to the concept of conditional\n",
      "independence.\n",
      "Deﬁnition 1.49 (conditional independence)\n",
      "X and Y are conditionally independent given Z iﬀthe joint conditional distri-\n",
      "bution equals the joint marginal distribution; that is, for the d-variate random\n",
      "variable X, iﬀ\n",
      "∀A ⊆IRd, Pr(X ∈A|Y, Z) = Pr(X ∈A|Y ).\n",
      "When two independent random variables are added to a third independent\n",
      "variable, the resulting sums are conditionally independent, given the third\n",
      "(common) random variable.\n",
      "Theorem 1.67\n",
      "Suppose the random variables X, Y , and Z are independent. Let U = X + Z\n",
      "and V = Y + Z. Then U|Z and V |Z are independent; that is, U and V are\n",
      "conditionally independent given Z.\n",
      "Proof. Exercise.\n",
      "Copulas\n",
      "One of the most important uses of copulas is to combine two marginal distri-\n",
      "butions to form a joint distribution with known bivariate characteristics. We\n",
      "can build the joint distribution from a marginal and a conditional.\n",
      "We begin with two U(0, 1) random variables U and V . For a given associ-\n",
      "ation between U and V speciﬁed by the copula C(u, v), from Sklar’s theorem,\n",
      "we can see that\n",
      "PU|V (u|v) = ∂\n",
      "∂v C(u, v)|v.\n",
      "(1.257)\n",
      "We denote\n",
      "∂\n",
      "∂vC(u, v)|v by Cv(u).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.6 Stochastic Processes\n",
      "121\n",
      "Conditional Entropy\n",
      "We deﬁne the conditional entropy of X given Y in two ways. The ﬁrst meaning\n",
      "just follows the deﬁnition of entropy in equation (1.81) on page 42 with the\n",
      "conditional PDF pX|Y used in place of the marginal PDF pX. This leads to\n",
      "an entropy that is a random variable or an entropy for a ﬁxed value Y = y.\n",
      "In the more common usage, we deﬁne the conditional entropy of X given Y\n",
      "(which is also called the equivocation of X about Y ) as the expected value of\n",
      "the term described above; that is,\n",
      "H(X|Y ) = −\n",
      "X\n",
      "y\n",
      "pY (y)\n",
      "X\n",
      "x\n",
      "pX|Y (x|y) log(pX|Y (x|y)).\n",
      "(1.258)\n",
      "As before, the basic deﬁnition is made in terms of a PDF derived by a counting\n",
      "measure, but we extend it to any PDF.\n",
      "From the deﬁnition we see that\n",
      "H(X|Y ) = H(X, Y ) −H(Y )\n",
      "(1.259)\n",
      "or\n",
      "H(X, Y ) = H(X|Y ) + H(Y ).\n",
      "Interpret H(X, Y ) as “total entropy”, and compare the latter expression with\n",
      "equation (1.245).\n",
      "1.6 Stochastic Processes\n",
      "Many interesting statistical problems concern stochastic processes, which we\n",
      "can think of as a measurable function\n",
      "X : I × Ω7→IRd,\n",
      "(1.260)\n",
      "where I is some index set (I could be any ordered set).\n",
      "In the expression above, X is a random variable, and for each i ∈I, Xi is\n",
      "a random variable. If the stochastic process is viewed as evolving in time, we\n",
      "usually denote the index by t and we may denote the process as {Xt}. In view\n",
      "of equation (1.260), it is also appropriate and common to use the notation\n",
      "{X(t, ω)}.\n",
      "The main interest in stochastic processes is the relationship among the\n",
      "distributions of {Xt} for diﬀerent values of t.\n",
      "The sequences we discussed in Section 1.3 are of course stochastic pro-\n",
      "cesses. The sequences considered in that section did not have any particular\n",
      "structure, however. In some cases, we required that they have no structure;\n",
      "that is, that the elements in the sequence were independent. There are many\n",
      "special types of interesting stochastic processes with various structures, such\n",
      "as Markov chains, martingales, and other types of time series. In this section,\n",
      "we will just give some basic deﬁnitions, and then discuss brieﬂy two important\n",
      "classes of stochastic process.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "122\n",
      "1 Probability Theory\n",
      "States, Times, Notation, and Basic Deﬁnitions\n",
      "The smallest set of measure 1 is called the state space of a stochastic process;\n",
      "that is, the range of X is called the state space. Any point in the state space\n",
      "is called a state.\n",
      "If the index set of a stochastic process is countable, we say the pro-\n",
      "cess is a discrete time stochastic process. We can index a discrete time pro-\n",
      "cess by 0, 1, 2, . . ., especially if there is a ﬁxed starting point, although often\n",
      ". . ., −2, −1, 0, 1, 2, . . . is more appropriate.\n",
      "In many applications, however, the index of a stochastic process ranges\n",
      "over a continuous interval. In that case, we often use a slightly diﬀerent no-\n",
      "tation for the index set. We often consider the index set to be the interval\n",
      "[0, T], which of course could be transformed into any ﬁnite closed interval. If\n",
      "the index set is a real interval we say the process is a continuous time stochas-\n",
      "tic process. For continuous time stochastic processes, we sometimes use the\n",
      "notation X(t), although we also use Xt. We will discuss continuous time pro-\n",
      "cesses in Section 1.6.2 below and consider a simple continuous time process\n",
      "in Example 1.32.\n",
      "A property that seems to occur often in applications and, when it does, af-\n",
      "fords considerable simpliﬁcations for analyses is the conditional independence\n",
      "of the future on the past given the present. This property, called the Markov\n",
      "property, can be made precise.\n",
      "Deﬁnition 1.50 (Markov property)\n",
      "Suppose in the sequence {Xt}, for any set t0 < t1 < · · · < tn < t and any x,\n",
      "we have\n",
      "Pr(Xt ≤x | Xt0, Xt1, . . ., Xtn) = Pr(Xt ≤x | Xtn).\n",
      "(1.261)\n",
      "Then {Xt} is said to be a Markov sequence or the sequence is said to be\n",
      "Markovian. The condition expressed in equation (1.261) is called the Markov\n",
      "property.\n",
      "Deﬁnition 1.51 (homogeneous process)\n",
      "If the marginal distribution of X(t) is independent of t, the process is said to\n",
      "be homogeneous.\n",
      "***ﬁx Many concepts are more easily deﬁned for discrete time processes,\n",
      "although most have analogs for continuous time processes.\n",
      "Deﬁnition 1.52 (stopping time)\n",
      "Given a discrete time stochastic process ***ﬁx change to continuous time\n",
      "X : {0, 1, 2, . . .} × Ω7→IR,\n",
      "a random variable\n",
      "T : Ω7→{0, 1, 2, . . .}\n",
      "(1.262)\n",
      "is called a stopping time if the event {T = t} depends only on X0, . . ., Xt for\n",
      "n = 0, 1, 2, . . ..\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.6 Stochastic Processes\n",
      "123\n",
      "Stopping times have several important characteristics, such as the fact\n",
      "that the Markov property holds at stopping times.\n",
      "Deﬁnition 1.53 (ﬁrst passage time)\n",
      "A special stopping time is the ﬁrst passage time deﬁned (for discrete time\n",
      "processes) as\n",
      "Tj = min{t ≥1 : Xt = j},\n",
      "(1.263)\n",
      "if this set is nonempty; otherwise, Tj = ∞.\n",
      "There are other types of useful properties that simplify the analyses of\n",
      "processes with those properties. While a ﬁrst passage time depends on the\n",
      "concept of a beginning point in the process, in the following we will usually\n",
      "allow the discrete time index to assume the values . . . , −2, −1, 0, 1, 2, . . ..\n",
      "One of the most interesting properties of a stochastic process is the rela-\n",
      "tionship of terms in the sequence to each other.\n",
      "Deﬁnition 1.54 (autocovariance and autocorrelation)\n",
      "For the process\n",
      "{Xt : t = . . . , −2, −1, 0, 1, 2, . . .}\n",
      "E(Xt) = µt < ∞, the function\n",
      "γ(s, t) = E((Xs −µs)(Xt −µt))\n",
      "= Cov(Xs, Xt)\n",
      "(1.264)\n",
      "if it is ﬁnite, is called the autocovariance function.\n",
      "If the autocovariance exists, the function\n",
      "ρ(s, t) =\n",
      "γ(s, t)\n",
      "p\n",
      "γ(s, s)γ(t, t)\n",
      "(1.265)\n",
      "is called the autocorrelation function. The autocorrelation function, which is\n",
      "generally more useful than the autocovariance function, is also called the ACF.\n",
      "Deﬁnition 1.55 (white noise process)\n",
      "A process\n",
      "{Xt : t = . . . , −2, −1, 0, 1, 2, . . .}\n",
      "with E(Xt) = 0\n",
      "and\n",
      "V(Xt) = σ2 < ∞∀t and such that ρ(s, t) = 0 ∀s ̸= t\n",
      "is called white noise.\n",
      "A zero-correlated process with constant ﬁnite mean and variance is also\n",
      "sometimes called white noise process even if the mean is nonzero. We denote\n",
      "a white noise process by Xt ∼WN(0, σ2) or Xt ∼WN(µ, σ2).\n",
      "Notice that the terms in a white noise process do not necessarily have\n",
      "identical distributions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "124\n",
      "1 Probability Theory\n",
      "Deﬁnition 1.56 ((weakly) stationary process)\n",
      "Suppose\n",
      "{Xt : t = . . . , −2, −1, 0, 1, 2, . . .}\n",
      "is such that E(Xt) = µ\n",
      "and\n",
      "V(Xt) < ∞∀t and γ(s, t) is constant for any\n",
      "ﬁxed value of |s −t|. Then the process {Xt} is said to be weakly stationary.\n",
      "A white noise is clearly stationary.\n",
      "In the case of a stationary process, the autocovariance function can be\n",
      "indexed by a single quantity, h = |s −t|, and we often write it as γh.\n",
      "It is clear that in a stationary process, V(Xt) = V(Xs); that is, the variance\n",
      "is also constant. The variance is γ0 in the notation above.\n",
      "Just because the means, variances, and autocovariances are constant, the\n",
      "distributions are not necessarily the same, so a stationary process is not nec-\n",
      "essarily homogeneous. Likewise, marginal distributions being equal does not\n",
      "insure that the autocovariances are constant, so a homogeneous process is not\n",
      "necessarily stationary.\n",
      "The concept of stationarity can be made stricter.\n",
      "Deﬁnition 1.57 (strictly stationary process)\n",
      "Suppose\n",
      "{Xt : t = . . . , −2, −1, 0, 1, 2, . . .}\n",
      "is such that for any k, any set t1, . . ., tk, and any h the joint distribution of\n",
      "Xt1, . . ., Xtk\n",
      "is identical to the joint distribution of\n",
      "Xt1+h, . . ., Xtk+h.\n",
      "Then the process {Xt} is said to be strictly stationary.\n",
      "A strictly stationary process is stationary, but the converse statement does\n",
      "not necessarily hold. If the distribution of each Xt is normal, however, and if\n",
      "the process is stationary, then it is strictly stationary.\n",
      "As noted above, a homogeneous process is not necessarily stationary. On\n",
      "the other hand, a strictly stationary process is homogeneous, as we see by\n",
      "choosing k = 1.\n",
      "Example 1.31 a central limit theorem for a stationary process\n",
      "Suppose X1, X2, . . . is a stationary process with E(Xt) = µ and V(Xt) = σ2.\n",
      "We have\n",
      "V(√n(X −µ)) = σ2 + 1\n",
      "n\n",
      "n\n",
      "X\n",
      "i̸=j=1\n",
      "Cov(Xi, Xj)\n",
      "= σ2 + 2\n",
      "n\n",
      "n\n",
      "X\n",
      "h=1\n",
      "(n −h)γh.\n",
      "(1.266)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.6 Stochastic Processes\n",
      "125\n",
      "(Exercise.) Now, if limn→∞2\n",
      "n\n",
      "Pn\n",
      "h=1(n −h)γh = τ 2 < ∞, then\n",
      "√n(X −µ)\n",
      "∼→N(0, σ2 + τ 2).\n",
      "1.6.1 Probability Models for Stochastic Processes\n",
      "A model for a stochastic process posits a sampling sequence over a sample\n",
      "space Ω. This yields a path or trajectory, (ω1, ω2, . . .). In continuous time we\n",
      "generally denote a path trajectory as ω(t). The sample space for the stochastic\n",
      "process becomes the set of paths. We denote this by ΩT .\n",
      "We think of a stochastic process in terms of a random variable, Xt, and\n",
      "an associated σ-ﬁeld Ft in which Xt is measurable.\n",
      "***ﬁx motivate and prove this ... a sequence of random variables {Xn} for\n",
      "any n, the joint CDF of X1, . . ., Xn is **** uniqueness\n",
      "Theorem 1.68 (Kolmogorov extension theorem)\n",
      "For any positive integer k and any t1, . . ., tk ∈T , let Pt1,...,tk be probability\n",
      "measures on IRnt such that for any Borel sets B1, . . ., Bk ∈IRn,\n",
      "PΠ(t1),...,Π(tk)(B1 × · · · × Bk) = Pt1,...,tk(BΠ−1(1) × · · · × BΠ−1(k)), (1.267)\n",
      "for all permutations Π on {1, . . ., k}, and for all positive integers m,\n",
      "Pt1,...,tk(B1×· · ·×Bk) = Pt1,...,tk,tk+1,...,tk+m(BΠ−1(1)×· · ·×BΠ−1(k)×IRn · · ·×IRn).\n",
      "(1.268)\n",
      "Then there exists a probability space (Ω, F, P ) and a stochastic process\n",
      "{Xt} on Ω, Xt : Ω7→IRn such that\n",
      "Pt1,...,tk(B1 × · · · × Bk) = P (Xt1 ∈B1, . . ., Xtk ∈Bk)\n",
      "(1.269)\n",
      "for all positive integers k, for all ti ∈T , and for all Borel sets Bi.\n",
      "Proof.\n",
      "***ﬁx\n",
      "Evolution of σ-Fields\n",
      "In many applications, we assume an evolution of σ-ﬁelds, which, under the\n",
      "interpretation of a σ-ﬁeld as containing all events of interest, is equivalent to\n",
      "an evolution of information. This leads to the concept of a ﬁltration and a\n",
      "stochastic process adapted to the ﬁltration.\n",
      "Deﬁnition 1.58 (ﬁltration; adaptation)\n",
      "Let {(Ω, Ft, P )} be a sequence of probability spaces such that if s ≤t, then\n",
      "Fs ⊆Ft. The sequence {Ft} is called a ﬁltration.\n",
      "For each t let Xt be a real function on Ωmeasurable wrt Ft. The stochastic\n",
      "process {Xt} is said to be adapted to the ﬁltration {Ft}.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "126\n",
      "1 Probability Theory\n",
      "If {Xt} is adapted to the ﬁltration {Ft}, we often write Xt ∈Ft. We also\n",
      "call the process {Xt} nonanticipating, for obvious reasons.\n",
      "Deﬁnition 1.59 (ﬁltered probability space)\n",
      "Given a probability space (Ω, F, P ) and a ﬁltration {Ft} of sub-σ-ﬁelds of F,\n",
      "we form the ﬁltered probability space (Ω, F, {Ft : t ∈[0, ∞[}, P ).\n",
      "1.6.2 Continuous Time Processes\n",
      "For a stochastic process over a continuous index set I we must be concerned\n",
      "about the continuity of the process in time. The problem arises because the\n",
      "countably-additive property of a measure (equation (0.1.8)) does not carry\n",
      "over to uncountable unions. For a process X(t, ω) where t is in uncountable\n",
      "index set, say, for example, an interval, we will be faced with the necessity\n",
      "to evaluate probabilities of sets of the form ∪t≥0At. Such unions are not\n",
      "necessarily in the underlying σ-ﬁeld.\n",
      "** continuation motivation\n",
      "We can deﬁne continuity of X(t, ω) on I in the usual way at a given point\n",
      "ω0 ∈Ω. Next, we consider continuity of a stochastic process over Ω.\n",
      "Deﬁnition 1.60 (sample continuous)\n",
      "Given a probability space (Ω, F, P ) and a function\n",
      "X : I × Ω7→IR,\n",
      "we say X is sample continuous if X(ω) : I 7→IR is continuous for almost all\n",
      "ω (with respect to P ).\n",
      "The phrase almost surely continuous, or just continuous, is often used instead\n",
      "of sample continuous.\n",
      "*** add more ... examples\n",
      "The path of a stochastic process may be continuous, but many useful\n",
      "stochastic processes are mixtures of continuous distributions and discrete\n",
      "jumps. In such cases, in order to assign any reasonable value to the path\n",
      "at the point of discontinuity, we naturally assume that time is unidirectional\n",
      "and the discontinuity occurs at the time of the jump, and then the path evolves\n",
      "continuously from that point; that is, after the fact, the path is continuous\n",
      "from the right. The last value from the left is a limit of a continuous function.\n",
      "In French, we would describe this as continu `a droite, limit´e `a gauche; that is\n",
      "cadlag. Most models of stochastic processes are assumed to be cadlag.\n",
      "1.6.3 Markov Chains\n",
      "The simplest stochastic process is a sequence of exchangeable random vari-\n",
      "ables; that is, a sequence with no structure. A simple structure can be im-\n",
      "posed by substituting conditioning for independence. A sequence of random\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.6 Stochastic Processes\n",
      "127\n",
      "variables with the Markov property is called a Markov process. A Markov\n",
      "process in which the state space is countable is called a Markov chain. (The\n",
      "term “Markov chain” is also sometimes used to refer to any Markov process,\n",
      "as in the phrase “Markov chain Monte Carlo”, in applications of which the\n",
      "state space is often continuous.)\n",
      "The theory of Markov chains is usually developed ﬁrst for discrete-time\n",
      "chains, that is, those with a countable index set, and then extended to\n",
      "continuous-time chains.\n",
      "If the state space is countable, it is equivalent to X = {1, 2, . . .}. If X is a\n",
      "random variable from some sample space to X , and\n",
      "πi = Pr(X = i),\n",
      "(1.270)\n",
      "then the vector π = (π1, π2, . . .) deﬁnes a distribution of X on X . Formally, we\n",
      "deﬁne a Markov chain (of random variables) X0, X1, . . . in terms of an initial\n",
      "distribution π and a conditional distribution for Xt+1 given Xt. Let X0 have\n",
      "distribution π, and given Xt = j, let Xt+1 have distribution (pij; i ∈X ); that\n",
      "is, pij is the probability of a transition from state j at time t to state i at time\n",
      "t + 1, and K = (pij) is called the transition matrix of the chain. The initial\n",
      "distribution π and the transition matrix K characterize the chain, which we\n",
      "sometimes denote as Markov(π, K). It is clear that K is a stochastic matrix,\n",
      "and hence ρ(K) = ∥K∥∞= 1, and (1, 1) is an eigenpair of K.\n",
      "If K does not depend on the time (and our notation indicates that we are\n",
      "assuming this), the Markov chain is stationary.\n",
      "A discrete-time Markov chain {Xt} with discrete state space {x1, x2, . . .}\n",
      "can be characterized by the probabilities pij = Pr(Xt+1 = xi | Xt = xj).\n",
      "Clearly, P\n",
      "i∈I pij = 1. A vector such as p∗j whose elements sum to 1 is called\n",
      "a stochastic vector or a distribution vector.\n",
      "Because for each j, P\n",
      "i∈I pij = 1, K is a right stochastic matrix.\n",
      "The properties of a Markov chain are determined by the properties of the\n",
      "transition matrix. Transition matrices have a number of special properties,\n",
      "which we discuss in Section 0.3.6, beginning on page 818.\n",
      "(Note that many people who work with Markov chains deﬁne the transition\n",
      "matrix as the transpose of K above. This is not a good idea, because in ap-\n",
      "plications with state vectors, the state vectors would naturally have to be row\n",
      "vectors. Until about the middle of the twentieth century, many mathematicians\n",
      "thought of vectors as row vectors; that is, a system of linear equations would\n",
      "be written as xA = b. Nowadays, almost all mathematicians think of vectors\n",
      "as column vectors in matrix algebra. Even in some of my previous writings,\n",
      "e.g., Gentle (2007), I have called the transpose of K the transition matrix,\n",
      "and I deﬁned a stochastic matrix in terms of the transpose. The transpose of\n",
      "a right stochastic matrix is a left stochastic matrix, which is what is commonly\n",
      "meant by the unqualiﬁed phrase “stochastic matrix”. I think that it is time to\n",
      "adopt a notation that is more consistent with current matrix/vector notation.\n",
      "This is merely a change in notation; no concepts require any change.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "128\n",
      "1 Probability Theory\n",
      "If we assume that Xt is a random variable taking values in {x1, x2, . . .}\n",
      "and with a PDF (or probability mass function) given by\n",
      "Pr(Xt = xi) = π(t)\n",
      "i ,\n",
      "(1.271)\n",
      "and we write π(t) = (π(t)\n",
      "1 , π(t)\n",
      "2 , . . .), then the PDF at time t + 1 is\n",
      "π(t+1) = Kπ(t).\n",
      "(1.272)\n",
      "Many properties of a Markov chain depend on whether the transition matrix\n",
      "is reducible or not.\n",
      "Because 1 is an eigenvalue and the vector 1 is the eigenvector associated\n",
      "with 1, from equation (0.3.70), we have\n",
      "lim\n",
      "t→∞Kt = 1πs,\n",
      "(1.273)\n",
      "where πs is the Perron vector of KT.\n",
      "This also gives us the limiting distribution for an irreducible, primitive\n",
      "Markov chain,\n",
      "lim\n",
      "t→∞π(t) = πs.\n",
      "The Perron vector has the property πs = KTπs of course, so this distribution\n",
      "is the invariant distribution of the chain.\n",
      "The deﬁnition means that (1, 1) is an eigenpair of any stochastic matrix.\n",
      "It is also clear that if K is a stochastic matrix, then ∥K∥∞= 1, and because\n",
      "ρ(K) ≤∥K∥for any norm and 1 is an eigenvalue of K, we have ρ(K) = 1.\n",
      "A stochastic matrix may not be positive, and it may be reducible or irre-\n",
      "ducible. (Hence, (1, 1) may not be the Perron root and Perron eigenvector.)\n",
      "If the state space is countably inﬁnite, the vectors and matrices have in-\n",
      "ﬁnite order; that is, they have “inﬁnite dimension”. (Note that this use of\n",
      "“dimension” is diﬀerent from our standard deﬁnition that is based on linear\n",
      "independence.)\n",
      "We write the initial distribution as π(0). A distribution at time t can be\n",
      "expressed in terms of π(0) and K:\n",
      "π(t) = Ktπ(0).\n",
      "(1.274)\n",
      "Kt is often called the t-step transition matrix.\n",
      "The transition matrix determines various relationships among the states of\n",
      "a Markov chain. State i is said to be accessible from state j if it can be reached\n",
      "from state j in a ﬁnite number of steps. This is equivalent to (Kt)ij > 0 for\n",
      "some t. If state i is accessible from state j and state j is accessible from\n",
      "state i, states i and j are said to communicate. Communication is clearly an\n",
      "equivalence relation. The set of all states that communicate with each other is\n",
      "an equivalence class. States belonging to diﬀerent equivalence classes do not\n",
      "communicate, although a state in one class may be accessible from a state\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.6 Stochastic Processes\n",
      "129\n",
      "in a diﬀerent class. If all states in a Markov chain are in a single equivalence\n",
      "class, the chain is said to be irreducible.\n",
      "The limiting behavior of the Markov chain is of interest. This of course can\n",
      "be analyzed in terms of limt→∞Kt. Whether or not this limit exists depends\n",
      "on the properties of K.\n",
      "Galton-Watson Process\n",
      "An interesting class of Markov chains are branching processes, which model\n",
      "numbers of particles generated by existing particles. One of the simplest\n",
      "branching processes is the Galton-Watson process, in which at time t each\n",
      "particle is assumed to be replaced by 0, 1, 2, . . . particles with probabilities\n",
      "π0, π1, π2, . . ., where πk ≥0, π0 + π1 < 1, and Pπk = 1. The replacements\n",
      "of all particles at any time t are independent of each other. The condition\n",
      "π0 + π1 < 1 prevents the process from being trivial.\n",
      "*** add more\n",
      "Continuous Time Markov Chains\n",
      "In many cases it seems natural to allow the index of the Markov process to\n",
      "range over a continuous interval. The simplest type of continuous time Markov\n",
      "chain is a Poisson process.\n",
      "Example 1.32 Poisson process\n",
      "Consider a sequence of iid random variables, Y1, Y2, . . . distributed as exponential(0, θ),\n",
      "and build the random variables Tk = Pk\n",
      "i=1 Yi. (The Yis are the exponential\n",
      "spacings as in Example 1.18.)\n",
      "*** prove Markov property\n",
      "*** complete\n",
      "birth process\n",
      "***add\n",
      "K(t) = etR\n",
      "***ﬁx R intensity rate. rii nonpositive, rij for i ̸= j nonnegative, P\n",
      "i∈I rij =\n",
      "0 for all j.\n",
      "1.6.4 L´evy Processes and Brownian Motion\n",
      "Many applications of stochastic processes, such as models of stock prices, focus\n",
      "on the increments between two points in time. One of the most widely-used\n",
      "models makes three assumptions about these increments. These assumptions\n",
      "deﬁne a L´evy process.\n",
      "Deﬁnition 1.61 (L´evy process)\n",
      "Given a ﬁltered probability space (Ω, F, {Ft}0≤t<∞, P ). An adapted process\n",
      "{X(t) : t ∈[0, ∞[} with X(0) a.s.\n",
      "= 0 is a L´evy process iﬀ\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "130\n",
      "1 Probability Theory\n",
      "(i) X(t) −X(s) is independent of Fs, for 0 ≤s < t < ∞.\n",
      "(ii) X(t) −X(s)\n",
      "d= X(t −s) for 0 ≤s < t < ∞.\n",
      "(iii) X(t)\n",
      "p→X(s) as t →s.\n",
      "One of the most commonly used L´evy processes is Brownian motion also\n",
      "called a Bachelier-Wiener process (see Section 0.2.1 on page 766).\n",
      "Deﬁnition 1.62 (Brownian motion)\n",
      "(i) X(t) is continuous in t almost surely.\n",
      "(ii) E(Xt)\n",
      "a.s.\n",
      "= X(0).\n",
      "(iii) X(t) −X(s) for 0 ≤s < t has a normal distribution with variance t −s.\n",
      "A Bachelier-Wiener process is also called a Brownian motion.\n",
      "*** properties: covariance, etc.; existence, etc.\n",
      "1.6.5 Brownian Bridges\n",
      "*** deﬁnition, properties\n",
      "Doob’s transformation: If {Y (t)} is a Brownian bridge and\n",
      "X(t) = (1 + t)Y (t/(1 + t))\n",
      "for t ≥0,\n",
      "(1.275)\n",
      "then {X(t)} is a Brownian motion.\n",
      "1.6.6 Martingales\n",
      "Martingales are an important class of stochastic processes. The concept of\n",
      "conditional expectation is important in developing a theory of martingales.\n",
      "Martingales are special sequences of random variables that have applications\n",
      "in various processes that evolve over time.\n",
      "Deﬁnition 1.63 (martingale, submartingale, supermartingale)\n",
      "Let {Ft} be a ﬁltration and let {Xt} be adapted to the ﬁltration {Ft}. We\n",
      "say the sequence {(Xt, Ft) : t ∈T } is a martingale iﬀ\n",
      "E(Xt|Ft−1)\n",
      "a.s.\n",
      "= Xt−1.\n",
      "(1.276)\n",
      "We say the sequence {(Xt, Ft) : t ∈T } is a submartingale iﬀ\n",
      "E(Xt|Ft−1)\n",
      "a.s.\n",
      "≥Xt−1.\n",
      "(1.277)\n",
      "We say the sequence {(Xt, Ft) : t ∈T } is a supermartingale iﬀ\n",
      "E(Xt|Ft−1)\n",
      "a.s.\n",
      "≤Xt−1.\n",
      "(1.278)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.6 Stochastic Processes\n",
      "131\n",
      "We also refer to a sequence of random variables {Xt : t ∈T } as a (sub, su-\n",
      "per)martingale if {(Xt, σ({Xs : s ≤t})) : t ∈T } is a (sub, super)martingale;\n",
      "that is, the martingale is the sequence {Xt} instead of {(Xt, Ft)}, and a cor-\n",
      "responding sequence {Ft} is implicitly deﬁned as {σ({Xs : s ≤t})}.\n",
      "This is consistent with the deﬁnition of {(Xt, Ft)\n",
      ":\n",
      "t ∈T } as a (sub,\n",
      "super)martingale because clearly\n",
      "σ({Xs : s ≤r})r ⊆σ({Xs : s ≤t})t\n",
      "if r ≤t\n",
      "(and so {σ({Xs : s ≤t})t} is a ﬁltration), and furthermore {Xt} is adapted\n",
      "to the ﬁltration {σ({Xs : s ≤t})t}.\n",
      "We often refer to the type of (sub, super)martingale deﬁned above as a\n",
      "forward (sub, super)martingale. We deﬁne a reverse martingale analogously\n",
      "with the conditions Ft ⊃Ft+1 ⊃· · · and E(Xt−1|Ft)\n",
      "a.s.\n",
      "= Xt.\n",
      "The sequence of sub-σ-ﬁelds, which is a ﬁltration, is integral to the deﬁni-\n",
      "tion of martingales. Given a sequence of random variables {Xt}, we may be\n",
      "interested in another sequence of random variables {Yt} that are related to\n",
      "the Xs. We say that {Yt} is a martingale with respect to {Xt} if\n",
      "E(Yt|{Xτ : τ ≤s}) a.s.\n",
      "= Ys, ∀s ≤t.\n",
      "(1.279)\n",
      "We also sometimes deﬁne martingales in terms of a more general sequence\n",
      "of σ-ﬁelds. We may say that {Xt\n",
      ": t ∈T } is a martingale relative to the\n",
      "sequence of σ-ﬁelds {Dt : t ∈T } in some probability space (Ω, F, P ), if\n",
      "Xs = E(Xt|Dt) for s > t.\n",
      "(1.280)\n",
      "Submartingales and supermartingales relative to {Dt : t ∈T } may be deﬁned\n",
      "analogously.\n",
      "Example 1.33 Polya’s urn process\n",
      "Consider an urn that initially contains r red and b blue balls, and Polya’s\n",
      "urn process (Example 1.6 on page 24). In this process, one ball is chosen\n",
      "randomly from the urn, and its color noted. The ball is then put back into the\n",
      "urn together with c balls of the same color. Let Xn be the number of red balls\n",
      "in the urn after n iterations of this procedure, and let Yn = Xn/(nc + r + b).\n",
      "Then the sequence {Yn} is a martingale (Exercise 1.82).\n",
      "Interestingly, if c > 0, then {Yn} converges to the beta distribution with\n",
      "parameters r/c and b/c; see Freedman (1965). Freedman also discusses a vari-\n",
      "ation on Polya’s urn process called Friedman’s urn process, which is the same\n",
      "as Polya’s, except that at each draw in addition to the c balls of the same\n",
      "color being added to the urn, d balls of the opposite color are added to the\n",
      "urn. Remarkably, the behavior is radically diﬀerent, and, in fact, if c > 0 and\n",
      "d > 0, then Yn\n",
      "a.s.\n",
      "→1/2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "132\n",
      "1 Probability Theory\n",
      "Example 1.34 likelihood ratios\n",
      "Let f and g be probability densities. Let X1, X2, . . . be an iid sequence of\n",
      "random variables whose range is within the intersection of the domains of f\n",
      "and g. Let\n",
      "Yn =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "g(Xi)/f(Xi).\n",
      "(1.281)\n",
      "(This is called a “likelihood ratio” and has applications in statistics. Note\n",
      "that f(x) and g(x) are likelihoods, as deﬁned in equation (1.19) on page 20,\n",
      "although the “parameters” are the functions themselves.) Now suppose that\n",
      "f is the PDF of the Xi. Then {Yn : n = 1, 2, 3, . . .} is a martingale with\n",
      "respect to {Xn : n = 1, 2, 3, . . .}.\n",
      "The martingale in Example 1.34 has some remarkable properties. Robbins\n",
      "(1970) showed that for any ϵ > 1,\n",
      "Pr(Yn ≥ϵ for some n ≥1) ≤1/ϵ.\n",
      "(1.282)\n",
      "Robbins’s proof of (1.282) is straightforward. Let N be the ﬁrst n ≥1 such\n",
      "that Qn\n",
      "i=1 g(Xi) ≥ϵ Qn\n",
      "i=1 f(Xi), with N = ∞if no such n occurs. Also, let\n",
      "gn(t) = Qn\n",
      "i=1 g(ti) and fn(t) = Qn\n",
      "i=1 f(ti).\n",
      "Pr(Yn ≥ϵ for some n ≥1) = Pr(N < ∞)\n",
      "=\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "Z\n",
      "I{n}(N)fn(t)dt\n",
      "≤1\n",
      "ϵ\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "Z\n",
      "I{n}(N)gn(t)dt\n",
      "≤1\n",
      "ϵ .\n",
      "Another important property of the martingale in Example 1.34 is\n",
      "Yn\n",
      "a.s.\n",
      "→0.\n",
      "(1.283)\n",
      "You are asked to show this in Exercise 1.83.\n",
      "Example 1.35 Bachelier-Wiener process\n",
      "If {W(t)\n",
      ":\n",
      "t ∈[0, ∞[} is a Bachelier-Wiener process, then W 2(t) −t is a\n",
      "martingale. (Exercise.)\n",
      "Example 1.36 A martingale that is not Markovian and a Markov\n",
      "process that is not a martingale\n",
      "The Markov property is based on conditional independence of distributions\n",
      "and the martingale property is based on equality of expectations. Thus it is\n",
      "easy to construct a martingale that is not a Markov chain beginning with\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.6 Stochastic Processes\n",
      "133\n",
      "X0 has any given distribution with V(X0) > 0. The sequence {Xt : EXt =\n",
      "EXt−1, VXt = Pt−1\n",
      "k=0 VXk} is not a Markov chain.\n",
      "A Markov chain that is not a martingale, for example, is {Xt :\n",
      "Xt\n",
      "d=\n",
      "2Xt−1}, where X0 has any given distribution with E(X0) ̸= 0.\n",
      "A common application of martingales is as a model for stock prices. As\n",
      "a concrete example, we can think of a random variable X1 as an initial sum\n",
      "(say, of money), and a sequence of events in which X2, X3, . . . represents a\n",
      "sequence of sums with the property that each event is a “fair game”; that\n",
      "is, E(X2|X1) = X1 a.s., E(X3|X1, X2) = X2 a.s., . . .. We can generalize this\n",
      "somewhat by letting Dn = σ(X1, . . ., Xn), and requiring that the sequence be\n",
      "such that E(Xn|Dn−1) a.s.\n",
      "= Xn−1.\n",
      "Doob’s Martingale Inequality\n",
      "A useful property of submartingales is Doob’s martingale inequality. This\n",
      "inequality is a more general case of Kolmogorov’s inequality (B.11), page 849,\n",
      "and the H´ajek-R`enyi inequality (B.12), both of which involve partial sums\n",
      "that are martingales.\n",
      "Theorem 1.69 (Doob’s Martingale Inequality)\n",
      "Let {Xt : t ∈[0, T]} be a submartingale relative to {Dt : t ∈[0, T]} taking\n",
      "nonnegative real values; that is, 0 ≤Xs ≤E(Xt|Dt) for s, t. Then for any\n",
      "constant ϵ > 0 and p ≥1,\n",
      "Pr\n",
      "\u0012\n",
      "sup\n",
      "0≤t≤T\n",
      "Xt ≥ϵ\n",
      "\u0013\n",
      "≤1\n",
      "ϵp E(|XT |p).\n",
      "(1.284)\n",
      "Proof. ***ﬁx\n",
      "Notice that Doob’s martingale inequality implies Robbins’s likelihood ratio\n",
      "martingale inequality (1.282).\n",
      "Azuma’s Inequality\n",
      "extension of Hoeﬀding’s inequality (B.10), page 848\n",
      "1.6.7 Empirical Processes and Limit Theorems\n",
      "For a given random sample, the relationship of the ECDF Fn to the CDF\n",
      "F of the underlying distribution is of interest. At a given x, the normalized\n",
      "diﬀerence\n",
      "Gn(x) = √n(Fn(x) −F (x))\n",
      "(1.285)\n",
      "is called an empirical process. The convergence of this process will be studied\n",
      "below.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "134\n",
      "1 Probability Theory\n",
      "Martingale Central Limit Theorem\n",
      "Most of the central limit theorems we discussed in Section 1.4.2 required\n",
      "identical distributions, and all required independence. Can we relax the inde-\n",
      "pendence assumption?\n",
      "We focus on partial sums as in equation (1.223).\n",
      "Theorem 1.70 (Martingale Central Limit Theorem)\n",
      "Let\n",
      "Yn =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pkn\n",
      "j=1(Xnj −E(Xnj))\n",
      "if n ≤kn\n",
      "Pkn\n",
      "j=1(Xknj −E(Xknj)) if n > kn.\n",
      "(1.286)\n",
      "Now, assume {Yn} is a martingale.\n",
      "Next, starting with a ﬁxed value for each subsequence, say Xn0 = 0, assume\n",
      "the sum of the normalized conditional variances converge to 1:\n",
      "1\n",
      "σn\n",
      "kn\n",
      "X\n",
      "j=2\n",
      "E \u0000(Xnj −E(Xnj))2|Xn1, . . ., Xn,j−1\n",
      "\u0001 p→1,\n",
      "where, as before, σ2\n",
      "n = V(Pkn\n",
      "j=1 Xnj). Then we have\n",
      "1\n",
      "σn\n",
      "kn\n",
      "X\n",
      "j=1\n",
      "(Xnj −E(Xnj)) d→N(0, 1).\n",
      "(1.287)\n",
      "The addends in Yn are called a triangular array as in the buildup to Linde-\n",
      "berg’s Central Limit Theorem (see page 106), and the result (1.287) is the\n",
      "same as in Lindeberg’s Central Limit Theorem on page 107.\n",
      "Proof. ***ﬁx\n",
      "Convergence of Empirical Processes\n",
      "Although we may write the ECDF as Fn or Fn(x), it is important to remember\n",
      "that it is a random variable. We may use the notation Fn(x, ω) to indicate\n",
      "that the ECDF is a random variable, yet to allow it to have an argument just\n",
      "as the CDF does. I will use this notation occasionally, but usually I will just\n",
      "write Fn(x). The randomness comes in the deﬁnition of Fn(x), which is based\n",
      "on the random sample.\n",
      "The distribution of nFn(x) (at the ﬁxed point x) is binomial, and so the\n",
      "pointwise properties of the ECDF are easy to see. From the SLLN, we see\n",
      "that it strongly converges pointwise to the CDF, and from the CLT, we have,\n",
      "at the point x,\n",
      "√n(Fn(x) −F (x)) d→N (0, F (x)(1 −F (x))) .\n",
      "(1.288)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "1.6 Stochastic Processes\n",
      "135\n",
      "Although the pointwise properties of the ECDF are useful, its global rela-\n",
      "tionship to the CDF is one of the most important properties of the ECDF. Our\n",
      "interest will be in the convergence of Fn, or more precisely, in the convergence\n",
      "of a metric on Fn and F . When we consider the convergence of metrics on\n",
      "functions, the arguments of the functions are sequences of random variables,\n",
      "yet the metric integrates out the argument.\n",
      "An important property of empirical processes is a stochastic bound on its\n",
      "sup norm that is called the Dvoretzky/Kiefer/Wolfowitz (DKW) inequality,\n",
      "after the authors of the paper in which a form of it was given (Dvoretzky et al.,\n",
      "1956). This inequality provides a bound for the probability that the sup dis-\n",
      "tance of the ECDF from the CDF exceeds a given value. Massart (1990)\n",
      "tightened the bound and gave a more useful form of the inequality. In one-\n",
      "dimension, for any positive z, the Dvoretzky/Kiefer/Wolfowitz/Massart in-\n",
      "equality states\n",
      "Pr(sup\n",
      "x (√n|Fn(x, ω) −F (x)|) > z) ≤2e−2z2.\n",
      "(1.289)\n",
      "This inequality is useful in proving various convergence results for the ECDF.\n",
      "For a proof of the inequality itself, see Massart (1990).\n",
      "A particularly important fact regards the strong convergence of the sup\n",
      "distance of the ECDF from the CDF to zero; that is, the ECDF converges\n",
      "strongly and uniformly to the CDF. This is stated in the following theorem.\n",
      "The DKW inequality can be used to prove the theorem, but the proof below\n",
      "does not use it directly.\n",
      "Theorem 1.71 (Glivenko-Cantelli) If X1, . . ., Xn are iid with CDF F and\n",
      "ECDF Fn, then supx(|Fn(x, ω) −F (x)|)\n",
      "wp1\n",
      "→0.\n",
      "Proof. First, note by the SLLN and the binomial distribution of Fn, ∀(ﬁxed) x,\n",
      "Fn(x, ω)\n",
      "wp1\n",
      "→F (x); that is,\n",
      "lim\n",
      "n→∞Fn(x, ω) = F (x)\n",
      "∀x, except x ∈Ax, where Pr(Ax) = 0.\n",
      "The problem here is that Ax depends on x and so there are uncountably\n",
      "many such sets. The probability of their union may possibly be positive. So\n",
      "we must be careful.\n",
      "We will work on the CDF and ECDF from the other side of x (the discon-\n",
      "tinuous side). Again, by the SLLN, we have\n",
      "lim\n",
      "n→∞Fn(x−, ω) = F (x−)\n",
      "∀x, except x ∈Bx, where Pr(Bx) = 0.\n",
      "Now, let\n",
      "φ(u) = inf{x ; u ≤F (x)}\n",
      "for 0 < u ≤1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "136\n",
      "1 Probability Theory\n",
      "(Notice F (φ(u)−) ≤u ≤F (φ(u)). Sketch the picture.)\n",
      "Now consider xm,k = φ(k/m) for positive integers m and k with 1 ≤k ≤\n",
      "m. (There are countably many xm,k, and so when we consider Fn(xm,k, ω) and\n",
      "F (xm,k), there are countably many null-probability sets, Axm,k and Bxm,k,\n",
      "where the functions diﬀer in the limit.)\n",
      "We immediately have the three relations:\n",
      "F (xm,k−) −F (xm,k−1) ≤m−1\n",
      "F (xm,1−) ≤m−1\n",
      "and\n",
      "F (xm,m) ≥1 −m−1,\n",
      "and, of course, F is nondecreasing.\n",
      "Now let Dm,n(ω) be the maximum over all k = 1, . . ., m of\n",
      "|Fn(xm,k, ω) −F (xm,k)|\n",
      "and\n",
      "|Fn(xm,k−, ω) −F (xm,k−)|.\n",
      "(Compare Dn(ω).)\n",
      "We now consider three ranges for x:\n",
      "] −∞, xm,1[\n",
      "[xm,k−1 , xm,k[ for k = 1, . . ., m\n",
      "[xm,m , ∞[\n",
      "Consider x ∈[xm,k−1, xm,k[. In this interval,\n",
      "Fn(x, ω) ≤Fn(xm,k−, ω)\n",
      "≤F (xm,k−) + Dm,n(ω)\n",
      "≤F (x) + m−1 + Dm,n(ω)\n",
      "and\n",
      "Fn(x, ω) ≥Fn(xm,k−1, ω)\n",
      "≥F (xm,k−1) −Dm,n(ω)\n",
      "≥F (x) −m−1 −Dm,n(ω)\n",
      "Hence, in these intervals, we have\n",
      "Dm,n(ω) + m−1 ≥sup\n",
      "x |Fn(x, ω) −F (x)|\n",
      "= Dn(ω).\n",
      "We can get this same inequality in each of the other two intervals.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "137\n",
      "Now, ∀m, except on the unions over k of Axm,k and Bxm,k, limn Dm,n(ω) =\n",
      "0, and so limn Dn(ω) = 0, except on a set of probability measure 0 (the\n",
      "countable unions of the Axm,k and Bxm,k.) Hence, we have the convergence\n",
      "wp1; i.e., a.s. convergence.\n",
      "The sup norm on the empirical process always exists because both func-\n",
      "tions are bounded. Other norms may not be ﬁnite.\n",
      "Theorem 1.72 If X1, . . ., Xn are iid with CDF F ∈L1 and ECDF Fn, then\n",
      "∥Fn(x, ω) −F (x)∥p\n",
      "wp1\n",
      "→0.\n",
      "Proof. ******* use relationship between Lp norms\n",
      "***************** add stuﬀDonsker’s theorem\n",
      "Notes and Further Reading\n",
      "Probability theory is the most directly relevant mathematical background for\n",
      "mathematical statistics. Probability is a very large subﬁeld of mathematics.\n",
      "The objective of this chapter is just to provide some of the most relevant\n",
      "material for statistics.\n",
      "The CDF\n",
      "I ﬁrst want to emphasize how important the CDF is in probability and statis-\n",
      "tics.\n",
      "This is also a good point to review the notation used in connection with\n",
      "functions relating to the CDF. The meaning of the notation in at least two\n",
      "cases (inverse and convolution) is slightly diﬀerent from the usual meaning of\n",
      "that same notation in other contexts.\n",
      "If we denote the CDF by F ,\n",
      "•\n",
      "F(x) = 1 −F (x);\n",
      "•\n",
      "F −1(p) = inf{x, s.t. F (x) ≥p} for p ∈]0, 1[;\n",
      "•\n",
      "F (2)(x) = F ⋆F (x) =\n",
      "R\n",
      "F (x −t)dF (t);\n",
      "•\n",
      "Fn(x) is the ECDF of an iid sample of size n from distribution with CDF\n",
      "F .\n",
      "•\n",
      "f(x) = dF (x)/dx.\n",
      "Foundations\n",
      "I began this chapter by expressing my opinion that probability theory is an\n",
      "area of pure mathematics: given a consistent axiomatic framework, “beliefs”\n",
      "are irrelevant. That attitude was maintained throughout the discussions in\n",
      "this chapter. Yet the literature on applications of probability theory is replete\n",
      "with interpretations of the meaning of “probability” by “frequentists”, by “ob-\n",
      "jectivists”, and by “subjectivists”, and discussions of the relative importance\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "138\n",
      "1 Probability Theory\n",
      "of independence and exchangeability; see, for example, Hamaker (1977) and\n",
      "de Finetti (1979), just to cite two of the most eloquent (and opinionated) of\n",
      "the interlocutors. While the airing of some of these issues may just be furious\n",
      "sound, there are truly some foundational issues in application of probability\n",
      "theory to decisions made in everyday life. There are various to statistical in-\n",
      "ference that diﬀer in fundamental ways (as alluded to by Hamaker (1977) and\n",
      "de Finetti (1979)) in whether or not prior “beliefs” or “subjective probabili-\n",
      "ties” are incorporated formally into the decision process.\n",
      "While the phrase “subjective probability” is current, that concept does not\n",
      "fall within the scope ot this chapter, but it will be relevant in later chapters\n",
      "on statistical applications of probability theory.\n",
      "There are, however, diﬀerent ways of developing the concept of probability\n",
      "as a set measure that all lead to the same set of results discussed in this\n",
      "chapter. I will now brieﬂy mention these alternatives.\n",
      "Alternative Developments of a Probability Measure\n",
      "Probability as a concept had been used by mathematicians and other sci-\n",
      "entists well before it was given a mathematical treatment. The ﬁrst major\n",
      "attempt to provide a mathematical framework was Laplace’s Th´eorie Ana-\n",
      "lytique des Probabilit´es in 1812. More solid advances were made in the lat-\n",
      "ter half of the 19th Century by Chebyshev, Markov, and Lyapunov at the\n",
      "University of St. Petersburg, but this work was not well known. (Lyapunov\n",
      "in 1892 gave a form of a central limit theorem. He developed this in the\n",
      "next few years into a central limit theorem similar to Lindeberg’s, which\n",
      "appeared in 1920 in a form very similar to Theorem 1.58.) Despite these de-\n",
      "velopments, von Mises (v. Mises) (1919a) said that “probability theory is not\n",
      "a mathematical science” (my translation), and set out to help to make it\n",
      "such. Indicating his ignorance of the work of both Lyapunov and Lindeberg,\n",
      "von Mises (v. Mises) (1919a) gives a more limited central limit theorem, but\n",
      "von Mises (v. Mises) (1919b) is a direct attempt to give a mathematical mean-\n",
      "ing to probability. In the “Grundlagen” he begins with a primitive concept of\n",
      "collective (or set), then deﬁnes probability as the limit of a frequency ratio,\n",
      "and formulates two postulates that essentially require invariance of the limit\n",
      "under any selections within the collective. This notion came to be called “sta-\n",
      "tistical probability”. Two years later, Keynes (1921) developed a concept of\n",
      "probability in terms of the relative support one statement leads to another\n",
      "statement. This idea was called “inductive probability”. As Kolmogorov’s ax-\n",
      "iomatic approach (see below) came to deﬁne probability theory and statistical\n",
      "inference for most mathematicians and statisticians, the disconnect between\n",
      "statistical probability and inductive probability continued to be of concern.\n",
      "Leblanc (1962) attempted to reconcile the two concepts, and his little book is\n",
      "recommended as a good, but somewhat overwrought, discussion of the issues.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "139\n",
      "Deﬁne probability as a special type of measure\n",
      "We have developed the concept of probability by ﬁrst deﬁning a measurable\n",
      "space, then deﬁning a measure, and ﬁnally deﬁning a special measure as a\n",
      "probability measure.\n",
      "Deﬁne probability by a set of axioms\n",
      "Alternatively, the concept of probability over a given measurable space could\n",
      "be stated as axioms. In this approach, there would be four axioms: nonnega-\n",
      "tivity, additivity over disjoint sets, probability of 1 for the sample space, and\n",
      "equality of the limit of probabilities of a monotonic sequence of sets to the\n",
      "probability of the limit of the sets. The axiomatic development of probability\n",
      "theory is due to Kolmogorov in the 1920s and 1930s. In Kolmogorov (1956),\n",
      "he starts with a sample space and a collection of subsets and gives six axioms\n",
      "that characterize a probability space. (Four axioms are the same or similar\n",
      "to those above, and the other two characterize the collection of subsets as a\n",
      "σ-ﬁeld.)\n",
      "Deﬁne probability from a coherent ordering\n",
      "Given a sample space Ωand a collection of subsets A, we can deﬁne a total\n",
      "ordering on A. (In some developments following this approach, A is required\n",
      "to be a σ-ﬁeld; in other approaches, it is not.) The ordering consists of the\n",
      "relations “≺”, “⪯”, “∼”, “≻”, and “⪰”. The ordering is deﬁned by ﬁve axioms\n",
      "it must satisfy. (“Five” depends on how you count, of course; in the ﬁve\n",
      "laid out below, which is the most common way the axioms are stated, some\n",
      "express multiple conditions.) For any sets, A, Ai, B, Bi ∈A whose unions and\n",
      "intersections are in A (if A is a a σ-ﬁeld this clause is unnecessary), the axioms\n",
      "are:\n",
      "1. Exactly one of the following relations holds: A ≻B, A ∼B, or A ≺B.\n",
      "2. Let A1, A2, B1, B2 be such that A1 ∩A2 = ∅, B1 ∩B2 = ∅, A1 ⪯B1, and\n",
      "A2 ⪯B2. Then A1 ∪A2 ⪯B1 ∪B2. Furthermore, if either A1 ≺B1 or\n",
      "A2 ≺B2, then A1 ∪A2 ≺B1 ∪B2.\n",
      "3. ∅⪯A and ∅≺Ω.\n",
      "4. If A1 ⊇A2 ⊇· · · and for each i, Ai ⪰B, then ∩iAi ⪰B.\n",
      "5. Let U ∼U(0, 1) and associate the ordering (≺, ⪯, ∼, ≻, ⪰) with Lebesgue\n",
      "measure on [0, 1]. Then for any interval I ⊆[0, 1], either A ≻I, A ∼I, or\n",
      "A ≺I.\n",
      "These axioms deﬁne a linear, or total, ordering on A. (Exercise 1.89).\n",
      "Given these axioms for a “coherent” ordering on Ω, we can deﬁne a proba-\n",
      "bility measure P on A by P (A) ≤P (B) iﬀA ⪯B, and so on. It can be shown\n",
      "that such a measure exists, satisﬁes the Kolmogorov axioms, and is unique.\n",
      "At ﬁrst it was thought that the ﬁrst 4 axioms were suﬃcient to deﬁne\n",
      "a probability measure that satisﬁed Kolmogorov’s axioms, but Kraft et al.\n",
      "(1959) exhibited an example that showed that more was required.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "140\n",
      "1 Probability Theory\n",
      "A good exposition of this approach based on coherency that includes a\n",
      "proof of the existence and uniqueness of the probability measure is given by\n",
      "DeGroot (1970).\n",
      "Deﬁne probability from expectations of random variables\n",
      "Although the measurable spaces of Sections 1.1.1 and 0.1 (beginning on\n",
      "page 692) do not necessarily consist of real numbers, we deﬁned real-valued\n",
      "functions (random variables) that are the basis of further development of\n",
      "probability theory. From the axioms characterizing probability (or equiva-\n",
      "lently from the deﬁnition of the concept of a probability measure), we devel-\n",
      "oped expectation and various unifying objects such as distributions of random\n",
      "variables.\n",
      "An alternate approach to developing a probability theory can begin with\n",
      "a sample space and random variables deﬁned on it. (Recall our deﬁnition\n",
      "of random variables did not require a deﬁnition of probability.) From this\n",
      "beginning, we can base a development of probability theory on expectation,\n",
      "rather than on a probability measure as we have done in this chapter. (This\n",
      "would be somewhat similar to our development of conditional probability from\n",
      "conditional expectation in Section 1.5.)\n",
      "In this approach we could deﬁne expectation in the usual way as an in-\n",
      "tegral, or we can go even further and deﬁne it in terms of characterizing\n",
      "properties. We characterize an expectation operator E on a random variable\n",
      "X (and X1 and X2) by four axioms:\n",
      "1. If X ≥0, then E(X) ≥0.\n",
      "2. If c is a constant in IR, then E(cX1 + X2) = cE(X1) + E(X2).\n",
      "3. E(1) = 1.\n",
      "4. If a sequence of random variables {Xn} increases monotonically to a limit\n",
      "{X}, then E(X) = limn→∞E(Xn).\n",
      "(In these axioms, we have assumed a scalar-valued random variable, although\n",
      "with some modiﬁcations, we could have developed the axioms in terms of\n",
      "random variables in IRd.) From these axioms, after deﬁning the probability of\n",
      "a set as\n",
      "Pr(A) = E(IA(ω)),\n",
      "we can develop the same probability theory as we did starting from a charac-\n",
      "terization of the probability measure. According to Hacking (1975), prior to\n",
      "about 1750 expectation was taken as a more basic concept than probability,\n",
      "and he suggests that it is more natural to develop probability theory from\n",
      "expectation. An interesting text that takes this approach is Whittle (2000).\n",
      "We have already seen a similar approach. This was in our development of\n",
      "conditional probability. In order to develop an idea of conditional probability\n",
      "and conditional distributions, we began by deﬁning conditional expectation\n",
      "with respect to a σ-ﬁeld, and then deﬁned conditional probability. While the\n",
      "most common kind of conditioning is with respect to a σ-ﬁeld, a conditional\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "141\n",
      "expectation with respect to a σ-lattice (see Deﬁnition 0.1.6 on page 695) can\n",
      "sometimes be useful; see Brunk (1963) and Brunk (1965).\n",
      "Transformations of Random Variables\n",
      "One of the most common steps in application of probability theory is to work\n",
      "out the distribution of some function of a random variable. The three meth-\n",
      "ods mentioned in Section 1.1.10 are useful. Of those methods, the change-of-\n",
      "variables method in equation (1.124), which includes the convolution form, is\n",
      "probably the one that can be used most often. In that method, we use the Ja-\n",
      "cobian of the inverse transformation. Why the inverse transformation? Think\n",
      "of the density as a diﬀerential; that is, it has a factor dx, so in the density for\n",
      "Y , we want a factor dy. Under pressure you may forget exactly how this goes,\n",
      "or want a quick conﬁrmation of the transformation. You should be able to\n",
      "construct a simple example quickly. An easy one is the right-triangular distri-\n",
      "bution; that is, the distribution with density pX(x) = 2x, for 0 < x < 1. Let\n",
      "y = 2x, so x = 1\n",
      "2y. Sketch the density of Y , and think of what transformations\n",
      "are necessary to get the expression pY (y) = 1\n",
      "2y, for 0 < y < 2.\n",
      "Structure of Random Variables\n",
      "Most useful probability distributions involve scalar random variables. The ex-\n",
      "tension to random vectors is generally straightforward, although the moments\n",
      "of vector random variables are quite diﬀerent in structure from that of the ran-\n",
      "dom variable itself. Nevertheless, we ﬁnd such random vectors as multivariate\n",
      "normal, Dirichlet, and multinomial very useful.\n",
      "Copulas provide useful methods for relating the distribution of a multivari-\n",
      "ate random variable to the marginal distributions of its components and for\n",
      "understanding, or at least modeling, the relationships among the components\n",
      "of the random variable. Balakrishnan and Lai (2009) use copulas extensively\n",
      "in discussions of a large number of bivariate distributions, many of which\n",
      "are extensions of familiar univariate distributions. Nelson (2006) provides an\n",
      "extensive coverage of copulas.\n",
      "The recent popularity of copulas in certain ﬁelds, such as ﬁnance, has\n",
      "probably led to some inappropriate use in probability models. See Mikosch\n",
      "(2006) and the discussion that follows his article.\n",
      "Most approaches to multivariate statistical analysis are based on random\n",
      "vectors. There are some cases in which random matrices are useful. The most\n",
      "common family of random matrices is the Wishart, whose range is limited\n",
      "to symmetric nonnegative deﬁnite matrices. An obvious way to construct a\n",
      "random matrices is by an iid random sample of random variables. The random\n",
      "sample approach, of course, would not increase the structural complexity of\n",
      "the covariance. If instead of a random sample, however, the random matrix\n",
      "would be constructed from random vectors that are not iid, its covariance\n",
      "would have a complicated structure. Kollo and von Rosen (2005) use random\n",
      "matrices, rather than random vectors, as the basis of multivariate analysis.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "142\n",
      "1 Probability Theory\n",
      "Characteristic Functions\n",
      "Characteristic functions play a major role in probability theory. Their use\n",
      "provides simple proofs for important facts, such as Geary’s theorem. (The\n",
      "proof given for this theorem given on page 189 is based on one by Lukacs.\n",
      "The theorem, with the additional requirement that moments of all orders\n",
      "exist, was ﬁrst proved by Geary (1936), using methods that are much more\n",
      "complicated.)\n",
      "Gnedenko and Kolmogorov (1954) utilize characteristic functions through-\n",
      "out their development of limiting distributions. Lukacs (1970) provides a thor-\n",
      "ough exposition of characteristic functions and their various applications, in-\n",
      "cluding use of methods of diﬀerential equations in characteristic function the-\n",
      "ory, as in the proof of Geary’s theorem.\n",
      "The Problem of Moments\n",
      "Thomas Stieltjes studied the problem of determining a nondecreasing function\n",
      "F , given a sequence of numbers ν0, ν1, ν2, . . . such that\n",
      "νk =\n",
      "Z ∞\n",
      "0\n",
      "xkdF (x),\n",
      "k = 0, 1, 2, . . .\n",
      "Stieltjes called this the “moment problem”. It is now often called the “Stieltjes\n",
      "problem”, and the related problem with limits of integration 0 and 1 is called\n",
      "the “Hausdorﬀproblem” and with limits −∞and ∞is called the “Hamburger\n",
      "problem”. These problems and the existence of a solution in each case are\n",
      "discussed by Shohat and Tamarkin (1943). (Although this is an older mono-\n",
      "graph, it is readily available in various reprinted editions.) In applications in\n",
      "probability, the existence of a solution is the question of whether there exists\n",
      "a probability distribution with a given set of moments.\n",
      "After existence of a solution, the next question is whether the solution is\n",
      "unique, or in our formulation of the problem, whether the moments uniquely\n",
      "determine the probability distribution.\n",
      "Many of the results concerning the moment problem involve probability\n",
      "distributions that are not widely used. Heyde (1963) was the ﬁrst to show\n",
      "that a particular interesting distribution, namely the lognormal distribution,\n",
      "was not uniquely determined by its moments. (This is Exercise 1.28.)\n",
      "Corollary 1.18.1 is due to Thomas Stieltjes who proved it without use of\n",
      "Theorem 1.18. Proofs and further discussion of the theorem and corollary can\n",
      "be found in Shohat and Tamarkin (1943).\n",
      "Sequences and Limit Theorems\n",
      "The various forms of the central limit theorem have a long history of both\n",
      "both the theory and the applications. Petrov (1995) provides an extensive\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "143\n",
      "coverage. Dudley (1999) discusses many of the intricacies of the theorems and\n",
      "gives extensions of the theory.\n",
      "The most important seminal result on the limiting distributions of extreme\n",
      "values was obtained by Fisher and Tippett (1928). von Mises (de Mis`es) (1939)\n",
      "and Gnedenko (1943) cleaned up some of the details, and Theorem 1.59 is es-\n",
      "sentially in the form stated in Gnedenko (1943). The limiting distributions of\n",
      "extreme values are discussed at some length by David and Nagaraja (2003),\n",
      "de Haan and Ferreira (2006), and Galambos (1978); and as mentioned in the\n",
      "text, a proof of Theorem 1.59, though not the same as given by Gnedenko is\n",
      "given by de Haan and Ferreira.\n",
      "De Finetti’s theorem allows the extension of certain results for independent\n",
      "sequences to similar results for exchangeable sequences. Taylor et al. (1985)\n",
      "prove a number of limit theorems for sums of exchangeable random variables.\n",
      "A quote from the Preface of Gnedenko and Kolmogorov (1954) is appro-\n",
      "priate:\n",
      "In the formal construction of a course in the theory of probability,\n",
      "limit theorems appear as a kind of superstructure over elementary\n",
      "chapters, in which all problems have ﬁnite purely arithmetical char-\n",
      "acter. In reality, however, the epistemological value of the theory of\n",
      "probability is revealed only by limit theorems. Moreover, without limit\n",
      "theorems it is impossible to understand the real content of the primary\n",
      "concept of all our sciences — the concept of probability. In fact, all\n",
      "epistemologic value of the theory of probability is based on this: that\n",
      "large-scale random phenomena in their collective action create strict,\n",
      "nonrandom regularity. The very concept of mathematical probability\n",
      "would be fruitless if it did not ﬁnd its realization in the frequency of\n",
      "occurrence of events under large-scale repetition of uniform conditions\n",
      "....\n",
      "Approximations and Expansions\n",
      "The central limit theorems provide a basis for asymptotic approximations in\n",
      "terms of the normal distribution. Serﬂing (1980) and Bhattacharya and Ranga Rao\n",
      "(1976) discuss a number of approximations.\n",
      "Other useful approximations are based on series representations of the\n",
      "PDF, CDF, or CF of the given distributions. Most of these series involve the\n",
      "normal PDF, CDF, or CF. Bhattacharya and Ranga Rao (1976) discuss a\n",
      "number of these series approximations. Hall (1992), especially Chapters 2 and\n",
      "3, provides an extensive coverage of series expansions. Power series expansions\n",
      "are not now used as often in probability theory as they once were.\n",
      "Cadlag Functions\n",
      "We referred to the common assumption for models of stochastic processes that\n",
      "the functions are cadlag with respect to the time argument. The term cadlag\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "144\n",
      "1 Probability Theory\n",
      "also applies to functions of arguments other than time that have this property.\n",
      "A CDF is a cadlag (or “c`al`ag”) function. Analysis of continuous CDFs is\n",
      "relatively straightforward, but analysis of CDFs with discrete jumps is more\n",
      "challenging. Derivation and proofs of convergence properties of CDFs (such\n",
      "as may be encountered it central limits of stochastic processes) are sometimes\n",
      "diﬃcult because of the discontinuities from the left. General results for a space\n",
      "of cadlag functions with a special metric have been developed. The space is\n",
      "called a Skorokhod space and the metric is called a Skorokhod metric. (Cadlag\n",
      "is synonymous with the English-derived acronyms RCCL, “right continuous\n",
      "[with] left limits”, and corlol, “continuous on [the] right limit on [the] left”,\n",
      "but there seems to be a preference for the French-derived acronym.)\n",
      "Markov Chains\n",
      "There are many other interesting properties of Markov chains that follow\n",
      "from various properties of nonnegative matrices (see Gentle (2007)). For more\n",
      "information on the properties of Markov chains, we refer the interested reader\n",
      "to the second edition of the classic text on Markov chains, Meyn and Tweedie\n",
      "(2009).\n",
      "There are many special Markov chains that are motivated by applications.\n",
      "Branching process, for example, have applications in modeling such distinct\n",
      "areas biological populations and elementary particles. Harris (1989) developed\n",
      "many properties of such processes, and Athreya and Ney (1972) extended the\n",
      "theory. Some Markov chains are martingales, but of course not all are; con-\n",
      "versely, not all martingales are Markov chains (see Example 1.36).\n",
      "Martingales\n",
      "Many of the basic ideas in martingale theory were developed by Joseph Doob,\n",
      "who gave this rather odd name to a class of stochastic processes after a type\n",
      "of betting system. Doob (1953) is still the classic text on stochastic processes\n",
      "generally and martingales in particular. Hall and Heyde (1980) cover many\n",
      "important limit theorems about martingales. Some of the most important ap-\n",
      "plications of martingale theory are in ﬁnancial modeling, in which a martin-\n",
      "gale model is equivalent to a no-arbitrage assumption. See Baxter and Rennie\n",
      "(1996) for applications of martingale theory in options pricing.\n",
      "Empirical Processes\n",
      "The standard texts on empirical processes are Shorack and Wellner (2009)\n",
      "and, especially for limit theorems relating to them, Dudley (1999).\n",
      "Massart (1990) *** a tight constant in the Dvoretzky-Kiefer-Wolfowitz\n",
      "inequality.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "145\n",
      "Additional References for Chapter 1\n",
      "Among the references on probability in the bibliography beginning on page 873,\n",
      "some have not been mentioned speciﬁcally in the text. These include the gen-\n",
      "eral references on probability theory: Ash and Doleans-Dade (1999), Athreya and Lahiri\n",
      "(2006), Barndorﬀ-Nielson and Cox (1994), Billingsley (1995), Breiman (1968),\n",
      "Chung (2000), Dudley (2002), Feller (1957) and Feller (1971), Gnedenko\n",
      "(1997), Gnedenko and Kolmogorov (1954), Gut (2005), and Pollard (2003).\n",
      "The two books by Feller, which are quite diﬀerent from each other but\n",
      "which together provide a rather comprehensive coverage of probability theory,\n",
      "the book by Breiman, and the book by Chung (the ﬁrst edition of 1968) were\n",
      "among the main books I used in learning about probability years ago. They\n",
      "may be somewhat older than the others in the bibliography, but I’d probably\n",
      "still start out with them.\n",
      "Exercises\n",
      "1.1. For the measurable space (IR, B), show that the collection of all open\n",
      "subsets of IR is a determining class for probability measures on (IR, B).\n",
      "1.2. Prove Theorem 1.1.\n",
      "1.3. For any theorem you should think carefully about the relevance of the\n",
      "hypotheses, and whenever appropriate consider the consequences of weak-\n",
      "ening the hypotheses. For the weakened hypotheses, you should construct\n",
      "a counterexample that shows the relevance of the omitted portion of the\n",
      "hypotheses. In Theorem 1.1, omit the condition that ∀i ∈I, A, B ∈Ci ⇒\n",
      "A ∩B ∈Ci, and give a counterexample to show that without this, the\n",
      "hypothesis is not suﬃcient.\n",
      "1.4. Prove Theorem 1.3.\n",
      "1.5. Let Ω= {1, 2, . . .} and let F be the collection of all subsets of Ω. Prove\n",
      "or disprove:\n",
      "P (A) = lim\n",
      "n→∞inf #(A ∩{1, . . ., n})\n",
      "n\n",
      ",\n",
      "where # is the counting measure, is a probability measure on (Ω, F).\n",
      "1.6. Let A, B, and C be independent events. Show that if D is any event in\n",
      "σ({B, C}) then A and D are independent.\n",
      "1.7. Prove Theorem 1.4.\n",
      "1.8. Let X and Y be random variables. Prove that σ(X) ⊆σ(X, Y ).\n",
      "1.9. Given a random variable X deﬁned on the probability space (Ω, F, P ),\n",
      "show that P ◦X−1 is a probability measure.\n",
      "1.10. Show that X\n",
      "a.s.\n",
      "= Y =⇒X\n",
      "d= Y .\n",
      "1.11. Write out a proof of Theorem 1.6.\n",
      "1.12. Let F (x) be the Cantor function (0.1.30) (page 723) extended below the\n",
      "unit interval to be 0 and extended above the unit interval to be 1, as\n",
      "indicated in the text.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "146\n",
      "1 Probability Theory\n",
      "a) Show that F (x) is a CDF.\n",
      "b) Show that the distribution associated with this CDF does not have a\n",
      "PDF wrt Lebesgue measure. (What is the derivative?)\n",
      "c) Does the distribution associated with this CDF have a PDF wrt count-\n",
      "ing measure?\n",
      "d) Is the probability measure associated with this random variable dom-\n",
      "inated by Lebesgue measure? by the counting measure?\n",
      "Let X be a random variable with this distribution.\n",
      "e) What is Pr(X = 1/3)?\n",
      "f) What is Pr(X ≤1/3)?\n",
      "g) What is Pr(1/3 ≤X ≤2/3)?\n",
      "1.13. a) Show that F in equation (1.29) is a CDF.\n",
      "b) Show that if each Fi in equation (1.29) is dominated by Lebesgue\n",
      "measure, then F is dominated by Lebesgue measure.\n",
      "c) Show that if each Fi in equation (1.29) is dominated by the counting\n",
      "measure, then F is dominated by the counting measure.\n",
      "1.14. Write out a proof of Theorem 1.8.\n",
      "1.15. Write out a proof of Theorem 1.9.\n",
      "1.16. Write out a proof of Theorem 1.10.\n",
      "1.17. Write out a proof of Theorem 1.11.\n",
      "1.18. Write out a proof of Theorem 1.12.\n",
      "1.19. a) Show that the random variables R1, R2, R3, R4 in Example 1.6 are\n",
      "exchangeable.\n",
      "b) Use induction to show that the sequence R1, R2, . . . in Example 1.6 is\n",
      "exchangeable.\n",
      "1.20. Give an example in which the linearity of the expectation operator (equa-\n",
      "tion (1.38)) breaks down.\n",
      "1.21. Write out a proof of Theorem 1.13.\n",
      "1.22. Show that if the scalar random variables X and Y are independent, then\n",
      "Cov(X, Y ) = Cor(X, Y ) = 0.\n",
      "1.23. a) Let X be a random variable such that it is not the case that X = E(X)\n",
      "a.s. Prove V(X) > 0.\n",
      "b) Let X = (X1, . . ., Xd) such that V(Xi) < ∞, and assume that it is\n",
      "not the case that Xi = E(Xi) a.s. for any i nor that ∃aj, bj for any\n",
      "element Xi of the vector X such that\n",
      "Xi =\n",
      "X\n",
      "j̸=i\n",
      "(aj + bjXj)\n",
      "a.s.\n",
      "Prove that V(X) is full rank.\n",
      "1.24. Show that the second raw moment E(X2) for the Cauchy distribution\n",
      "(equation (1.37)) does not exist.\n",
      "1.25. Expected values and quantile functions.\n",
      "a) Write a formal proof of equation (1.44).\n",
      "b) Extend equation (1.44) to E(X), where g is a bijective Borel function\n",
      "of X.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "147\n",
      "1.26. Expected values.\n",
      "a) Write a formal proof that equation (1.46) follows from the conditions\n",
      "stated.\n",
      "b) Write a formal proof that equation (1.48) follows from the conditions\n",
      "stated.\n",
      "c) Write a formal proof that equation (1.49) follows from the conditions\n",
      "stated.\n",
      "1.27. Let X be a random variable. Show that the map\n",
      "PX : B 7→Pr(X ∈B),\n",
      "where B is a Borel set, is a probability measure on the Borel σ-ﬁeld.\n",
      "1.28. Let X be a random variable with PDF\n",
      "pX(x) =\n",
      "1\n",
      "√\n",
      "2π\n",
      "1\n",
      "x exp(−(log(x))2/2)I¯IR+(x),\n",
      "(1.290)\n",
      "and let Y be a random variable with PDF\n",
      "pY (y) = pX(y)(1 + α sin(2π log(y)))I¯IR+(y),\n",
      "where α is a constant and 0 < |α| ≤1.\n",
      "Notice the similarity of the PDF of Y to the PDF given in equation (1.53).\n",
      "a) Show that X and Y have diﬀerent distributions.\n",
      "b) Show that for r = 1, 2, . . ., E(Xr) = E(Y r).\n",
      "c) Notice that the PDF (1.290) is that of the lognormal distribution,\n",
      "which, of course, is an absolutely continuous distribution. Now con-\n",
      "sider the discrete random variable Ya whose distribution, for given\n",
      "a > 0, is deﬁned by\n",
      "Pr(Ya = aek) = cae−k2/2/ak,\n",
      "for k = 0, 1, 2, . . .,\n",
      "where ca is an appropriate normalizing constant. Show that this dis-\n",
      "crete distribution has the same moments as the lognormal.\n",
      "Hint: First identify the support of this distribution. Then multiply the\n",
      "reciprocal of the rth moment of the lognormal by the rth of Ya.\n",
      "1.29. a) Prove equation (1.72):\n",
      "V(aX + Y ) = a2V(X) + V(Y ) + 2aCov(X, Y ).\n",
      "b) Prove equation (1.73):\n",
      "Cor(aX +Y, X +Z) = aV(X)+aCov(X, Z)+Cov(X, Y )+Cov(Y, Z).\n",
      "1.30. Prove the converse portion of Sklar’s theorem (Theorem 1.19).\n",
      "1.31. Let X and Y be random variables with (marginal) CDFs PX and PY\n",
      "respectively, and suppose X and Y are connected by the copula CXY .\n",
      "Prove:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "148\n",
      "1 Probability Theory\n",
      "Pr(max(X, Y ) ≤t) = CXY (PX(t), PY (t))\n",
      "and\n",
      "Pr(min(X, Y ) ≤t) = PX(t) + PY (t) −CXY (PX(t), PY (t)).\n",
      "1.32. a) Let X be a random variable that is normally distributed with mean\n",
      "µ and variance σ2. Determine the entropy of X.\n",
      "b) Let Y be a random variable that is distributed as beta(α, β). Deter-\n",
      "mine the entropy of Y in terms of the digamma function (page 865).\n",
      "Make plots of the entropy for various values of α and β. An R function\n",
      "that evaluates the entropy is\n",
      "entropy<-function(a,b){\n",
      "lbeta(a,b)-\n",
      "(a-1)*(digamma(a)-digamma(a+b))-\n",
      "(b-1)*(digamma(b)-digamma(a+b))\n",
      "}\n",
      "1.33. a) For the scalar random variable X prove equations (1.84) and (1.96).\n",
      "b) For the random variable X prove equations (1.97) and (1.98).\n",
      "c) Given the random variables X and Y with CF ϕX,Y (t1, t2) write out\n",
      "Cov(X, Y ) in terms of derivatives of the CF.\n",
      "1.34. a) Show that the moment-generating function does not exist for a Cauchy\n",
      "distribution.\n",
      "b) Determine the characteristic function for a Cauchy distribution, and\n",
      "show that it is not diﬀerentiable at 0.\n",
      "1.35. In Example 1.10 we showed that the moment-generating function does not\n",
      "exist for a lognormal distribution. Determine the characteristic function\n",
      "for a lognormal distribution. Simplify the expression.\n",
      "1.36. Consider the the distribution with PDF\n",
      "p(x) =\n",
      "c\n",
      "2x2 log(|x|) I{±2,±3,...}(x).\n",
      "Show that the characteristic function has a ﬁnite ﬁrst derivative at 0, yet\n",
      "that the ﬁrst moment does not exist (Zygmund, 1947).\n",
      "1.37. Write an expression similar to equation (1.96) for the cumulants, if they\n",
      "exist, in terms of the cumulant-generating function.\n",
      "1.38. Show that equations (1.99), (1.100), and (1.101) are correct.\n",
      "1.39. Show that equation (1.107) is correct.\n",
      "1.40. a) Let X and Y be iid N(0, 1). Work out the PDF of (X −Y )2/Y 2.\n",
      "b) Let X1, . . ., Xn and Y1, . . ., Yn be iid N(0, 1). Work out the PDF of\n",
      "P\n",
      "i(Xi −Yi)2/ P\n",
      "i Y 2\n",
      "i .\n",
      "1.41. Show that the distributions of the random variables X and Y in Exam-\n",
      "ple 1.7 are the same as, respectively, the ratio of two standard exponential\n",
      "random variables and the ratio of two standard normal random variables.\n",
      "1.42. Show that equation (1.132) is correct.\n",
      "1.43. Stable distributions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "149\n",
      "a) Show that an inﬁnitely divisible family of distributions is stable.\n",
      "b) Show that the converse of the previous statement is not true. (Hint:\n",
      "Show that the Poisson family is a family of distributions that is in-\n",
      "ﬁnitely divisible, but not stable.)\n",
      "c) Show that the deﬁnition of stability based on equation (1.136) is equiv-\n",
      "alent to Deﬁnition 1.33.\n",
      "d) Let X, X1, X2 be as in Deﬁnition 1.33. Show that Y = X1 −X2 has a\n",
      "stable distribution, and show that the distribution of Y is symmetric\n",
      "about 0. (Y has a symmetric stable distribution).\n",
      "e) Show that the normal family of distributions is stable with character-\n",
      "istic exponent of 2.\n",
      "f) Show that the standard Cauchy distribution is stable with character-\n",
      "istic exponent of 1.\n",
      "1.44. Prove Theorem 1.25.\n",
      "1.45. Provide a heuristic justiﬁcation for equation (1.138).\n",
      "1.46. Show that the PDF of the joint distribution of all order statistic in equa-\n",
      "tion (1.140) is equal to the PDF of the joint distribution of all of the\n",
      "(unordered) random variables, Q f(xi).\n",
      "1.47. Show that the Yi in Example 1.19 on page 64 are independent of both\n",
      "X(1) and X(n).\n",
      "1.48. a) Let X(1), . . ., X(n) be the order statistics in a sample of size n, let\n",
      "µ(k:n) = E(X(k:n)), and let X be a random variable with the distribu-\n",
      "tion of the sample. Show that µ(k:n) exists and is ﬁnite if E(X) exists\n",
      "and is ﬁnite.\n",
      "b) Let n be an odd integer, n = 2k + 1, and consider a sample of size n\n",
      "from a Cauchy distribution with PDF fX = 1/(π(1 +(x −θ)2)). Show\n",
      "that the PDF of X(k+1), the sample median, is\n",
      "fX(k+1) =\n",
      "n!\n",
      "(k!)2π\n",
      "\u00121\n",
      "4 −1\n",
      "π2 (arctan(x −θ))2\n",
      "\u0013k\n",
      "1\n",
      "1 + (x −θ)2 .\n",
      "What is µ(k:n) in this case?\n",
      "1.49. a) Prove equation (1.142).\n",
      "b) Prove the following generalization of equation (1.142):\n",
      "(n −k)E\n",
      "\u0010\n",
      "Xp\n",
      "(k:n)\n",
      "\u0011\n",
      "+ kE\n",
      "\u0010\n",
      "Xp\n",
      "(k+1:n)\n",
      "\u0011\n",
      "= nE\n",
      "\u0010\n",
      "Xp\n",
      "(k:n−1)\n",
      "\u0011\n",
      ".\n",
      "See David and Nagaraja (2003).\n",
      "1.50. Given the sequence of events A1, A2, . . ., show that a tail event of {An}\n",
      "occurs inﬁnitely often.\n",
      "1.51. Given the random variables X1, X2, . . . and X on a common probability\n",
      "space. For m = 1, 2, . . ., and for any ϵ > 0, let Am,ϵ be the event that\n",
      "∥Xm −X∥> ϵ. Show that almost sure convergence of {Xn} to X is\n",
      "equivalent to\n",
      "lim\n",
      "n→∞Pr (∪∞\n",
      "m=nAm,ϵ) = 0,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "150\n",
      "1 Probability Theory\n",
      "for every ϵ > 0.\n",
      "Hint: For j = 1, 2, . . ., consider the events\n",
      "Bj = ∪∞\n",
      "n=1 ∩∞\n",
      "m=n Ac\n",
      "m,1/j.\n",
      "1.52. Show that a convergence-determining class is a determining class.\n",
      "1.53. a) Show that the collection of all ﬁnite open intervals in IR that do not\n",
      "include 0 (as in Example 1.20) is a determining class for probability\n",
      "measures on (IR, B). (Compare Exercise 1.1.)\n",
      "b) Show that the collection of all ﬁnite open intervals in IR is a convergence-\n",
      "determining class for probability measures on (IR, B).\n",
      "1.54. a) Give an example of a sequence of random variables that converges in\n",
      "probability to X, but does not converge to X a.s.\n",
      "b) Give an example of a sequence of random variables that converges in\n",
      "probability to X, but does not converge to X in second moment.\n",
      "1.55. Prove Theorem 1.31.\n",
      "1.56. a) Weaken the hypothesis in Theorem 1.31 to Xn\n",
      "d→X, and give a\n",
      "counterexample.\n",
      "b) Under what condition does convergence in distribution imply conver-\n",
      "gence in probability?\n",
      "1.57. Let X1, . . ., Xn\n",
      "iid\n",
      "∼Bernoulli(π). Let Yn = Pn\n",
      "i=1 Xi. Show that as n →∞\n",
      "and π →0 in such a way that nπ →θ > 0, Yn\n",
      "d→Z where Z has a Poisson\n",
      "distribution with parameter θ.\n",
      "1.58. Given a sequence of scalar random variables {Xn}, prove that if\n",
      "E((Xn −c)2) →0\n",
      "then Xn converges in probability to c.\n",
      "1.59. A suﬃcient condition for a sequence Xn of random variables to converge\n",
      "to 0 a.s. is that, for every ϵ > 0, P∞\n",
      "n=1 Pr(|Xn| > ϵ) < ∞. Let U be\n",
      "uniformly distributed over (0, 1) and deﬁne\n",
      "Xn =\n",
      "\u001a 1 if U < 1\n",
      "n\n",
      "0 otherwise.\n",
      "Use this sequence to show that the condition P∞\n",
      "n=1 Pr(|Xn| > ϵ) < ∞is\n",
      "not a necessary condition for the sequence Xn to converge to 0 a.s.\n",
      "1.60. a) Show that if Xn\n",
      "d→X for any random variable X, then Xn ∈OP(1).\n",
      "b) Show that if Xn ∈oP(1), then also Xn ∈OP(1).\n",
      "1.61. Show the statements (1.165) through (1.174) (page 84) are correct.\n",
      "1.62. Show the statements (1.175) and (1.176) are correct.\n",
      "1.63. Show that the relationship in statement (1.177) is correct.\n",
      "1.64. Show that equation (1.196) for the second-order delta method follows from\n",
      "Theorem 1.47 with m = 2.\n",
      "1.65. a) Prove equation (1.206).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "151\n",
      "b) Show that the expression in equation (1.206) is a CDF.\n",
      "1.66. Prove Theorem 1.49.\n",
      "1.67. Show that Lindeberg’s condition, equation (1.221), implies Feller’s condi-\n",
      "tion, equation (1.222).\n",
      "1.68. The inequalities in Appendix B are stated in terms of unconditional prob-\n",
      "abilities and expectations. They all hold as well for conditional probabili-\n",
      "ties and expectations. In the following, assume the basic probability space\n",
      "(Ω, F, P ). Assume that A is a sub-σ-ﬁeld of F and Z is a random variable\n",
      "in the given probability space. Prove:\n",
      "a) An extension of Theorem B.3.1:\n",
      "For ϵ > 0, k > 0, and r.v. X ∋E(|X|k) exists,\n",
      "Pr(|X| ≥ϵ | A) ≤1\n",
      "ϵk E\n",
      "\u0000|X|k | A\n",
      "\u0001\n",
      ".\n",
      "b) An extension of Theorem B.4.1:\n",
      "For f a convex function over the support of the r.v. X (and all expec-\n",
      "tations shown exist),\n",
      "f(E(X | Z)) ≤E(f(X) | Z).\n",
      "c) An extension of Corollary B.5.1.4:\n",
      "If the second moments of X and Y are ﬁnite, then\n",
      "\u0000Cov(X, Y | Z)\n",
      "\u00012 ≤V(X | Z) V(Y | Z).\n",
      "1.69. a) Show that the alternative conditions given in equations (1.232), (1.233),\n",
      "and (1.234) for deﬁning conditional expectation are equivalent.\n",
      "b) Show that equation (1.242) follows from equation (1.236).\n",
      "1.70. Show that if E(X) exists, then so does E(X|B) for any event B such that\n",
      "Pr(B) ̸= 0.\n",
      "1.71. Let X and Y be random variables over the same probability space. Show\n",
      "that σ(X|Y ) ⊆σ(X). (Compare equation (0.1.7).)\n",
      "1.72. Prove Theorem 1.60.\n",
      "1.73. Modify your proof of Theorem 1.13 (Exercise 1.21) to prove Theorem 1.63.\n",
      "1.74. Let Tn be a function of the iid random variables X1, . . ., Xn, with V(Xi) <\n",
      "∞, V(Tn) < ∞, and V(E(Tn|Xi)) > 0. Now let eTn be the projection of\n",
      "Tn onto X1, . . ., Xn. Derive equations (1.254) and (1.255):\n",
      "E( eTn) = E(Tn),\n",
      "and\n",
      "V( eTn) = nV(E(Tn|Xi)).\n",
      "1.75. Prove: The random variables X and Y are independent iﬀthe conditional\n",
      "distribution of X given Y (or of Y given X) equals the marginal distribu-\n",
      "tion of X (or of Y ) (Theorem 1.66).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "152\n",
      "1 Probability Theory\n",
      "1.76. Prove Theorem 1.67.\n",
      "1.77. Let X be a nonnegative integrable random variable on (Ω, F, P ) and let\n",
      "A ⊆F be a σ-ﬁeld. Prove that E(X|A) =\n",
      "R ∞\n",
      "0\n",
      "Pr(X > t|A) dt a.s.\n",
      "1.78. Show that equation (1.266) is correct.\n",
      "1.79. Let {Xn : n = 1, 2, . . .} be a sequence of iid random variables with mean\n",
      "0 and ﬁnite variance σ2. Let Sn\n",
      "a.s.\n",
      "= X1 + · · · + Xn, and let\n",
      "Yn\n",
      "a.s.\n",
      "= S2\n",
      "n −nσ2.\n",
      "Prove that {Yn} is a martingale with respect to {Sn : n = 1, 2, . . .}.\n",
      "1.80. Let {Zi} be an iid sequence of Bernoulli(π) random variables. Let X0 = 0,\n",
      "and for n = 1, 2, . . ., let\n",
      "Xn\n",
      "a.s.\n",
      "= Xn−1 + 2Zn −1,\n",
      "and let\n",
      "Yn\n",
      "a.s.\n",
      "= ((1 −π)/π)Xn.\n",
      "Show that {Yn : n = 1, 2, 3, . . .} is a martingale with respect to {Xn :\n",
      "n = 1, 2, 3, . . .}. (This is sometimes called de Moivre’s martingale.)\n",
      "1.81. Show that {Xn : n = 1, 2, 3, . . .} of equation (1.184) is a martingale.\n",
      "1.82. Show that {Yn : n = 1, 2, 3, . . .} of Polya’s urn process (Example 1.33,\n",
      "page 131) is a martingale with respect to {Xn}.\n",
      "1.83. Show that the likelihood-ratio martingale, equation (1.281), converges al-\n",
      "most surely to 0.\n",
      "Hint: Take logarithms and use Jensen’s inequality and equation (1.63).\n",
      "1.84. Show that if {W(t)\n",
      ":\n",
      "t ∈[0, ∞[} is a Bachelier-Wiener process, then\n",
      "W 2(t) −t is a martingale.\n",
      "1.85. Show that Doob’s martingale inequality (1.284) implies Robbins’s likeli-\n",
      "hood ratio martingale inequality (1.282).\n",
      "1.86. Let {Mn} be a martingale, and let {Cn} be adapted to {σ(Mt : t ≤n)}.\n",
      "Let f\n",
      "M0 = 0, and for n ≥1, let\n",
      "f\n",
      "Mn =\n",
      "n\n",
      "X\n",
      "j=1\n",
      "Cj(Mj −Mj−1).\n",
      "Show that f\n",
      "Mn is a martingale.\n",
      "The sequence f\n",
      "Mn is called the martingale transform of {Mn} by {Cn}.\n",
      "1.87. Let X1, X2, . . . be a sequence of independent random variables over a\n",
      "common probability space such that for each E(X2\n",
      "i ) < ∞. Show that the\n",
      "sequence of partial sums\n",
      "Yn =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(Xi −E(Xi))\n",
      "is a martingale.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "153\n",
      "1.88. Let X be a random variable that is normally distributed with mean µ and\n",
      "variance σ2. Show that the entropy of X is at least as great as the entropy\n",
      "of any random variable with ﬁnite mean µ and ﬁnite variance σ2 and hav-\n",
      "ing a PDF that is dominated by Lebesgue measure. (See Exercise 1.32a.)\n",
      "1.89. Show that the axioms for coherency given on page 139 deﬁne a linear\n",
      "ordering, that is, a total ordering, on A. (See page 620.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2\n",
      "Distribution Theory and Statistical Models\n",
      "Given a measurable space, (Ω, F), diﬀerent choices of a probability measure\n",
      "lead to diﬀerent probability triples, (Ω, F, P ). A set of measures P = {P }\n",
      "associated with a ﬁxed (Ω, F) is called a family of distributions. Families can\n",
      "be deﬁned in various ways. For example, for Ωa real interval and F = BΩ, a\n",
      "very broad family is Pc = {P : P ≪ν}, where ν is the Lebesgue measure. An\n",
      "example of a very speciﬁc family for Ω= {0, 1} and F = 2Ωis the probability\n",
      "measure Pπ({1}) = π and Pπ({0}) = 1 −π. The probability measures in this\n",
      "family, the Bernoulli distributions, are dominated by the counting measure.\n",
      "Certain families of distributions have proven to be very useful as models\n",
      "of observable random processes. Familiar families include the normal or Gaus-\n",
      "sian family of distributions, the Poisson family of distributions, the binomial\n",
      "family of distributions, and so on. A list of some of the important families of\n",
      "distributions is given in Appendix A, beginning on page 835. Occasionally, as\n",
      "part of a parametric approach, transformations on the observations are used\n",
      "so that a standard distribution, such as the normal, models the phenomena\n",
      "better.\n",
      "A semi-parametric approach uses broader families whose distribution func-\n",
      "tions can take on a much wider range of forms. In this approach, a diﬀerential\n",
      "equation may be developed to model a limiting case of some discrete frequency\n",
      "model. The Pearson system is an example of this approach (in which the basic\n",
      "diﬀerential equation arises as a limiting case of a hypergeometric distribution).\n",
      "Other broad families of distributional forms have been developed by Johnson,\n",
      "by Burr, and by Tukey. The objective is to be able to represent a wide range of\n",
      "distributional properties (mean, variance, skewness, shape, etc.) with a small\n",
      "number of parameters, and then to ﬁt a speciﬁc case by proper choice of these\n",
      "parameters.\n",
      "Statistical inference, which is the main topic of this book, can be thought\n",
      "of as a process whose purpose is to use observational data within the context of\n",
      "an assumed family of probability distributions P to infer that the observations\n",
      "are associated with a subfamily PH ⊆P, or else to decide that the assumed\n",
      "family is not an adequate model for the observed data. For example, we may\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "156\n",
      "2 Distribution Theory and Statistical Models\n",
      "assume that the data-generating process giving rise to a particular set of data\n",
      "is in the Poisson family of distributions, and based on our methods of inference\n",
      "decide it is the Poisson distribution with θ = 5. (See Appendix A for how θ\n",
      "parameterizes the Poisson family.)\n",
      "A very basic distinction is the nature of the values the random variable\n",
      "assumes. If the set of values is countable, we call the distribution “discrete”;\n",
      "otherwise, we call it “continuous”.\n",
      "With a family of probability distributions is associated a random variable\n",
      "space whose properties depend on those of the family. For example, the ran-\n",
      "dom variable space associated with a location-scale family (deﬁned below) is\n",
      "a linear space.\n",
      "Discrete Distributions\n",
      "The probability measures of discrete distributions are dominated by the count-\n",
      "ing measure.\n",
      "One of the simplest types of discrete distribution is the discrete uniform.\n",
      "In this distribution, the random variable assumes one of m distinct values\n",
      "with probability 1/m.\n",
      "Another basic discrete distribution is the Bernoulli, in which random vari-\n",
      "able takes the value 1 with probability π and the value 0 with probability\n",
      "1 −π. There are two common distributions that arise from the Bernoulli:\n",
      "the binomial, which is the sum of n iid Bernoullis, and the negative binomial,\n",
      "which is the number of Bernoulli trials before r 1’s are obtained. A special ver-\n",
      "sion of the negative binomial with r = 1 is called the geometric distribution.\n",
      "A generalization of the binomial to sums of multiple independent Bernoullis\n",
      "with diﬀerent values of π is called the multinomial distribution.\n",
      "The random variable in the Poisson distribution takes the number of events\n",
      "within a ﬁnite time interval that occur independently and with constant prob-\n",
      "ability in any inﬁnitesimal period of time.\n",
      "A hypergeometric distribution models the number of selections of a certain\n",
      "type out of a given number of selections.\n",
      "A logarithmic distribution (also called a logarithmic series distribution)\n",
      "models phenomena with probabilities that fall oﬀlogarithmically, such as\n",
      "ﬁrst digits in decimal values representing physical measures.\n",
      "Continuous Distributions\n",
      "The probability measures of continuous distributions are dominated by the\n",
      "Lebesgue measure.\n",
      "Continuous distributions may be categorized ﬁrst of all by the nature of\n",
      "their support. The most common and a very general distribution with a ﬁnite\n",
      "interval as support is the beta distribution. Although we usually think of\n",
      "the support as [0, 1], it can easily be scaled into any ﬁnite interval [a, b]. Two\n",
      "parameters determine the shape of the PDF. It can have a U shape, a J shape,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2 Distribution Theory and Statistical Models\n",
      "157\n",
      "a backwards J shape, or a unimodal shape with the mode anywhere in the\n",
      "interval and with or without an inﬂection point on either side of the mode. A\n",
      "special case of the beta has a constant PDF over the support.\n",
      "Another distribution with a ﬁnite interval as support is the von Mises\n",
      "distribution, which provides useful models of a random variable whose value\n",
      "is to be interpreted as an angle.\n",
      "One of the most commonly-used distributions of all is the normal or Gaus-\n",
      "sian distribution. Its support is ]−∞, ∞[. There are a number of distributions,\n",
      "called stable distributions, that are similar to the normal. The normal has a\n",
      "multivariate extension that is one of the simplest multivariate distributions,\n",
      "in the sense that the second-order moments have intuitive interpretations. It\n",
      "is the prototypic member of an important class of multivariate distributions,\n",
      "the elliptically symmetric family.\n",
      "From the normal distribution, several useful sampling distributions can be\n",
      "derived. These include the chi-squared, the t, the F, and the Wishart, which\n",
      "come from sample statistics from a normal distribution with zero mean. There\n",
      "are noncentral analogues of these that come from statistics that have not been\n",
      "centered on zero. Two other common distributions that are related to the\n",
      "normal are the lognormal and the inverse Gaussian distribution. These are\n",
      "related by applications. The lognormal is an exponentiated normal. (Recall\n",
      "two interesting properties of the lognormal: the moments do not determine\n",
      "the distribution (Exercise 1.28) and although the moments of all orders exist,\n",
      "the moment-generating function does not exist (Example 1.10).) The inverse\n",
      "Gaussian models the length of time required by Brownian motion to achieve a\n",
      "certain distance, while the normal distribution models the distance achieved\n",
      "in a certain time.\n",
      "Two other distributions that relate to the normal are the inverted chi-\n",
      "squared and the inverted Wishart. They are useful because they are conjugate\n",
      "priors and they are also related to the reciprocal or inverse of statistics formed\n",
      "from samples from a normal. They are also called “inverse” distributions, but\n",
      "their origins are not at all related to that of the standard inverse Gaussian\n",
      "distribution.\n",
      "Continuous Distributions with Point Masses\n",
      "We can form a mixture of a continuous distribution and a discrete distribution.\n",
      "Such a distribution is said to be continuous with point masses. The probability\n",
      "measure is not dominated by either a counting measure or Lebesgue measure.\n",
      "A common example of such a distribution is the ϵ-mixture distribution, whose\n",
      "CDF is given in equation (2.45) on page 194.\n",
      "Entropy\n",
      "It is often of interest to know the entropy of a given probability distribution. In\n",
      "some applications we seek distributions with maximum or “large” entropy to\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "158\n",
      "2 Distribution Theory and Statistical Models\n",
      "use as probability models. It is interesting to note that of all distributions with\n",
      "given ﬁrst and second moments and having a PDF dominated by Lebesgue\n",
      "measure, the one with maximum entropy is the normal (Exercise 1.88).\n",
      "Characterizing a Family of Distributions\n",
      "A probability family or family of distributions, P = {Pθ, θ ∈Θ}, is a set\n",
      "of probability distributions of a random variable that is deﬁned over a given\n",
      "sample space Ω. The index of the distributions may be just that, an arbitrary\n",
      "index in some given set Θ which may be uncountable, or it may be some\n",
      "speciﬁc point in a given set Θ in which the value of θ carries some descriptive\n",
      "information about the distribution; for example, θ may be a 2-vector in which\n",
      "one element is the mean of the distribution and the other element is the\n",
      "variance of the distribution.\n",
      "The distribution functions corresponding to the members of most inter-\n",
      "esting families of distributions that we will discuss below do not constitute a\n",
      "distribution function space as deﬁned on page 754. This is because mixtures of\n",
      "distributions in most interesting families of distributions are not members of\n",
      "the same family. That is, distributions deﬁned by convex linear combinations\n",
      "of CDFs generally are not members of the same family of distributions. On\n",
      "the other hand, often linear combinations of random variables do have distri-\n",
      "butions in the same family of distributions as that of the individual random\n",
      "variables. (The sum of two normals is normal; but a mixture of two normals\n",
      "is not normal.) Table 1.1 on page 58 lists a number of families of distributions\n",
      "that are closed under addition of independent random variables.\n",
      "Likelihood Functions\n",
      "The problem of fundamental interest in statistics is to identify a particular\n",
      "distribution within some family of distributions, given observed values of the\n",
      "random variable. Hence, in statistics, we may think of θ or Pθ as a variable.\n",
      "A likelihood function is a function of that variable.\n",
      "Deﬁnition 2.1 (likelihood function)\n",
      "Given a PDF fθ, which is a function whose argument is a value of a random\n",
      "variable x, we deﬁne a likelihood function as a function of θ for the ﬁxed x:\n",
      "L(θ | x) = fθ(x).\n",
      "The PDF fθ(x) is a function whose argument is a value of a random variable\n",
      "x for a ﬁxed θ; the likelihood function L(θ | x) is a function of θ for a ﬁxed x;\n",
      "see Figure 1.2 on page 20.\n",
      "In statistical applications we may be faced with the problem of choosing\n",
      "between two distributions Pθ1 and Pθ2. For a given value of x, we may base\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2 Distribution Theory and Statistical Models\n",
      "159\n",
      "our choice on the two likelihoods, L(θ1 | x) and L(θ2 | x), perhaps using the\n",
      "likelihood ratio\n",
      "λ(θ1, θ2|x) = L(θ2 | x)\n",
      "L(θ1 | x).\n",
      "We have seen in equation (1.65) that the expectation of the likelihood\n",
      "ratio, taken wrt to distribution in the denominator, is 1.\n",
      "Parametric Families, Parameters, and Parameter Spaces\n",
      "In Deﬁnition 1.13, we say that a family of distributions on a measurable space\n",
      "(Ω, F) with probability measures Pθ for θ ∈Θ is called a parametric family if\n",
      "Θ ⊆IRd for some ﬁxed positive integer d and θ fully determines the measure.\n",
      "In that case, we call θ the parameter and Θ the parameter space.\n",
      "A family that cannot be indexed in this way is called a nonparametric\n",
      "family. In nonparametric methods, our analysis usually results in some general\n",
      "description of the distribution, such as that the CDF is continuous or that the\n",
      "distribution has ﬁnite moments or is continuous, rather than in a speciﬁcation\n",
      "of the distribution.\n",
      "The type of a family of distributions depends on the parameters that\n",
      "characterize the distribution. A “parameter” is a real number that can take on\n",
      "more than one value within a parameter space. If the parameter space contains\n",
      "only one point, the corresponding quantity characterizing the distribution is\n",
      "not a parameter.\n",
      "In most cases of interest the parameter space Θ is an open convex subset of\n",
      "IRd. In the N(µ, σ2) family, for example, Θ = IR × IR+. In the binomial(n, π)\n",
      "family Θ =]0, 1[ and n is usually not considered a “parameter” because in\n",
      "most applications it is assumed to be ﬁxed and known. In many cases, for a\n",
      "family with PDF pθ(x), the function ∂pθ(x)/∂θ exists and is an important\n",
      "characteristic of the family (see page 168).\n",
      "An example of a family of distributions whose parameter space is neither\n",
      "open nor convex is the hypergeometric(N, M, n) family (see page 838). In this\n",
      "family, as in the binomial, n is usually not considered a parameter because\n",
      "in most applications it is assumed known. Also, in most applications, either\n",
      "N or M is assumed known, but if they are both taken to be parameters,\n",
      "then Θ = {(i, j) : i = 2, 3, . . ., j = 1, . . ., i}. Obviously, in the case of the\n",
      "hypergeometric family, the function ∂pθ(x)/∂θ does not exist.\n",
      "Many common families are multi-parameter, and specialized subfamilies\n",
      "are deﬁned by special values of one or more parameters. As we have men-\n",
      "tioned and illustrated, a certain parameter may be referred to as a “location\n",
      "parameter” because it identiﬁes a point in the support that generally locates\n",
      "the support within the set of reals. A location parameter may be a boundary\n",
      "point of the support of it may be the mean or a median of the distribution.\n",
      "Another parameter may be referred to as a “scale parameter” because it is\n",
      "associated with scale transformations of the random variable. The standard\n",
      "deviation of a normal random variable, for example, is the scale parameter of\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "160\n",
      "2 Distribution Theory and Statistical Models\n",
      "that distribution. Other parameters may also have some common interpreta-\n",
      "tion, such as “shape”. For example, in the “three-parameter gamma” family\n",
      "of distributions there are three parameters, γ, called the “location”; β, called\n",
      "the “scale”; and α, called the “shape”. Its PDF is\n",
      "(Γ(α))−1β−α(x −γ)α−1e−(x−γ)/βI[γ,∞[(x).\n",
      "This family of distributions is sometimes called the three-parameter gamma,\n",
      "because often γ is taken to be a ﬁxed value, usually 0.\n",
      "Speciﬁc values of the parameters determine special subfamilies of distri-\n",
      "butions. For example, in the three-parameter gamma, if α is ﬁxed at 1, the\n",
      "resulting distribution is the two-parameter exponential, and if, additionally, γ\n",
      "is ﬁxed at 0, the resulting distribution is what most people call an exponential\n",
      "distribution.\n",
      "(Oddly, teachers of mathematical statistics many years ago chose the two-\n",
      "parameter exponential, with location and scale, to be “the exponential”, and\n",
      "chose the two-parameter gamma, with shape and scale, to be “the gamma”.\n",
      "The convenient result was that the exponential could be used as an example of\n",
      "a distribution that is not a member of the exponential class but is a member\n",
      "of the location-scale class, and the gamma could be used as an example of a\n",
      "distribution that is a member of the exponential class but is not a member\n",
      "of the location-scale class. This terminology is not nonstandard, and it seems\n",
      "somewhat odd to choose to include the location parameter in the deﬁnition of\n",
      "the exponential family of distributions and not in the deﬁnition of the more\n",
      "general gamma family of distributions. As noted, of course, it is just so we\n",
      "can have convenient examples of speciﬁc types of families of distributions.)\n",
      "Notation in Parametric Families\n",
      "I use notation of the form “N(µ, σ2)” or “gamma(α, β, γ)” to represent a para-\n",
      "metric family. The notation for the parameters is positional, and follows the\n",
      "notations of the tables in Appendix A beginning on page 838. I generally use\n",
      "a Greek letter to represent a parameter. Sometimes a distribution depends on\n",
      "an additional quantity that is not a parameter in the usual sense of that term,\n",
      "and I use a Latin letter to represent such a quantity, as in “binomial(n, π)”\n",
      "for example. (The notation for the hypergeometric, with parameters N and\n",
      "M, one of which is usually assigned a ﬁxed value, is an exception.)\n",
      "As noted above, if a parameter is assigned a ﬁxed value, then it ceases\n",
      "to be a parameter. If a parameter is ﬁxed at some known value, I use a\n",
      "subscript to indicate that fact, for example “N(µ, σ2\n",
      "0)” may represent a normal\n",
      "distribution with variance known to be σ2\n",
      "0. In that case, σ2\n",
      "0 is not a parameter.\n",
      "(A word of caution, however: I may use subscripts to distinguish between two\n",
      "distributional families, for example, N(µ1, σ2\n",
      "1) and N(µ2, σ2\n",
      "2).)\n",
      "Whether or not a particular characteristic of a distribution is a parameter\n",
      "is important in determining the class of a particular family. For example,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2 Distribution Theory and Statistical Models\n",
      "161\n",
      "the three-parameter gamma is not a member of the exponential class (see\n",
      "Section 2.4); it is a member of the parametric-support class (see Section 2.5).\n",
      "The standard two-parameter gamma, however, with γ ﬁxed at 0, is a member\n",
      "of the exponential class. If γ is ﬁxed at any value γ0, the gamma family is a\n",
      "member of the exponential class. Another example is the normal distribution,\n",
      "N(µ, σ2), which is a complete family (see Section 2.1); however, N(µ0, σ2) is\n",
      "not a complete family.\n",
      "Types of Families\n",
      "A reason for identifying a family of distributions is so that we can state inter-\n",
      "esting properties that hold for all distributions within the family. The state-\n",
      "ments that specify the family are the hypotheses for important theorems.\n",
      "These statements may be very speciﬁc: “if X1, X2, . . . is a random sample\n",
      "from a normal distribution...”, or they may be more general: “if X1, X2, . . . is\n",
      "a random sample from a distribution with ﬁnite second moment...”\n",
      "Some simple characterization such as “having ﬁnite second moment” is\n",
      "easy to state each time its need arises, so there is little to be gained by\n",
      "deﬁning such a class of distributions. On the other hand, if the characteristics\n",
      "are more complicated to state in each theorem that refers to that family of\n",
      "distributions, it is worthwhile giving a name to the set of characteristics.\n",
      "Because in statistical applications we are faced with the problem of choos-\n",
      "ing the particular distributions Pθ0 from a family of distributions, P = {Pθ :\n",
      "θ ∈Θ}, the behavior of the CDFs or PDFs as functions of θ are of interest. It\n",
      "may be important, for example, that the PDFs in this family be continuous\n",
      "with respect to θ or that derivatives of a speciﬁed order with respect to θ\n",
      "exist.\n",
      "We identify certain collections of families of distributions for which we\n",
      "can derive general results. Although I would prefer to call such a collection a\n",
      "“class”, most people call it a “family”, and so I will too, at least sometimes.\n",
      "Calling these collections of families “families” leads to some confusion, be-\n",
      "cause we can have a situation such as “exponential family” with two diﬀerent\n",
      "meanings.\n",
      "The most important class is the exponential class, or “exponential family”.\n",
      "This family has a number of useful properties that identify optimal procedures\n",
      "for statistical inference, as we will see in later chapters.\n",
      "Another important type of family of distributions is a group family, of\n",
      "which there are three important instances: a scale family, a location family,\n",
      "and a location-scale family.\n",
      "There are various other types of families characterized by their shape or by\n",
      "other aspects useful in speciﬁc applications or that lead to optimal standard\n",
      "statistical procedures.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "162\n",
      "2 Distribution Theory and Statistical Models\n",
      "Parametric Modeling Considerations\n",
      "In statistical applications we work with families of probability distribu-\n",
      "tions that seem to correspond to observed frequency distributions of a data-\n",
      "generating process. The ﬁrst considerations have to do with the nature of the\n",
      "observed measurements. The structure of the observation, just as the struc-\n",
      "ture of a random variable, as discussed on page 37, is one of the most relevant\n",
      "properties. As a practical matter, however, we will emphasize the basic scalar\n",
      "structure, and attempt to model more complicated structures by imposition of\n",
      "relationships among the individual components. Another important property\n",
      "is whether or not the measurement is within the set of integers. Often the\n",
      "measurement is a count, such as the number of accidents in a given period of\n",
      "time or such as the number of defective products in a batch of a given size. A\n",
      "PDF dominated by a counting measure would be appropriate in such cases.\n",
      "On the other hand, if the measurement could in principle be arbitrarily close\n",
      "to any of an uncountable set of irrational numbers, then a PDF dominated\n",
      "by Lebesgue would be more appropriate.\n",
      "The next consideration is the range of the measurements. This determines\n",
      "the support of a probability distribution used as a model. It is convenient to\n",
      "focus on three ranges, ]−∞, ∞[, [a, ∞[, and [a, b] where −∞< a < b < ∞. For\n",
      "integer-valued measurements within these three types of ranges, the Poisson\n",
      "family or the binomial family provide ﬂexible models. Both the Poisson and\n",
      "the binomial are unimodal, and so we may need to consider other distributions.\n",
      "Often, however, mixtures of members of one of these families can model more\n",
      "complicated situations.\n",
      "For continuous measurements over these three types of ranges, the normal\n",
      "family, the gamma family, and the beta family, respectively, provide ﬂexible\n",
      "models. Mixtures of members of one of these families provide even more ﬂex-\n",
      "ibility. The generality of the shapes of these distributions make them very\n",
      "useful for approximation of functions, and the most common series of orthog-\n",
      "onal polynomials are based on them. (See Table 0.2 on page 752.)\n",
      "2.1 Complete Families\n",
      "A family of distributions P is said to be complete iﬀfor any Borel function h\n",
      "that does not involve P ∈P\n",
      "E(h(X)) = 0 ∀P ∈P\n",
      "=⇒\n",
      "h(t) = 0 a.e. P.\n",
      "A slightly weaker condition, “bounded completeness”, is deﬁned as above,\n",
      "but only for bounded Borel functions h.\n",
      "Full rank exponential families are complete (exercise). The following ex-\n",
      "ample shows that a nonfull rank exponential family may not be complete.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.2 Shapes of the Probability Density\n",
      "163\n",
      "Example 2.1 complete and incomplete family\n",
      "Let\n",
      "P1 = {distributions with densities of the form (\n",
      "√\n",
      "2πσ)−1 exp(x2/(2σ2))}.\n",
      "(This is the N(0, σ2) family.) It is clear that E(h(X)) = 0 for h(x) = x, yet\n",
      "clearly it is not the case that h(t) = 0 a.e. λ, where λ is Lebesgue measure.\n",
      "Hence, this family, the family of normals with known mean, is not complete.\n",
      "This example of course would apply to any symmetric distribution with known\n",
      "mean.\n",
      "With some work, we can see that the family\n",
      "P2 = {distributions with densities of the form (\n",
      "√\n",
      "2πσ)−1 exp((x−µ)2/(2σ2))}\n",
      "is complete.\n",
      "Notice in the example that P1 ⊆P2; and P2 is complete, but P1 is not.\n",
      "This is a common situation.\n",
      "Going in the opposite direction, we have the following theorem.\n",
      "Theorem 2.1\n",
      "Let P2 be the family of distributions wrt which the expectation operator is de-\n",
      "ﬁned and assume that P2 is complete. Now let P2 ⊆P1, where all distributions\n",
      "in P1 have common support. Then the family P1 is complete.\n",
      "Proof. Exercise.\n",
      "2.2 Shapes of the Probability Density\n",
      "The general shape of a probability density may determine properties of sta-\n",
      "tistical inference procedures. We can easily identify various aspects of a prob-\n",
      "ability distribution that has a continuous density function. For discrete dis-\n",
      "tributions, some of the concepts carry over in an intuitive fashion, and some\n",
      "do not apply.\n",
      "In the following, we will assume that X is a random variable (or vector)\n",
      "with distribution in the family\n",
      "P = {Pθ : θ ∈Θ ⊆IRk}\n",
      "that is dominated by a σ-ﬁnite measure ν, and we let\n",
      "fθ(x) = dPθ/dν.\n",
      "First of all, we consider the shape only as a function of the value of the\n",
      "random variable, that is, for a ﬁxed member of the family the shape of the\n",
      "PDF.\n",
      "In some cases, the shape characteristic that we consider has (simple) mean-\n",
      "ing only for random variables in IR.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "164\n",
      "2 Distribution Theory and Statistical Models\n",
      "Empirical Distributions and Kernels of Power Laws\n",
      "Many important probability distributions that we identify and give names to\n",
      "arise out of observations on the behavior of some data-generating process. For\n",
      "example, in the process of forming coherent sentences on some speciﬁc topics\n",
      "there is an interesting data-generating process that yields the number of the\n",
      "most often used word, the number of the second most often used word and so\n",
      "on; that is, the observed data are x1, x2, . . ., where xi is the count of the word\n",
      "that occurs as the ith most frequent. The linguist George Kingsley Zipf studied\n",
      "this data-generating process and observed a remarkable empirical relationship.\n",
      "In a given corpus of written documents, the second most commonly-used word\n",
      "occurs approximately one-half as often as the most common-used word, the\n",
      "third most commonly-used word occurs approximately one-third as often as\n",
      "the second most common-used word. (This general kind of relationship had\n",
      "been known before Zipf, but he studied it more extensively.) A probability-\n",
      "generating function that expresses this empirical relationship has the kernel\n",
      "k(x) = x−α,\n",
      "x = 1, 2, . . .\n",
      "where α > 1.\n",
      "The salient characteristic, which determines the shape of the PDF, is that\n",
      "the relative frequency is a function of the value raised to some power. This kind\n",
      "of situation is observed often, both in naturally occurring phenomena such\n",
      "as magnitudes of earthquakes or of solar ﬂares, and in measures of human\n",
      "artifacts such as sizes of cities or of corporations. This is called a “power\n",
      "law”. Use of the kernel above leads to a Zipf distribution, also called a zeta\n",
      "distribution because the partition function is the (real) zeta function, ζ(s) =\n",
      "P∞\n",
      "i=1 zs. (The Riemann zeta function is the analytic continuation of series,\n",
      "and obviously it is much more interesting than the real series.) The PDF, for\n",
      "α > 1, is\n",
      "f(x) =\n",
      "1\n",
      "ζ(α)x−α,\n",
      "x = 1, 2, . . .\n",
      "power law distribution Pareto distribution Benford distribution power func-\n",
      "tion distribution (not power series distribution)\n",
      "f(x) = c(α, θ)x−αθx−1,\n",
      "x = 1, 2, . . .\n",
      "Symmetric Family\n",
      "A symmetric family is one for which for any given θ there is a constant τ that\n",
      "may depend on θ, such that\n",
      "fθ(τ + x) = fθ(τ −x),\n",
      "∀x.\n",
      "In this case, we say the distribution is symmetric about τ.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.2 Shapes of the Probability Density\n",
      "165\n",
      "In a symmetric family, the third standardized moment, η3, if it exists is\n",
      "0; however, skewness coeﬃcient. If η3 = 0, the distribution is not necessarily\n",
      "symmetric.\n",
      "The characteristic function of distribution that is symmetric about 0 is\n",
      "real, and any distribution whose characteristic function is real must have\n",
      "symmetries about 0 within the periods of the sine function (see equation (1.91)\n",
      "on page 46).\n",
      "Unimodal Family\n",
      "A family of distributions is said to be unimodal if for any given θ the mode\n",
      "of the distribution exists and is unique. This condition is sometimes referred\n",
      "to as strictly unimodal, and the term unimodal is used even with the mode of\n",
      "the distribution is not unique.\n",
      "A family of distributions with Lebesgue PDF p is unimodal if for any given\n",
      "θ, fθ(x) is strictly concave in x (exercise). This fact can be generalized to fam-\n",
      "ilies with superharmonic Lebesgue PDFs (see Deﬁnition 0.0.14 on page 659).\n",
      "Theorem 2.2\n",
      "A probability distribution with a Lebesgue PDF that is superharmonic is uni-\n",
      "modal.\n",
      "Proof. Exercise.\n",
      "If the PDF is twice diﬀerentiable, by Theorem 0.0.15 unimodality can\n",
      "be characterized by the Laplacian. For densities that are not twice diﬀer-\n",
      "entiable, negative curvature along the principal axes is sometimes called or-\n",
      "thounimodality.\n",
      "Logconcave Family\n",
      "If log fθ(x) is strictly concave in x for any θ, the family is called a logconcave\n",
      "family. It is also called a strongly unimodal family. A strongly unimodal fam-\n",
      "ily is unimodal; that is, if logfθ(x) is concave in x, then fθ(x) is unimodal\n",
      "(exercise). Strong unimodality is a special case of total positivity (see below).\n",
      "The relevance of strong unimodality for location families, that is, for fam-\n",
      "ilies in which fθ(x) = g(x −θ), is that the likelihood ratio is monotone in\n",
      "x (see below) iﬀthe distribution is strongly unimodal for a ﬁxed value of θ\n",
      "(exercise).\n",
      "Heavy-tailed Family\n",
      "A heavy-tailed family of probability distributions is one in which there is a\n",
      "relatively large probability in a region that includes ]−∞, b[ or ]b, ∞[ for some\n",
      "ﬁnite b. This general characterization has various explicit instantiations, and\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "166\n",
      "2 Distribution Theory and Statistical Models\n",
      "one ﬁnds in the literature various deﬁnitions of “heavy-tailed”. A standard\n",
      "deﬁnition of that term is not important, but various speciﬁc cases are worth\n",
      "study. A heavy-tailed distribution is also called an outlier-generating distri-\n",
      "bution, and it is because of “outliers” that such distributions ﬁnd interesting\n",
      "applications.\n",
      "The concept of a heavy tail is equally applicable to the “left” or the “right”\n",
      "tail, or even a mixture in the case of a random variable over IRd when d > 1.\n",
      "We will, however, consider only the right tail; that is, a region ]b, ∞[.\n",
      "Most characterizations of heavy-tailed distributions can be stated in terms\n",
      "of the behavior of the tail CDF. It is informative to recall the relationship of\n",
      "the ﬁrst moment of a positive-valued random variable in terms of the tail CDF\n",
      "(equation (1.46)):\n",
      "E(X) =\n",
      "Z ∞\n",
      "0\n",
      "F(t)dt.\n",
      "If for some constant b, x > b implies\n",
      "f(x) > c exp(−xTAx),\n",
      "(2.1)\n",
      "where c is some positive constant and A is some positive deﬁnite matrix, the\n",
      "distribution with PDF f is said to be heavy-tailed.\n",
      "Equivalent to the condition (2.1) in terms of the tail CDF is\n",
      "lim\n",
      "x→∞eaTxF(x) = ∞\n",
      "∀a > 0.\n",
      "(2.2)\n",
      "Another interesting condition in terms of the tail CDF that implies a\n",
      "heavy-tailed distribution is\n",
      "lim\n",
      "x→∞F(x + t) = F (x).\n",
      "(2.3)\n",
      "Distributions with this condition are sometimes called “long-tailed” distri-\n",
      "butions because of the “ﬂatness” of the tail in the left-hand support of the\n",
      "distribution. This condition states that F(log(x)) is a slowly varing function\n",
      "of x at ∞. (A function g is said to be slowly varying at ∞if for any a > 0,\n",
      "limx→∞g(ax)/g(x) = 1.)\n",
      "Condition (2.3) implies condition (2.2), but the converse is not true (Ex-\n",
      "ercise 2.3).\n",
      "Most heavy-tailed distributions of interest are univariate or else product\n",
      "distributions. A common family of distributions that are heavy-tailed is the\n",
      "Cauchy family. Another common example is the Pareto family with γ = 0.\n",
      "Subexponential Family\n",
      "Another condition that makes a family of distributions heavy-tailed is\n",
      "lim\n",
      "x→∞\n",
      "1 −F (2)(x)\n",
      "1 −F (x)\n",
      "= 2.\n",
      "(2.4)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.2 Shapes of the Probability Density\n",
      "167\n",
      "A family of distributions satisfying this condition is called a subexponential\n",
      "family (because the condition can be expressed as limx→∞e−aTx/F (x) = 0).\n",
      "Condition (2.4) implies condition (2.3), but the converse is not true (Ex-\n",
      "ercise 2.4).\n",
      "Monotone Likelihood Ratio Family\n",
      "The shape of parametric probability densities as a function of both the values\n",
      "of the random variable and the parameter may be important in statistical\n",
      "applications. Here and in the next section, we deﬁne some families based\n",
      "on the shape of the density over the cross product of the support and the\n",
      "parameter space. These characteristics are most easily expressed for the case\n",
      "of scalar parameters (k = 1), and they are also most useful in that case.\n",
      "Let y(x) be a scalar-valued function. The family P is said to have a mono-\n",
      "tone likelihood ratio iﬀfor any θ1 ̸= θ2, the likelihood ratio,\n",
      "λ(θ1, θ2|x) = fθ2(x)/fθ1(x)\n",
      "is a monotone function of x for all values of x for which fθ1(x) is positive.\n",
      "We also say that the family has a monotone likelihood ratio in y(x) iﬀthe\n",
      "likelihood ratio is a monotone function of y(x) for all values of x for which\n",
      "fθ1(x) is positive.\n",
      "Some common distributions that have monotone likelihood ratios are\n",
      "shown in Table 2.1. See also Exercise 2.5.\n",
      "Table 2.1. Some Common One-Parameter Families of Distributions with Monotone\n",
      "Likelihood Ratios\n",
      "normal(µ, σ2\n",
      "0)\n",
      "uniform(θ0, θ), uniform(θ, θ + θ0)\n",
      "exponential(θ) or exponential(α0, θ)\n",
      "double exponential(θ) or double exponential(µ0, θ)\n",
      "binomial(n, π) (n is assumed known)\n",
      "Poisson(θ)\n",
      "A subscript on a symbol for a parameter indicates that the symbol represents a known\n",
      "ﬁxed quantity. See Appendix A for meanings of symbols.\n",
      "Families with monotone likelihood ratios are of particular interest because\n",
      "they are easy to work with in testing composite hypotheses (see the discussion\n",
      "in Chapter 7 beginning on page 520).\n",
      "The concept of a monotone likelihood ratio family can be extended to fam-\n",
      "ilies of distributions with multivariate parameter spaces, but the applications\n",
      "in hypothesis testing are not as useful because we are usually interested in\n",
      "each element of the parameter separately.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "168\n",
      "2 Distribution Theory and Statistical Models\n",
      "Totally Positive Family\n",
      "A totally positive family of distributions is deﬁned in terms of the total posi-\n",
      "tivity of the PDF, treating it as a function of two variables, θ and x. In this\n",
      "sense, a family is totally positive of order r iﬀfor all x1 < · · · < xn and\n",
      "θ1 < · · · < θn,\n",
      "\f\f\f\f\f\f\f\n",
      "fθ1(x1) · · · fθ1(xn)\n",
      "...\n",
      "...\n",
      "...\n",
      "fθn(x1) · · · fθn(xn)\n",
      "\f\f\f\f\f\f\f\n",
      "≥0\n",
      "∀n = 1, . . ., r.\n",
      "(2.5)\n",
      "A totally positive family with r = 2 is a monotone likelihood ratio family.\n",
      "2.3 “Regular” Families\n",
      "Conditions that characterize a set of objects for which a theorem applies are\n",
      "called “regularity conditions”. I do not know the origin of this term, but it\n",
      "occurs in many areas of mathematics. In statistics there are a few sets of reg-\n",
      "ularity conditions that deﬁne classes of interesting probability distributions.\n",
      "We will often use the term “regularity conditions” to refer to continuity\n",
      "and diﬀerentiability of the PDF wrt the parameter.\n",
      "2.3.1 The Fisher Information Regularity Conditions\n",
      "The most important set of regularity conditions in statistics are some that\n",
      "allow us to put a lower bound on the variance of an unbiased estimator (see\n",
      "inequality (B.25) and Sections 3.1.3 and 5.1). Consider the family of distribu-\n",
      "tions P = {Pθ; θ ∈Θ} that have densities fθ.\n",
      "There are generally three conditions that together are called the Fisher\n",
      "information regularity conditions:\n",
      "•\n",
      "The parameter space Θ ⊆IRk is convex and contains an open set.\n",
      "•\n",
      "For any x in the support and θ ∈Θ◦, ∂fθ(x)/∂θ and ∂2fθ(x)/∂θ2 exist\n",
      "and are ﬁnite, and ∂2fθ(x)/∂θ2 is continuous in θ.\n",
      "•\n",
      "The support is independent of θ; that is, all Pθ have a common support.\n",
      "The latter two conditions ensure that the operations of integration and dif-\n",
      "ferentiation can be interchanged twice.\n",
      "Because the Fisher information regularity conditions are so important,\n",
      "the phrase “regularity conditions” is often taken to mean “Fisher information\n",
      "regularity conditions”. The phrase “Fisher regularity conditions” is also used\n",
      "synonymously, as is “FI regularity conditions”.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.4 The Exponential Class of Families\n",
      "169\n",
      "2.3.2 The Le Cam Regularity Conditions\n",
      "The Le Cam regularity conditions are the ﬁrst two of the usual FI regularity\n",
      "conditions plus the following.\n",
      "•\n",
      "The Fisher information matrix (see equation (1.82)) is positive deﬁnite for\n",
      "any ﬁxed θ ∈Θ.\n",
      "•\n",
      "There exists a positive number cθ and a positive function hθ such that\n",
      "E(hθ(X)) < ∞and\n",
      "sup\n",
      "γ:∥γ−θ∥<cθ\n",
      "\n",
      "∂2 log fγ(x)\n",
      "∂γ(∂γ)T\n",
      "\n",
      "F\n",
      "≤hθ(x) a.e.\n",
      "(2.6)\n",
      "where fθ(x) is a PDF wrt a σ-ﬁnite measure, and “a.e.” is taken wrt the\n",
      "same measure.\n",
      "2.3.3 Quadratic Mean Diﬀerentiability\n",
      "The Fisher information regularity conditions are often stronger than is needed\n",
      "to ensure certain useful properties. The double exponential distribution with\n",
      "Lebesgue PDF\n",
      "1\n",
      "2θe−|y−µ|/θ, for example, has many properties that make it a\n",
      "useful model, yet it is not diﬀerentiable wrt µ at the point x = µ, and so the\n",
      "FI regularity conditions do not hold. A slightly weaker regularity condition\n",
      "may be more useful.\n",
      "Quadratic mean diﬀerentiability is expressed in terms of the square root of\n",
      "the density. As with diﬀerentiability generally, we ﬁrst consider the property\n",
      "at one point, and then we apply the term to the function, or in this case,\n",
      "family, if the diﬀerentiability holds at all points in the domain.\n",
      "Consider again a family of distributions P = {Pθ; θ ∈Θ ⊆IRk} that have\n",
      "densities fθ. This family is said to be quadratic mean diﬀerentiable at θ0 iif\n",
      "there exists a real k-vector function η(x, θ0) = (η1(x, θ0), . . ., ηk(x, θ0)) such\n",
      "that\n",
      "∗∗∗fix\n",
      "Z\n",
      "(∗∗∗∗∗∗)2 dx ∈o(|h|2)\n",
      "as |h| →0.\n",
      "Compare quadratic mean diﬀerentiability with Fr´echet diﬀerentiability (Def-\n",
      "inition 0.1.57, on page 760).\n",
      "If each member of a family of distributions (speciﬁed by θ) is quadratic\n",
      "mean diﬀerentiable at θ, then the family is said to be quadratic mean diﬀer-\n",
      "entiable, or QMD.\n",
      "2.4 The Exponential Class of Families\n",
      "The exponential class is a set of families of distributions that have some partic-\n",
      "ularly useful properties for statistical inference. The important characteristic\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "170\n",
      "2 Distribution Theory and Statistical Models\n",
      "of a family of distributions in the exponential class is the way in which the pa-\n",
      "rameter and the value of the random variable can be separated in the density\n",
      "function. Another important characteristic of the exponential family is that\n",
      "the support of a distribution in this family does not depend on any “unknown”\n",
      "parameter.\n",
      "Deﬁnition 2.2 (exponential class of families)\n",
      "A member of a family of distributions in the exponential class is one with\n",
      "densities that can be written in the form\n",
      "pθ(x) = exp \u0000(η(θ))TT(x) −ξ(θ)\u0001 h(x),\n",
      "(2.7)\n",
      "where θ ∈Θ, and where T(x) is not constant in x.\n",
      "Notice that all members of a given family of distributions in the exponen-\n",
      "tial class have the same support. Any restrictions on the range may depend\n",
      "on x through h(x), but they cannot depend on the parameter.\n",
      "Many of the common families of distributions used as probability models\n",
      "of data-generating processes are in the exponential class. In Table 2.2, I list\n",
      "some families of distributions in the exponential class.\n",
      "Table 2.2. Some Common Families of Distributions in the Exponential Class\n",
      "Discrete Distributions\n",
      "binomial(n, π) (n is assumed known)\n",
      "multinomial(n, π) (n is assumed known)\n",
      "negative binomial(n, π) (n is assumed known)\n",
      "Poisson(θ)\n",
      "power series(θ, {hy}) ({hy} is assumed known)\n",
      "Continuous Distributions\n",
      "normal(µ, σ2), normal(µ0, σ2), or normal(µ, σ2\n",
      "0)\n",
      "log-normal(µ, σ2), log-normal(µ0, σ2), or log-normal(µ, σ2\n",
      "0)\n",
      "inverse Gaussian(µ, λ)\n",
      "beta(α, β)\n",
      "Dirichlet(α)\n",
      "exponential(θ) or exponential(α0, θ)\n",
      "double exponential(θ) or double exponential(µ0, θ)\n",
      "gamma(α, β) or gamma(α, β, γ0)\n",
      "gamma(α0, β) (which includes the exponential)\n",
      "gamma(α, β0) (which includes the chi-squared)\n",
      "inverted chi-squared(ν0)\n",
      "Weibull(α, β0)\n",
      "Pareto(α, γ0)\n",
      "logistic(µ, β)\n",
      "A subscript on a symbol for a parameter indicates that the symbol represents a known\n",
      "ﬁxed quantity. See Appendix A for meanings of symbols.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.4 The Exponential Class of Families\n",
      "171\n",
      "Note that the binomial, negative binomial, and Poisson families in Ta-\n",
      "ble 2.2 are all special cases of the general power series distributions whose\n",
      "PDF may be formed directly from equation (2.7); see page 175.\n",
      "In Table 2.3, I list some common families of distributions that are not in\n",
      "the exponential class. Notice that some families listed in Table 2.2, such as\n",
      "Pareto(α, γ0), for which some measure of the distribution is considered to be\n",
      "a ﬁxed constant are no longer in the exponential class if we consider that\n",
      "ﬁxed constant to be a parameter, as in the case of the two-parameter Pareto\n",
      "Pareto(α, γ).\n",
      "Table 2.3. Some Common Families of Distributions Not in the Exponential Class\n",
      "exponential(α, θ)\n",
      "gamma(α, β, γ)\n",
      "Weibull(α, β)\n",
      "uniform(θ1, θ2)\n",
      "double exponential(µ, θ)\n",
      "Cauchy(γ, β), Cauchy(γ0, β), or Cauchy(β)\n",
      "Pareto(α, γ)\n",
      "t(ν0)\n",
      "A family of distributions in the exponential class is called an exponential\n",
      "family, but do not confuse an “exponential family” in this sense with the\n",
      "“exponential family”, that is, the parametric family with density of the form\n",
      "1\n",
      "be−x/b I[0,∞[(x). (This is the usual form of the exponential family, and it is a\n",
      "member of the exponential class. In courses in mathematical statistics, it is\n",
      "common to deﬁne the exponential family to be the two-parameter family with\n",
      "density 1\n",
      "be−(x−a)/b I[a,∞[(x). This two-parameter form is not used very often,\n",
      "but it is popular in courses in mathematical statistics because this exponential\n",
      "family is not an exponential family(!) because of the range dependency.)\n",
      "The form of the expression for the PDF depends on the σ-ﬁnite dominating\n",
      "measure that deﬁnes it. If the expression above results from\n",
      "pθ = dPθ\n",
      "dν\n",
      "and we deﬁne a measure λ by λ(A) = R\n",
      "A hdν ∀A ∈F, then we could write\n",
      "the PDF as\n",
      "dPθ\n",
      "dλ = exp\n",
      "\u0000(η(θ))TT(x) −ξ(θ)\n",
      "\u0001\n",
      ".\n",
      "(2.8)\n",
      "Whatever the particular form of the PDF, an essential characteristic of it\n",
      "is the form of the decomposition as in equation (1.17). Formed from equa-\n",
      "tion (2.7), this is\n",
      "pθ(x) = exp(−ξ(θ)) exp \u0000(η(θ))TT(x)\u0001 h(x);\n",
      "(2.9)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "172\n",
      "2 Distribution Theory and Statistical Models\n",
      "that is, the kernel has the form k(x) = exp \u0000(η(θ))TT(x)\u0001 h(x). The im-\n",
      "portant thing to note is that the elements of the parameter vector η(θ) =\n",
      "(η(θ)1, . . ., η(θ)k) appear in the kernel only in an exponential and as a linear\n",
      "combination of functions of x.\n",
      "In the notation of equation (2.9), we see that the partition function is\n",
      "exp(ξ(θ)) =\n",
      "Z\n",
      "exp\n",
      "\u0000(η(θ))TT(x)\n",
      "\u0001\n",
      "h(x)dx.\n",
      "The form of the expression also depends on the parametrization; that is,\n",
      "the particular choice of the form of the parameters. First, notice that the only\n",
      "identiﬁable parameters must be in the elements of η(θ). The other function of\n",
      "the parameters, ξ(θ), which forms the partition, cannot introduce any more\n",
      "identiﬁable parameters; in fact, it can be written simply as\n",
      "ξ(θ) = log\n",
      "\u0012Z\n",
      "X\n",
      "exp \u0000(η(θ))TT(x)\u0001 h(x)dx\n",
      "\u0013\n",
      ".\n",
      "The expression\n",
      "µ = Eθ(T(x))\n",
      "(2.10)\n",
      "is called the mean-value parameter, and use of µ for η(θ) is called the mean-\n",
      "value parametrization. We can develop an explicit expression for Eθ(T(x))\n",
      "as\n",
      "E(T(X)) = ξ′(θ)/η′(θ).\n",
      "(See Section 2.4.7.)\n",
      "If a family of distributions has parameters α and β, we could equivalently\n",
      "say the family has parameters α and γ, where γ = α + β; that is,\n",
      "\u0012 α\n",
      "γ\n",
      "\u0013\n",
      "=\n",
      "\u00141 0\n",
      "1 1\n",
      "\u0015 \u0012 α\n",
      "β\n",
      "\u0013\n",
      ".\n",
      "In this case of course we would have to replace T(x) = (T1(x), T2(x))\n",
      "eT(x) = (T1(x) −T2(x), T2(x)).\n",
      "In fact, if η(θ) ∈IRd, and D is any nonsingular d × d matrix, then with\n",
      "˜η = Dη(θ), we can write an equivalent form of (η(θ))TT(x). To do so of\n",
      "course, we must transform T(x) also. So (η(θ))TT(x) = ˜ηT eT(x), where eT(x) =\n",
      "(DT)−1T(x).\n",
      "A PDF of the form f(x; θ)I(x; θ) with respect to a σ-ﬁnite measure λ\n",
      "(where I(x; θ) is an indicator function such that for some given x0, ∃θ1, θ2 ∈\n",
      "Θ ∋I(x; θ1) = 0, I(x; θ2) = 1) cannot be put in the form c exp(g(x; θ))h(x)\n",
      "because c exp(g(x; θ)) > 0 λ-a.e. (because the PDF must be bounded λ-a.e.).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.4 The Exponential Class of Families\n",
      "173\n",
      "2.4.1 The Natural Parameter Space of Exponential Families\n",
      "In the expression for the density, it might be more natural to think of the\n",
      "parameter as η rather than θ; that way we would have an expression of form\n",
      "ηTT(x) rather than (η(θ))TT(x). We call the form\n",
      "pθ(x) = exp\n",
      "\u0000(ηTT(x) −ζ(η)\n",
      "\u0001\n",
      "h(x)\n",
      "(2.11)\n",
      "the canonical exponential form, and we call\n",
      "H = {η :\n",
      "Z\n",
      "eηTT (x)h(x)dx < ∞}\n",
      "(2.12)\n",
      "the natural parameter space. (Notice that H is the upper-case form of η.) The\n",
      "conditions in equation (2.12) are necessary to ensure that a ζ(η) exists such\n",
      "that pθ(x) is a PDF. Another characterization of H is\n",
      "H = {η : η = η(θ), θ ∈Θ}\n",
      "(under the assumption that Θ is properly deﬁned, of course).\n",
      "2.4.2 The Natural Exponential Families\n",
      "An interesting subclass of exponential families is the class of exponential fam-\n",
      "ilies in which T(x) in the deﬁning expression (2.7) is linear. This subclass\n",
      "is variously called the “natural exponential families”, the “linear exponential\n",
      "families”, or the “canonical exponential families”.\n",
      "Given a random variable X whose distribution is in any exponential family,\n",
      "the random variable Y = T(X) has a distribution in the natural exponential\n",
      "family.\n",
      "The cumulant-generating and probability-generating functions of natu-\n",
      "ral exponential families have several simple properties (see Brown (1986) or\n",
      "Morris and Lock (2009)).\n",
      "2.4.3 One-Parameter Exponential Families\n",
      "An important subfamily of exponential families are those in which η(θ) ∈IR,\n",
      "that is, those whose parameter is a scalar (or eﬀectively a scalar). This family\n",
      "is called a one-parameter exponential.\n",
      "Theorem 2.3\n",
      "Suppose a PDF p(x|θ) can be written as exp(g(x; θ))h(x). where\n",
      "g(x; θ) = η(θ)T(x) −ξ(θ),\n",
      "with η(θ) ∈IR, and Let x1, x2, x3, x4 be any values of x for which p(x|θ) > 0.\n",
      "Then a necessary and suﬃcient condition that the distribution with the given\n",
      "PDF is in a one-parameter exponential family is that\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "174\n",
      "2 Distribution Theory and Statistical Models\n",
      "g(x1; θ) −g(x2; θ)\n",
      "g(x3; θ) −g(x4; θ)\n",
      "(2.13)\n",
      "is constant with respect to θ.\n",
      "Proof.\n",
      "It is clear from the deﬁnition that this condition is suﬃcient.\n",
      "To show that it is necessary, ﬁrst observe\n",
      "g(xi; θ) −g(xj; θ) = η(θ)(T(xi) −T(xj))\n",
      "Now, for x3 and x4 such that g(x3; θ) ̸= g(x4; θ), we see that the ratio (2.13)\n",
      "must be constant in θ, because η(θ) ∈IR.\n",
      "An example of an application of Theorem 2.3 is to show that the one-\n",
      "parameter Cauchy family of distributions is not in the exponential class. (In\n",
      "these distributions the scale parameter β = 1.)\n",
      "Example 2.2 the Cauchy family is not an exponential family\n",
      "The PDF of the Cauchy is\n",
      "p(x|γ) =\n",
      "1\n",
      "π\n",
      "\u0010\n",
      "1 + (x −γ)2\u0011\n",
      "= exp\n",
      "\u0010\n",
      "−log(π) −log\n",
      "\u0010\n",
      "1 + (xγ)2\u0011\u0011\n",
      ".\n",
      "Thus,\n",
      "g(x; θ) = −log(π) −log\n",
      "\u0010\n",
      "1 + (x −γ)2\u0011\n",
      "and for the four distinct points x1, x2, x3, x4,\n",
      "g(x1; θ) −g(x2; θ)\n",
      "g(x3; θ) −g(x4; θ) =\n",
      "−log\n",
      "\u0010\n",
      "1 + (x1 −γ)2\u0011\n",
      "+ log\n",
      "\u0010\n",
      "1 + (x2 −γ)2\u0011\n",
      "−log\n",
      "\u0010\n",
      "1 + (x3 −γ)2\u0011\n",
      "+ log\n",
      "\u0010\n",
      "1 + (x4 −γ)2\u0011\n",
      "is not constant in γ; hence the one-parameter Cauchy family of distributions\n",
      "is not in the exponential class.\n",
      "We often express the PDF for a member of a one-parameter exponential\n",
      "family as\n",
      "pη(x) = β(η)eηT (x)h(x).\n",
      "(2.14)\n",
      "In some cases if the support is IR, we can write the PDF as\n",
      "pη(x) = β(η)eηT (x).\n",
      "(2.15)\n",
      "One-parameter exponential families are monotone likelihood ratio families\n",
      "(exercise), and have useful applications in statistical hypothesis testing.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.4 The Exponential Class of Families\n",
      "175\n",
      "2.4.4 Discrete Power Series Exponential Families\n",
      "Various common discrete distributions can be formed directly from the general\n",
      "form of the PDF of one-parameter exponential families. In the notation of\n",
      "equation (2.14), let θ = eη, and suppose T(x) = x. If h(x) (or hx) is such that\n",
      "∞\n",
      "X\n",
      "x=0\n",
      "hxθx = c(θ) < ∞,\n",
      "for θ ∈Θ ⊆IR+,\n",
      "we have the probability mass function\n",
      "pθ(x) = hx\n",
      "c(θ)θxI{0,1,...}(x),\n",
      "(2.16)\n",
      "where θ ∈Θ. A family of distributions with PDFs of the form (2.16) is called\n",
      "a discrete power series family. Many of the common discrete families of dis-\n",
      "tributions, such as the Poisson, the binomial, and the negative binomial, are\n",
      "of the power series class (Exercise 2.13).\n",
      "2.4.5 Quadratic Variance Functions\n",
      "An interesting class of exponential families are those whose variance is at most\n",
      "a quadratic function of its mean. For example, in the binomial distribution\n",
      "with parameters n and π, the mean is µ = nπ and the variance is nπ(1 −π).\n",
      "The variance as a function of µ is\n",
      "nπ(1 −π) = −µ2/n + µ = v(µ)\n",
      "As another example, in the normal distribution N(µ, σ2), the variance is at\n",
      "most a quadratic function of the mean because, in fact, it is constant with re-\n",
      "spect to the mean. In the Poisson distribution, the variance is a linear function\n",
      "of the mean; in the gamma with parameters α and β, we have v(µ) = µ2/α;\n",
      "and in the negative binomial distribution with parameters r and π, we have\n",
      "v(µ) = µ2/n + µ.\n",
      "The normal, Poisson, gamma, binomial, negative binomial distributions,\n",
      "and one other family are in fact the only univariate natural exponential fam-\n",
      "ilies with quadratic variance functions. (The other family is formed from hy-\n",
      "perbolic secant distributions, and is not often used.) The quadratic variance\n",
      "property can be used to identify several other interesting properties, including\n",
      "inﬁnite divisibility, cumulants, orthogonal polynomials, large deviations, and\n",
      "limits in distribution.\n",
      "2.4.6 Full Rank and Curved Exponential Families\n",
      "We say the exponential family is of full rank if the natural parameter space\n",
      "contains an open set.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "176\n",
      "2 Distribution Theory and Statistical Models\n",
      "An exponential family that is not of full rank may also be degenerate,\n",
      "meaning that there exists a vector a and a constant r such that\n",
      "Z\n",
      "aTx=r\n",
      "pθ(x)dx = 1.\n",
      "(The term “degenerate” in this sense is also applied to any distribution,\n",
      "whether in an exponential family or not.) The support of a degenerate distri-\n",
      "bution within IRd is eﬀectively within IRk for k < d. An example of a nonfull\n",
      "rank exponential family that is also a degenerate family is the family of multi-\n",
      "nomial distributions (page 838). A continuous degenerate distribution is also\n",
      "called a singular distribution.\n",
      "An example of a family of distributions that is a nonfull rank exponential\n",
      "family is the normal family N(µ, µ2).\n",
      "A nonfull rank exponential family is also called a curved exponential family.\n",
      "2.4.7 Properties of Exponential Families\n",
      "Exponential families have a number of useful properties. First of all, we note\n",
      "that an exponential family satisﬁes the Fisher information regularity condi-\n",
      "tions. This means that we can interchange the operations of diﬀerentiation\n",
      "and integration, a fact that we will exploit below. Other implications of the\n",
      "Fisher information regularity conditions allow us to derive optimal statistical\n",
      "inference procedures, a fact that we will exploit in later chapters.\n",
      "In the following, we will use the usual form of the PDF,\n",
      "fθ(x) = exp(η(θ)TT(x) −ξ(θ))h(x),\n",
      "and we will assume that it is of full rank.\n",
      "We ﬁrst of all diﬀerentiate both sides of the identity, wrt θ,\n",
      "Z\n",
      "fθ(x) dx = 1\n",
      "(2.17)\n",
      "Carrying the diﬀerentiation on the left side under the integral, we have\n",
      "Z \u0010\n",
      "Jη(θ)T(x) −∇ξ(θ)\n",
      "\u0011\n",
      "exp\u0000η(θ)T(x) −ξ(θ)\u0001h(x) dx = 0.\n",
      "Hence, because by assumption Jη(θ) is of full rank, by rearranging terms under\n",
      "the integral and integrating out terms not involving x, we get the useful fact\n",
      "E(T(X)) = (Jη(θ))−1∇ξ(θ).\n",
      "(2.18)\n",
      "We now consider E(T(X)). As it turns out, this is a much more diﬃcult\n",
      "situation. Diﬀerentiation yields more complicated objects. (See Gentle (2007),\n",
      "page 152, for derivatives of a matrix wrt a vector.) Let us ﬁrst consider the\n",
      "scalar case; that is, η(θ) and T(x) are scalars, so η(θ)TT(x) is just η(θ)T(x).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.5 Parametric-Support Families\n",
      "177\n",
      "In this case, diﬀerentiating a second time with respect to θ, we get\n",
      "Z\n",
      "T(x)(η′(θ)T(x)−ξ′(θ)) exp(η(θ)T(x)−ξ(θ))h(x) dx = ξ′′(θ)η′(θ)/(η′(θ))2−ξ′(θ)η′′(θ)/(η′(θ))2,\n",
      "or\n",
      "η′(θ)E((T(X))2) −ξ′(θ)E(T(X)) = ξ′′(θ)/η′(θ) −ξ′(θ)η′′(θ)/(η′(θ))2\n",
      "or\n",
      "E((T(X))2) = (ξ′(θ))2/(η′(θ))2 −ξ′′(θ)/(η′(θ))2 −ξ′(θ)η′′(θ)/(η′(θ))3.\n",
      "Finally, collecting terms, we have\n",
      "V(T(X)) = E((T(X))2) −(E((T(X)))2\n",
      "=\n",
      "ξ′′(θ)\n",
      "(η′(θ))2 −ξ′(θ)η′′(θ)\n",
      "(η′(θ))3 .\n",
      "(2.19)\n",
      "********* proposition 3.2 in shao, page 171. look at two parameteriza-\n",
      "tions; natural and mean.\n",
      "V(T(X)) = Hζ(η) ∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗8,\n",
      "where Hζ(η) is the matrix of second derivatives of ζ with respect to η.\n",
      "It is often a simple matter to determine if a member of the exponential\n",
      "class of distributions is a monotone likelihood ratio family. If η(θ) and T(x)\n",
      "in equation (2.7) for the PDF of a distribution in the exponential class are\n",
      "scalars, and if η(θ) is monotone in θ, then the family has a monotone likelihood\n",
      "ratio in T(x).\n",
      "2.5 Parametric-Support Families\n",
      "Parametric-support families have simple range dependencies, that is, these are\n",
      "distributions whose supports depend on parameters. A distribution in any of\n",
      "these families has a PDF in the general form\n",
      "pθ(x) = f(x, θ)I[f1(θ),f2(θ)](x).\n",
      "(2.20)\n",
      "These families are also called “truncation families”, but most people use the\n",
      "term “truncated family” to refer to a family that is artiﬁcially truncated\n",
      "(for example, due to censoring; see Section 2.10.1). For example, to refer to\n",
      "the three-parameter gamma as a truncated distribution would be to confuse\n",
      "it with the more standard terminology in which a truncated gamma is the\n",
      "distribution formed from a two-parameter distribution with PDF\n",
      "c\n",
      "Γ(α)βα xα−1e−x/βI[τ1,τ2](x),\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "178\n",
      "2 Distribution Theory and Statistical Models\n",
      "where c is just the normalizing constant, which is a function of α, β, τ1, and τ2.\n",
      "In applications, the truncation points, τ1 and τ2, are often known ﬁxed values.\n",
      "If they are treated as parameters, of course, then the truncated distribution\n",
      "is a parametric-support family.\n",
      "Parametric-support families, such as the family of two-parameter exponen-\n",
      "tials, are not exponential families; likewise, exponential families, such as the\n",
      "family of one-parameter exponentials, are not parametric-support families.\n",
      "In some cases the parameters can be separated so some apply to the sup-\n",
      "port and others are independent of the support. If the parameters are func-\n",
      "tionally independent, various properties of the distribution may be identiﬁed\n",
      "with respect to some parameters only. Also diﬀerent statistical methods may\n",
      "be used for diﬀerent parameters. For example, in the three-parameter gamma\n",
      "with PDF\n",
      "1\n",
      "Γ(α)βα xα−1e−x/βI[γ,∞[(x),\n",
      "some aspects of the distribution are those of a family in the exponential class,\n",
      "while other aspects can be related to a simple uniform distribution, U(0, θ).\n",
      "2.6 Transformation Group Families\n",
      "“Group” families are distributions that have a certain invariance with respect\n",
      "to a group of transformations on the random variable. If g is a transformation\n",
      "within a group G of transformations (see Example 0.0.4 on page 630), and X\n",
      "is a random variable whose distribution is in the family PG and if the random\n",
      "variable g(X) also has a distribution in the family PG, the family PG is said\n",
      "to be invariant with respect to G.\n",
      "Transformations on the Sample Space and the Parameter Space\n",
      "Let G be a group of transformations that map the probability space onto\n",
      "itself. For g ∈G X and g(X) are random variables that are based on the same\n",
      "underlying measure, so the probability spaces are the same; the transformation\n",
      "is a member of a transformation group, so the domain and the range are equal\n",
      "and the transformations are one-to-one.\n",
      "g : X 7→X ,\n",
      "1 : 1 and onto\n",
      "For given g ∈G above, let ˜g be a 1:1 function that maps the parameter\n",
      "space onto itself, ˜g : Θ 7→Θ, in such a way that for any set A,\n",
      "Prθ(g(X) ∈A) = Pr˜g(θ)(X ∈A).\n",
      "(2.21)\n",
      "If this is the case we say ˜g preserves Θ. Any two functions that preserve the\n",
      "parameter space form a group of functions that preserve the parameter space.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.6 Transformation Group Families\n",
      "179\n",
      "The set of all such ˜g together with the induced structure is a group, eG. We\n",
      "write\n",
      "˜g(θ) = ˜θ.\n",
      "(2.22)\n",
      "˜g : Θ 7→Θ,\n",
      "1 : 1 and onto\n",
      "We may refer to eG as the induced group under G. The group eG is transitive in\n",
      "the sense deﬁned on page 755.\n",
      "Example 2.3 Transformations in a binomial distribution\n",
      "Suppose X is a random variable with a distribution in the binomial(n, π)\n",
      "family. In applications, the random variable is often taken as the sum of binary\n",
      "variables in which the 0 value is interpreted as one value of a binary state (“oﬀ-\n",
      "on”, “good-bad”, etc.). If the meaning of the binary state were changed, the\n",
      "binomial model for the application would remain unchanged. Instead of the\n",
      "original random variable, however, we would have g(X) = n −X. A further\n",
      "transformation ˜g(π) = 1 −π establishes the eﬀect on the parameter space\n",
      "occasioned by the transformation on the probability space.\n",
      "In the notation above G in G = (G, ◦) is given by\n",
      "G = {g(x) = x, g(x) = n −x : x ∈{0, . . ., n}},\n",
      "and eG in eG is given by\n",
      "eG = {˜g(x) = x, ˜g(x) = 1 −x : x ∈]0, 1[}.\n",
      "It is easy to see that both G and eG are groups (exercise).\n",
      "Formation of Transformation Group Families\n",
      "A group family can be formed from any family of distributions. (Notice that\n",
      "the preceding statement does not mean that any family is a group family;\n",
      "that depends on what variable parameters deﬁne the family.) The usual one-\n",
      "parameter exponential family of distributions, which is of the exponential\n",
      "class, is a transformation group family where the transformation is a scale\n",
      "(multiplicative) transformation, but is not a transformation group family\n",
      "where the transformation is a location and scale transformation. This family\n",
      "can be made into a location-scale group family by adding a location parame-\n",
      "ter. The resulting two-parameter exponential family is, of course, not of the\n",
      "exponential class.\n",
      "The random variable space associated with a transformation group of prob-\n",
      "ability distributions is closed with respect to that class of transformations.\n",
      "2.6.1 Location-Scale Families\n",
      "The most common group is the group of linear transformations, and this yields\n",
      "a location-scale group family, or just location-scale family, the general form of\n",
      "which is deﬁned below.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "180\n",
      "2 Distribution Theory and Statistical Models\n",
      "Given a d-variate random variable X, a d × d positive-deﬁnite matrix Σ\n",
      "and a d-vector µ, it is clear that if F (x) is the CDF associated with the random\n",
      "variable X, then |Σ−1/2|F (Σ−1/2(x −µ)) is the CDF associated with Y . The\n",
      "class of all distributions characterized by CDFs that can be formed in this\n",
      "way is of interest.\n",
      "Deﬁnition 2.3 (location-scale families)\n",
      "Let X be a random variable on IRk, let V ⊆IRk, and let Mk be the collection\n",
      "of k × k symmetric positive deﬁnite matrices. The family of distributions of\n",
      "the random variables of the form\n",
      "Y = Σ1/2X + µ, for µ ∈V, Σ ∈Mk\n",
      "(2.23)\n",
      "is called a location-scale family. The group of linear transformations y = g(x)\n",
      "in equation (2.23) is also called the location-scale group.\n",
      "The random variable space associated with a location-scale family is a\n",
      "linear space.\n",
      "If the PDF of a distribution in a location-scale family is f(x), the PDF\n",
      "of any other distribution in that family is |Σ−1/2|f(Σ−1/2(x −µ)). In the\n",
      "case of a scalar x, this simpliﬁes to f((x −µ)/σ)/σ. Thus, in a location-scale\n",
      "family the kernel of the PDF is invariant under linear transformations (see\n",
      "Deﬁnition 0.1.103 on page 755). The probability measure itself is invariant to\n",
      "the location transformation and equivariant to the scale transformation.\n",
      "We often use\n",
      "f((x −µ)/σ)/σ\n",
      "(2.24)\n",
      "generically to represent the PDF of a distribution in a location-scale family.\n",
      "While we can always form a location-scale family beginning with any dis-\n",
      "tribution, our interest is in which of the usual families of distributions are\n",
      "location-scale families. Clearly, a location-scale family must have enough pa-\n",
      "rameters and parameters of the right form in order for the location-scale\n",
      "transformation to result in a distribution in the same family. For example,\n",
      "a three-parameter gamma distribution is a location-scale family, but a two-\n",
      "parameter gamma (without the range dependency) is not.\n",
      "In Table 2.4, I list some common distribution families in which we can\n",
      "identify a location parameter. While the usual form of the family has more\n",
      "than one parameter, if all but one of the parameters are considered to be ﬁxed\n",
      "(that is, eﬀectively, they are not parameters), the remaining parameter is a\n",
      "location parameter.\n",
      "An interesting property of a location family is that the likelihood function\n",
      "is the same as the PDF. Figure 1.2 on page 20 illustrates the diﬀerence in a\n",
      "likelihood function and a corresponding PDF. In that case, the distribution\n",
      "family was exponential(0, θ), which of course is not a location family. A sim-\n",
      "ilar pair of plots for exponential(α, θ0), which is a location family, would be\n",
      "identical to each other (for appropriate choices of α on the one hand and x\n",
      "on the other, of course).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.6 Transformation Group Families\n",
      "181\n",
      "Table 2.4. Some Common One-Parameter Location Group Families of Distributions\n",
      "normal(µ, σ2\n",
      "0)\n",
      "exponential(α, θ0)\n",
      "double exponential(µ, θ0)\n",
      "gamma(α0, β0, γ)\n",
      "Cauchy(γ, β0)\n",
      "logistic(µ,β0)\n",
      "uniform(θ −θ0, θ + θ0)\n",
      "A subscript on a symbol for a parameter indicates that the symbol represents a known\n",
      "ﬁxed quantity. See Appendix A for meanings of symbols.\n",
      "Table 2.5. Some Common One-Parameter Scale Group Families of Distributions\n",
      "normal(µ0, σ2)\n",
      "inverse Gaussian(µ, λ)\n",
      "exponential(α0, θ)\n",
      "double exponential(µ0, θ)\n",
      "gamma(α0, β,γ0)\n",
      "Cauchy(γ0, β)\n",
      "logistic(µ0, β)\n",
      "uniform(θ0 −θ, θ0 + θ)\n",
      "A subscript on a symbol for a parameter indicates that the symbol represents a known\n",
      "ﬁxed quantity. See Appendix A for meanings of symbols.\n",
      "Often, a particular parameter in a parametric family can be identiﬁed as a\n",
      "“location parameter” or as a “scale parameter”, and the location-scale trans-\n",
      "formation aﬀects these two parameters in the obvious way. In some cases,\n",
      "however, a location transformation or a scale transformation alone aﬀects\n",
      "more than one parameter. For example, a scale transformation σX on a ran-\n",
      "dom variable with distribution inverse Gaussian(µ, λ) results in a random\n",
      "variable with distribution inverse Gaussian(σµ, σλ). (There is an alternative\n",
      "parametrization of the inverse Gaussian with ˜λ = λ and ˜µ =\n",
      "p\n",
      "λ/µ. In that\n",
      "notation, the scaling aﬀects only the ˜λ.)\n",
      "Many of the common parametric families are both location and scale fam-\n",
      "ilies; that is, they are location-scale group families. The families in both Ta-\n",
      "bles 2.4 and 2.5 can be combined into two-parameter families that are location-\n",
      "scale group families. The normal(µ, σ2), for example, is a location-scale group\n",
      "family.\n",
      "Some standard parametric families that are not location-scale group fam-\n",
      "ilies are the usual one-parameter exponential family, the binomial family, and\n",
      "the Poisson family.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "182\n",
      "2 Distribution Theory and Statistical Models\n",
      "Note that the only families of distributions that are in both the exponential\n",
      "class and the transformation group class are the normal, the inverse Gaussian,\n",
      "and the gamma.\n",
      "We have used the term “scale” to refer to a positive (or positive deﬁnite)\n",
      "quantity. There are other group families formed by a larger transformation\n",
      "group than those of equation (2.23). The transformations\n",
      "h(x) = a + Bx,\n",
      "(2.25)\n",
      "where B is a nonsingular matrix (but not necessarily positive deﬁnite), forms\n",
      "a transformation group, and the multivariate normal family Nd(µ, Σ) is a\n",
      "group family with respect to this group of transformation. Notice that the\n",
      "transformations in the group G = {h : h(x) = a + Bx, B nonsingular} can\n",
      "be formed by a smaller group, such as the same transformations in which B is\n",
      "a nonsingular lower triangular matrix (that is, one with nonnegative diagonal\n",
      "elements).\n",
      "2.6.2 Invariant Parametric Families\n",
      "Our interest in group families is often motivated by a certain symmetry in the\n",
      "sample space and the parameter space. That symmetry is expressed in the\n",
      "relationships between G the group of transformations that map the probability\n",
      "space onto itself and eG the induced group under G.\n",
      "Deﬁnition 2.4 (invariant class of families)\n",
      "Let P = {pθ : θ ∈Θ} be a parametric family of distributions on (X , B). Let\n",
      "X be a random variable with CDF Pθ0 ∈P. Let G be a group of transfor-\n",
      "mations on X . If for each g ∈G there is a 1:1 function ˜g on Θ such that the\n",
      "random variable g(X) has CDF P˜g(θ0) ∈P, then P is said to be an invariant\n",
      "parametric family under G.\n",
      "Some common families of distributions that are in the invariant parametric\n",
      "class under the location-scale group with their regular parametrization include\n",
      "the normal, the double exponential, the exponential, the uniform (even with\n",
      "parametric ranges), and the Cauchy.\n",
      "As suggested above, an invariant class under some transformation group\n",
      "can be generated by any distribution. This is not always possible for a speciﬁed\n",
      "group of transformations, however. For example, the (usual single-parameter)\n",
      "exponential family is not a member of a location invariant class.\n",
      "Other Invariant Distributions\n",
      "Given independent random variables X and Y , the distributions of X and\n",
      "Y may be such that there is a nontrivial transformation involving X and\n",
      "Y that yields a random variable that has the same distribution as X. that\n",
      "is, the distribution of X and g(X, Y ) is the same. For this to be the case,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.7 Inﬁnitely Divisible and Stable Families\n",
      "183\n",
      "clearly g must be a many-to-one map, so it is not an arbitrary member of a\n",
      "transformation group.\n",
      "An important property of the U(0, 1) distribution is the following. If a\n",
      "random variable is distributed as U(0, 1) and is scaled or convolved with a\n",
      "random variable that has any uniform distribution whose range includes [0, 1],\n",
      "and then reduced modulo 1, the resulting random variable has a U(0, 1) dis-\n",
      "tribution. To state this more clearly, let X ∼U(0, 1) and Y be distributed\n",
      "independently as U(a, b) where a < 0 and b > 1 and let c > 1. Now let\n",
      "Z = (cX + Y )\n",
      "mod 1.\n",
      "Then Z d= X.\n",
      "Another example of a distribution and transformation that is invariant (or\n",
      "nearly so) is the distribution of ﬁrst digits and a positive scaling transforma-\n",
      "tion. The digital representation of a random variable X is\n",
      "X = D1bK−1 + D2bK−2 + D3bK−3 + · · ·\n",
      "where b ≥3 is a ﬁxed integer, K is an integer-valued random variable, Di\n",
      "is a nonnegative integer-valued random variable less than b, and D1 > 0.\n",
      "If X has a uniform distribution over (bk−1, bk), the distribution of the ﬁrst\n",
      "digit D1 is not uniform over {1, . . ., b −1} as one might guess at ﬁrst glance.\n",
      "With a larger range of X, remarkably, the distribution of D1 is invariant\n",
      "to scale transformations of X (so long as the range of the scaled random\n",
      "variable includes a range of powers of b. (Note that the ﬁrst digit in the\n",
      "digital representation of aX is not necessarily aD1.)\n",
      "A wellknown example of this type of distribution and transformation is\n",
      "known as “Benford’s law”. See Exercise 2.10.\n",
      "2.7 Inﬁnitely Divisible and Stable Families\n",
      "The concept of divisibility was put forth in Deﬁnitions 1.31 and 1.32 on\n",
      "page 60. Distributions that are inﬁnitely divisible are of most interest because\n",
      "they yield tractable models with a wide range of applications, especially in\n",
      "stochastic processes.\n",
      "If {Xt : t ∈[0, ∞[} is a L´evy process, then any random variable X(t) is\n",
      "inﬁnitely divisible.\n",
      "Stable Families\n",
      "Stability of random variables was deﬁned in Deﬁnition 1.33 on page 61.\n",
      "Stable families are closely related to inﬁnitely divisible families. All sta-\n",
      "ble families are inﬁnitely divisible, but an inﬁnitely divisible family is not\n",
      "necessarily stable. The Poisson family is an example (exercise).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "184\n",
      "2 Distribution Theory and Statistical Models\n",
      "Most members of the stable family do not have PDFs or CDFs that can be\n",
      "represented in a closed form. The family is deﬁned by a cumulant-generating\n",
      "function of the form\n",
      "K(t) = iµt −|σt|α (1 −iβ sign(t)ω(α, t)) ,\n",
      "(2.26)\n",
      "where ω(α, t) = (2/π) log(|t|) for α = 1 and t ̸= 0, and ω(α, t) = tan(απ/2)\n",
      "for α ̸= 1. The parameters are analogous to parameters in other distributions\n",
      "that represent the mean, standard deviation, skewness, and kurtosis, although\n",
      "except in some special cases by the usual deﬁnitions of such measures of a\n",
      "distribution (based on expectations) these quantities do not exist or else are\n",
      "inﬁnite in the stable family of distributions. The parameters are\n",
      "•\n",
      "α ∈]0, 2]: stability coeﬃcient of equation (1.137)\n",
      "•\n",
      "β ∈[−1, 1]: skewness parameter (not necessarily related to a third mo-\n",
      "ment)\n",
      "•\n",
      "σ ∈IR+: scale parameter (not necessarily related to a second moment)\n",
      "•\n",
      "µ ∈IR: location parameter (not necessarily related to a ﬁrst moment)\n",
      "If β = 0 the distribution is symmetric (and note if also µ = 0, the cumulant-\n",
      "generating function and hence, the characteristic function, is real).\n",
      "The symmetric α-stable families, as α ranges from 1, which is the Cauchy\n",
      "distribution, to 2, which is the normal, has progressively lighter tails.\n",
      "2.8 Families of Distributions with Heavy Tails\n",
      "Exponential power family of distributions, also called the generalized error\n",
      "family of distributions.\n",
      "Kernel\n",
      "k(x) = e−|x/β|α\n",
      "The Pareto distribution has relatively heavy tails; for some values of the\n",
      "parameter, the mean exists but the variance does not. A “Pareto-type” dis-\n",
      "tribution is one whose distribution function satisﬁes the relationship\n",
      "P (x) = 1 −x−γg(x),\n",
      "where g(x) is a slowly varying function; that is, for ﬁxed t > 0,\n",
      "lim\n",
      "x→∞\n",
      "g(tx)\n",
      "g(x) = 1.\n",
      "The Burr distribution with the CDF given in (2.54) is of the Pareto type,\n",
      "with γ = αB.\n",
      "The stable family of distributions is a ﬂexible family of generally heavy-\n",
      "tailed distributions. This family includes the normal distribution at one ex-\n",
      "treme value of one of the parameters and the Cauchy distribution at the other\n",
      "extreme value. There are various parameterizations of the stable distributions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.9 The Family of Normal Distributions\n",
      "185\n",
      "Depending on one of the parameters, α, the index of stability, the charac-\n",
      "teristic function (equation (??), page ??) for random variables of this family\n",
      "of distributions has one of two forms:\n",
      "φ(t | α, σ, B, µ) = exp\n",
      "\u0010\n",
      "−σα|t|α\u00001 −iBsign(t) tan(πα/2)\u0001 + iµt\n",
      "\u0011\n",
      "if α ̸= 1,\n",
      "or\n",
      "φ(t | 1, σ, B, µ) = exp\n",
      "\u0010\n",
      "−σ|t|\n",
      "\u00001 + 2iBsign(t) log(t)/π\n",
      "\u0001\n",
      "+ iµt\n",
      "\u0011\n",
      "if α = 1\n",
      "for 0 < α ≤2, 0 ≤σ, and −1 ≤B ≤1. For α = 2, this is the normal\n",
      "distribution (in which case B is irrelevant), and for α = 1 and B = 0, this is\n",
      "the Cauchy distribution.\n",
      "The member of the stable family with α = 1 and B = 1 is called the\n",
      "Landau distribution, which has applications in modeling ﬂuctuation of energy\n",
      "loss in a system of charged particles.\n",
      "2.9 The Family of Normal Distributions\n",
      "The normal distribution is probably the most important probability distribu-\n",
      "tion. The normal family is in the exponential class. It is a complete family, a\n",
      "regular family, a group family, an inﬁnitely divisible family, a stable family,\n",
      "and an elliptical family. One reason that the family of normal distributions is\n",
      "so important in statistical applications is the central limit theorem that gives\n",
      "the normal as the limiting distribution of properly normalized sequences of\n",
      "random variables with other distributions.\n",
      "The family of normal distributions has a number of interesting and useful\n",
      "properties. One involves independence and covariance. It is easy to see that\n",
      "if the scalar random variables X and Y are independent, then Cov(X, Y ) =\n",
      "Cor(X, Y ) = 0, no matter how X and Y are distributed. An important prop-\n",
      "erty of the normal distribution is that if X and Y have a bivariate normal\n",
      "distribution and Cov(X, Y ) = Cor(X, Y ) = 0, then X and Y are indepen-\n",
      "dent. This is also easy to see by merely factoring the joint PDF. In addition\n",
      "to the bivariate normal, there are various other bivariate distributions for\n",
      "which zero correlation implies independence. Lancaster (1959) gave necessary\n",
      "and suﬃcient conditions for this implication.\n",
      "Interestingly, X and Y having normal marginal distributions and zero\n",
      "correlation is not suﬃcient for X and Y to be independent. This, of course,\n",
      "must mean that although the marginals are normal, the joint distribution\n",
      "is not bivariate normal. A simple example of this is the case in which X ∼\n",
      "N(0, 1), Z is a random variable such that Pr(Z = −1) = Pr(Z = 1) = 1\n",
      "2,\n",
      "and Y = ZX. Clearly, Y ∼N(0, 1) and Cor(X, Y ) = 0, yet X and Y are not\n",
      "independent. We also conclude that X and Y cannot be jointly normal.\n",
      "There are a number of interesting and useful properties that only the\n",
      "normal distribution has; that is, these properties characterize the normal dis-\n",
      "tribution. We will consider some of these properties in the next section.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "186\n",
      "2 Distribution Theory and Statistical Models\n",
      "Some properties of the normal family of distributions form the basis for\n",
      "many statistical methods, such as the use of the Student’s t for testing hy-\n",
      "potheses or setting conﬁdence limits, or the use of the F distribution in the\n",
      "analysis of variance. Many statistical methods depend on an assumption of a\n",
      "normal distribution.\n",
      "2.9.1 Multivariate and Matrix Normal Distribution\n",
      "The d-variate normal distribution, which we denote as Nd(µ, Σ), has the PDF\n",
      "1\n",
      "(2π)d/2|Σ|1/2 e−(x−µ)TΣ−1(x−µ)/2,\n",
      "where µ ∈IRd and Σ ≻0 ∈IRd×d.\n",
      "Notice that the exponent in this expression could alternatively be written\n",
      "as\n",
      "−tr(Σ−1(x −µ)(x −µ)T)/2.\n",
      "This form is often useful.\n",
      "As we noted above, each element of a random d-vector X may have a\n",
      "marginal normal distribution, yet X itself may not have a d-variate normal\n",
      "distribution.\n",
      "Generally, a “multivariate distribution” refers to the distribution of a ran-\n",
      "dom vector. If the random object has some other structure, however, a dis-\n",
      "tribution that recognizes the relationships within the structure may be more\n",
      "useful. One structure of interest is a matrix. Some random objects, such as a\n",
      "Wishart matrix (see page 841), arise naturally from other distributions. An-\n",
      "other useful random matrix is one in which all elements have a joint normal\n",
      "distribution and the columns of the matrix have one correlational structure\n",
      "and the rows have another correlational structure. This is called a multivari-\n",
      "ate matrix distribution, which we denote as MNn×m(M, Ψ, Σ). The PDF for\n",
      "the random n × m random matrix X is\n",
      "1\n",
      "(2π)nm/2|Ψ|n/2|Σ|m/2 e−tr(Ψ −1(X−M)TΣ−1(X−M))/2,\n",
      "where M ∈IRn×m, Ψ ≻0 ∈IRm×m, and Σ ≻0 ∈IRn×n.\n",
      "The variance-covariance matrix of X is V(X) = V(vec(X)) = Ψ ⊗Σ. The\n",
      "variance-covariance matrix of each row of X is Ψ, and the variance-covariance\n",
      "matrix of each column of X is Σ.\n",
      "The multivariate matrix normal distribution of the matrix X with PDF\n",
      "as given above is related to the ordinary multivariate normal for the vector\n",
      "vec(X) with PDF\n",
      "1\n",
      "(2π)nm/2|Ψ ⊗Σ|nm/2 e−vec(X−M)T(Ψ⊗Σ)−1vec(X−M)/2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.9 The Family of Normal Distributions\n",
      "187\n",
      "Complex Multivariate Normal Distribution\n",
      "Consider the random d-vector Z, where\n",
      "Z = X + iY.\n",
      "The vector Z has a complex d-variate normal distribution if (X, Y ) has a real\n",
      "2d-variate normal distribution. The PDF of Z has the form\n",
      "1\n",
      "(2π)d/2|Σ|1/2 e−(x−µ)HΣ−1(x−µ)/2,\n",
      "where µ ∈ICd and Σ ≻0 ∈ICd×d.\n",
      "2.9.2 Functions of Normal Random Variables\n",
      "One reason that the normal distribution is useful is that the distributions of\n",
      "certain functions of normal random variables are easy to derive and they have\n",
      "nice properties. These distributions can often be worked out from the CF of\n",
      "the normal distribution N(µ, σ2), which has a particularly simple form:\n",
      "ϕ(t) = eiµt−σ2t2/2.\n",
      "Given n iid N(µ, σ2) random variables, X1, X2, . . ., Xn, the sample mean\n",
      "and sample variance\n",
      "X =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "Xi/n\n",
      "(2.27)\n",
      "and\n",
      "S2 =\n",
      "n\n",
      "X\n",
      "i=1\n",
      " \n",
      "Xi −\n",
      "n\n",
      "X\n",
      "i=1\n",
      "Xi/n\n",
      "!2\n",
      "/(n −1)\n",
      "(2.28)\n",
      "are important functions.\n",
      "Using the CF and equations (1.99) and (1.100), it is easy to see that\n",
      "X ∼N(µ, σ2/n).\n",
      "(2.29)\n",
      "In Example 1.16, we saw that the sum of squares of n iid standard normal\n",
      "random variables is chi-squared with n degrees of freedom. Using properties\n",
      "of sums of independent normal random variables and of chi-squared random\n",
      "variables we see that X and S2 are independent and furthermore that\n",
      "(n −1)S2/σ2 d= χ2\n",
      "n−1.\n",
      "(2.30)\n",
      "Another way to establish the independence of X and S2 and to get the\n",
      "distribution of S2 is by use of the elegant Helmert transformation. We ﬁrst\n",
      "deﬁne\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "188\n",
      "2 Distribution Theory and Statistical Models\n",
      "Yk = Xk −X,\n",
      "k = 1, . . ., n −1\n",
      "(2.31)\n",
      "and\n",
      "Yn = −Y1 −· · · −Yn−1.\n",
      "(2.32)\n",
      "The joint density of X, Y1, . . ., Yn is proportional to\n",
      "exp\n",
      "\u0000−n(¯x −µ)2)/2σ2\u0001\n",
      "exp\n",
      "\u0000−(y2\n",
      "1 + · · · + y2\n",
      "n)/2σ2\u0001\n",
      ";\n",
      "(2.33)\n",
      "hence, we see that X is independent of Y1, . . ., Yn, or any function of them,\n",
      "including S2.\n",
      "The Helmert transformations are\n",
      "W1 =\n",
      "√\n",
      "2\n",
      "\u0000Y1 + 1\n",
      "2Y2 + · · · + 1\n",
      "2Yn−1\n",
      "\u0001\n",
      "W2 =\n",
      "q\n",
      "3\n",
      "2\n",
      "\u0000Y2 + 1\n",
      "3Y3 + · · · + 1\n",
      "3Yn−1\n",
      "\u0001\n",
      "W3 =\n",
      "q\n",
      "4\n",
      "3\n",
      "\u0000Y3 + 1\n",
      "4Y4 + · · · + 1\n",
      "4Yn−1\n",
      "\u0001\n",
      "· · ·\n",
      "Wn−1 =\n",
      "q\n",
      "n\n",
      "n−1Yn−1\n",
      "(2.34)\n",
      "We have\n",
      "n−1\n",
      "X\n",
      "k=1\n",
      "W 2\n",
      "k = (n −1)S2.\n",
      "(2.35)\n",
      "Because the joint density of W1, . . ., Wn−1 is the same as n −1 iid N(0, σ2)\n",
      "random variables (exercise), we have that (n−1)S2/σ2 is distributed as χ2\n",
      "n−1.\n",
      "If X is distributed as Nd(µ, Id), and for i = 1, . . .k, Ai is a d×d symmetric\n",
      "matrix with rank ri such that P\n",
      "i Ai = Id, then we can write\n",
      "XTX = XTA1X + · · · + XTAkX,\n",
      "and the XTAiX have independent noncentral chi-squared distributions χ2\n",
      "ri(δi)\n",
      "with δi = µTAiµ if and only if P\n",
      "i ri = d. This result is known as Cochran’s\n",
      "theorem. This form of the theorem and various preliminary forms leading up\n",
      "to it are proved beginning on page 430.\n",
      "From the family of central chi-squared distributions together with an inde-\n",
      "pendent normal family, we get the family of t distributions (central or noncen-\n",
      "tral, depending on the mean of the normal). From the family of chi-squared\n",
      "distributions (central or noncentral) we get the family of F distributions (cen-\n",
      "tral, or singly or doubly noncentral; see Example 1.15 on page 59 for the\n",
      "central distributions).\n",
      "The expectations of reciprocals of normal random variables have interest-\n",
      "ing properties. First of all, we see that for X ∼N(0, 1), E(1/X) does not exist.\n",
      "Now, for X ∼Nd(0, I) consider\n",
      "E\n",
      "\u0012\n",
      "1\n",
      "∥X∥2\n",
      "2\n",
      "\u0013\n",
      ".\n",
      "(2.36)\n",
      "For d ≤2, this expectation is inﬁnite (exercise). For d ≥3, however, this\n",
      "expectation is ﬁnite (exercise).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.9 The Family of Normal Distributions\n",
      "189\n",
      "2.9.3 Characterizations of the Normal Family of Distributions\n",
      "A simple characterization of a normal distribution was proven by Cram´er in\n",
      "1936:\n",
      "Theorem 2.4\n",
      "Let X1 and X2 be independent random variables. Then X1 and X2 have nor-\n",
      "mal distributions if and only if their sum X1 + X2 has a normal distribution.\n",
      "Proof. ***ﬁx\n",
      "The independence of certain functions of random variables imply that\n",
      "those random variables have a normal distribution; that is, the independence\n",
      "of certain functions of random variables characterize the normal distribution.\n",
      "Theorem 2.5 (Bernstein’s theorem)\n",
      "Let X1 and X2 be iid random variables with nondegenerate distributions, and\n",
      "let Y1 = X1 + X2 and Y2 = X1 −X2. If Y1 and Y2 are also independent then\n",
      "X1 and X2 have normal distributions.\n",
      "Proof. ***ﬁx\n",
      "An extension of Bernstein’s theorem is the Darmois theorem, also called\n",
      "the Darmois-Skitovich theorem.\n",
      "Theorem 2.6 (Darmois theorem)\n",
      "Let X1, . . ., Xn be iid random variables with nondegenerate distributions, and\n",
      "let\n",
      "Y1 =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "biXi\n",
      "and\n",
      "Y2 =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "ciXi,\n",
      "where the bi and ci are nonzero real constants. If Y1 and Y2 are also indepen-\n",
      "dent then X1, . . ., Xn have normal distributions.\n",
      "The proof of the Darmois theorem proceeds along similar lines as that of\n",
      "Bernstein’s theorem.\n",
      "The following theorem is a remarkable fact that provides a characterization\n",
      "of the normal distribution in terms of the sample mean X and the sample\n",
      "variance S2.\n",
      "Theorem 2.7 (Geary’s theorem)\n",
      "Let X1, X2, . . ., Xn be iid with PDF f with ﬁnite variance, σ2. A necessary\n",
      "and suﬃcient condition that the parent distribution be a normal distribution\n",
      "is that the sample mean and the sample variance be independent.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "190\n",
      "2 Distribution Theory and Statistical Models\n",
      "Proof. First, we note that the suﬃciency can be established directly quite\n",
      "easily.\n",
      "Now for the necessity. Assume that\n",
      "X =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "Xi/n\n",
      "and\n",
      "S2 =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(Xi −X)2/(n −1)\n",
      "are independent.\n",
      "We will work with various characteristic functions, all of which are deter-\n",
      "mined by EX (see page 49). We will adopt a simple notation for these CFs.\n",
      "Our main interest will be the CF of the joint distribution of X and S2, so we\n",
      "denote it simply as ϕ(t1, t2); that is,\n",
      "ϕ(t1, t2) = ϕX,S2(t1, t2)\n",
      "=\n",
      "Z\n",
      "eit1¯x+it2s2 Y\n",
      "f(xi)dxi.\n",
      "We denote the separate CFs as ϕ1(t1) and ϕ2(t2):\n",
      "ϕ1(t1) = ϕX(t1) = ϕX,S2(t1, 0)\n",
      "and\n",
      "ϕ2(t2) = ϕS2(t2) = ϕX,S2(0, t2).\n",
      "Finally, we let\n",
      "ϕX(t)\n",
      "be the CF of each Xi.\n",
      "From equation (1.131) (after dividing Y by n), we have\n",
      "ϕ1(t1) = (ϕX(t/n))n.\n",
      "From equation (1.104), the independence of X and S2 implies that\n",
      "ϕ(t1, t2) = ϕ1(t1)ϕ2(t2),\n",
      "and we have\n",
      "∂ϕ(t1, t2)\n",
      "∂t2\n",
      "\f\f\f\f\n",
      "t2=0\n",
      "= ϕ1(t1) ∂ϕ2(t2)\n",
      "∂t2\n",
      "\f\f\f\f\n",
      "t2=0\n",
      "= (ϕX(t/n))n ∂ϕ2(t2)\n",
      "∂t2\n",
      "\f\f\f\f\n",
      "t2=0\n",
      "(2.37)\n",
      "Directly from the deﬁnition of ϕ(t1, t2), we have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.9 The Family of Normal Distributions\n",
      "191\n",
      "∂ϕ(t1, t2)\n",
      "∂t2\n",
      "\f\f\f\f\n",
      "t2=0\n",
      "= i\n",
      "Z\n",
      "s2eit1 ¯x Y\n",
      "f(xi)dxi.\n",
      "(2.38)\n",
      "Now, substituting s2 = g(x1, . . ., xn) and ¯x = h(x1, . . ., xn) into this latter\n",
      "equation, we get\n",
      "∂ϕ(t1, t2)\n",
      "∂t2\n",
      "\f\f\f\f\n",
      "t2=0\n",
      "= i(ϕX(t1/n))n−1\n",
      "Z\n",
      "x2eit1x/nf(x)dx\n",
      "−i(ϕX(t1/n))n−2\n",
      "\u0012Z\n",
      "xeit1x/nf(x)dx\n",
      "\u00132\n",
      ".\n",
      "(2.39)\n",
      "Furthermore, because E(S2) = σ2, we have\n",
      "∂ϕ2(t2)\n",
      "∂t2\n",
      "\f\f\f\f\n",
      "t2=0\n",
      "= iσ2.\n",
      "(2.40)\n",
      "Now, substituting (2.39) and (2.40) into (2.37) and writing t = t1/n, we\n",
      "have\n",
      "ϕX(t)\n",
      "Z\n",
      "x2eitxf(x)dx −\n",
      "\u0012Z\n",
      "xeitxf(x)dx\n",
      "\u00132\n",
      "= (ϕX(t))2σ2.\n",
      "(2.41)\n",
      "Note that ik times the integrals in this latter equation are of the form\n",
      "dkϕX(t)/dtk, so we may re express equation (2.41) as the diﬀerential equation\n",
      "−ϕX(t)ϕ′′\n",
      "X(t) + (ϕ′\n",
      "X(t))2 = (ϕX(t))2σ2.\n",
      "(2.42)\n",
      "Now, solving this diﬀerential equation with the initial conditions\n",
      "ϕX(0) = 1,\n",
      "and\n",
      "ϕ′\n",
      "X(0) = iµ,\n",
      "where µ = E(X), we have\n",
      "ϕX(t) = eiµte−iσ2t2/2.\n",
      "(2.43)\n",
      "(The ordinary diﬀerential equation (2.42) is second order and second degree,\n",
      "so the solution is diﬃcult. We can conﬁrm that equation (2.43) is the solution\n",
      "by diﬀerentiation and substitution.)\n",
      "Equation (2.43) is the characteristic function of the normal distribution\n",
      "N(µ, σ2), and so the theorem is proved.\n",
      "This theorem can easily be extended to the multivariate case where the Xi\n",
      "are d-vectors. Because only ﬁrst and second moments are involved, the details\n",
      "of the proof are similar (see Exercise 2.31).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "192\n",
      "2 Distribution Theory and Statistical Models\n",
      "2.10 Generalized Distributions and Mixture\n",
      "Distributions\n",
      "Probability distributions used in statistical applications are chosen both for\n",
      "their general properties that determine the properties of the statistical meth-\n",
      "ods used and for their similarities to the frequency of observed data. Statistical\n",
      "methods based on a distributional family in the exponential, for example, can\n",
      "yield optimal unbiased procedures. On the other hand, statistical inference\n",
      "based on a family of distributions whose moments correspond to the sample\n",
      "moments of observed data would have greater credence than inference based\n",
      "on an assumption of a probability distribution with properties that diﬀer from\n",
      "the frequencies observed.\n",
      "Various families of probability distributions have been identiﬁed that are\n",
      "useful models of observed frequency distributions. The only speciﬁc family\n",
      "that we consider in this chapter is the normal family studied in Section 2.9.\n",
      "We list other important families in Appendix A. In this section, we consider\n",
      "general modiﬁcations of distributions that may yield more realistic models\n",
      "of observed data. We may ﬁnd, for example, that a normal distribution ﬁts\n",
      "observed data well over some ranges but not over others. The data may be\n",
      "censored (Section 2.10.1). On the other hand, it may be that the data fall\n",
      "into diﬀerent groups, some of which have frequencies corresponding to one\n",
      "normal probability distribution, and others have frequencies corresponding to\n",
      "a diﬀerent normal distribution or even to a distribution in a diﬀerent family\n",
      "(Section 2.10.2). Another possibility is that frequencies of the observational\n",
      "data are quite similar to a normal distribution, but that they are skewed one\n",
      "way or another (Section 2.10.3).\n",
      "Given a well-studied family of distributions such as the gamma or some\n",
      "other family in Appendix A, we may seek to generalize the family by incorpo-\n",
      "rating another parameter. (A simple example for the two-parameter gamma\n",
      "family is just to add a “starting” parameter, as in Table A.6.)\n",
      "More generally, we make seek to deﬁne a distribution with given properties,\n",
      "such as skewness or kurtosis. We may deﬁne a PDF or CDF that matches given\n",
      "quantiles, for example. We discuss some of these approaches in Section 2.10.4.\n",
      "The need to develop a probability distribution that models the frequency\n",
      "distribution of observed data has led to many useful distributions. Another\n",
      "motivation to developing useful and tractable probability distributions is to\n",
      "have meaningful prior distributions in Bayesian analysis (see Chapter 4).\n",
      "2.10.1 Truncated and Censored Distributions\n",
      "Often the support of a standard family is truncated to yield a family whose\n",
      "support is a proper subset of the standard family’s. The inﬁnite support of a\n",
      "normal distribution may be truncated to a ﬁnite interval, for example, because\n",
      "in a given application, no data will be, or can be, observed outside of some\n",
      "ﬁnite interval. Another common example is a truncated Poisson (also called\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.10 Generalized Distributions and Mixture Distributions\n",
      "193\n",
      "“positive Poisson”) in which the point 0 has been removed from the support.\n",
      "This may be because a realization of 0 does not make sense in the particular\n",
      "application.\n",
      "The kernel of the PDF of a truncated distribution over its support is the\n",
      "same as kernel of the original distribution over its support. The partition func-\n",
      "tion is adjusted as appropriate. The PDF of the positive Poisson distribution,\n",
      "for example, is\n",
      "f(x) =\n",
      "1\n",
      "eθ −1\n",
      "θx\n",
      "x! I{1,2,...}(x),\n",
      "(2.44)\n",
      "The support of a distribution may also be changed by censoring. “Cen-\n",
      "soring” refers what is done to a random variable, so strictly speaking, we do\n",
      "not study a “censored distribution”, but rather the distribution of a random\n",
      "variable that has been censored.\n",
      "There are various types of censoring. One type is similar to the truncation\n",
      "of a distribution, except that if a realization of the random variable occurs\n",
      "outside of the truncated support, that fact is observed, but the actual value\n",
      "of the realized is not known. This type of censoring is called “type I” ﬁxed\n",
      "censoring, and in the case that the support is an interval, the censoring is\n",
      "called “right” or “left” if the truncated region of the support is on the right\n",
      "(that is, large values) or on the left (small values). A common situation in\n",
      "which type I ﬁxed censoring occurs is when the random variable is a survival\n",
      "time, and several observational units are available to generate data. Various\n",
      "probability distributions such as exponential, gamma, Weibull, or lognormal\n",
      "may be used to model the survival time. If an observational unit survives\n",
      "beyond some ﬁxed time, say tc, only that fact is recorded and observation of\n",
      "the unit ceases.\n",
      "In another kind of ﬁxed censoring, also illustrated by observation of failure\n",
      "times of a given set of say n units, the realized failure times are recorded\n",
      "until say r units have failed. This type of censoring is called “type II” ﬁxed\n",
      "censoring.\n",
      "If an observational unit is removed prior to its realized value being observed\n",
      "for no particular reason relating to that unobserved value, the censoring is\n",
      "called “random censoring”.\n",
      "Censoring in general refers to a failure to observe the realized value of\n",
      "a random variable but rather to observe only some characteristic of that\n",
      "value. As another example, again one that may occur in studies of survival\n",
      "times, suppose we have independent random variables T1 and T2 with some\n",
      "assumed distributions. Instead of observing T1 and T2, however, we observe\n",
      "X = min(T1, T2) and G, an indicator of whether X = T1 or X = T2. In this\n",
      "case, T1 and T2 are censored, but the joint distribution of X and G may be\n",
      "relevant, and it may be determined based on the distributions of T1 and T2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "194\n",
      "2 Distribution Theory and Statistical Models\n",
      "2.10.2 Mixture Families\n",
      "In applications it is often the case that a single distribution models the ob-\n",
      "served data adequately. Sometimes two or more distributions from a single\n",
      "family of distributions provide a good ﬁt of the observations, but in other\n",
      "cases, more than one distributional family is required to provide an adequate\n",
      "ﬁt. In some cases most of the data seem to come from one population but\n",
      "a small number seem to be extreme outliers. Some distributions, such as a\n",
      "Cauchy, are said to be “outlier-generating”, but often such distributions are\n",
      "diﬃcult to work with (because they have inﬁnite moments, for example). Mix-\n",
      "tures of distributions, such as the ϵ-mixture distribution (see page 601), are\n",
      "often useful for modeling data with anomalous observations.\n",
      "A mixture family can be deﬁned in terms of a set of CDFs P0. The CDF of\n",
      "a mixture is P wiPi, where Pi ∈P0, 0 ≤wi ≤1, and P wi = 1. The set P of\n",
      "all such mixture CDFs is called a distribution function space (see page 754).\n",
      "If each the probability measure associated with each Pi is dominated by the\n",
      "measure ν, then the probability measure associated with P wiPi is dominated\n",
      "by the ν.\n",
      "One family that is useful in robustness studies is the ϵ-mixture distribution\n",
      "family, which is characterized by a given family with CDF P that we refer\n",
      "to as the reference distribution, together with a point xc and a weight ϵ. The\n",
      "CDF of a ϵ-mixture distribution family is\n",
      "Pxc,ϵ(x) = (1 −ϵ)P (x) + ϵI[xc,∞[(x),\n",
      "(2.45)\n",
      "where 0 ≤ϵ ≤1. The point xc may be thought of as a “contaminant” in the\n",
      "distribution with CDF P . In a common example of this kind of mixture, the\n",
      "probability measure associated with P is dominated by the Lebesgue measure\n",
      "µ, and in that case the probability measure associated with Pxc,ϵ is dominated\n",
      "by µ + δxc, where δxc is the dirac measure concentrated at xc.\n",
      "Another type of mixture family is composed of two distributions dominated\n",
      "by Lebesgue measure that have CDFs P1 and P2 such that at the point xc,\n",
      "P1(xc) < P2(xc) and whose CDF is given by\n",
      "P (x) =\n",
      "\u001a P1(x)\n",
      "−∞< x < xc\n",
      "P2(x)\n",
      "xc ≤x < ∞\n",
      "(2.46)\n",
      "The probability measure associated with P is dominated by µ + δxc.\n",
      "Finally, we consider a mixture family formed by censoring. Let Y1, . . ., Yn\n",
      "be iid with CDF P, and let\n",
      "Xi =\n",
      "\u001a\n",
      "Yi\n",
      "if Yi ≥c\n",
      "c\n",
      "if Yi < c\n",
      "i = 1, . . ., n.\n",
      "(2.47)\n",
      "If the distribution of the Yi is dominated by Lebesgue measure, then the\n",
      "distribution of the Xi is dominated by µ + δc.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.10 Generalized Distributions and Mixture Distributions\n",
      "195\n",
      "2.10.3 Skewed Distributions\n",
      "Most of the common skewed distributions, such as the gamma, the log normal,\n",
      "and the Weibull, have semi-inﬁnite range. The common distributions that have\n",
      "range (−∞, ∞), such as the normal and the t, are symmetric.\n",
      "There are several ways to form a skewed distribution from a symmetric\n",
      "one. Two simple ways are\n",
      "•\n",
      "CDF-skewing: take a random variable as the maximum (or minimum) of\n",
      "two independent and identically symmetrically distributed random vari-\n",
      "ables, or\n",
      "•\n",
      "diﬀerential scaling: for some constant ξ ̸= 0, scale a symmetric random\n",
      "variable by ξ if it is less than its mean and by 1/ξ if it is greater than its\n",
      "mean.\n",
      "In each case, it may be desirable to shift and scale the skewed random variable\n",
      "so that it has a mean of 0 and a variance of 1. (We can then easily shift and\n",
      "scale the random variable so as to have any desired mean and variance.)\n",
      "CDF-Skewing\n",
      "If a random variable has PDF f(x) and CDF F (x), from equation (??), the\n",
      "PDF of the maximum of two independent random variables with that distri-\n",
      "bution has the PDF\n",
      "2F (x)f(x).\n",
      "(2.48)\n",
      "Intuitively, we see that the maximum of two symmetrically distributed random\n",
      "variables has a skewed distribution.\n",
      "We can generalize this form by scaling the argument in the CDF,\n",
      "2F (αx)f(x).\n",
      "(2.49)\n",
      "A negative scaling, that is, α < 0 yields a negative skewness. (In the case\n",
      "of the normal distribution, the value α = −1 is equivalent to the minimum\n",
      "of two independent normal random variables.) Values of α larger in absolute\n",
      "value yield greater degrees of skewness. The scaling also changes the kurtosis,\n",
      "with larger absolute values of α yielding a larger kurtosis.\n",
      "Speciﬁcally, for the standard normal distribution, we form the PDF of a\n",
      "skewed normal as\n",
      "fSN1(x; α) = 2Φ(αx)φ(x),\n",
      "(2.50)\n",
      "φ(x) denotes the PDF of the standard normal distribution and Φ(x) denotes\n",
      "the CDF.\n",
      "Obviously, CDF-skewing can be applied to other distributions, such as the\n",
      "t or the generalized error distribution, and of course including normals as in\n",
      "equation (2.50) with other means and variances.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "196\n",
      "2 Distribution Theory and Statistical Models\n",
      "Diﬀerential Scaling\n",
      "Another way of forming a skewed distribution is by scaling the random vari-\n",
      "able diﬀerently on diﬀerent sides of the mean or some other central point.\n",
      "Speciﬁcally, for the normal distribution, we can form the PDF of a skewed\n",
      "normal as\n",
      "fSN2(x; ξ) =\n",
      "2\n",
      "ξ + 1\n",
      "ξ\n",
      "\n",
      "\n",
      "\n",
      "φ(x/ξ) for x < 0\n",
      "φ(ξx) for x ≥0,\n",
      "(2.51)\n",
      "where, as before, φ(x) denotes the PDF of the standard normal distribution.\n",
      "Values of ξ less than one produce a positive skew, and values greater than one\n",
      "produce a negative skew. In either case, the excess kurtosis is positive.\n",
      "The dividing point for diﬀerential scaling obviously could be chosen arbi-\n",
      "trarily. To form a skewed distribution from a unimodal symmetric distribution,\n",
      "an obvious dividing point would be the mode of the distribution.\n",
      "Distributions formed by diﬀerential scaling are sometimes called “two piece\n",
      "distributions”.\n",
      "2.10.4 Flexible Families of Distributions Useful in Modeling\n",
      "Some of the useful families of probability distributions arise from simple pro-\n",
      "cesses of “random” events. This is one way we naturally deﬁne the Bernoulli\n",
      "or binomial family, the hypergeometric family. the Poisson family, or the ex-\n",
      "ponential family, as examples. These discrete families can be generalized by\n",
      "developing a diﬀerential equation that models a limiting case of the discrete\n",
      "frequency model. The Pearson system is an example of this approach (in which\n",
      "the basic diﬀerential equation arises as a limiting case of a hypergeometric dis-\n",
      "tribution). Other broad families of distributional forms have been developed\n",
      "by Johnson, by Burr, and by Tukey. The objective is to be able to represent a\n",
      "wide range of distributional properties (mean, variance, skewness, shape, etc.)\n",
      "with a small number of parameters, and then to ﬁt a speciﬁc case by proper\n",
      "choice of these parameters.\n",
      "A special type of mixture distribution is a probability-skewed distribution,\n",
      "in which the mixing weights are the values of a CDF. The skew-normal dis-\n",
      "tribution is a good example.\n",
      "The (standard) skew-normal distribution has density\n",
      "g(x) =\n",
      "2\n",
      "√\n",
      "2π e−x2/2Φ(λx)\n",
      "for −∞≤x ≤∞,\n",
      "(2.52)\n",
      "where Φ(·) is the standard normal CDF, and λ is a constant such that −∞<\n",
      "λ < ∞. For λ = 0, the skew-normal distribution is the normal distribution,\n",
      "and in general, if |λ| is relatively small, the distribution is close to the normal.\n",
      "For larger |λ|, the distribution is more skewed, either positively or negatively.\n",
      "Other distributions symmetric about 0 can also be skewed by a CDF in\n",
      "this manner. The kernel of the probability density is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "2.10 Generalized Distributions and Mixture Distributions\n",
      "197\n",
      "k(x) = p(x)P (λx),\n",
      "where p(·) is the density of the underlying symmetric distribution, and P (·) is\n",
      "a CDF. (It is not necessary that the CDF be for the same distribution.) The\n",
      "idea also extends to multivariate distributions.\n",
      "In most cases, if |λ| is relatively small, generation of random variables from\n",
      "a probability-skewed symmetric distribution using an acceptance/rejection\n",
      "method with the underlying symmetric distribution as the majorizing density\n",
      "is entirely adequate. For larger values of |λ|, it is necessary to divide the\n",
      "support into two or more intervals. It is still generally possible to use the\n",
      "same majorizing density, but the multiplicative constant can be diﬀerent in\n",
      "diﬀerent intervals.\n",
      "Some commonly used ones are the Pearson family, the Johnson family,\n",
      "the generalized lambda family, and the Burr family. The Pearson family is\n",
      "probably the best known of these distributions. A speciﬁc member of the\n",
      "family is determined by the ﬁrst four moments, so a common way of ﬁtting\n",
      "a distribution to an observed set of data is by matching the moments of the\n",
      "distribution to those of the sample.\n",
      "Another widely used general family of distributions is the Johnson family.\n",
      "A speciﬁc member of this family is also determined by the ﬁrst four or ﬁve\n",
      "moments, depending on the parametrization.\n",
      "A generalized lambda family of distributions was described by Ramberg\n",
      "and Schmeiser (1974). This system, which is a generalization of a system\n",
      "introduced by John Tukey, has four parameters that can be chosen to ﬁt a\n",
      "variety of distributional shapes. They specify the distribution in terms of the\n",
      "inverse of its distribution function,\n",
      "P −1(u) = λ1 + uλ3 −(1 −u)λ4\n",
      "λ2\n",
      ".\n",
      "(2.53)\n",
      "The distribution function itself cannot be written in closed form, but the\n",
      "inverse allows deviates from this distribution to be generated easily by the\n",
      "inverse CDF method; just generate u and apply equation (2.53).\n",
      "Albert, Delampady, and Polasek (1991) deﬁned a family of distributions\n",
      "that is very similar to the lambda distributions and is particularly useful in\n",
      "Bayesian analysis with location-scale models.\n",
      "Another family of distributions that is very ﬂexible and that can have a\n",
      "wide range of shapes is the Burr family of distributions (Burr, 1942). One of\n",
      "the common forms (Burr and Cislak, 1968) has the CDF\n",
      "P (x) = 1 −\n",
      "1\n",
      "(1 + xα)B\n",
      "for 0 ≤x ≤∞; α, B > 0,\n",
      "(2.54)\n",
      "which is easily inverted. Other forms of the Burr family have more parameters,\n",
      "allowing modeling of a wider range of empirical distributions.\n",
      "Fleishman (1978) suggested representing the random variable of interest as\n",
      "a polynomial in a standard normal random variable, in which the coeﬃcients\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "198\n",
      "2 Distribution Theory and Statistical Models\n",
      "are determined so that the moments match speciﬁc values. If Z has a N(0, 1)\n",
      "distribution, then the random variable of interest, X, is expressed as\n",
      "X = c0 + c1Z + · · · + ckZk.\n",
      "(2.55)\n",
      "If m moments are to be matched to prespeciﬁed values, then k can be chosen\n",
      "as m −1, and the cs can be determined from m equations in m unknowns\n",
      "that involve expectations of powers of a N(0, 1) random variable. Fleishman\n",
      "used this representation to match four moments; hence, he used a third-degree\n",
      "polynomial in a standard normal random variable.\n",
      "The motivation for some of the early work with general families of distribu-\n",
      "tions was to use them as approximations to some standard distribution, such\n",
      "as a gamma, for which it is more diﬃcult to generate deviates. As methods for\n",
      "the standard distributions have improved, it is more common just to generate\n",
      "directly from the distribution of interest. The general families, however, often\n",
      "provide more ﬂexibility in choosing a distribution that better matches sample\n",
      "data. The distribution is ﬁt to the sample data using either percentiles or\n",
      "moments.\n",
      "2.11 Multivariate Distributions\n",
      "While our previous discussions have generally applied to multivariate distri-\n",
      "butions, the dimensionality of the range of a random variable may limit our\n",
      "studies or in other ways may have major eﬀects on the properties of the dis-\n",
      "tribution that aﬀect statistical analysis.\n",
      "2.11.1 Marginal Distributions\n",
      "Characterizations of families of multivariate probability distributions are often\n",
      "more diﬃcult or less intuitive. The covariance matrix is the common measure\n",
      "that relates the individual components of a random variable to each other in\n",
      "a pairwise manner. This is a very useful distribution measure for multivariate\n",
      "normal families, but it is much less useful for other multivariate families; con-\n",
      "sider, for example, a multivariate gamma family characterized by vectors α\n",
      "and β (generalizing the univariate parameters) and some variance-covariance\n",
      "matrix. It is not clear what that matrix would be and how it would be in-\n",
      "corporated in a simple manner into the PDF. In applications, copulas are\n",
      "often used in an ad hoc sense to express the relationship of the individual\n",
      "components of a random variable to each other.\n",
      "2.11.2 Elliptical Families\n",
      "Spherical and elliptical families are important in multivariate statistics. A d-\n",
      "variate random variable X is said to have a spherical distribution iﬀQTX d= X\n",
      "for every d × d orthogonal matrix Q.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "199\n",
      "Because QTQ = I, the density function must depend on X = x only\n",
      "through xxT. Spherical distributions include multivariate normals with di-\n",
      "agonal variance-covariance matrices and multivariate t distributions formed\n",
      "from iid scalar t variates.\n",
      "If the k-variate random variable Y has a spherical distribution, and for\n",
      "d ≤k, µ is a ﬁxed d-vector and A is a ﬁxed d × k matrix of full rank, then\n",
      "X = µ + AY\n",
      "is said to have an elliptical distribution.\n",
      "Any elliptical family is a location-scale family.\n",
      "General multivariate normal distributions, multivariate t distributions,\n",
      "and multivariate Cauchy distributions are members of elliptical families.\n",
      "2.11.3 Higher Dimensions\n",
      "Another problem with multivariate distributions arises from the properties of\n",
      "IRd as d increases. A simple instance of this can be seen in the multivariate\n",
      "normal distribution as d increases from 2 to 3 (see page 273). We can state a\n",
      "general result nontechnically: in a nondegenerate multivariate family, as the\n",
      "dimension increases, every observation becomes an outlier. See Exercise 2.27\n",
      "for a speciﬁc example of this result.\n",
      "Notes and Further Reading\n",
      "Distribution Theory\n",
      "In many applications of probability theory in statistics, the ﬁrst step is to asso-\n",
      "ciate a phenomenon of interest with a random variable. The statistical analysis\n",
      "then becomes a study of the distribution of the random variable. The “study”\n",
      "involves collecting or assimilating data, exploration of the data, transforma-\n",
      "tions of the data, comparisons of the observed data with regions and quantiles\n",
      "of probability distributions or families of distributions, and ﬁnally inference\n",
      "about some speciﬁc distribution or family of distributions. “Statistics” is the\n",
      "science of the methods of the study.\n",
      "Study of the characteristics of the probability distributions used in statis-\n",
      "tical analysis is part of the subject of probability theory.\n",
      "Although we do not want to draw a hard line between probability and\n",
      "statistics — drawing hard lines between any disciplines impedes the advance-\n",
      "ment of science — distribution theory per s´e is within the domain of prob-\n",
      "ability theory, rather than of statistical theory. That is why, for example,\n",
      "discussion of the exponential class of distributions is included in this chapter\n",
      "on probability, rather than placed in a later chapter on statistical theory.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "200\n",
      "2 Distribution Theory and Statistical Models\n",
      "Regular Families\n",
      "When some interesting properties apply to certain cases but not others, those\n",
      "cases for which the properties hold may be referred to as “regular” cases. The\n",
      "regularity conditions are essentially the hypotheses of a theorem that states\n",
      "that the particular properties hold. A theorem can seem more important if its\n",
      "conclusions hold for all except “irregular” cases.\n",
      "In statistics, the most common usage of the phrase “regularity conditions”\n",
      "or “regular families of distributions” is in connection with Fisher information.)\n",
      "Quadratic mean diﬀerentiable families play important roles in much of the\n",
      "asymptotic theory developed by Lucien Le Cam. Properties of these families\n",
      "are considered at some length by Le Cam and Yang (2000) and in TSH3,\n",
      "Chapter 12.\n",
      "The Exponential Class\n",
      "Extensive discussions of exponential families are provided by Barndorﬀ-Nielson\n",
      "(1978) and Brown (1986). Morris (1982) deﬁned the natural exponential fam-\n",
      "ily with quadratic variance function (NEF-QVF) class of distributions and\n",
      "showed that much theory could be uniﬁed by appeal to the quadratic vari-\n",
      "ance property. (See also Morris and Lock (2009).)\n",
      "Heavy-Tailed Families\n",
      "Various types of heavy-tailed distributions have been extensively studied, of-\n",
      "ten because of their applications in ﬁnancial analysis.\n",
      "Some of the basic results of subexponential families were developed by\n",
      "Teugels (1975), who also considered their applications in renewal theory. Mul-\n",
      "tivariate subexponential families with somewhat similar properties can be\n",
      "identiﬁed, but their deﬁnition is not as simple as the convergence of the ratio\n",
      "in expression (2.4) (see Cline and Resnick (1992)).\n",
      "Inﬁnitely Divisible and Stable Families\n",
      "Steutel and van Harn (2004) provide a general coverage of inﬁnitely divisible\n",
      "distributions in IR. Inﬁnitely divisible distributions arise often in applications\n",
      "of stochastic processes. Janicki and Weron (1994) discuss such distributions\n",
      "in this context and in other areas of application such as density estimation.\n",
      "Steutel (1970) considers mixtures of inﬁnitely divisible distributions.\n",
      "The discussion of stable distributions in this chapter generally follows\n",
      "the development by Feller (1971), but is also heavily inﬂuenced by Breiman\n",
      "(1968). Stable distributions also provide useful models for heavy-tailed pro-\n",
      "cesses. Samorodnitsky and Taqqu (1994) provide an extensive discussion of\n",
      "stable distributions in this context. Stable distributions are useful in studies\n",
      "of the robustness of statistical procedures.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "201\n",
      "Exercises\n",
      "2.1. Prove Theorem 2.2.\n",
      "2.2. State the conditions on the parameters of a beta(α, β) distribution for its\n",
      "PDF to be\n",
      "a) subharmonic\n",
      "b) superharmonic\n",
      "c) harmonic\n",
      "2.3. a) Show that condition (2.3) on page 166 implies condition (2.2).\n",
      "b) Find a counterexample to show that the converse is not true.\n",
      "2.4. a) Show that condition (2.4) on page 166 implies condition (2.3); that is,\n",
      "a subexponential family is a long-tailed family.\n",
      "Hint: Note that condition (2.3) is limx→∞F(x + t)/F (x) = 1. (See\n",
      "Athreya and Ney (1972) page 148, and Pitman (1980).)\n",
      "b) Find a counterexample to show that the converse is not true.\n",
      "2.5. a) Show that the following families of distributions are monotone likeli-\n",
      "hood ratio families.\n",
      "i. The one-parameter exponential class, with PDF\n",
      "exp (η(θ)T(x) −ξ(θ)) h(x),\n",
      "with θ ∈]a, b[⊆IR, where a and b are known and may be inﬁnite,\n",
      "and η(θ) is a monotone scalar function of θ.\n",
      "ii. U(0, θ), with PDF\n",
      "1\n",
      "θ I[0,θ](x).\n",
      "iii. U(θ, θ + 1), with PDF\n",
      "I[θ,θ+1](x).\n",
      "b) Show that the one-parameter Cauchy family is not a monotone likeli-\n",
      "hood ratio family. The Lebesgue PDF is\n",
      "1\n",
      "πβ(1 + x/β)2 .\n",
      "2.6. Show that a totally positive family with r = 2 (see equation (2.5)) is a\n",
      "monotone likelihood ratio family.\n",
      "2.7. Assume that log pθ(x), where pθ(x) is a PDF, is strictly concave in x.\n",
      "a) Show that pθ(x) is unimodal. Now, generalize this result (beyond the\n",
      "log function) and prove the generalization.\n",
      "b) Give an example of a family of distributions that is (strictly) unimodal\n",
      "but not strongly unimodal.\n",
      "c) Now for θ ∈IR, show that pθ(x) is a monotone likelihood ratio family\n",
      "in θ.\n",
      "2.8. Write the likelihood rato for each of the one-parameter families of distri-\n",
      "butions in Table 2.1, and show that it is monotone in the relevant variable.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "202\n",
      "2 Distribution Theory and Statistical Models\n",
      "2.9. Show that the Cauchy(γ, β0) family (with β0 known and ﬁxed) does not\n",
      "have a monotone likelihood rato.\n",
      "2.10. Benford’s law is used as a model of the probability distribution of the dig-\n",
      "its, particularly the ﬁrst digit, in the decimal representation of “naturally\n",
      "occurring” numbers, such as the lengths of rivers of the world, the areas\n",
      "of countries, and so on. By Benford’s law, the probability that the ﬁrst\n",
      "digit is d = 1, 2, . . ., 9 is\n",
      "p(d) = log10(d + 1) −log10(d),\n",
      "d = 1, 2, . . ., 9.\n",
      "This law has been used to detect artiﬁcially constructed data because\n",
      "data generated by a person tends to have ﬁrst digits that are uniformly\n",
      "distributed over {1, 2, . . ., 9}.\n",
      "a) There are many instances in which this law does not apply, of course.\n",
      "If all data in some speciﬁc area are between 300 and 500, say, then\n",
      "obviously the law would not be applicable. What is needed to know\n",
      "the distribution of the number being represented.\n",
      "Derive the probability function p(d) for the case that d is the ﬁrst digit\n",
      "in the decimal representation of the realization of a random variable\n",
      "with distribution U(0, 1).\n",
      "b) Of course, if this law is to correspond to naturally occurring numbers\n",
      "such as lengths of rivers, it must be invariant to the unit or measure-\n",
      "ment. To show exact invariance would require an assumption about\n",
      "the distribution of the number being represented (the lengths of rivers,\n",
      "for example). This in itself is not a straightforward task.\n",
      "Rather than making speciﬁc statements about the underlying distribu-\n",
      "tions, develop a heuristic argument that the Benford’s law probability\n",
      "function is approximately invariant to unit of measurement, and that\n",
      "it is the unique probability function with this approximate invariance\n",
      "property. (These approximate properties are often stated as facts (the-\n",
      "orems), and a heuristic argument, probably similar to yours is given\n",
      "as their “proofs”.)\n",
      "2.11. a) Write the PDF of each family of distributions listed in Table 2.2 in\n",
      "the form of equation (2.7).\n",
      "b) Write the PDF of each family of distributions listed in Table 2.2 in\n",
      "the form of equation (2.11), and identify the natural parameter space\n",
      "H.\n",
      "c) Which of the families of distributions listed in Table 2.2 are natural\n",
      "(linear) exponential families?\n",
      "2.12. Show that each of the distributions listed in Table 2.3 is not a member of\n",
      "the exponential class.\n",
      "2.13. Represent the probability mass functions of the Poisson, the binomial,\n",
      "and the negative binomial distributions as members of the discrete power\n",
      "series class; that is, for each of these distributions, identify θ, hx, and c(θ)\n",
      "in equation (2.16).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "203\n",
      "2.14. Show that the positive Poisson family, with PDF as given in equa-\n",
      "tion (2.44), is of the discrete power series class, and hence of the ex-\n",
      "ponential class.\n",
      "2.15. Suppose that X1, . . ., Xn are iid as a discrete power series(θ) distribu-\n",
      "tion, with given series {hx} and normalizing constant c(θ). Show that\n",
      "T = Pn\n",
      "i=1 Xi has a discrete power series distribution, and identify the\n",
      "corresponding terms in its probability mass function.\n",
      "2.16. Consider the family of 2-variate distributions with PDF\n",
      "1\n",
      "(2π)|Σ|1/2 exp\n",
      "\u0000−(x −µ)TΣ−1(x −µ)/2\n",
      "\u0001\n",
      ",\n",
      "where µ is a 2-vector of constants and Σ is a 2×2 positive deﬁnite matrix\n",
      "of constants.\n",
      "Show that this family is of the exponential class, and express the density\n",
      "in the canonical (or natural) form of the exponential class.\n",
      "2.17. Show that both G and eG in Exmaple 2.3 on page 179 are groups.\n",
      "2.18. a) Show that each of the distributions listed in Table 2.4 is a location\n",
      "family.\n",
      "b) Show that each of the distributions listed in Table 2.5 is a scale family.\n",
      "2.19. Show that the likelihood function and the PDF for a location familiy are\n",
      "the same function. Produce graphs similar to those in Figure 1.2 for the\n",
      "exponential(α, θ0) family.\n",
      "2.20. Show that these distributions are not location-scale families:\n",
      "the usual one-parameter exponential family, the binomial family, and the\n",
      "Poisson family.\n",
      "2.21. Show that a full rank exponential family is complete.\n",
      "2.22. Prove Theorem 2.1.\n",
      "2.23. Show that the normal, the Cauchy, and the Poisson families of distribu-\n",
      "tions are all inﬁnitely divisible.\n",
      "2.24. Show that the Poisson family of distributions is not stable.\n",
      "2.25. Show that the normal and the Cauchy families of distributions are stable,\n",
      "and show that their indexes of stability are 2 and 1 respectively.\n",
      "2.26. Express the PDF of the curved normal family N(µ, µ2) in the canonical\n",
      "form of an exponential family.\n",
      "2.27. Higher dimensions.\n",
      "a) Let the random variable X have a uniform distribution within the\n",
      "ball ∥x∥2 ≤1 in IRd. This is a spherical distribution. Now, for given\n",
      "0 < δ < 1, show that\n",
      "Pr(∥X∥> 1 −δ) →1\n",
      "as d increases without bound.\n",
      "b) Let the random variable X have a d-variate standard normal distri-\n",
      "bution distribution, Nd(0, Id). Determine\n",
      "lim\n",
      "d→∞Pr(∥X∥> 1).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "204\n",
      "2 Distribution Theory and Statistical Models\n",
      "2.28. a) Show that the joint density of X, Y1, . . ., Yn given in equations (2.27),\n",
      "(2.31), and (2.32) is proportional to\n",
      "exp \u0000−n(¯x −µ)2)/2σ2\u0001 exp \u0000−(y2\n",
      "1 + · · · + y2\n",
      "n)/2σ2\u0001 .\n",
      "b) Show that the joint density of W1, . . ., Wn−1 given in equation (2.35)\n",
      "is the same as n −1 iid N(0, σ2) random variables.\n",
      "2.29. Higher dimensions. Let X ∼Nd(0, I) and consider\n",
      "E\n",
      "\u0012\n",
      "1\n",
      "∥X∥2\n",
      "2\n",
      "\u0013\n",
      ".\n",
      "a) Show that for d ≤2, this expectation is inﬁnite.\n",
      "b) Show that for d ≥3, this expectation is ﬁnite. What is the value of\n",
      "this expectation as d →∞?\n",
      "2.30. Prove Theorem 2.4.\n",
      "2.31. Work through the details of the proof of Geary’s theorem (Theorem 2.7)\n",
      "for the case that X1, . . ., Xn are iid d-vectors.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3\n",
      "Basic Statistical Theory\n",
      "The ﬁeld of statistics includes various areas, such as descriptive statistics in-\n",
      "cluding statistical graphics, oﬃcial statistics, exploratory data analysis includ-\n",
      "ing data mining and statistical learning, and statistical inference, including\n",
      "forecasting or predictive inference. Statistical learning generally involves pre-\n",
      "dictive inference, so in that sense it is also part of the broad area of statistical\n",
      "inference. Mathematical statistics is generally concerned with the theory un-\n",
      "derlying statistical inference. Most of the important advances in mathematical\n",
      "statistics have been driven by real applications.\n",
      "In probability theory, we develop models of probability distributions and\n",
      "consider the characteristics of random variables generated by such models. The\n",
      "ﬁeld of statistics is concerned with an inverse problem; we have realizations\n",
      "of random variables from which we want to infer characteristics of the models\n",
      "of probability distributions.\n",
      "We develop methods of statistical inference using probability theory. Sta-\n",
      "tistical inference, as I describe it, requires data. Some people describe any\n",
      "probabilistic reasoning as “statistical inference”, and they actually use the\n",
      "term “no-data problem” to describe such a process of computing expected\n",
      "values under various scenarios.\n",
      "Data are the observable output from some data-generating process. The\n",
      "data-generating process can often be described in terms of a physical model.\n",
      "For statistical analysis, however, the data-generating process is described by\n",
      "an abstract probability distribution P , and this model may involve unob-\n",
      "servable quantities such as parameters or latent variables. The objective in\n",
      "statistical inference is to make decisions about unknown aspects of either the\n",
      "data-generating process itself or the probability distribution P . Whether we\n",
      "emphasize the data-generating process or the assumed probability distribu-\n",
      "tion may aﬀect our methods of inference. An issue that will arise from time\n",
      "to time is whether or not all aspects of the data-generating process should\n",
      "aﬀect the inference about P , or whether the inference should be based solely\n",
      "on the data and the assumed probability distribution P . If we have only the\n",
      "data and no knowledge of the process by which it was generated (that is, we\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "206\n",
      "3 Basic Statistical Theory\n",
      "lack “metadata”), then we may be limited in the methods of inference that\n",
      "we can use.\n",
      "The Canonical Problems in Statistical Inference\n",
      "The basic problem in statistical inference is to develop more precise models\n",
      "of the data-generating process that gave rise to a set of observed data. The\n",
      "problem we generally address, however, is that of reﬁning the probability\n",
      "distribution P by making decisions about unknown aspects of this assumed\n",
      "distribution. The models themselves are probability distributions or classes\n",
      "of probability distributions. Formal statistical inference (which I will just call\n",
      "“statistical inference”) focuses on the probability distribution.\n",
      "Statistical inference is the process of using observational data from a pop-\n",
      "ulation that is in an assumed family of distributions P to identify another\n",
      "family PH that “more likely” contains the population from which the data\n",
      "arose. In a restricted form of this problem, we have two families PH0 and PH1,\n",
      "and based on available data, we wish to decide which of these gave rise to the\n",
      "data. (This setup follows the Neyman-Pearson paradigm of hypothesis test-\n",
      "ing; there are various approaches, however.) In another restricted form of this\n",
      "problem, we have a single hypothesized family PH0, and based on available\n",
      "data, we wish to determine the plausibility that this family gave rise to the\n",
      "data. (This setup follows the Fisherian paradigm of signiﬁcance testing; there\n",
      "are various approaches, however.)\n",
      "The assumed family P must be broad enough to allow reasonable inferences\n",
      "from observational data. The choice of P may be somewhat subjective. It\n",
      "is based on whatever may be known or assumed about the data-generating\n",
      "process being studied. Generally, PH is a subfamily, PH ⊆P, but it may be\n",
      "the case that the data indicates that the original family P is not rich enough\n",
      "to contain a family of distributions that matches the observational data.\n",
      "Another way of describing the problem is to assume that we have a family\n",
      "of probability distributions P = {PΘ}, where Θ may be some parameter\n",
      "in a real-valued parameter space Θ (“parametric inference”), or Θ may just\n",
      "be some index in an index set I to distinguish one distribution, PΘ1, from\n",
      "another, PΘ2 (“nonparametric inference”). The parameter or the index is not\n",
      "observable; however, we assume PΘ1 ̸= PΘ2 if Θ1 ̸= Θ2 a.s. (This assumption\n",
      "guarantees “identiﬁability”. The almost sure condition is given so as not to\n",
      "exclude the possibility that Θ is a function.)\n",
      "Often the observable variable of interest has associated covariates. The\n",
      "inference problems described above involve development of probability models\n",
      "that include the covariates. When there are covariates, there are two related\n",
      "problems of statistical inference. One is to use use a set of observed data to\n",
      "develop a model to predict some aspect of future observations for a given value\n",
      "of the covariates. Another is to develop a rule to “estimate” an unobservable\n",
      "random variable, given observed covariates. (A common instance of this latter\n",
      "problem is called “classiﬁcation”.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3 Basic Statistical Theory\n",
      "207\n",
      "How Does PH Diﬀer from P?\n",
      "What we know about Θ, or more to the point, what we assume about Θ\n",
      "determine some of the details of the inference procedures. We may, for exam-\n",
      "ple, assume that Θ = θ, some ﬁxed but unknown real quantity. In that case,\n",
      "whether we view Θ as a parameter space or some more general index set, we\n",
      "may state our objective in statistical inference as to move from\n",
      "P = {Pθ | θ ∈Θ}\n",
      "(3.1)\n",
      "to\n",
      "PH = {Pθ | θ ∈ΘH}\n",
      "(3.2)\n",
      "by using observed data. On the other hand, we may assume that Θ is some\n",
      "Borel-measurable function. If Θ is a random variable, our interest may be in\n",
      "its distribution. In that case, the canonical problem in statistical inference,\n",
      "begins with a class of populations\n",
      "P = {PΘ | Θ ∼Q0 ∈Q},\n",
      "(3.3)\n",
      "where Θ is a random variable and Q0 is some “prior distribution”, and, using\n",
      "observed data, arrives at the class of populations\n",
      "PH = {PΘ | Θ ∼QH ∈Q},\n",
      "(3.4)\n",
      "where QH is some “posterior distribution” conditional on the observations.\n",
      "As we mentioned above, the choice of P, whether in the form of (3.1)\n",
      "or (3.3), is rather subjective. The form of equation (3.3) is “more subjective”,\n",
      "in the sense that Q0 allows direct incorporation of prior beliefs or subjective\n",
      "evidence. Statistical inference in the paradigm of equations (3.3) and (3.4)\n",
      "is sometimes referred to as “subjective inference”, and is said to based on\n",
      "“subjective probability”. We will consider this approach in more detail in\n",
      "Chapter 4.\n",
      "Conﬁdence, Signiﬁcance, and Posterior Conditional Distributions\n",
      "Statistical inference is a process of making decisions in the face of uncertainty.\n",
      "If there is no uncertainty, statistical inference is not a relevant activity. Given\n",
      "the uncertainty, the decision that is made may be “wrong”.\n",
      "Statistical inference must be accompanied by some quantiﬁcation of how\n",
      "“likely” the decision is to be “correct”. Exactly how this should be done is\n",
      "a very deep question whose answers may involve careful consideration of the\n",
      "philosophical foundations of statistical inference. In this book we will not get\n",
      "involved in these foundations. Rather, we will consider some speciﬁc ways of\n",
      "trying to come to grips with the question of quantifying the uncertainty in our\n",
      "decisions. Two ways this is done involve the paradigm of repetitive sampling\n",
      "from a stationary data-generating process, which leads to the concepts of\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "208\n",
      "3 Basic Statistical Theory\n",
      "“conﬁdence” and “signiﬁcance”. The formulation of the problem of statistical\n",
      "inference as in equations (3.3) and (3.4) avoids a direct confrontation of the\n",
      "question of how likely the decision is to be correct or incorrect. The conclusion\n",
      "in that kind of setup is the simple statement that the conditional distribution\n",
      "of Θ is QH, given that its marginal distribution is Q0 and the conditional\n",
      "distribution of the data is PΘ.\n",
      "Examples\n",
      "As an example of the approach indicated in equations (3.1) and (3.2), assume\n",
      "that a given sample y1, . . ., yn is taken in some prescribed manner from some\n",
      "member of a family of distributions\n",
      "P = {N(µ, σ2) | µ ∈IR, σ2 ∈IR+}.\n",
      "Statistical inference in this situation may lead us to place the population\n",
      "giving rise to the observed sample in the family of distributions\n",
      "PH = {N(µ, σ2) | µ ∈[µ1, µ2], σ2 ∈IR+}\n",
      "(think conﬁdence intervals!). The process of identifying the subfamily may be\n",
      "associated with various auxiliary statements (such as level of “conﬁdence”).\n",
      "As another example, we assume that a given sample y1, . . ., yn is taken\n",
      "independently from some member of a family of distributions\n",
      "P = {P | P ≪λ},\n",
      "where λ is the Lebesgue measure, and our inferential process may lead us to\n",
      "decide that the sample arose from the family\n",
      "PH =\n",
      "\u001a\n",
      "P | P ≪λ and\n",
      "Z t\n",
      "−∞\n",
      "dP = .5 ⇒t ≥0\n",
      "\u001b\n",
      "(think hypothesis tests concerning the median!).\n",
      "Notice that “P ” in the example above is used to denote both a population\n",
      "and the associated probability measure; this is a notational convention that\n",
      "we adopted in Chapter 1 and which we use throughout this book.\n",
      "Statistical inference following the setup of equations (3.3) and (3.4) can\n",
      "be illustrated by observable data that follows a Bernoulli distribution with a\n",
      "parameter Π which, in turn, has a beta marginal distribution with parameters\n",
      "α and β. (That is, in equation (3.3), Θ is Π, PΠ is a Bernoulli distribution with\n",
      "parameter Π, and Q0 is a beta distribution with parameters α and β.) Given\n",
      "the single observation X = x, we can work out the conditional distribution of\n",
      "Π to be a beta with parameters x + α and 1 −x + β.\n",
      "It is interesting to note the evolutionary nature of this latter example.\n",
      "Suppose that we began with Q0 (of equation (3.3)) being a beta with pa-\n",
      "rameters x1 + α and 1 −x1 + β, where x1 is the observed value as described\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3 Basic Statistical Theory\n",
      "209\n",
      "above. (That is, the marginal “prior” is in the same parametric family of\n",
      "distributions as the original conditional “posterior”.) Now, given the single\n",
      "observation X = x2, we can work out the conditional distribution of Π to be\n",
      "a beta with parameters x1 +x2 +α and 2−x1 −x2 +β, which of course would\n",
      "be the same conclusion we would have reached if we had begun as originally\n",
      "described, but had observed a sample of size 2, {x1, x2}.\n",
      "Covariates\n",
      "Often a problem in statistical inference may involve other observable quanti-\n",
      "ties. In that case, we may represent the family of distributions for an observ-\n",
      "able random variable Y (which, of course, may be a vector) in the form\n",
      "P = {Pθ,x},\n",
      "where θ ⊆IRd is an unobservable parameter or index and x is an observable\n",
      "concomitant variable.\n",
      "There are two common types of interesting problems when we have ob-\n",
      "servable covariates. In one case, we have a model of the form\n",
      "Y = g(x; θ) + E,\n",
      "where g is a function of known form, and our objective is to make inferences\n",
      "concerning θ.\n",
      "The other common problem is to predict or “estimate” Y for a given x.\n",
      "If we assume an additive model such as that above, predictive inference on\n",
      "Y is based on the inferences concerning θ. In other situations, such as in the\n",
      "problem of classiﬁcation, the random variable Y represents some class of data,\n",
      "and given x, the objective is to “estimate” Y .\n",
      "Asymptotic Considerations\n",
      "Statistical inference is based on an observed sample of the random variable\n",
      "of interest along with observed values of any concomitant variable. Often the\n",
      "precision with which we can state conclusions depends on the dimensions of\n",
      "the parameter and any concomitant variables (that is on the structure of P),\n",
      "as well as on the size of the sample. For any statistical procedure, it is of\n",
      "interest to know how the precision improves with increasing sample size.\n",
      "Although we cannot have an inﬁnite sample size, we often carry the math-\n",
      "ematical analysis to the limit. This may give us an indication of how well the\n",
      "statistical methods perform with increasing sample sizes. Another reason that\n",
      "we often consider limiting cases is that asymptotic properties are often more\n",
      "mathematically tractable than properties for ﬁnite samples, and we use the\n",
      "asymptotic properties as approximations for the actual ﬁnite case.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "210\n",
      "3 Basic Statistical Theory\n",
      "Statistical Reasoning\n",
      "We have indicated above that there may be a diﬀerence in inference regarding\n",
      "a data-generating process of interest and inference regarding an assumed prob-\n",
      "ability distribution governing the data-generating process, or at least some\n",
      "aspects of that process.\n",
      "Statistical inference leads us to select a subfamily of distributions. The\n",
      "particular subfamily may depend on how we formulate the inference problem\n",
      "and how we use a standard method of statistical inference. We must ensure\n",
      "that the formulation of the problem will lead to a reasonable conclusion.\n",
      "An example in which the usual conclusions of a statistical inference proce-\n",
      "dure may not be justiﬁed is in common types of tests of statistical hypotheses.\n",
      "We must be careful in the speciﬁcation of the alternative hypotheses. The two\n",
      "sample Wilcoxon statistic or the Mann-Whitney statistic is often used to test\n",
      "whether one population has a larger median than another population. While\n",
      "this test does have a connection with the medians, the procedure actually tests\n",
      "that the distributions of two populations are the same versus the alternative\n",
      "that a realization from one distribution is typically smaller (or larger) than a\n",
      "realization from the other distribution. If the distributions have quite diﬀerent\n",
      "shapes, a typical value from the ﬁrst population may tend to be smaller than\n",
      "a typical value from the second population, and the relationships between the\n",
      "medians may not aﬀect the results of the test. See Example 5.22 for further\n",
      "discussion of these issues.\n",
      "***************\n",
      "Data-generating process versus a given probability distribution\n",
      "***************\n",
      "waiting time paradox\n",
      "Renewal process paradox\n",
      "***************\n",
      "An example in which clear statistical reasoning may not follow the lines\n",
      "of “common sense” is the Monte Hall problem. The name of this problem is\n",
      "derived from the host of a television game show that was ﬁrst aired in 1963.\n",
      "While the problem itself does not involve statistical inference, it illustrates\n",
      "use of a probability model to make a decision.\n",
      "***************\n",
      "Subjective Statistical Inference\n",
      "In Chapter 1, I referred to the role of “beliefs” in applications of probabil-\n",
      "ity theory and the diﬀering attitudes of “objectivists” and “subjectivists”.\n",
      "The two diﬀerent starting points of statistical inference expressed in equa-\n",
      "tions (3.1) and (3.3) establish the diﬀerent attitudes. These diﬀerences lead\n",
      "to diﬀerences in the fundamental ways in which statistical inference is ap-\n",
      "proached. The diﬀerences involve how prior beliefs are incorporated into the\n",
      "inference process. The diﬀerences also lead to diﬀerences in how the concept\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "211\n",
      "of probability (however it is interpreted) is used in the interpretation of the\n",
      "results of statistical analyses. In an objective approach, we may speak of the\n",
      "probability, given a data-generating process, of observing a given set of values.\n",
      "In a subjective approach, given a set of values, we may speak of the probabil-\n",
      "ity that the data-generating process has certain properties. This latter setting\n",
      "may lead to the use of the phrase “inverse probability”, although this phrase\n",
      "is often used in diﬀerent ways.\n",
      "The objective and subjective approaches also diﬀer in how the overall\n",
      "data-generating process aﬀects the inferences on the underlying probability\n",
      "distribution P . (This diﬀerence is associated with the “likelihood principle”,\n",
      "which I shall mention from time to time.) In this chapter I give a brief overview\n",
      "of statistical inference without much emphasis on whether the approach is\n",
      "“objective” or “subjective”.\n",
      "Ranks of Mathematical Objects\n",
      "In statistical inference we deal with various types of mathematical objects. We\n",
      "would like to develop concepts and methods that are independent of the type\n",
      "of the underlying objects, but that is not always possible. Occasionally we\n",
      "will ﬁnd it necessary to discuss scalar objects, rank one objects (vectors), and\n",
      "rank two objects (matrices) separately. In general, most degree-one properties,\n",
      "such as expectations of linear functions, can be considered uniformly across\n",
      "the diﬀerent types of mathematical objects. Degree-two properties, such as\n",
      "variances, however, must usually be considered separately for scalars, vectors,\n",
      "and matrices.\n",
      "Matrices often require special consideration because of the richness of that\n",
      "kind of structure. Sometimes we must consider the special cases of symmetric\n",
      "matrices, full-rank matrices, and positive-deﬁnite matrices.\n",
      "3.1 Inferential Information in Statistics\n",
      "In statistics, we generally assume that we have a random sample of obser-\n",
      "vations X1, . . ., Xn on a random variable X. We usually assume either that\n",
      "they are independent or that they are exchangeable, although we may assume\n",
      "other relations among the variables that depend on the sampling design.\n",
      "We will often use X to denote a random sample on the random variable\n",
      "X. (This may sound confusing, but it is always clear from the context.) The\n",
      "common distribution of the variables is called the parent distribution of the\n",
      "random sample, and a common objective in statistics is to use the sample to\n",
      "make inferences about properties of the parent distribution.\n",
      "In many statistical applications we also have covariates associated with\n",
      "the observations on the random variable of interest. In this case, in order to\n",
      "conform to common usage in statistics, I will use Y to represent the random\n",
      "variable of interest and X or x to represent the covariates or concomitant\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "212\n",
      "3 Basic Statistical Theory\n",
      "variables. Both the random variable of interest and any covariates are assumed\n",
      "to be observable. There may also be unobservable variables, called “latent\n",
      "variables”, associated with the observed data. Such variables are artifacts of\n",
      "the statistical model and may or may not correspond to phenomena of interest.\n",
      "When covariates are present our interest usually is in the conditional dis-\n",
      "tribution of Y , given X. For making statistical inferences, we generally assume\n",
      "that the conditional distributions of Y1|X1, . . ., Yn|Xn are either conditionally\n",
      "independent or at least conditionally exchangeable.\n",
      "A statistic is any function T of the observables that does not involve any\n",
      "unobservable values. We often use a subscript Tn to indicate the number of\n",
      "observations, but usually a statistic is deﬁned as some formula that applies to\n",
      "a general number of observations (such as the sample mean). While we most\n",
      "often work with statistics based on a random sample, that is, an iid set of\n",
      "variables, or at least based on an exchangeable sample, we may have a statistic\n",
      "that is a function of a general set of random variables, X1, . . ., Xn. We see\n",
      "that if the random variables are exchangeable, then the statistic is symmetric,\n",
      "in the sense that T(Xk1, . . ., Xkn) = T(X1, . . ., Xn) for any indices k1, . . ., kn\n",
      "such that {k1, . . ., kn} = {1, . . ., n}.\n",
      "Statistical Models\n",
      "We assume that the sample arose from some distribution Pθ, which is a mem-\n",
      "ber of some family of probability distributions P. The family of probability\n",
      "distributions P is a statistical model. We fully specify the family P (it can be\n",
      "a very large family), but we assume some aspects of Pθ are unknown. (If the\n",
      "distribution Pθ that yielded the sample is fully known, while there may be\n",
      "some interesting questions about probability, there are no interesting statisti-\n",
      "cal questions.) Our objective in statistical inference is to determine a speciﬁc\n",
      "Pθ ∈P, or some subfamily Pθ ⊆P, that could likely have generated the\n",
      "sample.\n",
      "The distribution may also depend on other observable variables. In general,\n",
      "we assume we have observations Y1, . . ., Yn on Y , together with associated ob-\n",
      "servations on any related variable X or x. We refer to the associated variables\n",
      "as “covariates”. In this context, a statistic, which in our common use of the\n",
      "term is a function that does not involve any unobserved values, may also\n",
      "involve the observed covariates.\n",
      "A general statistical model that includes covariates is\n",
      "Y = f(x ; θ) + E,\n",
      "(3.5)\n",
      "where Y and x are observable variables, f is some unknown function, θ is an\n",
      "unknown parameter, and E is an unobservable random variable with unknown\n",
      "distribution Pτ independent of other quantities in the model. In the usual\n",
      "setup, Y is a scalar random random variable, and x is a p-vector. Given\n",
      "independent observations (Y1, x1), . . ., (Yn, xn), we often use the notation Y\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "213\n",
      "to represent an n-vector, X to represent an n × p matrix whose rows are the\n",
      "xT\n",
      "i , and E to represent an n-vector of iid random variables. A model such\n",
      "as (3.5) is often called a regression model.\n",
      "A common statistical model that expresses a relationship of an observable\n",
      "random variable and other observable variables is the linear model\n",
      "Y = βTx + E,\n",
      "(3.6)\n",
      "where Y is the observable random variable, x is an observable p-vector of\n",
      "covariates, β is an unknown and unobservable p-vector of parameters, and E\n",
      "is an unobservable random variable with E(E) = 0 and V(E) = σ2I. The\n",
      "parameter space for β is B ⊆IRp.\n",
      "A random sample may be written in the vector-matrix form\n",
      "Y = Xβ + E,\n",
      "(3.7)\n",
      "where Y and E are n-vectors, X is an n×p matrix whose rows are the xT\n",
      "i , and\n",
      "β is the p-vector above. (The notation “βTx” in equation (3.6) and “Xβ” in\n",
      "equation (3.7) is more natural in the separate contexts. All vectors are consid-\n",
      "ered to be column vectors. We could of course write “xTβ” in equation (3.6).)\n",
      "Because the linear model is familiar from applications of statistics, we will\n",
      "refer to it from time to time, but we will not study it systematically until\n",
      "Section 5.5.1.\n",
      "In statistical inference, we distinguish observable random variables and\n",
      "“parameters”, but we are not always careful in referring to parameters. We\n",
      "think of two kinds of parameters; “known” and “unknown”. A statistic is a\n",
      "function of observable random variables that does not involve any unknown\n",
      "parameters.\n",
      "Algorithmic Statistical Models\n",
      "There are various types of models that may have diﬀerent purposes. A common\n",
      "form of a model is a mathematical equation or a system of equations. If the\n",
      "purpose of the model is to enhance the understanding of some phenomenon,\n",
      "there would be a large premium on simplicity of the model. If the model is very\n",
      "complicated, it may correspond very well to the reality being studied, but it is\n",
      "unlikely to be understandable. If its primary purpose is to aid understanding,\n",
      "an equation model should be relatively simple..\n",
      "A model may be embedded in a computer program. In this case, the model\n",
      "itself is not ordinarily scrutinized; only its input and output are studied. The\n",
      "complexity of the model is not of essential consequence. Especially if the ob-\n",
      "jective is prediction of a response given values of the associated variables, and\n",
      "if there is a large premium on making accurate predictions or classiﬁcations in\n",
      "a very short time, an algorithmic model may be appropriate. An algorithmic\n",
      "model prioritizes prediction accuracy. The details of the model may be very\n",
      "diﬀerent from the details of the data-generating process being modeled. That\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "214\n",
      "3 Basic Statistical Theory\n",
      "is not relevant; the important thing is how well the output of the algorithmic\n",
      "model compares to the output of the data-generating process being modeled\n",
      "when they are given the same input.\n",
      "The asymmetric relationship between a random variable Y and a variable\n",
      "x may be represented as a black box that accepts x as input and outputs Y :\n",
      "Y ←unknown process ←x.\n",
      "(3.8)\n",
      "The relationship might also be described by a statement of the form\n",
      "Y ←f(x)\n",
      "or\n",
      "Y ≈f(x).\n",
      "(3.9)\n",
      "Asymmetry of Statistical Models; Systematic and Random\n",
      "Components\n",
      "If f has an inverse, the model (3.9) appears symmetric. Even in that case,\n",
      "however, there is an asymmetry that results from the role of random variables\n",
      "in the model. We model the response as a random variable and our methods\n",
      "of analysis would not apply to the model\n",
      "x ≈f−1(Y ).\n",
      "We may think of f(x) as a systematic eﬀect and write the model with an\n",
      "additive adjustment, or error, as\n",
      "Y = f(x) + E\n",
      "(3.10)\n",
      "or with a multiplicative error as\n",
      "Y = f(x)∆,\n",
      "(3.11)\n",
      "where E and ∆are assumed to be random variables. (The “E” is the Greek\n",
      "uppercase epsilon.) We refer to these as “errors”, although this word does not\n",
      "indicate a mistake. In additive models, E is also called the “residual”. The\n",
      "model therefore is composed of a systematic component related to the values\n",
      "of x and a random component that accounts for the indeterminacy between\n",
      "Y and f(x).\n",
      "An objective in statistical analysis often is to understand the systematic\n",
      "and random components better. The relative contribution to the variability\n",
      "in the observed Y due to the systematic component and due to the random\n",
      "component is called the signal to noise ratio. (Notice that this is a nontechnical\n",
      "term here; we could quantify it more precisely in certain classes of models.\n",
      "We can view an F ratio in analysis of variance as a type of quantiﬁcation of\n",
      "a signal to noise ratio.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "215\n",
      "An additive model has the advantage of separability of ﬁrst order expec-\n",
      "tations of the two components no matter what assumptions are made about\n",
      "joint probability distributions of elements of the one component and those of\n",
      "the other. Note a questionable requirement for this separability: the variance\n",
      "of the residual component must be constant no matter what the magnitude\n",
      "of the expectation of the systematic component. Despite these issues, in the\n",
      "following, we will concentrate on models with additive random eﬀects.\n",
      "In the case of the black-box model (3.8), both the systematic and random\n",
      "components are embedded in the box. The objectives of statistical analysis\n",
      "may be to identify the individual components or, more often, to determine\n",
      "“average” or “most likely” output Y for given input x.\n",
      "Because the functional form f of the relationship between Y and x may\n",
      "contain a parameter, we may write the equation in the model as\n",
      "Y = f(x; θ) + E,\n",
      "(3.12)\n",
      "where θ is a parameter whose value determines a speciﬁc relationship within\n",
      "the family speciﬁed by f. In most cases, θ is a vector. In the usual linear re-\n",
      "gression model, for example, the parameter is a vector with two more elements\n",
      "than the number of elements in x,\n",
      "Y = β0 + xTβ + E,\n",
      "(3.13)\n",
      "where θ = (β0, β, σ2).\n",
      "3.1.1 Statistical Inference: Point Estimation\n",
      "Statistical inference is a process of identifying a family of distributions that\n",
      "generated a given set of observations. The process begins with an assumed\n",
      "family of distributions P. This family may be very large; for example, it may\n",
      "be the family of distributions with probability measures dominated by the\n",
      "counting measure. Often the assumed family is narrowly deﬁned; for example,\n",
      "it may be the family of univariate normal distributions. In any event, the\n",
      "objective of statistical inference is to identify a subfamily, PH ⊆P, that\n",
      "contains the population from which the data arose.\n",
      "Types of Statistical Inference\n",
      "There are various types of inference related to the problem of determining\n",
      "the speciﬁc Pθ ∈P or else some speciﬁc element of θ. Some of these are\n",
      "point estimation, hypothesis tests, conﬁdence sets, and function estimation\n",
      "(estimation of the PDF, for example). Hypothesis tests and conﬁdence sets\n",
      "are associated with probability statements that depend on Pθ.\n",
      "Beginning on page 12 and later in Chapter 2, we have distinguished fam-\n",
      "ilies of probability distributions as either parametric or nonparametric. In\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "216\n",
      "3 Basic Statistical Theory\n",
      "statistical inference, we also speak of methods as being either parametric or\n",
      "nonparametric. The meanings in this case are somewhat nebulous, however.\n",
      "We often refer to a method as “nonparametric” if it does not make use of spe-\n",
      "ciﬁc properties of the assumed family of distributions. We also use the term\n",
      "“nonparametric” if the assumed family of distributions is very broad, or if it\n",
      "includes more than one of the standard parametric families.\n",
      "In parametric settings, each of the types of inference listed above concerns a\n",
      "parameter, θ, in a parameter space, Θ ⊆IRk. In this case, function estimation\n",
      "is essentially estimation of a parameter that determines the function.\n",
      "In some cases whether the inference is “parametric” or “nonparametric”\n",
      "is not determined by the nature of the assumed family of probability distri-\n",
      "butions. Parametric statistical inference often may involve only some element\n",
      "of the parameter (for example, estimation only of the mean and not the vari-\n",
      "ance), and so we may, eﬀectively, perform parametric inference in a nonpara-\n",
      "metric family by an ad hoc deﬁnition of a “parameter”, say the mean or the\n",
      "median of a distribution in the family.\n",
      "In parametric inference about a parameter θ, we generally identify a Borel\n",
      "function, say g or h, of that parameter and then consider inference speciﬁcally\n",
      "on g(θ) or h(θ).\n",
      "Whether the object of the statistical inference is a scalar or some other\n",
      "element of a vector space, often makes a diﬀerence in the simplicity of the\n",
      "inference procedures. We will often emphasize the “one-parameter” case be-\n",
      "cause it is simpler. There are a large number of useful families of distributions\n",
      "that are indeed characterized by a single parameter, such as the binomial or\n",
      "the (one-parameter) exponential.\n",
      "Two related problems in inference are prediction and causal inference. For\n",
      "either of these problems, in addition to the random variable Y with the prob-\n",
      "ability triple (Ω, F, P ), we have a measurable function X that maps (Ω, F)\n",
      "to (Λ, G). Our interest is in the probability measure on the (Λ, G) space. This\n",
      "is the conditional distribution of Y |X. In either case, the strength of the in-\n",
      "ference depends on the extent of the diﬀerence in the conditional distribution\n",
      "of Y |X and the marginal distribution of Y .\n",
      "For prediction, given X, we wish to predict Y ; that is, we seek a Borel\n",
      "function g such that E(g(X)) is “close to” E(Y ). We will discuss issues in\n",
      "prediction in Section 3.1.6.\n",
      "In causal inference, as in prediction, we have an associated measurable\n",
      "function XY , but the values of this function may be said to “cause” the\n",
      "conditional distribution of Y |X to be diﬀerent from the marginal distribution\n",
      "of Y . The methods of causal inference invoke an intermediary variable that\n",
      "depends on X and on which Y depends. We will not consider causal inference\n",
      "in this text. It is a popular topic in research in the social “sciences”; see, for\n",
      "example, Morgan and Winship (2007).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "217\n",
      "The Basic Paradigm of Point Estimation\n",
      "We will focus our attention in the remainder of this section on point estima-\n",
      "tion.\n",
      "The setting for point estimation is a real-valued observable random vari-\n",
      "able X that has a distribution that depends in some way on a real-valued sta-\n",
      "tistical function (in the sense of a distribution measure, as in Section 1.1.9).\n",
      "Often the statistical function can be viewed as a parameter θ that takes a\n",
      "value in the set Θ. We assume we have observations X1, . . ., Xn on X.\n",
      "The statistical function to be estimated is called the estimand. Although\n",
      "it may be an underlying natural parameter, it is often a Borel function of that\n",
      "parameter. We will use g(θ) to represent the estimand. (Some authors use the\n",
      "symbol ϑ for the function of θ that is the estimand.) We want to estimate g(θ)\n",
      "using observations on X. We denote the estimator as T(X). We think of T as\n",
      "a rule or formula. We also use T to denote a decision in hypothesis testing. We\n",
      "also denote the rule as δ(X). In many cases, there is an observable covariate,\n",
      "and so the notation we have just adopted would be modiﬁed to indicate the\n",
      "presence of the covariate.\n",
      "Approaches to Estimation\n",
      "There are several approaches to estimation of g(θ). We generally assume that\n",
      "a speciﬁc value of g(θ) results in a speciﬁc distribution for X. If d\n",
      "g(θ) is an\n",
      "estimate of g(θ) and we make the substitution g(θ) = d\n",
      "g(θ) we have a speciﬁc\n",
      "family of distributions with CDF P d\n",
      "g(θ), say.\n",
      "In most cases we assume that the set Θ is open, and hence the range of\n",
      "g(θ) is open. We generally allow a point estimator to be in the closure of those\n",
      "spaces. For example, in the case of a binomial(n, π) distribution, the parameter\n",
      "space for π is usually taken to be ]0, 1[; however, a “good” estimator of π may\n",
      "take the value 0 or 1. A common approach to estimation, called maximum\n",
      "likelihood estimation, explicitly requires that the estimator be in the closure\n",
      "of the parameter space instead of the parameter space itself. (See page 242.)\n",
      "Other “good” estimators may not even be in the closure of the parameter\n",
      "space; see Example 5.31 on page 436.\n",
      "A good estimation scheme is one that speciﬁes a distribution of X that\n",
      "corresponds in some sense to the observed values of X. We start on the prob-\n",
      "lem by deﬁning some computable, heuristic estimation procedure, and then\n",
      "analytically study the properties of that procedure under various scenarios,\n",
      "that is, under diﬀerent assumed distributions.\n",
      "Optimal Point Estimators\n",
      "We seek an estimator with “good” properties. We will brieﬂy discuss some de-\n",
      "sirable properties for estimators: small bias, small mean squared error, Pitman\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "218\n",
      "3 Basic Statistical Theory\n",
      "closeness, and equivariance under transformations. In this section we consider\n",
      "these properties only in the context of estimation, but the same properties\n",
      "have meaning in other types of statistical inference, although the speciﬁc def-\n",
      "inition may be diﬀerent. “Good” can be deﬁned either in terms of various\n",
      "measures associated with an estimator (bias, mean squared error, Pitman\n",
      "closeness), or as a property that the estimator either possesses or does not\n",
      "(equivariance).\n",
      "Bias\n",
      "The bias of T(X) for estimating g(θ) is\n",
      "E(T(X)) −g(θ).\n",
      "(3.14)\n",
      "One of the most commonly required desirable properties of point estima-\n",
      "tors is unbiasedness.\n",
      "Deﬁnition 3.1 (unbiased point estimator)\n",
      "The estimator T(X) is unbiased for g(θ) if\n",
      "E(T(X)) = g(θ)\n",
      "∀θ ∈Θ.\n",
      "Unbiasedness is a uniform property of the expected value.\n",
      "We can also deﬁne other types of unbiasedness in terms of other aspects\n",
      "of a probability distribution. For example, an estimator whose median is the\n",
      "estimand is said to be median-unbiased.\n",
      "Unbiasedness has diﬀerent deﬁnitions in other settings (estimating func-\n",
      "tions, for example, see page 255) and for other types of statistical inference (for\n",
      "example, testing, see page 296, and determining conﬁdence sets, see page 300),\n",
      "but the meanings are similar.\n",
      "If two estimators are unbiased, we would reasonably prefer one with smaller\n",
      "variance.\n",
      "Mean Squared Error\n",
      "Another measure of the goodness of a scalar estimator is the mean squared\n",
      "error or MSE,\n",
      "MSE(T(x)) = E((T(X) −g(θ))2),\n",
      "(3.15)\n",
      "which is the square of the bias plus the variance:\n",
      "MSE(T(x)) = (E(T(X)) −g(θ))2 + V(T(X)).\n",
      "(3.16)\n",
      "An example due to C. R. Rao (Rao, 1981) causes us to realize that we\n",
      "often face a dilemma in ﬁnding a good estimate. A “bad” estimator may have\n",
      "a smaller MSE than a “good” estimator.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "219\n",
      "Example 3.1 an unreasonable estimator with a smaller MSE\n",
      "Suppose we have n observations X1, . . ., Xn from a distribution with mean µ1\n",
      "and ﬁnite standard deviation σ. We wish to estimate µ1. An obvious estimator\n",
      "is the sample mean X. (We will see that this is generally a good estimator\n",
      "under most criteria.) The MSE of X is σ2/n. Now, suppose we have m obser-\n",
      "vations Y1, . . ., Ym from a diﬀerent distribution with mean µ2 = µ1 + δσ and\n",
      "the same standard deviation σ. Let\n",
      "T = (nX + mY )/(n + m),\n",
      "so we have\n",
      "E((T −µ1)2) =\n",
      "σ2\n",
      "n + m\n",
      "\u0012\n",
      "1 + m2δ2\n",
      "n + m\n",
      "\u0013\n",
      ".\n",
      "Now if δ2 < m−1 + n−1, then\n",
      "MSE(T) < MSE(X);\n",
      "that is, in this case, the MSE is improved by using spurious observations. If\n",
      "δ < 1, just using a single spurious observation improves the MSE.\n",
      "Pitman Closeness\n",
      "While the MSE gives us some sense of how “close” the estimator is to the\n",
      "estimand, another way of thinking about closeness is terms of the probability\n",
      "that |T(X) −g(θ)| is less than some small value ϵ:\n",
      "Pr\n",
      "\u0000|T(X) −g(θ)| < ϵ\n",
      "\f\fθ\n",
      "\u0001\n",
      "ϵ > 0.\n",
      "(3.17)\n",
      "This type of measure is called Pitman closeness.\n",
      "Deﬁnition 3.2 (Pitman-closer; Pitman-closest)\n",
      "Given two estimators T1(X) and T2(X) of g(θ), we say that T1(X) is Pitman-\n",
      "closer than T2(X), if\n",
      "Pr\n",
      "\u0000|T1(X) −g(θ)| ≤|T2(X) −g(θ)|\n",
      "\f\fθ\n",
      "\u0001\n",
      "≥1\n",
      "2\n",
      "(3.18)\n",
      "for all θ ∈Θ and for some θ0 ∈Θ\n",
      "Pr\n",
      "\u0000|T1(X) −g(θ)| < |T2(X) −g(θ)|\n",
      "\f\fθ0\n",
      "\u0001\n",
      "> 1\n",
      "2.\n",
      "(3.19)\n",
      "We say that T1(X) is the Pitman-closest estimator, if T1(X) is Pitman-closer\n",
      "than T(X) for any other statistic T(X).\n",
      "Pitman closeness is aﬀected more by the properties of the distribution in\n",
      "the region of interest, rather than by the behavior of statistics in the tail\n",
      "regions. Measures such as MSE, for example, may be unduly aﬀected by the\n",
      "properties of the distribution in the tail regions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "220\n",
      "3 Basic Statistical Theory\n",
      "Example 3.2 Lack of transitivity of Pitman closeness\n",
      "Although Pitman closeness is a useful measure in evaluating an estimator,\n",
      "the measure lacks the desirable property of transitivity; that is, T1(X) may\n",
      "be Pitman-closer than T2(X) and T2(X) Pitman-closer than T3(X), but yet\n",
      "T3(X) may be Pitman-closer than T1(X). It is easy to construct an example\n",
      "to illustrate that this is possible. Rather than trying to construct a realistic\n",
      "distribution and statistics, let us just consider three independent random vari-\n",
      "ables T1, T2, and T3 and assign probability distributions to them (following\n",
      "Blyth (1972)):\n",
      "Pr(T1 = 3) = 1.0\n",
      "Pr(T2 = 1) = 0.4, Pr(T2 = 4) = 0.6\n",
      "Pr(T3 = 2) = 0.6, Pr(T3 = 5) = 0.4\n",
      "We see that\n",
      "Pr(T1 < T2) = 0.6,\n",
      "Pr(T2 < T3) = 0.64,\n",
      "Pr(T1 < T3) = 0.4.\n",
      "The point is that probability itself is not transitive over inequalities of random\n",
      "variables.\n",
      "This example illustrates the fact that there is often no Pitman-closest\n",
      "estimator.\n",
      "Example 3.3 Pitman closeness of shrunken estimators\n",
      "Efron (1975) gives an example of an otherwise “good” estimator that is not\n",
      "as close in the Pitman sense as a biased estimator.\n",
      "Consider the problem of estimating the mean µ in a normal distribution\n",
      "N(µ, 1), given a random sample X1, . . ., Xn. The usual estimator, the sample\n",
      "mean X, is unbiased and has minimum variance among all unbiased esti-\n",
      "mators, so clearly it is a “good” estimator. Consider, however, the biased\n",
      "estimator\n",
      "T(X) = X −∆n(X),\n",
      "(3.20)\n",
      "where\n",
      "∆n(u) = min(u√n, Φ(−u√n))\n",
      "2√n\n",
      ",\n",
      "for u ≥0,\n",
      "(3.21)\n",
      "in which Φ(·) is the standard normal CDF. This “shrinkage” of X toward 0\n",
      "yields an estimator that is Pitman-closer to the population mean µ than the\n",
      "sample mean X. (Exercise.)\n",
      "On page 273, we will encounter a more dramatic example of the eﬀect of\n",
      "shrinking the sample mean in a multivariate normal distributional model.\n",
      "Equivariance\n",
      "In Section 2.6, beginning on page 178, we discussed families of distributions\n",
      "that are characterized as equivalence classes under groups of transformations\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "221\n",
      "of the random variables. In a parametric setting the group G of transforma-\n",
      "tions of the random variable can be associated with a group G of transforma-\n",
      "tions of the parameter. Likewise, we consider a group of transformations on\n",
      "the estimator, G∗. For g ∈G and g∗∈G∗an estimator T(X) is equivariant if\n",
      "T(g(X)) = g∗(T(X)).\n",
      "(3.22)\n",
      "Some people use the terms “invariant” and “invariance” for equivariant\n",
      "and equivariance, but I prefer the latter terms unless, indeed there is no\n",
      "change in the statistical procedure.\n",
      "For equivariant or invariant statistical procedures, there are issues that\n",
      "relate to other properties of the estimator that must be considered (see, for\n",
      "example, the discussion of L-invariance on page 266). We will discuss the\n",
      "equivariance property of statistical procedures in more detail in Section 3.4.\n",
      "Uniform Properties\n",
      "If the goodness of an estimator does not depend on the parameter, we say\n",
      "the estimator is uniformly good (and, of course, in this statement we would\n",
      "be more precise in what we mean by “good”). All discussions of statistical\n",
      "inference are in the context of some family of distributions, and when we speak\n",
      "of a “uniform” property, we mean a property that holds for all members of\n",
      "the family.\n",
      "Unbiasedness, by deﬁnition, is a uniform property. We will see, however,\n",
      "that many other desirable properties cannot be uniform.\n",
      "Statements of Probability Associated with Statistics\n",
      "Although much of the development of inferential methods emphasizes the\n",
      "expected value of statistics, often it is useful to consider the probabilities\n",
      "of statistics being in certain regions. Pitman closeness is an example of the\n",
      "use of probabilities associated with estimators. Two other approaches involve\n",
      "the probabilities of various sets of values that the statistics may take on.\n",
      "These approaches lead to statistical tests of hypotheses and determination\n",
      "of conﬁdence sets. These topics will be discussed in Section 3.5, and more\n",
      "thoroughly in later chapters.\n",
      "3.1.2 Suﬃciency, Ancillarity, Minimality, and Completeness\n",
      "There are important properties of statistics, such as suﬃciency and complete\n",
      "suﬃciency, that determine the usefulness of those statistics in statistical infer-\n",
      "ence. These general properties often can be used as guides in seeking optimal\n",
      "statistical procedures.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "222\n",
      "3 Basic Statistical Theory\n",
      "Suﬃciency\n",
      "The most fundamental concept in statistical inference is suﬃciency, because\n",
      "it relates functions of observations to the object of the inference.\n",
      "Deﬁnition 3.3 (suﬃcient statistic)\n",
      "Let X be a sample from a population P ∈P. A statistic T(X) is suﬃcient\n",
      "for P ∈P if and only if the conditional distribution of X given T does not\n",
      "depend on P .\n",
      "In general terms, suﬃciency involves the conditional independence from\n",
      "the parameter of the distribution of any other function of the random variable,\n",
      "given the suﬃcient statistic.\n",
      "Suﬃciency depends on the family of distributions, P, wrt which the con-\n",
      "ditional expectation, which ultimately deﬁnes the conditional distribution, is\n",
      "deﬁned. If a statistic is suﬃcient for P, it may not be suﬃcient for a larger\n",
      "family, P1, where P ⊆P1.\n",
      "Suﬃciency determines the nature and extent of any reduction of data that\n",
      "can be made without sacriﬁce of information. Thus, a function of a suﬃcient\n",
      "statistic may not be suﬃcient, but if a suﬃcient statistic can be deﬁned as a\n",
      "measurable function of another statistic, then that other statistic is necessarily\n",
      "suﬃcient (exercise).\n",
      "We can establish suﬃciency by the factorization criterion.\n",
      "Theorem 3.1 (factorization criterion)\n",
      "Let P be a family of distributions dominated by a σ-ﬁnite measure ν. A neces-\n",
      "sary and suﬃcient condition for a statistic T to be suﬃcient for P ∈P is that\n",
      "there exist nonnegative Borel functions gP and h, where h does not depend on\n",
      "P , such that\n",
      "dP/dν(x) = gP (T(x))h(x)\n",
      "a.e. ν.\n",
      "(3.23)\n",
      "Proof.\n",
      "***************\n",
      "If X1, . . ., Xn are iid whose distribution is dominated by a σ-ﬁnite mea-\n",
      "sure, the joint PDF of the order statistics given in equation (1.140) is the\n",
      "same as the joint distribution of all of the (unordered) random variables (see\n",
      "Exercise 1.46), we have the immediate and useful corollary to Theorem 3.1.\n",
      "Corollary 3.1.1 (the order statistics are suﬃcient) The order statistics\n",
      "of a random sample from a distribution dominated by a σ-ﬁnite measure are\n",
      "suﬃcient.\n",
      "Actually, the requirement that the distribution be dominated by a σ-ﬁnite\n",
      "measure is not necessary. The proof is a simple exercise in permutations.\n",
      "If the family of distributions is characterized by a parameter θ, then instead\n",
      "of referring to a statistic as begin suﬃcient for the distribution, we may say\n",
      "that the statistic is suﬃcient for θ.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "223\n",
      "When the density can be written in the separable form c(θ)f(x), unless\n",
      "c(θ) is a constant, the support must be a function of θ, and a suﬃcient statistic\n",
      "for θ must be an extreme order statistic. When the support depends on the\n",
      "parameter, and that parameter does not in any other way characterize the\n",
      "distribution, then the extreme order statistic(s) at the boundary of the support\n",
      "determined by the parameter carry the full information about the parameter.\n",
      "An important consequence of suﬃciency in an estimation problem with\n",
      "convex loss is the Rao-Blackwell theorem (see Section 3.3.2).\n",
      "The Suﬃciency Principle\n",
      "Suﬃciency is such an important concept in statistical inference that it leads\n",
      "to a principle that often guides statistical decisions. The suﬃciency principle\n",
      "states that if T(X) is a suﬃcient statistic for P , and x1 and x2 are results of\n",
      "two independent experiments in P with\n",
      "T(x1) = T(x2)\n",
      "a.e. P,\n",
      "(3.24)\n",
      "then any decision about P based on one experiment should be in agreement\n",
      "with any decision about P based on the other experiment. Principles should,\n",
      "of course, be consistent with objectives. In the introductory section for this\n",
      "chapter, we stated that the objective in statistical inference is to make de-\n",
      "cisions about either the data-generating process or about P . The suﬃciency\n",
      "principle cannot be consistent with an objective of understanding the data-\n",
      "generating process.\n",
      "Ancillarity\n",
      "Often a probability model contains a parameter of no interest for inference.\n",
      "Such a parameter is called a nuisance parameter. A statistic to be used for\n",
      "inferences about the parameters of interest should not depend on any nui-\n",
      "sance parameter. This lack of dependence on a parameter is called ancillarity.\n",
      "Ancillarity is, in a way, the opposite of suﬃciency.\n",
      "Deﬁnition 3.4 (ancillary statistic; ﬁrst-order ancillary statistic)\n",
      "A statistic U(X) is called ancillary for P (or θ) if the distribution of U(X)\n",
      "does not depend on P (or θ). If E(U(X)) does not depend on P (or θ), then\n",
      "U(X) is said to be ﬁrst-order ancillary for P (or θ).\n",
      "Restating the intuitive remark before the deﬁnition, we can say a statistic\n",
      "to be used for inferences about the parameters of interest should be ancillary\n",
      "for a nuisance parameter.\n",
      "In a probability space (Ω, F, Pθ) and random variable X, if U(X) is ancil-\n",
      "lary for Pθ, then the deﬁnition implies that\n",
      "σ(U(X)) ⊂σ(X),\n",
      "(3.25)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "224\n",
      "3 Basic Statistical Theory\n",
      "and, further, for any set B,\n",
      "Pθ((U(X))−1(B)) is constant.\n",
      "(3.26)\n",
      "These facts merely state that the ancillary statistic provides no information\n",
      "about Pθ.\n",
      "Minimal Suﬃciency\n",
      "While the whole sample is a suﬃcient statistic, suﬃciency in this case is not\n",
      "very meaningful. We might more reasonably ask what, if any, statistics of\n",
      "lower dimension are also suﬃcient.\n",
      "Deﬁnition 3.5 (minimially suﬃcient)\n",
      "Let T be a given suﬃcient statistic for P ∈P. The statistic T is minimal\n",
      "suﬃcient if for any suﬃcient statistic for P ∈P, S, there is a measurable\n",
      "function h such that T = h(S) a.s. P.\n",
      "Minimal suﬃciency has a heuristic appeal: it relates to the greatest amount\n",
      "of data reduction that is possible without losing information, in the sense of\n",
      "losing suﬃciency.\n",
      "When the range does not depend on the parameter, we can often establish\n",
      "minimality by use of one of the following two theorems.\n",
      "Theorem 3.2 (minimal suﬃciency I)\n",
      "Let P be a family with densities p0, p1, . . ., pk, all with the same support. The\n",
      "statistic\n",
      "T(X) =\n",
      "\u0012p1(X)\n",
      "p0(X), . . ., pk(X)\n",
      "p0(X)\n",
      "\u0013\n",
      "(3.27)\n",
      "is minimal suﬃcient.\n",
      "Proof.\n",
      "This follows from the following corollary of the factorization theorem.\n",
      "Corollary 3.1.1 (factorization theorem (page 222))\n",
      "A necessary and suﬃcient condition for a statistic T(X) to be suﬃcient for a\n",
      "family P of distributions of a sample X dominated by a σ-ﬁnite measure ν is\n",
      "that for any two densities p1 and p2 in P, the ratio p1(x)/p2(x) is a function\n",
      "only of T(x).\n",
      "Theorem 3.3 (minimal suﬃciency II)\n",
      "Let P be a family of distributions with the common support, and let P0 ⊆P.\n",
      "If T is minimal suﬃcient for P0 and is suﬃcient for P, then T is minimal\n",
      "suﬃcient for P.\n",
      "Proof.\n",
      "Consider any statistic U that is suﬃcient for P. Then U must also be suﬃcient\n",
      "for P0, and since T is minimal suﬃcient for P0, T is a function of U.\n",
      "We can also establish minimal suﬃciency by use of completeness, as we\n",
      "see below.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "225\n",
      "Completeness\n",
      "A suﬃcient statistic T is particularly useful in a complete family or a bound-\n",
      "edly complete family of distributions (see Section 2.1 beginning on page 162).\n",
      "In this case, for every Borel (bounded) function h that does not involve P ,\n",
      "EP (h(T)) = 0 ∀P ∈P ⇒h(t) = 0 a.e. P.\n",
      "Complete families are deﬁned in terms of properties of any Borel function of\n",
      "a random variable that does not involve the particular distribution (that is,\n",
      "of a “statistic”).\n",
      "We are now interested in the completeness of a statistic, rather than the\n",
      "completeness of the family. A statistic can be complete even though the un-\n",
      "derlying family of distributions of the observables is not complete. (This is\n",
      "because of the deﬁnition of a complete family; see Example 2.1 on page 163.)\n",
      "The family of marginal distributions of the statistic itself must be complete.\n",
      "We now give a deﬁnition of completeness for a statistic.\n",
      "Deﬁnition 3.6 (complete statistic)\n",
      "Given a family of distributions {Pθ} a statistic T(X), where T is a nonconstant\n",
      "function, is said to be complete for P ∈{Pθ} iﬀfor any Borel function h that\n",
      "does not involve P\n",
      "E(h(T(X))) = 0 ∀P ∈P ⇒h(T(x)) = 0 a.e. P.\n",
      "The weaker condition, “bounded completeness of a statistic”, is deﬁned in\n",
      "a similar manner, but only for bounded Borel functions h. A complete statistic\n",
      "is boundedly complete.\n",
      "Completeness and suﬃciency are diﬀerent properties; either one can exist\n",
      "without the other. Suﬃciency relates to a statistic and a sample. There is\n",
      "always a suﬃcient statistic: the sample itself. There may or may not be a\n",
      "complete statistic within a given family.\n",
      "We are generally interested in statistics that are complete and suﬃcient.\n",
      "Complete suﬃciency depends on P, the family of distributions wrt which\n",
      "E is deﬁned. If a statistic is complete and suﬃcient with respect to P, and if it\n",
      "is suﬃcient for P∞, where P ⊆P1 and all distributions in P∞have common\n",
      "support, then it is complete and suﬃcient for P1, because in this case, the\n",
      "condition a.s. P implies the condition a.s. P1.\n",
      "We can establish complete suﬃciency by the exponential criterion.\n",
      "Theorem 3.4 (exponential criterion)\n",
      "Let P be a family of distributions dominated by a σ-ﬁnite measure ν. Given\n",
      "a statistic T suppose there exist Borel functions c and q and a nonnegative\n",
      "Borel function h, where h does not depend on P , such that\n",
      "dP/dν(x) = exp((q(P ))TT(x) −c(P )h(x)\n",
      "a.e. ν.\n",
      "(3.28)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "226\n",
      "3 Basic Statistical Theory\n",
      "A suﬃcient condition for the statistic T to be complete and suﬃcient for\n",
      "P ∈P is that q(P ) contain an interior point.\n",
      "Complete suﬃciency is useful for establishing independence using Basu’s\n",
      "theorem (see below), and in estimation problems in which we seek an unbiased\n",
      "estimator that has minimum variance uniformly (UMVUE, discussed more\n",
      "fully in Section 5.1).\n",
      "It is often relatively straightforward to identify complete suﬃcient statis-\n",
      "tics in certain families of distributions, such as those in the exponential class;\n",
      "see Example 3.6. In a parametric-support family, there may be a complete\n",
      "statistic. If so, it is usually an extreme order statistic; see Example 3.7.\n",
      "Theorem 3.5 (minimal suﬃciency III)\n",
      "If T is a complete statistic in P and T is suﬃcient, then T is minimal suﬃ-\n",
      "cient.\n",
      "Proof. Exercise (follows from deﬁnitions).\n",
      "Complete suﬃciency implies minimal suﬃciency, but minimal suﬃciency\n",
      "does not imply completeness, as we see in the following example.\n",
      "Example 3.4 minimal suﬃcient but not complete suﬃcient\n",
      "Consider a sample X of size 1 from U(θ, θ+1). Clearly, X is minimal suﬃcient.\n",
      "Any bounded periodic function h(x) with period 1 that is not a.e. 0 serves to\n",
      "show that X is not complete. Let h(x) = sin(2πx). Then\n",
      "E(h(X)) =\n",
      "Z θ+1\n",
      "θ\n",
      "dx = 0.\n",
      "Clearly, however h(X) is not 0 a.e., so X is not complete. We can see from\n",
      "this that there can be no complete statistic in this case.\n",
      "We will later deﬁne completeness of a class of statistics called decision\n",
      "rules, and in that context, deﬁne minimal completeness of the class.\n",
      "Basu’s Theorem\n",
      "Complete suﬃciency, ancillarity, and independence are related.\n",
      "Theorem 3.6 (Basu’s theorem)\n",
      "Let T(X) and U(X) be statistics from the population Pθ in the family P If\n",
      "T(X) is a boundedly complete suﬃcient statistic for Pθ ∈P, and if U(X) is\n",
      "ancillary for Pθ ∈P, then T and U are independent.\n",
      "Proof.\n",
      "If U is ancillary for Pθ and A is any set, then Pr(U ∈A) is independent of\n",
      "Pθ. Now, consider pA(t) = Pr(U ∈A|T = t). We have\n",
      "EPθ(pA(T)) = Pr(U ∈A);\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "227\n",
      "and so by completeness,\n",
      "pA(T) = Pr(U ∈A) a.e.P.\n",
      "Hence U and T are independent.\n",
      "An interesting example shows the importance of completeness in Basu’s\n",
      "theorem. This example also shows that minimality does not imply complete-\n",
      "ness.\n",
      "Example 3.5 minimal suﬃcient but ancillary is not independent\n",
      "Let X1, . . ., Xn, with n ≥2, be iid as U(θ −1/2, θ + 1/2). It is clear that\n",
      "T = {X(1), X(n)} is suﬃcient; in fact, it is minimal suﬃcient. Now consider\n",
      "U = X(n) −X(1), which we easily see is ancillary. It is clear that T and U are\n",
      "not independent (U is a function of T).\n",
      "If T were complete, then Basu’s theorem would say that T and U are\n",
      "independent, but writing U = h(T), where h is a measurable function, we can\n",
      "see that T is not complete (although it is minimal).\n",
      "Suﬃciency, Minimality, and Completeness in Various Families\n",
      "We can use general properties of speciﬁc families of distributions to establish\n",
      "properties of statistics quickly and easily.\n",
      "Complete suﬃciency is often easy to show in exponential family.\n",
      "Example 3.6 complete suﬃcient statistics in a normal distribution\n",
      "Consider the normal family of distributions with parameter θ = (µ, σ2). Sup-\n",
      "pose we have observations X1, X2, . . ., Xn. Let T1 = (P Xi, PX2\n",
      "i ). Then\n",
      "T1(X) is suﬃcient and complete for θ. (Exercise)\n",
      "Now, let T2 = (X, S2), where X and S2 are respectively the sample mean\n",
      "and sample variance (equations (1.32) and (1.33)). Then T2(X) is also suﬃ-\n",
      "cient and complete for θ. (Exercise)\n",
      "We have seen in Section 2.9.3 that X and S2 are independent and we\n",
      "worked out their distributions there.\n",
      "Complete suﬃciency is also often easy to show in distributions whose range\n",
      "depends on θ in a simple way. (We can relate any such range-dependent dis-\n",
      "tribution to U(θ1, θ2).)\n",
      "In general, proof of suﬃciency is often easy, but proof of minimality or\n",
      "completeness is often diﬃcult. We often must rely on the awkward use of the\n",
      "deﬁnitions of minimality and completeness. Completeness of course implies\n",
      "minimality.\n",
      "Example 3.7 complete suﬃcient statistics in a uniform distribution\n",
      "Consider the uniform family of distributions with parameter θ that is the up-\n",
      "per bound of the support, U(0, θ). Suppose we have observations X1, . . ., Xn.\n",
      "Then T(X) = X(n) is complete suﬃcient for θ. (Exercise)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "228\n",
      "3 Basic Statistical Theory\n",
      "Parametric-support families (or “truncation families”) have simple range\n",
      "dependencies. A distribution in any of these families has a PDF in the general\n",
      "form\n",
      "fθ(x) = c(θ)g(x)IS(θ)(x).\n",
      "The most useful example of distributions whose support depends on the\n",
      "parameter is the uniform U(0, θ), as in Example 3.7. Many other distribu-\n",
      "tions can be transformed into this one. For example, consider X1, . . ., Xn iid\n",
      "as a shifted version of the standard exponential family of distributions with\n",
      "Lebesgue PDF\n",
      "e−(x−α)I]α,∞[(x),\n",
      "and Yi = e−Xi and θ = e−α, then Y1, . . ., Yn are iid U(0, θ); hence if we can\n",
      "handle one problem, we can handle the other. We can also handle distribu-\n",
      "tions like U(θ1, θ2) a general shifted exponential, as well as some other related\n",
      "distributions, such as a shifted gamma.\n",
      "We can show completeness using the fact that\n",
      "Z\n",
      "A\n",
      "|g| dµ = 0 ⇐⇒g = 0 a.e. on A.\n",
      "(3.29)\n",
      "Another result we often need in going to a multiparameter problem is Fubini’s\n",
      "theorem.\n",
      "The suﬃcient statistic in the simple univariate case where S(θ) = (θ1, θ2)\n",
      "is T(X) = (X(1), X(n)), as we can see from the the factorization theorem by\n",
      "writing the joint density of a sample as\n",
      "c(θ)g(x)I]x(1),x(n)[(x).\n",
      "For example, for a distribution such as U(0, θ) we see that X(n) is suﬃcient\n",
      "by writing the joint density of a sample as\n",
      "1\n",
      "θ I]0,x(n)[.\n",
      "Example 3.8 complete suﬃcient statistics in a two-parameter expo-\n",
      "nential distribution\n",
      "In Examples 1.11 and 1.18, we considered a shifted version of the exponential\n",
      "family of distributions, called the two-parameter exponential with parameter\n",
      "(α, θ). The Lebesgue PDF is\n",
      "θ−1e−(x−α)/θI]α,∞[(x)\n",
      "Suppose we have observations X1, X2, . . ., Xn.\n",
      "In Examples 1.11 and 1.18, we worked out the distributions of X(1) and\n",
      "PXi −nX(1). Now, we want to show that T = (X(1), PXi −nX(1)) is\n",
      "suﬃcient and complete for (α, θ).\n",
      "******************\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "229\n",
      "The properties of a speciﬁc family of distributions are useful in identifying\n",
      "optimal methods of statistical inference. Exponential families are particularly\n",
      "useful for ﬁnding UMVU estimators. We will discuss UMVU estimators more\n",
      "fully in Section 5.1. A group family is useful in identifying equivariant and\n",
      "invariant statistical procedures. We will discuss procedures of these types in\n",
      "Section 3.4.\n",
      "3.1.3 Information and the Information Inequality\n",
      "The term “information” is used in various ways in statistical inference. In\n",
      "general, information relates to the variability in the probability distribution\n",
      "or to the variability in a random sample.\n",
      "A common type of information is Shannon information, which for an event\n",
      "is the negative of the log of the probability of the event; see page 41. In this\n",
      "view, an observed event that is less probable than another event provides more\n",
      "information than that more probable event.\n",
      "In parametric families of probability distributions, we also use the term\n",
      "“information” in another sense that relates to the extent of the diﬀerence\n",
      "between two PDFs in the same family, but with diﬀerence values of the pa-\n",
      "rameters. This kind of information, called Fisher information, is measured by\n",
      "taking derivatives with respect to the parameters.\n",
      "A fundamental question is how much information does a realization of the\n",
      "random variable X contain about the scalar parameter θ.\n",
      "If a random variable X has a PDF f(x; θ) wrt a σ-ﬁnite measure that is\n",
      "diﬀerentiable in θ, the rate of change of the PDF at a given x with respect to\n",
      "diﬀerent values of θ intuitively is an indication of the amount of information x\n",
      "provides. If the support of the random variable, however, depends on θ, that\n",
      "derivative may not be so useful. Let us restrict our attention to distributions\n",
      "that satisfy the ﬁrst two Fisher information regularity conditions we deﬁned\n",
      "on on page 168 for a family of distributions P = {Pθ; θ ∈Θ} that have\n",
      "densities fθ:\n",
      "•\n",
      "The parameter space Θ is real and connected and contains an open set (in\n",
      "one dimension, it is an interval with positive measure).\n",
      "•\n",
      "For any x in the support and θ ∈Θ◦, ∂fθ(x)/∂θ exists and is ﬁnite.\n",
      "For such distributions, we deﬁne the “information” (or “Fisher informa-\n",
      "tion”) that X contains about θ as\n",
      "I(θ) = Eθ\n",
      " \u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013 \u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013T!\n",
      ".\n",
      "(3.30)\n",
      "Information is larger when there is larger relative variation in the density\n",
      "as the parameter changes, but the information available from an estimator is\n",
      "less when the estimator exhibits large variation (i.e., has large variance), so\n",
      "we want to use statistics with smaller variance.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "230\n",
      "3 Basic Statistical Theory\n",
      "The third Fisher information regularity condition guarantees that integra-\n",
      "tion and diﬀerentiation can be interchanged.\n",
      "•\n",
      "The support is independent of θ; that is, all Pθ have a common support.\n",
      "In Fisher information regular families, we have\n",
      "E\n",
      "\u0012∂log(f(X, θ))\n",
      "∂θ\n",
      "\u0013\n",
      "=\n",
      "Z\n",
      "1\n",
      "f(x, θ)\n",
      "∂f(x, θ)\n",
      "∂θ\n",
      "f(x, θ)dx\n",
      "= ∂\n",
      "∂θ\n",
      "Z\n",
      "f(x, θ)dx\n",
      "= 0;\n",
      "(3.31)\n",
      "therefore, the expectation in the information deﬁnition (3.30) is also the vari-\n",
      "ance of ∂log(f(X, θ))/∂θ:\n",
      "I(θ) = V\n",
      "\u0012∂log(f(X, θ))\n",
      "∂θ\n",
      "\u0013\n",
      ".\n",
      "(3.32)\n",
      "If the second derivative with respect to θ also exists for all x and θ, and\n",
      "if it can be obtained by diﬀerentiation twice under the integral sign in (3.31),\n",
      "then we also have a relationship with the second derivative:\n",
      "I(θ) = E\n",
      " \u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013 \u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013T!\n",
      "= −E\n",
      "\u0012∂2 log(f(X, θ))\n",
      "∂θ2\n",
      "\u0013\n",
      ".\n",
      "(3.33)\n",
      "We see this by writing\n",
      "∂2 log f(X; θ)\n",
      "∂θ2\n",
      "=\n",
      "∂2f(X;θ)\n",
      "∂θ2\n",
      "f(X; θ) −\n",
      "\u0010\n",
      "∂f(X;θ)\n",
      "∂θ\n",
      "\u0011 \u0010\n",
      "∂f(X;θ)\n",
      "∂θ\n",
      "\u0011T\n",
      "(f(X; θ))2\n",
      "=\n",
      "∂2f(X;θ)\n",
      "∂θ2\n",
      "f(X; θ) −\n",
      "\u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013 \u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013T\n",
      "and taking the expectation of both sides, noting that the ﬁrst term on the right\n",
      "is zero as before (after again interchanging diﬀerentiation and integration).\n",
      "Example 3.9 Fisher information in a normal distribution\n",
      "Consider the N(µ, σ2) distribution with θ = (µ, σ) (which is simpler than for\n",
      "θ = (µ, σ2)):\n",
      "log f(µ,σ)(x) = c −log(σ) −(x −µ)2/(2σ2).\n",
      "We have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "231\n",
      "∂\n",
      "∂µ log f(µ,σ)(x) = x −µ\n",
      "σ2\n",
      "and\n",
      "∂\n",
      "∂σ log f(µ,σ)(x) = −1\n",
      "σ + (x −µ)2\n",
      "σ3\n",
      ",\n",
      "so\n",
      "I(µ, σ) = Eθ\n",
      " \u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013 \u0012∂log f(X; θ)\n",
      "∂θ\n",
      "\u0013T!\n",
      "= E(µ,σ)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(X−µ)2\n",
      "(σ2)2\n",
      "X−µ\n",
      "σ2\n",
      "\u0010\n",
      "−1\n",
      "σ + (x−µ)2\n",
      "σ3\n",
      "\u0011\n",
      "x−µ\n",
      "σ2\n",
      "\u0010\n",
      "−1\n",
      "σ + (X−µ)2\n",
      "σ3\n",
      "\u0011\n",
      "+\n",
      "\u0010\n",
      "−1\n",
      "σ + (X−µ)2\n",
      "σ3\n",
      "\u00112\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=\n",
      "\u0014 1\n",
      "σ2 0\n",
      "0\n",
      "2\n",
      "σ2\n",
      "\u0015\n",
      ".\n",
      "The normal is rather unusual among common multiparameter distributions\n",
      "in that the information matrix is diagonal.\n",
      "Notice that the Fisher information matrix is dependent on the parametriza-\n",
      "tion. The parametrization of the normal distribution in either the canonical\n",
      "exponential form or even θ = (µ, σ2) would result in a diﬀerent Fisher infor-\n",
      "mation matrix (see Example 5.11 on page 400).\n",
      "Example 3.10 Fisher information in a gamma distribution\n",
      "Consider the gamma(α, β) distribution. We have for x > 0\n",
      "log f(α,β)(x) = −log(Γ(α)) −α log(β) + (α −1) log(x) −x/β.\n",
      "This yields the Fisher information matrix\n",
      "I(θ) =\n",
      "\"\n",
      "ψ′(α)\n",
      "1\n",
      "β\n",
      "1\n",
      "β\n",
      "α2\n",
      "β2\n",
      "#\n",
      ",\n",
      "where ψ(α) is the digamma function, d log(Γ(α))/dα, and ψ′(α) is the\n",
      "trigamma function, dψ(α)/dα.\n",
      "In the natural parameters, α−1 and 1/β, obviously the Fisher information\n",
      "would be diﬀerent. (Remember, derivatives are involved, so we cannot just\n",
      "substitute the transformed parameters in the information matrix.)\n",
      "Fisher Information in Families in the Exponential Class\n",
      "Consider the general canonical exponential form for a distribution in the ex-\n",
      "ponential class:\n",
      "fθ(x) = exp\n",
      "\u0000(ηTT(x) −ζ(η)\n",
      "\u0001\n",
      "h(x)\n",
      "(see page 173). If η is the mean-value parameter (see equation (2.10)), then\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "232\n",
      "3 Basic Statistical Theory\n",
      "I(θ) = V −1,\n",
      "where\n",
      "V = V(T(X)).\n",
      "***************** prove this\n",
      "Example 3.11 Fisher information in a beta distribution\n",
      "Consider the beta(α, β) distribution. We have for x ∈]0, 1[\n",
      "log f(α,β)(x) = log(Γ(α+β))−log(Γ(α))−log(Γ(β))+(α−1) log(x)+(β−1) log(1−x).\n",
      "This yields the Fisher information matrix\n",
      "I(θ) =\n",
      "\u0014\n",
      "ψ′(α) −ψ′(α + β)\n",
      "−ψ′(α + β)\n",
      "−ψ′(α + β)\n",
      "ψ′(β) −ψ′(α + β)\n",
      "\u0015\n",
      ".\n",
      "***** Show that this follows from above.\n",
      "Fisher Information in Location-Scale Families\n",
      "The Fisher information for the two parameters θ = (µ, σ) in a location-scale\n",
      "family with Lebesgue PDF\n",
      "1\n",
      "σ f\n",
      "\u0012x −µ\n",
      "σ\n",
      "\u0013\n",
      "has a particularly simple form:\n",
      "I(θ) = n\n",
      "σ2\n",
      "\n",
      "\n",
      "Z \u0012f′(x)\n",
      "f(x)\n",
      "\u00132\n",
      "f(x)dx\n",
      "Z\n",
      "x\n",
      "\u0012f′(x)\n",
      "f(x)\n",
      "\u00132\n",
      "f(x)dx\n",
      "Z\n",
      "x\n",
      "\u0012f′(x)\n",
      "f(x)\n",
      "\u00132\n",
      "f(x)dx\n",
      "Z \u0012xf′(x)\n",
      "f(x) + 1\n",
      "\u00132\n",
      "f(x)dx\n",
      "\n",
      "\n",
      ".\n",
      "(3.34)\n",
      "The prime on f′(x) indicates diﬀerentiation with respect to x of course. (The\n",
      "information matrix is deﬁned in terms of diﬀerentiation with respect to the\n",
      "parameters followed by an expectation.)\n",
      "Another expression for the information matrix for a location-scale family\n",
      "is\n",
      "I(θ) = n\n",
      "σ2\n",
      "\n",
      "\n",
      "Z (f′(x))2\n",
      "f(x)\n",
      "dx\n",
      "Z f′(x) (xf′(x) + f(x))\n",
      "f(x)\n",
      "dx\n",
      "Z f′(x) (xf′(x) + f(x))\n",
      "f(x)\n",
      "dx\n",
      "Z (xf′(x) + f(x))2\n",
      "f(x)\n",
      "dx\n",
      "\n",
      "\n",
      ".\n",
      "(3.35)\n",
      "(This is given in a slightly diﬀerent form in Example 3.9 of MS2, which is\n",
      "Exercise 3.34, which is solved in his Solutions, using the form above, which is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "233\n",
      "a more straightforward expression from the derivation that begins by deﬁning\n",
      "the function g(µ, σ, x) = log(f((x −µ)/σ)/σ), and then proceeding with the\n",
      "deﬁnition of the information matrix.)\n",
      "Also we can see that in the location-scale family, if it is symmetric about\n",
      "the origin (that is about µ), the covariance term is 0.\n",
      "The Information Inequality\n",
      "For distributions satisfying the Fisher information regularity conditions we\n",
      "have the information inequality that relates the variance of a statistic to the\n",
      "Fisher information about the parameters. In the following we will consider a\n",
      "scalar statistic, T(X), and a scalar function of θ, g(θ).\n",
      "Following TPE2, we ﬁrst give two lemmas for an unbiased estimator of\n",
      "g(θ).\n",
      "Lemma 3.7.1\n",
      "Given the Borel function g(θ) and statistics T(X) and S(X) such that\n",
      "E(T(X)) = g(θ). A necessary and suﬃcient condition for Cov(T(X), S(X))\n",
      "to depend on θ only through g(θ) is that for all θ\n",
      "Covθ(U, S(X)) = 0\n",
      "∀U ∋Eθ(U) = 0, Eθ(U 2) < ∞.\n",
      "(3.36)\n",
      "Proof.\n",
      "We only need to show that for any T1(X) and T2(X) with E(T1(X)) =\n",
      "E(T2(X)) = g(θ), Covθ(T1(X), S(X)) = Covθ(T2(X), S(X)). We have\n",
      "Covθ(T1(X), S(X)) −Covθ(T2(X), S(X)) = Covθ(T1(X) −T2(X), S(X))\n",
      "= Covθ(U, S(X)).\n",
      "We have therefore Covθ(T1(X), S(X)) = Covθ(T2(X), S(X)) for any T1(X)\n",
      "and T2(X) if and only if Covθ(U, S(X)) = 0 for all U as in equation (3.36).\n",
      "A comment about notation may be in order here. First, we have been\n",
      "writing T(X) and S(X) to emphasize the common random variable in the\n",
      "statistics. A simpler notation may be T and S. A more complicated notation\n",
      "would be T(X, θ) and S(X, θ) to emphasize the dependence of the distribution\n",
      "of T and S on θ, just as we have written f(X, θ) for the PDF above. In the\n",
      "next lemma, we will consider a vector of functions S(X, θ) = (Si(X, θ)). As\n",
      "usual, this is a column vector, and so is (Cov(T(X), Si(X)), for example.\n",
      "Lemma 3.7.2\n",
      "Let T(X) be an unbiased estimator of g(θ) and let S(X, θ) = (Si(X, θ)), where\n",
      "the Si(X, θ) are any stochastically independent functions with ﬁnite second\n",
      "moments. Then\n",
      "V(T(X)) ≥(Cov(T(X), Si(X)))T (V(S))−1 Cov(T(X), Si(X)).\n",
      "(3.37)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "234\n",
      "3 Basic Statistical Theory\n",
      "Proof.\n",
      "Let a1, . . ., ak be any constants. From the covariance inequality for scalar\n",
      "random variables Y and Z,\n",
      "V(Y ) ≥(Cov(Y, Z))2\n",
      "V(Z)\n",
      ",\n",
      "we have\n",
      "V(T(X)) ≥(Cov(T(X), P\n",
      "i aiSi(X)))2\n",
      "V(P\n",
      "i aiSi(X))\n",
      ".\n",
      "(3.38)\n",
      "Rewriting, we have,\n",
      "Cov(T(X),\n",
      "X\n",
      "i\n",
      "aiSi(X)) = aTCov(T(X), Si(X))\n",
      "and\n",
      "V\n",
      " X\n",
      "i\n",
      "aiSi(X)\n",
      "!\n",
      "= aTV(S)a.\n",
      "Because (3.38) is true for any a, this yields\n",
      "V(T(X)) ≥max\n",
      "a\n",
      "\u0000aTCov(T(X), Si(X))\n",
      "\u00012\n",
      "aTV(S)a\n",
      ".\n",
      "Noting that\n",
      "\u0000aTCov(T(X), Si(X))\n",
      "\u00012 = aTCov(T(X), Si(X))(Cov(T(X), Si(X))Ta,\n",
      "from equation (0.3.17) on page 788, we have\n",
      "V(T(X)) ≥(Cov(T(X), Si(X)))T (V(S))−1 Cov(T(X), Si(X)).\n",
      "Theorem 3.7 (information inequality)\n",
      "Assume that the Fisher information regularity conditions hold for the distribu-\n",
      "tion with PDF f(x; θ) and that I(θ) is positive deﬁnite, where θ is a k-vector.\n",
      "Let T(X) be any scalar statistic with ﬁnite second moment, and assume for\n",
      "i = 1, . . ., k, that (∂/∂θi)Eθ(T(X)) exists and can be obtained by diﬀerentiat-\n",
      "ing under the integral sign. Then Eθ((∂/∂θi) log(f(x; θ))) = 0 and\n",
      "V(T(X)) ≥\n",
      "\u0012 ∂\n",
      "∂θ E(T(θ))\n",
      "\u0013T\n",
      "(I(θ))−1 ∂\n",
      "∂θ E(T(θ)).\n",
      "(3.39)\n",
      "Proof. Take the functions Si in Lemma 3.7.2 to be (∂/∂θi) log(f(x; θ)).\n",
      "An alternate direct proof of Theorem 3.7 can be constructed using equa-\n",
      "tions (3.31) through (3.33) and the covariance inequality.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "235\n",
      "The right side of inequality (3.39) is called the information or the Cram´er-\n",
      "Rao lower bound (CRLB). The CRLB results from the covariance inequality.\n",
      "The proof of the CRLB is an “easy piece” that every student should be able\n",
      "to provide quickly. (Notice that for the one-parameter case, this is Corol-\n",
      "lary B.5.1.7 of H¨older’s inequality on page 852.)\n",
      "This inequality plays a very important role in unbiased point estimation,\n",
      "as we will see in Section 5.1.5 on page 399.\n",
      "3.1.4 “Approximate” Inference\n",
      "When the exact distribution of a statistic is known (based, of course, on an\n",
      "assumption of a given underlying distribution of a random sample), use of\n",
      "the statistic for inferences about the underlying distribution is called exact\n",
      "inference.\n",
      "Often the exact distribution of a statistic is not known, or is too compli-\n",
      "cated for practical use. In that case, we may resort to approximate inference.\n",
      "There are basically three types of approximate inference.\n",
      "One type occurs when a simple distribution is very similar to another\n",
      "distribution. For example, the Kumaraswamy distribution with PDF\n",
      "p(x) = αβxα−1(1 −xα)β−1I[0,1](x)\n",
      "(3.40)\n",
      "may be used as an approximation to the beta distribution.\n",
      "Another type of approximate inference, called computational inference, is\n",
      "used when an unknown distribution can be simulated by resampling of the\n",
      "given observations.\n",
      "Asymptotic inference is probably the most commonly used type of approx-\n",
      "imate inference. In asymptotic approximate inference we are interested in the\n",
      "properties of Tn as the sample size increases. We focus our attention on the\n",
      "sequence {Tn} for n = 1, 2, . . ., and, in particular, consider the properties of\n",
      "{Tn} as n →∞. We discuss asymptotic inference in more detail in Section 3.8.\n",
      "3.1.5 Statistical Inference in Parametric Families\n",
      "A real-valued observable random variable X has a distribution that may de-\n",
      "pend in some way on a real-valued parameter θ that takes a value in the set\n",
      "Θ, called the parameter space. This random variable is used to model some\n",
      "observable phenomenon.\n",
      "As the parameter ranges over Θ it determines a family of distributions, P.\n",
      "We denote a speciﬁc member of that family as Pθ for some ﬁxed value of θ.\n",
      "We often want to make inferences about the value of θ or about some\n",
      "function or transformation of an underlying parameter θ. To generalize our\n",
      "object of interest, we often denote it as ϑ, or g(θ) or g(θ; z), where g is some\n",
      "Borel function, and z may represent auxiliary data.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "236\n",
      "3 Basic Statistical Theory\n",
      "Summary of Suﬃcient Statistics and Their Distributions for Some\n",
      "Common Parametric Families\n",
      "3.1.6 Prediction\n",
      "In addition to the three diﬀerent types of inference discussed in the preceding\n",
      "sections, which were related to the problem of determining the speciﬁc Pθ ∈P,\n",
      "we may also want to predict the value that a random variable will realize.\n",
      "In the prediction problem, we have a random variable Y with the prob-\n",
      "ability triple (Ω, F, P ) and a measurable function X that maps (Ω, F, P ) to\n",
      "(Λ, G). Given an observed value of X we wish to predict Y ; that is, to ﬁnd a\n",
      "Borel function g such that E(∥g(X)∥2\n",
      "2) < ∞and E(g(X)) is “close to” E(Y ).\n",
      "A useful measure of closeness in the prediction problem is the mean squared\n",
      "prediction error or MSPE:\n",
      "MSPE(g) = E(∥Y −g(X)∥2\n",
      "2).\n",
      "(3.41)\n",
      "Conditional expectation plays a major role in prediction. If E(Y 2) < ∞,\n",
      "it may be of interest to determine the best predictor in the sense of minimiz-\n",
      "ing the mean squared prediction error. Letting T be the class of all functions\n",
      "g(X) such that E((g(X))2) < ∞and assuming E(Y 2) < ∞, we expand the\n",
      "mean-squared prediction error in a manner similar to the operations in in-\n",
      "equality (B.1) on page 845. For g(X) ∈T , we have\n",
      "E(∥Y −g(X)∥2\n",
      "2) = E(∥Y −E(Y |X) + E(Y |X) −g(X)∥2\n",
      "2)\n",
      "= E(∥Y −E(Y |X)∥2\n",
      "2) + E(∥E(Y |X) −g(X)∥2\n",
      "2) +\n",
      "2E((Y −E(Y |X))T)(E(Y |X) −g(X)))\n",
      "= E(∥Y −E(Y |X)∥2\n",
      "2) + E(∥E(Y |X) −g(X)∥2\n",
      "2) +\n",
      "2E \u0000E((Y −E(Y |X))T)(E(Y |X) −g(X)))|X\u0001\n",
      "= E(∥Y −E(Y |X)∥2\n",
      "2) + E(∥E(Y |X) −g(X)∥2\n",
      "2)\n",
      "≥E(∥Y −E(Y |X)∥2\n",
      "2).\n",
      "(3.42)\n",
      "(This proves Theorem 1.13 on page 27.)\n",
      "3.1.7 Other Issues in Statistical Inference\n",
      "In addition to properties of statistical methods that derive from tight assump-\n",
      "tions about the underlying probability distribution, there are two additional\n",
      "considerations. One issue concerns the role of the assumed probability dis-\n",
      "tribution, that is, how critical is that assumption to the performance of the\n",
      "statistical procedure. The other issue has to do with how the data are col-\n",
      "lected.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.1 Inferential Information in Statistics\n",
      "237\n",
      "Robustness\n",
      "We seek statistical methods that have optimal properties. We study these\n",
      "methods in the context of a family of probability distributions P. An optimal\n",
      "method over one family of probability distributions may be far from optimal\n",
      "in other families. Hence, although focusing on one family we may also consider\n",
      "the performance over a larger class of probability families, {P, Q, R, . . .}. This\n",
      "is called “robustness”.\n",
      "Data and Sampling: Probability Distributions and\n",
      "Data-Generating Processes\n",
      "For many statistical methods we begin by assuming that we have observations\n",
      "X1, . . ., Xn on a random variable X from some distribution Pθ, which is a\n",
      "member of some family of probability distributions P. We usually assume\n",
      "either that the observations (or the “data”), constitute a random sample and\n",
      "that they are independent or they are at least exchangeable.\n",
      "The data-generating process that yields the sample depends on the proba-\n",
      "bility distribution Pθ, but it may not be fully characterized by that underlying\n",
      "distribution. An issue is whether or not all aspects of the data-generating pro-\n",
      "cess should aﬀect the inference about Pθ, or whether the inference should be\n",
      "based solely on the data and the assumed probability distribution Pθ.\n",
      "In the case of sampling within a ﬁnite population, the observations are\n",
      "often taken according to some eﬃcient scheme, such as stratiﬁcation or clus-\n",
      "tering, in which the observations taken as a whole do not constitute a random\n",
      "sample of realizations of iid random variables. The method of analysis must\n",
      "take the nature of the data into consideration.\n",
      "The problem of collecting data for making inferences concerning a Bernoulli\n",
      "parameter π provides a simple example of diﬀerent data-generating processes.\n",
      "Example 3.12 Sampling in a Bernoulli distribution\n",
      "The family of Bernoulli distributions is that formed from the class of the\n",
      "probability measures Pπ({1}) = π and Pπ({0}) = 1 −π on the measurable\n",
      "space (Ω= {0, 1}, F = 2Ω). A simple problem is statistical inference about π.\n",
      "One approach is to take a random sample of size n, X1, . . ., Xn from the\n",
      "Bernoulli(π), and then use some function of that sample as an estimator. An\n",
      "obvious statistic to use is the number of 1’s in the sample, that is, T = P Xi.\n",
      "This is a suﬃcient statistic. The distribution of T is very simple; it is binomial\n",
      "with parameters n and π, and its PDF is\n",
      "pT (t ; n, π) =\n",
      "\u0012n\n",
      "t\n",
      "\u0013\n",
      "πt(1 −π)n−t,\n",
      "t = 0, 1, . . ., n.\n",
      "(3.43)\n",
      "We could use this distribution, which depends only on π and the pre-chosen n,\n",
      "to form unbiased estimators, set conﬁdence sets, or perform tests of hypotheses\n",
      "regarding π.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "238\n",
      "3 Basic Statistical Theory\n",
      "A very diﬀerent approach is to take a sequential sample, X1, X2, . . ., until\n",
      "a ﬁxed number t of 1’s have occurred. This is a diﬀerent data-generating\n",
      "process. In this case, the size of the sample N is random, and its distribution\n",
      "is the negative binomial with parameters t and π, and its PDF is\n",
      "pN(n ; t, π) =\n",
      "\u0012n −1\n",
      "t −1\n",
      "\u0013\n",
      "πt(1 −π)n−t,\n",
      "n = t, t + 1, . . ..\n",
      "(3.44)\n",
      "(The negative binomial distribution is often deﬁned slightly diﬀerently so that\n",
      "it is the distribution of the random variable N −t above; either deﬁnition of\n",
      "course completely determines the other.) In this process, the sample size N is\n",
      "a suﬃcient statistic. We could use the distribution of N, which depends only\n",
      "on π and the pre-chosen t, to form unbiased estimators, set conﬁdence sets,\n",
      "or perform tests of hypotheses regarding π.\n",
      "In the description of the two experiments we have n (pre-chosen), N, T,\n",
      "and t (pre-chosen). If the realized value of N is n should the conclusions in the\n",
      "second experiment be the same as those in the ﬁrst experiment if the realized\n",
      "value of T is t?\n",
      "While one or the other approach may be better from either a practical\n",
      "or a theoretical standpoint, we may adopt the principle that similar results\n",
      "from the two experiments should lead to similar conclusions (where “similar\n",
      "results” is deﬁned in the likelihood principle; see page 245). In Examples 4.3,\n",
      "4.6 and 4.7, we see that a Bayesian analysis would lead to similar conclusions.\n",
      "In Example 6.1 on page 447 and in Example 6.4, we see that the maximum\n",
      "likelihood estimators of π are the same.\n",
      "Further comments on Example 3.12\n",
      "We also consider this example in Example 4.1 on page 333, but there consider\n",
      "only the case in which the Xi are independent (or at least exchangeable). In\n",
      "that case, of course, the appropriate model is the binomial, and we can ignore\n",
      "the overall data-generating process. On the other hand, however, because one\n",
      "experiment was based on a stopping rule that was conditional on the data,\n",
      "perhaps diﬀerent conclusions should be drawn. The sample is quite diﬀerent;\n",
      "in the latter case, the Xi are not exchangeable, only the ﬁrst n−1 are. Because\n",
      "the distributions are diﬀerent, we may expect to reach diﬀerent conclusions for\n",
      "many inferences that depend on expected values of the random observables.\n",
      "The simplest concern about expected values is just the bias, and indeed, the\n",
      "unbiased estimators of π are diﬀerent (see Example 5.1 on page 390). We may\n",
      "also expect to reach diﬀerent conclusions for many inferences that depend on\n",
      "quantiles of the random observables, and, indeed, hypothesis tests concerning\n",
      "π may be diﬀerent (see Example 7.12 on page 539). You are asked to explore\n",
      "these issues further in Exercise 7.5.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "239\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "If we assume that we have a random sample of observations X1, . . ., Xn on\n",
      "a random variable X from some distribution Pθ, which is a member of some\n",
      "family of probability distributions P, our objective in statistical inference is\n",
      "to determine a speciﬁc Pθ ∈P, or some subfamily Pθ ⊆P, that could likely\n",
      "have generated the sample. In the previous section we have discussed various\n",
      "ways of assessing the information content of the sample, but we have yet to\n",
      "address the question of how to use the data in statistical inference.\n",
      "How should we approach this problem?\n",
      "Five Approaches to Statistical Inference\n",
      "Five approaches to statistical inference are\n",
      "•\n",
      "use of a likelihood function\n",
      "for example, maximum likelihood\n",
      "an estimator is an MLE\n",
      "•\n",
      "use of the empirical cumulative distribution function (ECDF)\n",
      "for example, method of moments\n",
      "an estimator is an MME\n",
      "•\n",
      "ﬁtting expected values\n",
      "for example, least squares\n",
      "an estimator may be an LSE or a BLUE\n",
      "•\n",
      "ﬁtting a probability distribution\n",
      "for example, maximum entropy\n",
      "•\n",
      "deﬁnition and use of a loss function; a “decision-theoretic” approach\n",
      "for example, uniform minimum variance unbiased estimation, and most\n",
      "Bayesian methods.\n",
      "an estimator may be a UMVUE, a UMRE (rarely!), or a UMREE (or\n",
      "UMRIE)\n",
      "These approaches are associated with various philosophical/scientiﬁc prin-\n",
      "ciples, sometimes explicitly stated and sometimes not. The suﬃciency princi-\n",
      "ple (see page 223) guides most aspects of statistical inference, and is generally\n",
      "consistent with the more speciﬁc principles associated with various approaches\n",
      "to inference. Some of these principles, such as the substitution principle (see\n",
      "page 247) and the likelihood principle (see page 245), inform a major class of\n",
      "statistical methods, while other principles, such as the bootstrap principle (see\n",
      "page 249), are more local in application. Although some statisticians feel that\n",
      "an axiomatic approach to statistical inference should be based on universal\n",
      "principles, a substantial proportion of statisticians feel that abstract principles\n",
      "may be too general to guide inference in a speciﬁc case. Most general statis-\n",
      "tical principles focus on the observed data rather than on the data-generating\n",
      "process. Statisticians who emphasize general principles often characterize con-\n",
      "sideration of the data-generating process as “adhockery”.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "240\n",
      "3 Basic Statistical Theory\n",
      "Certain elements that may be central to a particular one of these ap-\n",
      "proaches may be found in other approaches; for example the concept of like-\n",
      "lihood can be found in most approaches. The principles of data reduction\n",
      "and the inferential information in a sample that we discussed in the previous\n",
      "section obviously must be recognized in any approach to statistical inference.\n",
      "Finally, there are certain methods that are common to more than one of these\n",
      "approaches. Abstracting and studying the speciﬁc method itself may illumi-\n",
      "nate salient properties of the overall approach. An example of a method that\n",
      "can be studied in a uniﬁed manner is the use of estimating equations, which\n",
      "we discuss in Section 3.2.5.\n",
      "In the following four subsections, 3.2.1 through 3.2.4, we will brieﬂy discuss\n",
      "the ﬁrst four of the approaches list above. The “decision theory” approach\n",
      "to statistical inference is based on a loss function, and we will discuss this\n",
      "important approach in Section 3.3.\n",
      "Some Methods in Statistical Inference\n",
      "Within the broad framework of a particular approach to statistical inference,\n",
      "there are various speciﬁc methods that may be applied. I do not attempt a\n",
      "comprehensive listing of these methods, but in order to emphasize the hierar-\n",
      "chy of general approaches and speciﬁc methods, I will list a few.\n",
      "•\n",
      "transformations\n",
      "•\n",
      "transforms (functional transformations)\n",
      "•\n",
      "asymptotic inference\n",
      "this includes a wide range of methods such as\n",
      "–\n",
      "the delta method (ﬁrst order and second order)\n",
      "–\n",
      "various Taylor series expansions (of which the delta method is an ex-\n",
      "ample)\n",
      "–\n",
      "orthogonal series representations\n",
      "•\n",
      "computational inference\n",
      "(this includes a wide range of methods, including MCMC)\n",
      "•\n",
      "decomposition of variance into variance components\n",
      "•\n",
      "Rao-Blackwellization\n",
      "•\n",
      "scoring\n",
      "•\n",
      "EM methods\n",
      "•\n",
      "bootstrap\n",
      "•\n",
      "jackknife\n",
      "•\n",
      "empirical likelihood\n",
      "•\n",
      "tilting\n",
      "•\n",
      "use of saddlepoint approximations\n",
      "•\n",
      "PDF decomposition\n",
      "It is worthwhile to be familiar with a catalog of common operations in\n",
      "mathematical statistics. A list such as that above can be useful when working\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "241\n",
      "in statistical theory (or applications, of course). In Section 0.0.9 beginning on\n",
      "page 676 we list several general methods to think of when doing mathematics.\n",
      "We will illustrate these methods in various examples throughout this book.\n",
      "3.2.1 Likelihood\n",
      "Given a sample X1, . . ., Xn from distributions with probability densities pi(x),\n",
      "where all PDFs are deﬁned with respect to a common σ-ﬁnite measure, the\n",
      "likelihood function is\n",
      "Ln(pi ; X) = c\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "pi(Xi),\n",
      "(3.45)\n",
      "where c ∈IR+ is any constant independent of the pi. A likelihood function,\n",
      "therefore, may be considered to be an equivalence class of functions. It is\n",
      "common to speak of Ln(pi ; X) with c = 1 as “the” likelihood function.\n",
      "We can view the sample either as a set of random variables or as a set\n",
      "of constants, the realized values of the random variables, in which case we\n",
      "usually use lower-case letters.\n",
      "The likelihood function arises from a probability density, but it is not a\n",
      "probability density function. It does not in any way relate to a “probability”\n",
      "associated with the parameters or the model.\n",
      "Although non-statisticians will often refer to the “likelihood of an obser-\n",
      "vation”, in statistics, we use the term “likelihood” to refer to a model or a\n",
      "distribution given observations.\n",
      "The log-likelihood function is the log of the likelihood:\n",
      "lLn(pi ; x) = log Ln(pi ; xi),\n",
      "(3.46)\n",
      "It is a sum rather than a product.\n",
      "The n subscript serves to remind us of the sample size, and this is often\n",
      "very important in use of the likelihood or log-likelihood function particularly\n",
      "because of their asymptotic properties. We often drop the n subscript, how-\n",
      "ever.\n",
      "In many cases of interest, the sample is from a single parametric family. If\n",
      "the PDF is p(x ; θ) then the likelihood and log-likelihood functions are written\n",
      "as\n",
      "L(θ ; x) =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "p(xi ; θ),\n",
      "(3.47)\n",
      "and\n",
      "l(θ ; x) = log L(θ ; x).\n",
      "(3.48)\n",
      "The Parameter Is the Variable\n",
      "Note that the likelihood is a function of θ for a given x, while the PDF is\n",
      "a function of x for a given θ. We sometimes write the expression for the\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "242\n",
      "3 Basic Statistical Theory\n",
      "likelihood without the observations: L(θ). I like to think of the likelihood as a\n",
      "function of some dummy variable t that ranges over the parameter space Θ,\n",
      "and I write L(t ; x) or l(t ; x). While if we think of θ as a ﬁxed, but unknown,\n",
      "value, it does not make sense to think of a function of that particular value,\n",
      "and if we have an expression in terms of that value, it does not make sense to\n",
      "perform operations such as diﬀerentiation with respect to that quantity.\n",
      "For certain properties of statistics derived from a likelihood approach, it\n",
      "is necessary to consider the parameter space Θ to be closed (see, for example,\n",
      "Wald (1949)). Except for cases when those properties are important, we will\n",
      "not assume Θ to be closed, but may, however, consider the closure Θ.\n",
      "In a multiparameter case, we may be interested in only some of the pa-\n",
      "rameters. There are two ways of approaching this, use of a proﬁle likelihood\n",
      "or of a conditional likelihood.\n",
      "If θ = (θ1, θ2) and if θ2 is ﬁxed, the likelihood L(θ1 ; θ2, x) is called a proﬁle\n",
      "likelihood or concentrated likelihood of θ1 for given θ2 and x.\n",
      "If the PDFs can be factored so that one factor includes θ2 and some func-\n",
      "tion of the sample, S(x), and the other factor, given S(x), is free of θ2, then\n",
      "this factorization can be carried into the likelihood. Such a likelihood is called\n",
      "a conditional likelihood of θ1 given S(x).\n",
      "Maximum Likelihood Estimation\n",
      "The maximum likelihood estimate (MLE) of θ is deﬁned as\n",
      "ˆθ = arg max\n",
      "θ∈Θ\n",
      "L(θ ; x),\n",
      "(3.49)\n",
      "if it exists (that is, if supθ∈Θ L(θ ; x) ∈IR).\n",
      "Because the logarithm is a strictly increasing function, the MLE is also\n",
      "the argmax of the log-likelihood. Also, of course, the maximum of L(θ) occurs\n",
      "at the same value of the argument as the maximum of cL(θ).\n",
      "The MLE in general is not unbiased for its estimand. A simple example is\n",
      "the MLE of the variance σ2 in a normal distribution with unknown mean.\n",
      "Example 3.13 MLE of the mean and the variance in a normal dis-\n",
      "tribution\n",
      "Consider the normal family of distributions with parameters µ and σ2. Sup-\n",
      "pose we have observations x1, x2, . . ., xn. The log-likelihood is\n",
      "lL(µ, σ2 ; x) = −1\n",
      "2 log(2πσ2) −\n",
      "Pn\n",
      "i=1(xi −µ)2\n",
      "2σ2\n",
      ".\n",
      "(3.50)\n",
      "We seek values that could be substituted for µ and σ2 so as to maximize this\n",
      "quantity. If µ and σ2 are treated as variables, we see that the function in\n",
      "equation (3.50) is diﬀerentiable with respect to them. We have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "243\n",
      "∂\n",
      "∂µlL(µ, σ2 ; x) =\n",
      "Pn\n",
      "i=1(xi −µ)\n",
      "σ2\n",
      "(3.51)\n",
      "∂\n",
      "∂σ2 lL(µ, σ2 ; x) =\n",
      "Pn\n",
      "i=1(xi −µ)2\n",
      "2(σ2)2\n",
      "−\n",
      "n\n",
      "2σ2\n",
      "(3.52)\n",
      "We set the derivatives equal to zero to obtain the “likelihood equations”, and\n",
      "solving that system, we obtain the stationary points\n",
      "ˆµ = ¯x\n",
      "(3.53)\n",
      "and if ∃i, j ∈{1, . . ., n} ∋xi ̸= xj,\n",
      "ˆσ2 =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(xi −¯x)2/n.\n",
      "(3.54)\n",
      "Next, we compute the Hessian of lL(µ, σ2 ; x), and observe that it is negative\n",
      "deﬁnite at the stationary point; hence ˆµ and ˆσ2 maximize the log-likelihood\n",
      "(exercise).\n",
      "We know that S2 = Pn\n",
      "i=1(Xi −¯X)2/(n −1) where X1, X2, . . ., Xn is a\n",
      "random sample from a normal distribution with variance σ2 is unbiased for\n",
      "σ2; hence, the MLE is biased.\n",
      "Note that if we had only one observation or if all observations had the same\n",
      "value, the log-likelihood would be unbounded when µ = x1 and σ2 approaches\n",
      "zero.\n",
      "The MLE for σ2 in a normal distribution with unknown mean is the same\n",
      "as the plug-in estimator or MME (3.64). Note that the plug-in estimator (or\n",
      "MME) is not based on an assumed underlying distribution, but the MLE is.\n",
      "The MLE may have smaller MSE than an unbiased estimator. This is the\n",
      "case in the example above. The estimator S2 in equation (3.65) is unbiased,\n",
      "and the MLE is (n −1)S2/n. Consider any estimator of the form c(n −1)S2.\n",
      "Note that (n −1)S2/σ2 has a χ2\n",
      "n−1 distribution. In the case of N(µ, σ2) we\n",
      "have the MSE\n",
      "E((c(n −1)S2 −σ2)2) = σ4((n2 −1)c2 −2(n −1)c + 1).\n",
      "(3.55)\n",
      "From this we see that the MLE of σ2, that is, where c = 1/n, has uniformly\n",
      "smaller MSE than the unbiased estimator S2.\n",
      "Likelihood Equation\n",
      "In statistical estimation, the point at which the likelihood attains its maximum\n",
      "(which is, of course, the same point at which the log-likelihood attains its\n",
      "maximum) is of interest. We will consider this approach to estimation more\n",
      "thoroughly in Chapter 6.\n",
      "If the likelihood is diﬀerentiable with respect to the parameter, we may\n",
      "be able to obtain the maximum by setting the derivative equal to zero and\n",
      "solving the resulting equation:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "244\n",
      "3 Basic Statistical Theory\n",
      "∂l(θ ; x)\n",
      "∂θ\n",
      "= 0.\n",
      "(3.56)\n",
      "This is called the likelihood equation. The derivative of the likelihood equated\n",
      "to zero, ∂L(θ ; x)/∂θ = 0, is also called the likelihood equation.\n",
      "The roots of the likelihood equation are often of interest, even if these\n",
      "roots do not provide the maximum of the likelihood function.\n",
      "Equation (3.56) is an estimating equation; that is, its solution, if it exists,\n",
      "is an estimator. Note that this estimator is not necessarily MLE; it is a root\n",
      "of the likelihood equation, or RLE. We will see in Chapter 6 that RLEs have\n",
      "desirable asymptotic properties.\n",
      "It is often useful to deﬁne an estimator as the solution of some estimating\n",
      "equation. We will see other examples of estimating equations in subsequent\n",
      "sections.\n",
      "Score Function\n",
      "The derivative of the log-likelihood on the left side of equation (3.56) plays\n",
      "an important role in statistical inference. It is called the score function, and\n",
      "often denoted as sn(θ ; x):\n",
      "sn(θ ; x) = ∂l(θ ; x)\n",
      "∂θ\n",
      ".\n",
      "(3.57)\n",
      "(I should point out that I use the notation “∇l(θ ; x)” and the slightly more\n",
      "precise “∂l(θ ; x)/∂θ” more-or-less synonymously.)\n",
      "Finding an RLE is called scoring.\n",
      "In statistical inference, knowing how the likelihood or log-likelihood would\n",
      "vary if θ were to change is important. For a likelihood function (and hence,\n",
      "a log-likelihood function) that is diﬀerentiable with respect to the parameter,\n",
      "the score function represents this change.\n",
      "Likelihood Ratio\n",
      "When we consider two diﬀerent distributions for a sample x, we have two dif-\n",
      "ferent likelihoods, say L0 and L1. (Note the potential problems in interpreting\n",
      "the subscripts; here the subscripts refer to the two diﬀerent distributions. For\n",
      "example L0 may refer to L(θ0 | x) in a notation consistent with that used\n",
      "above.) In this case, it may be of interest to compare the two likelihoods in\n",
      "order to make an inference about the two possible distributions. A simple\n",
      "comparison, of course, is the ratio. The ratio\n",
      "L(θ0 ; x)\n",
      "L(θ1 ; x),\n",
      "(3.58)\n",
      "or L0/L1 in the simpler notation, is called the likelihood ratio with respect to\n",
      "the two possible distributions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "245\n",
      "Although in most contexts we consider the likelihood to be a function of\n",
      "the parameter for given, ﬁxed values of the observations, it may also be useful\n",
      "to consider the likelihood ratio to be a function of x. On page 167, we deﬁned\n",
      "a family of distributions based on their having a “monotone” likelihood ratio.\n",
      "Monotonicity in this case is with respect to a function of x. In a family with\n",
      "a monotone likelihood ratio, for some scalar-valued function y(x) and for any\n",
      "θ1 < θ0, the likelihood ratio is a nondecreasing function of y(x) for all values\n",
      "of x for which fθ1(x) is positive.\n",
      "The most important use of the likelihood ratio is as the basis for a statis-\n",
      "tical test.\n",
      "Under certain conditions that we will detail later, with L0 and L1, with\n",
      "corresponding log-likelihoods l0 and l1, based on a random variable (that is,\n",
      "Li = L(pi ; X), instead of being based on a ﬁxed x), the random variable\n",
      "λ = −2 log\n",
      "\u0012L0\n",
      "L1\n",
      "\u0013\n",
      "(3.59)\n",
      "= −2(l0 −l1)\n",
      "has an approximate chi-squared distribution with degrees of freedom whose\n",
      "number depends on the numbers of parameters. (We will discuss this more\n",
      "fully in Chapter 7.)\n",
      "This quantity in a diﬀerent setting is also called the deviance. We encounter\n",
      "the deviance in the analysis of generalized linear models, as well as in other\n",
      "contexts.\n",
      "The likelihood ratio, or the log of the likelihood ratio, plays an important\n",
      "role in statistical inference. Given the data x, the log of the likelihood ratio is\n",
      "called the support of the hypothesis that the data came from the population\n",
      "that would yield the likelihood L0 versus the hypothesis that the data came\n",
      "from the population that would yield the likelihood L1. The support clearly\n",
      "is relative and ranges over IR. The support is also called the experimental\n",
      "support.\n",
      "Likelihood Principle\n",
      "The likelihood principle in statistical inference asserts that all of the informa-\n",
      "tion that the data provide concerning the relative merits of two hypotheses\n",
      "(two possible distributions that give rise to the data) is contained in the likeli-\n",
      "hood ratio of those hypotheses and the data. An alternative statement of the\n",
      "likelihood principle is that if for x and y,\n",
      "L(θ ; x)\n",
      "L(θ ; y) = c(x, y)\n",
      "∀θ,\n",
      "(3.60)\n",
      "where c(x, y) is constant for given x and y, then any inference about θ based\n",
      "on x should be in agreement with any inference about θ based on y.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "246\n",
      "3 Basic Statistical Theory\n",
      "3.2.2 The Empirical Cumulative Distribution Function\n",
      "Given a sample X1 = x1, . . ., Xn = xn as independent observations on a ran-\n",
      "dom variable X, we can form another random variable X∗that has a discrete\n",
      "uniform distribution with probability mass 1/n at each of the sample points.\n",
      "(In the case that xi = xj for some i and j, X∗would not have a uniform\n",
      "distribution of course. Whether or not this is the case, however, for conve-\n",
      "nience, we will continue to refer to the distribution as uniform.) This discrete\n",
      "uniform distribution can be used for making inferences on the distribution of\n",
      "the random variable of interest\n",
      "From observations on a random variable, X1, . . ., Xn, we can form an\n",
      "empirical cumulative distribution function, or ECDF, that corresponds in a\n",
      "natural way with the CDF of the random variable.\n",
      "For the sample, X1, . . ., Xn, the ECDF is deﬁned as the CDF of X∗; that\n",
      "is,\n",
      "Pn(x) = #{Xi ≤x}\n",
      "n\n",
      ".\n",
      "(3.61)\n",
      "The ECDF is a random simple function, and often it is appropriate to treat\n",
      "the ECDF as a random variable. It is clear that the ECDF conditional on a\n",
      "given sample is itself a CDF. (Conditionally it is not a “random” variable;\n",
      "that is, it is a degenerate random variable.) It has the three properties that\n",
      "deﬁne a CDF:\n",
      "•\n",
      "limx→−∞Pn(x) = 0 and limx→∞Pn(x) = 1.\n",
      "•\n",
      "Pn(x) is monotone increasing.\n",
      "•\n",
      "Pn(x) is continuous from the right.\n",
      "The ECDF deﬁnes a discrete population with mass points at each value in\n",
      "the sample.\n",
      "The ECDF is particularly useful in nonparametric inference. We will see\n",
      "below how it can allow us to “bootstrap” an unknown population and also\n",
      "how it can allow us to use the principles of likelihood even though we may be\n",
      "unable to write out the density of the population.\n",
      "Plug-In Estimators; The Substitution Principle\n",
      "As discussed in Section 1.1.9, many distribution parameters and other mea-\n",
      "sures can be represented as a statistical function, that is, as a functional of the\n",
      "CDF. The functional of the CDF that deﬁnes a parameter deﬁnes a plug-in\n",
      "estimator of that parameter when the functional is applied to the ECDF. A\n",
      "functional of a population distribution function, Θ(P ), that deﬁnes a param-\n",
      "eter θ can usually be expressed as\n",
      "θ = Θ(P )\n",
      "=\n",
      "Z\n",
      "g(y) dP (y).\n",
      "(3.62)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "247\n",
      "The plug-in estimator T is the same functional of the ECDF:\n",
      "T = Θ(Pn)\n",
      "=\n",
      "Z\n",
      "g(y) dPn(y).\n",
      "(3.63)\n",
      "(In both of these expressions, we are using the integral in a general sense. In\n",
      "the second expression, the integral is a ﬁnite sum. It is also a countable sum\n",
      "in the ﬁrst expression if the random variable is discrete. Note also that we use\n",
      "the same symbol to denote the functional and the random variable.)\n",
      "The use of the functional that deﬁnes a statistical function on the ECDF\n",
      "to make inferences on the statistical function is called the substitution prin-\n",
      "ciple. It is one of the most useful and most pervasive principles in statistical\n",
      "inference.\n",
      "We may base inferences on properties of the distribution with CDF P\n",
      "by identifying the corresponding properties of the ECDF Pn. In some cases,\n",
      "it may not be clear what we mean by “corresponding”. If a property of a\n",
      "distribution can be deﬁned by a functional on the CDF, the corresponding\n",
      "property is the same functional applied to the ECDF. This is the underlying\n",
      "idea of the method of moments, for example.\n",
      "The asymptotic properties of plug-in estimators can be developed by\n",
      "Taylor-series-type expansions of the statistical functions, as discussed on\n",
      "page 95 in Section 1.3.7. We consider this further on page 316.\n",
      "Method of Moments Estimators\n",
      "In the method of moments, sample moments, which are moments of the dis-\n",
      "crete population represented by the sample, are used for making inferences\n",
      "about population moments. The MME of the population mean, E(X), is the\n",
      "sample mean, X. The thing to be estimated is the functional M in equa-\n",
      "tion (1.109), and the estimator is M applied to the ECDF:\n",
      "M(Pn) =\n",
      "X\n",
      "XiPn(Xi).\n",
      "We call a method-of-moments estimator an MME.\n",
      "The plug-in estimator Θ(Pn) in general is not unbiased for the associated\n",
      "statistical function Θ(P ). A simple example is the variance,\n",
      "Σ(P ) = σ2 =\n",
      "Z \u0012\n",
      "x −\n",
      "Z\n",
      "x dP\n",
      "\u00132\n",
      "dP.\n",
      "The plug-in estimator, which in this case is also a MME, is\n",
      "Σ(Pn) = 1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(Xi −X)2.\n",
      "(3.64)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "248\n",
      "3 Basic Statistical Theory\n",
      "We see that if n ≥2, the MME Σ(Pn) = (n −1)S2/n, where\n",
      "S2 =\n",
      "1\n",
      "n −1\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(Xi −X)2\n",
      "(3.65)\n",
      "is the usual sample variance.\n",
      "On the other hand, the plug-in estimator may have smaller MSE than an\n",
      "unbiased estimator, and, in fact, that is the case for the plug-in estimator of\n",
      "σ2 (see equation (3.55)). Also, plug-in estimators often have good limiting and\n",
      "asymptotic properties, as we might expect based on convergence properties of\n",
      "the ECDF.\n",
      "Convergence of the ECDF\n",
      "The ECDF is one of the most useful statistics, especially in nonparametric\n",
      "and robust inference. It is essentially the same as the set of order statistics,\n",
      "so like them, it is a suﬃcient statistic. Although we may write the ECDF as\n",
      "Fn or Fn(x), it is important to remember that it is a random variable.\n",
      "The distribution of the ECDF at a point is binomial, and so the pointwise\n",
      "properties of the ECDF are easy to see. From the SLLN, we see that it strongly\n",
      "converges pointwise to the CDF. At the point x, by the CLT we have\n",
      "√n(Fn(x) −F (x)) d→N (0, F (x)(1 −F (x))) .\n",
      "Although the pointwise properties of the ECDF are useful, its global re-\n",
      "lationship to the CDF is one of the most important properties of the ECDF.\n",
      "Dvoretzky/Kiefer/Wolfowitz/Massart inequality (1.289)\n",
      "Pr(ρ∞(Fn, F ) > z) ≤2e−2nz2\n",
      "provides a tight bound on the diﬀerence in the ECDF and the CDF.\n",
      "The Glivenko-Cantelli theorem (page 135) tells us that the sup distance\n",
      "of the ECDF from the CDF, ρ∞(Fn, F ), converges almost surely to zero; that\n",
      "is, the ECDF converges strongly and uniformly to the CDF.\n",
      "The Bootstrap\n",
      "The ECDF plays a major role in a bootstrap method, in which the population\n",
      "of interest is studied by sampling from the population deﬁned by a given\n",
      "sample from the population of interest. This is a method of resampling.\n",
      "Resampling methods involve the use of many samples, each taken from a\n",
      "single sample that was taken from the population of interest. Inference based\n",
      "on resampling makes use of the conditional sampling distribution of a new\n",
      "sample (the “resample”) drawn from a given sample. Statistical functions on\n",
      "the given sample, a ﬁnite set, can easily be evaluated. Resampling methods\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "249\n",
      "therefore can be useful even when very little is known about the underlying\n",
      "distribution.\n",
      "A basic idea in bootstrap resampling is that, because the observed sample\n",
      "contains all the available information about the underlying population, the\n",
      "observed sample can be considered to be the population; hence, the distribu-\n",
      "tion of any relevant test statistic can be simulated by using random samples\n",
      "from the “population” consisting of the original sample.\n",
      "Suppose that a sample y1, . . ., yn is to be used to estimate a population\n",
      "parameter, θ. For a statistic T that estimates θ, as usual, we wish to know\n",
      "the sampling distribution so as to correct for any bias in our estimator or to\n",
      "set conﬁdence intervals for our estimate of θ. The sampling distribution of T\n",
      "is often intractable in applications of interest.\n",
      "A basic bootstrapping method formulated by Efron (1979) uses the discrete\n",
      "distribution represented by the sample to study the unknown distribution from\n",
      "which the sample came. The basic tool is the empirical cumulative distribution\n",
      "function. The ECDF is the CDF of the ﬁnite population that is used as a model\n",
      "of the underlying population of interest.\n",
      "For a parameter θ of a distribution with CDF P deﬁned as θ = Θ(P ),\n",
      "we can form a plug-in estimator T as T = Θ(Pn). Various properties of the\n",
      "distribution of T can be estimated by use of “bootstrap samples”, each of\n",
      "the form {y∗\n",
      "1, . . ., y∗\n",
      "n}, where the y∗\n",
      "i ’s are chosen from the original yi’s with\n",
      "replacement.\n",
      "We deﬁne a resampling vector, p∗, corresponding to each bootstrap sample\n",
      "as the vector of proportions of the elements of the original sample in the given\n",
      "bootstrap sample. The resampling vector is a realization of a random vector\n",
      "P ∗for which nP ∗has an n-variate multinomial distribution with parameters\n",
      "n and (1/n, . . ., 1/n). The resampling vector has random components that\n",
      "sum to 1. For example, if the bootstrap sample (y∗\n",
      "1, y∗\n",
      "2, y∗\n",
      "3, y∗\n",
      "4) happens to be\n",
      "the sample (y2, y2, y4, y3), the resampling vector p∗is\n",
      "(0, 1/2, 1/4, 1/4).\n",
      "The bootstrap replication of the estimator T is a function of p∗, T(p∗).\n",
      "The resampling vector can be used to estimate the variance of the bootstrap\n",
      "estimator. By imposing constraints on the resampling vector, the variance of\n",
      "the bootstrap estimator can be reduced.\n",
      "The Bootstrap Principle\n",
      "The bootstrap principle involves repeating the process that leads from a popu-\n",
      "lation CDF to an ECDF. Taking the ECDF Pn to be the CDF of a population,\n",
      "and resampling, we have an ECDF for the new sample, P (1)\n",
      "n . (In this notation,\n",
      "we could write the ECDF of the original sample as P (0)\n",
      "n .) The diﬀerence is\n",
      "that we know more about P (1)\n",
      "n\n",
      "than we know about Pn. Our knowledge about\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "250\n",
      "3 Basic Statistical Theory\n",
      "P (1)\n",
      "n\n",
      "comes from the simple discrete uniform distribution, whereas our knowl-\n",
      "edge about Pn depends on knowledge (or assumptions) about the underlying\n",
      "population.\n",
      "The bootstrap resampling approach can be used to derive properties of\n",
      "statistics, regardless of whether any resampling is done. Most common uses of\n",
      "the bootstrap involve computer simulation of the resampling; hence, bootstrap\n",
      "methods are usually instances of computational inference.\n",
      "Empirical Likelihood\n",
      "The ECDF can also be used to form a probability density for a method based\n",
      "on a likelihood function.\n",
      "3.2.3 Fitting Expected Values\n",
      "Given a random sample X1, . . ., Xn from distributions with probability den-\n",
      "sities pi(xi; θ), where all PDFs are deﬁned with respect to a common σ-ﬁnite\n",
      "measure, if we have that E(Xi) = gi(θ), a reasonable approach to estimation\n",
      "of θ may be to choose a value ˆθ that makes the diﬀerences E(Xi)−gi(θ) close\n",
      "to zero. If the Xi are iid, then all gi(θ) are the same, say g(θ).\n",
      "We must deﬁne the sense in which the diﬀerences are close to zero. A\n",
      "simple way to do this is to deﬁne a nonnegative scalar-valued Borel function\n",
      "of scalars, ρ(u, v), that is increasing in the absolute diﬀerence of its arguments.\n",
      "One simple choice is ρ(u, v) = (u −v)2. We then deﬁne\n",
      "Sn(θ, x) =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "ρ(xi, g(θ)).\n",
      "(3.66)\n",
      "For a random sample X = X1, . . ., Xn, an estimator ﬁtted to the expected\n",
      "values is g(T) where\n",
      "T = arg min\n",
      "θ∈Θ\n",
      "Sn(θ, X).\n",
      "(3.67)\n",
      "Compare this with the maximum likelihood estimate of θ, deﬁned in equa-\n",
      "tion (3.49).\n",
      "As with solving the maximization of the likelihood, if the function to be\n",
      "optimized is diﬀerentiable, the solution to the minimization problem (3.67)\n",
      "may be obtained by solving\n",
      "sn(θ ; x) = ∂Sn(θ ; x)\n",
      "∂θ\n",
      "= 0.\n",
      "(3.68)\n",
      "Equation (3.68) is an estimating equation; that is, its solution, if it exists, is\n",
      "taken as an estimator. Note that this estimator is not necessarily a solution\n",
      "to the optimization problem (3.67).\n",
      "In common applications, we have covariates, Z1, . . ., Zn, and the E(Xi)\n",
      "have a constant form that depends on the covariate: E(Xi) = g(Zi, θ).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "251\n",
      "Example 3.14 least squares in a linear model\n",
      "Consider the linear model (3.7)\n",
      "Y = Xβ + E,\n",
      "where Y is the random variable that is observable, in the least squares setup\n",
      "of equations (3.66) and (3.67) we have\n",
      "Sn(β ; y, X) = ∥y −Xβ∥2.\n",
      "(3.69)\n",
      "In the case of the linear model, we have the estimating equation\n",
      "sn(β ; y, X) = XTy −XTXβ = 0.\n",
      "(3.70)\n",
      "This system of equations is called the “normal equations”.\n",
      "For the estimand g(β) = lTβ for some ﬁxed l, the least squares estimator\n",
      "is lT(XTX)−XTY , where M −denotes a generalized inverse of a matrix M.\n",
      "See page 424 and the following pages for a more thorough discussion of the\n",
      "linear model in this example.\n",
      "Example 3.14 illustrates a very simple and common application of estima-\n",
      "tion by ﬁtting expected values; the expected values are those of the observable\n",
      "random variable. The next example is a somewhat less common situation of\n",
      "deﬁning which expected values to focus on.\n",
      "Example 3.15 estimation in a stable family; the empirical CF\n",
      "Most members of the stable family of distributions are quite complicated.\n",
      "In general, there is no closed form for the CDF of the PDF, and none of\n",
      "the moments exist or else are inﬁnite. The family of distributions is usually\n",
      "speciﬁed by means of the characteristic function (see equation (2.26)),\n",
      "ϕ(t) = exp (iµt −|σt|α (1 −iβ sign(t)ω(α, t))) .\n",
      "Because the characteristic function is an expected value, its sample analogue,\n",
      "that is, the empirical characteristic function can be formed easily from a sam-\n",
      "ple, x1, . . ., xn:\n",
      "ϕn(t) = 1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "eitxi.\n",
      "(3.71)\n",
      "The empirical characteristic function can be computed at any point t.\n",
      "The expected values would be ﬁt by minimizing\n",
      "Sn(x, r, µ, σ, α, β) = ∥ϕn(t) −ϕ(t)∥r\n",
      "(3.72)\n",
      "for given r at various values of t.\n",
      "While this is a well-deﬁned problem (for some given values of t) and the\n",
      "resulting estimators are strongly consistent (for the same reason that estima-\n",
      "tors based on the ECDF are strongly consistent), there are many practical\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "252\n",
      "3 Basic Statistical Theory\n",
      "issues in the implementation of the method. Press (1972) proposed ﬁtting\n",
      "moments as an approximation to the values that would be obtained by min-\n",
      "imizing sn in equation (3.72). For the case of r = 2, Koutrouvelis (1980)\n",
      "proposed a regression method that seems to perform fairly well in simulation\n",
      "studies. Kogan and Williams (1998) summarize these and other methods for\n",
      "estimating the parameters in a stable family.\n",
      "Quantile Based Estimators\n",
      "The expected values of order statistics can often yield good estimators when\n",
      "sample quantiles are ﬁt to them. In most cases, the statistical properties of\n",
      "these estimators are not as good as alternative estimators, but there are some\n",
      "cases where they are useful. One such situation is where the distribution is\n",
      "very complicated, such as the family of stable distributions in Example 3.15.\n",
      "Fama and Roll (1971) describe methods for estimation of the parameters in a\n",
      "symmetric stable family (that is, one in which β = 0).\n",
      "Estimators based on quantiles are especially useful in heavy-tailed distri-\n",
      "butions. (The stable family is heavy-tailed.) Beginning on page 608, I will\n",
      "discuss a type of robust estimators, called L-estimators, that are linear com-\n",
      "binations of order sample quantiles.\n",
      "Regularization of Fits\n",
      "The objective function for ﬁtting expected values may not be well-behaved.\n",
      "Small variations in the sample may yield large diﬀerences in the estimates. The\n",
      "ill-conditioned objective function yield estimators with large variances. An\n",
      "approach to this problem is to “regularize” the objective function by modifying\n",
      "it to be better conditioned. In a minimization problem a simple way of making\n",
      "the objective function better conditioned is to add a penalty term for variation\n",
      "in the solution. This means that the solution is pulled toward some ﬁxed value.\n",
      "Often there is no obvious ﬁxed value to bias an estimator toward. A common\n",
      "procedure is merely to shrink the estimator toward 0. Given an objective\n",
      "function of the form (3.66), a modiﬁed objective function that shrinks the\n",
      "estimator toward 0 is\n",
      "eSn(θ, x) =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "ρ1(xi, g(θ)) + ρ2(θ, θ).\n",
      "(3.73)\n",
      "Use of this type of objective function in regression analysis leads to “ridge\n",
      "regression”; see Example 5.27.\n",
      "The idea of regularization is also used in other estimation methods that\n",
      "involve a minimization or maximization, such as maximum likelihood estima-\n",
      "tion of a PDF as on page 581, for example.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "253\n",
      "3.2.4 Fitting Probability Distributions\n",
      "Another approach to statistical inference is to use the observed data to ﬁt\n",
      "the probability distribution over the full support. The ﬁt is chosen to min-\n",
      "imize some measure of the diﬀerence between it and the values of the PDF\n",
      "(or probability function). To pursue this idea, we need some measure of the\n",
      "diﬀerence.\n",
      "The quantity\n",
      "d(P, Q) =\n",
      "Z\n",
      "IR\n",
      "φ\n",
      "\u0012dP\n",
      "dQ\n",
      "\u0013\n",
      "dQ,\n",
      "(3.74)\n",
      "if it exists, is called the φ-divergence from Q to P . The φ-divergence is also\n",
      "called the f-divergence.\n",
      "The expression often has a more familiar form if both P and Q are domi-\n",
      "nated by Lebesgue measure and we write p = dP and q = dQ.\n",
      "A speciﬁc instance of φ-divergence is the Kullback-Leibler measure,\n",
      "Z\n",
      "IR\n",
      "p(x) log\n",
      "\u0012p(x)\n",
      "q(x)\n",
      "\u0013\n",
      "dx.\n",
      "(3.75)\n",
      "(Recall from page 850 that this quantity is nonnegative.)\n",
      "The φ-divergence is in general not a metric because it is not symmetric.\n",
      "One function is taken as the base from which the other function is measured. In\n",
      "Section 0.1.9 beginning on page 747, we discuss metrics and also φ-divergence\n",
      "in the more general context of comparing two functions.\n",
      "While this idea can be used for any type of distribution, it is most useful\n",
      "in the case of a discrete distribution with a ﬁnite number of mass points. In\n",
      "the case of a distribution with d mass points with probabilities π1, . . ., πd, the\n",
      "full information content of a sample X1, . . ., Xn is the information in a sample\n",
      "Y1, . . ., Yd from a multinomial distribution with parameters n and π1, . . ., πd.\n",
      "Various measures of divergence in a multinomial distribution are well-\n",
      "known. The most commonly used measure is the chi-squared measure, which\n",
      "is given in a general form in equation (0.1.86) on page 748. . This measure\n",
      "has a simpler form in the multinomial case. It is also a member of the family\n",
      "of power divergence measures, which for λ ∈IR, is\n",
      "Iλ =\n",
      "1\n",
      "λ(λ + 1)\n",
      "d\n",
      "X\n",
      "i=1\n",
      "Yi\n",
      " \u0012 Yi\n",
      "nπi\n",
      "\u0013λ\n",
      "−1\n",
      "!\n",
      ".\n",
      "(3.76)\n",
      "There are equivalent forms of this measure that are scaled by d or by some\n",
      "other constant, and for given λ the constant factor plays no role in minimizing\n",
      "the divergence. For λ = 1, this is the same as the chi-squared discrepancy mea-\n",
      "sure, For λ = 0 in the limit, this is the same as the log-likelihood ratio statis-\n",
      "tic, and for λ = −1/2, it is the Freeman-Tukey statistic. Cressie and Read\n",
      "(1984) studied this general family of power divergence measures, and sug-\n",
      "gested λ = 2/3 as a value that has some of the desirable properties of both\n",
      "the chi-squared and log-likelihood ratio statistics.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "254\n",
      "3 Basic Statistical Theory\n",
      "The parameters of the multinomial are just π1, . . ., πd and estimators of\n",
      "them based on a power divergence measure are straightforward. When a multi-\n",
      "nomial distribution is formed from another distribution, however, the estima-\n",
      "tion problem is more interesting.\n",
      "Example 3.16 minimum distance estimation in a Poisson model\n",
      "Suppose we have observations x1, x2, . . ., xn from a Poisson distribution with\n",
      "unknown parameter θ. The sample values are all nonnegative integers and if\n",
      "θ is relatively small, there may be very few observations that exceed some\n",
      "small number. Suppose we form a multinomial model, as indicated above,\n",
      "with d = 3; that is, y1 is the number of 0s observed, y2 is the number of 1s\n",
      "observed, and y3 is the number of observed values greater than or equal to 2.\n",
      "We have π1 = e−θ, π2 = θe−θ, and π3 = 1 −(1 + θ)e−θ.\n",
      "The minimum power divergence estimator of θ is obtained by substituting\n",
      "the appropriate values of π in expression (3.76) and then minimizing it with\n",
      "respect to θ. Thus, given the observations y1, y2, y3, the Cressie-Read estimate\n",
      "is\n",
      "arg minθ∈IR\n",
      "\u0012\n",
      "y1\n",
      "\u0012\u0010\n",
      "y1/n\n",
      "e−θ\n",
      "\u00112/3\n",
      "−1\n",
      "\u0013\n",
      "+y2\n",
      "\u0012\u0010\n",
      "y2/n\n",
      "θe−θ\n",
      "\u00112/3\n",
      "−1\n",
      "\u0013\n",
      "+y3\n",
      "\u0012\u0010\n",
      "y3/n\n",
      "1−(1+θ)e−θ\n",
      "\u00112/3\n",
      "−1\n",
      "\u0013\u0013\n",
      ".\n",
      "In an approach to statistical inference based on information theory, the\n",
      "true but unknown distribution is compared with information in the sample.\n",
      "The focus is on “information” or “entropy”, in the sense discussed on page 42.\n",
      "The basic quantity is of the form E(−log(dP )). The principle underlying\n",
      "methods of statistical inference using these concepts and quantities is called\n",
      "maximum entropy.\n",
      "3.2.5 Estimating Equations\n",
      "Equations (3.56) and (3.68) are estimating equations; that is, their solutions,\n",
      "if they exist, are taken as estimates. Note that the solutions to the estimating\n",
      "equations are not necessarily the solutions to the optimization problems that\n",
      "gave rise to them. They are both merely roots of estimating equations.\n",
      "Estimating Functions and Generalized Estimating Equations\n",
      "Estimating equations arise often in statistical inference. There are also several\n",
      "modiﬁcations of the basic equations; for example, sometimes we cannot form a\n",
      "tractable likelihood, so we form some kind of “quasi-likelihood”. We therefore\n",
      "consider a generalized class of estimating equations.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "255\n",
      "We consider an independent sample X1, . . ., Xn of random vectors with\n",
      "orders d1, . . ., dn, with sup di < ∞. We assume the distributions of the Xi are\n",
      "deﬁned with respect to a common parameter θ ∈Θ ⊆IRk. We now deﬁne\n",
      "Borel functions ψi(Xi, t) and let\n",
      "sn(t ; X) =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "ψi(Xi, t)\n",
      "t ∈Θ.\n",
      "(3.77)\n",
      "If Eθ((sn(θ ; X))2) < ∞, we call\n",
      "sn(t ; X)\n",
      "(3.78)\n",
      "an estimating function. We often write the estimating function simply as sn(t).\n",
      "(Also, note that I am writing “t” instead of “θ” to emphasize that it is a\n",
      "variable in place of the unknown parameter.)\n",
      "Two prototypic estimating functions are the score function, equation (3.57)\n",
      "and the function on the left side of the normal equations (3.70).\n",
      "We call\n",
      "sn(t) = 0\n",
      "(3.79)\n",
      "a generalized estimating equation (GEE) and we call a root of the generalized\n",
      "estimating equation a GEE estimator. If we take\n",
      "ψi(Xi, t) = ∂ρ(Xi, t)/∂t,\n",
      "we note the similarity of the GEE to equation (3.68).\n",
      "Unbiased Estimating Functions\n",
      "The estimating function is usually chosen so that\n",
      "Eθ(sn(θ ; X)) = 0,\n",
      "(3.80)\n",
      "or else so that the asymptotic expectation of {sn} is zero.\n",
      "If sn(θ ; X) = T(X) −g(θ), the condition (3.80) is equivalent to the es-\n",
      "timator T(X) being unbiased for the estimand g(θ). This leads to a more\n",
      "general deﬁnition of unbiasedness for a function.\n",
      "Deﬁnition 3.7 (unbiased estimating function)\n",
      "The estimating function sn(θ ; X) is unbiased if\n",
      "Eθ(sn(θ ; X)) = 0\n",
      "∀θ ∈Θ.\n",
      "(3.81)\n",
      "An unbiased estimating function does not necessarily lead to an unbiased\n",
      "estimator of g(θ), unless, of course, sn(θ ; X) = T(X) −g(θ).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "256\n",
      "3 Basic Statistical Theory\n",
      "We also note that unbiased estimating functions are essentially members\n",
      "of equivalence classes formed by multiples that are independent of the random\n",
      "variable. That is, if sn(θ ; X) is unbiased, and g(θ) is a function that does not\n",
      "depend on X, then g(θ)sn(θ ; X) is also unbiased.\n",
      "Notice that equation (3.80) holds for the normal equations (3.70); there-\n",
      "fore, the estimating function in the normal equations, XTY −XTXβ is un-\n",
      "biased. On page 463, we will also see that the score function is unbiased.\n",
      "Eﬃciency of Estimating Functions\n",
      "The eﬃciency of a statistical procedure generally refers to the mean squared\n",
      "error of the procedure. For certain families of distributions, we can establish\n",
      "lower bounds on the variance of a statistic.\n",
      "An approach to estimation that we have mentioned a few times already and\n",
      "will study more fully in later sections and chapters is to restrict attention to\n",
      "unbiased statistics and to determine one of those that minimizes the variance\n",
      "at all points in the parameter space. If sn(θ ; X) is unbiased, then\n",
      "Vθ(sn(θ ; X)) = Eθ((sn(θ ; X))2).\n",
      "(3.82)\n",
      "For the case of unbiased estimators in certain families of distributions, the\n",
      "lower bound on the variance takes on special importance.\n",
      "For an unbiased estimator T of g(θ) in a family of densities satisfying\n",
      "the regularity conditions and such that T has a ﬁnite second moment, from\n",
      "inequality (3.39) on page 234, we have the matrix relationship\n",
      "V(T(X)) ≥\n",
      "\u0012 ∂\n",
      "∂θ g(θ)\n",
      "\u0013T\n",
      "(I(θ))−1 ∂\n",
      "∂θ g(θ),\n",
      "(3.83)\n",
      "where we assume the existence of all quantities in the expression.\n",
      "Deﬁnition 3.8 (eﬃcient estimator; Fisher eﬃcient)\n",
      "Given a family of distributions {Pθ} satisfying the FI regularity conditions,\n",
      "an unbiased estimator T(X) of g(θ) is said to be eﬃcient or Fisher eﬃcient\n",
      "if V(T(X)) attains the lower bound in inequality (3.83).\n",
      "Notice the slight diﬀerence in “eﬃciency” and “eﬃcient”; while one mean-\n",
      "ing of “eﬃciency” is a relative term that is not restricted to unbiased estima-\n",
      "tors (or other unbiased procedures, as we will see later), “eﬃcient” is absolute.\n",
      "“Eﬃcient” only applies to unbiased estimators, and an estimator either is or\n",
      "is not eﬃcient. The state of being eﬃcient, of course is called “eﬃciency”.\n",
      "This is another meaning of the term. The phrase “Fisher eﬃciency” helps to\n",
      "emphasis this diﬀerence.\n",
      "To minimize the variance among all unbiased estimating functions leads to\n",
      "the trivial solution (sn(θ ; X)) ≡0, because, as we noted above, any multiple\n",
      "of sn(θ ; X) that does not involve X is unbiased. We therefore seek other ways\n",
      "of deﬁning optimality among a class of unbiased estimating functions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.2 Statistical Inference: Approaches and Methods\n",
      "257\n",
      "We consider a generalization of the Fisher information (3.30) with sn(θ ; X) =\n",
      "∂log p(X; θ)/∂θ:\n",
      "Eθ\n",
      "\u0010\n",
      "(sn(θ ; X)) (sn(θ ; X))T\u0011\n",
      ".\n",
      "Now we deﬁne eﬃciency of unbiased estimating functions in terms of this\n",
      "quantity.\n",
      "Deﬁnition 3.9 (eﬃciency of unbiased estimating functions)\n",
      "Let sn(θ ; X) be an unbiased estimating function that is diﬀerentiable in θ.\n",
      "The eﬃciency of sn is\n",
      "(Eθ(∂sn(θ ; X)/∂θ))T \u0010\n",
      "Eθ\n",
      "\u0010\n",
      "(sn(θ ; X)) (sn(θ ; X))T\u0011\u0011−1\n",
      "(Eθ(∂sn(θ ; X)/∂θ)) .\n",
      "The eﬃciency of unbiased estimating functions is sometimes called Godambe\n",
      "eﬃciency, after V. P. Godambe. Compare this expression for the eﬃciency of\n",
      "an unbiased estimating function with the CRLB, which is expressed in terms\n",
      "of a score function.\n",
      "Notice that for estimating functions, we deﬁne eﬃciency only for unbiased\n",
      "functions. Just as in the case of point estimators, with estimating functions,\n",
      "we use the word “eﬃcient” in the sense of “most eﬃcient”.\n",
      "Deﬁnition 3.10 (eﬃcient unbiased estimating functions)\n",
      "Let s∗\n",
      "n(θ ; X) be an unbiased estimating function that is diﬀerentiable in θ. If\n",
      "the eﬃciency of s∗\n",
      "n is at least as great as the eﬃciency of any other unbiased\n",
      "estimating function that is diﬀerentiable in θ, then we say s∗\n",
      "n is eﬃcient, or\n",
      "(synonymously) Godambe eﬃcient.\n",
      "That is, while “eﬃciency” is a relative term, “eﬃcient” is absolute. An eﬃcient\n",
      "estimating function is not necessarily unique, however.\n",
      "Deﬁnition 3.11 (martingale estimating function)\n",
      "Let {(Xt, Ft) : t ∈T } be a forward martingale, and let {st(θ ; Xt) : t ∈T }\n",
      "be adapted to the ﬁltration {Ft)}. Then {st(θ ; Xt)\n",
      ":\n",
      "t ∈T } is called a\n",
      "martingale estimating function iﬀ\n",
      "s0(θ ; X0) a.s.\n",
      "= 0\n",
      "and\n",
      "E(st(θ ; Xt)|Ft−1) a.s.\n",
      "= st−1(θ ; Xt−1).\n",
      "Martingale estimating functions arise in applications of stochastic process\n",
      "models, for example, in the analysis of ﬁnancial data.\n",
      "Our interest in estimating functions is due to their use in forming estimat-\n",
      "ing equations and subsequently in yielding estimators. We will consider some\n",
      "asymptotic properties of solutions to estimating equations in Section 3.8.1\n",
      "(consistency) and in Section 6.3.4 (asymptotic normality).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "258\n",
      "3 Basic Statistical Theory\n",
      "3.2.6 Summary and Preview\n",
      "We have discussed four general approaches to statistical inference, and have\n",
      "identiﬁed a ﬁfth one that we implied would warrant more careful study later.\n",
      "At this point, let us review and summarize the procedures that we have dis-\n",
      "cussed and brieﬂy introduce the other approach, which we will discuss in\n",
      "Section 3.3.\n",
      "•\n",
      "estimation based on the ECDF\n",
      "–\n",
      "estimate g(θ) so that the quantiles of P d\n",
      "g(θ) are close to the quantiles\n",
      "of the data\n",
      "How many and which quantiles to match?\n",
      "Use of a plug-in estimator from the empirical cumulative distribution\n",
      "function follows this approach, and in that case all quantiles from the\n",
      "data are used.\n",
      "This approach may involve questions of how to deﬁne sample quantiles.\n",
      "We will continue to use the term “sample quantile” of order π to refer\n",
      "to the order statistic X(⌈nπ⌉+1:n).\n",
      "An example of this approach is the requirement of median-unbiasedness\n",
      "(one speciﬁc quantile).\n",
      "–\n",
      "estimate g(θ) so that the moments of P d\n",
      "g(θ) are close to the sample\n",
      "moments\n",
      "How many and which moments to match?\n",
      "Do the population moments exist?\n",
      "Method-of-moments estimators may have large variances; hence, while\n",
      "this method may be simple (and widely-used), it is probably not a good\n",
      "method generally.\n",
      "An example of this approach is the requirement of unbiasedness (one\n",
      "speciﬁc moment).\n",
      "•\n",
      "use the likelihood\n",
      "–\n",
      "estimate g(θ) as g(ˆθ), where ˆθ maximizes the likelihood function,\n",
      "L(θ, x, z).\n",
      "Maximum likelihood estimation is closely related to minimum-residual-\n",
      "norm estimation. For the normal distribution, for example, MLE is the\n",
      "same as LS, and for the double exponential distribution, MLE is the\n",
      "same as LAV.\n",
      "If there is a suﬃcient statistic, a MLE is a function of it. (This does\n",
      "not say that every MLE is a function of the suﬃcient statistic.)\n",
      "MLEs often have very good statistical properties. The are particularly\n",
      "easy to work with in exponential families.\n",
      "•\n",
      "estimation by ﬁtting expected values\n",
      "–\n",
      "estimate g(θ) so that residuals ∥xi −E d\n",
      "g(θ)(Xi, zi)∥are small.\n",
      "An example of this approach is least squares (LS) estimation (the Eu-\n",
      "clidean norm of the vector of residuals, or square root of an inner prod-\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "259\n",
      "uct of the vector with itself). If the expectation exists, least squares\n",
      "yields unbiasedness.\n",
      "Another example of this approach is least absolute values (LAV) esti-\n",
      "mation, in which the L1 norm of the vector of residuals is minimized.\n",
      "This yields median-unbiasedness.\n",
      "•\n",
      "ﬁt an empirical probability distribution\n",
      "This approach is somewhat similar to ﬁtting an ECDF, but in the case\n",
      "of PDFs, the criterion of closeness of the ﬁt must be based on regions of\n",
      "nonzero probability; that is, it can be based on divergence measures.\n",
      "•\n",
      "deﬁne and use a loss function\n",
      "(This is an approach based on “decision theory”, which we introduce for-\n",
      "mally in Section 3.3. The speciﬁc types of estimators that result from this\n",
      "approach are the subjects of several later chapters.)\n",
      "The loss function increases the more the estimator diﬀers from the esti-\n",
      "mand, and then estimate g(θ) so as to minimize the expected value of the\n",
      "loss function (that is, the “risk”) at points of interest in the parameter\n",
      "space.\n",
      "–\n",
      "require unbiasedness and minimize the variance at all points in the\n",
      "parameter space (this is UMVU estimation, which we discuss more\n",
      "fully in Section 5.1)\n",
      "–\n",
      "require equivariance and minimize the risk at all points in the parame-\n",
      "ter space (this is MRE or MRI estimation, which we discuss more fully\n",
      "in Section 3.4)\n",
      "–\n",
      "minimize the maximum risk over the full parameter space\n",
      "–\n",
      "deﬁne an a priori averaging function for the parameter, use the observed\n",
      "data to update the averaging function and minimize the risk deﬁned\n",
      "by the updated averaging function.\n",
      "3.3 The Decision Theory Approach to Statistical\n",
      "Inference\n",
      "3.3.1 Decisions, Losses, Risks, and Optimal Actions\n",
      "In the decision-theoretic approach to statistical inference, we call the inference\n",
      "a decision or an action, and we identify a cost or loss that depends on the\n",
      "decision and the true (but unknown) state of nature modeled by P ∈P.\n",
      "(Instead of loss, we could use its opposite, which is called utility.)\n",
      "Our objective is to choose an action that minimizes the expected loss, or\n",
      "conversely maximizes the expected utility.\n",
      "We call the set of allowable actions or decisions the action space or decision\n",
      "space, and we denote it as A. We base the inference on the random variable\n",
      "X; hence, the decision is a mapping from X , the range of X, to A.\n",
      "In estimation problems, the action space may be a set of real numbers\n",
      "corresponding to a parameter space. In tests of statistical hypotheses, we may\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "260\n",
      "3 Basic Statistical Theory\n",
      "deﬁne the action space as A = {0, 1}, in which 0 represents not rejecting and\n",
      "1 represents rejecting.\n",
      "If we observe X, we take the action T(X) = a ∈A. An action or a decision\n",
      "may be the assignment of a speciﬁc value to an estimator, that is, an estimate,\n",
      "or it may be to decide whether or not to reject a statistical hypothesis.\n",
      "Decision Rules\n",
      "Given a random variable X with associated measurable space (X , FX) and an\n",
      "action space A with a σ-ﬁeld FA, a decision rule is a function, T, from X to\n",
      "A that is measurable FX/FA.\n",
      "A decision rule is also often denoted by δ or δ(X).\n",
      "Randomized Decision Rules\n",
      "Sometimes the available data, that is, the realization of X, does not provide\n",
      "suﬃcient evidence to make a decision. In such cases, of course, it would be\n",
      "best to obtain more data before making a decision. If a decision must be made,\n",
      "however, it may be desirable to choose an action randomly, perhaps under a\n",
      "probability model that reﬂects the available evidence. A randomized decision\n",
      "rule is a function δ over X ×FA such that for every A ∈FA, δ(·, A) is a Borel\n",
      "function, and for every x ∈X , δ(x, ·) is a probability measure on (A, FA).\n",
      "To evaluate a randomized decision rule requires the realization of an ad-\n",
      "ditional random variable. As suggested above, this random variable may not\n",
      "be independent of the data. Randomized decision rules are rarely appropriate\n",
      "in actual applications, but an important use of randomized decision rules is\n",
      "to evaluate properties of statistical procedures. In the development of sta-\n",
      "tistical theory, we often use randomized decision rules to show that certain\n",
      "deterministic rules do or do not have certain properties.\n",
      "Loss Function\n",
      "A loss function, L, is a mapping from P×A to [0, ∞[. The value of the function\n",
      "at a given distribution P for the action a is L(P, a). More commonly, we refer\n",
      "to the loss function associated with a given nonrandomized decision rule T(X)\n",
      "as composition L(P, T(X)). For a given rule T, we may also denote the loss\n",
      "function as LT (P ). If the class of distributions is indexed by a parameter θ,\n",
      "we may use the equivalent notation L(θ, T) or LT (θ).\n",
      "Given a loss function L(P, a), the loss function associated with a given\n",
      "randomized decision rule δ(X, A) is\n",
      "L(P, δ(X, A)) = Eδ(X,·)(L(P, Y ))\n",
      "where Y is a random variable corresponding to the probability measure δ(X, ·).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "261\n",
      "If P indexed by θ, we can write the value of the function at a given value\n",
      "θ for the action a as L(θ, a).\n",
      "The loss function is deﬁned with respect to the objectives of the statistical\n",
      "inference in such a way that a small loss is desired.\n",
      "Depending on Θ, A, and our objectives, the loss function often is a function\n",
      "only of a −g(θ) or of a/g(θ), where if a and g(θ) are vectors, a/g(θ) may\n",
      "represent element-wise division or some other appropriate operation. We may\n",
      "have, for example,\n",
      "L(θ, a) = Ll(g(θ) −a) = ∥g(θ) −a∥.\n",
      "In this case, which might be appropriate for estimating g(θ),\n",
      "L(θ, a) ≥0\n",
      "∀θ, a\n",
      "L(θ, a) = 0\n",
      "if a = g(θ).\n",
      "Notice that the loss function is just a mathematical function associated with a\n",
      "function g of distribution measures. There are no assumed underlying random\n",
      "variables. It does not matter what θ and a are; they are just mathematical\n",
      "variables, or placeholders, taking values in Θ and A. In this case, the loss\n",
      "function generally should be nondecreasing in ∥g(θ)−a∥. A loss function that\n",
      "is convex has nice mathematical properties. (There is some heuristic appeal\n",
      "to convexity, but we like it because of its mathematical tractability. There are\n",
      "lots of other properties of statistical procedures that are deemed interesting\n",
      "for this nonreason.)\n",
      "While the loss function may take on various forms, in a situation where we\n",
      "assume the underlying class of probability distributions is P, for any function\n",
      "L that is a loss function we will assume that for any action a, there is a subclass\n",
      "PL ⊆P of positive measure (of the appropriate type, usually Lebesgue) such\n",
      "that\n",
      "L(P, a) > 0,\n",
      "for P ∈PL.\n",
      "(3.84)\n",
      "Without this condition, the loss function would have no meaning for evaluating\n",
      "statistical procedures.\n",
      "Common Forms of Loss Functions\n",
      "In the following, for simplicity, we will assume that g(θ) and a are scalars.\n",
      "Each of these forms of loss functions can easily be extended to the vector case\n",
      "by use of an appropriate norm.\n",
      "A particularly nice loss function, which is strictly convex, is the “squared-\n",
      "error loss”:\n",
      "L2(θ, a) = ∥g(θ) −a∥2.\n",
      "(3.85)\n",
      "If g(θ) and a are scalars, the squared-error loss becomes\n",
      "L2(θ, a) = (g(θ) −a)2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "262\n",
      "3 Basic Statistical Theory\n",
      "Another loss function that is often appropriate is the “absolute-error loss”:\n",
      "L1(θ, a) = ∥g(θ) −a∥1,\n",
      "(3.86)\n",
      "which is just the absolute value of the diﬀerence if g(θ) and a are scalars.\n",
      "The absolute-error loss, which is convex but not strictly convex, is not as\n",
      "mathematically tractable as the squared-error loss.\n",
      "Sometimes, especially if g(θ) and a are scalars, it is appropriate that the\n",
      "loss function be asymmetric; that is, the cost if g(θ) > a increases more (or\n",
      "less) rapidly than if g(θ) < a. A simple generalization of the absolute-error\n",
      "loss that provides this asymmetry is\n",
      "L(θ, a) =\n",
      "\u001ac(a −g(θ))\n",
      "for a ≥g(θ)\n",
      "(1 −c)(g(θ) −a) for a < g(θ)\n",
      "(3.87)\n",
      "for 0 < c < 1.\n",
      "Another common loss function that is asymmetric is the so-called “linex”\n",
      "loss function,\n",
      "L(θ, a) = ec(g(θ)−a) −c(g(θ) −a) −1,\n",
      "(3.88)\n",
      "for scalars g(θ) and a. If c is negative than the linex loss function increases\n",
      "linearly if g(θ) > a and exponentially if g(θ) < a (hence, the name “linex”),\n",
      "and just the opposite if c is positive.\n",
      "When the action space is binary, that is, A = {0, 1}, a reasonable loss\n",
      "function may be the 0-1 loss function\n",
      "L0−1(θ, a) = 0\n",
      "if g(θ) = a\n",
      "L0−1(θ, a) = 1\n",
      "otherwise.\n",
      "(3.89)\n",
      "Any strictly convex loss function over an unbounded interval is unbounded.\n",
      "Even when the action space is dense, it is not always realistic to use an un-\n",
      "bounded loss function. In such a case we may use the 0-1 loss function deﬁned\n",
      "as\n",
      "L0−1(θ, a) = 0\n",
      "if |g(θ) −a| ≤α(n)\n",
      "L0−1(θ, a) = 1\n",
      "otherwise.\n",
      "(3.90)\n",
      "In a binary decision problem in which the true state is also 0 or 1, we often\n",
      "formulate a loss function of the form\n",
      "L(θ, a) =\n",
      "\u001aca for 0 true\n",
      "ba for 1 true\n",
      "(3.91)\n",
      "where c1 > c0 and b0 > b1.\n",
      "It is common to choose c0 = b1 = 0, b0 = 1, and c1 = γ > 0. In this case,\n",
      "this is called a 0-1-γ loss function.\n",
      "Another approach to account for all possibilities in the binary case, and\n",
      "to penalize errors diﬀerently depending on the true state and the decision is\n",
      "to deﬁne a weighted 0-1 loss function:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "263\n",
      "L(θ, a) =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "if a = 0 and 0 true\n",
      "0\n",
      "if a = 1 and 1 true\n",
      "α0 if a = 1 and 0 true\n",
      "α1 if a = 0 and 1 true.\n",
      "(3.92)\n",
      "This is sometimes called a α0-α1 loss or a weighted 0-1 loss.\n",
      "Risk Function\n",
      "To choose an action rule T so as to minimize the loss function is not a well-\n",
      "deﬁned problem. The action itself depends on the random observations, so\n",
      "the action is T(X), which is a random variable.\n",
      "We can make the problem somewhat more precise by considering the ex-\n",
      "pected loss based on the action T(X), which we deﬁne to be the risk:\n",
      "R(P, T) = E\n",
      "\u0000L(P, T(X))\n",
      "\u0001\n",
      ".\n",
      "(3.93)\n",
      "We also often write the risk R(P, T) as RT (P ).\n",
      "The expectation that deﬁnes the risk is taken with respect to the distri-\n",
      "bution P , the “true”, but unknown distribution; thus, the risk is a function\n",
      "of the distribution, both because the loss is a function of the distribution and\n",
      "because the expectation is taken with respect to the distribution.\n",
      "If the family of distributions are indexed by a parameter θ, then the risk\n",
      "is a function of that parameter, and we may write R(θ, T).\n",
      "Optimal Decision Rules\n",
      "We compare decision rules based on their risk with respect to a given loss\n",
      "function and a given family of distributions. If a decision rule T ∗has the\n",
      "property\n",
      "R(P, T ∗) ≤R(P, T)\n",
      "∀P ∈P,\n",
      "(3.94)\n",
      "for all T, then T ∗is called an optimal decision rule.\n",
      "Often we limit the set of possible rules. If\n",
      "R(P, T ∗) ≤R(P, T)\n",
      "∀P ∈P and ∀T ∈T ,\n",
      "(3.95)\n",
      "then T ∗is called a T -optimal decision rule.\n",
      "For the case of a convex loss function, when a suﬃcient statistic exists an\n",
      "optimal decision rule depends on that suﬃcient statistic. This fact derives from\n",
      "Jensen’s inequality (B.13) on page 849, and is codiﬁed in the Rao-Blackwell\n",
      "theorem:\n",
      "Theorem 3.8 (Rao-Blackwell theorem)\n",
      "Suppose the loss function is convex. Let T be a suﬃcient statistic for P ∈P,\n",
      "and let T0 be a statistic with ﬁnite risk. Let\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "264\n",
      "3 Basic Statistical Theory\n",
      "TRB = E(T0|T).\n",
      "Then\n",
      "R(P, TRB) ≤R(P, T0)\n",
      "∀P ∈P.\n",
      "The statistic TRB is called a “Rao-Blackwellized” version of T0.\n",
      "Admissibility\n",
      "Before considering speciﬁc deﬁnitions of minimum-risk procedures, we deﬁne\n",
      "another general desirable property for a decision rule, namely, admissibility.\n",
      "We deﬁne admissibility negatively in terms of dominating rules.\n",
      "Deﬁnition 3.12 (dominating rules)\n",
      "Given decision rules T and T ∗for a family of distributions P, with a risk\n",
      "function R. The rule T is said to dominate the rule T ∗iﬀ\n",
      "R(P, T) ≤R(P, T ∗)\n",
      "∀P ∈P,\n",
      "(3.96)\n",
      "and\n",
      "R(P, T) < R(P, T ∗)\n",
      "for some P ∈P.\n",
      "(3.97)\n",
      "Deﬁnition 3.13 (admissible rules) A decision rule T ∗is admissible if\n",
      "there does not exist a decision rule T that dominates T ∗.\n",
      "Note that admissibility depends on\n",
      "•\n",
      "the loss function L\n",
      "•\n",
      "P, the family of distributions wrt which E is deﬁned\n",
      "For a given problem there may be no admissible decision rule.\n",
      "The fact that a decision rule T ∗is admissible does not mean that the risk\n",
      "curve of some other decision rule cannot dip below the risk curve of T ∗at\n",
      "some points.\n",
      "Often we limit the set of possible rules to a set T , and we have T -\n",
      "admissibility:\n",
      "A decision rule T ∗is T -admissible if there does not exist a decision rule within\n",
      "the class of decision rules T that dominates T ∗.\n",
      "A slightly more general form of admissibility is λ-admissibility:\n",
      "A decision rule T ∗is λ-admissible if T ∗is admissible almost everywhere with\n",
      "respect to the measure λ deﬁned over the sample space.\n",
      "Optimality of a decision rule under whatever criterion implies admissibility\n",
      "of the rule under that criterion.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "265\n",
      "Completeness of a Class of Decision Rules\n",
      "We have deﬁned completeness of distributions (on page 162) and of statistics.\n",
      "We now deﬁne completeness of a class of decision rules. A class of decision\n",
      "rules T is said to be complete if for any decision rule T /∈T , there exists a\n",
      "rule in T that dominates T. A class is said to be minimal complete if it does\n",
      "not contain a complete proper subclass.\n",
      "If two decision rules have identical risk functions, we would like to think\n",
      "of them as equivalent, but we do not want necessarily to include all such\n",
      "equivalent rules in a class of interest. We therefore deﬁne a class of rules T\n",
      "to be essentially complete if for any rule T there is a rule T0 ∈T such that\n",
      "R(P, T0) ≤R(P, T) ∀P .\n",
      "Let T be a class of decision rules and let T0 ⊆T . The class T0 is said to\n",
      "be T -complete if ∀T ∈T −T0, ∃T0 ∈T0 that dominates T.\n",
      "The class T0 is said to be T -minimal complete if T0 is T -complete and no\n",
      "proper subset of T0 is T -complete.\n",
      "It is easy to see (using the method of proving one set is equal to another\n",
      "by showing each is a subset of the other) that if a T -minimal complete class\n",
      "exists, it is identical to the class of T -admissible decision rule.\n",
      "One of the most fundamental approaches to statistical inference is to iden-\n",
      "tify a complete class of decision rules and then to seek rules within that class\n",
      "that have various desirable properties. One of the most widely-used complete\n",
      "class theorem is the one that states that Bayes rules and simple generalizations\n",
      "of them constitute a complete class (see page 353).\n",
      "***** Wolfowitz (1951) ϵ-complete classes of decision functions\n",
      "L-Unbiasedness\n",
      "Admissibility involves the relationship between the expected values of the loss\n",
      "function with diﬀerent decision rules at the same distribution in the family\n",
      "being considered. We can also consider the expected values taken at a given\n",
      "point in the distribution space of the loss function of a given decision rule\n",
      "at the given value of the parameter compared with the loss at some other\n",
      "distribution. This leads to the concept of L-unbiasedness.\n",
      "A decision rule T is L-unbiased for a given loss function L if for all P and\n",
      "˜P,\n",
      "EP\n",
      "\u0000L(P, T(X))\n",
      "\u0001\n",
      "≤EP\n",
      "\u0000L( ˜P , T(X))\n",
      "\u0001\n",
      ".\n",
      "(3.98)\n",
      "The expression on the left of equation (3.98) is the risk of T for given L,\n",
      "but the expression on the right is not a risk. Notice that L-unbiasedness\n",
      "relates to the same rule evaluated under the same expectation at diﬀerent\n",
      "points in the space of distributions. Admissibility, on the other hand, relates\n",
      "to diﬀerent rules evaluated at the same point in the space of distributions\n",
      "as the distribution used in the expectation operation. A decision rule may\n",
      "be L-unbiased but not admissible; in the N(µ, 1) distribution, for example,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "266\n",
      "3 Basic Statistical Theory\n",
      "the sample median is L-unbiased under a squared-error loss, but it is not\n",
      "admissible under that loss, while the sample mean is L-unbiased under an\n",
      "absolute-error loss, but it is not admissible.\n",
      "This is the basis for deﬁning unbiasedness for statistical tests and conﬁ-\n",
      "dence sets.\n",
      "Unbiasedness for estimators has a simple deﬁnition. For squared-error loss\n",
      "for estimating g(θ), if T is L-unbiased, then, and only then, T is an unbi-\n",
      "ased estimator of g(θ). Of course, in this case, the loss function need not be\n",
      "considered and the requirement is just Eθ(T(X)) = g(θ).\n",
      "L-Invariance\n",
      "On page 221 we referred to equivariant estimators in parametric transforma-\n",
      "tion group families (see Section 2.6). We mentioned that associated with the\n",
      "group G of transformations of the random variable is a group, G, of transfor-\n",
      "mations of the parameter and a group of transformations on the estimator,\n",
      "G∗.\n",
      "In a decision-theoretic approach, the relevance of equivariance depends\n",
      "not only on the family of distributions, but also on the equivariance of the\n",
      "loss function. In the loss function L(P, T(X)), the ﬁrst argument under a\n",
      "transformation can be thought of as a map PX →Pg(X) or equivalently as a\n",
      "map Pθ →P¯g(θ). The statistical decision procedure T(X) is L-invariant for\n",
      "a given loss function L if for each g ∈G, there exists a unique g∗∈G∗, such\n",
      "that\n",
      "L(PX, T(X)) = L(Pg(X), g∗(T(X))),\n",
      "(3.99)\n",
      "or equivalently for each ¯g ∈G,\n",
      "L(Pθ, T(X)) = L(P¯g(θ), g∗(T(X))).\n",
      "(3.100)\n",
      "The g∗in these expressions is the same as in equation (3.22). We will of-\n",
      "ten require that statistical procedures be equivariant, in the sense that the\n",
      "quantities involved (the estimators, the conﬁdence sets, and so on) change in\n",
      "a manner that is consistent with changes in the parametrization. The main\n",
      "point of this requirement, however, is to ensure L-invariance, that is, invari-\n",
      "ance of the loss. We will discuss equivariance of statistical procedures in more\n",
      "detail in Section 3.4.\n",
      "Uniformly Minimizing the Risk\n",
      "All discussions of statistical inference are in the context of some family of\n",
      "distributions, and when we speak of a “uniform” property, we mean a property\n",
      "that holds for all members of the family.\n",
      "If we have the problem of estimating g(θ) under some given loss function\n",
      "L, it is often the case that for some speciﬁc value of θ, say θ1, one particular\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "267\n",
      "estimator, say T1, has the smallest expected loss, while for another value of\n",
      "θ, say θ2, another estimator, say T2, has a smaller expected loss. Neither T1\n",
      "nor T2 is uniformly optimal.\n",
      "The risk is a function of the parameter being estimated; therefore, to\n",
      "minimize the risk is not a well-posed problem. A solution is to seek a decision\n",
      "rule that is uniformly best within some restricted class of decision rules.\n",
      "3.3.2 Approaches to Minimizing the Risk\n",
      "We use the principle of minimum risk in the following restricted ways. In all\n",
      "cases, the approaches depend, among other things, on a given loss function.\n",
      "•\n",
      "If there is a suﬃcient statistic and if the loss function is convex, we use\n",
      "the result of the Rao-Blackwell theorem; that is, we condition any given\n",
      "statistic T0 on the suﬃcient statistic, T:\n",
      "TRB = E(T0|T).\n",
      "Finding a statistic with a smaller risk by this method is called “Rao-\n",
      "Blackwellization”.\n",
      "Note that If the loss function is strictly convex and T0 is not a function of\n",
      "T, then T0 is inadmissible.\n",
      "•\n",
      "We may ﬁrst place a restriction on the statistical procedure and then\n",
      "minimize risk subject to that restriction.\n",
      "For example, in estimation problems:\n",
      "–\n",
      "require unbiasedness\n",
      "In this case, we can often eliminate θ from consideration; that is, we\n",
      "can uniformly minimize the risk.\n",
      "In a common situation we deﬁne loss as squared-error (because it is\n",
      "unbiased, this means variance), and this yields UMVU.\n",
      "Suﬃciency and completeness play a major role in UMVUE.\n",
      "The information inequality is important in unbiased estimation.\n",
      "This approach is great for exponential families.\n",
      "–\n",
      "require equivariance\n",
      "This must be made more precise (unlike unbiasedness, “equivariance”\n",
      "requires more qualiﬁcation).\n",
      "Equivariance implies independence of the risk from θ; we can uniformly\n",
      "minimize the risk by just minimizing it anywhere.\n",
      "This yields UMRE, or just MRE because uniformity is implied.\n",
      "This approach is especially useful for group families.\n",
      "•\n",
      "We may minimize some global property of the risk (“global” over the values\n",
      "of θ).\n",
      "For example:\n",
      "–\n",
      "minimize maximum risk\n",
      "The risk may be unbounded, so obviously in that case, it does not\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "268\n",
      "3 Basic Statistical Theory\n",
      "make sense to attempt to minimize the maximum risk. Even if the risk\n",
      "is unbounded, the maximum risk may not exist, so we consider\n",
      "sup\n",
      "θ∈Θ\n",
      "R(θ, T(X)).\n",
      "(3.101)\n",
      "The estimator that yields\n",
      "inf\n",
      "T sup\n",
      "θ∈Θ\n",
      "R(θ, T(X))\n",
      "(3.102)\n",
      "is the minimax estimator.\n",
      "A comment about the supremum may be in order here. We mentioned\n",
      "earlier that in parametric inference, we often consider the closure of the\n",
      "parameter space, Θ, and in the maximum likelihood estimator in equa-\n",
      "tion (3.49), for example, that allowed us to consider max{θ ∈Θ}. We\n",
      "cannot do this in considering the “maximum” risk in equation (3.101)\n",
      "because we do not know how R behaves over Θ. (It could be discon-\n",
      "tinuous anywhere within Θ.)\n",
      "–\n",
      "minimize “average” risk\n",
      "How to average? Let Λ(θ) be such that R\n",
      "Θ dΛ(θ) = 1, then average risk\n",
      "is\n",
      "R\n",
      "Θ R(θ, T)dΛ(θ).\n",
      "The estimator that minimizes the average risk wrt Λ(θ), TΛ, is called\n",
      "the Bayes estimator, and the minimum risk,\n",
      "R\n",
      "Θ R(θ, TΛ)dΛ(θ), is called\n",
      "the Bayes risk.\n",
      "The averaging function allows various interpretations, and it allows the\n",
      "ﬂexibility of incorporating prior knowledge or beliefs. The regions over\n",
      "which Λ(θ) is large will be given more weight; therefore the estimator\n",
      "will be pulled toward those regions.\n",
      "In formal Bayes procedures we follow the approach indicated in equa-\n",
      "tions (3.3) and (3.4). The distribution Q0 in equation (3.3) has the\n",
      "PDF dΛ(θ), which we call the prior probability density for θ.\n",
      "We then form the joint distribution of θ and X, and then the con-\n",
      "ditional distribution of θ given X, which is the distribution QH in\n",
      "equation (3.3) and is called the posterior distribution. The Bayes esti-\n",
      "mator is determined by minimizing the risk, where the expectation is\n",
      "taken with respect to the posterior distribution.\n",
      "Because the Bayes estimator is determined by the posterior distribu-\n",
      "tion, the Bayes estimator must be a function of a suﬃcient statistic.\n",
      "We will discuss Bayesian inference more fully in Chapter 4.\n",
      "–\n",
      "combinations of global criteria\n",
      "We could consider various combinations of the global criteria. For ex-\n",
      "ample, we may see an estimator that generally minimizes the average\n",
      "risk, but such that its maximum risk is not so large. An intuitively\n",
      "reasonable bound on the maximum risk would bs some excess of the\n",
      "minimum maximum bound. This approach is called restricted Bayes,\n",
      "and results in the following constrained optimization problem:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "269\n",
      "minT\n",
      "R R(θ, T)dΛ(θ)\n",
      "s.t. supθ∈Θ R(θ, T(X)) ≤(M + ϵ) infT supθ∈Θ R(θ, T(X))\n",
      "•\n",
      "We may combine various criteria.\n",
      "It is often appropriate to combine criteria or to modify them. This often\n",
      "results in “better” estimators. For example, if for θ ∈Θ, g(θ) ∈[γ1, γ2],\n",
      "and T(X) is an estimator of g(θ) such that Pr(T(X) /∈[γ1, γ2]) ̸= 0, then\n",
      "T ∗(X) deﬁned as\n",
      "T ∗(X) =\n",
      "\n",
      "\n",
      "\n",
      "T(X) if T(X) ∈[γ1, γ2]\n",
      "γ1\n",
      "if T(X) < γ1\n",
      "γ2\n",
      "if T(X) > γ2\n",
      "dominates T(X).\n",
      "•\n",
      "We may focus on asymptotic criteria.\n",
      "Sometimes we seek estimators that have good asymptotic properties, such\n",
      "as consistency.\n",
      "Optimal Point Estimation under Squared-Error Loss\n",
      "In estimation problems, squared-error loss functions are often the most logical\n",
      "(despite the examples above!). A squared-error loss function is strictly convex,\n",
      "so the useful properties of convex loss functions, such as those relating to\n",
      "the use of suﬃcient statistics (Rao-Blackwell, for example), hold for squared-\n",
      "error loss functions. Squared-error is of course the loss function in UMVU\n",
      "estimation, and so we use it often.\n",
      "Example 3.17 UMVUE of binomial parameter\n",
      "Consider the binomial family of distributions with ﬁxed n and parameter π.\n",
      "Consider the estimator T(X) = X/n for π. We see that E(T(X)) = π: hence\n",
      "T is unbiased, and therefore under squared-error loss, the risk is the variance,\n",
      "which is pi(1 −π)/n. The binomial is a Fisher information regular family, and\n",
      "from equation (3.39), we see that the CRLB is I(π)−1 = pi(1 −π)/n; hence\n",
      "T is a UMVUE.\n",
      "Squared-error loss functions yield nice properties for linear functions of\n",
      "estimands. If T is an estimator of g(θ), then an obvious estimator of ag(θ)+b\n",
      "is aT + b. Under squared-error loss, we have the properties stated in the\n",
      "following theorem.\n",
      "Theorem 3.9 (linearity of optimal estimators under squared-error loss)\n",
      "If T is\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bayes\n",
      "UMVU\n",
      "minimax\n",
      "admissible\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for g(θ), then aT + b is\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bayes\n",
      "UMVU\n",
      "minimax\n",
      "admissible\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for ag(θ) + b,\n",
      "where all properties are taken under squared-error loss.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "270\n",
      "3 Basic Statistical Theory\n",
      "The various pieces of this theorem will be considered in other places where\n",
      "the particular type of estimation is discussed.\n",
      "If in a Bayesian setup, the prior distribution and the posterior distribution\n",
      "are in the same parametric family, that is, if Q in equations (3.3) and (3.4)\n",
      "represents a single parametric family, then a squared-error loss yield Bayes\n",
      "estimators for E(X) that are linear in X. (If a prior distribution on the pa-\n",
      "rameters together with a conditional distribution of the observables yield a\n",
      "posterior in the same parametric family as the prior, the prior is said to be\n",
      "conjugate with respect to the conditional distribution of the observables. We\n",
      "will consider various types of priors more fully in Chapter 4.)\n",
      "Because we use squared-error loss functions so often, we must be careful\n",
      "not to assume certain common properties hold. Other types of loss functions\n",
      "can provide useful counterexamples.\n",
      "3.3.3 Admissibility\n",
      "By Deﬁnition 3.13, a decision δ∗is admissible if there does not exist a decision\n",
      "δ that dominates δ∗. Because this deﬁnition is given as a negative condition, it\n",
      "is often easier to show that a rule is inadmissible, because all that is required\n",
      "to do that is to exhibit another rule that dominates it. In this section we\n",
      "consider some properties of admissibility and ways of identifying admissible\n",
      "or inadmissible rules.\n",
      "Admissibility of Estimators under Squared-Error Loss\n",
      "Any property deﬁned in terms of the risk depends on the loss function. As we\n",
      "have seen above, the squared-error loss often results in estimators that have\n",
      "“nice” properties. Here is another one.\n",
      "Under a squared-error loss function an unbiased estimator is always at\n",
      "least as good as a biased estimator unless the bias has a negative correlation\n",
      "with the unbiased estimator.\n",
      "Theorem 3.10\n",
      "Let E(T(X)) = g(θ), and let eT(X) = T(X) + B, where Cov(T, B) ≥0. Then\n",
      "under squared-error loss, the risk of T(X) is uniformly less than the risk of\n",
      "eT(X); that is, eT(X) is inadmissible.\n",
      "Proof.\n",
      "R(g(θ), eT ) = R(g(θ), T) + V(B) + Cov(T, B)\n",
      "∀θ.\n",
      "Also under a squared-error loss function, an unbiased estimator dominates\n",
      "a biased estimator unless the bias is a function of the parameter.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "271\n",
      "Theorem 3.11\n",
      "Let E(T(X)) = g(θ), and let eT(X) = T(X) + B, where B ̸= 0 a.s. and B is\n",
      "independent of θ. Then under squared-error loss, eT(X) is inadmissible.\n",
      "Proof.\n",
      "R(g(θ), eT ) = R(g(θ), T) + B2.\n",
      "Now, let us consider linear estimators of g(θ) = E(T(X))\n",
      "eT(X) = aT(X) + b\n",
      "that generalize the estimators above, except we consider a and b to be con-\n",
      "stants. We have the following results under squared-error loss.\n",
      "•\n",
      "If a = 1 and b ̸= 0, then eT(X) is inadmissible by Theorem 3.11.\n",
      "•\n",
      "If a > 1, then eT(X) is inadmissible for any b because\n",
      "R(g(θ), eT ) = a2R(g(θ), T) > R(g(θ), T).\n",
      "•\n",
      "If a < 0, then eT(X) is inadmissible for any b because\n",
      "R(g(θ), eT ) > R(g(θ), 0)\n",
      "(Exercise 3.9).\n",
      "Admissibility of Estimators in One-Parameter Exponential\n",
      "Families\n",
      "In the previous section, we identiﬁed conditions that assured the inadmissibil-\n",
      "ity of linear estimators, and later we will see some examples in which we easily\n",
      "establish inadmissibility. It is of course a more interesting problem to identify\n",
      "conditions that assure admissibility. Eﬀorts to do this are much less success-\n",
      "ful, but we do have a useful result for linear estimators in a one-parameter\n",
      "exponential family with PDF as given in equation (2.15),\n",
      "f(x) = β(θ)eθT (x).\n",
      "(3.103)\n",
      "Karlin’s theorem MS2 Theorem 4.14\n",
      "use information inequality (3.39).\n",
      "*** in binomial, show that X is admissible **** example\n",
      "Admissible and Bayes Estimators\n",
      "There are important and useful connections between admissible estimators\n",
      "and Bayes estimators.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "272\n",
      "3 Basic Statistical Theory\n",
      "•\n",
      "A unique Bayes estimator is admissible with respect to the same loss func-\n",
      "tion and distribution.\n",
      "•\n",
      "An admissible estimator is either Bayes or limiting Bayes with respect to\n",
      "the same loss function and distribution.\n",
      "We will consider these properties in Chapter 4. It is sometimes easy to con-\n",
      "struct a Bayes estimator, and having done so, if the estimator is unique, we\n",
      "immediately have an admissible estimator.\n",
      "Inadmissible Estimators\n",
      "Some estimators that have generally good properties or that are of a standard\n",
      "type may not be admissible. Heuristic methods such as MLE or the method of\n",
      "moments are not developed in the context of decision theory, so it should not\n",
      "be surprising that estimators based ont these methods may not be admissible.\n",
      "Example 3.18 Inadmissible Method of Moments Estimator\n",
      "Consider the case of estimating θ in the ﬁnite population {1, . . ., θ}. Suppose\n",
      "we sample from this population with replacement, obtaining X1, . . ., Xn. Be-\n",
      "cause E(X) = (θ+1)/2, the method of moments estimator of θ is T = 2X −1.\n",
      "This estimator is inadmissible (for any reasonable loss function including\n",
      "squared-error loss), since T ∗= max(X(n), T) is always at least as close to\n",
      "θ, and can be closer.\n",
      "The method of moments estimator in this case is not even a function of\n",
      "a suﬃcient statistic, so we would not expect it to have good properties. Note\n",
      "also that the MME of θ may produce a value that could never be the true\n",
      "value of θ. (Of course, that is also the case with T ∗.)\n",
      "There are many surprising cases of inadmissibility, as we see in the follow-\n",
      "ing examples. We show that a given rule is not admissible by exhibiting a rule\n",
      "that dominates it. It is important to recognize, of course, that the dominating\n",
      "rule may also not be admissible either.\n",
      "It may be possible to construct a randomized estimator that shows that\n",
      "a given estimator is not admissible. Another way is to form a scalar multiple\n",
      "of a “good” estimator, and show that it dominates the “good” estimator. In\n",
      "Example 3.19 the scaling is a function of the statistic, and in Example 3.20\n",
      "the scaling is a constant.\n",
      "Example 3.19 Inadmissible Estimator of the Mean in a Multivari-\n",
      "ate Normal Distribution\n",
      "The estimation of the mean of a normal distribution has interesting admissi-\n",
      "bility properties. It is relatively straightforward to show that X is admissible\n",
      "for estimating θ in N(θ, 1) under squared-error loss. It can also be shown that\n",
      "X is admissible for estimating θ in N2(θ, I2), and of course, in the simpler\n",
      "case of n = 1, X is admissible for estimating θ.\n",
      "However, for r > 2, X is not admissible for estimating θ in Nr(θ, Ir)!\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "273\n",
      "For r > 2, the estimator\n",
      "ˆθJ =\n",
      "\u0012\n",
      "1 −c r −2\n",
      "∥X∥2\n",
      "\u0013\n",
      "X\n",
      "(3.104)\n",
      "though biased, dominates X. This is called the James-Stein estimator.\n",
      "Why this is the case for r > 2 has to do with the existence of\n",
      "E\n",
      "\u0012\n",
      "1\n",
      "∥X∥2\n",
      "\u0013\n",
      ";\n",
      "see page 188.\n",
      "Further comments on Example 3.19\n",
      "The James-Stein estimator is generally shrunk toward 0. This type of adjust-\n",
      "ment is called Stein shrinkage. Choice of c allows for diﬀerent amounts of bias\n",
      "and diﬀerent amounts of reduction in the risk. The regularization parameter\n",
      "in ridge regression is similar to the c in this expression; see Example 5.27.\n",
      "The fact that shrinkage in the case of the multivariate normal distribution\n",
      "may improve the estimator is related to the outlyingness of data in higher\n",
      "dimensions.\n",
      "A shrunken estimator is biased, and ordinarily we would not expect a\n",
      "biased estimator to dominate a “good” unbiased one. It should be noted,\n",
      "however, that the bias of the shrunken estimator has a negative correlation\n",
      "with the basic estimator (recall Theorem 3.10).\n",
      "The fact that the James-Stein estimator dominates the UMVUE, however,\n",
      "does not mean that the James-Stein estimator itself is admissible. Indeed, it\n",
      "is not. The estimator\n",
      "ˆθJ+ = min\n",
      "\u0012\n",
      "1, c r −2\n",
      "∥X∥2\n",
      "\u0013\n",
      "X\n",
      "(3.105)\n",
      "dominates the James-Stein estimator under squared-error loss. This is some-\n",
      "times called the positive-part James-Stein estimator. The positive-part James-\n",
      "Stein estimator, however, is also inadmissible under squared-error loss (see\n",
      "Strawderman (1971) for further discussion).\n",
      "Consider another example, due to Lehmann, for a general one-parameter\n",
      "exponential family.\n",
      "Example 3.20 Inadmissible Estimator in a One-Parameter Expo-\n",
      "nential Family\n",
      "Let X have the density\n",
      "pθ(x) = β(θ)eθxe−|x|,\n",
      "where θ ∈]−1, 1[ and β(θ) = 1−θ2 (so it integrates to 1). Consider a sample of\n",
      "size one, X, and the problem of estimating g(θ) = Eθ(X) with squared-error\n",
      "loss. Now,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "274\n",
      "3 Basic Statistical Theory\n",
      "Eθ(X) = −β′(θ)\n",
      "β(θ)\n",
      "=\n",
      "2θ\n",
      "1 −θ2 ,\n",
      "and\n",
      "Vθ(X) = d\n",
      "dθ Eθ(X)\n",
      "= 2 1 + θ2\n",
      "(1 −θ2)2 ;\n",
      "hence, the risk is\n",
      "R(g(θ), X) = 2 1 + θ2\n",
      "(1 −θ2)2 .\n",
      "Now, consider the estimator Ta = aX. Its risk under squared-error is\n",
      "R(θ, Ta) = Eθ(L(θ, Ta))\n",
      "= Eθ((g(θ) −Ta)2)\n",
      "= 2a2 1 + θ2\n",
      "(1 −θ2)2 + 4(1 −a2)\n",
      "θ2\n",
      "(1 −θ2)2 .\n",
      "If a = 0, that is, the estimator is the constant 0, the risk is 4θ2/(1 −θ2)2,\n",
      "which is smaller than the risk for X for all θ ∈] −1, 1[.\n",
      "The natural suﬃcient statistic in this one-parameter exponential family is\n",
      "inadmissible for its expectation!\n",
      "Other Forms of Admissibility\n",
      "We have deﬁned admissibility in terms of a speciﬁc optimality criterion,\n",
      "namely minimum risk. Of course, the risk depends on the loss function, so\n",
      "admissibility depends on the particular loss function.\n",
      "Although this meaning of admissibility, which requires a decision-theory\n",
      "framework, is by far the most common meaning, we can deﬁne admissibility\n",
      "in a similar fashion with respect to any optimality criterion; for example,\n",
      "the estimator T(X) is Pitman-admissible for g(θ) if there does not exist an\n",
      "estimator that is Pitman-closer to g(θ). In Example 3.3 on page 220 we saw\n",
      "that the sample mean even in a univariate normal distribution is not Pitman\n",
      "admissible. The type of estimator used in that example to show that the\n",
      "univariate mean is not Pitman admissible is a shrinkage estimator, just as a\n",
      "shrinkage estimator was used in Example 3.19.\n",
      "3.3.4 Minimaxity\n",
      "Instead of uniform optimality properties for decisions restricted to be unbiased\n",
      "or equivariant or optimal average properties, we may just seek to ﬁnd one with\n",
      "the smallest maximum risk. This is minimax estimation.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "275\n",
      "For a given decision problem, the maximum risk may not exist, so we\n",
      "consider\n",
      "sup\n",
      "θ∈Ω\n",
      "R(θ, δ(X)).\n",
      "The decision that yields\n",
      "inf\n",
      "δ sup\n",
      "θ∈Ω\n",
      "R(θ, δ(X))\n",
      "(3.106)\n",
      "is the minimax decision.\n",
      "Minimaxity, as with most optimality properties, depends on the loss func-\n",
      "tion.\n",
      "Minimaxity and Admissibility\n",
      "There are various connections between minimaxity and admissibility.\n",
      "Theorem 3.12\n",
      "An admissible estimator with a constant risk is minimax with respect to the\n",
      "same loss function and distribution.\n",
      "Proof.\n",
      "We see that this must be the case because if such an estimator were not\n",
      "minimax, then an estimator with smaller maximum risk would dominate it\n",
      "and hence it would not be admissible.\n",
      "Minimax and Bayes Estimators\n",
      "Just as with admissible estimators, there are interesting connections between\n",
      "minimax and Bayes estimators. One of the most important is the fact that a\n",
      "Bayes estimator with a constant risk is minimax with respect to the same loss\n",
      "function and distribution. (This is Theorem 4.4.) Hence, one way of ﬁnding\n",
      "a minimax estimator is to ﬁnd a Bayes estimator with constant risk. For a\n",
      "given loss function, and given distribution of the observable random variable,\n",
      "the minimax estimator is the Bayes estimator for “worst” prior distribution.\n",
      "We will consider this and other properties in Chapter 4.\n",
      "Minimax Estimators under Squared-Error Loss in Exponential\n",
      "Families\n",
      "For one-parameter exponential families, under squared-error loss, Theorem\n",
      "4.14 in MS2 provides a condition for identifying admissible estimators, and\n",
      "hence minimax estimators. The minimax estimator is not always the obvious\n",
      "one.\n",
      "Often a randomized estimator can be constructed so that it is minimax.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "276\n",
      "3 Basic Statistical Theory\n",
      "Example 3.21 Risk functions for estimators of the parameter in a\n",
      "binomial distribution\n",
      "Suppose we have an observation X from a binomial distribution with param-\n",
      "eters n and π. The PDF (wrt the counting measure) is\n",
      "pX(x) =\n",
      "\u0012n\n",
      "x\n",
      "\u0013\n",
      "πx(1 −π)n−xI{0,1,...,n}(x).\n",
      "We wish to estimate π.\n",
      "The MLE of π is\n",
      "T(X) = X/n.\n",
      "(3.107)\n",
      "We also see that this is an unbiased estimator. Under squared-error loss, the\n",
      "risk is\n",
      "RT (π) = E\n",
      "\u0000(X/n −π)2\u0001\n",
      "= π(1 −π)/n,\n",
      "which, of course, is just variance.\n",
      "Let us consider a randomized estimator, for some 0 ≤α ̸= 1,\n",
      "δα(X) =\n",
      "\u001aT\n",
      "with probability 1 −α\n",
      "1/2 with probability α\n",
      "(3.108)\n",
      "This is a type of shrunken estimator. The motivation to move T toward 1/2 is\n",
      "that the maximum of the risk of T occurs at 1/2. By increasing the probability\n",
      "of selecting that value the risk at that point will be reduced, and so perhaps\n",
      "this will reduces the risk in some overall way.\n",
      "Under squared-error loss, the risk of δα(X) is\n",
      "Rδα(π) = (1 −α)E\n",
      "\u0000(X/n −π)2\u0001\n",
      "+ αE\n",
      "\u0000(1/2 −π)2\u0001\n",
      "= (1 −α)π(1 −π)/n + α(1/2 −π)2.\n",
      "The mass point has a spreading eﬀect and the risk dips smoothly The risk\n",
      "of δα(X) also has a maximum at π = 1/2, but it is (1 −α)/4n, compared to\n",
      "RT(1/2) = 1/4n.\n",
      "We see that for α = 1/(n + 1) the risk is constant with respect to π;\n",
      "therefore δ1/(n+1)(X) is a minimax estimator wrt squared-error loss.\n",
      "Risk functions are shown in Figure 3.1 for T(X) and for δ.05(X) and\n",
      "δ1/(n+1)(X). Notice that neither δα(X) nor T(X) dominates the other.\n",
      "3.3.5 Summary and Review\n",
      "We have discussed ﬁve general approaches to statistical inference, and we have\n",
      "identiﬁed certain desirable properties that a method of inference may have.\n",
      "A ﬁrst objective in mathematical statistics is to characterize optimal prop-\n",
      "erties of statistical methods. The setting for statistical inference includes the\n",
      "distribution families that are assumed a priori, the objectives of the statistical\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.3 The Decision Theory Approach to Statistical Inference\n",
      "277\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0.005\n",
      "0.010\n",
      "0.015\n",
      "0.020\n",
      "0.025\n",
      "π\n",
      "Risk\n",
      "MLE\n",
      "randomized; α = 1 (n + 1)\n",
      "randomized;\n",
      "α = 0.05\n",
      "Figure 3.1. Risk Functions for Squared-Error Loss in Binomial with n = 10 (Ex-\n",
      "ample 3.21).\n",
      "inference, and the criteria by which the statistical methods to achieve those\n",
      "objectives are to be evaluated.\n",
      "A second objective in mathematical statistics is to develop techniques for\n",
      "ﬁnding optimal methods in a particular setting. We have considered some of\n",
      "these procedures above, and they will be major recurring topics throughout\n",
      "the rest of this book.\n",
      "Nonexistence of Optimal Methods\n",
      "There are many criteria by which to evaluate a statistical method. In a given\n",
      "setting there may not be a statistical procedure that is optimal with\n",
      "respect to a given criterion.\n",
      "The criteria by which to evaluate a statistical method include basic things\n",
      "about the nature of the statistics used in the method, such as suﬃciency, min-\n",
      "imality, and completeness. These properties are independent of the objectives\n",
      "of the procedure and of the particular statistical method used. They depend\n",
      "on the assumed distribution family and the nature of the available sample.\n",
      "•\n",
      "suﬃciency. There is always a suﬃcient statistic.\n",
      "•\n",
      "minimal suﬃciency. There is always a minimal suﬃcient statistic.\n",
      "•\n",
      "completeness. There may not be a complete statistic. This depends on\n",
      "the assumed family of distributions. (See Example 3.4.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "278\n",
      "3 Basic Statistical Theory\n",
      "For a given objective in a given family of distributions maximizing the\n",
      "likelihood is a desirable property for a statistical procedure; that is, other\n",
      "things being equal, we would choose a statistical procedure that maximizes\n",
      "the likelihood.\n",
      "•\n",
      "maximum likelihood. There may not be a procedure that maximizes\n",
      "the likelihood. (See Example 6.13.)\n",
      "For a given objective there are a number of criteria for evaluating a statis-\n",
      "tical method that is to achieve that objective. Speciﬁcally, if the objective is\n",
      "estimation, we can identify three general criteria for evaluating an estimator\n",
      "•\n",
      "Pitman closeness. There may not be an estimator that is Pitman-closest.\n",
      "•\n",
      "equivariance. Equivariance depends ﬁrst of all on the assumed family\n",
      "of distributions and the group of transformations. There may not be an\n",
      "equivariant estimator in a given setting.\n",
      "•\n",
      "expected diﬀerence. There are various functions of the diﬀerence Tn(X)−\n",
      "g(θ), whose expected values may be relevant. Whether or not an expected\n",
      "value of the diﬀerence can be minimized depends on the family of distri-\n",
      "butions, the nature of Tn (and on n itself), and on the nature of g(θ). For\n",
      "example in a Cauchy distribution, for many functions Tn, E(Tn(X)) does\n",
      "not exist. In this particular case, E(Tn(X)) may exist for n greater than\n",
      "some value k, but not exist for n < k.\n",
      "–\n",
      "bias; E(T(X) −g(θ)). An unbiased estimator would seem to be desir-\n",
      "able. For a given family of distributions and a given g(θ), there may\n",
      "not be an unbiased estimator. (See Examples 5.2 and 5.3.)\n",
      "–\n",
      "mean absolute error; E(|T(X) −g(θ)|). For a given family of dis-\n",
      "tributions and a given g(θ), E(|T(X) −g(θ)|) may not exist, but if it\n",
      "does there may be no estimator Tn that minimizes it for all θ.\n",
      "–\n",
      "mean squared error; E((T(X) −g(θ))2). For a given family of dis-\n",
      "tributions and a given g(θ), E((T(X) −g(θ))2) may not exist, but if it\n",
      "does there may be no estimator Tn that minimizes it for all θ.\n",
      "–\n",
      "conditional expectation; for example, E((T(X)−g(θ))2|E(T(X)−\n",
      "g(θ)) = 0). For a given family of distributions and a given g(θ), the\n",
      "conditional distribution may not exist, but if it does there may be no\n",
      "estimator Tn that minimizes the conditional expectation for all θ.\n",
      "In a decision theory approach, the criteria for evaluating a statistical\n",
      "method revolve around the loss function, which depends on the objectives\n",
      "of the procedure. These criteria generally involve some kind of expectation of\n",
      "the loss function, such as the risk or the expectation of the loss taken with\n",
      "respect to the posterior distribution.\n",
      "•\n",
      "minimum risk. For any nontrivial inference problem, it is generally not\n",
      "possible to minimize the risk uniformly for almost any reasonable loss\n",
      "function. (See page 266.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.4 Invariant and Equivariant Statistical Procedures\n",
      "279\n",
      "•\n",
      "minimum risk with restrictions. We often impose the restriction that\n",
      "the procedure be unbiased or be equivariant. Under either of these restric-\n",
      "tions there may not be an optimal statistical procedure. As we have already\n",
      "noted, there may not even be a procedure that satisﬁes the restriction of\n",
      "being unbiased.\n",
      "•\n",
      "minimum maximum risk. The risk may be unbounded, so no minimax\n",
      "procedure can exist.\n",
      "•\n",
      "minimum average risk. Whether or not there can be a procedure that\n",
      "minimizes the average risk clearly depends on the averaging process.\n",
      "In the frequentist approach to decision theory, admissibility is an impor-\n",
      "tant unifying concept. Having deﬁned the concept, we can limit our search for\n",
      "optimal procedures to admissible procedures if they can be identiﬁed. From\n",
      "a negative perspective, if we have a procedure that is optimal with respect\n",
      "to other criteria, we generally ask whether or not it is admissible. We may\n",
      "show that the procedure being considered is not admissible by demonstrating\n",
      "a procedure that dominates the procedure in question. Often, when we do this\n",
      "however, the dominating procedure is not admissible either.\n",
      "In the next section we consider the restriction of equivariance that we\n",
      "have referred to already. This property is relevant only in inference problems\n",
      "that can be formulated in a way that connects the underlying sample space,\n",
      "parameter space, and loss function in a special way.\n",
      "3.4 Invariant and Equivariant Statistical Procedures\n",
      "Statistical decisions or actions based on data should not be aﬀected by simple\n",
      "transformations on the data or by reordering of the data, so long as these\n",
      "changes on the data are reﬂected in the statement of the decision; that is,\n",
      "the actions should be invariant. If the action is a yes-no decision, such as\n",
      "in hypothesis testing, it should be completely invariant. If a decision is a\n",
      "point estimate, its value is not unaﬀected, but it should be equivariant, in the\n",
      "sense that it reﬂects the transformations in a meaningful way so that the loss\n",
      "function should be invariant to the transformations.\n",
      "Given a decision problem with loss function L, we seek a decision rule that\n",
      "is L-invariant (see page 266).\n",
      "In the following, we will formalize this equivariance principle by deﬁning\n",
      "appropriate classes of transformations, and then specifying rules that sta-\n",
      "tistical decision functions must satisfy. We identify “reasonable” classes of\n",
      "transformations on the sample space and the corresponding transformations\n",
      "on other components of the statistical decision problem. We will limit consid-\n",
      "eration to transformations that are one-to-one and onto.\n",
      "Development of equivariant statistical procedures is based on an algebraic\n",
      "group of transformations (a group in which the operation is composition of\n",
      "functions; see Deﬁnition 0.0.2 on page 629 and Section 0.1.11 beginning on\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "280\n",
      "3 Basic Statistical Theory\n",
      "page 754) and a suitable family of probability distributions, such as a “group\n",
      "family” of distributions. (See Section 2.6, beginning on page 178, for further\n",
      "discussion of such families of distributions.)\n",
      "3.4.1 Formulation of the Basic Problem\n",
      "We are interested in what happens under a one-to-one transformation of the\n",
      "random variable g(X); in particular, we are interested in a transformation of\n",
      "the parameter ˜g(θ) such that Pg(X)|˜g(θ) is a member of the same distributional\n",
      "family. (In this section we will consider only parametric inference; that is, we\n",
      "will consider distributions PX|θ for θ ∈Θ, but in a more general sense, we\n",
      "can just consider θ to be some index in the distributional family.) We want\n",
      "to identify optimal methods of inference for PX|θ that will remain optimal for\n",
      "Pg(X)|˜g(θ).\n",
      "Whether or not such methods exist depends on the type of transforma-\n",
      "tions, the distributions and parameters of interest, and the form of the loss\n",
      "function. In the following, we will identify the special cases that admit mini-\n",
      "mum risk equivariant procedures.\n",
      "The invariance or equivariance of interest is with respect to a given class of\n",
      "transformations. A family of distributions whose probability measures accom-\n",
      "modate a group of transformations in a natural way is called a group family.\n",
      "The most common class of transformations of interest is the group of linear\n",
      "transformations of the form ˜x = Ax + c, and the group families of interest\n",
      "have a certain invariance with respect to a group of linear transformations\n",
      "on the random variable. We call such a group family a location-scale family\n",
      "(Deﬁnition 2.3). More generally, given a distribution with parameter θ, that\n",
      "distribution together with a group of transformations on θ forms a group\n",
      "family.\n",
      "Transformations on the Sample Space, the Parameter Space, and\n",
      "the Decision Space\n",
      "Following Deﬁnition 2.4 for an invariant parametric family of distributions,\n",
      "we have two transformation groups, G, with elements\n",
      "g : X 7→X ,\n",
      "1 : 1 and onto,\n",
      "and the induced group eG, with elements\n",
      "˜g : Θ 7→Θ,\n",
      "1 : 1 and onto,\n",
      "in such a way that for given g ∈G, there is a ˜g ∈eG such that for any set A,\n",
      "Prθ(g(X) ∈A) = Pr˜g(θ)(X ∈A);\n",
      "(3.109)\n",
      "that is, ˜g preserves Θ. The group eG is transitive in the sense deﬁned on\n",
      "page 755.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.4 Invariant and Equivariant Statistical Procedures\n",
      "281\n",
      "**** we consider h *** give restrictions on h ***** Given a Borel function\n",
      "h on Θ and the general problem of making inferences about h(θ) under a\n",
      "given loss function L, if transformations g ∈G and ˜g ∈eG that preserve the\n",
      "probability model are made, we seek a statistical procedure T such that\n",
      "L(h(θ), T(X)) = L(h(˜g(θ)), T(g(X))).\n",
      "(3.110)\n",
      "For a general statistical decision function T, we seek a transformation\n",
      "g∗that yields the same (or appropriately transformed) decision within the\n",
      "transformed distribution using the transformed data. The decision function\n",
      "takes the sample space into the decision space A; that is, T : X 7→A ⊆IR.\n",
      "*** give formal deﬁnition\n",
      "*** give examples of orbits\n",
      "Invariance of the Loss Function\n",
      "For a given loss function L(θ, T(X)) with transformations g and ˜g applied to\n",
      "the observable random variable and to the parameter, we seek a transforma-\n",
      "tion g∗such that\n",
      "L(θ, T(X)) = L(˜g(θ), g∗(T(X))).\n",
      "(3.111)\n",
      "Such transformations yield invariance of the risk:\n",
      "Eθ(L(θ, T(X))) = E˜g(θ)(L(˜g(θ), g∗(T(X)))).\n",
      "(3.112)\n",
      "The question is whether or not such a transformation exists. Its existence\n",
      "clearly depends on the loss function. If equation (3.111) holds, then the trans-\n",
      "formation g∗is said to be L-invariant with respect to the loss function L.\n",
      "In most statistical decision problems, we assume a symmetry or invariance\n",
      "or equivariance of the problem before application of any of these transforma-\n",
      "tions, and the problem that results from applying a transformation. For given\n",
      "classes of transformations, we consider loss functions that admit L-invariant\n",
      "transformations; that is, we require that the transformation have the prop-\n",
      "erty of L-invariance with respect to the loss function as expressed in equa-\n",
      "tion (3.111). This means that a good statistical procedure, T, for the original\n",
      "problem is good for the transformed problem. Note that this is an assump-\n",
      "tion about the class of meaningful loss functions for this kind of statistical\n",
      "problem.\n",
      "Example 3.22 Transformations in a Bernoulli distribution\n",
      "Suppose we have a random sample of size n, X1, . . ., Xn from the Bernoulli(π)\n",
      "distribution and we wish to estimate π. In Example 3.17, we found that\n",
      "T(X) = P Xi/n is a UMVUE for π; that is, it is optimal under squared-\n",
      "error loss among the class of unbiased estimators.\n",
      "Now, consider the binomial transformation of Example 2.3. In this trans-\n",
      "formation, the values assumed by the binary random variables are reversed;\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "282\n",
      "3 Basic Statistical Theory\n",
      "that is, the random variable is transformed as g(X) = 1 −X. We see that the\n",
      "transformation ˜g(π) = 1 −π preserves the parameter space, in the sense of\n",
      "equation (3.109).\n",
      "Under this new setup, following the same approach that led to the esti-\n",
      "mator T(X), we see that T(g(X)) = T(1 −X) is an optimal estimator of\n",
      "˜g(π) = 1 −π under squared-error loss among the class of unbiased estimators.\n",
      "Hence, in this case, the squared-error loss function allowed us to develop an\n",
      "equivariant procedure.\n",
      "We note that the estimator T(g(X)) = g∗(T(X)) = 1 −T(X), and we\n",
      "have, as in equation (3.111),\n",
      "L(π, T(X)) = L(˜g(π), g∗(T(g(X)))).\n",
      "In the Bernoulli example above, loss functions of various forms would\n",
      "have allowed us to develop an equivariant procedure for estimation of the\n",
      "transformed π. This is not always the case. For some types of transformations\n",
      "g and ˜g on the sample and parameter spaces, we can develop equivariant\n",
      "procedures only if the loss function is of some particular form. For example,\n",
      "in a location family, with transformations of the form g(X) = X + c and\n",
      "˜g(µ) = µ + c, in order to develop an equivariant procedure that satisﬁes\n",
      "equation (3.111) we need a loss function that is a function only of a −g(θ).\n",
      "************\n",
      "Following the same approach as above, we see that in a univariate scale\n",
      "family, with transformations of the form g(X) = cX, in order to develop\n",
      "an equivariant procedure, we need a loss function that is a function only of\n",
      "a/g(θ). In order to develop equivariant procedures for a general location-scale\n",
      "family P(µ,Σ) we need a loss function of the form\n",
      "L((µ, Σ), a) = Lls(Σ1/2(a −µ)).\n",
      "(3.113)\n",
      "In order to achieve invariance of the loss function for a given group of\n",
      "transformations G, for each g ∈G, we need a 1:1 function g∗that maps the\n",
      "decision space onto itself, g∗: A 7→A. The set of all such g∗together with\n",
      "the induced structure is a group, G∗with elements\n",
      "g∗: A 7→A,\n",
      "1 : 1 and onto.\n",
      "The relationship between G and G∗is an isomorphism; that is, for g ∈G\n",
      "and g∗∈G∗, there is a function h such that if g∗= h(g), then h(g1 ◦g2) =\n",
      "h(g1) ◦h(g2).\n",
      "Invariance of Statistical Procedures\n",
      "To study invariance of statistical procedures we will now identify three groups\n",
      "of transformations G, eG, and G∗, and the relationships among the groups. This\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.4 Invariant and Equivariant Statistical Procedures\n",
      "283\n",
      "notation is widely used in mathematical statistics, maybe with some slight\n",
      "modiﬁcations.\n",
      "Let G be a group of transformations that map the probability space onto\n",
      "itself. We write\n",
      "g(X) = ˜X.\n",
      "(3.114)\n",
      "Note that X and ˜X are random variables, so the domain and the range of\n",
      "the mapping are subsets of probability spaces; the random variables are based\n",
      "on the same underlying measure, so the probability spaces are the same; the\n",
      "transformation is a member of a transformation group, so the domain and the\n",
      "range are equal and the transformations are one-to-one.\n",
      "For a given statistical procedure T that yields the action a for an obser-\n",
      "vation X, we have under the various transformations\n",
      "g∗(T(g−1( ˜X))) = g∗(T(X))\n",
      "(3.115)\n",
      "= g∗(a)\n",
      "(3.116)\n",
      "= ˜a.\n",
      "(3.117)\n",
      "We are interested in a probability space, (Ω, F, PΘ), that is invariant to\n",
      "a class of transformations G; that is, one in which PΘ is a group family with\n",
      "respect to G. The induced groups G and G∗determine the transformations to\n",
      "be applied to the parameter space and the action space.\n",
      "The basic idea underlying invariance of statistical procedures naturally is\n",
      "invariance of the risk under the given transformations.\n",
      "We seek a statistical procedure T(x) that is an invariant function under\n",
      "the transformations.\n",
      "Because if there is a maximal invariant function m (see Deﬁnition 0.1.53 on\n",
      "page 755) all invariant functions are dependent on m, our search for optimal\n",
      "invariant procedures can use m. The concept of maximal invariance is simi-\n",
      "lar to the concept of suﬃciency. A suﬃcient statistic may reduce the sample\n",
      "space; a maximal invariant statistic may reduce the parameter space. (Max-\n",
      "imal invariant statistics have some technical issues regarding measurability,\n",
      "however; X being measurable does not guarantee m(X) is measurable under\n",
      "the same measure.)\n",
      "A probability model may be deﬁned in diﬀerent ways. There may be an\n",
      "equivalence between two diﬀerent models that is essentially a result of a\n",
      "reparametrization: ˜θ = ˜g(θ). A random variable in the one model may be\n",
      "a function of the random variable in the other model: e\n",
      "X = g(X). There are\n",
      "two ways of thinking of estimation under a reparametrization, both in the\n",
      "context of an estimator T(X) of h(θ), and with the transformations deﬁned\n",
      "above:\n",
      "•\n",
      "functional, g∗(T(X)) estimates g∗(h(θ));\n",
      "•\n",
      "formal, T(g(X)) estimates g∗(h(θ)).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "284\n",
      "3 Basic Statistical Theory\n",
      "Functional equivariance is trivial. This is the equivariance we expect under\n",
      "a simple change of units, for example. If X is a random variable that mod-\n",
      "els physical temperatures in some application, it should not make any real\n",
      "diﬀerence whether the temperatures are always measured in degrees Celsius\n",
      "or degrees Fahrenheit. The random variable itself does not include units, of\n",
      "course (it is a real number). If the measurements are made in degrees Celsius\n",
      "at a time when X is the random variable used to model the distribution of\n",
      "the data and the estimator T(X) and the estimand h(θ) relates to X in a\n",
      "linear fashion (if h(θ) is the mean of X, for example), and later in a simi-\n",
      "lar application the measurements are made in degrees Fahrenheit, applying\n",
      "g∗(t) = 9t/5 + 32 to both T(X) and h(θ) preserves the interpretation of the\n",
      "model.\n",
      "Formal equivariance, however, is not meaningful unless the problem itself\n",
      "has fundamentally symmetric properties; the family of probability distribu-\n",
      "tions is closed under some group of transformations on the sample space one\n",
      "on the parameter space. In this case, we need a corresponding transformation\n",
      "on the decision space. The statistical procedure is equivariant if the functional\n",
      "equivariance is the same as the formal equivariance; that is,\n",
      "T(g(X)) = g∗(T(X)).\n",
      "(3.118)\n",
      "3.4.2 Optimal Equivariant Statistical Procedures\n",
      "In the decision-theoretic approach to statistical inference, we generally seek\n",
      "procedures that have minimum risk with respect to a given loss function. As\n",
      "we have seen, there are situations where we cannot obtain this uniformly.\n",
      "By restricting attention to procedures with properties such as L-unbiasedness\n",
      "or L-invariance, however, we may be able to achieve uniformly best proce-\n",
      "dures within that restricted class. Within the class of unbiased procedures,\n",
      "we seek UMVU estimators and UMPU tests. Likewise, within a collection of\n",
      "equivariant procedures, we seek ones with minimum risk.\n",
      "The simplest and most interesting transformations are translations and\n",
      "scalings, and the combinations of these two, that is linear transformations.\n",
      "Consequently, the two most common types of invariant inference problems\n",
      "are those that are location invariant (or equivariant) and those that are scale\n",
      "invariant (or equivariant). Because in a linear transformation we scale ﬁrst,\n",
      "a scale invariant procedure is invariant to location transformations, but a\n",
      "location invariant procedure is not invariant to scale transformations.\n",
      "In the remainder of this section, we concentrate on problems of point\n",
      "estimation. In Section 7.2.5 beginning on page 525 we discuss equivariant\n",
      "(invariant) test procedures, and in Section 7.9.3 beginning on page 549 we\n",
      "discuss equivariant conﬁdence sets. We discuss equivariance in the context of\n",
      "Bayesian analysis in Section 4.3.2 beginning on page 354.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.4 Invariant and Equivariant Statistical Procedures\n",
      "285\n",
      "Equivariant Point Estimation\n",
      "If the estimand under the untransformed problem is θ, the estimand after\n",
      "the transformations is ˜g(θ). If T(X) is an estimator of θ, equivariance of the\n",
      "estimator requires that g∗(T(X)) = T(g(X)) be an estimator of ˜g(θ) with the\n",
      "same risk.\n",
      "The properties of the estimator in the untransformed problem are pre-\n",
      "served under the transformations. An estimator that is equivariant except\n",
      "possibly on a set of zero probability is said to be almost equivariant.\n",
      "Within a collection of equivariant estimators, we would choose the one\n",
      "with minimum risk. This is MRE estimation, and the estimator is an MREE.\n",
      "(Some authors call it MRI and MRIE.)\n",
      "By the deﬁnition of “equivariance” in this context, the MRE estimator is\n",
      "UMRE, so the concept of uniformity does not arise as a separate issue here.\n",
      "Finding an Optimal Equivariant Point Estimator\n",
      "To ﬁnd an MREE for a given group of transformations, we\n",
      "1. identify the necessary form(s) of the loss function for the transformations\n",
      "2. identify necessary and/or suﬃcient properties of equivariant estimators\n",
      "3. identify an equivariant estimator\n",
      "4. characterize all equivariant estimators in terms of a given one\n",
      "5. identify the one that minimizes the risk for a given loss function\n",
      "We must accept certain limitations alluded to above: the statistical in-\n",
      "ference problem must have be of a special type with respect to the types of\n",
      "transformations, the probability distribution and parameter of interest, and\n",
      "the given loss function.\n",
      "In the next two sections, we illustrate these steps for estimators with lo-\n",
      "cation equivariance and scale equivariance.\n",
      "Location Equivariant Estimation\n",
      "In location equivariant estimation, we assume a family of distributions that\n",
      "are location invariant. We write a PDF of a member of this family as p(x+c).\n",
      "The basic transformation is a translation on both the random variable and\n",
      "the location parameter: eX = X + c and ˜µ = µ + c. The estimand of interest\n",
      "is µ.\n",
      "A reasonable loss function eL must have the property (3.113), that is, eL(µ+\n",
      "c, a+c) = eL(µ, a) for any c, µ and a; hence, eL(µ, a) is a function only of (a−µ):\n",
      "eL(µ, a) = L(a −µ).\n",
      "(3.119)\n",
      "(To repeat the argument that led to equation (3.113) and to see it in this\n",
      "particular case, let µ = −c, and so we have eL(0, a) = eL(0, a −µ), and this\n",
      "equality must continue to hold as µ and c move in tandem.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "286\n",
      "3 Basic Statistical Theory\n",
      "Now, we consider properties of location equivariant estimators.\n",
      "The estimator must have the property (3.118), that is,\n",
      "T(x + a) = T(x) + a.\n",
      "(3.120)\n",
      "It is easy to see that if T0 is a location equivariant estimator and\n",
      "T(x) = T0(x) + u(x),\n",
      "(3.121)\n",
      "where u is any Borel function that is invariant to translations (that is,\n",
      "u(x + a) = u(x)), then T(x) is a location equivariant estimator. (Notice the\n",
      "diﬀerence in “invariant” and “equivariant”.)\n",
      "We now show that any location equivariant estimator must be of the\n",
      "form (3.121) and furthermore, we characterize the function u.\n",
      "Theorem 3.13\n",
      "Given x = (x1, . . ., xn) ∈IRn and let T0 be a location equivariant estimator.\n",
      "(i)If n = 1, then any location equivariant estimator T must satisfy\n",
      "T(x) = T0(x) + u,\n",
      "(3.122)\n",
      "where u is constant.\n",
      "(ii)If n > 1, then any location equivariant estimator T must satisfy\n",
      "T(x) = T0(x) + u(d),\n",
      "(3.123)\n",
      "where\n",
      "d = (xi −xn)\n",
      "for i = 1, . . ., n −1.\n",
      "Proof.\n",
      "Part (i) follows from equation (3.120).\n",
      "Part (ii), n > 1: Let T0 be a location equivariant estimator. Suppose T is a\n",
      "location equivariant estimator, and ∀x ∈IRn let ˜u(x) = T(x)−T0(x). Because\n",
      "T and T0 are location equivariant, we have for any c ∈IR,\n",
      "T(x1, . . ., xn) −T0(x1, . . ., xn) = T(x1 + c, . . ., xn + c) −T0(x1 + c, . . ., xn + c)\n",
      "= ˜u(x1 + c, . . ., xn + c).\n",
      "Now, let c = −xn. So\n",
      "˜u(x1 −xn, . . ., xn−1 −xn, 0) = T(x1, . . ., xn) −T0(x1, . . ., xn)\n",
      "or, with u(x1 −xn, . . ., xn−1 −xn) = ˜u(x1 −xn, . . ., xn−1 −xn, 0) and d =\n",
      "(xi −xn),\n",
      "T(x) = T0(x) + u(d).\n",
      "With this knowledge about the form of any location equivariant estimator,\n",
      "we now seek one with minimum risk. For an estimator based on only one\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.4 Invariant and Equivariant Statistical Procedures\n",
      "287\n",
      "observation, the problem is trivial and it is just to determine an optimal\n",
      "constant u.\n",
      "We will assume a random sample of size n > 1, we will use d as deﬁned\n",
      "above, and we will assume a distribution with PDF p(x −c). If we have a\n",
      "location equivariant estimator T0 with ﬁnite risk, we determine the MREE (if\n",
      "it exists) as\n",
      "T∗(x) = T0(x) −u∗(d),\n",
      "(3.124)\n",
      "where u∗(d) minimizes the conditional risk at c\n",
      "Ec\n",
      "\u0010\n",
      "L(T0(X) −u(D)) | D = d\n",
      "\u0011\n",
      ".\n",
      "(3.125)\n",
      "For simplicity, we take c = 0, and write E0 for the expectation.\n",
      "Whether or not such u∗(d) exists, and if so is unique, depends on the\n",
      "form of the loss function (which, in any event, must be of the form of equa-\n",
      "tion (3.119).) In particular, for squared-error loss, which is of this form, we\n",
      "have\n",
      "u∗(d) = E0\n",
      "\u0000T0(X) | d\n",
      "\u0001\n",
      ".\n",
      "(3.126)\n",
      "Note that for squared-error loss, if a UMVUE exists and is equivariant, it is\n",
      "MRE.\n",
      "For squared-error loss, a location-equivariant point estimator of the loca-\n",
      "tion has a special form, as given in the following theorem.\n",
      "Theorem 3.14\n",
      "Given a sample X1, . . ., Xn from a location family with joint Lebesgue PDF\n",
      "p(x1−µ, . . . , xn−µ), if there is a location-equivariant estimator of µ with ﬁnite\n",
      "risk under squared-error loss, then the unique MREE of µ under squared-error\n",
      "loss is\n",
      "T∗(x) =\n",
      "R t p(X1 −t, . . ., Xn −t)dt\n",
      "R\n",
      "p(X1 −t, . . ., Xn −t)dt ;\n",
      "(3.127)\n",
      "that is, T∗(x) in equation (3.124), with u∗(x) from equation (3.126)), can be\n",
      "written as (3.127).\n",
      "Proof.\n",
      "Let T0(X) be a location-equivariant estimator of µ with ﬁnite risk. MS2 the-\n",
      "orem 4.5\n",
      "The estimator T∗(x) in equation (3.127) is called a Pitman estimator.\n",
      "Note that a location equivariant estimator is not necessarily invariant to\n",
      "scale transformations.\n",
      "Scale Equivariant Estimation\n",
      "In scale equivariant estimation, the basic transformation is a multiplication\n",
      "on both the random variable and the a power nonzero power of the scale\n",
      "parameter: e\n",
      "X = rX, for r > 0, and ˜σ = rhσh. This development parallels\n",
      "that for location equivariant estimation in the previous section.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "288\n",
      "3 Basic Statistical Theory\n",
      "The estimand of interest is σh, for some nonzero h. A reasonable loss\n",
      "function eL must have the property (3.113), eL(rσ, rha) = eL(σ, a), hence,\n",
      "eL(σ, a) = L(a/σh),\n",
      "(3.128)\n",
      "and the estimator must have the property\n",
      "T(rx) = rhT(x).\n",
      "(3.129)\n",
      "If T0 is a scale equivariant estimator, then any scale equivariant estimator\n",
      "must be of the form\n",
      "T(x) = T0(x)\n",
      "u(z) ,\n",
      "(3.130)\n",
      "where\n",
      "zi = x1\n",
      "xn\n",
      ", for i = 1, . . ., n −1,\n",
      "and zn = xn\n",
      "|xn|.\n",
      "If we have a scale equivariant estimator T0 with ﬁnite risk, we determine\n",
      "the MREE (if it exists) as\n",
      "T∗(x) = T0(x)/u∗(z),\n",
      "(3.131)\n",
      "where u∗(z) minimizes the conditional risk at r = 1:\n",
      "E1\n",
      "\u0010\n",
      "γ(T0(X)/u(z)) | z\n",
      "\u0011\n",
      ".\n",
      "(3.132)\n",
      "Note that the loss function has a special form. In the scale equivariant esti-\n",
      "mation problem, there are a couple of special loss functions. One is a squared\n",
      "error of the form\n",
      "L(a/σh) = (a −σh)2\n",
      "σ2h\n",
      ",\n",
      "(3.133)\n",
      "in which case\n",
      "u∗(z) =\n",
      "E1\n",
      "\u0010\n",
      "(T0(x))2 | y\n",
      "\u0011\n",
      "E1\n",
      "\u0010\n",
      "T0(x) | y\n",
      "\u0011 ,\n",
      "(3.134)\n",
      "and the estimator is a Pitman estimator.\n",
      "Another special loss functions is of the form\n",
      "L(a/σh) = a/σh −log(a/σh) −1,\n",
      "(3.135)\n",
      "called “Stein’s loss”, in which case\n",
      "u∗(z) = E1(T0(X) | y).\n",
      "(3.136)\n",
      "Stein’s loss has the interesting property that it is the only scale-invariant loss\n",
      "function for which the UMVUE is also the MREE (diﬃcult proof).\n",
      "A scale equivariant estimator is invariant to location transformations; that\n",
      "is, if T is scale invariant, then T(x + a) = T(x).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.4 Invariant and Equivariant Statistical Procedures\n",
      "289\n",
      "Location-Scale Equivariant Estimation\n",
      "Location-scale equivariance involves the combination of the two separate de-\n",
      "velopments. The basic transformations are location and scale: e\n",
      "X = bX + c\n",
      "and ˜θ = bθ + c.\n",
      "The loss function (3.128) for estimation of the scale parameter is invariant\n",
      "to both location and scale transformations, and the estimator of the scale\n",
      "must have the form of (3.130).\n",
      "In order for the loss function for estimation of the location parameter to\n",
      "be invariant under a location and scale transformation, the loss function must\n",
      "be of the form\n",
      "eL(µ, a) = L((a −µ)/σ),\n",
      "(3.137)\n",
      "and the location estimator must have the property\n",
      "T(bx + c) = brT(x) + c.\n",
      "(3.138)\n",
      "Analysis of these estimators does not involve anything fundamentally dif-\n",
      "ferent from combinations of the ideas discussed separately for the location\n",
      "and scale cases.\n",
      "Equivariant Estimation in a Normal Family\n",
      "MRE estimation has particular relevance to the family of normal distributions,\n",
      "which is a location-scale group family.\n",
      "Example 3.23 Equivariant Estimation in a Normal Family\n",
      "Suppose X1, X2, . . ., Xn are iid as N(µ, σ2) distribution, and consider the\n",
      "problem of estimation of µ and σ2.\n",
      "************************\n",
      "Tσ2(X) =\n",
      "1\n",
      "n + 1\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(Xi −X)2\n",
      "(3.139)\n",
      "**** compare MLE, minimum MSE\n",
      "Tµ(X) = X\n",
      "(3.140)\n",
      "The MRE estimator of the location under a convex and even loss func-\n",
      "tion of the form (3.138) and MRE estimator of the scale under a loss of the\n",
      "form (3.130) are independent of each other. Another interesting fact is that\n",
      "in location families that have densities with respect to Lebesgue measure and\n",
      "with ﬁnite variance, the risk of a MRE location estimator with scaled squared-\n",
      "error loss is larger in the normal family than in any other location-scale group\n",
      "family.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "290\n",
      "3 Basic Statistical Theory\n",
      "3.5 Probability Statements in Statistical Inference\n",
      "In a statistical paradigm in which a parameter characterizing the probability\n",
      "distribution of the observable random variable is itself a random variable,\n",
      "the objective of statistical inference is to use observable data to adjust the\n",
      "assumed probability distribution of the parameter. In this case, the results of\n",
      "the statistical analysis can be summarized by probability statements.\n",
      "If the parameter is not considered to be a random variable, statements of\n",
      "probability that the parameter has given values do not make sense, except as\n",
      "ways of quantifying “beliefs” about the values.\n",
      "Within a “objective” paradigm for statistical inference, there are two in-\n",
      "stances in which statements about probability are associated with the de-\n",
      "cisions of the inferential methods. In hypothesis testing, under assumptions\n",
      "about the distributions, we base our inferential methods on probabilities of\n",
      "two types of errors. In conﬁdence sets the decisions are associated with prob-\n",
      "ability statements about coverage of the parameters.\n",
      "In both of these types of inference, the basic set up is the standard one in\n",
      "statistical inference. We have a random sample of independent observations\n",
      "X1, . . ., Xn on a random variable X that has a distribution Pθ, some aspects\n",
      "of which are unknown. We assume some family of probability distributions P\n",
      "such that Pθ ∈P. We begin with some preassigned probability that, following\n",
      "the prescribed method of inference, we will arrive at set of distributions Pθ\n",
      "that contain the distribution Pθ. Our objective is to determine such meth-\n",
      "ods, and among a class of such methods, determine ones that have optimal\n",
      "properties with respect to reasonable criteria.\n",
      "After having completed such a process, it may or may not be appropriate\n",
      "to characterize the relationship of the “true” unknown distribution Pθ to the\n",
      "set of Pθ with any statement about “probability”. If the particular distribution\n",
      "or some parameter in the distribution is considered to be a (nondegenerate)\n",
      "random variable, we may speak of a probability conditional on the observa-\n",
      "tions used in the inference process. (This is a “posterior” probability.) On the\n",
      "other hand, if the underlying probability model of the observable data is ﬁxed,\n",
      "then either Pθ ∈Pθ with probability 1, or else Pθ /∈Pθ with probability 1.\n",
      "In these types of statistical inference, as we will describe below, we use\n",
      "the terms “signiﬁcance level”, “size”, “conﬁdence level”, and “conﬁdence co-\n",
      "eﬃcient” to describe our ﬁndings.\n",
      "3.5.1 Tests of Hypotheses\n",
      "Given a set of data, X, and a family of possible distributions that gave rise\n",
      "to the data, P, a common objective of statistical inference is to specify a\n",
      "particular member or subclass of P that “likely” generated X. For example, if\n",
      "P = {N(µ, σ2) : µ ∈IR, σ2 ∈IR+}, given X = x, we may choose N(¯x, s2) as\n",
      "a good candidate for the population from which the data arose. This choice\n",
      "is based on statistical estimators that we know to be “good” ones.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.5 Probability Statements in Statistical Inference\n",
      "291\n",
      "In another kind of statistical inference, given a set of data X and a family\n",
      "of distributions P, we are to decide whether the data “likely” came from\n",
      "some hypothesized subfamily P0 of P. Our possible decisions are “yes” or\n",
      "“no”. Rather than a general “no”, a speciﬁc alternative may be hypothesized.\n",
      "This kind of statistical inference is called “testing statistical hypotheses”.\n",
      "We will discuss this topic more fully in Chapter 7. In Section 4.5 we discuss\n",
      "testing from a Bayesian perspective. Here, we just introduce some terms and\n",
      "consider some simple cases.\n",
      "Statistical Hypotheses\n",
      "The hypotheses concern a speciﬁc member P ∈P. This is the distribution\n",
      "that generated the observed data.\n",
      "We have a null hypothesis\n",
      "H0 :\n",
      "P ∈P0\n",
      "(3.141)\n",
      "and an alternative hypothesis\n",
      "H1 :\n",
      "P ∈P1,\n",
      "(3.142)\n",
      "where P0 ⊆P, P1 ⊆P, and P0 ∩P1 = ∅. If P0 ∪P1 = P, the alternative\n",
      "hypothesis is eﬀectively “everything else”.\n",
      "In the paradigm of equations (3.1) and (3.2), in which we characterize\n",
      "statistical inference as beginning with a family of probability distributions\n",
      "P = {Pθ | θ ∈Θ} and, using observed data, deciding that the family is PH,\n",
      "where PH ⊆P, the problem of statistical hypothesis testing can be described\n",
      "as beginning with P = P0 ∪P1, and deciding either that P = P0 or P = P1.\n",
      "In a Bayesian setup of the canonical problem in statistical inference as\n",
      "described in equations (3.3) and (3.4), the problem of statistical hypothesis\n",
      "testing can be described as beginning with P = {PΘ | Θ ∼Q0 ∈Q} where\n",
      "Q0 is some prior distribution, and then deciding that the family of probability\n",
      "distributions giving rise to the observed data is PH = {PΘ | Θ ∼QH ∈Q}.\n",
      "An hypothesis that speciﬁes exactly one distribution is called a simple\n",
      "hypothesis; otherwise it is called a composite hypothesis. H0 above is a simple\n",
      "hypothesis if there is only one distribution in P0.\n",
      "If the family of distributions is associated with a parameter space Θ, we\n",
      "may equivalently describe the tests as\n",
      "H0 : θ ∈Θ0\n",
      "versus\n",
      "H1 : θ ∈Θ1.\n",
      "An hypothesis H : θ ∈ΘH in which #ΘH = 1 is a simple hypothesis;\n",
      "if #ΘH > 1 it is a composite hypothesis. Of course we are often interested\n",
      "in the case where Θ = Θ0 ∪Θ1. An hypothesis of the form H0 : θ = θ0 is a\n",
      "simple hypothesis, while Hi : θ ≥θ0 is a composite hypothesis.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "292\n",
      "3 Basic Statistical Theory\n",
      "Test Statistics and Critical Regions\n",
      "A straightforward way of performing the test involves use of a test statistic,\n",
      "T(X), computed from a random sample of data, with which we associated a\n",
      "rejection region C, and if T(X) ∈C, we reject H0 in favor of H1. Now, if H0\n",
      "is true and we reject it, we have made an error. So a reasonable procedure\n",
      "is to choose C such that if the null hypothesis is true, for some preassigned\n",
      "(small) value, α,\n",
      "Pr (T(X) ∈C|H0) ≤α.\n",
      "(3.143)\n",
      "We call this bound a signiﬁcance level of the test.\n",
      "Although the term “signiﬁcance level” is widely used, the fact that we have\n",
      "deﬁned it as a bound means that it is not very useful (although the deﬁnition\n",
      "in equation (3.143) is the standard one). The LUB is clearly the measure of\n",
      "interest. We call\n",
      "sup\n",
      "P∈H0\n",
      "Pr (T(X) ∈C|P )\n",
      "(3.144)\n",
      "the size of the test.\n",
      "We seek a statistic T(X) such that Pr (T(X) ∈C) is large if the null\n",
      "hypothesis is not true. Thus, C is a region of more “extreme” values of the\n",
      "test statistic if the null hypothesis is true. A statistical test in this kind of\n",
      "scenario is called a “signiﬁcance test”.\n",
      "If T(X) ∈C, the null hypothesis is rejected. The rejection region is also\n",
      "called the critical region. The complement of the rejection region is called the\n",
      "acceptance region.\n",
      "It is desirable that the test have a high probability of rejecting the null\n",
      "hypothesis if indeed the null hypothesis is not true.\n",
      "p-Values\n",
      "A procedure for testing that is mechanically equivalent to this is to compute\n",
      "the realization of the test statistic T(X), say t, and then to determine the\n",
      "probability that T(X) is more extreme than t. In this approach, the realized\n",
      "value of the test statistic determines a region Ct of more extreme values.\n",
      "The probability that the test statistic is in Ct if the null hypothesis is true,\n",
      "Pr (T ∈Ct), is called the “p-value” or “observed signiﬁcance level” of the\n",
      "realized test statistic.\n",
      "In this framework we are testing one hypothesis versus another hypothe-\n",
      "sis. The two hypotheses are not treated symmetrically, however. We are still\n",
      "directly testing the null hypothesis. This asymmetry allows us to focus on two\n",
      "kinds of losses that we might incur. The losses relate to the two kinds of errors\n",
      "that we might make.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.5 Probability Statements in Statistical Inference\n",
      "293\n",
      "Test Rules\n",
      "Instead of thinking of a test statistic T and a rejection region C, as above, we\n",
      "can formulate the testing procedure in a slightly diﬀerent way. We can think of\n",
      "the test as a decision rule, δ(X), which is a statistic that relates more directly\n",
      "to the decision about the hypothesis. We sometimes refer to the statistic δ(X)\n",
      "as “the test”, because its value is directly related to the outcome of the test;\n",
      "that is, there is no separately deﬁned rejection region.\n",
      "A nonrandomized test procedure is a rule δ(X) that assigns two decisions to\n",
      "two disjoint subsets, C0 and C1, of the range of T(X). In general, we require\n",
      "C0 ∪C1 be the support of T(X). We equate those two decisions with the real\n",
      "numbers d0 and d1, so δ(X) is a real-valued function,\n",
      "δ(x) =\n",
      "\u001ad0 for T(x) ∈C0\n",
      "d1 for T(x) ∈C1.\n",
      "(3.145)\n",
      "For simplicity, we choose d0 = 0 and d1 = 1. Note for i = 0, 1,\n",
      "Pr(δ(X) = i) = Pr(T(X) ∈Ci).\n",
      "(3.146)\n",
      "As above, we call C1 the critical region, and generally denote it by just C.\n",
      "If δ(X) takes the value 0, the decision is not to reject; if δ(X) takes the\n",
      "value 1, the decision is to reject. If the range of δ(X) is {0, 1}, the test is a\n",
      "nonrandomized test. Sometimes, however, it is useful to expand the range of\n",
      "δ(X) to be [0, 1], where we can interpret a value of δ(X) as the probability\n",
      "that the null hypothesis is rejected. If it is not the case that δ(X) equals 0 or\n",
      "1 a.s., we call the test a randomized test.\n",
      "Testing as an Estimation Problem\n",
      "In the general setup above, we can deﬁne an indicator function IΘ0(θ). The\n",
      "testing problem is equivalent to the problem of estimating IΘ0(θ). Let us use\n",
      "a statistic S(X) as an estimator of IΘ0(θ). The estimand is in {0, 1}, and so\n",
      "S(X) should be in {0, 1}, or at least in [0, 1].\n",
      "Notice the relationship of S(X) to δ(X). For the estimation approach using\n",
      "S(X) to be equivalent to use of the test rule δ(X), it must be the case that\n",
      "S(X) = 1 ⇐⇒δ(X) = 0\n",
      "(i.e., do not reject)\n",
      "(3.147)\n",
      "and\n",
      "S(X) = 0 ⇐⇒δ(X) = 1\n",
      "(i.e., reject)\n",
      "(3.148)\n",
      "Following a decision-theoretic approach to the estimation problem, we de-\n",
      "ﬁne a loss function. In the a simple framework for testing, the loss function is\n",
      "0-1. Under this loss, using S(X) = s as the rule for the test, we have\n",
      "L(θ, s) =\n",
      "\u001a0 if s = IΘ0(θ)\n",
      "1 otherwise.\n",
      "(3.149)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "294\n",
      "3 Basic Statistical Theory\n",
      "Power of the Test\n",
      "We now can focus on the test under either hypothesis (that is, under either\n",
      "subset of the family of distributions) in a uniﬁed fashion. We deﬁne the power\n",
      "function of the test, for any given P ∈P as\n",
      "β(δ, P ) = EP (δ(X)).\n",
      "(3.150)\n",
      "We also often use the notation βδ(P ) instead of β(δ, P ). In general, the prob-\n",
      "ability of rejection of the null hypothesis is called the power of the test.\n",
      "An obvious way of deﬁning optimality for tests is in terms of the power for\n",
      "distributions in the class of the alternative hypothesis; that is, we seek “most\n",
      "powerful” tests.\n",
      "Errors\n",
      "If P ∈P0 and δ(X) = 1, we make an error; that is, we reject a true hypothesis.\n",
      "We call that a “type I error”. For a randomized test, we have the possibility\n",
      "of making a type I error if δ(X) > 0. In general, if P ∈P0, βδ(P ) is the\n",
      "probability of a type I error. Conversely, if P ∈P1, then 1 −βδ(P ) is the\n",
      "probability of a “type II error”, that is failing to reject a false hypothesis.\n",
      "Testing as a Decision Problem\n",
      "For a statistical hypothesis as described above with δ(x) as in equation (3.145),\n",
      "and d0 = 0 and d1 = 1, write\n",
      "φ(x) = Pr(δ(X) = 1 | X = x).\n",
      "(3.151)\n",
      "Notice that this is the same as the power, except φ here is a function of the\n",
      "observations, while we think of the power as a function of the true distribution.\n",
      "Assuming only the two outcomes, we have\n",
      "1 −φ(x) = Pr(δ(X) = 0 | X = x).\n",
      "(3.152)\n",
      "For this decision problem, an obvious choice of a loss function is the 0-1\n",
      "loss function:\n",
      "L(P, i) = 0\n",
      "if Hi\n",
      "L(P, i) = 1\n",
      "otherwise.\n",
      "(3.153)\n",
      "It may be useful to consider a procedure with more than just two outcomes;\n",
      "in particular, a third outcome, γ, may make sense. In an application in anal-\n",
      "ysis of data, this decision may suggest collecting more data; that is, it may\n",
      "correspond to “no decision”, or, usually only for theoretical analyses, it may\n",
      "suggest that a decision be made randomly. We will, at least in the beginning,\n",
      "however, restrict our attention to procedures with just two outcomes.\n",
      "For the two decisions and two state of nature case, there are four possibil-\n",
      "ities:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.5 Probability Statements in Statistical Inference\n",
      "295\n",
      "•\n",
      "the test yields 0 and H0 is true (correct decision);\n",
      "•\n",
      "the test yields 1 and H1 is true (correct decision);\n",
      "•\n",
      "the test yields 1 and H0 is true (type I error); and\n",
      "•\n",
      "the test yields 0 and H1 is true (type II error).\n",
      "We obviously want a test procedure that minimizes the probability of either\n",
      "type of error. It is clear that we can easily decrease the probability of one\n",
      "(if its probability is positive) at the cost of increasing the probability of the\n",
      "other.\n",
      "We do not treat H0 and H1 symmetrically; H0 is the hypothesis to be\n",
      "tested and H1 is the alternative. This distinction is important in developing\n",
      "a practical methodology of testing.\n",
      "We adopt the following approach for choosing δ (under the given assump-\n",
      "tions on X, and the notation above):\n",
      "1. Choose α ∈]0, 1[ and require that δ(X) be such that\n",
      "Pr(δ(X) = 1 | H0) ≤α.\n",
      "α is called the level of signiﬁcance.\n",
      "2. Subject to this, ﬁnd δ(X) so as to minimize\n",
      "Pr(δ(X) = 0 | H1).\n",
      "The deﬁnition of signiﬁcance level is not as ambiguous as it may appear at\n",
      "ﬁrst glance.\n",
      "One chooses α; that is the level of signiﬁcance.\n",
      "For some ˜α > α, although Pr(δ(X) = 1 | θ ∈Θ0) ≤˜α, we would not say\n",
      "that ˜α is the level (or a level) of signiﬁcance.\n",
      "Notice that the restriction on the type I error in the ﬁrst step applies\n",
      "∀P ∈H0. If the size is less than the level of signiﬁcance, the test is said to be\n",
      "conservative, and in that case, we often refer to α as the “nominal size”.\n",
      "Approximate Tests\n",
      "If the distribution of the test statistic T or δ under the null hypothesis is\n",
      "known, the critical region or the p-value can be determined. If the distribu-\n",
      "tion is not known, some other approach must be used. A common method is to\n",
      "use some approximation to the distribution. The objective is to approximate\n",
      "a quantile of T under the null hypothesis. In asymptotic inference, the ap-\n",
      "proximation is often based on an asymptotic distribution of the test statistic.\n",
      "In computational inference, a Monte Carlo test may be used. In Monte\n",
      "Carlo tests the quantile of T is estimated by simulation of the distribution.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "296\n",
      "3 Basic Statistical Theory\n",
      "Unbiased Tests\n",
      "A test δ of H0 : P ∈P0 versus H1 : P ∈P1 is said to be unbiased at level α\n",
      "if the power function satisﬁes\n",
      "βδ(P ) ≤α\n",
      "for P ∈P0\n",
      "βδ(P ) ≥α\n",
      "for P ∈P1\n",
      "Uniformly Best Tests\n",
      "The risk or the expected error in a test depends on the speciﬁc distribution\n",
      "within the family of distributions assumed. We may seek a test that has\n",
      "minimum expected errors of both types, or, in a practical approach to this\n",
      "objective, we may cap the probability of a type I error and seek the most\n",
      "powerful test for distributions within the class of the alternative hypothesis.\n",
      "As we have seen in the estimation problem, optimality generally depends\n",
      "on the speciﬁc distribution, and it may not be possible to achieve it uniformly;\n",
      "that is, for all distributions within a given family.\n",
      "We may then take the approach mentioned on page 267 for estimation\n",
      "and restrict the allowable tests in some way. We may require that the tests\n",
      "be unbiased, for example. That approach leads us to seek a UMPU test, that\n",
      "is, a uniformly most powerful unbiased test. Alternatively, as we mentioned\n",
      "before, we may seek a test that is optimal over the full set of distributions P\n",
      "by some global measure of optimality.\n",
      "3.5.2 Conﬁdence Sets\n",
      "In a problem of statistical inference for a family of distributions P, given a\n",
      "random sample X, a level 1 −α conﬁdence set, or conﬁdence set (the terms\n",
      "are synonymous), is a is a random subset of P, PS, such that\n",
      "PrP (PS ∋P ) ≥1 −α\n",
      "∀P ∈P.\n",
      "(3.154)\n",
      "More precisely, we call PS a random family of level 1 −α conﬁdence sets.\n",
      "This deﬁnition obviously leaves many issues to be examined because of the\n",
      "≥relationship. A family of 1 −α1 conﬁdence sets is also a family of 1 −α2\n",
      "conﬁdence set for α2 ≥α1; and if PS is a level 1 −α conﬁdence set, then PeS\n",
      "is also a level 1 −α conﬁdence set if PeS ⊃PS.\n",
      "The “S” in this notation refers to a random sample of X, and the notation\n",
      "PS is intended to imply that a random set is being indicated, in contrast to\n",
      "the notation PH used above to refer to an hypothesized set. The set PS is\n",
      "determined by the random sample, while the set PH is determined a priori.\n",
      "The source of randomness also accounts for my preferred notation, PS ∋P ,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.5 Probability Statements in Statistical Inference\n",
      "297\n",
      "which can be thought of a referring to the random event in which the set PS\n",
      "includes the element P .\n",
      "As with the term “signiﬁcance” in the hypothesis testing problem, the\n",
      "standard usage of the term “conﬁdence” is subject to a certain amount of\n",
      "ambiguity. In hypothesis testing, we use “level of signiﬁcance” and “size” of\n",
      "the tests. (Recall that, adding to the confusion” we also use “signiﬁcance\n",
      "level” of a test statistic to refer to a minimal size test that would reject the\n",
      "null hypothesis; that is, to refer to a p-value.) In setting conﬁdence regions,\n",
      "we refer to “conﬁdence level” and “conﬁdence coeﬃcient”. We call\n",
      "inf\n",
      "P∈P PrP (PS ∋P )\n",
      "(3.155)\n",
      "the conﬁdence coeﬃcient of PS.\n",
      "The conﬁdence coeﬃcient is also called the coverage probability.\n",
      "In a parametric setting, we can equivalently deﬁne a random family ΘS of\n",
      "1 −α conﬁdence regions (sets) for the parameter space Θ by\n",
      "Prθ (ΘS ∋θ) ≥1 −α\n",
      "∀θ ∈Θ.\n",
      "A realization of a conﬁdence set, say Θs, is also called a conﬁdence set.\n",
      "Although it may seem natural to state that the “probability that θ is in\n",
      "A(x) is 1 −α”, this statement can be misleading unless a certain underlying\n",
      "probability structure is assumed.\n",
      "We will introduce and discuss other terms in Chapter 7. In Chapter 4 we\n",
      "discuss conﬁdence sets from a Bayesian perspective. Here, we just deﬁne the\n",
      "term and consider some simple cases.\n",
      "Pivot Functions\n",
      "For forming conﬁdence sets, we often can use a function of the sample that\n",
      "also involves the parameter of interest, g(T, θ). The conﬁdence set is then\n",
      "formed by separating the parameter from the sample values.\n",
      "A class of functions that are particularly useful for forming conﬁdence sets\n",
      "are called pivotal values, or pivotal functions. A function g(T, θ) is said to be\n",
      "a pivotal function if its distribution does not depend on any unknown param-\n",
      "eters. This allows exact conﬁdence intervals to be formed for the parameter\n",
      "θ.\n",
      "Conﬁdence Intervals\n",
      "Our usual notion of a conﬁdence leads to the deﬁnition of a 1 −α conﬁdence\n",
      "interval for the (scalar) parameter θ as the random interval [TL, TU], that has\n",
      "the property\n",
      "Pr (TL ≤θ ≤TU) ≥1 −α.\n",
      "(3.156)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "298\n",
      "3 Basic Statistical Theory\n",
      "This is also called a (1 −α)100% conﬁdence interval. The interval [TL, TU ] is\n",
      "not uniquely determined.\n",
      "The concept extends easily to vector-valued parameters. Rather than tak-\n",
      "ing vectors TL and TU, however, we generally deﬁne an ellipsoidal region,\n",
      "whose shape is determined by the covariances of the estimators.\n",
      "A realization of the random interval, say [tL, tU], is also called a conﬁdence\n",
      "interval.\n",
      "In practice, the interval is usually speciﬁed with respect to an estimator\n",
      "of θ, T. If we know the sampling distribution of T −θ, we may determine c1\n",
      "and c2 such that\n",
      "Pr (c1 ≤T −θ ≤c2) = 1 −α;\n",
      "and hence\n",
      "Pr (T −c2 ≤θ ≤T −c1) = 1 −α.\n",
      "If either TL or TU is inﬁnite or corresponds to a bound on acceptable\n",
      "values of θ, the conﬁdence interval is one-sided. Suppose Θ = (a, b), where\n",
      "a or b may be inﬁnite. In equation (3.156), if TL = a, then TU is called an\n",
      "upper conﬁdence bound, and if TU = b, then TL is called a lower conﬁdence\n",
      "bound. (It is better not to use the terms “upper conﬁdence interval” or “lower\n",
      "conﬁdence interval”, because of the possible ambiguity in these terms.)\n",
      "For two-sided conﬁdence intervals, we may seek to make the probability\n",
      "on either side of T to be equal, to make c1 = −c2, and/or to minimize |c1| or\n",
      "|c2|. This is similar in spirit to seeking an estimator with small variance.\n",
      "We can use a pivot function g(T, θ) to form conﬁdence intervals for the\n",
      "parameter θ. We ﬁrst form\n",
      "Pr\n",
      "\u0010\n",
      "g(α/2) ≤g(T, θ) ≤g(1−α/2)\n",
      "\u0011\n",
      "= 1 −α,\n",
      "where g(α/2) and g(1−α/2) are quantiles of the distribution of g(T, θ); that is,\n",
      "Pr(g(T, θ) ≤g(π)) = π.\n",
      "If, as in the case considered above, g(T, θ) = T −θ, the resulting conﬁdence\n",
      "interval has the form\n",
      "Pr\n",
      "\u0010\n",
      "T −g(1−α/2) ≤θ ≤T −g(α/2)\n",
      "\u0011\n",
      "= 1 −α.\n",
      "Example 3.24 Conﬁdence Interval for Mean of a Normal Distribu-\n",
      "tion\n",
      "Suppose Y1, Y2, . . ., Yn are iid as N(µ, σ2) distribution, and Y is the sample\n",
      "mean. The quantity\n",
      "g(Y , µ) =\n",
      "p\n",
      "n(n −1) (Y −µ)\n",
      "qP \u0000Yi −Y\n",
      "\u00012\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.5 Probability Statements in Statistical Inference\n",
      "299\n",
      "has a Student’s t distribution with n −1 degrees of freedom, no matter what\n",
      "is the value of σ2. This is one of the most commonly-used pivotal values.\n",
      "The pivotal value can be used to form a conﬁdence value for θ by ﬁrst\n",
      "writing\n",
      "Pr \u0000t(α/2) ≤g(Y , µ) ≤t(1−α/2)\n",
      "\u0001 = 1 −α,\n",
      "where t(π) is a percentile from the Student’s t distribution. Then, after making\n",
      "substitutions for g(Y , µ), we form the familiar conﬁdence interval for µ:\n",
      "\u0010\n",
      "Y −t(1−α/2) S/√n,\n",
      "Y −t(α/2) S/√n\n",
      "\u0011\n",
      ",\n",
      "where S2 is the usual sample variance, P(Yi −Y )2/(n −1).\n",
      "(Note the notation: t(π), or for clarity, tν,(π) is the π quantile of a Student’s\n",
      "t distribution. That means that\n",
      "Pr(Y ≤tν,(π)) = π.\n",
      "Other authors sometimes use a similar notation to mean the 1 −π quantile\n",
      "and other times to mean the π quantiles; that is, the same authors use it both\n",
      "ways. I always use the notation in the way I indicate above. The reasons for\n",
      "the diﬀerent symbols go back to the fact that tν,(π) = −tν,(1−π), as for any\n",
      "distribution that is symmetric about 0.)\n",
      "Other similar pivotal functions have F distributions. For example, consider\n",
      "the usual linear regression model in which the n-vector random variable Y has\n",
      "a Nn(Xβ, σ2I) distribution, that is,\n",
      "Y ∼Nn(Xβ, σ2I),\n",
      "(3.157)\n",
      "where X is an n×m known matrix, and the m-vector β and the scalar σ2 are\n",
      "unknown. A pivotal value useful in making inferences about β is\n",
      "g(bβ, β) =\n",
      "\u0000X(bβ −β)\n",
      "\u0001TX(bβ −β)/m\n",
      "(Y −X bβ)T(Y −X bβ)/(n −m)\n",
      ",\n",
      "where\n",
      "bβ = (XTX)+XTY.\n",
      "The random variable g(bβ, β) for any ﬁnite value of σ2 has an F distribution\n",
      "with m and n −m degrees of freedom.\n",
      "For a given parameter and family of distributions there may be multiple\n",
      "pivotal values. For purposes of statistical inference, such considerations as\n",
      "unbiasedness and minimum variance may guide the choice of a pivotal value\n",
      "to use. Alternatively, it may not be possible to identify a pivotal quantity for\n",
      "a particular parameter. In that case, we may seek an approximate pivot. A\n",
      "function is asymptotically pivotal if a sequence of linear transformations of\n",
      "the function is pivotal in the limit as n →∞.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "300\n",
      "3 Basic Statistical Theory\n",
      "If the distribution of T is known, c1 and c2 can be determined. If the\n",
      "distribution of T is not known, some other approach must be used. A common\n",
      "method is to use some numerical approximation to the distribution. Another\n",
      "method is to use bootstrap resampling.\n",
      "Optimal Conﬁdence Sets\n",
      "We seek conﬁdence sets that are “small” or “tight” in some way. We want\n",
      "the region of the parameter space that is excluded by the conﬁdence set to\n",
      "be large; that is, we want the probability that the conﬁdence set exclude\n",
      "parameters that are not supported by the observational evidence to be large.\n",
      "This is called “accuracy”. We see most accurate conﬁdence sets.\n",
      "As with point estimation and tests of hypotheses, the risk in determining\n",
      "a conﬁdence set depends on the speciﬁc distribution within the family of\n",
      "distributions assumed. We, therefore, seek uniformly most accurate conﬁdence\n",
      "sets.\n",
      "As in other cases where we seek uniform optimality, such procedures may\n",
      "not exist. We, therefore, may then take a similar approach for determining\n",
      "conﬁdence sets, and restrict the allowable regions in some way. We may require\n",
      "that the conﬁdence sets be unbiased, for example.\n",
      "Unbiased Conﬁdence Sets\n",
      "A family of conﬁdence sets ΘS for θ is said to be unbiased (without regard to\n",
      "the level) if\n",
      "Prθ0 (ΘS ∋θ1) ≤Prθ0 (ΘS ∋θ0) ∀θ0, θ1 ∈Θ.\n",
      "(3.158)\n",
      "Prediction Sets and Tolerance Sets\n",
      "We often want to identify a set in which a future observation on a random\n",
      "variable has a high probability of occurring. This kind of set is called a pre-\n",
      "diction set. For example, we may assume a given sample X1, . . ., Xn is from\n",
      "a N(µ, σ2) and we wish to determine a measurable set S(X) such that for a\n",
      "future observation Xn+1\n",
      "inf\n",
      "P∈P PrP(Xn+1 ∈S(X)) ≥1 −α.\n",
      "(3.159)\n",
      "More generally, instead of Xn+1, we could deﬁne a prediction interval for\n",
      "any random variable V .\n",
      "The diﬀerence in this and a conﬁdence set for µ is that there is an addi-\n",
      "tional source of variation. The prediction set will be larger, so as to account\n",
      "for this extra variation.\n",
      "We may want to separate the statements about V and S(X). A tolerance\n",
      "set attempts to do this.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.6 Variance Estimation\n",
      "301\n",
      "Given a sample X, a measurable set S(X), and numbers δ and α in ]0, 1[,\n",
      "if\n",
      "inf\n",
      "P∈P PrP (PrP (V ∈S(X)|X) ≥δ) ≥1 −α,\n",
      "(3.160)\n",
      "then S(X) is called a δ-tolerance set for V with conﬁdence level 1 −α.\n",
      "Approximate Conﬁdence Sets\n",
      "In some cases, we have a tractable probability model, so that we can determine\n",
      "conﬁdence sets with exact levels of signiﬁcance. In other cases the problem is\n",
      "not tractable analytically, so we must resort to approximations, which may be\n",
      "base on asymptotic distributions, or to estimates, which may be made using\n",
      "simulations.\n",
      "Asymptotic inference uses asymptotic approximations. Computational in-\n",
      "ference uses probabilities estimated by simulation of an assumed or hypothe-\n",
      "sized data generating process or by resampling of an observed sample.\n",
      "3.6 Variance Estimation\n",
      "Statistical inferences that involve or are derived from statements of prob-\n",
      "ability, such as hypothesis testing and determining conﬁdence sets, require\n",
      "knowledge of the distribution of the statistic that is used. Often we know or\n",
      "can work out that distribution exactly, given the assumptions in the under-\n",
      "lying probability model. In other cases we use approximate distributions. In\n",
      "either case, we are often faced with the problem of estimating the variance of\n",
      "a statistic.\n",
      "In this section we ﬁrst restrict our attention to the case in which the\n",
      "statistic of interest is a scalar; that is, the case in which the variance itself is\n",
      "a scalar. We describe two general methods, the jackknife and the bootstrap,\n",
      "based on resampling. We then consider the more general problem of estimat-\n",
      "ing the variance-covariance matrix for a vector statistic. In either case, the\n",
      "ﬁrst issue to address is the meaning of consistency of a variance-covariance\n",
      "estimator, which we will consider in a general way in Section 3.8.1, and deﬁne\n",
      "speciﬁcally in Deﬁnition 3.18. The jackknife and bootstrap can be used to\n",
      "estimate a variance-covariance matrix, and we also consider a “substitution”\n",
      "estimator.\n",
      "3.6.1 Jackknife Methods\n",
      "Jackknife methods make use of systematic partitions of a dataset to estimate\n",
      "properties of an estimator computed from the full sample.\n",
      "Suppose that we have a random sample, Y1, . . ., Yn, from which we com-\n",
      "pute a statistic T as an estimator of a parameter θ in the population from\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "302\n",
      "3 Basic Statistical Theory\n",
      "which the sample was drawn. In the jackknife method, we compute the statis-\n",
      "tic T using only a subset of size n −d of the given dataset; that is, we delete\n",
      "a set of size d.\n",
      "There are of course\n",
      "Cn\n",
      "d =\n",
      "\u0012n\n",
      "d\n",
      "\u0013\n",
      "such sets.\n",
      "Let T(−j) denote the estimator computed from the sample with the jth\n",
      "set of observations removed; that is, T(−j) is based on a sample of size n −d.\n",
      "The estimator T(−j) has properties similar to those of T. For example, if T is\n",
      "unbiased, so is T(−j). If T is not unbiased, neither is T(−j); its bias, however,\n",
      "is likely to be diﬀerent.\n",
      "The mean of the T(−j),\n",
      "T (•) = 1\n",
      "Cn\n",
      "d\n",
      "Cn\n",
      "d\n",
      "X\n",
      "j=1\n",
      "T(−j),\n",
      "(3.161)\n",
      "can be used as an estimator of θ. The T(−j) may also provide some information\n",
      "about the estimator T from the full sample.\n",
      "For the case in which T is a linear functional of the ECDF, then T (•) =\n",
      "T, so the systematic partitioning of a random sample will not provide any\n",
      "additional information.\n",
      "Consider the weighted diﬀerences in the estimate for the full sample and\n",
      "the reduced samples:\n",
      "T ∗\n",
      "j = nT −(n −d)T(−j).\n",
      "(3.162)\n",
      "The T ∗\n",
      "j are called “pseudovalues”. (If T is a linear functional of the ECDF\n",
      "and d = 1, then T ∗\n",
      "j = T(xj); that is, it is the estimator computed from the\n",
      "single observation, xj.)\n",
      "We call the mean of the pseudovalues the “jackknifed” T and denote it as\n",
      "J(T):\n",
      "J(T) = 1\n",
      "Cn\n",
      "d\n",
      "Cn\n",
      "d\n",
      "X\n",
      "j=1\n",
      "T ∗\n",
      "j\n",
      "= T ∗.\n",
      "(3.163)\n",
      "In most applications of the jackknife, it is common to take d = 1, in which\n",
      "case Cn\n",
      "d = n. The term “jackknife” is often reserved to refer to the case of\n",
      "d = 1, and if d > 1, the term “delete d jackknife” is used. In the case of d = 1,\n",
      "we can also write J(T) as\n",
      "J(T) = T + (n −1)\n",
      "\u0000T −T (•)\n",
      "\u0001\n",
      "or\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.6 Variance Estimation\n",
      "303\n",
      "J(T) = nT −(n −1)T (•).\n",
      "(3.164)\n",
      "It has been shown that d = 1 has some desirable properties under cer-\n",
      "tain assumptions about the population (see Rao and Webster (1966)). On the\n",
      "other hand, in many cases consistency requires that d →∞, although at a\n",
      "rate substantially less than n, that is, such that n −d →∞.\n",
      "Notice that the number of arithmetic operations required to compute a\n",
      "jackknife statistic can be large. When d = 1, a naive approach requires a\n",
      "number of computations of O(n2), although in most cases computations can\n",
      "be reduced to O(n). In general, the order of the number of computations may\n",
      "be in O(nd+1). Even if the exponent of n can be reduced by clever updating\n",
      "computations, a delete-d jackknife can require very intensive computations.\n",
      "Instead of evaluating the statistic over all Cn\n",
      "d subsets, in practice, we often\n",
      "use an average of the statistic computed only over a random sampling of the\n",
      "subsets.\n",
      "Jackknife Variance Estimator\n",
      "Although the pseudovalues are not independent (except when T is a linear\n",
      "functional), we treat them as if they were independent, and use V(J(T)) as\n",
      "an estimator of the variance of T, V(T). The intuition behind this is simple:\n",
      "a small variation in the pseudovalues indicates a small variation in the esti-\n",
      "mator. The sample variance of the mean of the pseudovalues can be used as\n",
      "an estimator of V(T):\n",
      "[\n",
      "V(T)J =\n",
      "PCn\n",
      "d\n",
      "j=1\n",
      "\u0000T ∗\n",
      "j −T ∗\u00012\n",
      "r(r −1)\n",
      ".\n",
      "(3.165)\n",
      "Notice that when T is the mean and d = 1, this is the standard variance\n",
      "estimator.\n",
      "From expression (3.165), it may seem more natural to take [\n",
      "V(T)J as an\n",
      "estimator of the variance of J(T), and indeed it often is.\n",
      "A variant of this expression for the variance estimator uses the original\n",
      "estimator T:\n",
      "PCn\n",
      "d\n",
      "j=1(T ∗\n",
      "j −T)2\n",
      "r(r −1)\n",
      ".\n",
      "(3.166)\n",
      "How good a variance estimator is depends on the estimator T and on\n",
      "the underlying distribution. Monte Carlo studies indicate that [\n",
      "V(T)J is often\n",
      "conservative; that is, it often overestimates the variance.\n",
      "The alternate expression (3.166) is greater than or equal to [\n",
      "V(T)J, as is\n",
      "easily seen (exercise); hence, it may be an even more conservative estimator.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "304\n",
      "3 Basic Statistical Theory\n",
      "3.6.2 Bootstrap Methods\n",
      "From a given sample y1, . . ., yn, suppose that we have an estimator T(y). The\n",
      "estimator T ∗computed as the same function T, using a bootstrap sample\n",
      "(that is, T ∗= T(y∗)), is a bootstrap observation of T.\n",
      "The bootstrap estimate of some function of the estimator T is a plug-in\n",
      "estimate that uses the empirical distribution Pn in place of P . This is the\n",
      "bootstrap principle, and this bootstrap estimate is called the ideal bootstrap.\n",
      "For the variance of T, for example, the ideal bootstrap estimator is the\n",
      "variance V(T ∗). This variance, in turn, can be estimated from bootstrap sam-\n",
      "ples. The bootstrap estimate of the variance, then, is the sample variance of\n",
      "T ∗based on the m samples of size n taken from Pn:\n",
      "bV(T) = bV(T ∗)\n",
      "(3.167)\n",
      "=\n",
      "1\n",
      "m −1\n",
      "X\n",
      "(T ∗j −T ∗)2,\n",
      "(3.168)\n",
      "where T ∗j is the jth bootstrap observation of T. This, of course, can be com-\n",
      "puted by Monte Carlo methods by generating m bootstrap samples and com-\n",
      "puting T ∗j for each.\n",
      "If the estimator of interest is the sample mean, for example, the bootstrap\n",
      "estimate of the variance is bV(Y )/n, where bV(Y ) is an estimate of the variance\n",
      "of the underlying population. (This is true no matter what the underlying\n",
      "distribution is, as long as the variance exists.) The bootstrap procedure does\n",
      "not help in this situation.\n",
      "3.6.3 Substitution Methods\n",
      "The jackknife and bootstrap can be used to estimate a variance-covariance\n",
      "estimator. Another useful type of estimator is called a substitution estimator\n",
      "or sandwich estimator.\n",
      "The idea in the “substitution method” for estimating Vn is to arrive at\n",
      "an expression for Vn that involves a simpler variance along with quantities\n",
      "that are known functions of the sample. Often that simpler variance can be\n",
      "estimated by an estimator with known desirable properties. An estimator of\n",
      "Vn in which the simpler estimator and the known sample functions are used\n",
      "is called a substitution estimator. A simple example is the estimator of the\n",
      "variance of bβ in a linear regression following the model (3.157) on page 299.\n",
      "The variance-covariance matrix is (XTX)−1σ2. A substitution estimator is\n",
      "one in which the regression MSE is substituted for σ2.\n",
      "The so-called “sandwich estimators” are often substitution estimators.\n",
      "(ZTZ)−1V (ZTZ)−1\n",
      "V is some variance-covariance estimator that probably includes a scalar\n",
      "multiple of bσ2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.7 Applications\n",
      "305\n",
      "3.7 Applications\n",
      "3.7.1 Inference in Linear Models\n",
      "3.7.2 Inference in Finite Populations\n",
      "One of the most important areas of application of statistics is in sampling and\n",
      "making inferences about a ﬁnite set of objects, such as all inhabitants of a given\n",
      "country. Finite population sampling or, as it is often called, “survey sampling”\n",
      "is the process of designing and collecting a sample of some characteristic of the\n",
      "members of the ﬁnite set. There are various issues and considerations in ﬁnite\n",
      "population sampling that rarely arise in other areas of statistical inference.\n",
      "A ﬁnite population is some ﬁnite set P = {(1, y1), . . ., (N, yN)}, where\n",
      "the yi are real numbers associated with the set of objects of interest. (Note\n",
      "that here we use “population” in a diﬀerent way from the use of the term\n",
      "as a probability measure.) Finite population sampling is the collection of a\n",
      "sample S = {(L1, X1), . . ., (Ln, Xn)} where Xi = yj, for some j. (In general,\n",
      "the set X = {X1, . . ., Xn} may be a multiset.) In discussions of sampling it\n",
      "is common to use n to denote the size of the sample and N to denote the\n",
      "size of the population. Another common notation used in sampling is Y to\n",
      "denote the population total, Y = PN\n",
      "i=1 yi. The objective of course is to make\n",
      "inferences about the population, such as to estimate the total Y .\n",
      "From a parametric point of view, the parameter that characterizes the\n",
      "population is θ = (y1, . . ., yN), and the parameter space, Θ, is the subspace\n",
      "of IRN containing all possible values of the yi.\n",
      "There are various ways of collecting the sample. A simple random sample\n",
      "with replacement is a sample in which the Xi are iid. A related concept\n",
      "is a simple random sample without replacement, in which the Xi = yj are\n",
      "constrained so that a given value of j can occur once at most.\n",
      "A common method of collecting a sample is to select elements from the ﬁ-\n",
      "nite population with diﬀerent probabilities. If πi > 0 for all i is the probability\n",
      "that yi is included in the sample, and if\n",
      "LS = {i : yi is included in the sample}\n",
      "then clearly\n",
      "bY =\n",
      "X\n",
      "i∈LS\n",
      "yi\n",
      "πi\n",
      "is an unbiased estimator of the population total.\n",
      "The variance of this estimator depends on the πi as well as πi, where πij\n",
      "is the probability that both yi and yj are included in the sample.\n",
      "Much of the theory for ﬁnite population inference depends on how a prob-\n",
      "ability distribution is used. As we have implied above the probability distri-\n",
      "bution used in inference arises from the random selection of the sample itself.\n",
      "This is called a “design based” approach. Other approaches to statistical in-\n",
      "ference in ﬁnite populations begin by modeling the population as a realization\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "306\n",
      "3 Basic Statistical Theory\n",
      "(y1, . . ., yN) of a random vector (Y1, . . ., YN). This is called a superpopulation\n",
      "model. In the context of a superpopulation, probability distributions can be\n",
      "assumed for each Yi, possibly with associated covariates. This may lead to a\n",
      "“model based” approach to statistical inference in ﬁnite populations.\n",
      "Just as with statistical procedures in other settings, a superpopulation\n",
      "model also allows us to investigate asymptotic properties of statistics.\n",
      "We will discuss some topics of inference in ﬁnite populations further in\n",
      "Section 5.5.2.\n",
      "3.8 Asymptotic Inference\n",
      "In the standard problem in statistical inference, we are given some family of\n",
      "probability distributions, we take random observations on a random variable,\n",
      "and we use some function of the random sample to estimate some aspect of\n",
      "the underlying probability distribution or to test some statement about the\n",
      "probability distribution.\n",
      "The approach to statistical inference that we would like to follow is to\n",
      "identify a reasonable statistic to use as an estimator or a test statistic, then\n",
      "work out its distribution under the given assumptions and under any null hy-\n",
      "pothesis, and, knowing that distribution, assess its goodness for the particular\n",
      "application and determine levels of conﬁdence to associate with our inference.\n",
      "In many of interesting problems in statistical inference we cannot do this,\n",
      "usually because the distributions are not tractable.\n",
      "It is often easy, however, to determine the limiting distribution of a statis-\n",
      "tic. In that case, we can base an approximate inference on the asymptotic\n",
      "properties of the statistic. This is asymptotic inference.\n",
      "The Basic Setup and Notation\n",
      "As usual in statistical inference, we have a family of probability distributions\n",
      "P = {Pθ}, where θ may be some parameter in a real-valued parameter space\n",
      "Θ (“parametric inference”), or θ may just be some index in an index set\n",
      "I to distinguish one distribution, Pθ1, from another, Pθ2 (“nonparametric\n",
      "inference”). The parameter or the index is not observable; however, we assume\n",
      "Pθ1 ̸= Pθ2 if θ1 ̸= θ2 (“identiﬁability”).\n",
      "We have an observable random variable X. We have a random sample,\n",
      "X1, . . ., Xn, which we may also denote by X; that is, we may use X not just\n",
      "as the random variable (that is, a Borel function on the sample space) but\n",
      "also as the sample: X = X1, . . ., Xn.\n",
      "Both θ and X may be vectors. (Recall that I use “real-valued” to mean\n",
      "either a scalar (that is, an element in IR) or a real-valued vector (that is, an\n",
      "element in IRk, where k is a positive integer possibly larger than 1).)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.8 Asymptotic Inference\n",
      "307\n",
      "The canonical problem in parametric inference is to estimate g(θ) or to\n",
      "test an hypothesis concerning g(θ), where g is some real-valued measurable\n",
      "function.\n",
      "We denote the statistic (possibly an estimator or a test statistic) as Tn(X),\n",
      "or just Tn. We also use the same symbol to denote the sequence of statistics,\n",
      "although to emphasize the sequence, as opposed to the nth term in the se-\n",
      "quence, we may write {Tn}.\n",
      "We will often be interested in weak convergence, and in that case the order\n",
      "of convergence O(f(n)) as in Example 1.23 on page 85 will be of interest.\n",
      "3.8.1 Consistency\n",
      "Consistency is a general term used for various types of asymptotic convergence\n",
      "and has diﬀerent meanings for diﬀerent statistical procedures. Unless it is clear\n",
      "from the context, we must qualify the word “consistency” with the type of\n",
      "convergence and with the type of inference. We speak of consistent point\n",
      "estimators and consistent tests of hypotheses.\n",
      "In this section we will discuss consistency of point estimators. This relates\n",
      "to the convergence of the sequence of estimators Tn(X) to the estimand g(θ),\n",
      "and these types correspond directly to those discussed in Section 1.3.3. We will\n",
      "consider consistency of hypothesis tests and the related concepts of asymptotic\n",
      "correctness and accuracy in Chapter 7.\n",
      "Convergence is deﬁned with respect to a distribution. In a problem of\n",
      "statistical inference we do not know the distribution, only the distributional\n",
      "family, P. To speak of consistency, therefore, we require that the convergence\n",
      "be with respect to every distribution in P.\n",
      "The three most common kinds of consistency for point estimators are weak\n",
      "consistency, strong consistency, and Lr-consistency.\n",
      "Deﬁnition 3.14 (weak consistency)\n",
      "Tn(X) is said to be weakly consistent for g(θ) iﬀ\n",
      "Tn(X)\n",
      "p→g(θ)\n",
      "wrt any P ∈P.\n",
      "This kind of consistency involves a weak convergence. It is often what is meant\n",
      "when we refer to “consistency” without a qualiﬁer. Whenever the asymptotic\n",
      "expectation of a sequence of estimators is known, consistency is usually proved\n",
      "by use of a Chebyshev-type inequality.\n",
      "Deﬁnition 3.15 (strong (or a.s.) consistency)\n",
      "Tn(X) is said to be strongly consistent for g(θ) iﬀ\n",
      "Tn(X) a.s.\n",
      "→g(θ)\n",
      "wrt any P ∈P.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "308\n",
      "3 Basic Statistical Theory\n",
      "Deﬁnition 3.16 (Lr-consistency)\n",
      "Tn(X) is said to be Lr-consistent for g(θ) iﬀ\n",
      "Tn(X)\n",
      "Lr\n",
      "→g(θ)\n",
      "wrt any P ∈P.\n",
      "Lr-convergence applies to convergence in expectation:\n",
      "lim\n",
      "n→∞E(∥Tn(X) −g(θ)∥r\n",
      "r) = 0.\n",
      "For r = 1, Lr-consistency is called consistency in mean. For r = 2, Lr-\n",
      "consistency is called consistency in mean squared error.\n",
      "The term “consistency” or “consistent” is often used without a qualiﬁer.\n",
      "While this may be considered bad practice, it is fairly common. In certain\n",
      "contexts, we often refer to weak consistency as just “consistency”, without\n",
      "the qualiﬁer. “Consistency” or “consistent” without a qualiﬁer, however, often\n",
      "means consistency or consistent in mean squared error.\n",
      "There are relationships among these types of consistency that are similar\n",
      "to those among the types of convergence. As in Figure 1.3 on page 81, we have\n",
      "Recall Example 1.21 to see that a.s. consistency does not imply consistency\n",
      "Lr\n",
      "strong\n",
      "QQQQ\n",
      "s\n",
      "\u0011\n",
      "\u0011\n",
      "\u0011\n",
      "\u0011\n",
      "+\n",
      "weak\n",
      "in mean squared error.\n",
      "For any convergent sequence, the rate of convergence is of interest. We\n",
      "quantify this rate of convergence in terms of another sequence, which is often\n",
      "some nice function of n.\n",
      "Deﬁnition 3.17 (an-consistency)\n",
      "Given a sequence of positive constants {an} with limn→∞an = ∞, Tn(X) is\n",
      "said to be an-consistent for g(θ) iﬀan(Tn(X)−g(θ)) ∈OP(1) wrt any P ∈P,\n",
      "that is,\n",
      "∀ϵ > 0 ∃constant Cϵ > 0 ∋sup\n",
      "n Pr(an∥Tn(X) −g(θ)∥≥Cϵ) < ϵ.\n",
      "Notice that this is a kind of weak consistency. Although it is common to\n",
      "include the an sequence as a scale factor, if Tn(X) is an-consistent for g(θ)\n",
      "then we could write (Tn(X) −g(θ)) ∈OP(a−1\n",
      "n ). an-consistency plays a major\n",
      "role in asymptotic inference.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.8 Asymptotic Inference\n",
      "309\n",
      "The most common kind of an-consistency that we consider is √n-consistency;\n",
      "that is, an = √n, as in Example 1.23 on page 85, where we saw that\n",
      "Xn −µ ∈OP(n−1/2).\n",
      "We are interested the limiting behavior of such properties of statistics as\n",
      "the variance, the bias, and the mean squared error. We often express sequences\n",
      "of these properties in terms of big O (not in probability) of a convergent\n",
      "sequence. The variance and mean squared error are often in O(n−1), as for\n",
      "the variance of Xn, for example. A sequence of estimators whose variance or\n",
      "mean squared error is in O(n−r) is a better sequence of estimators than one\n",
      "whose mean squared error is in O(n−s) if r > s.\n",
      "Quantities such as the variance, the bias, and the mean squared error are\n",
      "deﬁned in terms of expectations, so ﬁrstly, we need to be precise in our mean-\n",
      "ing of asymptotic expectation. In the following we will distinguish asymptotic\n",
      "expectation from limiting expectation. A related term is “approximate” ex-\n",
      "pectation, but this term is sometimes used in diﬀerent ways. Some authors\n",
      "use the term “approximately unbiased” in reference to a limiting expectation.\n",
      "Other authors and I prefer the term “unbiased in the limit” to refer to this\n",
      "property. This property is diﬀerent from asymptotically unbiased, as we will\n",
      "see.\n",
      "Consistency of a Sequence to a Sequence\n",
      "In some cases, rather than a ﬁxed estimand g(θ), we are interested in a se-\n",
      "quence of estimands gn(θ). In such cases, it may not be adequate just to con-\n",
      "sider |Tn −gn(θ)|. This would not be meaningful if, for example, gn(θ) →0.\n",
      "This kind of situation occurs, for example, when gn(θ) is the variance of the\n",
      "mean of a sample of size n from a population with ﬁnite variance. In such\n",
      "cases we could deﬁne any of the types of consistency described above using\n",
      "the appropriate type of convergence in this expression,\n",
      "|Tn/gn(θ) −1| →0.\n",
      "(3.169)\n",
      "A common situation is one in which gn(θ) is a sequence of variance-\n",
      "covariance matrices, say Σn. Because, for nondegenerate distributions, these\n",
      "are positive deﬁnite matrices, we may restrict our attention to a sequence of\n",
      "positive deﬁnite estimators, say Vn. For this case, we give a special deﬁnition\n",
      "for consistent estimators of the sequence of variance-covariance matrices.\n",
      "Deﬁnition 3.18 (consistent estimators of variance-covariance matrices)\n",
      "Let {Σn} be a sequence of k×k positive deﬁnite matrices and Vn be a positive\n",
      "deﬁnite matrix estimator of Σn for each n. Then Vn is said to be consistent\n",
      "for Σn if\n",
      "Σ−1/2\n",
      "n\n",
      "VnΣ−1/2\n",
      "n\n",
      "−Ik\n",
      "\n",
      "p→0.\n",
      "(3.170)\n",
      "Also Vn is said to be strongly consistent for Σn if\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "310\n",
      "3 Basic Statistical Theory\n",
      "Σ−1/2\n",
      "n\n",
      "VnΣ−1/2\n",
      "n\n",
      "−Ik\n",
      "\n",
      "a.s.\n",
      "→0.\n",
      "(3.171)\n",
      "Note the similarity of these expressions to expression (3.169). In many cases of\n",
      "interest ∥Σn∥→0, so these expressions are not the same as ∥Vn −Σn∥→0.\n",
      "Theorem 3.15\n",
      "Assume the conditions of Deﬁnition 3.18. Equation (3.170) holds iﬀfor every\n",
      "sequence {ln} ∈IRk,\n",
      "lT\n",
      "n Vnln\n",
      "p→1.\n",
      "(3.172)\n",
      "Proof. Exercise.\n",
      "3.8.2 Asymptotic Expectation\n",
      "Asymptotic inference is based on the asymptotic distribution of a statistic, Tn.\n",
      "Recall that the asymptotic distribution is deﬁned in terms of the convergence\n",
      "of the CDFs at each point of continuity t of the CDF of X, F : limn→∞Fn(t) =\n",
      "F (t), and an expectation can be deﬁned in terms of a CDF. The properties of\n",
      "the asymptotic distribution, such as its mean or variance, are the asymptotic\n",
      "values of the corresponding properties of Tn.\n",
      "Because {Tn} may converge to a degenerate random variable, it may be\n",
      "more useful to consider more meaningful sequences of the form {an(Xn −c)}\n",
      "as in Sections 1.3.7 and 1.3.8. Even if Tn is a normalized statistic, such as\n",
      "X, with variance of the form σ2/n, the limiting values of various properties\n",
      "of Tn may not be very useful. We need an “asymptotic variance” diﬀerent\n",
      "from limn→∞σ2/n. Hence, we deﬁned “an asymptotic expectation” in Deﬁ-\n",
      "nition 1.43 in terms of the expectation in the asymptotic distribution.\n",
      "We refer to limn→∞E(Tn) as the limiting expectation. It is important to\n",
      "recognize the diﬀerence in limiting expectation and asymptotic expectation.\n",
      "(These two terms are not always used in this precise way, so the student must\n",
      "be careful to understand the meaning of the terms in their context.)\n",
      "Notice that this deﬁnition of asymptotic expectation may allow us to ad-\n",
      "dress more general situations. For example, we may consider the asymptotic\n",
      "variance of a sequence of estimators √nTn(X). The asymptotic variance may\n",
      "be of the form V(T/n) (which we should not be tempted to say is just 0,\n",
      "because n →∞).\n",
      "Notation and Terminology\n",
      "The deﬁnition of asymptotic expectation in Deﬁnition 1.43 is a standard one.\n",
      "Terminology related to this deﬁnition, however, is not always standard. To\n",
      "illustrate, we consider a result in a common situation: √n(X −µ)\n",
      "d→N(0, σ2).\n",
      "By the deﬁnition, we would say that the asymptotic variance of X is σ2/n;\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.8 Asymptotic Inference\n",
      "311\n",
      "whereas, often in the literature (see, for example, TPE2, page 436, following\n",
      "equation (1.25)), σ2 would be called the asymptotic variance. This confusion\n",
      "in the notation is not really very important, but the reader should be aware\n",
      "of it. To me, it just seems much more logical to follow Deﬁnition 1.43 and\n",
      "call σ2/n the asymptotic variance. (See Jiang (2010), page 19, for additional\n",
      "comments on the terminology.)\n",
      "3.8.3 Asymptotic Properties and Limiting Properties\n",
      "After deﬁning asymptotic expectation, we noted an alternative approach\n",
      "based on a limit of the expectations, which we distinguished by calling it\n",
      "the limiting expectation. These two types of concepts persist in properties of\n",
      "interest that are deﬁned in terms of expectations, such as bias and variance\n",
      "and their combination, the mean squared error.\n",
      "One is based on the asymptotic distribution and the other is based on\n",
      "limiting moments. Although in some cases they may be the same, in general\n",
      "they are diﬀerent, as we will see.\n",
      "Asymptotic Bias and Limiting Bias\n",
      "Now consider a sequence of estimators {Tn(X)} for g(θ), with E(|Tn|) < ∞,\n",
      "in the family of distributions P = {Pθ}. We deﬁne the limiting bias of {Tn}\n",
      "within the family P to be limn→∞E(Tn) −g(θ).\n",
      "Suppose Tn(X)\n",
      "d→T and E(|T|) < ∞. The limiting bias of {Tn} within\n",
      "the family P is E(T) −g(θ).\n",
      "Notice that the bias may be a function of θ; that is, it may depend on the\n",
      "speciﬁc distribution within P.\n",
      "If the limiting bias is 0 for any distribution within P, we say {Tn(X)} is\n",
      "unbiased for g(θ) in the limit.\n",
      "It is clear that if Tn(X) is unbiased for g(θ) for all n, then {Tn(X)} is\n",
      "unbiased for g(θ) in the limit.\n",
      "We can easily construct an estimator that is biased in any ﬁnite sample,\n",
      "but is unbiased in the limit. Suppose we want an estimator of the mean µ\n",
      "(which we assume is ﬁnite). Let\n",
      "Tn = Xn + c\n",
      "n,\n",
      "for some c ̸= 0. Now, the bias for any n is c/n. The limiting bias of Tn for µ,\n",
      "however, is 0, and since this does not depend on µ, we say it is unbiased in\n",
      "the limit.\n",
      "To carry this further, suppose X1, . . ., Xn\n",
      "iid\n",
      "∼N(µ, σ2), and with\n",
      "Tn = Xn + c\n",
      "n\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "312\n",
      "3 Basic Statistical Theory\n",
      "as above, form √n(Tn −µ) = √n(Xn −µ) + c/√n. We know √n(Xn −µ) d→\n",
      "N(0, σ2) and c/√n →0, so by Slutsky’s theorem,\n",
      "√n(Tn −µ)\n",
      "d→N(0, σ2).\n",
      "Hence, the limiting bias of Tn for µ is also 0, and since this does not depend\n",
      "on µ, we say it is unbiased in the limit.\n",
      "We deﬁne the asymptotic bias in terms of the asymptotic expectation of\n",
      "{Tn} given a sequence of positive constants {an} with limn→∞an = ∞or\n",
      "with limn→∞an = a > 0, and such that anTn(X)\n",
      "d→T. An asymptotic bias\n",
      "of {Tn} is E(T −g(θ))/an.\n",
      "If E(T −g(θ))/an = 0, we say {Tn(X)} is asymptotically unbiased for g(θ).\n",
      "It is clear that if Tn(X) is unbiased for g(θ) for any n, then {Tn(X)} is\n",
      "asymptotically unbiased for g(θ).\n",
      "Example 3.25 unbiased in limit but asymptotically biased\n",
      "To illustrate the diﬀerence in asymptotic bias and limiting bias, consider\n",
      "X1, . . ., Xn\n",
      "iid\n",
      "∼U(0, θ), and the estimator X(n) (which we know to be suﬃcient\n",
      "for g(θ) = θ). We can work out the asymptotic distribution of n(θ−X(n)) to be\n",
      "exponential with parameter θ. (The distributions of the order statistics from\n",
      "the uniform distribution are betas. These distributions are interesting and you\n",
      "should become familiar with them.) Hence, X(n) is asymptotically biased. We\n",
      "see, however, that the limiting bias is limn→∞E(X(n) −θ) = n−1\n",
      "n θ −θ = 0;\n",
      "that is, X(n) is unbiased in the limit.\n",
      "Notice the role that the sequence {an} plays. This would allow us to con-\n",
      "struct a sequence that is biased in the limit, but is asymptotically unbiased.\n",
      "Consistency\n",
      "There are also, of course, relationships between consistency and limiting bias.\n",
      "Unbiasedness in the limit implies consistency in mean.\n",
      "Example 3.26 consistency and limiting and asymptotic bias\n",
      "Consider X1, . . ., Xn\n",
      "iid\n",
      "∼N(µ, σ2), and an estimator of the mean\n",
      "Sn = Xn +\n",
      "c\n",
      "√n,\n",
      "for some c ̸= 0. (Notice this estimator is slightly diﬀerent from Tn above.) As\n",
      "above, we see that this is unbiased in the limit (consistent in the mean), and\n",
      "furthermore, we have the mean squared error\n",
      "MSE(Sn, µ) = E((Sn −µ)2)\n",
      "= σ2\n",
      "n +\n",
      "\u0012 c\n",
      "√n\n",
      "\u00132\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.8 Asymptotic Inference\n",
      "313\n",
      "tending to 0, hence we see that this is consistent in mean squared error.\n",
      "However, √n(Sn −µ) = √n(Xn −µ) + c has limiting distribution N(c, σ2);\n",
      "hence Sn is asymptotically biased.\n",
      "We also note that an estimator can be asymptotically unbiased but not\n",
      "consistent in mean squared error. In Example 3.26, we immediately see that\n",
      "X1 is asymptotically unbiased for µ, but it is not consistent in mean squared\n",
      "error for µ.\n",
      "Another interesting example arises from a distribution with slightly heavier\n",
      "tails than the normal.\n",
      "Example 3.27 consistency and limiting bias\n",
      "Consider the double exponential distribution with θ = 1, and the estimator\n",
      "of the mean\n",
      "Rn(X) = X(n) + X(1)\n",
      "2\n",
      ".\n",
      "(This is the mid-range.) We can see that Rn is unbiased for any ﬁnite sample\n",
      "size (and hence, is unbiased in the limit); however, we can show that\n",
      "V(Rn) = π2\n",
      "12,\n",
      "and, hence, Rn is not consistent in mean squared error.\n",
      "Asymptotic Variance, Limiting Variance, and Eﬃciency\n",
      "We deﬁne the asymptotic variance and the limiting variance in similar ways\n",
      "as in deﬁning the asymptotic bias and limiting bias, and we also note that\n",
      "they are diﬀerent from each other. We also deﬁne asymptotic mean squared\n",
      "error and the limiting mean squared error in a similar fashion. The limiting\n",
      "mean squared error is of course related to consistency in mean squared error.\n",
      "Our interest in asymptotic (not “limiting”) variance or mean squared error\n",
      "is as they relate to optimal properties of estimators. The “eﬃciency” of an\n",
      "estimator is related to its mean squared error.\n",
      "Usually, rather than consider eﬃciency in a absolute sense, we use it to\n",
      "compare two estimators, and so speak of the relative eﬃciency. When we\n",
      "restrict our attention to unbiased estimators, the mean-squared error is just\n",
      "the variance, and in that case we use the phrase eﬃcient or Fisher eﬃcient\n",
      "(Deﬁnition 3.8) to refer to an estimator that attains its Cram´er-Rao lower\n",
      "bound (the right-hand side of inequality (B.25) on page 854.)\n",
      "As before, assume a family of distributions P, a sequence of estimators\n",
      "{Tn} of g(θ, and a sequence of positive constants {an} with limn→∞an = ∞\n",
      "or with limn→∞an = a > 0, and such that anTn(X)\n",
      "d→T and 0 < E(T) < ∞.\n",
      "We deﬁne the asymptotic mean squared error of {Tn} for estimating g(θ) wrt\n",
      "P as an asymptotic expectation of (Tn −g(θ))2; that is, E((T −g(θ))2)/an,\n",
      "which we denote as AMSE(Tn, g(θ), P).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "314\n",
      "3 Basic Statistical Theory\n",
      "For comparing two estimators, we may use the asymptotic relative eﬃ-\n",
      "ciency. The asymptotic relative eﬃciency of the estimators Sn and Tn for\n",
      "g(θ) wrt P is deﬁned as\n",
      "ARE(Sn, Tn) = AMSE(Sn, g(θ), P)/AMSE(Tn, g(θ), P).\n",
      "(3.173)\n",
      "The ARE is essentially a scalar concept; for vectors, we usually do one at\n",
      "a time, ignoring covariances.\n",
      "Asymptotic Signiﬁcance\n",
      "For use of asymptotic approximations for conﬁdence sets and hypothesis test-\n",
      "ing, we need a concept of asymptotic signiﬁcance. As in the case of exact\n",
      "signiﬁcance, the concepts in conﬁdence sets and hypothesis tests are essen-\n",
      "tially the same.\n",
      "We assume a family of distributions P, a sequence of statistics {Tn}, and\n",
      "a sequence of tests {δ(Xn)} based on the iid random variables X1, . . ., Xn.\n",
      "The test statistic δ(·) is deﬁned in terms the decisions; it takes the value 1\n",
      "for the case of deciding to reject H0 and conclude H1, and the value 0 for the\n",
      "case of deciding not to reject H0.\n",
      "Asymptotic Properties of Tests\n",
      "In hypothesis testing, the standard setup is that we have an observable random\n",
      "variable with a distribution in the family P. Our hypotheses concern a speciﬁc\n",
      "member P ∈P. We have a null hypothesis\n",
      "H0 :\n",
      "P ∈P0\n",
      "and an alternative hypothesis\n",
      "H1 :\n",
      "P ∈P1,\n",
      "where P0 ⊆P, P1 ⊆P, and P0 ∩P1 = ∅.\n",
      "Deﬁnition 3.19 (limiting size)\n",
      "Letting β(δ(Xn), P ) be the power function,\n",
      "β(δ(Xn), P ) = PrP (δ(Xn) = 1).\n",
      "We deﬁne\n",
      "lim\n",
      "n→∞sup\n",
      "P∈P0\n",
      "β(δ(Xn), P ),\n",
      "(3.174)\n",
      "if it exists, as the limiting size of the test.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "3.8 Asymptotic Inference\n",
      "315\n",
      "Deﬁnition 3.20 (asymptotic signiﬁcance)\n",
      "Given the power function β(δ(Xn), P ). If\n",
      "lim sup\n",
      "n\n",
      "β(δ(Xn), P ) ≤α ∀P ∈P0,\n",
      "(3.175)\n",
      "then α is called an asymptotic signiﬁcance level of the test.\n",
      "Deﬁnition 3.21 (consistency)\n",
      "The sequence of tests {δ(Xn)} is consistent for the test P ∈P0 versus P ∈P1\n",
      "iﬀ\n",
      "lim\n",
      "n→∞(1 −β(δ(Xn), P )) = 0 ∀P ∈P1.\n",
      "(3.176)\n",
      "Deﬁnition 3.22 (uniform consistency)\n",
      "The sequence of tests {δn} is uniformly consistent iﬀ\n",
      "lim\n",
      "n→∞sup\n",
      "P∈P1\n",
      "(1 −β(δn, P )) = 0.\n",
      "Asymptotic Properties of Conﬁdence Sets\n",
      "Let C(X) be a conﬁdence set for g(θ).\n",
      "Deﬁnition 3.23 (asymptotic signiﬁcance level)\n",
      "If\n",
      "lim inf\n",
      "n\n",
      "Pr(C(X) ∋g(θ)) ≥1 −α ∀P ∈P,\n",
      "(3.177)\n",
      "then 1 −α is an asymptotic signiﬁcance level of C(X).\n",
      "Deﬁnition 3.24 (limiting conﬁdence coeﬃcient)\n",
      "If\n",
      "lim\n",
      "n→∞inf\n",
      "P∈P Pr(C(X) ∋g(θ))\n",
      "(3.178)\n",
      "exists, then it is the limiting conﬁdence coeﬃcient of C(X).\n",
      "“The” Asymptotic Distribution\n",
      "In determining asymptotic conﬁdence sets or asymptotic relative eﬃciencies,\n",
      "we need expressions that do not depend on unknown parameters. This fact\n",
      "determines which asymptotic expectations are useful.\n",
      "The asymptotic expectation of some sequence of statistics, or of pivotal\n",
      "quantities, is determined by the sequence {an} (used above in the deﬁnitions).\n",
      "In the univariate delta method, for example, we ﬁnd a quantity an(g(Xn)−\n",
      "g(c)) that converges in distribution to N(0, v), where v does not depend on\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "316\n",
      "3 Basic Statistical Theory\n",
      "an unknown parameter. In that case, we can set a conﬁdence interval based\n",
      "on the approximate distribution of g(Xn) as N(g(c), v/a2\n",
      "n).\n",
      "To speak of the asymptotic distribution of an(g(Xn) −g(c)) is clear; but\n",
      "to refer to “the” asymptotic distribution of g(Xn) is somewhat less so.\n",
      "Because it is the useful approximate distribution resulting from asymp-\n",
      "totic expectations, we often say that “the asymptotic distribution” of g(Xn)\n",
      "is N(g(c), v/a2\n",
      "n). You should recognize that “the” in this statement is some-\n",
      "what arbitrary. It might be better to call it “the asymptotically approximate\n",
      "distribution that I’m going to use in this application”.\n",
      "Again, we should distinguish “asymptotic” from “limiting”.\n",
      "In the example of the delta method above, it is likely that\n",
      "g(Xn)\n",
      "d→g(c);\n",
      "that is, g(Xn) converges in distribution to the constant g(c); or the limiting\n",
      "distribution of g(Xn) is degenerate at g(c). “The” asymptotic variance is 0.\n",
      "**** discuss expansion of statistical functionals *** refer to Serﬂing\n",
      "This would not be very useful in asymptotic inference. We therefore seek\n",
      "“an” asymptotic variance that is more useful. In asymptotic estimation using\n",
      "g(Xn), we begin with an expression of the form an(g(Xn) −g(c)) that has\n",
      "a limiting distribution of the desired form (usually that means such that the\n",
      "variance does not involve any unknown parameters and it does not involve n).\n",
      "If this distribution is in a location-scale family, then we make the appropriate\n",
      "linear transformation (which probably results in a variance that does involve\n",
      "n).\n",
      "We then often refer to this as the asymptotic distribution of g(Xn). Some-\n",
      "times, as mentioned above, however, the limiting distribution of g(Xn) is\n",
      "degenerate.\n",
      "This is not to imply that asymptotic expectations are entirely arbitrary.\n",
      "Proposition 2.3 in MS2 shows that there is a certain uniqueness in the asymp-\n",
      "totic expectation. This proposition involves three cases regarding whether the\n",
      "expectation of g(Xn) (without the an sequence) is 0. In the example above,\n",
      "we have a degenerate distribution, and hence the asymptotic expectation that\n",
      "deﬁnes the asymptotic variance is 0.\n",
      "3.8.4 Properties of Estimators of a Variance Matrix\n",
      "If the statistic is a vector, we need an estimator of the variance-covariance ma-\n",
      "trix. Because a variance-covariance matrix is positive deﬁnite, it is reasonable\n",
      "to consider only estimators that are positive deﬁnite a.s.\n",
      "We have deﬁned what it means for such an estimator to be consistent\n",
      "(Deﬁnition 3.18 on page 309).\n",
      "Theorem 3.16\n",
      "conditions for the consistency of substitution estimators.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "317\n",
      "Proof.\n",
      "***************\n",
      "Theorem 3.17\n",
      "Given a sequence of estimators {Tn} of {gn(θ)} with variance-covariance ma-\n",
      "trices {Σn}, if\n",
      "Σ−1/2\n",
      "n\n",
      "(Tn −gn(θ))\n",
      "d→N(0, Ik),\n",
      "and if Vn is consistent for Σn, then\n",
      "V −1/2\n",
      "n\n",
      "(Tn −gn(θ))\n",
      "d→N(0, Ik).\n",
      "(3.179)\n",
      "Proof.\n",
      "***************\n",
      "Notes and Further Reading\n",
      "The general problem of statistical inference, that is, the use of observed data\n",
      "for which we have a family of probability distributions to provide information\n",
      "about those probability distributions, is an “inverse problem”. Nineteenth and\n",
      "twentieth century scientists who made inferences about probability models\n",
      "referred to the problem as one of “inverse probability”. Statisticians in the\n",
      "early twentieth century also used this term. Although the maximum likelihood\n",
      "approach could be thought of as a method of inverse probability, R. A. Fisher,\n",
      "who developed likelihood methods, made a distinction between the methods\n",
      "and “inverse probability” as a general term fell into disuse.\n",
      "Foundations\n",
      "Although statistical science has been very successful in addressing real-world\n",
      "problems, there are some issues at the foundations of statistics that remain\n",
      "somewhat controversial. One of these issues is the incorporation of subjectivity\n",
      "in statistical analysis, and another is the relevance of certain principles, such\n",
      "as “conditionality” and suﬃciency in statistical inference.\n",
      "In Chapter 1 I took the view that probability theory is an area of pure\n",
      "mathematics; hence, given a consistent axiomatic framework, “beliefs” are\n",
      "irrelevant. Distinctions between “objectivists” and “subjectivists” have no\n",
      "place in probability theory.\n",
      "In statistics, however, this is a diﬀerent matter. Instead of a vague notion of\n",
      "“subjective probability”, we may explicitly incorporate the subjectivity in our\n",
      "decisions, that is, in our statistical inference. Press and Tanur (2001) argue\n",
      "that scientists have never behaved fully objectively, but rather, some of the\n",
      "greatest scientiﬁc minds have relied on intuition, hunches, and personal beliefs\n",
      "to understand empirical data. These subjective inﬂuences have often aided\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "318\n",
      "3 Basic Statistical Theory\n",
      "in the greatest scientiﬁc achievements. Press and Tanur (2001) also express\n",
      "the view that science will advance more rapidly if the methods of Bayesian\n",
      "statistical analysis, which may incorporate subjective beliefs, are used more\n",
      "widely. In this chapter, we have laid a framework for Bayesian approaches,\n",
      "and in Chapter 4, we will discuss it more fully.\n",
      "Samaniego (2010) provides an interesting comparison of the general Bayesian\n",
      "and frequentist approaches to estimation.\n",
      "Evidence and Decision Making\n",
      "The “conditionality” principle was formulated by Birnbaum (1962) as a con-\n",
      "nection between the suﬃciency principle and the likelihood principle. The\n",
      "context of the conditionality principle is a set of possible experiments (de-\n",
      "signs, or data-generating processes) for obtaining data for statistical inference.\n",
      "The conditionality principle basically states that if a particular experiment is\n",
      "selected randomly (that is, independent of any observations), then only the\n",
      "experiment actually performed is relevant. Berger and Wolpert (1988) discuss\n",
      "these principles and their relationships to each other.\n",
      "The likelihood principle, alluded to in Example 3.12 on page 237 and stated\n",
      "on page 245, is perhaps the principle that brings into sharpest contrast some\n",
      "diﬀerent fundamental approaches to statistical inference. To many statisti-\n",
      "cians, general statistical principles that focus on the observed data rather\n",
      "than on the data-generating process often miss salient points about the pro-\n",
      "cess. Lucien Le Cam (in Berger and Wolpert (1988), page 185.1) expressed a\n",
      "common opinion among statisticians, “One should keep an open mind and be\n",
      "a bit ‘unprincipled’.”\n",
      "Royall (1997) evidence versus hypothesis testing ********\n",
      "Information\n",
      "Fisher information is the most familiar kind of information to most statisti-\n",
      "cians, but information that is related to entropy (see Section 1.1.5) is often\n",
      "used as a basis for statistical inference. Sooﬁ(1994) discusses some subtleties\n",
      "in the diﬀerent deﬁnitions of information that are ultimately based on Shan-\n",
      "non information.\n",
      "General Approaches\n",
      "While I have classiﬁed the general approaches to statistical inference into\n",
      "ﬁve groups, there are obviously other classiﬁcations, and, in any event, there\n",
      "are overlaps among the classes; for example, approaches based on empirical\n",
      "likelihood follow ideas from both likelihood and ECDF methods. In subsequent\n",
      "chapters, we will have more to say about each of these approaches, especially\n",
      "a decision-theoretic approach and use of a likelihood function.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "319\n",
      "Least-squares estimation was ﬁrst studied systematically by C. F. Gauss in\n",
      "the early 1800’s in the context of curve ﬁtting. The name of the Gauss-Markov\n",
      "theorem reminds us of his work in this area.\n",
      "Karl Pearson around the beginning of the twentieth century promoted esti-\n",
      "mation based on ﬁtting moments. These methods are the earliest and simplest\n",
      "of the general plug-in methods. This class of methods is also called “analog”\n",
      "estimation (see Manski (1988) for a general discussion in a speciﬁc area of\n",
      "application). The good statistical properties of the ECDF lend appeal to plug-\n",
      "in methods. The strong uniform convergence given in the Glivenko-Cantelli\n",
      "theorem is another illustration of Littlewood’s third principle regarding the\n",
      "“nearly” uniform convergence of pointwise convergent sequences.\n",
      "R. A. Fisher developed and studied maximum likelihood methods in the\n",
      "1920’s. These are probably the most widely-used statistical methods across a\n",
      "broad range of applications.\n",
      "Ideas and approaches developed by engineers and physical scientists lead\n",
      "to statistical methods characterized by maximum entropy. Much of this work\n",
      "dates back to Claude Shannon in the 1930’s. E. T. Jaynes in the 1950’s formal-\n",
      "ized the approach and incorporated it in a Bayesian framework. His posthu-\n",
      "mous book edited by G. Larry Bretthorst (Jaynes, 2003) is a very interesting\n",
      "discussion of a view toward probability that leads to a Bayesian maximum en-\n",
      "tropy principle for statistical inference. We will discuss the maximum entropy\n",
      "principle in more detail in the context of Bayesian priors in Section 4.2.5,\n",
      "beginning on page 346. Wu (1997) expands on the use of the maximum en-\n",
      "tropy principle in various areas of application. Pardo (2005) gives an extensive\n",
      "overview of the use of functionals from information theory in statistical infer-\n",
      "ence. In some disciplines, such as electrical engineering, this approach seems\n",
      "to arrive very naturally.\n",
      "Pitman’s measure of closeness was introduced in 1937. The idea did not\n",
      "receive much attention until the article by Rao (1981), in which was given\n",
      "the deﬁnition we have used, which is slightly diﬀerent from Pitman’s. Pit-\n",
      "man’s original article was reproduced in a special issue of Communications in\n",
      "Statistics (Pitman, 1991) devoted to the topic of Pitman closeness. The lack\n",
      "of transitivity of Pitman’s closeness follows from Arrow’s “impossibility the-\n",
      "orem”, and is a natural occurrence in paired comparisons (see David (1988)).\n",
      "The example on page 220 is called a “cyclical triad”.\n",
      "David and Salem had considered estimators similar to (3.20) for a normal\n",
      "mean in 1973, and in David and Salem (1991) they generalized these shrunken\n",
      "estimators to estimators of the means that are Pitman-closer than the sample\n",
      "mean in a broad class of location families.\n",
      "The basic ideas of the “decision theory” approach, such as risk and admis-\n",
      "sibility, were organized and put forth by Wald (1950). Wald showed that many\n",
      "of the classical statistical methods, such as hypothesis testing and even de-\n",
      "sign of experiments, could be developed within the context of decision theory.\n",
      "Wald also related many of the basic ideas of decision theory to game the-\n",
      "ory for two-person games, and Blackwell and Girshick (1954) and Ferguson\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "320\n",
      "3 Basic Statistical Theory\n",
      "(1967) expanded on these relations. In a two-person game, one player, “na-\n",
      "ture”, chooses action “θ” and the other player, “the statistician”, chooses\n",
      "action “a” and the elements of the payoﬀmatrix are the values of the loss\n",
      "function evaluated at θ and a.\n",
      "Complete Class Theorems\n",
      "Wald, starting in Wald (1939) and especially in Wald (1947a), gave the ﬁrst\n",
      "characterizations of a class of decision rules that are complete or are es-\n",
      "sentially complete. These theorems are collected in Wald (1950). See also\n",
      "Kiefer (1953), who proved some additional properties of complete classes, and\n",
      "Le Cam (1955), who relaxed some of the assumptions in the theorems.\n",
      "Estimating Functions and Generalized Estimating Equations\n",
      "The idea of an estimating function is quite old; a simple instance is in\n",
      "the method of moments. A systematic study of estimating functions and\n",
      "their eﬃciency was begun independently by Godambe (1960) and Durbin\n",
      "(1960). Small and Wang (2003) provide a summary of estimating functions\n",
      "and their applications. Estimating functions also play a prominent role in\n",
      "quasi-likelihood methods, see Heyde (1997). We will discuss this further in\n",
      "Chapter 6.\n",
      "Unbiasedness\n",
      "The concept of unbiasedness in point estimation goes back to Gauss in the\n",
      "early nineteenth century, who wrote of ﬁtted points with no “systematic er-\n",
      "ror”. Although nowadays unbiasedness is most often encountered in the con-\n",
      "text of point estimation, the term “unbiased” was actually ﬁrst used by statis-\n",
      "ticians to refer to tests (Neyman and Pearson, 1936, cited in Lehmann (1951)),\n",
      "then used to refer to conﬁdence sets (Neyman, 1937, cited in Lehmann (1951)),\n",
      "and later introduced to refer to point estimators (David and Neyman, 1938,\n",
      "cited in Lehmann (1951)). See Halmos (1946) and Lehmann (1951) for gen-\n",
      "eral discussions, and see page 296 for unbiased tests and page 300 for unbiased\n",
      "conﬁdence sets. The idea of unbiasedness of an estimating function was intro-\n",
      "duced by Kendall (1951).\n",
      "In a decision-theoretic framework, L-unbiasedness provides an underlying\n",
      "unifying concept.\n",
      "Equivariant and Invariant Statistical Procedures\n",
      "Equivariant and invariant statistical models have a heuristic appeal in ap-\n",
      "plications. The basic ideas of invariance and the implications for statistical\n",
      "inference are covered in some detail in the lectures of Eaton (1989).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "321\n",
      "Approximations and Asymptotic Inference\n",
      "In many cases of interest we cannot work out the distribution of a particular\n",
      "statistic. There are two ways that we can proceed. One is to use computer\n",
      "simulation to estimate properties of our statistic. This approach is called com-\n",
      "putational inference (see Gentle (2009)). The other approach is to make some\n",
      "approximations, either of the underlying assumptions or for the unknown dis-\n",
      "tribution.\n",
      "Some approximations are just based on known similarities between two\n",
      "distributions. The most common kind of approximation, however, is based on\n",
      "the asymptotic or “large-sample” properties of the statistic. This approach of\n",
      "asymptotic inference, as discussed in Section 3.8, is generally quite straight-\n",
      "forward and so it has widespread usage.\n",
      "It is often diﬃcult to know how the asymptotic properties relate to the\n",
      "properties for any given ﬁnite sample. The books by Barndorﬀ-Nielson and Cox\n",
      "(1994), DasGupta (2008), Jiang (2010), Lehmann (1999), Serﬂing (1980), and\n",
      "van der Vaart (1998) provide extensive coverage of asymptotic inference.\n",
      "Variance Estimation\n",
      "A sandwich-type estimator was introduced introduced by Eiker (1963) for\n",
      "estimation of the variance-covariance matrix of the least-squares estimator\n",
      "of the coeﬃcient vector in linear regression in the case where the errors are\n",
      "uncorrelated, but possibly have diﬀerent distributions. Huber (1967) used a\n",
      "similar kind of estimator as a robust estimator. White (1980) introduced a\n",
      "similar estimator for heteroscedastic situations in economics. The term “sand-\n",
      "wich estimator” was introduced in the context of estimation of the variance-\n",
      "covariance matrix for the solution of a generalized estimation equation, and\n",
      "it is widely used in that type of problem.\n",
      "Subsampling and Resampling\n",
      "The idea of the jackknife goes back to Quenouille in 1949. The ordinary stan-\n",
      "dard (delete-1) jackknife was popularized by John Tukey for both bias correc-\n",
      "tion and variance estimation. (Tukey, of course, gave it the poetic name.) It is\n",
      "currently widely-used, especially in sample surveys. The delete-d (d > 1) jack-\n",
      "knife was introduced and studied by Wu (1986). Shao and Tu (1995) provide\n",
      "an extensive discussion of the jackknife.\n",
      "The theory and methods of the bootstrap were largely developed by Efron,\n",
      "and Efron and Tibshirani (1993) introduce the principles and discuss many\n",
      "extensions and applications.\n",
      "Predictive Inference and Algorithmic Statistical Models\n",
      "Geisser (1993) argues that predictive inference is a more natural problem\n",
      "that the ordinary objective in statistical inference of identifying a subfamily\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "322\n",
      "3 Basic Statistical Theory\n",
      "of distributions that characterizes the data-generating process that gave rise\n",
      "to the observed data as well as future observations from that process.\n",
      "Breiman (2001) emphasizes the role of algorithmic models when the ob-\n",
      "jective is prediction instead of a simple descriptive model with a primary aim\n",
      "of aiding understanding.\n",
      "Exercises\n",
      "3.1. Show that the estimator (3.20) is Pitman-closer to µ than is X.\n",
      "3.2. a) Suppose T1(X) and T2(X) have continuous symmetric PDFs p1(t−θ)\n",
      "and p2(t −θ) (that is, their distributions are both location families).\n",
      "Suppose further that p1(0) > p2(0). Show that for some ϵ > 0\n",
      "Pr(|T1 −θ| < ϵ) > Pr(|T2 −θ| < ϵ).\n",
      "b) Is T1 in question 3.2a Pitman-closer to θ than T2? Tell why or why\n",
      "not.\n",
      "c) Now suppose X1 and X2 are iid with a continuous symmetric PDF p.\n",
      "Let T1(X) = X1 and T2(X) = (X1 + X2)/2. Show that if\n",
      "2\n",
      "Z\n",
      "(p(x))2dx < p(0),\n",
      "then for some ϵ\n",
      "Pr(|T1 −θ| < ϵ) > Pr(|T2 −θ| < ϵ).\n",
      "3.3. a) Prove that if T(X) is a suﬃcient statistic for θ and if Y (X) is dis-\n",
      "tributed independently of T(X), then the distribution of Y does not\n",
      "depend on θ.\n",
      "b) Does a converse hold? State and prove, or show why not.\n",
      "3.4. Let T be a suﬃcient statistic for P . Prove the statements made on page 222\n",
      "about functions and suﬃcient statistics; speciﬁcally,\n",
      "a) show by example that W = f(T) for some function f is not necessarily\n",
      "a suﬃcient statistic for P ; however\n",
      "b) if T = g(S), where g is a measurable function and S is a statistic,\n",
      "then S is suﬃcient for P .\n",
      "3.5. Show that T1(X) and T2(X) in Example 3.6 on page 227 are both suﬃcient\n",
      "and complete for θ.\n",
      "3.6. Show that T(X) in Example 3.7 on page 227 is suﬃcient and complete\n",
      "for θ.\n",
      "3.7. Work out the information matrix for θ = (µ, σ) in the N(µ, σ2) family\n",
      "using\n",
      "a) the expectation of the product of ﬁrst derivatives with respect to the\n",
      "parameters\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "323\n",
      "b) the expectation of the second derivatives with respect to the parame-\n",
      "ters\n",
      "c) the integrals of the derivatives with respect to the variable (which is\n",
      "an expectation).\n",
      "3.8. Determine the Hessian of the log-likelihood in Example 3.13, and show\n",
      "that at the stationary point it is negative deﬁnite. (The oﬀ-diagonals are\n",
      "0.)\n",
      "3.9. Suppose T(X) has ﬁnite ﬁrst and second moments, and let g(θ) =\n",
      "E(T(X)). Show that for the estimator eT(X) = aT(X) + b when a < 0,\n",
      "R(g(θ), eT) > R(g(θ), 0);\n",
      "that is, eT(X) is inadmissible for any b because it is dominated by the\n",
      "estimator T(X) ≡0.\n",
      "3.10. What can you say about a point estimator that is admissible under an\n",
      "absolute-error loss? (Compare Theorem 3.10.)\n",
      "3.11. Show that the expression (3.166) is greater than or equal to [\n",
      "V(T)J.\n",
      "3.12. Assume a random sample of size n > 2 from a distribution with PDF of\n",
      "the form\n",
      "p(x; θ) = f(x)\n",
      "h(θ) I(0, θ)(x).\n",
      "a) Show that X(n) is not unbiased for θ.\n",
      "b) Show that T = 2X(n) −X(n−1) is unbiased to order O(n−1).\n",
      "c) Show that the asymptotic risk with squared error loss for T and X(n)\n",
      "are the same to O(n−2).\n",
      "3.13. Prove Theorem 3.15.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4\n",
      "Bayesian Inference\n",
      "We have used an urn process to illustrate several aspects of probability and\n",
      "sampling. An urn that contains balls of diﬀerent colors can be used to illustrate\n",
      "a primitive notion of probability – “What is the probability of drawing a red\n",
      "ball?” – that can be integrated into our axiomatic development of probability\n",
      "(as a set measure). Almost 250 years ago Pierre-Simon Laplace, the French\n",
      "mathematician and astronomer, considered the urn problem and asked a very\n",
      "diﬀerent question: “Given that there are n balls in the urn, some of which are\n",
      "red, if the ﬁrst ball drawn is red, what is the probability that the proportion\n",
      "of red balls, P , is p0 (some constant)?” While this form of question may be\n",
      "quite natural to a layman, it is not consistent with our notion of probability.\n",
      "There is a ﬁxed number of red balls in the urn; the proportion P is either p0\n",
      "or it is not.\n",
      "Even if we adhere to our deﬁnitions of “probability”, we should be able\n",
      "to rephrase this question into one for which statistical decision theory should\n",
      "provide an answer. We might feel more comfortable, however, using diﬀer-\n",
      "ent words, and maybe even asking about “subjective probability” or “belief”\n",
      "about the proportion of red balls in the urn. Laplace went on to answer the\n",
      "question in a manner that we will identify later as a systematic approach to\n",
      "such problems:\n",
      "Pr(P = p0|ﬁrst ball red) =\n",
      "p0/(n −2)\n",
      "P(n−1)/n\n",
      "p=2/n\n",
      "p/(n −2)\n",
      "=\n",
      "p0\n",
      "n(n −1)/2 −1.\n",
      "Another question that Laplace addressed concerned the probability π that\n",
      "a human birth would be male. From the point of view that this is a random\n",
      "process, the word “probability” in this context is consistent with our under-\n",
      "standing of the word. Laplace, however, went on to pose the question, “What\n",
      "is the probability that the probability π is less than or equal to one half?”\n",
      "Whether he felt it was relevant or not, he did not remark on the diﬀerences\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "326\n",
      "4 Bayesian Inference\n",
      "in the meanings of “probability” in the question. Laplace’s answer to this\n",
      "question, based on the observed numbers of male and female births in Paris\n",
      "during a period in the second half of the eighteenth century, is the same as\n",
      "we would get using Bayes theorem with a uniform prior on π. We will later\n",
      "consider other examples of this genre, so we will not elaborate on this one\n",
      "here.\n",
      "These two examples have two main points: how the word “probability” is\n",
      "used, and the statistical framework in the analysis. The setup and the analysis\n",
      "used in these problems are of the type called Bayesian inference.\n",
      "In this chapter, we will consider the general framework of Bayesian infer-\n",
      "ence. In most of the discussion in this chapter we will assume a parametric\n",
      "model; that is, we assume that the observable random variable of interest has\n",
      "a distribution in the family P = {PΘ : Θ ∈Θ ⊆IRk}.\n",
      "4.1 The Bayesian Paradigm\n",
      "The ﬁeld of Bayesian statistics has become a mainstream part of statistical\n",
      "inference. Bayesian methods allow us easily to incorporate prior information\n",
      "into the inference. One of the major ways in which Bayesian methods diﬀer\n",
      "from other statistical methods, however, is in the basic deﬁnition of the prob-\n",
      "lem. In the standard paradigm of parametric statistical inference, as expressed\n",
      "in equations (3.1) and (3.2) of Chapter 3, the objective of the inference is to\n",
      "make a decision about the values of the parameter. In Bayesian statistics, the\n",
      "parameter, that is, the index of the underlying distribution, is viewed as a\n",
      "random variable, so the canonical problem in Bayesian statistics is somewhat\n",
      "diﬀerent.\n",
      "Although we now have two random variables, Θ and X, we must not con-\n",
      "fuse them. Realizations of the random variable X are observable; realizations\n",
      "of Θ are not directly observable. Realizations of Θ determine aspects of the\n",
      "distribution of X.\n",
      "We still address the fundamental problem in statistics: beginning with a\n",
      "given distributional family P = {PΘ : Θ ∈Θ}, we use observable realizations\n",
      "of X to make inferences about how Θ ranges over Θ. Instead of the formulation\n",
      "of the problem in terms of equations (3.1) and (3.2), a formulation in terms\n",
      "of equations (3.3) and (3.4) on page 207 may be more appropriate. We begin\n",
      "with\n",
      "P = {PΘ | Θ ∼Q0 ∈Q},\n",
      "(4.1)\n",
      "where Θ is a random variable and Q0 is a “prior distribution”. Using observed\n",
      "data from a distribution PΘ that depends on Θ and relations among joint,\n",
      "marginal, and conditional distributions, we arrive at the class of populations\n",
      "PH = {PΘ | Θ ∼QH ∈Q},\n",
      "(4.2)\n",
      "where QH is some “posterior distribution” conditional on the observations.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.1 The Bayesian Paradigm\n",
      "327\n",
      "For many families of distributions of the observable random variable, there\n",
      "are corresponding families of prior distributions that yield a parametric family\n",
      "of posterior distributions that is the same as the family of priors. This means\n",
      "that Q in equations (4.1) and (4.2) represents a single parametric family. We\n",
      "call a member of such a family of priors a conjugate prior with respect to the\n",
      "conditional distributional family of the observables. Clearly, we can always\n",
      "deﬁne a family of conjugate priors. A trivial example is when Q is the family\n",
      "of all distributions on Θ. The concept of conjugate priors, however, becomes\n",
      "relevant when the family Q is fairly restrictive, especially as a parametric\n",
      "family.\n",
      "In the sense that Q0 allows direct incorporation of prior beliefs or sub-\n",
      "jective evidence, statistical inference following this paradigm is sometimes\n",
      "referred to as subjective inference.\n",
      "In Bayesian inference, as usual, we assume that PΘ, Q0, and QH are dom-\n",
      "inated by some σ-ﬁnite measure or by a mixture of σ-ﬁnite measures.\n",
      "Notation\n",
      "As we proceed, even in the simplest cases, there are ﬁve CDFs and PDFs that\n",
      "are involved in this approach. Rather than introduce specialized notation, we\n",
      "will use the same simple and consistent notation for these that we have used\n",
      "previously. The notation makes use of upper and lower case fonts of the same\n",
      "letter together with subscripts as symbols for the CDFs and PDFs of par-\n",
      "ticular random variables. Thus, for example, “fX|θ” represents the PDF of\n",
      "the observable random variable whose distribution is conditional on the ran-\n",
      "dom variable Θ, and “fΘ|x” represents the PDF of the unobservable random\n",
      "parameter whose distribution is conditional on the observations, that is, the\n",
      "“posterior PDF”. The corresponding CDFs would be represented as “FX|θ”\n",
      "and “FΘ|x”. The visual clutter of the subscripts is an overhead that is more\n",
      "than compensated for by the consistency of the notation.\n",
      "Occasionally, especially in the context of computations as in Section 4.7,\n",
      "we will use a specialized and simple notation: [X, Y ], [X|Y ], and [X] represent\n",
      "the joint, conditional, and marginal densities, respectively.\n",
      "Bayesian Inference\n",
      "The inference in going from the family in equations (4.1) to the family in\n",
      "equations (4.2) is just based on our probability models. Once the models\n",
      "are in place, this inference does not depend on any loss function. Bayesian\n",
      "inference for making decisions, as for the speciﬁc questions raised in the two\n",
      "examples from Laplace above, however, is generally set in a decision theory\n",
      "framework.\n",
      "In the decision-theoretic approach to statistical inference, we ﬁrst quantify\n",
      "the loss relative to the true state of nature in any decision that we make. There\n",
      "is an obvious gap between the theory and the practice. It is easy to write some\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "328\n",
      "4 Bayesian Inference\n",
      "loss function as we discussed on page 260 and the following pages, but whether\n",
      "these correspond to reality is another question.\n",
      "Once we are willing to accept a particular loss function as a quantiﬁcation\n",
      "of our reality, we still have a problem because the loss function involves the\n",
      "true state of nature, which we do not know. As we discussed in Section 3.3, we\n",
      "use an expectation of the loss function, under some less restrictive assumptions\n",
      "about the true state of nature.\n",
      "Our objective is to develop methods of statistical inference that minimize\n",
      "our losses or expected losses. Some of the issues in deﬁning this goal more\n",
      "precisely were discussed in Section 3.3.2. A more fundamental question arises\n",
      "as to how to deﬁne the expected loss.\n",
      "Functionals of the Loss Function\n",
      "In Section 3.3, given an assumed family of distributions of the observable\n",
      "{Pθ}, we deﬁned the risk,\n",
      "R(Pθ, T) = EPθ\n",
      "\u0000L(Pθ, T)\n",
      "\u0001\n",
      ",\n",
      "(4.3)\n",
      "and took this as the basis of the decision-theoretic approach.\n",
      "The risk is a function of two elements, T and Pθ or just θ in a parametric\n",
      "setting. For a given T we often write the risk as RT (θ). We generally wish to\n",
      "choose T so as minimize the risk in (4.3). The risk is the basis for deﬁning\n",
      "admissibility, which from some perspectives is one of the most important\n",
      "properties of a statistical procedure.\n",
      "Various considerations, as discussed on page 266 and the following pages,\n",
      "led us to use a distribution with PDF dFΘ(θ) to average the risk, yielding\n",
      "r(FΘ, T) =\n",
      "Z\n",
      "Θ\n",
      "R(Pθ, T)dFΘ(θ)\n",
      "(4.4)\n",
      "=\n",
      "Z\n",
      "Θ\n",
      "Z\n",
      "X\n",
      "L(θ, T(x))dFX|θ(x)dFΘ(θ),\n",
      "(4.5)\n",
      "which is called the Bayes risk. (The term “Bayes risk” is sometimes used\n",
      "to refer to the minimum of r(FΘ, T(X)) with respect to T.) We denote the\n",
      "Bayes risk in various ways, r(FΘ, T(X)), rT (FΘ), and so on. While the risk is\n",
      "a function of θ or of the distribution family Pθ, the Bayes risk is a function of\n",
      "the distribution FΘ.\n",
      "The expectation in (4.3) is taken with respect to the distribution [X|Θ],\n",
      "and thus is a “conditional risk”. Because it is an expected value with respect\n",
      "to the observable random variable, it is a “frequentist risk”.\n",
      "Because admissibility is deﬁned in terms of this risk, “Inadmissibility is a\n",
      "frequentist concept, and hence its relevance to a Bayesian can be debated”\n",
      "(Berger (1985), page 257). We, however, will continue to consider the admissi-\n",
      "bility of statistical procedures, and even consider it to be a relevant fact that\n",
      "most Bayes procedures are admissible (Theorem 4.3).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.1 The Bayesian Paradigm\n",
      "329\n",
      "A quantity that is of more interest in the Bayesian paradigm is a diﬀerent\n",
      "functional of the loss function. It is an expectation taken with respect to\n",
      "the distribution of the parameter, [Θ] or [Θ|X]. This is called the Bayesian\n",
      "expected loss. The Bayesian expected loss is deﬁned “at the time of decision\n",
      "making”. If no statistical analysis is involved, but rather the Bayesian expected\n",
      "loss is merely a probabilistic construct, it is\n",
      "ρ(FΘ, T) =\n",
      "Z\n",
      "Θ\n",
      "L(θ, T)dFΘ(θ).\n",
      "In Bayesian statistical analysis, the expected loss is deﬁned in terms of the\n",
      "conditional distribution:\n",
      "ρ(FΘ, T) =\n",
      "Z\n",
      "Θ\n",
      "L(θ, T)dFΘ|x(θ).\n",
      "(4.6)\n",
      "In summary, given a loss function L(θ, T), where T is a function of a ran-\n",
      "dom variable X, a conditional distribution [X|Θ] and a marginal distribution\n",
      "[Θ], we have the three quantities that are functionals of L.\n",
      "•\n",
      "Risk:\n",
      "RT (θ) = EX|θ(L(θ, T)).\n",
      "(4.7)\n",
      "•\n",
      "Bayes risk:\n",
      "rT (Θ) = EΘ\n",
      "\u0000EX|θ(L(θ, T))\n",
      "\u0001\n",
      "(4.8)\n",
      "= EX\n",
      "\u0000EΘ|X(L(θ, T))\u0001 .\n",
      "(4.9)\n",
      "•\n",
      "Bayes expected loss:\n",
      "ρT (Θ) = EΘ|X(L(θ, T)).\n",
      "(4.10)\n",
      "Any expectation taken wrt [X|Θ] or [X] is a “frequentist” concept.\n",
      "Bayes Actions\n",
      "If FΘ(θ) in equation (4.5) is a CDF, the rule or action that minimizes the\n",
      "conditional expectation with respect to the distribution with that CDF is\n",
      "called the Bayes action or the Bayes rule. We often denote the Bayes rule wrt\n",
      "FΘ as δFΘ(X).\n",
      "The risk that is achieved by the Bayes rule, that is, the minimum average\n",
      "risk, is\n",
      "Z\n",
      "Θ\n",
      "R(θ, δFΘ(X))dFΘ(θ).\n",
      "(As noted above, sometimes this minimum value is called the Bayes risk.)\n",
      "The averaging function allows various interpretations, and it allows the\n",
      "ﬂexibility of incorporating prior knowledge or beliefs. The regions over which\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "330\n",
      "4 Bayesian Inference\n",
      "dFΘ(θ) is large will be given more weight; therefore an estimator will be pulled\n",
      "toward those regions.\n",
      "In formal Bayes procedures, dFΘ(θ) is normalized so that\n",
      "R\n",
      "Θ dFΘ(θ) = 1,\n",
      "we call dFΘ(θ) the prior probability density for θ. The prior distribution of Θ\n",
      "may also depend on parameters, which are called “hyperparameters”.\n",
      "In an exercise in simple probabilistic reasoning without any data, we may\n",
      "set up a quantity similar to r(FΘ, T) in expression (4.5). In that case instead\n",
      "of an expected value R(Pθ, T), we have the loss function L(Pθ, a) for a speciﬁc\n",
      "action a. Using this setup is not statistical inference, of course, but it may be\n",
      "useful in analyzing expected losses under an assumed probability model FΘ.\n",
      "This situation is referred to as a “no-data problem”.\n",
      "To continue with the statistical inference, we next form the joint distribu-\n",
      "tion of Θ and X, and then the conditional distribution of Θ given X, called\n",
      "the posterior distribution. The Bayes rule is determined by minimizing the\n",
      "risk, where the expectation is taken with respect to the posterior distribution.\n",
      "Because the Bayes rule is determined by the posterior distribution, the Bayes\n",
      "rule must be a function of a suﬃcient statistic.\n",
      "In some cases, we wish to determine a rule similar to a Bayes rule that\n",
      "minimizes the average risk in equation (4.5) even though FΘ(θ) is not a CDF\n",
      "and dFΘ(θ) cannot be normalized to integrate to 1. If the integral in equa-\n",
      "tion (4.5) exists, we can proceed to determine an action that minimizes it\n",
      "without actually determining a posterior distribution. In that case, we say\n",
      "that the prior distribution is an improper prior; and of course dFΘ(θ) is not a\n",
      "PDF, but it serves the same purpose as a PDF in the sense that it is a prior\n",
      "weighting function. We will consider this situation on page 345 and give an\n",
      "example on page 359.\n",
      "Probability Statements in Statistical Inference\n",
      "Some methods of statistical inference are based on probabilities of a statis-\n",
      "tic taking on certain values given a speciﬁc member of a family of proba-\n",
      "bility distributions; that is, perhaps, given a value of a parameter. The two\n",
      "main statistical methods that rely on statements of probability are hypoth-\n",
      "esis testing and determining conﬁdence sets. In these methods we assume a\n",
      "model PΘ for the state of nature and then consider probabilities of the form\n",
      "Pr(T(X) = 1|Θ = θ) or Pr(T(X) ∋Θ|Θ = θ). The proper interpretation of a\n",
      "conﬁdence set, for example, is “[... given the assumptions, etc. ...] the proba-\n",
      "bility that a random region formed in this manner includes true the value of\n",
      "the parameter is ...”\n",
      "This kind of probability statement is somewhat awkward for use in inter-\n",
      "preting the results of a statistical analysis.\n",
      "Instead of a statement about Pr(δ(X)|θ), many people would prefer a\n",
      "statement about Pr(Θ ∈T(X)|X = x), that is,\n",
      "Pr(Θ ∈T(x))\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "331\n",
      "even if they don’t think of Θ as a random variable. In the Bayesian approach\n",
      "to testing and determining conﬁdence sets, we do think of the parameter as a\n",
      "random variable and so we can make statements about the probability of the\n",
      "parameter taking on certain values.\n",
      "If the parameter is a random variable, especially if it is a continuous ran-\n",
      "dom variable, point estimation of the parameter or a test of an hypothesis that\n",
      "a parameter takes a speciﬁc value when the parameter is modeled as a continu-\n",
      "ous random variable does not make much sense. The idea of a point estimator\n",
      "that formally minimizes the Bayes risk, however, remains viable. Going be-\n",
      "yond point estimation, the Bayesian paradigm provides a solid theoretical\n",
      "infrastructure for other aspects of statistical inference, such as conﬁdence in-\n",
      "tervals and tests of hypotheses. The parameter random variable is diﬀerent in\n",
      "a fundamental way from the other random variable in the estimation problem:\n",
      "the parameter random variable is not observable; the other random variable\n",
      "is — that is, we can observe and record realizations of this random variable\n",
      "of interest, and those observations constitute the sample, which is the basis\n",
      "for the statistical inference.\n",
      "4.2 Bayesian Analysis\n",
      "The starting point in ordinary Bayesian inference is the conditional distribu-\n",
      "tion of the observable random variable. (In a frequentist approach, this is just\n",
      "the distribution — not the “conditional” distribution.)\n",
      "The prior density represents a probability distribution of the parameter\n",
      "assumed a priori, that is, without the information provided by a random\n",
      "sample. Bayesian inference depends on the conditional distribution of the\n",
      "parameter, given data from the random variable of interest.\n",
      "4.2.1 Theoretical Underpinnings\n",
      "The relationships among the conditional, marginal, and joint distributions\n",
      "can be stated formally in the “Bayes formula”. The simple relationship of\n",
      "probabilities of events as in equations (1.230) and (1.231) allows us to express\n",
      "a conditional probability in terms of the two marginal probabilities and the\n",
      "conditional probability with the conditioning reversed;\n",
      "Pr(A|B) = Pr(B|A)Pr(A)\n",
      "Pr(B)\n",
      ".\n",
      "(4.11)\n",
      "This relationship expresses the basic approach in Bayesian statistical anal-\n",
      "ysis. Instead of probabilities of discrete events, however, we wish to utilize\n",
      "relationships among probability densities.\n",
      "We consider the random variable X with range X ⊆IRd and Θ with range\n",
      "Θ ⊆IRk. We consider the product space X × Θ together with the product\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "332\n",
      "4 Bayesian Inference\n",
      "σ-ﬁeld σ(B(X ) × B(Θ)) where B(X ) and B(Θ) are the Borel σ-ﬁelds over\n",
      "the respective ranges. We will consider the family of distributions P with\n",
      "probability measures dominated by a σ-ﬁnite measure ν, and characterized\n",
      "by the PDFs fX|θ(x) with θ ∈Θ.\n",
      "Theorem 4.1 (Bayes theorem)\n",
      "Assume the density, fX|θ(x) = dFX|θ(x)/dν is Borel on the product measure\n",
      "space, (X × Θ, σ(B(X ) × B(Θ)). Let FΘ be a (prior) CDF on Θ. We have\n",
      "that the posterior distribution FΘ|x is dominated by the probability measure\n",
      "associated with FΘ, and if fX(x) =\n",
      "R\n",
      "Θ fX|θ(x)dFΘ > 0 a.e. ν,\n",
      "dFΘ|x\n",
      "dFΘ\n",
      "= fX|θ(x)\n",
      "fX(x) .\n",
      "(4.12)\n",
      "Proof. We ﬁrst show that fX(x) < ∞a.e., by directly integrating it using\n",
      "Fubini’s theorem:\n",
      "Z\n",
      "X\n",
      "fX(x)dν =\n",
      "Z\n",
      "X\n",
      "Z\n",
      "Θ\n",
      "fX|θ(x)dFΘdν\n",
      "=\n",
      "Z\n",
      "Θ\n",
      "Z\n",
      "X\n",
      "fX|θ(x)dνdFΘ\n",
      "=\n",
      "Z\n",
      "Θ\n",
      "dFΘ\n",
      "= 1.\n",
      "Thus, fX(x) is a proper PDF and is ﬁnite a.e. ν.\n",
      "Now for x ∈X and B ∈B(Θ), let\n",
      "P (B, x) =\n",
      "1\n",
      "fX(x)\n",
      "Z\n",
      "B\n",
      "fX|θ(x)dFΘ.\n",
      "Because P (B, x) ≥0 for all B and P (Θ, x) = 1, P (B, x) > 0 is a probability\n",
      "measure on B(Θ).\n",
      "Furthermore, by Fubini’s theorem\n",
      "Z\n",
      "Θ\n",
      "Z\n",
      "X\n",
      "P (B, x)dνdFΘ =\n",
      "Z\n",
      "X\n",
      "Z\n",
      "Θ\n",
      "P (B, x)dFΘdν\n",
      "and so P (B, x) is measurable ν.\n",
      "Now, for any A ∈σ(X) and B ∈B(Θ), again using Fubini’s theorem, we\n",
      "have\n",
      "Z\n",
      "A×Θ\n",
      "IB(θ)dFX,Θ =\n",
      "Z\n",
      "A\n",
      "Z\n",
      "B\n",
      "fX|θ(x)dFΘdν\n",
      "=\n",
      "Z\n",
      "A\n",
      "\u0012Z\n",
      "B\n",
      "fX|θ(x)\n",
      "fX(x) dFΘ\n",
      "\u0013 \u0012Z\n",
      "Θ\n",
      "fX(x)dFΘ\n",
      "\u0013\n",
      "dν\n",
      "=\n",
      "Z\n",
      "Θ\n",
      "Z\n",
      "A\n",
      "\u0012Z\n",
      "B\n",
      "fX|θ(x)\n",
      "fX(x) dFΘ\n",
      "\u0013\n",
      "fX|θ(x)dνdFΘ\n",
      "=\n",
      "Z\n",
      "A×Θ\n",
      "P (B, x)dFX,Θ.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "333\n",
      "Hence P (B, x) = Pr(Θ ∈B|X = x); that is, it is dominated by the probability\n",
      "measure associated with FΘ and it is the resulting conditional probability\n",
      "measure of Θ given X = x. Its associated CDF is FΘ|x.\n",
      "Furthermore, if λ is a σ-ﬁnite measure that dominates the probability\n",
      "measure associated with FΘ (in a slight abuse of the notation, we write FΘ ≪\n",
      "λ), and fΘ(θ) = dFΘ\n",
      "dλ , then the chain rule applied to expression (4.12) yields\n",
      "dFΘ|x\n",
      "dλ\n",
      "= fX|θ(x)fΘ(θ)\n",
      "fX(x)\n",
      ".\n",
      "(4.13)\n",
      "The Consequences of Exchangeability\n",
      "In Example 3.12 on page 237, we encountered a data-generating process with\n",
      "an underlying Bernoulli distribution that presented us with a quandary. The\n",
      "analysis required us to use knowledge of the data-generating process (specif-\n",
      "ically, the stopping rule). An alternative approach using only the assump-\n",
      "tion that the Bernoulli observations are exchangeable allows us to ignore the\n",
      "stopping rule. This approach is based on de Finetti’s representation theorem\n",
      "(Theorem 1.30 on page 75).\n",
      "We now reconsider the problem discussed in Example 3.12.\n",
      "Example 4.1 Sampling in a Bernoulli distribution\n",
      "We assume an exchangeable sample of size n, X1, . . ., Xn, from the Bernoulli(π).\n",
      "Suppose that k of the observations in the sample have a value of 1, and the\n",
      "other n −k have a value of 0. Given only this information, we ask what is the\n",
      "probability that a new observation Xn+1 has a value of 1.\n",
      "Let {Xi}∞\n",
      "i=1 be an inﬁnite sequence of binary random variables such that\n",
      "for any n, {Xi}n\n",
      "i=1 is exchangeable. Then there is a unique probability measure\n",
      "P on [0, 1] such that for each ﬁxed sequence of zeros and ones {ei}n\n",
      "i=1,\n",
      "Pr(X1 = e1, . . ., Xn = en) =\n",
      "Z 1\n",
      "0\n",
      "πk(1 −π)n−kdµ(π),\n",
      "where k = Pn\n",
      "i=1 ei.\n",
      "In Example 3.12, we considered a variation of this problem in which the\n",
      "sample size n was random. If we have an exchangeable sequence X1, X2, . . .\n",
      "and we choose a ﬁnite value of n to form a set X1, . . ., Xn, we may ask whether\n",
      "the set is exchangeable. The sample is exchangeable so long as the chosen value\n",
      "of n has nothing to do with the Xis. Suppose, however, the value of n is chosen\n",
      "conditionally such that Xn = 1. The sample is no longer exchangeable, and\n",
      "the argument above no longer holds. The Bayesian analysis remains the same,\n",
      "however, as we see in Example 4.3.\n",
      "We will consider other aspects of this problem again in Example 6.1 on\n",
      "page 447 and in Example 7.12 on page 539.\n",
      "In most cases, a random sample is a set of iid random variables; in the\n",
      "Bayesian framework, we only assume that the X1, . . ., Xn are exchangeable\n",
      "and that they are conditionally independent given θ.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "334\n",
      "4 Bayesian Inference\n",
      "4.2.2 Regularity Conditions for Bayesian Analyses\n",
      "Many interesting properties of statistical procedures depend on common sets\n",
      "of assumptions, for example, the Fisher information regularity conditions. For\n",
      "some properties of a Bayesian procedure, or of the posterior distribution itself,\n",
      "there is a standard set of regularity conditions, often called the Walker regular-\n",
      "ity conditions, after Walker (1969), who assumed them in the proofs of various\n",
      "asymptotic properties of the posterior distribution. The regularity conditions\n",
      "apply to the parameter space Θ, the prior PDF fΘ(θ), the conditional PDF\n",
      "of the observables fX|θ(x), and to the support X = {x : fX|θ(x) > 0}. All\n",
      "elements are real, and µ is Lebesgue measure.\n",
      "The Walker regularity conditions are grouped into three sets:\n",
      "A1. Θ is closed.\n",
      "When a general family of distributions is assumed for the observables,\n",
      "this condition may allow for a distribution that is not in that family (for\n",
      "example, in a Bernoulli(π), the parameter is not allowed to take the values\n",
      "0 and 1), but the convenience of this condition in certain situations more\n",
      "than pays for this anomaly, which occurs with 0 probability anyway.\n",
      "A2. X does not depend on θ.\n",
      "This is also one of the FI regularity conditions.\n",
      "A3. For θ1 ̸= θ2 ∈Θ, µ({x : fX|θ(x) ̸= fX|θ(x)}) > 0.\n",
      "This is identiﬁability; see equation (1.25). Without it parametric inference\n",
      "does not make much sense.\n",
      "A4. Given x ∈X and θ1 ∈Θ and δ a suﬃciently small real positive number,\n",
      "then ∀θ ∋∥θ −θ1∥< δ,\n",
      "| log(fX|θ(x)) −log(fX|θ1(x))| < Hδ(x, θ1)\n",
      "where Hδ(x, θ1) is a measurable function of x and θ1 such that\n",
      "lim\n",
      "δ→0+ Hδ(x, θ1) = 0\n",
      "and, ∀˜θ ∈Θ,\n",
      "lim\n",
      "δ→0+\n",
      "\u0012Z\n",
      "X\n",
      "Hδ(x, θ1)fX|˜θ(x)dx\n",
      "\u0013\n",
      "= 0.\n",
      "This is a continuity condition.\n",
      "A5. If Θ is not bounded, then for any ˜θ ∈Θ and a suﬃciently large real\n",
      "number ∆,\n",
      "∥θ∥> ∆=⇒log(fX|θ(x)) −log(fX|˜θ(x)) < K∆(x, ˜θ),\n",
      "where K∆(x, ˜θ) is a measurable function of x and ˜θ such that\n",
      "lim\n",
      "δ→0+\n",
      "\u0012Z\n",
      "X\n",
      "K∆(x, ˜θ)fX|˜θ(x)dx\n",
      "\u0013\n",
      "< 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "335\n",
      "B1. ∀θ0 ∈Θ◦, log(fX|θ(x)) is twice diﬀerentiable wrt θ in some neighborhood\n",
      "of θ0.\n",
      "B2. ∀θ0 ∈Θ◦,\n",
      "0 ≺\n",
      "Z\n",
      "X\n",
      " \n",
      "∂log(fX|θ(x))\n",
      "∂θ\n",
      "\f\f\f\f\n",
      "θ=θ0\n",
      "!  \n",
      "∂log(fX|θ(x))\n",
      "∂θ\n",
      "\f\f\f\f\n",
      "θ=θ0\n",
      "!T\n",
      "fX|θ0(x)dx < ∞.\n",
      "B3. ∀θ0 ∈Θ◦,\n",
      "Z\n",
      "X\n",
      "∂fX|θ(x)\n",
      "∂θ\n",
      "\f\f\f\f\n",
      "θ=θ0\n",
      "dx = 0\n",
      "and\n",
      "Z\n",
      "X\n",
      "∂2fX|θ(x))\n",
      "∂θ(∂θ)T\n",
      "\f\f\f\f\n",
      "θ=θ0\n",
      "dx = 0.\n",
      "(Note that in the ﬁrst condition, the integrand may be a vector, and in\n",
      "the second, it may be a matrix.)\n",
      "B4. Given θ0 ∈Θ◦and δ a suﬃciently small real positive number, ∀θ ∈Θ ∋\n",
      "∥θ −θ0∥< δ and for each i and j,\n",
      "\f\f\f\f\f\n",
      "∂2 log(fX|θ(x))\n",
      "∂θi∂θj\n",
      "−∂2 log(fX|θ(x))\n",
      "∂θi∂θj\n",
      "\f\f\f\f\n",
      "θ=θ0\n",
      "\f\f\f\f\f < Mδ(x, θ0)\n",
      "where Mδ(x, θ0) is a measurable function of x and θ0 such that\n",
      "lim\n",
      "δ→0+\n",
      "\u0012Z\n",
      "X\n",
      "Mδ(x, θ0)fX|θ0(x)dx\n",
      "\u0013\n",
      "< 0.\n",
      "This is also a continuity condition.\n",
      "C1. ∀θ0 ∈Θ◦, fΘ(θ) is continuous at θ0 and fΘ(θ0) > 0.\n",
      "4.2.3 Steps in a Bayesian Analysis\n",
      "We can summarize the approach in a Bayesian statistical analysis as beginning\n",
      "with these steps:\n",
      "1. Identify the conditional distribution of the observable random variable;\n",
      "assuming the density exists, call it\n",
      "fX|θ(x).\n",
      "(4.14)\n",
      "This is the PDF of the distribution Q0 in equation (4.1).\n",
      "2. Identify the prior (marginal) distribution of the parameter; assuming the\n",
      "density exists, call it\n",
      "fΘ(θ).\n",
      "(4.15)\n",
      "This density may have parameters also, of course. Those parameters are\n",
      "called “hyperparameters”, as we have mentioned.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "336\n",
      "4 Bayesian Inference\n",
      "3. Identify the joint distribution; if densities exist, it is\n",
      "fX,Θ(x, θ) = fX|θ(x)fΘ(θ).\n",
      "(4.16)\n",
      "4. Determine the marginal distribution of the observable; if densities exist,\n",
      "it is\n",
      "fX(x) =\n",
      "Z\n",
      "Θ\n",
      "fX,Θ(x, θ)dθ.\n",
      "(4.17)\n",
      "This marginal distribution is also called the prior predictive distribution.\n",
      "In slightly diﬀerent notation, we can also write it as\n",
      "fX(x) =\n",
      "Z\n",
      "Θ\n",
      "fX|θ(x)fΘ(θ)dθ.\n",
      "5. Determine the posterior conditional distribution of the parameter given\n",
      "the observable random variable. If densities exist, it is\n",
      "fΘ|x(θ) = fX,Θ(x, θ)/fX(x).\n",
      "(4.18)\n",
      "This is the PDF of the distribution QH in equation (4.2), which is often\n",
      "just called the “posterior”. The posterior conditional distribution is then\n",
      "the basis for whatever decisions are to be made.\n",
      "6. Assess the posterior conditional distribution in the light of prior beliefs.\n",
      "This is called a sensitivity analysis. Repeat the steps above as appropriate.\n",
      "These ﬁrst steps in a Bayesian analysis involve identifying the components\n",
      "in the equation\n",
      "fX|θ(x)fΘ(θ) = fX,Θ(x, θ) = fΘ|x(θ)fX(x).\n",
      "(4.19)\n",
      "Although we have written the PDFs above in terms of single random vari-\n",
      "ables (any of which of course could be vectors), in applications we assume we\n",
      "have multiple observations on X. In place of fX|θ(x|θ) in (4.14), for example,\n",
      "we would have the joint density of the iid random variables X1, . . ., Xn, or\n",
      "Q fX|θ(xi|θ). The other PDFs would be similarly modiﬁed.\n",
      "Given a posterior based on the random sample X1, . . ., Xn, we can form\n",
      "a posterior predictive distribution for Xn+1, . . ., Xn+k:\n",
      "fXn+1,...,Xn+k|x1,...,xn(xn+1, . . ., xn+k) =\n",
      "R\n",
      "Θ fX,Θ(xn+i, θ)dFΘ|x1,...,xn(θ).\n",
      "(4.20)\n",
      "Rather than determining the densities in equations (4.14) through (4.18)\n",
      "it is generally suﬃcient to determine kernel functions. That is, we write the\n",
      "densities as\n",
      "fD(z) ∝g(z).\n",
      "(4.21)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "337\n",
      "This means that we can avoid computation of the normalizing constant, or\n",
      "partition functions. This is especially important for the densities fΘ|x and fX,\n",
      "where in some cases these involve integrals that are diﬃcult to evaluate.\n",
      "There are other shortcuts we can take in forming all of the relevant ex-\n",
      "pressions for a Bayesian analysis. In the next few examples we will go through\n",
      "all of the gory details. Before considering speciﬁc examples, however, let’s\n",
      "consider some relationships among the various densities.\n",
      "Conjugate Priors\n",
      "If the conditional posterior distribution for a given conditional distribution\n",
      "of the observable is in the same family of distributions as the marginal prior\n",
      "distribution, we say that the prior distribution is a conjugate prior with respect\n",
      "to the family of distributions of the observable. In equation (4.19), for example,\n",
      "fΘ(θ) is a conjugate prior for fX|θ(x) if fΘ|x(θ) is in the same family of\n",
      "distributions as fΘ(θ).\n",
      "Conjugate priors often have attractive properties for a Bayesian analysis,\n",
      "as we will see in the examples.\n",
      "For a PDF in the exponential class, written in the form of equation (2.7),\n",
      "fX|θ(x) = exp\n",
      "\u0000(η(θ))TT(x) −ξ(θ)\n",
      "\u0001\n",
      "h(x),\n",
      "(4.22)\n",
      "the general form of a conjugate prior is\n",
      "fΘ(θ) = c(µ, Σ) exp\n",
      "\u0000|Σ|(η(θ))Tµ −|Σ|ξ(θ)\n",
      "\u0001\n",
      ",\n",
      "(4.23)\n",
      "where c(µ, Σ) is a constant with respect to the hyperparameters µ and Σ,\n",
      "which can be thought of as a mean and variance-covariance (Exercise 4.2).\n",
      "In Table 4.1, I show some conjugate prior distributions for various single-\n",
      "parameter distributions of observables. See Appendix A for meanings of the\n",
      "parameters. In the table, a parameter with a subscript of 0, for example, θ0\n",
      "is assumed to be known (that is, not a parameter).\n",
      "I assume a sample x1, . . ., xn, and I use t to represent T(x) = P xi.\n",
      "Examples\n",
      "Example 4.2 Binomial with Beta Prior\n",
      "The Bayesian approach can be represented nicely by a problem in which we\n",
      "model the conditional distribution of an observable random variable X as\n",
      "a binomial(n, π) distribution, conditional on π, of course. (Recall from Ex-\n",
      "ample 4.1 that if we wish to view the binomial as a sum of Bernoullis, the\n",
      "Bernoullis must at least be exchangeable.)\n",
      "Suppose we assume that π comes from a beta(α, β) prior distribution; that\n",
      "is, we consider a random variable Π that has beta distribution.\n",
      "We work out the density functions in the following order:\n",
      "The conditional distribution of X given π has density (probability function)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "338\n",
      "4 Bayesian Inference\n",
      "Table 4.1. Univariate Conjugate Prior Distributions\n",
      "observable\n",
      "prior\n",
      "posterior\n",
      "Bernoulli(π)\n",
      "beta(α, β)\n",
      "beta(α + t, β + n −t)\n",
      "geometric(π)\n",
      "beta(α, β)\n",
      "beta(α + n, β −n + t)\n",
      "Poisson(θ)\n",
      "gamma(α, β)\n",
      "gamma(α + t, β/(nβ + 1))\n",
      "normal(µ, σ2\n",
      "0)\n",
      "normal(ν, τ 2)\n",
      "normal\n",
      "“\n",
      "νσ2\n",
      "0+τ2t\n",
      "σ2\n",
      "0+nτ2 ,\n",
      "τ2σ2\n",
      "0\n",
      "σ2\n",
      "0+nτ2\n",
      "”\n",
      "normal(µ0, 1/θ)\n",
      "inverted gamma(α, β)\n",
      "inverted gamma\n",
      "“\n",
      "α + n/2,\n",
      "`\n",
      "1/β + 1\n",
      "2\n",
      "P(xi −µ0)2´−1”\n",
      "uniform(θ0, θ)\n",
      "Pareto(α, γ)\n",
      "Pareto(α + n, max(γ, x1, . . . , xn))\n",
      "exponential(θ)\n",
      "inverted gamma(α, β)\n",
      "inverted gamma\n",
      "`\n",
      "α + n, (1/β + t)−1´\n",
      "fX|π(x) =\n",
      "\u0012 n\n",
      "x\n",
      "\u0013\n",
      "πx(1 −π)n−xI{0,1,...,n}(x).\n",
      "(4.24)\n",
      "The marginal (prior) distribution of Π has density\n",
      "fΠ(π) = Γ(α + β)\n",
      "Γ(α)Γ(β)πα−1(1 −π)β−1I]0,1[(π).\n",
      "(4.25)\n",
      "Suppose the hyperparameters in the beta prior are taken to be α = 3 and\n",
      "β = 5. The prior, that is, the marginal distribution of Π, is as shown in the\n",
      "upper left of Figure 4.1.\n",
      "How one might decide that α = 3 and β = 5 are appropriate may depend\n",
      "on some prior beliefs or knowledge about the general range of π in the bi-\n",
      "nomial distribution in this particular setting. We will consider this issue in\n",
      "Section 4.2.5.\n",
      "The joint distribution of X and Π has density\n",
      "fX,Π(x, π) =\n",
      "\u0012n\n",
      "x\n",
      "\u0013 Γ(α + β)\n",
      "Γ(α)Γ(β)πx+α−1(1 −π)n−x+β−1I{0,1,...,n}×]0,1[(x, π).\n",
      "(4.26)\n",
      "Integrating out π, we get the marginal distribution of X to be beta-binomial,\n",
      "with density\n",
      "fX(x) =\n",
      "\u0012 n\n",
      "x\n",
      "\u0013 Γ(α + β)Γ(x + α)Γ(n −x + β)\n",
      "Γ(α)Γ(β)Γ(n + α + β)\n",
      "I{0,1,...,n}(x).\n",
      "(4.27)\n",
      "Finally, the conditional distribution of Π given x (the posterior) has density,\n",
      "fΠ|x(π) =\n",
      "Γ(n + α + β)\n",
      "Γ(x + α)Γ(n −x + β)πx+α−1(1 −π)n−x+β−1I]0,1[(π).\n",
      "(4.28)\n",
      "We note that this is a beta distribution; hence, the beta is a conjugate prior\n",
      "for the binomial. If the parameters of the beta prior are α and β, given one\n",
      "observation x, the posterior is a beta with parameters x + α and n −x + β.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "339\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.5\n",
      "2.0\n",
      "π\n",
      "Prior\n",
      "(α,β) = (3.0, 5.0)\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "π\n",
      "Prior\n",
      "(α,β) = (0.3, 0.5)\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "π\n",
      "Prior\n",
      "(α,β) = (3.0, 0.5)\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "π\n",
      "Prior\n",
      "(α,β) = (0.3, 5.0)\n",
      "Figure 4.1.\n",
      "Priors with Diﬀerent Values of α and β\n",
      "Finally, we need to assess the posterior conditional distribution in the light\n",
      "of prior beliefs. Consider the possible eﬀects of one observation when n is 10.\n",
      "Suppose ﬁrst that we observe x = 2. With the beta(3,5) prior, we get the\n",
      "posterior conditional distribution of Π, as a beta with parameters x + α = 5\n",
      "and n −x + β = 13. Secondly, suppose that we observe x = 6. With the\n",
      "beta(3,5) prior, we get the posterior conditional distribution of Π, as a beta\n",
      "with parameters x + α = 9 and n −x + β = 9. The posterior densities are\n",
      "shown in top panel of Figure 4.2. Compare them with the prior density for\n",
      "beta(3,5) in Figure 4.1.\n",
      "Now, consider the beta(3,0.5) prior, and ﬁrst suppose ﬁrst that we observe\n",
      "x = 2. The posterior conditional distribution of Π, is a beta with parameters\n",
      "x + α = 5 and n −x + β = 8.5. Secondly, suppose that we observe x = 6, and\n",
      "so with the beta(3,0.5) prior, we get the posterior conditional distribution of\n",
      "Π, as a beta with parameters x + α = 9 and n −x + β = 4.5. The posterior\n",
      "densities are shown in lower panel of Figure 4.2. Compare them with the\n",
      "prior density for beta(3,0.5) in Figure 4.1. We can assess the possible posterior\n",
      "conditional distributions in the light of prior beliefs, and how we might expect\n",
      "to modify those prior beliefs after observing speciﬁc values of x. This is called\n",
      "a sensitivity analysis.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "340\n",
      "4 Bayesian Inference\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "π\n",
      "Posterior\n",
      "(α,β) = (3.0, 5.0);    x=2\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "π\n",
      "Posterior\n",
      "(α,β) = (3.0, 5.0);    x=6\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "π\n",
      "Posterior\n",
      "(α,β) = (3.0, 0.5);    x=2\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "π\n",
      "Posterior\n",
      "(α,β) = (3.0, 0.5);    x=6\n",
      "Figure 4.2.\n",
      "Posteriors Resulting from Two Diﬀerent Priors (Upper and Lower\n",
      "Panels) after Observing x = 2 or x = 6 (Left and Right Panels)\n",
      "The posterior distribution of Π represents everything that we know about\n",
      "this parameter that controls the distribution of X, and so, in a sense, our\n",
      "statistical inference is complete. We may, however, wish to make more tradi-\n",
      "tional inferences about the parameter. For example, we may wish to estimate\n",
      "the parameter, test hypotheses concerning it, or determine conﬁdence sets for\n",
      "it. We will return to this problem in Examples 4.6, 4.15, and 4.18.\n",
      "Example 4.3 (Continuation of Examples 3.12 and 4.1) Sampling in\n",
      "a Bernoulli distribution; Negative binomial with a beta prior\n",
      "Again consider the problem of statistical inference about π in the family of\n",
      "Bernoulli distributions. We have discussed two data-generating processes: one,\n",
      "take a random sample of size n, X1, . . ., Xn from the Bernoulli(π) and count\n",
      "the number of 1’s; and two, take a sequential sample, X1, X2, . . ., until a ﬁxed\n",
      "number t of 1’s have occurred. The ﬁrst process leads to a binomial distribu-\n",
      "tion and the second leads to a negative binomial distribution. In Example 4.2,\n",
      "we considered a Bayesian approach to inference on π using a binomial dis-\n",
      "tribution with a beta prior. Now consider inference on π using a negative\n",
      "binomial distribution again with a beta prior.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "341\n",
      "We go through the same steps as in equations (4.24) through (4.28). In\n",
      "place of the conditional distribution of the observable binomial variable X,\n",
      "whose density was given in equation (4.24), we have the conditional distribu-\n",
      "tion of the observable negative binomial variable N, whose density is\n",
      "pN(n ; t, π) =\n",
      "\u0012n −1\n",
      "t −1\n",
      "\u0013\n",
      "πt(1 −π)n−t,\n",
      "n = t, t + 1, . . ..\n",
      "(4.29)\n",
      "(Recall that this is one form of the negative binomial distribution probability\n",
      "function.)\n",
      "Again starting with a beta(α, β) prior distribution on the random variable\n",
      "Π, and going through the standard steps of forming the joint distribution and\n",
      "the marginal of the observable N, we arrive at the conditional distribution of\n",
      "Π, given N = n. The PDF is\n",
      "fΠ|x(π) =\n",
      "Γ(n + α + β)\n",
      "Γ(x + α)Γ(n −x + β)πx+α−1(1 −π)n−x+β−1I]0,1[(π).\n",
      "(4.30)\n",
      "This is the same posterior distribution as in Example 4.2, and since this\n",
      "represents everything that we know about this parameter that controls the\n",
      "distribution of N, if the data from the two diﬀerent experiments are the same,\n",
      "as in Example 3.12, then any inference about Π would be the same. Thus,\n",
      "the Bayesian approach conforms to the likelihood principle.\n",
      "In the next two examples we consider inference in the context of a normal\n",
      "distribution. The ﬁrst example is very simple because we assume only one\n",
      "unknown parameter.\n",
      "Example 4.4 Normal with Known Variance and a Normal Prior on\n",
      "the Mean\n",
      "Suppose we assume the observable random variable X has a N(µ, σ2) distri-\n",
      "bution in which σ2 is known.\n",
      "The parameter of interest, µ, is assumed to be a realization of an unob-\n",
      "servable random variable M ∈IR.\n",
      "Let us use a prior distribution for M that is N(µ0, σ2\n",
      "0), where the hyper-\n",
      "parameters are chosen to reﬂect prior information or beliefs about the mean\n",
      "of X.\n",
      "Let us assume that we have one observation on X = x. We easily go\n",
      "through the standard steps. First, we get the joint PDF,\n",
      "fX,M (x, µ) =\n",
      "1\n",
      "2πσσ0\n",
      "e−1\n",
      "2 (x−µ)2/σ2−1\n",
      "2 (µ−µ0)2/σ2\n",
      "0.\n",
      "To get the marginal PDF, we expand the quadratics in the exponent and\n",
      "collect terms in µ, which we want to integrate out. This is a standard operation\n",
      "(see page 684), but nevertheless it is tedious and we’ll write it out this once:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "342\n",
      "4 Bayesian Inference\n",
      "(x −µ)2\n",
      "σ2\n",
      "+ (µ −µ0)2\n",
      "σ2\n",
      "0\n",
      "= x2\n",
      "σ2 + µ2\n",
      "0\n",
      "σ2\n",
      "0\n",
      "+ σ2 + σ2\n",
      "0\n",
      "σ2σ2\n",
      "0\n",
      "µ2 −2σ2µ0 + σ2\n",
      "0x\n",
      "σ2σ2\n",
      "0\n",
      "µ\n",
      "(4.31)\n",
      "= x2\n",
      "σ2 + µ2\n",
      "0\n",
      "σ2\n",
      "0\n",
      "+\n",
      "\u0012\n",
      "µ2 −2σ2µ0 + σ2\n",
      "0x\n",
      "σ2 + σ2\n",
      "0\n",
      "µ\n",
      "\u0013\n",
      "/\n",
      "\u0012 σ2σ2\n",
      "0\n",
      "σ2 + σ2\n",
      "0\n",
      "\u0013\n",
      "= x2\n",
      "σ2 + µ2\n",
      "0\n",
      "σ2\n",
      "0\n",
      "−(σ2µ0 + σ2\n",
      "0x)2\n",
      "σ2σ2\n",
      "0(σ2 + σ2\n",
      "0)\n",
      "+\n",
      "\u0012\n",
      "µ −σ2µ0 + σ2\n",
      "0x\n",
      "σ2 + σ2\n",
      "0\n",
      "\u00132\n",
      "/\n",
      "\u0012 σ2σ2\n",
      "0\n",
      "σ2 + σ2\n",
      "0\n",
      "\u0013\n",
      "The last quadratic in the expression above corresponds to the exponential in\n",
      "a normal distribution with a variance of σ2σ2\n",
      "0/(σ2 +σ2\n",
      "0), so we adjust the joint\n",
      "PDF so we can integrate out the µ, leaving\n",
      "fX(x) =\n",
      "1\n",
      "√\n",
      "2πσσ0\n",
      "s\n",
      "σ2 + σ2\n",
      "0\n",
      "σ2σ2\n",
      "0\n",
      "exp\n",
      "\u0012\n",
      "−1\n",
      "2\n",
      "\u0012x2\n",
      "σ2 + µ2\n",
      "0\n",
      "σ2\n",
      "0\n",
      "−(σ2µ0 + σ2\n",
      "0x)2\n",
      "σ2σ2\n",
      "0(σ2 + σ2\n",
      "0)\n",
      "\u0013\u0013\n",
      ".\n",
      "Combining the exponential in this expression with (4.31), we get the exponen-\n",
      "tial in the conditional posterior PDF, again ignoring the −1/2 while factoring\n",
      "out σ2σ2\n",
      "0/(σ2 + σ2\n",
      "0), as\n",
      "\u0012\n",
      "µ2 −2σ2µ0 + σ2\n",
      "0x\n",
      "σ2 + σ2\n",
      "0\n",
      "µ + (σ2µ0 + σ2\n",
      "0x)2\n",
      "(σ2 + σ2\n",
      "0)2\n",
      "\u0013\n",
      "/ σ2σ2\n",
      "0\n",
      "σ2 + σ2\n",
      "0\n",
      "Finally, we get the conditional posterior PDF,\n",
      "fM|x(µ) =\n",
      "1\n",
      "√\n",
      "2π\n",
      "s\n",
      "σ2σ2\n",
      "0\n",
      "σ2 + σ2\n",
      "0\n",
      "exp\n",
      " \n",
      "−1\n",
      "2\n",
      "\u0012\n",
      "µ −σ2µ0 + σ2\n",
      "0x\n",
      "σ2 + σ2\n",
      "0\n",
      "\u00132\n",
      "/ σ2σ2\n",
      "0\n",
      "σ2 + σ2\n",
      "0\n",
      "!\n",
      ",\n",
      "and so we see that the posterior is a normal distribution with a mean that is a\n",
      "weighted average of the prior mean and the observation x, and a variance that\n",
      "is likewise a weighted average of the prior variance and the known variance of\n",
      "the observable X.\n",
      "This example, although quite simple, indicates that there can be many tedious\n",
      "manipulations. It also illustrates why it is easier to work with PDFs without\n",
      "normalizing constants.\n",
      "We will now consider a more interesting example, in which neither µ nor\n",
      "σ2 is known. We will also assume that we have multiple observations.\n",
      "Example 4.5 Normal with Inverted Chi-Squared and Conditional\n",
      "Normal Priors\n",
      "Suppose we assume the observable random variable X has a N(µ, σ2), and we\n",
      "wish to make inferences on µ and σ2. Let us assume that µ is a realization\n",
      "of an unobservable random variable M ∈IR and σ2 is a realization of an\n",
      "unobservable random variable Σ2 ∈IR+.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "343\n",
      "We construct a prior family by ﬁrst deﬁning a marginal prior on Σ2 and\n",
      "then a conditional prior on M|σ2. From consideration of the case of known\n",
      "variance, we choose an inverted chi-squared distribution for the prior on Σ2:\n",
      "fΣ2(σ2) ∝1\n",
      "σ σ−(ν0/2+1)e(ν0σ2\n",
      "0)/(2σ2)\n",
      "where we identify the parameters ν0 and σ2\n",
      "0 as the degrees of freedom and the\n",
      "scale for Σ2.\n",
      "Given σ2, let us choose a normal distribution for the conditional prior of\n",
      "M|σ2. It is convenient to express the variance of M|σ2 as a scaling of σ2. Let\n",
      "this variance be σ2/κ0. Now combining the prior of Σ2 with this conditional\n",
      "prior of M|σ2, we have the joint prior PDF\n",
      "fM,Σ2(µ, σ2) ∝1\n",
      "σ (σ2)−(ν0/2+1) exp\n",
      "\u0012\n",
      "−1\n",
      "2σ2\n",
      "\u0000ν0σ2\n",
      "0 + κ0(µ −µ0)2\u0001\u0013\n",
      ".\n",
      "We assume a sample X1, . . ., Xn, with the standard statistics X and S2.\n",
      "We next form the joint density of (X, M, Σ2), then the marginal of X, and\n",
      "ﬁnally the joint posterior of (M, Σ2|x). This latter is\n",
      "fM,Σ2|x(µ, σ2; x) ∝1\n",
      "σ (σ2)−(ν0/2+1) exp\n",
      "\u0012\n",
      "−1\n",
      "2σ2\n",
      "\u0000ν0σ2\n",
      "0 + κ0(µ −µ0)2\u0001\u0013\n",
      "×(σ2)−n/2 exp\n",
      "\u0012\n",
      "−1\n",
      "2σ2\n",
      "\u0000(n −1)s2 + n(¯x −µ)2\u0001\u0013\n",
      ".(4.32)\n",
      "We would also like to get the conditional marginal posterior of Σ2 given x.\n",
      "Then, corresponding to our conditional prior of M given σ2, we would like to\n",
      "get the conditional posterior of M given σ2 and x. This involves much tedious\n",
      "algebra, completing squares and rearranging terms, but once the expressions\n",
      "are simpliﬁed, we ﬁnd that\n",
      "M|σ2, x ∼N\n",
      "\u0012κ0µ0 + n¯x\n",
      "κ0 + n\n",
      ",\n",
      "σ2\n",
      "κ0 + n\n",
      "\u0013\n",
      "(4.33)\n",
      "and\n",
      "(ν0σ2\n",
      "0 + (n −1)s2 + nκ0(¯x −µ0)2/(κ0 + n))/2\n",
      "Σ2\n",
      "|x ∼χ2(ν0 + n).\n",
      "(4.34)\n",
      "Suﬃcient Statistics and the Posterior Distribution\n",
      "Suppose that in the conditional distribution of the observable X for given θ,\n",
      "there is a suﬃcient statistic T for θ. In that case, the PDF in equation (4.14)\n",
      "can be written as\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "344\n",
      "4 Bayesian Inference\n",
      "fX|θ(x) = g(t|θ)h(x),\n",
      "(4.35)\n",
      "where t = T(x).\n",
      "Then, following the steps on page 335 leading up to the posterior PDF in\n",
      "equation (4.18), we have the posterior\n",
      "fΘ|x(θ) ∝p(t, θ),\n",
      "(4.36)\n",
      "that is, the posterior distribution depends on X only through T. Notice that\n",
      "the analysis in Example 4.5 was performed using the suﬃcient statistics X\n",
      "and S2.\n",
      "All Bayesian inference, therefore, can be based on suﬃcient statistics.\n",
      "Use of the Posterior Distribution\n",
      "The conditional posterior distribution of the parameter contains all of the\n",
      "relevant information about the parameter, based on any prior information\n",
      "(or beliefs) incorporated in the prior distribution, as well as the information\n",
      "contained in the observations under the assumption that their distribution is\n",
      "known conditional on the value of the parameter following the model used in\n",
      "the analysis. Hence, in Example 4.2, the posterior beta distribution of Π tells\n",
      "us everything we might want to know about that parameter, given our prior\n",
      "assumptions and the data observed.\n",
      "There are diﬀerent ways we might interpret the analysis. Under the view\n",
      "that the parameter is a random variable, a narrow interpretation of the analy-\n",
      "sis outlined above is that it addresses the speciﬁc value of the random variable\n",
      "that was operative at the time that the data were observed. This interpreta-\n",
      "tion emphasizes the changing nature of the phenomenon being studied. In any\n",
      "speciﬁc situation, a given realization of the random parameter governs a data-\n",
      "generating process in that speciﬁc instance. A less ephemeral interpretation\n",
      "of the analysis is that the analysis provides a more general inference about\n",
      "the distribution of the random parameter.\n",
      "4.2.4 Bayesian Inference\n",
      "Although as we pointed out above, once we have the conditional posterior\n",
      "distribution for the parameter, we have all of the information about the pa-\n",
      "rameter. We may, however, wish to make more traditional inferences about\n",
      "the random parameter; that is, we may want to estimate it, test hypotheses\n",
      "concerning it, or set conﬁdence sets for it. Since the parameter is a random\n",
      "variable, the meaning of such inferences requires some interpretation. One\n",
      "simple interpretation is that the inference is about the speciﬁc value of the\n",
      "parameter when the data were observed.\n",
      "We can base the inference on simple heuristics relating to the posterior\n",
      "distribution. For example, in a manner similar to the heuristic that leads to\n",
      "a maximum likelihood estimator, we may consider the mode of the posterior\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "345\n",
      "as the most likely value. The mode is sometimes called the maximum a pos-\n",
      "terior probability (MAP) point estimator. Similar heuristics lead to a type of\n",
      "conﬁdence set based on the probabilities of various regions in the support of\n",
      "the posterior distribution (see Sections 4.6.1 and 4.6.2).\n",
      "Bayes Actions\n",
      "While we can arrive at the conditional posterior distribution for the parameter\n",
      "without any reference to a loss function, in order to make speciﬁc inferences\n",
      "about the parameter in a formal Bayesian decision-theoretic approach, we\n",
      "need to deﬁne a loss function, then formulate the conditional risk for a given\n",
      "inference procedure δ(X),\n",
      "R(FX|θ, δ(X)) =\n",
      "Z\n",
      "X\n",
      "L(θ, T(x))dFX|θ(x),\n",
      "(4.37)\n",
      "and ﬁnally determine the posterior average risk,\n",
      "r(FΘ, δ(X)) =\n",
      "Z\n",
      "Θ\n",
      "R(FX|θ, δ(X))dFΘ(θ).\n",
      "(4.38)\n",
      "The procedure that minimizes the average risk is called the Bayes action. If\n",
      "the action is estimation, the procedure is called the Bayes estimator. Although\n",
      "we view the MAP estimator from a heuristic standpoint, it can be shown to\n",
      "be a limit of Bayes estimators under the 0-1 loss function.\n",
      "We will discuss Bayesian estimation in Section 4.3, Bayesian hypothesis\n",
      "testing in Section 4.5, and Bayesian conﬁdence intervals in Section 4.6. We will\n",
      "continue considering the problem of inferences on π in the binomial distribu-\n",
      "tion that we ﬁrst addressed in Example 4.2. In that example, there was no loss\n",
      "function and we stopped with the posterior PDF. In Examples 4.6 (page 355),\n",
      "4.15 (page 365), and 4.18 (page 374), we will consider Bayes actions relating\n",
      "to π.\n",
      "Generalized Bayes Actions\n",
      "We often seek a Bayes action even though we do not want to go through any\n",
      "formal steps of identifying a posterior distribution. In some cases, the prior\n",
      "may not actually be a PDF (or proportional to a PDF); that is, the integral\n",
      "of the prior may not be ﬁnite. The prior is said to be improper. If the prior is\n",
      "improper, the posterior may or may not be improper. If the posterior is not a\n",
      "PDF or proportional to a PDF, then we must be careful in any interpretation\n",
      "we may make of the posterior. We may, however, be able to identify a rule or\n",
      "action in the usual way that we determine a Bayes action when the posterior\n",
      "is proper.\n",
      "A Bayes action is one that minimizes the risk in equation (4.5) if the\n",
      "weighting function dFΘ(θ) is a PDF. If dFΘ(θ) is not a PDF, that is, if the\n",
      "prior is improper, so long as the integral\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "346\n",
      "4 Bayesian Inference\n",
      "r(FΘ, T) =\n",
      "Z\n",
      "Θ\n",
      "Z\n",
      "X\n",
      "L(θ, T(x))dFX|θ(x)dFΘ(θ).\n",
      "(4.39)\n",
      "exists, we call a rule that minimizes it a generalized Bayes action.\n",
      "In Example 4.9 on page 359, we determine the Bayes estimator of the mean\n",
      "in a normal distribution when the prior is uniform over the real line, which\n",
      "obviously is an improper prior.\n",
      "Limits of Bayes Actions\n",
      "Another variation on Bayes actions is the limit of the Bayes action as some\n",
      "hyperparameters approach ﬁxed limits. In this case, we have a sequence of\n",
      "prior PDFs, {F (k)\n",
      "Θ\n",
      ": k = 1, 2, . . .}, and consider the sequence of Bayes actions,\n",
      "δ(k)(X) that result from these priors. The limit limk→∞δ(k)(X) is called a\n",
      "limiting Bayes action. The limiting prior, limk→∞F (k)\n",
      "Θ , may not be a PDF.\n",
      "In Example 4.6 on page 356, we determine a limiting Bayes action, given\n",
      "in equation (4.47).\n",
      "4.2.5 Choosing Prior Distributions\n",
      "It is important to choose a reasonable prior distribution that reﬂects prior\n",
      "information or beliefs about the phenomenon of interest.\n",
      "Families of Prior Distributions\n",
      "Various families of prior distributions can provide both ﬂexibility in repre-\n",
      "senting prior beliefs and computational simplicity. Conjugate priors, as in\n",
      "Examples 4.2 through 4.5, are often very easy to work with and to inter-\n",
      "pret. In many cases, a family of conjugate priors would seem to range over\n",
      "most reasonable possibilities, as shown in Figure 4.1 for priors of the binomial\n",
      "parameter.\n",
      "Generalized distributions or mixtures of common distributions, as dis-\n",
      "cussed in Section 2.10, may correspond to prior beliefs. The ideas of choosing a\n",
      "distribution that matches general assumed properties such as the shape of the\n",
      "distribution or that corresponds to ﬁxed quantiles discussed in Section 2.10.4\n",
      "may also lead to reasonable prior distributions.\n",
      "Within a given family of prior distributions, it may be useful to consider\n",
      "ones that are optimal in some way. For example, in testing composite hypothe-\n",
      "ses we may seek a “worst case” for rejecting or accepting the hypotheses. This\n",
      "leads to consideration of a “least favorable prior distribution”. We may also\n",
      "wish to use a prior that reﬂects an almost complete lack of prior informa-\n",
      "tion.. This leads to consideration of “noninformative priors”, or priors with\n",
      "maximum entropy within a given class.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "347\n",
      "If the priors are restricted to a particular class of distributions, say Γ,\n",
      "we may seek an action whose worst risk with respect to any prior in Γ is\n",
      "minimized, that is, we may see an action δ that yields\n",
      "inf\n",
      "δ sup\n",
      "P∈Γ\n",
      "r(P, δ).fΘ(θ) ∝|I(θ)|1/2.\n",
      "(4.40)\n",
      "Such an action is said to be Γ-minimax, usually written as gamma-minimax.\n",
      "Clearly, any minimax Bayes action is gamma-minimax Bayes with respect to\n",
      "the same loss function.\n",
      "Assessing the Problem Formulation\n",
      "In any statistical analysis, the formulation of a model is important. In Ex-\n",
      "ample 4.2 above, we must consider whether or not it is reasonable to assume\n",
      "that the observable data follows some kind of binomial distribution. From\n",
      "ﬁrst principles, this means that we are willing to assume that there is a set\n",
      "of n independent outcomes that may be 0 or 1, in each case with a constant\n",
      "probability π.\n",
      "In a Bayesian analysis, not only must we think about whether or not the\n",
      "assumed distribution of the observable is reasonable, we must also consider\n",
      "the assumed prior distribution. Various possibilities for a beta prior for the\n",
      "binomial distribution in Example 4.2 are shown in Figure 4.1. If we know\n",
      "something about the data-generating process, we may conclude that some\n",
      "general shape of the prior is more appropriate than another. Often a scientist\n",
      "who may not know much about statistics, but is familiar with the the data-\n",
      "generating process, will have some general beliefs about what values π is more\n",
      "likely to have. The scientist, for example, may be able to state that there is a\n",
      "“50% chance that π is between 0.2 and 0.6”. In that case, the hyperparameters\n",
      "of a beta could be determined that would yield that probability. The process\n",
      "of developing a prior by discussions with a subject matter expert is called\n",
      "“elicitation”. As mentioned above, a generalized distribution that corresponds\n",
      "to reasonable quantiles of prior beliefs may be useful.\n",
      "In the Bayesian approach taken in Example 4.2, we assume that while the\n",
      "n observations were being collected, some random variable Π had a ﬁxed value\n",
      "of π. We are interested both in that value and in the conditional distribution\n",
      "of the random variable Π, given what we have observed. For particular choices\n",
      "of hyperparameters characterizing the prior distribution on Π, we obtain the\n",
      "posterior distributions shown in Figure 4.2. Do these seem reasonable?\n",
      "In deciding whether the prior is appropriate, sometimes it is worthwhile\n",
      "to consider the eﬀects of various possible outcomes of the experiment. The\n",
      "issue is whether the posterior conditional distribution conforms to how the\n",
      "observed data should change our prior beliefs.\n",
      "This sensitivity analysis can be done without actually taking any observa-\n",
      "tions because we can determine the posterior density that would result from\n",
      "the given prior density. In Figure 4.3, we plot the posterior distribution of\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "348\n",
      "4 Bayesian Inference\n",
      "Π based on a beta(3, 5) prior given various values of that we might have\n",
      "observed.\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "x=2\n",
      "π\n",
      "Posterior\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "x=4\n",
      "π\n",
      "Posterior\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "x=6\n",
      "π\n",
      "Posterior\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "x=8\n",
      "π\n",
      "Posterior\n",
      "Figure 4.3.\n",
      "Posteriors Resulting from a Beta(3,5) Prior after Various Possible\n",
      "Observations\n",
      "Assessing the eﬀect on the posterior of various possible observations may\n",
      "give us some feeling of conﬁdence in our choice of a prior distribution.\n",
      "*** Bayesian robustness\n",
      "Choice of Hyperparameters\n",
      "Usually in a Bayesian analysis, it is instructive to consider various priors and\n",
      "particularly various hyperparameters in some detail.\n",
      "Of course, in most cases, we must also take into account the loss function.\n",
      "Recall the eﬀects in this problem of diﬀerent hyperparameter values on the\n",
      "point estimation problem (that is, the choice of the Bayes action to minimize\n",
      "the posterior risk) when the loss is squared error.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "349\n",
      "We might also consider what might be the eﬀect of diﬀerent hyperpa-\n",
      "rameters. There are several possibilities we could consider. Let’s just look at\n",
      "one possibility, which happens to be bimodal, as shown in the upper right of\n",
      "Figure 4.1. In this case, we have chosen α = 0.3 and β = 0.5. This would\n",
      "correspond to a general prior belief that π is probably either close to 0 or\n",
      "close to 1.\n",
      "Now, again we might consider the eﬀect of various observations on our\n",
      "belief about Π. We get the posteriors shown in Figure 4.4 for various possible\n",
      "values of the observations.\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "x=0\n",
      "π\n",
      "Posterior\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "x=1\n",
      "π\n",
      "Posterior\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "x=9\n",
      "π\n",
      "Posterior\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "x=10\n",
      "π\n",
      "Posterior\n",
      "Figure 4.4.\n",
      "Posteriors from the Bimodal Beta(0.3,0.5) Prior\n",
      "Compare the posteriors in Figure 4.4 with the prior in Figure 4.1. In each\n",
      "case in this example we see that our posterior belief is unimodal instead of\n",
      "bimodal, as our prior belief had been. Although in general a posterior may\n",
      "be multimodal, in the case of a binomial(n, π) distribution with a beta(α, β)\n",
      "prior, the posterior is unimodal, because as we have seen, the posterior is beta\n",
      "with parameters x + α and n −x + β, both of which cannot be less than 1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "350\n",
      "4 Bayesian Inference\n",
      "Objective Priors\n",
      "The extent to which the observed data updates the prior distribution into\n",
      "a posterior distribution depends to some extent on the nature of the prior\n",
      "distribution. Whenever there is little knowledge or only weak belief about the\n",
      "distribution of the parameter of interest, we may wish to deﬁne a prior that\n",
      "will allow the “data to speak for themselves”. We call such priors “objective”,\n",
      "“vague”, “ﬂat”, “diﬀuse”, or “noninformative”. (Although one or the other of\n",
      "these terms may be more appropriate in a given setting, there is no technical\n",
      "diﬀerence.) Such priors are often improper.\n",
      "An example of a noninformative prior is one in which dFΘ(θ) = dν where\n",
      "ν is Lebesgue measure and Θ is the reals, or some unbounded interval subset\n",
      "of the reals. This is a noninformative prior, in the sense that it gives equal\n",
      "weights to equal-length intervals for Θ. Such a prior is obviously improper.\n",
      "Another type of noninformative prior is Jeﬀreys’s noninformative prior.\n",
      "This prior is proportional to\n",
      "p\n",
      "det(I(θ)), where det(I(θ)) or |I(θ)| is the\n",
      "determinant of the Fisher information matrix; that is,\n",
      "fΘ(θ) ∝|I(θ)|1/2.\n",
      "(4.41)\n",
      "The idea is that such a prior is invariant to the parametrization. We can see\n",
      "this by considering the reparametrization ˜θ = ¯g(θ), where ¯g is a 1:1 diﬀeren-\n",
      "tiable function that maps the parameter space onto itself in such a way that\n",
      "the underlying conditional probability distribution of X is unchanged when\n",
      "X is transformed appropriately to e\n",
      "X (see page 178 in Chapter 1). We see this\n",
      "by writing\n",
      "I(˜θ) = −E\n",
      " ∂2 log f e\n",
      "X|˜θ( e\n",
      "X)\n",
      "∂2˜θ\n",
      "!\n",
      "= −E\n",
      " \n",
      "∂2 log fX|˜θ(X)\n",
      "∂2θ\n",
      "\f\f\f\f\n",
      "∂2θ\n",
      "∂˜θ\n",
      "\f\f\f\f\n",
      "2!\n",
      "= I(θ)\n",
      "\f\f\f\f\n",
      "∂2θ\n",
      "∂˜θ\n",
      "\f\f\f\f\n",
      "2\n",
      ",\n",
      "and so\n",
      "f e\n",
      "Θ(˜θ) ∝|I(θ)|1/2,\n",
      "(4.42)\n",
      "where the additional constant of proportionality is the Jacobian of the inverse\n",
      "transformation.\n",
      "If Θ is the reals, or some unbounded interval subset of the reals, Jeﬀreys’s\n",
      "noninformative prior is improper. If the support of Θ is ﬁnite, Jeﬀreys’s non-\n",
      "informative prior is generally proper; see equation (4.49) on page 357.\n",
      "In a variation of Jeﬀreys’s noninformative prior when θ = (θ1, θ2), where\n",
      "θ2 is a nuisance parameter, we ﬁrst deﬁne fΘ2|Θ1(θ2) as the Jeﬀreys’s prior\n",
      "associated with fX|θ where θ1 is ﬁxed and then using fΘ2|Θ1(θ2) derive the\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.2 Bayesian Analysis\n",
      "351\n",
      "marginal conditional distribution fX|θ1 if it exists. Finally, we compute the\n",
      "prior on Θ1 as the Jeﬀreys’s prior using fX|θ1. Such a prior, if it exists, that\n",
      "is, if fX|θ1 exists, is called a reference noninformative prior.\n",
      "Maximum Entropy\n",
      "Entropy can be thought of as the inverse of “information” (see Section 1.1.5),\n",
      "hence large entropy provides less information in this heuristic sense. This pro-\n",
      "vides another general type of “noninformative”, or at least, “less informative”\n",
      "prior distribution.\n",
      "For some family of distributions with ﬁnite variance whose support is IR,\n",
      "the larger the variance, the larger the entropy We may, however, seek a prior\n",
      "distribution that has maximum entropy for given mean and variance. Of all\n",
      "such distributions dominated by Lebesgue measure, the normal family attains\n",
      "this maximum. (Showing this is Exercise 1.88.)\n",
      "The appropriate support of the prior, of course, depends on the nature\n",
      "of the distribution of the observables. For a binomial or negative binomial\n",
      "distribution conditional on the parameter π, the prior should have support\n",
      "]0, 1[. A very nice class of priors for this problem, as we have seen, is the\n",
      "family of beta(α, β) distributions. The entropy of a beta(α, β) distribution is\n",
      "log\n",
      "\u0012 Γ(α + β)\n",
      "Γ(α)Γ(β)\n",
      "\u0013\n",
      "−(α −1)(ψ(α) −ψ(α + β)) −(β −1)(ψ(β) −ψ(α)),\n",
      "and its maximum occurs at α = β = 1 (see Exercise 1.32b).\n",
      "Empirical Priors\n",
      "The hyperparameters in a prior distribution are often based on estimates from\n",
      "prior(!) samples of similar data. This seems reasonable, of course, if the prior\n",
      "distribution is to reﬂect rational beliefs.\n",
      "The use of data to provide values of hyperparameters can even occur with\n",
      "the same data that is to be used in the Bayesian inference about the condi-\n",
      "tional distribution, as we will discuss in Section 4.2.6 below.\n",
      "Hierarchical Priors\n",
      "In the basic Bayes setup of (4.1),\n",
      "P = {PΘ | Θ ∼QΞ ∈Q},\n",
      "we can consider a Bayesian model for the priors,\n",
      "Q = {QΞ | Ξ ∼R0 ∈R}.\n",
      "(4.43)\n",
      "We follow the same analytic process as in Section 4.2.3, except that the dis-\n",
      "tributions have another layer of conditioning. We begin with a distribution\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "352\n",
      "4 Bayesian Inference\n",
      "of the observable random that is conditional on both the basic parameters\n",
      "and the hyperparameters, that is, we have the density fX|θξ(x) instead of just\n",
      "fX|θ(x); and the prior distribution is now considered a conditional distribu-\n",
      "tion. The prior PDF is now fΘ|ξ(θ) instead of fΘ(θ). Now, after introducing a\n",
      "prior for Ξ with PDF fΞ(ξ), we proceed to manipulate the densities to arrive\n",
      "at a posterior conditional distribution for the parameter of interest, Θ.\n",
      "The computations associated with producing the posterior distribution in\n",
      "a hierarchical model are likely to be more diﬃcult than those in a simpler\n",
      "Bayesian model.\n",
      "4.2.6 Empirical Bayes Procedures\n",
      "In the basic Bayes setup of (4.1),\n",
      "P = {PΘ | Θ ∼Qξ ∈Q},\n",
      "we might consider a “frequentist” model of the priors:\n",
      "Q = {Qξ | ξ ∈Ξ}.\n",
      "(4.44)\n",
      "Now, we determine the marginal of X conditional on ξ (there’s no θ);\n",
      "that is, instead of the PDF fX(x) in Section 4.2.3, we have the PDF fX|ξ(x).\n",
      "Given the data on X, we can estimate ξ using traditional statistical methods.\n",
      "Because we have the PDF, a commonly-used method of estimation of ξ is\n",
      "maximum likelihood.\n",
      "One way of interpreting this setup is that the observable random vari-\n",
      "able X is a actually a set of random variables, and the kth observation has\n",
      "PDF fXk|θk(x), where the θk is a realization of a random variable Θ with\n",
      "distribution that depends on ξ.\n",
      "Most people who subscribe to the Bayesian paradigm for statistical anal-\n",
      "ysis eschew empirical Bayes procedures.\n",
      "4.3 Bayes Rules\n",
      "To determine a Bayes action, we begin with the standard steps in Section 4.2.3.\n",
      "A loss function was not used in deriving the posterior distribution, but to get\n",
      "a Bayes action, we must use a loss function. Given the loss function, we focus\n",
      "on the posterior risk.\n",
      "The Risk Function\n",
      "**** modify this to be more similar to the introductory discussion of Bayes\n",
      "risk and expected loss\n",
      "After getting the posterior conditional distribution of the parameter given\n",
      "the observable random variable, for a given loss function L, we determine the\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.3 Bayes Rules\n",
      "353\n",
      "estimator δ that minimizes the posterior risk, which is the expected value of\n",
      "the loss wrt the posterior distribution on the parameter:\n",
      "arg min\n",
      "δ\n",
      "Z\n",
      "Θ\n",
      "L(θ, δ(x))fΘ|x(θ)dθ.\n",
      "The action that minimizes the posterior risk the Bayes rule.\n",
      "The Bayes rule is determined by\n",
      "•\n",
      "the conditional distribution of the observable\n",
      "•\n",
      "the prior distribution of the parameter\n",
      "•\n",
      "the nature of the decision; for example, if the decision is an estimator, then\n",
      "the function of the parameter to be estimated, that is, the estimand\n",
      "•\n",
      "the loss function\n",
      "The expected loss with respect to the posterior distribution of the parameter\n",
      "is the objective to be minimized.\n",
      "The Complete Class Theorem\n",
      "One of the most important characteristics of Bayesian estimation is that all\n",
      "“good” estimators are either Bayes or limiting Bayes; that is, the class of\n",
      "Bayes and limiting Bayes estimators is a complete class of decision rules (see\n",
      "page 265).\n",
      "Theorem 4.2 (admissible estimators are Bayes)\n",
      "An admissible estimator is either Bayes or limiting Bayes.\n",
      "Proof.\n",
      "4.3.1 Properties of Bayes Rules\n",
      "For any loss function we have the following relations with admissibility and\n",
      "minimaxity. First, despite the fact that admissibility is not really relevant in\n",
      "the Bayesian paradigm (see page 328), if a Bayes rule is unique it is admissible.\n",
      "Theorem 4.3 (admissibility of unique Bayes rule)\n",
      "Suppose that δ(X) is a unique Bayes rule in a decision problem. Then δ(X)\n",
      "is admissible in that decision problem.\n",
      "Proof. Suppose that ˜δ(X) is a Bayes rule.****\n",
      "Theorem 3.12 states that an admissible estimator with a constant risk is\n",
      "minimax with respect to the same loss function and distribution. The same\n",
      "statement is true for a Bayes estimator with a constant risk:\n",
      "Theorem 4.4 (Bayes rule or limiting Bayes rule is minimax if it has constant risk )\n",
      "A Bayes rule or limiting Bayes rule with a constant risk is minimax with\n",
      "respect to the same loss function and distribution.\n",
      "Proof.\n",
      "***** prove these\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "354\n",
      "4 Bayesian Inference\n",
      "4.3.2 Equivariant Bayes Rules\n",
      "We discussed the general problem of invariance and equivariance of statistical\n",
      "procedures in Section 3.4. We now consider these concepts in the context of\n",
      "Bayesian inference.\n",
      "We say that a prior distribution Q for Θ is invariant with respect to eG if\n",
      "for ∀˜g ∈eG, the distribution of ˜g(Θ) is also Q.\n",
      "That is, for all measurable B,\n",
      "EQ(IB(˜g(Θ))) = EQ(IB(Θ))\n",
      "We deﬁne a σ-ﬁeld L over the set of functions in a group G, and then for a\n",
      "measurable set of transformations B, we consider right compositions Bh (for\n",
      "h ∈G, this is {gh : g ∈B}), and left compositions gB.\n",
      "Deﬁnition 0.1.18\n",
      "If λ(Bh) = λ(B) for all B ∈L and h ∈G, λ is said to be right Haar\n",
      "invariant, and if λ(gB) = λ(B) for all B ∈L and h ∈G, λ is said to be left\n",
      "Haar invariant.\n",
      "**** Relevance: relation to Jeﬀrey’s noninformative prior.\n",
      "4.3.3 Bayes Estimators with Squared-Error Loss Functions\n",
      "The Bayes estimator depends on the loss function as well as the prior dis-\n",
      "tribution. As in many cases, if the loss function is squared-error, the optimal\n",
      "procedure has some useful properties. For Bayes estimators with squared-error\n",
      "loss functions, we have the following properties.\n",
      "Theorem 4.5\n",
      "Under squared-error loss, the Bayes estimator is the posterior mean; that is,\n",
      "the expected value of the estimand, where the expected value is taken wrt the\n",
      "posterior conditional distribution.\n",
      "Proof. Exercise.\n",
      "Theorem 4.6\n",
      "Squared-error loss and a conjugate prior yield Bayes estimators for E(X) that\n",
      "are linear in X.\n",
      "Proof. Exercise.\n",
      "Theorem 4.7\n",
      "Under squared-error loss, if T is the Bayes estimator for g(θ), then aT + b is\n",
      "the Bayes estimator for ag(θ) + b for constants a and b.\n",
      "Proof. Exercise.\n",
      "Lemma 4.8.1\n",
      "If T is a Bayes estimator under squared-error loss, and if T is unbiased, then\n",
      "the Bayes risk rT (PΘ) = 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.3 Bayes Rules\n",
      "355\n",
      "Proof. For the Bayes estimator T(X) with E(T(X)|θ) = g(θ), we have\n",
      "E(g(θ)T(X)) = E(g(θ)E(T(X)|θ)) = E((g(θ))2).\n",
      "Alternatively,\n",
      "E(g(θ)T(X)) = E(T(X)E(g(θ)|X)) = E((T(X))2).\n",
      "Then we have\n",
      "rT (PΘ) = E((T(X) −g(θ)|X))2)\n",
      "= E((T(X))2) + E((g(θ))2) −2E(g(θ)T(X))\n",
      "= 0.\n",
      "Hence, by the condition of equation (3.84) we have the following theorem.\n",
      "Theorem 4.8\n",
      "Suppose T is an unbiased estimator. Then T is not a Bayes estimator under\n",
      "squared-error loss.\n",
      "Examples\n",
      "There are two standard examples of Bayesian analyses that serve as models\n",
      "for Bayes estimation under squared-error loss. These examples, Example 4.6\n",
      "and 4.8, should be in the student’s bag of easy pieces. In both of these exam-\n",
      "ples, the prior is in a parametric conjugate family.\n",
      "In this section, we also consider estimation using an improper prior (Ex-\n",
      "ample 4.9).\n",
      "Example 4.6 (Continuation of Example 4.2) Estimation of the Bi-\n",
      "nomial Parameter with Beta Prior and a Squared-Error Loss\n",
      "We return to the problem in which we model the conditional distribution of an\n",
      "observable random variable X as a binomial(n, π) distribution, conditional on\n",
      "π, of course. Suppose we assume π comes from a beta(α, β) prior distribution;\n",
      "that is, we consider a random variable Π that has beta distribution. We wish\n",
      "to estimate Π.\n",
      "Let us choose the loss to be squared-error. In this case we know the risk is\n",
      "minimized by choosing the estimate as δ(x) = E(Π|x), where the expectation\n",
      "is taken wrt the distribution with density fΠ|x.\n",
      "We recognize the posterior conditional distribution as a beta(x + α, n −\n",
      "x + β), so we have the Bayes estimator for squared-error loss and beta prior\n",
      "α + X\n",
      "α + β + n.\n",
      "(4.45)\n",
      "We should study this estimator from various perspectives.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "356\n",
      "4 Bayesian Inference\n",
      "linear combination of expectations\n",
      "First, we note that it is a weighted average of the mean of the prior\n",
      "and the standard UMVUE. (We discuss the UMVUE for this problem in\n",
      "Examples 5.1 and 5.5.)\n",
      "\u0012\n",
      "α + β\n",
      "α + β + n\n",
      "\u0013\n",
      "α\n",
      "α + β +\n",
      "\u0012\n",
      "n\n",
      "α + β + n\n",
      "\u0013 X\n",
      "n .\n",
      "(4.46)\n",
      "This is a useful insight, but we should not suppose that all Bayes estima-\n",
      "tors work that way.\n",
      "unbiasedness\n",
      "We see that the Bayes estimator cannot be unbiased if α ̸= 0 or β ̸= 0\n",
      "in the prior beta distribution (see Theorem 4.8). If α = 0 and β = 0, the\n",
      "prior is improper because the integral of the prior density above does not\n",
      "converge. We can, however, set up the risk in the form of equation (4.39),\n",
      "and minimize it without ever determining a posterior distribution. The\n",
      "solution, X/n, which happens to be the UMVUE, is a generalized Bayes\n",
      "estimator. Because\n",
      "lim\n",
      "α→0+,β→0+\n",
      "α + X\n",
      "α + β + n = X\n",
      "n ,\n",
      "(4.47)\n",
      "and for α > 0 and β > 0, the prior is proper, we see that the UMVUE is\n",
      "a limit of Bayes estimators.\n",
      "admissibility\n",
      "By Theorem 3.10 we know that the biased Bayes estimator (4.45) is not\n",
      "admissible under a squared-error loss. The limiting Bayes estimator (4.47)\n",
      "is admissible under squared-error loss, and thus we see an application of\n",
      "Theorem 4.2.\n",
      "minimaxity\n",
      "Could the Bayes estimator with this prior and squared-error loss function\n",
      "be minimax? Work out the risk (Exercise 4.10), and determine values of\n",
      "α and β such that it is constant. This will be minimax. The solution (to\n",
      "make it independent of π) is\n",
      "α = β = √n/2.\n",
      "(4.48)\n",
      "Notice what this does: it tends to push the estimator toward 1/2, which\n",
      "has a maximum loss of 1/2, that is, the minimum maximum loss possible.\n",
      "Recall the randomized minimax estimator δ1/(n+1)(X), equation (3.108)\n",
      "in Example 3.21.\n",
      "Jeﬀreys’s noninformative prior\n",
      "The Jeﬀreys’s noninformative prior in this case is proportional to\n",
      "p\n",
      "I(π);\n",
      "see equation (4.41). Because the binomial is a member of the exponential\n",
      "family, we know I(π) = 1/V(T), where E(T) = π. So I(π) = n/π(1 −π).\n",
      "Jeﬀreys’s prior is therefore beta(1/2, 1/2). The Bayes estimator corre-\n",
      "sponding to this noninformative prior is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.3 Bayes Rules\n",
      "357\n",
      "X + 1\n",
      "2\n",
      "n + 1 .\n",
      "(4.49)\n",
      "This is often used as an estimator of π in situations where X > 0 is rare.\n",
      "An estimator of π as 0 may not be very reasonable.\n",
      "equivariance\n",
      "For the group invariant problem in which g(X) = n−X and ¯g(π) = 1−π,\n",
      "we see that the loss function is invariant if g∗(T) = 1−T. In this case, the\n",
      "Bayes estimator is equivariant if the prior is symmetric, that is, if α = β.\n",
      "empirical Bayes\n",
      "We can make an empirical Bayes model from this example, as discussed\n",
      "in Section 4.2.6. We consider the observable random variable to be one of\n",
      "a set, Xk, each with conditional distribution binomial(n, πk), where the\n",
      "πk are all distributed independently as beta(α, β). An empirical Bayes\n",
      "procedure involves estimating α and β, and then proceeding as before.\n",
      "Although any (reasonable) estimates of α and β would work, we generally\n",
      "use the MLEs. We get those by forming the conditional likelihood of x\n",
      "given α and β, and then maximizing to get ˆα and ˆβ. (We do this numer-\n",
      "ically because it cannot be done in closed form. We get the conditional\n",
      "likelihood of x given α and β by ﬁrst forming the joint of x and the πk’s,\n",
      "and integrating out the πk’s.) The empirical Bayes estimator for πk is\n",
      "ˆα + Xk\n",
      "ˆα + ˆβ + n\n",
      ".\n",
      "(4.50)\n",
      "hierarchical Bayes\n",
      "If we put prior distributions on α and β, say gamma distributions with\n",
      "diﬀerent parameters, we could form a hierarchical Bayes model and use\n",
      "iterative conditional simulated sampling to compute the estimates. (This\n",
      "type of approach is called Markov chain Monte Carlo, or speciﬁcally in this\n",
      "case, Gibbs sampling. We discuss this approach in general in Section 4.7,\n",
      "and Gibbs sampling speciﬁcally beginning on page 669.) We would do this\n",
      "by working out the full conditionals.\n",
      "The squared-error loss function is a very simple and common loss function.\n",
      "(In fact, the student must be very careful to remember that many simple\n",
      "properties of statistical methods depend on this special loss function.) We\n",
      "will consider the estimation problem of Example 4.6 with other loss functions\n",
      "in Example 4.11 and in Exercise 4.9.\n",
      "The prior distribution in Example 4.6 is a conjugate prior (when it exists;\n",
      "that is, when α > 0 and β > 0), because the posterior is in the same parametric\n",
      "family. A conjugate prior and a squared-error loss function always yield Bayes\n",
      "estimators for E(X) that are linear in X, as we see in this speciﬁc case. Other\n",
      "priors may not be as easy to work with.\n",
      "Example 4.7 Estimation of the Negative Binomial Parameter with\n",
      "Beta Prior and a Squared-Error Loss\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "358\n",
      "4 Bayesian Inference\n",
      "In Example 3.12 on page 237 we discussed two diﬀerent data-generating pro-\n",
      "cess, one of which led to the (conditional) binomial distribution of Exam-\n",
      "ple 4.6. The other, related data-generating process led to a (conditional) neg-\n",
      "ative binomial distribution for a random variable N, that corresponds to the\n",
      "parameter n in Example 4.6 This negative binomial distribution has the same\n",
      "parameter π and another parameter that corresponds to the observed x in\n",
      "Example 4.6.\n",
      "Given a ﬁxed value x, we have a random variable N whose conditional\n",
      "distribution given π, has the probability function\n",
      "pN|π(n) =\n",
      "\u0012n −1\n",
      "x −1\n",
      "\u0013\n",
      "πx(1 −π)n−xIx,x+1,...(n).\n",
      "Again assuming a beta(α, β) prior, with known α and β, and going through\n",
      "the usual steps, we obtain the conditional of the parameter given the data as\n",
      "pΠ|n(π) = Γ(x + α)Γ(n −x + β)\n",
      "Γ(n + α + β)\n",
      "πx+α−1(1 −π)n−x+β−1I(0,1)(π),\n",
      "which is a beta distribution. As for the binomial, the beta is a conjugate prior\n",
      "for the negative binomial. Now, we want to estimate π under a squared error\n",
      "loss. We know that the Bayesian estimate under a squared error loss is the\n",
      "posterior mean. In this case, because the distribution is a beta, and we easily\n",
      "work out the mean. Hence, we have have the Bayes estimator,\n",
      "ˆπ = EΠ|n(π) =\n",
      "α + x\n",
      "α + β + N .\n",
      "(4.51)\n",
      "Notice that this is the same estimator as expression (4.45) for the binomial\n",
      "parameter; thus, the estimators would conform to the likelihood principle.\n",
      "As in Example 4.6 with the estimator of the binomial parameter, we could\n",
      "consider whether we can choose speciﬁc values of the hyperparameters so as\n",
      "to yield Bayes estimators with various properties (see Exercise 4.12). We see,\n",
      "for example, that with the given loss and any beta prior, it is not possible to\n",
      "obtain even a generalized estimator that is unbiased.\n",
      "Example 4.8 (Continuation of Example 4.5) Estimation of the Nor-\n",
      "mal Mean and Variance with Inverted Chi-Squared and Conditional\n",
      "Normal Priors and a Squared-Error Loss\n",
      "For estimating both µ and σ2 in N(µ, σ2), as in Example 4.5, a conjugate\n",
      "prior family can be constructed by ﬁrst deﬁning a marginal prior on σ2 and\n",
      "then a conditional prior on µ|σ2.\n",
      "For the estimators, we minimize the expected loss with respect to the joint\n",
      "posterior distribution given in equation (4.32). For a squared-error loss, this\n",
      "yields the posterior means as the estimators.\n",
      "Another way this problem may be approached is by reparametrizing the\n",
      "normal, and in place of σ2, using 1/(2τ).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.3 Bayes Rules\n",
      "359\n",
      "We now consider use of an improper prior when estimating the mean of a\n",
      "normal distribution with known variance.\n",
      "Example 4.9 (Continuation of Example 4.4) Use of an Improper\n",
      "Prior and a Squared-Error Loss for Estimation of the Normal Mean\n",
      "When the Variance Is Known\n",
      "Suppose we assume the observable random variable X has a N(µ, σ2) distri-\n",
      "bution in which σ2 is known, and the parameter of interest, µ, is assumed to\n",
      "be a realization of an unobservable random variable M ∈IR.\n",
      "Let us use a prior distribution for M that is uniform over IR. This is\n",
      "obviously an improper prior, and the measure dFM(µ) is just the Lebesgue\n",
      "measure. Let us assume that we have n observations x1, . . ., xn. Instead of\n",
      "going through the standard steps to get a posterior PDF, we go directly to\n",
      "the problem of determining the Bayes estimator by minimizing the risk in\n",
      "equation (4.39), if that minimum exists. We have\n",
      "r(FM, T) =\n",
      "Z\n",
      "M\n",
      "Z\n",
      "X\n",
      "L(µ, T(x))dFX|µ(x)dFM(µ)\n",
      "=\n",
      "Z ∞\n",
      "−∞\n",
      "Z ∞\n",
      "−∞\n",
      "(µ −T(x))2\n",
      "1\n",
      "\u0010√\n",
      "2πσ2\n",
      "\u0011n e−P(xi−µ)2/2σ2dx dµ.\n",
      "The questions are whether we can reverse the integrations and whether the\n",
      "integral with respect to dµ is ﬁnite. The two questions are the same, and we\n",
      "see that the answer is aﬃrmative because for ﬁxed x,\n",
      "Z ∞\n",
      "−∞\n",
      "(µ −a)2\n",
      "1\n",
      "\u0010√\n",
      "2πσ2\n",
      "\u0011n e−P(xi−µ)2/2σ2 dµ < ∞.\n",
      "We determine the estimator that minimizes the Bayes risk by diﬀerentiating\n",
      "the expression above wrt a, setting the result to 0, and solving for a. Using the\n",
      "“Pythagorean Theorem” of statistics, equation (0.0.99), in the exponential, we\n",
      "get the minimizer as T(x) = ¯x.\n",
      "This generalized Bayes estimator is the optimal estimator under various\n",
      "other criteria: it is the MLE (Example 3.13), it is the MREE under a con-\n",
      "vex and even location-scale invariant loss function (Example 3.23), and it is\n",
      "UMVU (Example 5.6).\n",
      "4.3.4 Bayes Estimation with Other Loss Functions\n",
      "For certain loss functions, even in relatively simple settings, a Bayes estimator\n",
      "may not exist; see Example 4.10. In some cases, however, we may choose a\n",
      "particular loss function so as to obtain an unbiased estimator (recall Theo-\n",
      "rem 4.8 concerning the squared-error loss). In other cases, we may seek a loss\n",
      "function that yields a constant risk. This gives us an admissible estimator.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "360\n",
      "4 Bayesian Inference\n",
      "Example 4.10 Nonexistence of a Bayes Estimator\n",
      "Suppose that X ∼N(θ, 1) given Θ = θ and let the prior for Θ be N(0, 1).\n",
      "Consider the loss function\n",
      "L(T, θ) =\n",
      "\u001a\n",
      "0 if T ≥θ\n",
      "1 if T < θ.\n",
      "(That is, we want to be sure not to underestimate θ.) Now, consider the\n",
      "constant estimator Tn = n (where n is the sample size). The risk function\n",
      "is R(Tn, θ) = I]−∞,θ[(n). Hence, the average risk is Pr(Θ > n) where Θ ∼\n",
      "N(0, 1). Now, consider any Bayes estimator δ, and let Φ be the CDF for\n",
      "N(0, 1). We have\n",
      "0 ≤inf\n",
      "δ\n",
      "Z\n",
      "R(δ, θ)dΦ(θ)\n",
      "≤inf\n",
      "n\n",
      "Z\n",
      "R(Tn, θ)dΦ(θ)\n",
      "= inf\n",
      "n Pr(Θ > n)\n",
      "= 0.\n",
      "So, in order for any estimator δ to be a Bayes estimator, it must have an\n",
      "average risk of 0, which is not possible.\n",
      "See Exercise 4.21 for further issues concerning this example.\n",
      "Example 4.11 (Continuation of Example 4.6) Estimation of the Bi-\n",
      "nomial Parameter with Beta Prior and Other Loss Functions\n",
      "We return to the problem in which we model the conditional distribution of\n",
      "an observable random variable X as a binomial(n, π) distribution, conditional\n",
      "on π, of course. Suppose we assume π comes from a beta(α, β) prior distribu-\n",
      "tion; that is, we consider a random variable Π that has beta distribution. As\n",
      "in Example 4.6, we wish to estimate Π.\n",
      "•\n",
      "Could we deﬁne a loss function so that the Bayes estimator is unbiased for\n",
      "a proper prior? Yes. Take\n",
      "L(π, d) = (d −π)2\n",
      "π(1 −π),\n",
      "(4.52)\n",
      "and take a beta(1,1) (that is, uniform) prior. This yields the Bayes esti-\n",
      "mator\n",
      "X\n",
      "n .\n",
      "(4.53)\n",
      "•\n",
      "For any loss function other than the squared-error, will the Bayes estimator\n",
      "be minimax? Yes, the loss function (4.52) yields this property. The Bayes\n",
      "estimator X/n has constant risk (Exercise 4.22); therefore, it is minimax\n",
      "wrt that loss.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.4 Probability Statements in Statistical Inference\n",
      "361\n",
      "4.3.5 Some Additional (Counter)Examples\n",
      "Example 4.12 An Admissible Estimator that Is Not Bayes\n",
      "Example 4.13 A Bayes Estimator that Is Minimax but Not Admis-\n",
      "sible\n",
      "If a Bayes estimator is unique under any loss function, then it is admissi-\n",
      "ble under that loss (Theorem 4.3). Ferguson (1967) gave an example of a\n",
      "(nonunique) Bayes estimator that is not admissible, but has constant risk and\n",
      "so is minimax. ***********\n",
      "Example 4.14 A Limit of Unique Bayes Admissible Estimators that\n",
      "Is Not Admissible\n",
      "4.4 Probability Statements in Statistical Inference\n",
      "The process of parametric point estimation, as discussed in Section 4.3, or\n",
      "of testing a simple hypothesis, as discussed in Section 4.5.4, is not consis-\n",
      "tent with the fundamental Bayesian description of the random nature of the\n",
      "parameter. Because of the widespread role of point estimation and simple\n",
      "hypothesis testing in science and in regulatory activities, however, Bayesian\n",
      "statistical procedures must be developed and made available. Tests of com-\n",
      "posite hypotheses and identiﬁcation of Bayesian conﬁdence sets are more con-\n",
      "sistent with the general Bayesian paradigm. (The standard terminology for a\n",
      "Bayesian analogue of a conﬁdence set is credible set.)\n",
      "In the classical (frequentist) approach to developing methods for hypoth-\n",
      "esis testing and for determining conﬁdence sets, we assume a model Pθ for\n",
      "the state of nature and develop procedures by consideration of probabilities\n",
      "of the form Pr(T(X) ◦C(θ)|θ), where T(X) is a statistic, C(θ) is some region\n",
      "determined by the true (unknown) value of θ, and ◦is some relationship. The\n",
      "forms of T(X) and C(θ) vary depending on the statistical procedure. The\n",
      "procedure may be a test, in which case we may have T(X) = 1 or 0, accord-\n",
      "ing to whether the hypothesis is rejected or not, or it may by a procedure\n",
      "to deﬁne a conﬁdence set, in which case T(X) is a set. For example, if θ is\n",
      "given to be in ΘH, and the procedure T(X) is an α-level test of H, then\n",
      "Pr(T(X) = 1|θ ∈ΘH) ≤α. In a procedure to deﬁne a conﬁdence set, we may\n",
      "be able to say Pr(T(X) ∋θ) = 1 −α.\n",
      "These kinds of probability statements in the frequentist approach are some-\n",
      "what awkward, and a person without training in statistics may ﬁnd them par-\n",
      "ticularly diﬃcult to interpret. Instead of a statement of the form Pr(T(X)|θ),\n",
      "many people would prefer a statement of the form Pr(Θ ∈ΘH|X = x).\n",
      "In order to make such a statement, however, we ﬁrst must think of the\n",
      "parameter as a random variable and then we must formulate a conditional\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "362\n",
      "4 Bayesian Inference\n",
      "distribution for Θ, given X = x. In the usual Bayesian paradigm, we use a\n",
      "model that has several components: a marginal (prior) probability distribution\n",
      "for the unobservable random variable Θ; a conditional probability distribution\n",
      "for the observable random variable X, given Θ = θ; and other assumptions\n",
      "about the distributions. We denote the prior density of Θ as pΘ, and the\n",
      "conditional density of X as pX|θ. The procedure is to determine the conditional\n",
      "(posterior) distribution of Θ, given X = x. Since we model our information\n",
      "about Θ as a probability distribution, it is natural and appropriate to speak of\n",
      "probabilities about Θ. This is the kind of approach Laplace took in analyzing\n",
      "the urn problem, as we described at the beginning of this chapter.\n",
      "We can think of these diﬀerences in another way. If M is the model or\n",
      "hypothesis and D is the data, the diﬀerence is between\n",
      "Pr(D|M)\n",
      "(a “frequentist” interpretation), and\n",
      "Pr(M|D)\n",
      "(a “Bayesian” interpretation). People who support the latter interpretation\n",
      "will sometimes refer to the “prosecutor’s fallacy” in which Pr(E|H) is confused\n",
      "with Pr(H|E), where E is some evidence and H is some hypothesis.\n",
      "While in parametric point estimation, as discussed in Section 4.3, state-\n",
      "ments about probability may not be so meaningful, in tests of composite hy-\n",
      "potheses and identiﬁcation of credible sets, they are natural and appropriate.\n",
      "We discuss testing and determining credible sets in the next two sections.\n",
      "4.5 Bayesian Testing\n",
      "In statistical hypothesis testing, the basic problem is to decide whether or\n",
      "not to reject a statement about the distribution of a random variable. The\n",
      "statement must be expressible in terms of membership in a well-deﬁned class.\n",
      "We usually formulate the testing problem as one of deciding between two\n",
      "statements:\n",
      "H0 : θ ∈Θ0\n",
      "and\n",
      "H1 : θ ∈Θ1,\n",
      "where Θ0 ∩Θ1 = ∅.\n",
      "We do not treat H0 and H1 symmetrically; H0 is the hypothesis to be\n",
      "tested and H1 is the alternative. This distinction is important in developing a\n",
      "methodology of testing. We sometimes also refer to H0 as the “null hypothesis”\n",
      "and to H1 as the “alternative hypothesis”.\n",
      "In a Bayesian approach to this problem, we treat θ as a random variable,\n",
      "Θ and formulate the testing problem as beginning with prior probabilities\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.5 Bayesian Testing\n",
      "363\n",
      "p0 = Pr(Θ ∈Θ0)\n",
      "and\n",
      "p1 = Pr(Θ ∈Θ1),\n",
      "and then, given data x, determining posterior conditional probabilities\n",
      "ˆp0 = Pr(Θ ∈Θ0)\n",
      "and\n",
      "ˆp1 = Pr(Θ ∈Θ1).\n",
      "These latter probabilities can be identiﬁed with the posterior likelihoods, say\n",
      "L0 and L1.\n",
      "In the Bayesian framework, we are interested in the probability that H0 is\n",
      "true. The prior distribution provides an a priori probability, and the posterior\n",
      "distribution based on the data provides a posterior probability that H0 is true.\n",
      "Clearly, we would choose to reject H0 when the probability that it is true is\n",
      "small.\n",
      "4.5.1 A First, Simple Example\n",
      "Suppose we wish to test\n",
      "H0 : P = P0\n",
      "versus\n",
      "H1 : P = P1,\n",
      "and suppose that known probabilities p0 and p1 = 1 −p0 can be assigned to\n",
      "H0 and H1 prior to the experiment. We see\n",
      "•\n",
      "The overall probability of an error resulting from the use of the test δ is\n",
      "p0E0(δ(X)) + p1E1(1 −δ(X)).\n",
      "•\n",
      "The Bayes test that minimizes this probability is given by\n",
      "δ(x) =\n",
      "\n",
      "\n",
      "\n",
      "1 when ˆp1(x) > kˆp0(x)\n",
      "0 when ˆp1(x) < kˆp0(x),\n",
      "for k = p0/p1.\n",
      "•\n",
      "The conditional probability of Hi given X = x, that is, the posterior\n",
      "probability of Hi, is\n",
      "pi ˆpi(x)\n",
      "p0ˆp0(x) + p1ˆp1(x)\n",
      "and the Bayes test therefore decides in favor of the hypothesis with the\n",
      "larger posterior probability.\n",
      "Testing as an Estimation Problem\n",
      "As an estimation problem, the testing problem is equivalent to estimating the\n",
      "indicator function IΘ0(θ). We use a statistic S(X) as an estimator of IΘ0(θ).\n",
      "The estimand is in {0, 1}, and so S(X) should be in {0, 1}, or at least in [0, 1].\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "364\n",
      "4 Bayesian Inference\n",
      "For a 0-1 loss function, the Bayes estimator of IΘ0(θ) is the function that\n",
      "minimizes the posterior risk, EΘ|x(L(Θ, s)). The risk is just the posterior\n",
      "probability, so the Bayesian solution using this loss is\n",
      "S(x) =\n",
      "\u001a1 if Pr(θ ∈Θ0|x) > Pr(θ /∈Θ0|x)\n",
      "0 otherwise,\n",
      "where Pr(·) is evaluated with respect to the posterior distribution PΘ|x.\n",
      "4.5.2 Loss Functions\n",
      "Due to the discrete nature of the decision regarding a test of an hypothesis,\n",
      "discrete loss functions are often more appropriate.\n",
      "The 0-1-γ Loss Function\n",
      "In a Bayesian approach to hypothesis testing using the test δ(X) ∈{0, 1}, we\n",
      "often formulate a loss function of the form\n",
      "L(θ, d) =\n",
      "\u001a cd for θ ∈Θ0\n",
      "bd for θ ∈Θ1\n",
      "where c1 > c0 and b0 > b1, with c0 = b1 = 0, b0 = 1, and c1 = γ > 0. (This is\n",
      "a 0-1-γ loss function; see page 261.)\n",
      "A Bayesian action for hypothesis testing with a 0-1-γ loss function is fairly\n",
      "easy to determine. The posterior risk for choosing δ(X) = 1, that is, for\n",
      "rejecting the hypothesis, is\n",
      "cPr(Θ ∈ΘH0|X = x),\n",
      "and the posterior risk for choosing δ(X) = 0 is\n",
      "Pr(Θ ∈ΘH1|X = x),\n",
      "hence the optimal decision is to choose δ(X) = 1 if\n",
      "cPr(Θ ∈ΘH0|X = x) < Pr(Θ ∈ΘH1|X = x),\n",
      "which is the same as\n",
      "Pr(Θ ∈ΘH0|X = x) <\n",
      "1\n",
      "1 + c.\n",
      "In other words, the Bayesian approach says to reject the hypothesis if its\n",
      "posterior probability is small. The Bayesian approach has a simpler interpre-\n",
      "tation than the frequentist approach. It also makes more sense for other loss\n",
      "functions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.5 Bayesian Testing\n",
      "365\n",
      "The Weighted 0-1 or α0-α1 Loss Function\n",
      "Another approach to account for all possibilities and to penalize errors diﬀer-\n",
      "ently when the null hypothesis is true or false is use a weighted 0-1 loss function\n",
      "such as a α0-α1 loss (see page 261). Using the estimator S(X) = s ∈{0, 1},\n",
      "as above, we deﬁne\n",
      "L(θ, s) =\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "if s = IΘ0(θ)\n",
      "α0 if s = 0 and θ ∈Θ0\n",
      "α1 if s = 1 and θ /∈Θ0.\n",
      "The 0-1-γ loss and the α0-α1 loss could be deﬁned either in terms of the\n",
      "test rule δ or the estimator S; I chose to do one one way and the other another\n",
      "way just for illustration.\n",
      "The Bayes estimator of IΘ0(θ) using this loss is\n",
      "S(x) =\n",
      "\u001a 1 if Pr(θ ∈Θ0|x) >\n",
      "α1\n",
      "α0+α1\n",
      "0 otherwise,\n",
      "where again Pr(·) is evaluated with respect to the posterior distribution. To\n",
      "see that this is the case, we write the posterior loss\n",
      "Z\n",
      "Θ\n",
      "L(θ, s)dPΘ|x = a0Pr(θ ∈Θ0|x)I{0}(s) + a1Pr(θ /∈Θ0|x)I{1}(s),\n",
      "and then minimize it.\n",
      "Under a α0-α1 loss, the null hypothesis H0 is rejected whenever the pos-\n",
      "terior probability of H0 is too small. The acceptance level, α1/(α0 + α1), is\n",
      "determined by the speciﬁc values chosen in the loss function. The Bayes test,\n",
      "which is the Bayes estimator of IΘ0(θ), depends only on α0/α1. The larger\n",
      "α0/α1 is the smaller the posterior probability of H0 that allows for it to be\n",
      "accepted. This is consistent with the interpretation that the larger α0/α1 is\n",
      "the more important a wrong decision under H0 is relative to H1.\n",
      "Examples\n",
      "Let us consider two familiar easy pieces using a α0-α1 loss.\n",
      "Example 4.15 Binomial with Uniform Prior\n",
      "First, let X|π ∼binomial(π, n) and assume a prior on Π of U(0, 1) (a special\n",
      "case of the conjugate beta prior from Example 4.2). Suppose Θ0 = [0, 1/2].\n",
      "The posterior probability that H0 is true is\n",
      "(n + 1)!\n",
      "x!(n −x)!\n",
      "Z 1/2\n",
      "0\n",
      "πx(1 −π)n−xdπ.\n",
      "This is computed and then compared to the acceptance level. (Note that the\n",
      "integral is a sum of fractions.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "366\n",
      "4 Bayesian Inference\n",
      "Example 4.16 Normal with Known Variance and a Normal Prior\n",
      "on Mean\n",
      "For another familiar example, consider X|µ ∼N(µ, σ2), with σ2 known, and µ\n",
      "a realization of a random variable from N(µ0, σ2\n",
      "0). We considered this problem\n",
      "in Example 4.4 on page 341. We recall that M|x ∼N(µ0(x), ω2), where\n",
      "µ0(x) = σ2µ0 + σ2\n",
      "0x\n",
      "σ2 + σ2\n",
      "0\n",
      "and\n",
      "ω2 =\n",
      "σ2σ2\n",
      "0\n",
      "σ2 + σ2\n",
      "0\n",
      ".\n",
      "To test H0, we compute the posterior probability of H0. Suppose the null\n",
      "hypothesis is\n",
      "H0 : µ ≤0.\n",
      "Then\n",
      "Pr(H0|x) = Pr\n",
      "\u0012µ −µ0(x)\n",
      "ω\n",
      "≤−µ0(x)\n",
      "ω\n",
      "\u0013\n",
      "= Φ(−µ0(x)/ω).\n",
      "The decision depends on the α1/(α0 + α1) quantile of N(0, 1). Let zα0,α1 be\n",
      "this quantile; that is, Φ(zα0,α1) = α1/(α0 + α1). The H0 is accepted if\n",
      "−µ0(x) ≥zα0,α1ω.\n",
      "Rewriting this, we see that the null hypothesis is rejected if\n",
      "x > −σ2\n",
      "σ2\n",
      "0\n",
      "µ0 −\n",
      "\u0012\n",
      "1 + σ2\n",
      "σ2\n",
      "0\n",
      "\u0013\n",
      "ωzα0,α1.\n",
      "Notice a very interesting aspect of these tests. There is no predetermined\n",
      "acceptance level. The decision is based simply on the posterior probability\n",
      "that the null hypothesis is true.\n",
      "A diﬃculty of the α0-α1 loss function, of course, is the choice of α0 and\n",
      "α1. Ideally, we would like to choose these based on some kind of utility con-\n",
      "siderations, but sometimes this takes considerable thought.\n",
      "4.5.3 The Bayes Factor\n",
      "Given a prior distribution PΘ, let p0 be the prior probability that H0 is true,\n",
      "and p1 be the prior probability that H1 is true. The prior odds then is p0/p1.\n",
      "Similarly, let ˆp0 be the posterior probability that H0 is true given x, and ˆp1\n",
      "be the posterior probability that H1 is true, yielding the posterior odds ˆp0/ˆp1.\n",
      "The posterior odds is the ratio of the posterior likelihoods, L0/L1.\n",
      "The posterior probability of the event can be related to the relative odds.\n",
      "The posterior odds is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.5 Bayesian Testing\n",
      "367\n",
      "ˆp0\n",
      "ˆp1\n",
      "= p0\n",
      "p1\n",
      "fX|θ0(x)\n",
      "R\n",
      "fX|θ(x)dFΘ\n",
      ".\n",
      "The term\n",
      "BF(x) =\n",
      "fX|θ0(x)\n",
      "R fX|θ(x)dFΘ\n",
      "(4.54)\n",
      "is called the Bayes factor. The Bayes factor obviously also depends on the\n",
      "prior fΘ(θ).\n",
      "Rather than computing the posterior odds directly, we emphasize the\n",
      "Bayes factor, which for any stated prior odds yields the posterior odds. The\n",
      "Bayes factor is the posterior odds in favor of the hypothesis if p0 = 0.5.\n",
      "Note that, for the simple hypothesis versus a simple alternative, the Bayes\n",
      "factor simpliﬁes to the likelihood ratio:\n",
      "fX|θ0(x)\n",
      "fX|θ1(x).\n",
      "One way of looking at this likelihood ratio is to use MLEs under the two\n",
      "hypotheses:\n",
      "supΘ0 fX|θ(x)\n",
      "supΘ1 fX|θ(x).\n",
      "This approach, however, assigns Dirac masses at the MLEs, ˆθ0 and ˆθ1.\n",
      "The Bayes factor is more properly viewed as a Bayesian likelihood ratio,\n",
      "BF(x) =\n",
      "p0\n",
      "R\n",
      "Θ0 fX|θ(x)dθ\n",
      "p1\n",
      "R\n",
      "Θ1 fX|θ(x)dθ,\n",
      "and, from a decision-theoretic point of view, it is entirely equivalent to the\n",
      "posterior probability of the null hypothesis. Under the α0-α1 loss function,\n",
      "H0 is accepted when\n",
      "BF(x) > a1\n",
      "a0\n",
      "/p0\n",
      "p1\n",
      "From this, we see that the Bayesian approach eﬀectively gives an equal\n",
      "prior weight to the two hypotheses, p0 = p1 = 1/2 and then modiﬁes the error\n",
      "penalties as ˜ai = aipi, for i = 0, 1, or alternatively, incorporates the weighted\n",
      "error penalties directly into the prior probabilities:\n",
      "˜p0 =\n",
      "a0p0\n",
      "a0p0 + a1p1\n",
      "˜p1 =\n",
      "a1p1\n",
      "a0p0 + a1p1\n",
      ".\n",
      "The ratios such as likelihood ratios and relative odds that are used in\n",
      "testing carry the same information content if they are expressed as their re-\n",
      "ciprocals. These ratios can be thought of as evidence in favor of one hypothesis\n",
      "or model versus another hypothesis or model. The ratio provides a compar-\n",
      "ison of two alternatives, but there can be more than two alternatives under\n",
      "consideration. Instead of just H0 and H1 we may contemplate Hi and Hj, and\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "368\n",
      "4 Bayesian Inference\n",
      "follow the same steps using pi/pj. The Bayes factor then depends on i and j,\n",
      "and of course whether we use the odds ratio pi/pj or pj/pi. We therefore some-\n",
      "times write the Bayes factor as BFij(x) where the subscript ij indicates use\n",
      "of the ratio pi/pj. In this notation, the Bayes factor (4.54) would be written\n",
      "as BF01(x).\n",
      "Jeﬀreys (1961) suggested a subjective “scale” to judge the evidence of the\n",
      "data in favor of or against H0. Kass and Raftery (1995) discussed Jeﬀreys’s\n",
      "scale and other issues relating to the Bayes factor. They modiﬁed his original\n",
      "scale (by combining two categories), and suggested\n",
      "•\n",
      "if 0 < log10(BF10) < 0.5, the evidence against H0 is “poor”,\n",
      "•\n",
      "if 0.5 ≤log10(BF10) < 1, the evidence against H0 is “substantial”,\n",
      "•\n",
      "if 1 ≤log10(BF10) < 2, the evidence against H0 is “strong”, and\n",
      "•\n",
      "if 2 ≤log10(BF10), the evidence against H0 is “decisive”.\n",
      "Note that the Bayes factor is the reciprocal of the one we ﬁrst deﬁned in equa-\n",
      "tion (4.54). While this scale makes some sense, the separations are of course\n",
      "arbitrary, and the approach is not based on a decision theory foundation.\n",
      "Given such a foundation, however, we still have the subjectivity inherent in\n",
      "the choice of a0 and a1, or in the choice of a signiﬁcance level.\n",
      "Kass and Raftery (1995) also gave an interesting example illustrating the\n",
      "Bayesian approach to testing of the “hot hand” hypothesis in basketball. They\n",
      "formulate the null hypothesis (that players do not have a “hot hand”) as the\n",
      "distribution of good shots by a given player, Yi, out of ni shots taken in game\n",
      "i as binomial(ni, π), for games i = 1, . . ., g; that is, the probability for a given\n",
      "player, the probability of making a shot is constant in all games (within some\n",
      "reasonable period). A general alternative is H1 : Yi ∼binomial(ni, πi). We\n",
      "choose a ﬂat U(0, 1) conjugate prior for the H0 model. For the H1 model, we\n",
      "choose a conjugate prior beta(α, β) with α = ξ/ω and β = (1 −ξ)/ω. Under\n",
      "this prior, the prior expectation E(πi|ξ, ω) has an expected value of ξ, which\n",
      "is distributed as U(0, 1) for ﬁxed ω. The Bayes factor is is very complicated,\n",
      "involving integrals that cannot be solved in closed form. Kass and Raftery use\n",
      "this to motivate and to compare various methods of evaluating the integrals\n",
      "that occur in Bayesian analysis. One simple method is Monte Carlo.\n",
      "Often, however, the Bayes factor can be evaluated relatively easily for a\n",
      "given prior, and then it can be used to investigate the sensitivity of the results\n",
      "to the choice of the prior, by computing it for another prior.\n",
      "From Jeﬀreys’s Bayesian viewpoint, the purpose of hypothesis testing is\n",
      "to evaluate the evidence in favor of a particular scientiﬁc theory. Kass and\n",
      "Raftery make the following points in the use of the Bayes factor in the hy-\n",
      "pothesis testing problem:\n",
      "•\n",
      "Bayes factors oﬀer a straightforward way of evaluating evidence in favor\n",
      "of a null hypothesis.\n",
      "•\n",
      "Bayes factors provide a way of incorporating external information into the\n",
      "evaluation of evidence about a hypothesis.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.5 Bayesian Testing\n",
      "369\n",
      "•\n",
      "Bayes factors are very general and do not require alternative models to be\n",
      "nested.\n",
      "•\n",
      "Several techniques are available for computing Bayes factors, including\n",
      "asymptotic approximations that are easy to compute using the output\n",
      "from standard packages that maximize likelihoods.\n",
      "•\n",
      "In “nonstandard” statistical models that do not satisfy common regularity\n",
      "conditions, it can be technically simpler to calculate Bayes factors than to\n",
      "derive non-Bayesian signiﬁcance tests.\n",
      "•\n",
      "The Schwarz criterion (or BIC) gives a rough approximation to the log-\n",
      "arithm of the Bayes factor, which is easy to use and does not require\n",
      "evaluation of prior distributions. The BIC is\n",
      "BIC = −2 log(L(θm|x)) + k log n,\n",
      "where θm is the value of the parameters that specify a given model, k is\n",
      "the number of unknown or free elements in θm, and n is the sample size.\n",
      "The relationship is\n",
      "−BIC/2 −log(BF)\n",
      "log(BF)\n",
      "→0,\n",
      "as n →∞.\n",
      "•\n",
      "When we are interested in estimation or prediction, Bayes factors may be\n",
      "converted to weights to be attached to various models so that a composite\n",
      "estimate or prediction may be obtained that takes account of structural\n",
      "or model uncertainty.\n",
      "•\n",
      "Algorithms have been proposed that allow model uncertainty to be taken\n",
      "into account when the class of models initially considered is very large.\n",
      "•\n",
      "Bayes factors are useful for guiding an evolutionary model-building pro-\n",
      "cess.\n",
      "•\n",
      "It is important, and feasible, to assess the sensitivity of conclusions to the\n",
      "prior distributions used.\n",
      "***** stuﬀto add:\n",
      "pseudo-Bayes factors\n",
      "training sample\n",
      "arithmetic intrinsic Bayes factor\n",
      "geometric intrinsic Bayes factor\n",
      "median intrinsic Bayes factor\n",
      "The Bayes Risk Set\n",
      "A risk set can be useful in analyzing Bayesian procedures when the parameter\n",
      "space is ﬁnite. If\n",
      "Θ = {θ1, . . ., θk},\n",
      "(4.55)\n",
      "the risk set for a procedure T is a set in IRk:\n",
      "{(z1, ..., zk) : zi = R(θi, T)}.\n",
      "(4.56)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "370\n",
      "4 Bayesian Inference\n",
      "In the case of 0-1 loss, the risk set is a subset of the unit hypercube;\n",
      "speciﬁcally, for Θ = {0, 1}, it is a subset of the unit square: [0, 1] × [0, 1].\n",
      "4.5.4 Bayesian Tests of a Simple Hypothesis\n",
      "Although the test of a simple hypothesis versus a simple alternative, as in the\n",
      "example Section 4.5.1, is easy to understand and helps to direct our thinking\n",
      "about the testing problem, it is somewhat limited in application. In a more\n",
      "common application, we may have a dense parameter space Θ, and hypotheses\n",
      "that specify diﬀerent subsets of Θ. A common situation is the “one-sided”\n",
      "test for H0 : θ ≤θ0 versus H1 : θ > θ0. We can usually develop meaningful\n",
      "approaches to this problem, perhaps based on some boundary point of H0. A\n",
      "“two-sided” test, in which, for example, the alternative speciﬁes\n",
      "Θl = {θ : θ < θ0} ∪Θu = {θ : θ > θ0},\n",
      "(4.57)\n",
      "presents more problems for the development of reasonable procedures.\n",
      "In a Bayesian approach, when the parameter space Θ is dense, but either\n",
      "hypothesis is simple, there is a particularly troubling situation. This is because\n",
      "of the Bayesian interpretation of the problem as one in which a probability\n",
      "is to be associated with a statement about a speciﬁc value of a continuous\n",
      "random variable.\n",
      "Consider the problem in a Bayesian approach to deal with an hypothesis of\n",
      "the form H0 : Θ = θ0, that is Θ0 = {θ0}; versus the alternative H1 : Θ ̸= θ0.\n",
      "A reasonable prior for Θ with a continuous support would assign a prob-\n",
      "ability of 0 to Θ = θ0.\n",
      "One way of getting around this problem may be to modify the hypothesis\n",
      "slightly so that the null is a small interval around θ0. This may make sense,\n",
      "but it is not clear how to proceed.\n",
      "Another approach is, as above, to assign a positive probability, say p0,\n",
      "to the event Θ = θ0. Although it may not appear how to choose p0, just as\n",
      "it would not be clear how to choose an interval around θ0, we can at least\n",
      "proceed to simplify the problem following this approach. We can write the\n",
      "joint density of X and Θ as\n",
      "fX,Θ(x, θ) =\n",
      "\u001a p0fX|θ0(x)\n",
      "if θ = θ0,\n",
      "(1 −p0)fX|θ(x) if θ ̸= θ0.\n",
      "(4.58)\n",
      "There are a couple of ways of simplifying. Let us proceed by denoting the\n",
      "prior density of Θ over Θ −θ0 as λ. We can write the marginal of the data\n",
      "(the observable X) as\n",
      "fX(x) = p0fX|θ0(x) + (1 −p0)\n",
      "Z\n",
      "fX|θ(x)dλ(θ).\n",
      "(4.59)\n",
      "We can then write the posterior density of Θ as\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.5 Bayesian Testing\n",
      "371\n",
      "fΘ|x(θ|x) =\n",
      "(\n",
      "p1\n",
      "if θ = θ0,\n",
      "(1 −p1)\n",
      "fX|θ(x)\n",
      "fX(x) if θ ̸= θ0,\n",
      "(4.60)\n",
      "where\n",
      "p1 = p0fX|θ0(x)\n",
      "fX(x)\n",
      ".\n",
      "(4.61)\n",
      "This is the posterior probability of the event Θ = θ0.\n",
      "The Lindley-Jeﬀrey “Paradox”\n",
      "In testing a simple null hypothesis against a composite alternative, an anomaly\n",
      "can occur in which a classical frequentist test can strongly reject the null, but\n",
      "a Bayesian test constructed with a mixed prior consisting of a point mass at\n",
      "the null and a diﬀuse continuous prior over the remainder of the parameter\n",
      "space.\n",
      "Given a simple null hypothesis H0, the result of an experiment x, and a\n",
      "prior distribution that favors H0 weakly, a “paradox” occurs when the result\n",
      "x is signiﬁcant by a frequentist test, indicating suﬃcient evidence to reject H0\n",
      "at a given level, but the posterior probability of H0 given x is high, indicating\n",
      "strong evidence that H0 is in fact true. This is called Lindley’s paradox or the\n",
      "Lindley-Jeﬀrey paradox.\n",
      "This can happen at the same time when the prior distribution is the sum\n",
      "of a sharp peak at H0 with probability p and a broad distribution with the\n",
      "rest of the probability 1 −p. It is a result of the prior having a sharp feature\n",
      "at H0 and no sharp features anywhere else.\n",
      "Consider the testing problem in Example 4.16, except this time for a simple\n",
      "null hypothesis.\n",
      "Example 4.17 Normal with Known Variance and a Normal Prior\n",
      "on Mean; Simple Null Hypothesis (Lindley, 1957)\n",
      "Consider again X|µ ∼N(µ, σ2), with σ2 known. As before, to test H0, we\n",
      "compute the posterior probability of H0. Now, suppose the null hypothesis is\n",
      "H0 : µ = 0.\n",
      "In the case of the prior that supposed that µ a realization of a random variable\n",
      "from N(µ0, σ2\n",
      "0), which for a realization X = x yielded M|x ∼N(µ0(x), ω2).\n",
      "For this, we get the posterior probability of H0 to be 0.\n",
      "Let us modify the prior so as to give a non-zero probability p0 to the null\n",
      "hypothesis. As suggested above, we take a prior of the form\n",
      "˜fM (θ|x) =\n",
      "\u001a\n",
      "p0\n",
      "if µ = 0,\n",
      "(1 −p0)fM(µ) if µ ̸= 0,\n",
      "(4.62)\n",
      "where fM is the PDF of a N(µ0, σ2\n",
      "0). Suppose, further, our prior beliefs about\n",
      "µ are not strong, so we choose σ2\n",
      "0 much greater than σ2 = 1. Actually, it is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "372\n",
      "4 Bayesian Inference\n",
      "not important that the prior fM be proportional to the normal. The overall\n",
      "likelihood of the alternative hypothesis is\n",
      "L1 =\n",
      "Z ∞\n",
      "−∞\n",
      "1\n",
      "√\n",
      "2π exp(−(x −µ)2/2)(1 −p0)fM(µ)dµ,\n",
      "and the likelihood of the null hypothesis is\n",
      "L0 =\n",
      "1\n",
      "√\n",
      "2π exp(−x2/2).\n",
      "The important fact to note is that L1 can be quite small. This is because the\n",
      "prior gives a very small probability to a neighborhood of x, even a relatively\n",
      "large neighborhood in terms of σ = 1.\n",
      "The posterior odds are\n",
      "Pr(µ = 0|X = x)\n",
      "Pr(µ ̸= 0|X = x) =\n",
      "p0\n",
      "1 −p0\n",
      "L0\n",
      "L1\n",
      "which can be very large even if p0 is very small.\n",
      "This rather unreasonable conclusion of a standard Bayesian analysis has\n",
      "been discussed in many articles and books; see, for example, Shafer (1982) or\n",
      "Johnson and Rossell (2010).\n",
      "4.5.5 Least Favorable Prior Distributions\n",
      "In testing composite hypotheses, we often ask what is the “worst case” within\n",
      "the hypothesis. In a sense, this is the attempt to reduce the composite hypoth-\n",
      "esis to a simple hypothesis. This is the idea behind a p-value. In a Bayesian\n",
      "testing problem, this corresponds to a bound on the posterior probability.\n",
      "Again, consider the problem of testing H0 : Θ = θ0 versus the alternative\n",
      "H1 : Θ ̸= θ0. ***\n",
      "4.6 Bayesian Conﬁdence Sets\n",
      "4.6.1 Credible Sets\n",
      "In a Bayesian setup, we deﬁne a random variable Θ that corresponds to the\n",
      "parameter of interest, and the usual steps in a Bayesian analysis allows us\n",
      "to compute Pr(Θ ∈ΘH0|X = x). The problem in determining a conﬁdence\n",
      "set is an inverse problem; that is, for a given α, we determine Cα such that\n",
      "Pr(Θ ∈Cα|X = x) = 1 −α. Of course there may be many sets with this\n",
      "property. We need some additional condition(s).\n",
      "In the frequentist approach, we add the property that the region be the\n",
      "smallest possible. “Smallest” means with respect to some simple measure such\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.6 Bayesian Conﬁdence Sets\n",
      "373\n",
      "as the usual Lebesgue measure; in the one-dimensional continuous case, we\n",
      "seek the shortest interval. In the Bayesian approach, we so something similar,\n",
      "except we use the posterior density as a measure.\n",
      "The mechanics of determining credible sets begin with the standard\n",
      "Bayesian steps that yield the conditional distribution of the parameter given\n",
      "the observable random variable. If the density exists, we denote it as fΘ|x.\n",
      "At this point, we seek regions of θ in which fΘ|x(θ|x) is large. In general, the\n",
      "problem may be somewhat complicated, but in many situations of interest\n",
      "it is relatively straightforward. Just as in the frequentist approach, the iden-\n",
      "tiﬁcation of the region often depends on pivotal values, or pivotal functions.\n",
      "(Recall that a function g(T, θ) is said to be a pivotal function if its distribution\n",
      "does not depend on any unknown parameters.)\n",
      "It is often straightforward to determine one with posterior probability\n",
      "content of 1 −α.\n",
      "4.6.2 Highest Posterior Density Credible sets\n",
      "If the posterior density is fΘ|x(θ|x), we determine a number c such that the\n",
      "set\n",
      "Cα(x) = {θ : fΘ|x(θ) ≥cα}\n",
      "(4.63)\n",
      "is such that Pr(Θ ∈Cα|X = x) = 1 −α. Such a region is called a level 1 −α\n",
      "highest posterior density or HPD credible set.\n",
      "We may impose other conditions. For example, in a one-dimensional con-\n",
      "tinuous parameter problem, we may require that one endpoint of the interval\n",
      "be inﬁnite (that is, we may seek a one-sided conﬁdence interval).\n",
      "An HPD region can be disjoint if the posterior is multimodal.\n",
      "If the posterior is symmetric, all HPD regions will be symmetric about x.\n",
      "For a simple example, consider a N(0, 1) prior distribution on Θ and a\n",
      "N(θ, 1) distribution on the observable. The posterior given X = x is N(x, 1).\n",
      "All HPD regions will be symmetric about x. In the case of a symmetric den-\n",
      "sity, the HPD is the same as the centered equal-tail credible set; that is, the\n",
      "one with equal probabilities outside of the credible set. In that case, it is\n",
      "straightforward to determine one with posterior probability content of 1 −α.\n",
      "4.6.3 Decision-Theoretic Approach\n",
      "We can also use a speciﬁed loss function to approach the problem of deter-\n",
      "mining a conﬁdence set.\n",
      "We choose a region so as to minimize the expected posterior loss.\n",
      "For example, to form a two-sided interval in a one-dimensional continuous\n",
      "parameter problem, a reasonable loss function may be\n",
      "L(θ, [c1, c2]) =\n",
      "\n",
      "\n",
      "\n",
      "k1(c1 −θ) if θ < c1,\n",
      "0\n",
      "if c1 ≤θ ≤c2,\n",
      "k2(θ −c2) if θ > c2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "374\n",
      "4 Bayesian Inference\n",
      "This loss function also leads to the interval between two quantiles of the\n",
      "posterior distribution.\n",
      "It may not be HPD, and it may not be symmetric about some pivot quan-\n",
      "tity even if the posterior is symmetric.\n",
      "4.6.4 Other Optimality Considerations\n",
      "We may impose other conditions. For example, in a one-dimensional continu-\n",
      "ous parameter problem, we may require that one endpoint of the interval be\n",
      "inﬁnite (that is, we may seek a one-sided conﬁdence interval).\n",
      "Example 4.18 Credible sets for the Binomial Parameter with a Beta\n",
      "Prior\n",
      "Consider the problem of estimating π in a binomial(n, π) distribution with a\n",
      "beta(α, β) prior distribution, as in Example 4.6 on page 355.\n",
      "Suppose we choose the hyperparameters in the beta prior as α = 3 and\n",
      "β = 5. The prior, that is, the marginal distribution of Π, is as shown in\n",
      "Figure 4.1 and if n is 10 and we take one observation, x = 2 we have the\n",
      "conditional distribution of Π, as a beta with parameters x + α = 5 and\n",
      "n −x + β = 13, as shown in Figure 4.2.\n",
      "Now, given x = 2, and the original beta(3,5) prior, let’s ﬁnd an equal-tail\n",
      "95% credible set. Here’s some R code:\n",
      "a<-3\n",
      "b<-5\n",
      "n<-10\n",
      "x<-2\n",
      "alpha<-0.05\n",
      "lower<-qbeta(alpha/2,x+a,n-x+b)\n",
      "upper<-qbeta(1-alpha/2,x+a,n-x+b)\n",
      "pi<-seq(0,1,0.01)\n",
      "plot(pi,dbeta(pi,x+a,n-x+b),type=’l’,\n",
      "main=\"95\\% Credible set with x=2\",\n",
      "ylab=\"Posterior\",xlab=expression(pi))\n",
      "lines(c(lower,lower),c(0,dbeta(lower,x+a,n-x+b)))\n",
      "lines(c(upper,upper),c(0,dbeta(upper,x+a,n-x+b)))\n",
      "lines(c(0,1),c(0,0))\n",
      "We get the credible set shown in Figure 4.5. The probability in each tail\n",
      "is 0.025.\n",
      "Because the posterior density is not symmetric, it is not an easy matter\n",
      "to get the HPD credible set.\n",
      "The ﬁrst question is whether the credible set is an interval. This depends\n",
      "on whether the posterior is unimodal. As we have already seen in Section 4.2,\n",
      "the posterior in this case is unimodal if n > 0, and so the credible set is indeed\n",
      "an interval.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.6 Bayesian Conﬁdence Sets\n",
      "375\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "95% Credible Region with x=2\n",
      "π\n",
      "Posterior\n",
      "Figure 4.5.\n",
      "95% Credible set after Observing x = 2\n",
      "We can determine the region iteratively by starting with the equal-tail\n",
      "credible set. At each step in the iteration we have a candidate lower bound\n",
      "and upper bound. We determine which one has the higher value of the density,\n",
      "and then shift the interval in that direction. We continue this process, keeping\n",
      "the total probability constant at each step. Doing this we get the credible set\n",
      "shown in Figure 4.6. The probability in the lower tail is 0.014 and that in\n",
      "the upper tail is 0.036. The density is 0.642 at each endpoint; that is, in\n",
      "equation (4.63), cα = 0.642.\n",
      "Here’s the R code that yielded the HPD:\n",
      "a<-3\n",
      "b<-5\n",
      "n<-10\n",
      "x<-2\n",
      "alpha<-0.05\n",
      "#\n",
      "start by determining the equal-tail CR, using the posterior\n",
      "lower<-qbeta(alpha/2,x+a,n-x+b)\n",
      "upper<-qbeta(1-alpha/2,x+a,n-x+b)\n",
      "#\n",
      "set a tolerance for convergence\n",
      "tol <- 0.005 # to get the density values to agree to 3 decimal places\n",
      "a10 <- 0\n",
      "a20 <- 0\n",
      "a1 <- alpha/2\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "376\n",
      "4 Bayesian Inference\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "95% HPD Credible Region with x=2\n",
      "π\n",
      "Posterior\n",
      "Figure 4.6.\n",
      "HPD 95% Credible set after Observing x = 2\n",
      "a2 <- 1-alpha/2\n",
      "adj <- a1\n",
      "d <- 1\n",
      "while (abs(d)>tol){\n",
      "#\n",
      "determine difference in the density at the two candidate points\n",
      "d <- dbeta(lower,x+a,n-x+b)-dbeta(upper,x+a,n-x+b)\n",
      "#\n",
      "halve the adjustment in each iteration\n",
      "adj <- adj/2\n",
      "#\n",
      "if density at lower boundary is higher, shift interval to the left\n",
      "s <- 1\n",
      "if(d>0) s <- -1\n",
      "a1 <- a1 + s*adj\n",
      "a2 <- a2 + s*adj\n",
      "lower<-qbeta(a1,x+a,n-x+b)\n",
      "upper<-qbeta(a2,x+a,n-x+b)\n",
      "}\n",
      "4.7 Computational Methods in Bayesian Inference;\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.7 Computational Methods\n",
      "377\n",
      "Markov Chain Monte Carlo\n",
      "Monte Carlo techniques often allow us to make statistical inferences when the\n",
      "statistical method involves intractable expressions. In applications in Bayesian\n",
      "inference, we can study the posterior distribution, which may be intractable or\n",
      "which may be known only proportionally, by studying random samples from\n",
      "that distribution.\n",
      "In parametric Bayesian inference, the objective is to obtain the conditional\n",
      "posterior distribution of the parameter, given the observed data. This is QH\n",
      "in equation (4.2), and it is deﬁned by the density in step 5 in the procedure\n",
      "outlined in Section 4.2.3. This density contains all of the information about\n",
      "the parameter of interest, although we may wish to use it for speciﬁc types of\n",
      "inference about the parameter, such as a point estimator or a credible set.\n",
      "Understanding the Posterior Distribution\n",
      "As with any probability distribution, a good way to understand the posterior\n",
      "distribution is to take a random sample from it. In the case of the posterior\n",
      "distribution, we cannot take a physical random sample. We can, however,\n",
      "simulate a random sample, using methods discussed in Section 0.0.7, beginning\n",
      "on page 663.\n",
      "In single-parameter cases, random samples from the posterior distribu-\n",
      "tion can often be generated using a direct acceptance/rejection method if the\n",
      "constant of proportionality is known. If the posterior density is known only\n",
      "proportionally, a Metropolis-Hastings method often can be used.\n",
      "Often the posterior density is a fairly complicated function, especially in\n",
      "multi-parameter cases or in hierarchical models. In such cases, we may be able\n",
      "to express the conditional density of each parameter given all of the other\n",
      "parameters. In this case, it is fairly straightforward to use a Gibbs sampling\n",
      "method to generate samples from the multivariate distribution. Consider the\n",
      "relatively simple case in Example 4.5. The joint posterior PDF is given in\n",
      "equation (4.32). We can get a better picture of this distribution by simulating\n",
      "random observations from it. To do this we generate a realization σ2 from\n",
      "the marginal posterior with PDF given in equation (4.33), and then with that\n",
      "value of σ2, we generate a realization µ from the conditional posterior with\n",
      "PDF given in equation (4.34).\n",
      "Example 4.19 illustrates this technique for a hierarchical model.\n",
      "The simulated random samples from the posterior distribution gives us a\n",
      "picture of the density. It is often useful to make pair-wise scatter plots of the\n",
      "samples or estimated contour plots of the density based on the samples.\n",
      "Simulated random samples can be used to approximate expectations of\n",
      "functions of the random parameters with respect to the posterior density\n",
      "(this is Monte Carlo quadrature), and they can also be used to identify other\n",
      "properties of the posterior distribution, such as its mode.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "378\n",
      "4 Bayesian Inference\n",
      "Computing the MAP\n",
      "Computation of the MAP is essentially an optimization problem. In many\n",
      "cases, simulated annealing (see Section 0.4.3 on page 829) is a very eﬀective\n",
      "method of determining the optimum point. The approach for optimizing the\n",
      "posterior probability density function is essentially the same as a Metropolis\n",
      "method for simulating random observations from the posterior distribution.\n",
      "A Hierarchical Bayesian Model\n",
      "Following custom, we use brackets to denote densities; [X, Y ], [X|Y ], and [X]\n",
      "represent the joint, conditional, and marginal densities, respectively.\n",
      "In a hierarchical Bayesian model, the joint distribution of the data and\n",
      "parameters is\n",
      "[X|θ1] × [θ1|θ2] × [θ2|θ3] × · · · × [θk−1|θk] × [θk]\n",
      "The thing of interest is posterior density [θ1|X].\n",
      "The hierarchical structure implies\n",
      "[θ1|X, θi,(i̸=1)] = [θ1|X, θ2]\n",
      "= [θi|θi−1, θi+1]\n",
      "= [θk|θk−1]\n",
      "Gibbs sampling can be used to estimate the marginal posterior densities.\n",
      "Example 4.19 Gibbs Sampling Example from Gelfand and Smith,\n",
      "JASA\n",
      "The paper by Gelfand and Smith (1990) was very important in popularizing\n",
      "the Gibbs method.\n",
      "Consider an exchangeable Poisson model in which independent counts are\n",
      "observed over diﬀering periods of time.\n",
      "The data are {(si, ti)}. Each yields a rate ri.\n",
      "Assume [si|λi] = P(λiti).\n",
      "Assume a gamma prior distribution on the λi’s with density\n",
      "1\n",
      "βαΓ(α)λα−1\n",
      "i\n",
      "e−λi/β\n",
      "Further, assume β has an inverted gamma distribution with density\n",
      "1\n",
      "βγ+1Γ(γ)δγe−δ/β\n",
      "Beginning with X = (s1, s2, . . ., sk), the conditional distribution of λi\n",
      "given X, β, and λj(j̸=i) is merely the gamma with parameters α + sj and\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "4.7 Computational Methods\n",
      "379\n",
      "β/(tj + 1), and the conditional distribution of β given X and the λi’s is an\n",
      "inverted gamma with parameters γ + kα and P λi + δ.\n",
      "The various parameters (α, δ, γ) have interpretions that can be used to\n",
      "select reasonable values.\n",
      "The Gibbs sampling method would estimate the marginal density of λi by\n",
      "generating λ(1)\n",
      "i\n",
      "from the appropriate gamma distribution, i.e., with parame-\n",
      "ters α + si and β(0)/(ti + 1) for i = 1, . . ., k, and then generating β(1) for the\n",
      "ﬁrst iteration.\n",
      "Continue this for k iterations.\n",
      "Do it m times to have a density.\n",
      "Miscellaneous Results and Comments\n",
      "Markov chain Monte Carlo has special applications when dealing with distri-\n",
      "butions that have densities known up to a constant of proportionality, that is\n",
      "densities speciﬁed as follows. Let h be a nonnegative integrable function that\n",
      "is not zero almost everywhere. Then h speciﬁes a probability distribution, all\n",
      "we need to do to get the density f is normalize it.\n",
      "f(x) = h(x)/c\n",
      "where\n",
      "c =\n",
      "Z\n",
      "h(x)dµ(x)\n",
      "The Hastings algorithm only uses h to simulate realizations from f, knowl-\n",
      "edge of the integral c is not required.\n",
      "In Bayesian inference, h is the likelihood times the prior. This is always\n",
      "known, but the integral c is generally hard. MCMC permits easy simulations\n",
      "of realizations from the posterior (no knowledge of c necessary).\n",
      "In most cases where there is complex dependence in the data, there is no\n",
      "simple probability model with c known, but it is easy to specify a model up\n",
      "to a constant of proportionality using an h. These are just very complicated\n",
      "exponential families.\n",
      "Let t be a vector-valued statistic on the sample space and\n",
      "h(x) = exp(t(x)Tθ)\n",
      "Then these specify a family of densities\n",
      "fθ(x) = exp(t(x)Tθ)/c(θ).\n",
      "In the expression\n",
      "exp(t(x)Tθ)/c(θ),\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "380\n",
      "4 Bayesian Inference\n",
      "c(θ) =\n",
      "Z\n",
      "exp(t(x)Tθ)dµ(x),\n",
      "but in MCMC it does not need to be known.\n",
      "This is just an exponential family with canonical statistic t(x) and canon-\n",
      "ical parameter θ.\n",
      "Using Markov chain Monte Carlo we can simulate realizations from any\n",
      "distribution in the model, and using the simulations from any one distribution,\n",
      "we can calculate maximum likelihood estimates, bootstrap estimates of their\n",
      "sampling distribution and so forth.\n",
      "There are also ways to get (randomized) signiﬁcance tests with exact p-\n",
      "values using Markov chain Monte Carlo.\n",
      "The output of the sampler is a Markov chain X1, X2, . . . whose equilibrium\n",
      "distribution is the distribution of interest, the one you want to sample from.\n",
      "Averages with respect to that distribution are approximated by averages\n",
      "over the chain.\n",
      "Notes and Further Reading\n",
      "In this chapter we have presented Bayesian methods as an approach to the\n",
      "decision-theoretic principle of minimizing average risk. In this presentation we\n",
      "have glossed over the philosophic excitement that attended the evolution of\n",
      "the Bayesian approach to statistical inference.\n",
      "In the early nineteenth century, Laplace developed a theory of “inverse\n",
      "probability”, in which the frequency of observations are used to infer the prob-\n",
      "ability that they arose from a particular data-generating process. Although\n",
      "inverse probability as a formal theory is not in current vogue, some of the\n",
      "underlying motivating ideas persist in inference based on likelihood and on\n",
      "“subjective probability”. For more discussion of Laplace’s work and the two\n",
      "examples at the beginning of this chapter, see Stigler (1986).\n",
      "The idea that statistical inference can (and should) take into account not\n",
      "only strictly objective observations but also subjective and even personal ev-\n",
      "idence was ﬁrst expounded in a clear mathematical theory by Savage in 1954\n",
      "in the ﬁrst edition of Savage (1972). Savage stated seven “postulates of a per-\n",
      "sonalistic theory of decision” that lead to the existence of a subjective proba-\n",
      "bility and a utility function. The essays in the volume edited by Kadane et al.\n",
      "(1999) address and expound on Savage’s book. Kadane, Schervish, and Sei-\n",
      "denfeld also consider the general cooperative Bayesian decision making. A\n",
      "satisfactory theory of group coherence in decisions may require the relaxation\n",
      "of one of Savage’s postulates on the simple preferential ordering of decisions.\n",
      "Good (1983) *** discuss\n",
      "In a more applied context, Schlaifer (1959) incorporated a personalistic\n",
      "approach into statistical decision making. Many of the ideas in the Bayesian\n",
      "approach derive from those books and from the book by Jeﬀreys (1961).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "381\n",
      "An alternative approach to probabilistic reasoning is the Dempster-Shafer\n",
      "theory of belief functions (see Shafer (1976) and Yager and Liu (2008)).\n",
      "In some cases, especially in hypothesis, the Bayesian approach is funda-\n",
      "mentally diﬀerent from the frequentist approach. The diﬀerences arise from\n",
      "the deﬁnition of the problem that is addressed. The articles by Casella and Berger\n",
      "(1987) (Roger) and (Jim) Berger and Sellke (1987) with accompanying discus-\n",
      "sion by several authors identify some of the diﬀerences in perspectives.\n",
      "Berger (1985) and Robert (2001) provide extensive coverage of statistical\n",
      "inference from a Bayesian perspective. Both of these books compare the “fre-\n",
      "quentist” and Bayesian approaches and argue that the Bayesian paradigm is\n",
      "more solidly grounded.\n",
      "Ghosh and Sen (1991) have considered Pitman closeness in the context of\n",
      "a posterior distribution, and deﬁned posterior Pitman closeness in terms of\n",
      "probabilities evaluated with respect to the posterior distribution. Interestingly,\n",
      "the posterior Pitman closeness is transitive, while as we have seen on page 219,\n",
      "Pitman closeness does not have the transitive property.\n",
      "Notation and Lingo\n",
      "There are several instances in which the notation and terminology used in\n",
      "Bayesian statistics diﬀer from the classical statistics that had evolved with a\n",
      "strong mathematical ﬂavor.\n",
      "I generally like to use uppercase letters to distinguish random variables\n",
      "from realizations of those random variables, which I generally represent by\n",
      "corresponding lowercase letters, but it is common in writing about a Bayesian\n",
      "analysis not to distinguish a random variable from its realization.\n",
      "People who work with simple Bayes procedures began calling the distri-\n",
      "bution of the reciprocal of a chi-squared random variable an “inverse” chi-\n",
      "squared distribution. Because “inverse” is used in the names of distributions\n",
      "in a diﬀerent way (“inverse Gaussian”, for example), I prefer the term inverted\n",
      "chi-squared, or inverted gamma.\n",
      "What is often called a “simple hypothesis” by most statisticians is often\n",
      "called a “sharp hypothesis in Bayesian analyses.\n",
      "The Bayesian Religious Wars of the Mid-Twentieth Century\n",
      "The analysis by Lindley and Phillips (1976) ********************.\n",
      "in Example 3.12\n",
      "Hartley (1963)\n",
      "A rather humorous historical survey of the antithetical Bayesian and fre-\n",
      "quentist approaches is given in McGrayne (2011).\n",
      "Prior Distributions\n",
      "Ghosh (2011) objective priors\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "382\n",
      "4 Bayesian Inference\n",
      "Early versions of the maximum entropy principle were stated by Jaynes\n",
      "(1957a,b). Kass and Wasserman (1996) critique maximum entropy priors and\n",
      "other priors that are selected on the basis of being less informative.\n",
      "Applications\n",
      "Bayesian procedures have been somewhat slow to permeate the traditional\n",
      "areas of statistical applications, such as analysis of linear models, time series\n",
      "analysis and forecasting, and ﬁnite population sampling. This is not because\n",
      "the underlying theory has not been developed. Broemeling (1984) discusses\n",
      "Bayesian analysis of general linear models, and the articles in the book edited\n",
      "by Dey et al. (2000) provide an extensive coverage of Bayesian methods in\n",
      "generalized linear models. See Prado and West (2010) and West and Harrison\n",
      "(1997), for discussions of Bayesian methods in time series analysis. Bayesian\n",
      "methods for sampling from ﬁnite populations are discussed in Ghosh and Meeden\n",
      "(1998), and assessed further in Rao (2011).\n",
      "Nonparametric Models\n",
      "We have limited our discussion in this chapter to parametric models; that is,\n",
      "to situations in which the probability distributions of the observable random\n",
      "variables can be indexed by a real number of ﬁnite dimension. Nonparamet-\n",
      "ric models can often be deﬁned in terms of an index (or “parameter”) of\n",
      "inﬁnite dimension. A standard example in Bayesian analysis uses a Dirichlet\n",
      "process as a prior for an inﬁnite discrete distribution (see Ferguson (1973),\n",
      "and Sethuraman (1994)).\n",
      "Exercises\n",
      "4.1. Formulate Laplace’s urn problem at the beginning of this chapter in the\n",
      "modern Bayesian context; that is, identify the prior, the conditional of\n",
      "the observable data, the joint, the marginal, and the conditional posterior\n",
      "distributions.\n",
      "4.2. Show that the family of distributions with PDF given in equation (4.23)\n",
      "is a conjugate family for an exponential family with PDF expressed in the\n",
      "form of equation (4.22).\n",
      "4.3. Consider the exponential distribution with PDF\n",
      "fX|θ(x|θ) = θ−1ex/θ I¯IR+(x).\n",
      "a) Show that the inverted gamma distribution is a conjugate prior for\n",
      "this conditional distribution.\n",
      "b) Given a random sample of size n from the exponential distribution and\n",
      "an inverted gamma with parameters α and β, determine the posterior\n",
      "conditional mean and variance.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "383\n",
      "4.4. Given the conditional PDF\n",
      "fX|γ(x) ∝(1 + (x −γ)2)−1.\n",
      "a) Under the prior\n",
      "fΓ (γ) ∝e−|γ−µ|,\n",
      "given a single observation, determine the MAP estimator of γ. Is this\n",
      "a meaningful estimator? Comment on why we might have expected\n",
      "such a useless estimator.\n",
      "b) For the same distribution of the observables, consider the prior\n",
      "fΓ (γ) ∝e−α|γ−µ|.\n",
      "For what values of α > 0 will this prior yield a diﬀerent estimator\n",
      "from that in the previous question?\n",
      "c) Consider now an opposite kind of setup. Let the conditional density\n",
      "of the observable be\n",
      "fX|γ(x) ∝e−|x−γ|,\n",
      "and let the prior be\n",
      "fΓ (γ) ∝(1 + (γ −µ)2)−1.\n",
      "Determine the MAP estimator of γ. Comment on the diﬀerence in this\n",
      "estimator and that in the ﬁrst part. Why might expect this situation?\n",
      "4.5. Prove Theorem 4.5.\n",
      "4.6. Prove Theorem 4.6.\n",
      "4.7. Prove Theorem 4.7.\n",
      "4.8. Consider the binomial(n, π) family of distributions in Example 4.6. Given\n",
      "a random sample X1, . . ., Xn on the random variable X with conditional\n",
      "distribution in the binomial family, formulate the relevant PDF for ob-\n",
      "taining ˆα and ˆβ in the empirical Bayes estimator of equation (4.50).\n",
      "4.9. Consider again the binomial(n, π) family of distributions in Example 4.6.\n",
      "Given a random sample X1, . . ., Xn on the random variable X with condi-\n",
      "tional distribution in the binomial family, determine the Bayes estimator\n",
      "of π under linex loss (equation (3.88) on page 262) with a beta(α, β) prior.\n",
      "4.10. Consider again the binomial(n, π) family of distributions in Example 4.6.\n",
      "We wish to estimate π under squared-error loss with a beta(α, β) prior.\n",
      "a) Determine the risk of the Bayes estimator (4.45), under squared-error\n",
      "loss.\n",
      "b) Now consider the estimator\n",
      "T ∗= X\n",
      "n\n",
      "n1/2\n",
      "1 + n1/2 +\n",
      "1\n",
      "2(1 + n1/2),\n",
      "in the form of equation (4.46). Determine a prior under which T ∗is\n",
      "Bayes (one such prior is a beta distribution – which?), and show that\n",
      "T ∗under squared-error loss has constant risk with respect to π.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "384\n",
      "4 Bayesian Inference\n",
      "4.11. Assume that in a batch of N items, M are defective. We are interested\n",
      "in the number of defective items in a random sample of n items from the\n",
      "batch of N items.\n",
      "a) Formulate this as a hypergeometric distribution.\n",
      "b) Now assume that M ∼binomial(π, N). What is the Bayes estimator\n",
      "of the number of defective items in the random sample of size n using\n",
      "a squared-error loss?\n",
      "4.12. For Example 4.7, consider each of the ﬁrst ﬁve issues discussed in Ex-\n",
      "ample 4.6. Give the corresponding solutions for the negative binomial\n",
      "distribution, if the solutions are possible.\n",
      "4.13. Consider the problem of estimating θ in the Poisson, assuming a ran-\n",
      "dom sample of size n. (The probability function, or density, is fX|θ(x) =\n",
      "θxe−θ/x! for nonnegative integers, and the parameter space is IR+.)\n",
      "a) Determine the Bayes estimator of θ under squared-error loss and the\n",
      "prior fΘ(θ) = θp exp(−θpθ).\n",
      "b) Determine the Bayes estimator under linex loss and the prior fΘ(θ) =\n",
      "θp exp(−θpθ).\n",
      "c) Determine the Bayes estimator under zero-one loss and the prior\n",
      "fΘ(θ) = θp exp(−θpθ).\n",
      "d) In the previous questions, you should have noticed something about\n",
      "the prior. What is a more general prior that is a conjugate prior?\n",
      "Under that prior and the squared-error loss, what is the Bayes esti-\n",
      "mator? What property is shared by this estimator and the estimator\n",
      "in Exercise 4.13a)?\n",
      "e) Determine the Bayes estimator under squared-error loss and a uniform\n",
      "(improper) prior.\n",
      "f) Determine the Bayes estimator under zero-one loss and a uniform\n",
      "(improper) prior.\n",
      "g) Determine a minimax estimator under zero-one loss. Would you use\n",
      "this estimator? Why?\n",
      "h) Now restrict estimators of θ to δc(X) = cX. Consider the loss function\n",
      "L(θ, δ) =\n",
      "\u0012δ\n",
      "θ −1\n",
      "\u00132\n",
      ".\n",
      "i. Compute the risk R(δc, θ). Determine whether δc is admissible if\n",
      "c > 1.\n",
      "ii. Compute the Bayes risk r(fΘ, δc) and determine the optimal value\n",
      "of c under fΘ. (The prior fΘ is the one used in Exercise 4.13a).)\n",
      "iii. Determine the optimal c for the minimax criterion applied to this\n",
      "class of estimators.\n",
      "i) As in Exercise 4.13a) with squared-error loss, consider the problem of\n",
      "estimating θ given the sample X1, . . ., Xn and using the prior, fΘ(θ),\n",
      "where θp is empirically estimated from the data using a method-of-\n",
      "moments estimator.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "385\n",
      "4.14. Consider again the binomial(n, π) family of distributions in Example 4.6.\n",
      "Let Pα,β be the beta(α, β) distribution.\n",
      "a) Determine the gamma-minimax estimator of π under squared-error\n",
      "loss within the class of priors Γ = {Pα,β : 0 < α, β}.\n",
      "b) Determine the gamma-minimax estimator of π under squared-error\n",
      "loss within the class of priors Γ = {Pα,β : 0 < α, β ≤1}.\n",
      "4.15. Consider a generalization of the absolute-error loss function, |θ −d|:\n",
      "L(θ, d) =\n",
      "\u001ac(d −θ)\n",
      "for d ≥θ\n",
      "(1 −c)(θ −d) for d < θ\n",
      "for 0 < c < 1 (equation (3.87)). Given a random sample X1, . . ., Xn on\n",
      "the random variable X, determine the Bayes estimator of θ = E(X|θ).\n",
      "(Assume whatever distributions are relevant.)\n",
      "4.16. Let X ∼U(0, θ) and the prior density of Θ be θ−2I[1,∞)(θ). The posterior\n",
      "is therefore\n",
      "fΘ|x(θ|x) = 2c2\n",
      "θ3 I[c,∞)(θ),\n",
      "where c = max(1, x).\n",
      "a) For squared-error loss, show that the Bayes estimator is the posterior\n",
      "mean. What is the posterior mean?\n",
      "b) Consider a reparametrization: ˜θ = θ2, and let ˜δ be the Bayes estimator\n",
      "of ˜θ. The prior density now is\n",
      "1\n",
      "2˜θ3/2 I[1,∞)(˜θ).\n",
      "In order to preserve the connection, take the loss function to be\n",
      "L(˜θ, ˜δ) = (\n",
      "p˜δ −\n",
      "p˜θ)2. What is the posterior mean? What is the\n",
      "Bayes estimator of ˜θ?\n",
      "c) Compare the two estimators. Comment on the relevance of the loss\n",
      "functions and of the prior for the relationship between the two esti-\n",
      "mators.\n",
      "4.17. Let X1 depend on θ1 and X2 be independent of X1 and depend on θ2. Let\n",
      "θ1 and θ2 have independent prior distributions. Assume a squared-error\n",
      "loss. Let δ1 and δ2 be the Bayes estimators of θ1 and θ2 repectively.\n",
      "a) Show that δ1 −δ2 is the Bayes estimator of θ1 −θ2 given X = (X1, X2)\n",
      "and the setup described.\n",
      "b) Now assume that θ2 > 0 (with probability 1), and let ˜δ2 be the Bayes\n",
      "estimator of 1/θ2 under the setup above. Show that δ1 ˜δ2 is the Bayes\n",
      "estimator of θ1/θ2 given X = (X1, X2).\n",
      "4.18. In the problem of estimating π given X from a binomial(10, π) with\n",
      "beta(α, β) prior and squared-error loss, as in Example 4.6, sketch the\n",
      "risk functions, as in Figure 3.1 on page 277, for the unbiased estimator,\n",
      "the minimax estimator, and the estimator resulting from Jeﬀreys’s non-\n",
      "informative prior.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "386\n",
      "4 Bayesian Inference\n",
      "4.19. Given an estimation problem with an integrable Lebesgue conditional\n",
      "PDF fX|θ for the observables, an integrable Lebesgue prior PDF fΘ, and\n",
      "with loss function L(θ, a) = w(θ)(θ −a)2, where w(θ) is a ﬁxed weighting\n",
      "function. Determine the Bayes rule and comment on the role of w(θ) in\n",
      "this problem.\n",
      "4.20. Let X1, . . ., Xn\n",
      "iid\n",
      "∼N(µ, σ2\n",
      "0), with σ2\n",
      "0 known and µ unknown. Determine\n",
      "the generalized Bayes action for estimating µ under squared error loss and\n",
      "the noninformative prior of the Lebesgue measure on ] −∞, ∞[.\n",
      "4.21. Refer to the problem described in Example 4.10.\n",
      "a) Show that every decision rule is inadmissible.\n",
      "b) We have implicitly assumed the action space to be ] −∞, ∞[. Show\n",
      "that if the action space is [−∞, ∞], then there is a Bayes rule, and\n",
      "that it is the only admissible rule.\n",
      "4.22. Show that the unbiased Bayes estimator in Example 4.11 has constant\n",
      "risk wrt the loss function of equation (4.52).\n",
      "4.23. As in Example 4.7, consider the problem of estimation of the negative\n",
      "binomial parameter with a beta prior, but instead of a squared-error loss,\n",
      "use the loss function of Example 4.11, given in equation (4.52). Determine\n",
      "the Bayes estimator. Do the estimators of the binomial parameter and the\n",
      "negative binomial parameter conform to the likelihood principle?\n",
      "4.24. Consider the sample (Y1, x1), . . ., (Yn, xn) where the Yi are iid as N(xT\n",
      "i β, σ2)\n",
      "for the ﬁxed vectors xi and for the unknown p-vector β. In the matrix rep-\n",
      "resentation Y = Xβ +E, assume that the n×p matrix X is of rank p. Let\n",
      "l be a given p-vector, and consider the problem of estimating lTβ under\n",
      "a squared-error loss.\n",
      "a) Assume σ2 = σ2\n",
      "0, a known positive number. Using the prior distribu-\n",
      "tion of β Np(β0, Σ), where β0 is a known p-vector and Σ is a known\n",
      "positive deﬁnite matrix, determine the Bayes estimator of lTβ.\n",
      "b) Now assume σ2 is unknown. We will simplify the prior on β to be,\n",
      "conditional on σ2, Np(β0, σ2V ), where again β0 is a known p-vector\n",
      "and V is a known positive deﬁnite matrix. Let the prior on σ2 be the\n",
      "inverted gamma distribution with parameters α and β (see page 842).\n",
      "Determine the Bayes estimator of lTβ.\n",
      "4.25. Let X1, . . ., Xn\n",
      "iid\n",
      "∼Bernoulli(π).\n",
      "a) Under the prior beta(α, β) and some given π0, determine the Bayes\n",
      "factor and the Bayes test for\n",
      "H0 : π ≤π0\n",
      "versus\n",
      "H1 : π > π0.\n",
      "b) Now, consider testing\n",
      "H0 : π = π0\n",
      "versus\n",
      "H1 : π ̸= π0.\n",
      "i. Make an appropriate modiﬁcation to the beta prior.\n",
      "ii. Determine the Bayes factor and the Bayes test under your modi-\n",
      "ﬁed prior.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "387\n",
      "4.26. As in Exercise 4.13, consider the problem of making inferences about\n",
      "θ in the Poisson, assuming a random sample of size n under the prior\n",
      "fΘ(θ) = θp exp(−θpθ).\n",
      "a) Let θ0 be some given positive number. Determine the Bayes factor\n",
      "and the Bayes test for\n",
      "H0 : θ ≤θ0\n",
      "versus\n",
      "H1 : θ > θ0.\n",
      "b) Now, consider testing\n",
      "H0 : θ = θ0\n",
      "versus\n",
      "H1 : θ ̸= θ0.\n",
      "i. Make an appropriate modiﬁcation to the prior.\n",
      "ii. Determine the Bayes factor and the Bayes test under your modi-\n",
      "ﬁed prior.\n",
      "4.27. Let X1, . . ., Xn1\n",
      "iid\n",
      "∼N(µ1, σ2) and let Y1, . . ., Yn2\n",
      "iid\n",
      "∼N(µ2, σ2).\n",
      "a) Assume σ2 = σ2\n",
      "0, a known positive number. As the prior distribution\n",
      "for M = M2 −M1 take N(µp, σ2\n",
      "p), where mup and σ2\n",
      "p are known\n",
      "constants. Determine the Bayes factor and the Bayes test for\n",
      "H0 : µ1 ≤µ2\n",
      "versus\n",
      "H1 : µ1 > µ2.\n",
      "b) Now assume σ2 is unknown. As the conditional prior distribution for\n",
      "M = M2 −M1 given σ2, take N(µp, σ2/κp), where σ2 is a realization\n",
      "of a random variable from a chi-squared distribution with parameters\n",
      "νp as degrees of freedom and σp as scale of σ. Determine the Bayes\n",
      "factor and the Bayes test for the test in the previous part.\n",
      "4.28. Consider the problem of determining a credible set for a scalar parameter\n",
      "θ. Suppose that the conditional posterior has a Lebesgue PDF fΘ|x(θ)\n",
      "that is unimodal and not monotone. (It has a shape similar to that in\n",
      "Figure 4.6.)\n",
      "a) Show that a (1−α)100% HPD credible set is an interval and that the\n",
      "interval is unique.\n",
      "b) Show that the (1 −α)100% HPD credible set has the shortest lenght\n",
      "of any interval [a, b] satisfying\n",
      "Z b\n",
      "a\n",
      "fΘ|x(θ) dθ = 1 −α.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5\n",
      "Unbiased Point Estimation\n",
      "In a decision-theoretic approach to statistical inference, we seek a method that\n",
      "minimizes the risk no matter what is the true state of nature. In a problem of\n",
      "point estimation, for example, we seek an estimator T(X) which for a given\n",
      "loss function L(g(θ), T(X)) yields a minimum of the risk, Eθ(L(g(θ), T(X))).\n",
      "For some speciﬁc value of θ, say θ1, one particular estimator, say T1, may\n",
      "have the smallest expected loss, while for another value of θ, say θ2, another\n",
      "estimator, say T2, may a smaller expected loss.\n",
      "What we would like is an estimator with least expected loss no matter\n",
      "what is the value of θ; that is, we would like an estimator with uniformly\n",
      "minimum risk. Because the risk depends on the value of θ, however, we see that\n",
      "we cannot devise such an estimator. The optimal estimator would somehow\n",
      "involve θ. We would prefer a procedure that does not depend on the unknown\n",
      "quantity we are trying to estimate, that is, we would like a procedure with\n",
      "uniformly good properties.\n",
      "Since, in general, there is no procedure with uniformly minimum risk, we\n",
      "might consider restricting our procedures to some class of procedures that\n",
      "have some other desirable properties, for example, to procedures that are\n",
      "unbiased. As we will see in Section 5.1, this is often possible in the case of\n",
      "point estimation.\n",
      "Unbiasedness also, of course, is desirable from an intuitive perspective.\n",
      "Although we may think that the concept does not have much practical im-\n",
      "portance because, after all, we are not going to repeat the procedure inﬁnitely\n",
      "many times, we could raise the same issues of practical importance of mini-\n",
      "mum risk, which is also an expected value.\n",
      "Unbiased Point Estimators\n",
      "Our objective is to develop “good” estimators of statistical functions. The\n",
      "statistical function to be estimated is the estimand. It may be deﬁned as a\n",
      "functional of the CDF, Υ(F ), or, in a parametric setting, as a measurable\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "390\n",
      "5 Unbiased Point Estimation\n",
      "function of some underlying parameter, g(θ). In the following, I will gener-\n",
      "ally represent the estimand as g(θ), but the concepts apply to more general\n",
      "estimands that may only be represented as some functional, Υ(F ).\n",
      "Although some “good” estimators are not unbiased, unbiasedness relates\n",
      "easily to fundamental concepts such as what does it mean to “estimate” a\n",
      "statistical function. Can any statistical function be estimated meaningfully?\n",
      "How many observations are required to yield a meaningful estimate?\n",
      "An estimator T(X) of a given estimand, g(θ), is unbiased with respect to\n",
      "θ if\n",
      "Eθ(T(X)) = g(θ) ∀θ ∈Θ.\n",
      "(5.1)\n",
      "Thus we see that unbiasedness is a property of a statistic that relates to a\n",
      "parameter, but does not depend on the value of the parameter; hence, by\n",
      "deﬁnition, unbiasedness of a point estimator is a uniform property.\n",
      "Unbiasedness depends on the distribution of the observable, which in turn\n",
      "depends on the data-generating process.\n",
      "Example 5.1 Sampling in a Bernoulli distribution\n",
      "In Example 3.12, we considered the problem of making inferences about π\n",
      "using data-generating processes governed by a family of Bernoulli distribu-\n",
      "tions with parameter π. In one case, the approach was to take a random\n",
      "sample of size n, X1, . . ., Xn which are iid as Bernoulli(π). This yielded\n",
      "a data-generating process in which T = P Xi has a binomial distribution\n",
      "with parameters n and π. In another approach we took a sequential sample,\n",
      "X1, X2, . . ., from the Bernoulli(π) until a ﬁxed number t of 1’s have occurred.\n",
      "In this data-generating process, the sample size N is random, and it is mod-\n",
      "eled by a negative binomial with parameters t and π. (Note that in a common\n",
      "formulation of a negative binomial distribution, the random variable is N −t\n",
      "in the formulation we are using here. In the present formulation, N ≥t.)\n",
      "In Examples 4.6 and 4.7 we see that the estimator of π under a squared\n",
      "error loss and a beta prior is the same for the two distributions that result\n",
      "from the two data-generating processes, and neither of them is unbiased.\n",
      "In the ﬁrst data-generating process, we see that an unbiased estimator of\n",
      "π is\n",
      "W = T\n",
      "n .\n",
      "(5.2)\n",
      "In the second data-generating process, we see that an unbiased estimator of\n",
      "π is\n",
      "U =\n",
      "\u001a t−1\n",
      "N−1 if N > 1\n",
      "1\n",
      "otherwise\n",
      "(5.3)\n",
      "(Exercise 5.1). The latter estimator is essentially the same as the former one,\n",
      "because by the deﬁnition of the data-generating process, the last observation\n",
      "does not count because its value is determined a priori.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5 Unbiased Point Estimation\n",
      "391\n",
      "Estimability\n",
      "A statistical function for which there is an unbiased estimator is said to be\n",
      "U-estimable. We often refer to such estimands simply as “estimable”. There\n",
      "are estimands for which there is no unbiased estimator.\n",
      "Example 5.2 an estimand that is not U-estimable\n",
      "Consider the problem of estimating 1/π in binomial(n, π) for π ∈]0, 1[. Sup-\n",
      "pose T(X) is an unbiased estimator of 1/π. Then\n",
      "n\n",
      "X\n",
      "x=0\n",
      "T(x)\n",
      "\u0012n\n",
      "x\n",
      "\u0013\n",
      "πx(1 −π)n−x = 1/π.\n",
      "If 1/π were U-estimable, the equation above would say that some polynomial\n",
      "in π is equal to 1/π for all π ∈]0, 1[. That clearly cannot be; hence, 1/π is not\n",
      "U-estimable. Notice also as π →0, the left side tends to T(0), which is ﬁnite,\n",
      "but the right side tends to ∞.\n",
      "Another related example, but one that corresponds to a more common\n",
      "parameter, is an estimator of the odds, π/(1 −π).\n",
      "Example 5.3 another estimand that is not U-estimable\n",
      "Consider the problem of estimating π/(1 −π) in binomial(n, π) for π ∈]0, 1[.\n",
      "The possible realizations of the n Bernoulli trials are (X1, . . ., Xn), where\n",
      "Xi = 0 or 1; hence, there are 2n possibilities and any estimator T must take\n",
      "each realization into a number tj, where j ranges from 1 to 2n.\n",
      "Now,\n",
      "E(T) =\n",
      "2n\n",
      "X\n",
      "j=1\n",
      "tjπnj(1 −π)n−nj,\n",
      "where nj is the number of ones in the jth string of zeros and ones. If T is\n",
      "unbiased, then it must be the case that\n",
      "2n\n",
      "X\n",
      "j=1\n",
      "tjπnj(1 −π)n−nj =\n",
      "π\n",
      "1 −π\n",
      "∀π ∈(0, 1).\n",
      "But it is not possible that the polynomial in π on the left can equal π/(1 −\n",
      "π)∀π ∈(0, 1).\n",
      "Unbiasedness, while a uniform property, is not invariant to transforma-\n",
      "tions. It is easy to see by simple examples that if E(T) = θ, in general,\n",
      "E(g(T)) ̸= g(θ).\n",
      "Unbiasedness may lead to estimators that we would generally consider to\n",
      "be poor estimators, as the following example from Romano and Siegel (1986)\n",
      "shows.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "392\n",
      "5 Unbiased Point Estimation\n",
      "Example 5.4 an unbiased estimator with poor properties\n",
      "Consider the problem of using a sample of size 1 for estimating g(θ) = e−3θ\n",
      "where θ is the parameter in a Poisson distribution. An unbiased is\n",
      "T(X) = (−2)X,\n",
      "as you are asked to show in Exercise 5.2.\n",
      "The estimator is ridiculous. It can be negative, even though g(θ) > 0.\n",
      "It is increasing in the positive integers, even though g(θ) decreases over the\n",
      "positive integers.\n",
      "Degree of a Statistical Function\n",
      "If a statistical function is estimable, we may ask how many observations are\n",
      "required to estimate it; that is, to estimate it unbiasedly. We refer to this\n",
      "number as the degree of the statistical function. Obviously, this depends on\n",
      "the distribution as well as the functional. A mean functional may not even\n",
      "be exist, for example, in a Cauchy distribution, but if the mean functional\n",
      "exists, it is estimable and its degree is 1. The variance functional in a normal\n",
      "distribution is estimable and its degree is 2 (see page 405).\n",
      "5.1 Uniformly Minimum Variance Unbiased Point\n",
      "Estimation\n",
      "An unbiased estimator may not be unique. If there are more than one unbiased\n",
      "estimator, we will seek one that has certain optimal properties.\n",
      "5.1.1 Unbiased Estimators of Zero\n",
      "Unbiased estimators of 0 play a useful role in UMVUE problems.\n",
      "If T(X) is unbiased for g(θ) then T(X) −U(X) is also unbiased for g(θ)\n",
      "for any U such that E(U(X)) = 0; in fact, all unbiased estimators of g(θ)\n",
      "belong to an equivalence class deﬁned as\n",
      "{T(X) −U(X)},\n",
      "(5.4)\n",
      "where Eθ(U(X)) = 0.\n",
      "In Theorem 5.2 and its corollary we will see ways that unbiased estimators\n",
      "of zero can be used to identify optimal unbiased estimators.\n",
      "In some cases, there may be no such nontrivial U(X) that yields a diﬀerent\n",
      "unbiased estimator in (5.4). Consider for example, a single Bernoulli trial\n",
      "with probability of success π, yielding the random variable X, and consider\n",
      "T(X) = X as an estimator of π. We immediately see that T(X) is unbiased\n",
      "for π. Now, let S be an estimator of π, and let S(0) = s0 and S(1) = s1. For\n",
      "S to be unbiased, we must have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.1 UMVUE\n",
      "393\n",
      "s1π + s0(1 −π) = π,\n",
      "but this means (s1 −s0)π + s0 = π. This means s0 = 0 and s1 = 1; that is,\n",
      "S(X) = T(X) for X = 0 or 1. In this case the unbiased point estimator is\n",
      "unique.\n",
      "5.1.2 Optimal Unbiased Point Estimators\n",
      "Restricting our attention to unbiased estimators, we return to the problem\n",
      "of selecting an estimator with uniform minimum risk (UMRU). We ﬁnd that\n",
      "in general, no UMRUE exists for bounded loss functions. Such loss functions\n",
      "cannot be (strictly) convex. If, however, we consider only loss functions that\n",
      "are strictly convex, which means that they are unbounded, we may be able to\n",
      "ﬁnd a UMRUE.\n",
      "5.1.3 Unbiasedness and Squared-Error Loss; UMVUE\n",
      "A squared-error loss function is particularly nice for an unbiased estimator\n",
      "that has a ﬁnite second moment, because in that case the expected loss is just\n",
      "the variance; that is, an unbiased estimator with minimum risk is an unbiased\n",
      "estimator with minimum variance.\n",
      "Unbiasedness alone, of course, does not ensure that an estimator is good;\n",
      "the variance of the estimator may be quite large. Also, a biased estimator\n",
      "may in fact dominate a very good unbiased estimator; see Example 3.19 on\n",
      "page 272. In Theorem 3.10 on page 270, however, we saw that any bias in an\n",
      "admissible estimator under squared-error loss must have a negative correlation\n",
      "with the estimator.\n",
      "The requirement of unbiasedness also protects us from “bad” estimators\n",
      "that have superior squared-error risk in some regions of the parameter space,\n",
      "such as in Example 3.1 on page 219. (The estimator in that example, of\n",
      "course, does not dominate the “good” estimator, as the shrunken estimator\n",
      "in Example 3.19 does.)\n",
      "If the unbiased estimator has minimum variance among all unbiased esti-\n",
      "mators at each point in the parameter space, we say that such an estimator is\n",
      "a uniformly (for all values of θ) minimum variance unbiased estimator, that\n",
      "is, a UMVUE.\n",
      "An unbiased estimator that has minimum variance among all unbiased es-\n",
      "timators within a subspace of the parameter space is called a locally minimum\n",
      "variance unbiased estimator, or LMVUE.\n",
      "UMVU is a special case of uniform minimum risk (UMRU), which generally\n",
      "only applies to convex loss functions.\n",
      "Uniformity (the ﬁrst “U”) means the MVU property is independent of the\n",
      "estimand. “Unbiasedness” is itself a uniform property, because it is deﬁned in\n",
      "terms of an expectation for any distribution in the given family.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "394\n",
      "5 Unbiased Point Estimation\n",
      "UMVU is closely related to complete suﬃciency, which means that it prob-\n",
      "ably has nice properties (like being able to be identiﬁed easily) in exponential\n",
      "families. One of the most useful facts is the Lehmann-Scheﬀ´e theorem.\n",
      "Theorem 5.1 (Lehmann-Scheﬀ´e Theorem)\n",
      "Let T be a complete suﬃcient statistic for θ, and suppose T has ﬁnite second\n",
      "moment. If g(θ) is U-estimable, then there is a unique UMVUE of g(θ) of the\n",
      "form h(T), where h is a Borel function.\n",
      "The ﬁrst part of this is just a corollary to the Rao-Blackwell theorem,\n",
      "Theorem 3.8. The uniqueness comes from the completeness, and of course,\n",
      "means unique a.e.\n",
      "The Lehmann-Scheﬀ´e theorem may immediately identify a UMVUE.\n",
      "Example 5.5 UMVUE of Bernoulli parameter\n",
      "Consider the Bernoulli family of distributions with parameter π. Suppose\n",
      "we take a random sample X1, . . ., Xn. Now the Bernoulli (or in this case,\n",
      "the binomial(n, π)) is a complete one-parameter exponential family, and T =\n",
      "Pn\n",
      "i=1 Xi is a complete suﬃcient statistic for π with expectation nπ. By the\n",
      "Lehmann-Scheﬀ´e theorem, therefore, the unique UMVUE of π is\n",
      "W =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "Xi/n.\n",
      "(5.5)\n",
      "In Example 3.17, page 269, we showed that the variance of W achieves the\n",
      "CRLB; hence it must be UMVUE.\n",
      "The random sample from a Bernoulli distribution is the same as a single\n",
      "binomial observation, and W is an unbiased estimator of π, as in Exam-\n",
      "ple 5.1. We also saw in that example that a constrained random sample from\n",
      "a Bernoulli distribution is the same as a single negative binomial observation\n",
      "N, and an unbiased estimator of π in that case is (t −1)/(N −1), where t is\n",
      "the required number of 1’s in the constrained random sample. This estimator\n",
      "is also UMVU for π (Exercise 5.1).\n",
      "In the usual deﬁnition of this family, π ∈Π =]0, 1[. Notice that if\n",
      "Pn\n",
      "i=1 Xi = 0 or if Pn\n",
      "i=1 Xi = n, W /∈Π. Hence, the UMVUE may not be valid\n",
      "in the sense of being a legitimate parameter for the probability distribution.\n",
      "Useful ways for checking that an estimator is UMVU are based on the\n",
      "following theorem and corollary.\n",
      "Theorem 5.2\n",
      "Let P = {Pθ}. Let T be unbiased for g(θ) and have ﬁnite second moment.\n",
      "Then T is a UMVUE for g(θ) iﬀE(TU) = 0 ∀θ ∈Θ and ∀U ∋E(U) =\n",
      "0, E(U 2) < ∞∀θ ∈Θ.\n",
      "Proof.\n",
      "First consider “only if”.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.1 UMVUE\n",
      "395\n",
      "Let T be UMVUE for g(θ) and let U be such that E(U) = 0 and E(U 2) < ∞.\n",
      "Let c be any ﬁxed constant, and let Tc = T + cU; then E(T) = g(θ). Since T\n",
      "is UMVUE,\n",
      "V(Tc) ≥V(T),\n",
      "∀θ ∈Θ,\n",
      "or\n",
      "c2V(U) + 2cCov(T, U) ≥0,\n",
      "∀θ ∈Θ.\n",
      "This implies E(TU) = 0 ∀θ ∈Θ.\n",
      "Now consider “if”.\n",
      "Assume E(T) = g(θ) and E(T 2) < ∞and U is such that E(TU) = 0, E(U) =\n",
      "0, E(U 2) < ∞∀θ ∈Θ. Now let T0 be an unbiased estimator of g(θ), that is,\n",
      "E(T0) = g(θ). Therefore, because E(TU) = 0, E(T(T −T0)) = 0 ∀θ ∈Θ, and\n",
      "so V(T) = Cov(T, T0). Therefore, because (Cov(T, T0))2 ≤V(T)V(T0), we\n",
      "have\n",
      "V(T) ≤V(T0) ∀θ ∈Θ\n",
      "implying that T is UMVUE.\n",
      "Corollary 5.2.1\n",
      "Let eT be a suﬃcient statistic for θ, and let T = h( eT) where h is a Borel\n",
      "function. Let r be any Borel function such that for eU = r( eT), E(eU) = 0 and\n",
      "E(eU 2) < ∞∀θ ∈Θ. Then T is a UMVUE for g(θ) iﬀE(T eU) = 0 ∀θ ∈Θ.\n",
      "Proof.\n",
      "This follows from the theorem because if E(T eU) = 0 ∀θ ∈Θ, then ∀θ ∈\n",
      "Θ, E(TU) = 0, E\n",
      "\u0010\n",
      "E(U| eT)\n",
      "\u0011\n",
      "= 0, and E\n",
      "\u0010\n",
      "E(U| eT)2\u0011\n",
      "< ∞. This is the case\n",
      "because\n",
      "E(TU) = E\n",
      "\u0010\n",
      "E\n",
      "\u0010\n",
      "TU| eT\n",
      "\u0011\u0011\n",
      "= E\n",
      "\u0010\n",
      "E\n",
      "\u0010\n",
      "h\n",
      "\u0010\n",
      "eT\n",
      "\u0011\n",
      "U| eT\n",
      "\u0011\u0011\n",
      "= E\n",
      "\u0010\n",
      "h\n",
      "\u0010\n",
      "eT\n",
      "\u0011\n",
      "E\n",
      "\u0010\n",
      "U| eT\n",
      "\u0011\u0011\n",
      ".\n",
      "How to ﬁnd a UMVUE\n",
      "We have seen how that is some cases, the Lehmann-Scheﬀ´e theorem may\n",
      "immediately identify a UMVUE.\n",
      "In more complicated cases, we generally ﬁnd an UMVUE by beginning\n",
      "with a “good” estimator and manipulating it to make it UMVUE. It might be\n",
      "unbiased to begin with, and we reduce its variance while keeping it unbiased.\n",
      "It might not be unbiased to begin with but it might have some other desirable\n",
      "property, and we manipulate it to be unbiased.\n",
      "If we have a complete suﬃcient statistic T for θ, the Lehmann-Scheﬀ´e\n",
      "theorem leads to two methods. Another method uses unbiased estimators of\n",
      "zero and is based on the equivalence class of unbiased estimators (5.4).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "396\n",
      "5 Unbiased Point Estimation\n",
      "1. Given the complete suﬃcient statistic T for θ, ﬁnd a function of T that\n",
      "makes it unbiased; that is, ﬁnd a UMVUE directly by ﬁnding h(T) such\n",
      "that Eθ(h(T)) = g(θ).\n",
      "2. Given the complete suﬃcient statistic T for θ and another statistic T0 that\n",
      "is unbiased, condition the unbiased statistic on the complete suﬃcient\n",
      "statistic; that is, ﬁnd a UMVUE as h(T) = Eθ(T0(X)|T). (This process is\n",
      "sometimes called “Rao-Blackwellization”.)\n",
      "3. Let T0 be such that Eθ(T0) = g(θ) and Eθ(T 2\n",
      "0 ) < ∞. We ﬁnd a UMVUE\n",
      "by ﬁnding U where Eθ(U) = 0 so as to minimize E((T0 −U)2). Useful\n",
      "estimators clearly must have ﬁnite second moment, otherwise, we cannot\n",
      "minimize a variance by combining the estimators. This method makes use\n",
      "of the equivalence class of unbiased estimators.\n",
      "We will now consider examples of each of these methods.\n",
      "Finding an UMVUE by Forming an Unbiased Function of a\n",
      "Complete Suﬃcient Statistic\n",
      "Example 5.6 UMVUE of various parametric functions in a normal\n",
      "distribution\n",
      "Let X1, X2, . . ., Xn be a random sample from a N(µ, σ2) distribution with\n",
      "unknown θ = (µ, σ2). (Notice that n ≥2.) In Example 3.6, we have seen that\n",
      "T = (X, S2) is suﬃcient and complete for θ.\n",
      "For various g(θ) we will ﬁnd the UMVUEs directly by ﬁnding h(T) such\n",
      "that Eθ(h(T)) = g(θ):\n",
      "•\n",
      "for g(θ) = µ:\n",
      "h(T) = X\n",
      "(5.6)\n",
      "•\n",
      "for g(θ) = σ2:\n",
      "h(T) = S2\n",
      "(5.7)\n",
      "•\n",
      "for g(θ) = µ2:\n",
      "h(T) = X2 −S2/n\n",
      "(5.8)\n",
      "•\n",
      "for g(θ) = σp, with p ≥2:\n",
      "h(T) = (n −1)p/2Γ((n −1)/2)\n",
      "2p/2Γ((n −1 + p)/2) Sp\n",
      "(5.9)\n",
      "•\n",
      "for g(θ) = µ/σ if n ≥3:\n",
      "h(T) =\n",
      "21/2Γ((n −1)/2)\n",
      "(n −1)1/2Γ((n −2)/2)X/S.\n",
      "(5.10)\n",
      "We get the last two estimators by using the fact that (n −1)S2/σ2 ∼χ2\n",
      "n−1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.1 UMVUE\n",
      "397\n",
      "Example 5.7 UMVUE of the variance in a Bernoulli distribution\n",
      "Given random sample of size n from Bernoulli(π). We want to estimate g(π) =\n",
      "π(1−π). We have a complete suﬃcient statistic, T = P Xi. The unbiasedness\n",
      "condition is\n",
      "n\n",
      "X\n",
      "t=0\n",
      "\u0012n\n",
      "t\n",
      "\u0013\n",
      "h(t)πt(1 −π)n−t = π(1 −π).\n",
      "Rewriting this in terms of the odds ρ = π/(1 −π), we have, for all ρ ∈]0, ∞[,\n",
      "n\n",
      "X\n",
      "t=0\n",
      "\u0012n\n",
      "t\n",
      "\u0013\n",
      "h(t)ρt = ρ(1 + ρ)n−2\n",
      "=\n",
      "n−1\n",
      "X\n",
      "t=1\n",
      "\u0012n −2\n",
      "t −1\n",
      "\u0013\n",
      "ρt.\n",
      "Now since for each t, the coeﬃcient of ρt must be the same on both sides of\n",
      "the equation, we have the UMVUE of the Bernoulli variance to be\n",
      "P xi(n −P xi)\n",
      "n(n −1)\n",
      ".\n",
      "(5.11)\n",
      "Note that this is the same estimator as 5.7 for the variance in a normal\n",
      "distribution.\n",
      "Example 5.8 UMVUE of the upper limit in a uniform distribution\n",
      "Consider the uniform distribution U(0, θ). In Example 3.7 we saw that X(n)\n",
      "is complete suﬃcient for θ. An UMVUE for θ therefore is (1 + 1/n)X(n).\n",
      "Also see Example 3.1 in MS2.\n",
      "Example 5.9 UMVUE in a two-parameter exponential distribution\n",
      "Lebesgue PDF of the two-parameter exponential with parameter (α, θ) is\n",
      "θ−1e−(x−α)/θI]α,∞[(x)\n",
      "Suppose we have observations X1, X2, . . ., Xn. In Examples 1.11 and 1.18, we\n",
      "found the distributions of X(1) and PXi −nX(1), and in Example 3.8 we\n",
      "showed that T = (X(1), P Xi −nX(1)) is suﬃcient and complete for (α, θ).\n",
      "Hence, all we have to do is adjust them to be unbiased.\n",
      "Tα = X(1) −\n",
      "1\n",
      "n(n −1)\n",
      "X\n",
      "(Xi −X(1)).\n",
      "and\n",
      "Tθ =\n",
      "1\n",
      "n −1\n",
      "X\n",
      "(Xi −X(1)).\n",
      "Also see Example 3.2 in MS2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "398\n",
      "5 Unbiased Point Estimation\n",
      "UMVUE by Conditioning an Unbiased Estimator on a Suﬃcient\n",
      "Statistic\n",
      "See Example 3.3 in MS2.\n",
      "UMVUE by Minimizing the Variance within the Equivalence\n",
      "Class of Unbiased Estimators\n",
      "******\n",
      "5.1.4 Other Properties of UMVUEs\n",
      "In addition to the obvious desirable properties of UMVUEs, we should point\n",
      "out that UMVUEs lack some other desirable properties. We can do this by\n",
      "citing examples.\n",
      "First, as in Example 5.5, we see that the UMVUE may not be in the\n",
      "parameter space.\n",
      "The next example shows that the UMVUE may not be a minimax estima-\n",
      "tion, even under the same loss function, that is, squared-error.\n",
      "Example 5.10 UMVUE that is not minimax (continuation of Ex-\n",
      "ample 5.5)\n",
      "Consider a random sample of size n from the Bernoulli family of distributions\n",
      "with parameter π. The UMVUE of π is T = X/n. Under the squared-error\n",
      "loss, the risk, that is, the variance in this case is π(1 −π)/n. This is the\n",
      "smallest risk possible for an unbiased estimator, by inequality (3.39).\n",
      "The maximum risk for T is easily seen to be 1/(4n) (when π = 1/2). Now,\n",
      "consider the estimator\n",
      "T ∗= X\n",
      "n\n",
      "n1/2\n",
      "1 + n1/2 +\n",
      "1\n",
      "2(1 + n1/2).\n",
      "This has risk\n",
      "R(T ∗, π) = Eπ((T ∗−π)2)\n",
      "= Eπ\n",
      " \u0012X\n",
      "n\n",
      "n1/2\n",
      "1 + n1/2 +\n",
      "πn1/2\n",
      "2(1 + n1/2) −\n",
      "πn1/2\n",
      "2(1 + n1/2) +\n",
      "1\n",
      "2(1 + n1/2) −π\n",
      "\u00132!\n",
      "=\n",
      "\u0012\n",
      "n1/2\n",
      "1 + n1/2\n",
      "\u00132\n",
      "Eπ\n",
      " \u0012X\n",
      "n −π\n",
      "\u00132!\n",
      "+\n",
      "\u0012 πn1/2\n",
      "1 + n1/2 +\n",
      "1\n",
      "2(1 + n1/2) −π\n",
      "\u00132\n",
      "=\n",
      "\u0012\n",
      "n1/2\n",
      "1 + n1/2\n",
      "\u00132 π(1 −π)\n",
      "n\n",
      "+\n",
      "\u0012\n",
      "1 −2π\n",
      "2(1 + n1/2)\n",
      "\u00132\n",
      "=\n",
      "1\n",
      "4(1 + n1/2)2 .\n",
      "(5.12)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.1 UMVUE\n",
      "399\n",
      "The risk of T ∗is less than the maximum risk of T; therefore, T is not\n",
      "minimax.\n",
      "Now we might ask is T ∗minimax?\n",
      "We ﬁrst note that the risk (5.12) is constant, so T ∗is minimax if it is\n",
      "admissible or if it is a Bayesian estimator (in either case with respect to the\n",
      "squared-error loss). We can see that T ∗is Bayesian estimator (with a beta\n",
      "prior). (You are asked to prove this in Exercise 4.10 on page 383.) As we show\n",
      "in Chapter 4, a Bayes estimator with a constant risk is a minimax estimator;\n",
      "hence, δ∗is minimax. (This example is due to Lehmann.)\n",
      "Although we may initially be led to consideration of UMVU estimators\n",
      "by consideration of a squared-error loss, which leads to a mean squared-error\n",
      "risk, the UMVUE may not minimize the MSE. It was the fact that we could\n",
      "not minimize the MSE uniformly that led us to add on the requirement of un-\n",
      "biasedness. There may, however, be estimators that have a uniformly smaller\n",
      "MSE than the UMVUE. An example of this is in the estimation of the vari-\n",
      "ance in a normal distribution. In Example 5.6 we have seen that the UMVUE\n",
      "of σ2 in the normal distribution is S2, while in Example 3.13 we have seen\n",
      "that the MLE of σ2 is (n −1)S2/n, and by equation (3.55) on page 243, we\n",
      "see that the MSE of the MLE is uniformly less than the MSE of the UMVUE.\n",
      "There are other ways in which UMVUEs may not be very good as es-\n",
      "timators; see, for example, Exercise 5.2. A further undesirable property of\n",
      "UMVUEs is that they are not invariant to transformation.\n",
      "5.1.5 Lower Bounds on the Variance of Unbiased Estimators\n",
      "The three Fisher information regularity conditions (see page 168) play a major\n",
      "role in UMVUE. In particular, these conditions allow us to develop a lower\n",
      "bound on the variance of any unbiased estimator.\n",
      "The Information Inequality (CRLB) for Unbiased Estimators\n",
      "What is the smallest variance an unbiased estimator can have? For an un-\n",
      "biased estimator T of g(θ) in a family of densities satisfying the regularity\n",
      "conditions and such that T has a ﬁnite second moment, the answer results\n",
      "from inequality (3.83) on page 256 for the scalar estimator T and estimand\n",
      "g(θ). (Note that θ itself may be a vector.) That is the information inequality\n",
      "or the Cram´er-Rao lower bound (CRLB), and it results from the covariance\n",
      "inequality.\n",
      "If g(θ) is a vector, then ∂g(θ)/∂θ is the Jacobian, and we have\n",
      "V(T(X)) ⪰\n",
      "\u0012 ∂\n",
      "∂θ g(θ)\n",
      "\u0013T\n",
      "(I(θ))−1 ∂\n",
      "∂θ g(θ),\n",
      "(5.13)\n",
      "where we assume the existence of all quantities in the expression.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "400\n",
      "5 Unbiased Point Estimation\n",
      "Note the meaning of this relationship in the multiparameter case: it says\n",
      "that the matrix\n",
      "V(T(X)) −\n",
      "\u0012 ∂\n",
      "∂θ g(θ)\n",
      "\u0013T\n",
      "(I(θ))−1 ∂\n",
      "∂θ g(θ)\n",
      "(5.14)\n",
      "is nonnegative deﬁnite. (This includes the zero matrix; the zero matrix is\n",
      "nonnegative deﬁnite.)\n",
      "Example 5.11 Fisher eﬃciency in a normal distribution\n",
      "Consider a random sample X1, X2, . . ., Xn from the N(µ, σ2) distribution. In\n",
      "Example 3.9, we used the parametrization θ = (µ, σ). Now we will use the\n",
      "parametrization θ = (µ, σ2). The joint log density is\n",
      "log p(µ,σ)(x) = c −n\n",
      "2 log(σ2) −\n",
      "X\n",
      "i\n",
      "(xi −µ)2/(2σ2).\n",
      "(5.15)\n",
      "The information matrix is diagonal, so the inverse of the information matrix\n",
      "is particularly simple:\n",
      "I(θ)−1 =\n",
      "\"\n",
      "σ2\n",
      "n\n",
      "0\n",
      "0\n",
      "σ4\n",
      "2(n−1)\n",
      "#\n",
      ".\n",
      "(5.16)\n",
      "For the simple case of g(θ) = (µ, σ2), we have the unbiased estimator,\n",
      "T(X) =\n",
      " \n",
      "X,\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(Xi −X)2/(n −1)\n",
      "!\n",
      ",\n",
      "and\n",
      "V(T(X)) =\n",
      "\"\n",
      "σ2\n",
      "n\n",
      "0\n",
      "0\n",
      "σ4\n",
      "2(n−1)\n",
      "#\n",
      ",\n",
      "(5.17)\n",
      "which is the same as the inverse of the information matrix. The estimators\n",
      "are Fisher eﬃcient.\n",
      "It is important to know in what situations an unbiased estimator can\n",
      "achieve the CRLB. Notice this would depend on both p(X, θ) and g(θ). Let\n",
      "us consider this question for the case of scalar θ and scalar function g. The\n",
      "necessary and suﬃcient condition that an estimator T of g(θ) attain the CRLB\n",
      "is that (T −g(θ)) be proportional to ∂log(p(X, θ))/∂θ a.e.; that is, for some\n",
      "a that does not depend on X,\n",
      "∂log(p(X, θ))\n",
      "∂θ\n",
      "= a(θ)(T −g(θ))\n",
      "a.e.\n",
      "(5.18)\n",
      "This means that the CRLB can be obtained by an unbiased estimator only in\n",
      "the one-parameter exponential family.\n",
      "For example, there are unbiased estimators of the mean in the normal,\n",
      "Poisson, and binomial families that attain the CRLB. There is no unbiased\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.1 UMVUE\n",
      "401\n",
      "estimator of θ that attains the CRLB in the family of distributions with\n",
      "Lebesgue densities proportional to (1+(x−θ)2)−1 (this is the Cauchy family).\n",
      "If the CRLB is attained for an estimator of g(θ), it cannot be attained\n",
      "for any other (independent) function of θ. For example, there is no unbiased\n",
      "estimator of µ2 in the normal distribution that achieves the CRLB.\n",
      "If the CRLB is not sharp, that is, if it cannot be attained, there may be\n",
      "other (larger) bounds, for example the Bhattacharyya bound. These sharper\n",
      "bounds are usually based on higher-order derivatives.\n",
      "The following example is from Romano and Siegel (1986), who attribute\n",
      "it to Bickel and Doksum.\n",
      "Example 5.12 UMVUE in Exponential Family That Does Not At-\n",
      "tain the CRLB\n",
      "Let X have a Poisson distribution with PDF\n",
      "p(x) = θye−θ/y!,\n",
      "y = 0, 1, 2, . . .,\n",
      "and suppose we want to estimate g(θ) = e−θ.\n",
      "For a sample of size 1, let\n",
      "T(X) =\n",
      "\u001a 1 if X = 0\n",
      "0 otherwise.\n",
      "We see that T(X) has expectation e−θ and so is unbiased for the estimand.\n",
      "We know that X is suﬃcient, and we see that it is complete by considering\n",
      "a function g such that E(g(X)) = 0 for all θ > 0. For such a function, for all\n",
      "θ > 0, we have\n",
      "e−θ\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "g(i) 1\n",
      "i!θi = 0.\n",
      "A power series that is identically zero in an interval must have all coeﬃcients\n",
      "zero, and so g(x) = 0 a.e.; hence, X is complete.\n",
      "Now, by the Lehmann-Scheﬀ´e theorem, T = E(T|X) is UMVUE for e−θ,\n",
      "and since it has ﬁnite variance, V(T) = e−θ(1−e−θ), it is the unique UMVUE.\n",
      "We can work out the Fisher information to be\n",
      "Iθ = E\n",
      " \u0012∂log(p(X; θ))\n",
      "∂θ\n",
      "\u00132!\n",
      "= E\n",
      " \u0012\n",
      "−1 + X\n",
      "θ\n",
      "\u00132!\n",
      "= 1\n",
      "θ2 E(X2) −2\n",
      "θ E(X) + 1\n",
      "= 1\n",
      "θ .\n",
      "Hence, the CRLB for the variance of unbiased estimators of g(θ) = e−θ is\n",
      "θe−2θ. By expanding e−θ in a Taylor series, we see that V(T) = e−θ(1−e−θ) >\n",
      "θe−2θ; hence, the UMVUE does not attain the CRLB.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "402\n",
      "5 Unbiased Point Estimation\n",
      "The Bhattacharyya Lower Bound\n",
      "We now consider a simple case in which θ is a scalar (and, hence the estimand\n",
      "g(θ) and the estimator T(X) are scalars).\n",
      "For the PDF f(x; θ) and the Borel scalar function g(θ) assume that each\n",
      "is diﬀerentiable r times, and write\n",
      "f(r) = ∂rf(x; θ)\n",
      "∂r\n",
      "and\n",
      "g(r) = ∂rg(θ)\n",
      "∂r\n",
      ".\n",
      "Let T be an unbiased estimator of g(θ).\n",
      "Now, form the function\n",
      "Ds = T −g(θ) −\n",
      "s\n",
      "X\n",
      "r=1\n",
      "arf(r)/f,\n",
      "(5.19)\n",
      "where the ar are constants to be determined. Now, we have\n",
      "E(f(r)/f) = 0\n",
      "(5.20)\n",
      "as before, and since T be an unbiased estimator for g(θ), we have\n",
      "E(Ds) = 0.\n",
      "The variance of Ds is therefore,\n",
      "E(D2\n",
      "s) =\n",
      "Z  \n",
      "T −g(θ) −\n",
      "s\n",
      "X\n",
      "r=1\n",
      "arf(r)/f\n",
      "!2\n",
      "fdx.\n",
      "(5.21)\n",
      "We now seek to minimize this quantity in the ar. To do so, for p = 1, . . ., s,\n",
      "we diﬀerentiate and set equal to zero:\n",
      "Z  \n",
      "T −g(θ) −\n",
      "s\n",
      "X\n",
      "r=1\n",
      "arf(r)/f\n",
      "!\n",
      "(f(p)/f)fdx = 0,\n",
      "(5.22)\n",
      "which yields\n",
      "Z\n",
      "(T −g(θ))f(p)dx =\n",
      "s\n",
      "X\n",
      "r=1\n",
      "ar\n",
      "Z f(r)\n",
      "f\n",
      "f(p)\n",
      "f fdx.\n",
      "(5.23)\n",
      "Because of (5.20), the left-hand side of (5.23) is\n",
      "Z\n",
      "Tf(p)dx = g(p)(θ).\n",
      "(5.24)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.1 UMVUE\n",
      "403\n",
      "(Compare this with R Tfdx = g(θ).)\n",
      "The right-hand side of (5.23) is\n",
      "s\n",
      "X\n",
      "r=1\n",
      "arE\n",
      "\u0012f(r)\n",
      "f\n",
      "f(p)\n",
      "f\n",
      "\u0013\n",
      ".\n",
      "Substituting back into (5.23) we have\n",
      "g(p)(θ) =\n",
      "s\n",
      "X\n",
      "r=1\n",
      "arE\n",
      "\u0012f(r)\n",
      "f\n",
      "f(p)\n",
      "f\n",
      "\u0013\n",
      ",\n",
      "(5.25)\n",
      "for p = 1, . . ., s. If the matrix of coeﬃcients of the ar is nonsingular, we can\n",
      "invert them to solve. For notational simplicity, let,\n",
      "Jrp = E\n",
      "\u0012f(r)\n",
      "f\n",
      "f(p)\n",
      "f\n",
      "\u0013\n",
      ".\n",
      "Then\n",
      "ar =\n",
      "s\n",
      "X\n",
      "p=1\n",
      "g(p)(θ)J−1\n",
      "rp .\n",
      "Hence, at its minimum value\n",
      "Ds = T −g(θ) −\n",
      "s\n",
      "X\n",
      "r=1\n",
      "s\n",
      "X\n",
      "p=1\n",
      "g(p)(θ)J−1\n",
      "rp f(r)/f.\n",
      "(5.26)\n",
      "and the variance of Ds from (5.21) is\n",
      "E(D2\n",
      "s) =\n",
      "Z  \n",
      "T −g(θ) −\n",
      "s\n",
      "X\n",
      "r=1\n",
      "s\n",
      "X\n",
      "p=1\n",
      "g(p)(θ)J−1\n",
      "rp f(r)/f\n",
      "!2\n",
      "fdx.\n",
      "(5.27)\n",
      "We now use the fact that the derivative is zero, equation (5.22), to get\n",
      "E(D2\n",
      "s) =\n",
      "Z\n",
      "(T −g(θ))2fdx −\n",
      "s\n",
      "X\n",
      "r=1\n",
      "s\n",
      "X\n",
      "p=1\n",
      "g(p)(θ)J−1\n",
      "rp\n",
      "Z\n",
      "Tf(r)dx,\n",
      "(5.28)\n",
      "which, because T is unbiased using equation (5.24), yields\n",
      "E(D2\n",
      "s) = V(T) −\n",
      "s\n",
      "X\n",
      "r=1\n",
      "s\n",
      "X\n",
      "p=1\n",
      "g(p)(θ)J−1\n",
      "rp g(r)(θ).\n",
      "Finally, because the left-hand side of this is nonnegative, we have the Bhat-\n",
      "tacharyya bound on the variance of T:\n",
      "V(T) ≥\n",
      "s\n",
      "X\n",
      "r=1\n",
      "s\n",
      "X\n",
      "p=1\n",
      "g(p)(θ)J−1\n",
      "rp g(r)(θ).\n",
      "(5.29)\n",
      "Notice that in the case of s = 1, this is the same as the CRLB.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "404\n",
      "5 Unbiased Point Estimation\n",
      "5.2 U-Statistics\n",
      "In estimation problems it is often fruitful to represent the estimand as some\n",
      "functional of the CDF, P . The mean, for example, if it exists is\n",
      "M(P ) =\n",
      "Z\n",
      "x dP.\n",
      "(5.30)\n",
      "Given the exchangeable random variables X1, . . ., Xn with CDF P , we can\n",
      "form a plug-in estimator of M(P ) by applying the functional to the ECDF.\n",
      "In more complicated cases, the property of interest may be the quantile\n",
      "associated with π, that is, the unique value yπ deﬁned by\n",
      "Ξπ(P ) = inf\n",
      "y {y : P (y) ≥π}.\n",
      "(5.31)\n",
      "There is a basic diﬀerence in the functionals in equations (5.30) and (5.31).\n",
      "The ﬁrst is an expected value, E(Xi) for each i. The second functional,\n",
      "however, cannot be written as an expectation. (Bickel and Lehmann (1969)\n",
      "showed this.)\n",
      "5.2.1 Expectation Functionals and Kernels\n",
      "In the following, we will consider the class of statistical functions that can be\n",
      "written as an expectation of a function h of some subsample, Xi1, . . ., Xim,\n",
      "where i1, . . ., im are distinct elements of {1, . . ., n}:\n",
      "θ = Θ(P )\n",
      "= E(h(Xi1, . . ., Xim)).\n",
      "(5.32)\n",
      "Such Θs are called expectation functionals. The function h is called the kernel\n",
      "of the expectation functional. The number of arguments of the kernel is called\n",
      "the order of the kernel.\n",
      "In the case of M in equation (5.30) above, h is the identity and the order\n",
      "m is 1.\n",
      "Notice that we have unbiasedness of the kernel function for θ by the way\n",
      "we deﬁne the terms.\n",
      "Expectation functionals that relate to parameter of interest are often easy\n",
      "to deﬁne. The simplest is just E(h(Xi)). The utility of expectation function-\n",
      "als lies in the ease of working with them coupled with some useful general\n",
      "properties.\n",
      "Note that without loss of generality we can assume that h is symmetric in\n",
      "its arguments because the Xis are exchangeable, and so even if h is not sym-\n",
      "metric, any permutation (i1, . . ., im) of the indexes has the same expectation,\n",
      "so we could form a function that is symmetric in the arguments and has the\n",
      "same expectation:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.2 U-Statistics\n",
      "405\n",
      "¯h(X1, . . ., Xm) = 1\n",
      "m!\n",
      "X\n",
      "all permutations\n",
      "h(Xi1, . . ., Xim).\n",
      "Example 5.13 symmetric kernel\n",
      "If X1\n",
      "d= X2\n",
      "d= X and X1, X2, and X are exchangeable, we can write the\n",
      "variance of the random variable X as\n",
      "V(X) = E(X2\n",
      "1 ) −E(X1)E(X2).\n",
      "This may suggest the kernel\n",
      "h(x1, x2) = x2\n",
      "1 −x1x2,\n",
      "(5.33)\n",
      "which is not symmetric, that is, h(x1, x2) ̸= h(x2, x1). We can, however, form\n",
      "a kernel that is equivalent (in the sense of expected value) by a linear combi-\n",
      "nation (with equal weights):\n",
      "¯h(x1, x2) = 1\n",
      "2 (h(x1, x2) + h(x2, x1))\n",
      "= 1\n",
      "2(x1 −x2)2,\n",
      "(5.34)\n",
      "which is symmetric.\n",
      "Because of the symmetry, we will just need to consider h evaluated over\n",
      "the possible combinations of m items from the sample of size n. Furthermore,\n",
      "because the Xij are exchangeable, the properties of h(Xi1, . . ., Xim) are the\n",
      "same as the properties of h(X1, . . ., Xm).\n",
      "Degree of Expectation Functional\n",
      "We might wonder what is the minimum number of arguments a kernel that\n",
      "is associated with a given expectation functional must have.\n",
      "Example 5.14\n",
      "Consider a single observation X from a N(µ, σ2) distribution with both µ\n",
      "and σ2 unknown. Is there an unbiased estimator of σ2 based on X? Suppose\n",
      "T(X) is unbiased for σ2. Now, suppose σ2 = σ2\n",
      "0, some ﬁxed value; that is,\n",
      "E(T(X)) = σ2\n",
      "0. Because X is complete suﬃcient statistic for µ, E(T(X)) = σ2\n",
      "0\n",
      "for all µ implies T(X) = σ2\n",
      "0 a.e.; that is, T(X) cannot be unbiased for σ2.\n",
      "We have seen that we do have an unbiased estimator of the variance from\n",
      "a sample of size 2, X1 and X2. It is the sample variance, which can be written\n",
      "as 1\n",
      "2(X1 −X2)2, as suggested in Example 5.13.\n",
      "The analysis in the previous example leads us to the concept of the degree\n",
      "of an expectation functional or statistical function (see page 392). This is the\n",
      "minimum number of observations that can be combined in such a way that\n",
      "the expectation of the combination is the given functional. From the facts\n",
      "above, we see that the degree of the variance functional is 2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "406\n",
      "5 Unbiased Point Estimation\n",
      "5.2.2 Kernels and U-Statistics\n",
      "Now consider the estimation of the expectation functional Θ(P ) in equa-\n",
      "tion (5.32), given a random sample X1, . . ., Xn, where n ≥m.\n",
      "Clearly h(X1, . . ., Xm) is an unbiased estimator of θ = Θ(P ), and so is\n",
      "h(Xi1, . . ., Xim) for any m-tuple, 1 ≤i1 < · · · < im ≤n; hence, we have that\n",
      "U =\n",
      "1\n",
      "\u0000n\n",
      "m\n",
      "\u0001\n",
      "X\n",
      "all combinations\n",
      "h(Xi1, . . ., Xim)\n",
      "(5.35)\n",
      "is unbiased for θ.\n",
      "A statistic of this form is called a U-statistic. The U-statistic is a function\n",
      "of all n items in the sample. The function h, which is called the kernel of the\n",
      "U-statistic is a function of m arguments. We also refer to the order of the\n",
      "kernel as the order of the U-statistic.\n",
      "Examples\n",
      "Example 5.15 rth raw moment: M ′\n",
      "r(P ) = E(Xr)\n",
      "In the simplest U-statistic for r = 1, the kernel is of order 1 and h is the\n",
      "identity, h(x) = x. This is just the sample mean. More generally, we have the\n",
      "rth raw population moment by deﬁning hr(xi) = xr\n",
      "i , yielding the ﬁrst order\n",
      "U-statistic\n",
      "U(X1, . . ., Xn) = 1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "Xr\n",
      "i ,\n",
      "which is the rth sample moment.\n",
      "(The notation hr will be used diﬀerently below.***)\n",
      "Example 5.16 rth power of the mean: (E(X))r\n",
      "Another simple U-statistic with expectation (E(X))r where the rth order ker-\n",
      "nel is h(x1, . . ., xr) = x1 · · ·xr. The U-statistic\n",
      "U(X1, . . ., Xn) =\n",
      "1\n",
      "\u0000n\n",
      "r\n",
      "\u0001\n",
      "X\n",
      "all combinations\n",
      "Xi1 · · · Xir\n",
      "has expectation (E(X))r.\n",
      "Example 5.17 Pr(X ≤a): Θ(P ) = E \u0000I]∞,a](X)\u0001 = P (a)\n",
      "Compare this with the quantile functional in equation (5.31), which cannot\n",
      "be expressed as an expectation functional. The quantile problem is related to\n",
      "an inverse problem in which the property of interest is the π; that is, given a\n",
      "value a, estimate P (a). We can write an expectation functional and arrive at\n",
      "the U-statistic\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.2 U-Statistics\n",
      "407\n",
      "U(X1, . . ., Xn) =\n",
      "1\n",
      "\u0000n\n",
      "1\n",
      "\u0001\n",
      "n\n",
      "X\n",
      "i=1\n",
      "I]−∞,a](Xi)\n",
      "= Pn(a),\n",
      "where Pn is the ECDF.\n",
      "Example 5.18 Gini’s mean diﬀerence\n",
      "Θ(P ) = E (|X1 −X2|)\n",
      "A familiar second order U-statistic is Gini’s mean diﬀerence, in which h(x1, x2) =\n",
      "|x2 −x1|, for n ≥2,\n",
      "U =\n",
      "1\n",
      "\u0000n\n",
      "2\n",
      "\u0001\n",
      "X\n",
      "i<j\n",
      "|Xj −Xi|.\n",
      "(5.36)\n",
      "Example 5.19 covariance: Σ(PY Z) = Cov(Y, Z)\n",
      "Let X = (Y, Z). We form the second order kernel\n",
      "h(x1, x2) = 1\n",
      "2(y1 −y2)(z1 −z2),\n",
      "(5.37)\n",
      "where xi = (yi, zi). We see that\n",
      "E(h(X1, X2)) = 1\n",
      "2 (E(Y1Z1) −E(Y1)E(Z2) + E(Y2Z2) −E(Y1)E(Z2))\n",
      "= Cov(U, Z).\n",
      "We form the U-statistic\n",
      "U(X1, . . ., Xn) =\n",
      "1\n",
      "\u0000n\n",
      "2\n",
      "\u0001\n",
      "n\n",
      "X\n",
      "i<j\n",
      "h(Xi, Xj).\n",
      "(5.38)\n",
      "This U-statistic is the sample covariance S2\n",
      "y,z, that is,\n",
      "U(x1, . . ., xn) =\n",
      "1\n",
      "n −1\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(yi −¯y)(zi −¯z),\n",
      "which is unbiased for the population covariance if it exists.\n",
      "Notice that if Y = Z, the U-statistic\n",
      "U(X1, . . ., Xn) =\n",
      "2\n",
      "n(n −1)\n",
      "n\n",
      "X\n",
      "i<j\n",
      "h(Xi, Xj).\n",
      "(5.39)\n",
      "is the sample variance S2, which is unbiased for the population variance if it\n",
      "exists. The kernel (5.37) is the same as (5.33), which we put in the symmetric\n",
      "form (5.34).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "408\n",
      "5 Unbiased Point Estimation\n",
      "Partitioning the Sample\n",
      "Notice that while the kernel is a function of only m arguments, U is a function\n",
      "of all n random variables, U(X1, . . ., Xn).\n",
      "Some useful statistical techniques involve partitioning of a given sample.\n",
      "The jackknife (see Section 3.6.1 beginning on page 301) is based on a system-\n",
      "atic partitioning of the sample, and the bootstrap (see Section 3.6.2 beginning\n",
      "on page 304) is based on a random resampling of the sample.\n",
      "If we index the elements of a given sample of size n as {1, . . ., n}, for given\n",
      "a integer r with 1 ≤r ≤n, we may form the sample with indexes\n",
      "S = {i1, . . ., ir} ⊆{1, . . ., n}.\n",
      "Corresponding to a statistic Tn computed from the full sample, we often use\n",
      "the notation Tr,S to denote the corresponding statistic computed from the\n",
      "sample indexed by S; that is,\n",
      "Tr,S = Tr(Xi1, . . ., Xir).\n",
      "To analyze statistical properties of the U statistic, we need to know which\n",
      "elements of the sample occur in each term in the sum (5.35) over all combina-\n",
      "tions. Sometimes it is useful to order these combinations in a systematic way.\n",
      "A lexicographic ordering is often the best way to do this. In one lexicographic\n",
      "ordering, we write the labels as an m-tuple (i1, . . ., im) and index the set of\n",
      "combinations such that (i1, . . ., im) is less than (j1, . . ., jm), if i1 < j1 or else\n",
      "if for r > 1, ik = jk for k < r and ir < jr. This ordering makes it easy\n",
      "to identify a pattern of the terms in the sum (5.35) in which any particular\n",
      "Xi appears. The element X1, for example, appears in the ﬁrst\n",
      "\u0000n−1\n",
      "m−1\n",
      "\u0001\n",
      "terms,\n",
      "and the element X2, appears in the ﬁrst\n",
      "\u0000n−2\n",
      "m−2\n",
      "\u0001\n",
      "terms and in the\n",
      "\u0000n−2\n",
      "m−3\n",
      "\u0001\n",
      "terms\n",
      "following the ﬁrst \u0000n−1\n",
      "m−1\n",
      "\u0001 terms. Hence, X1 and X2 occur together in \u0000 n−2\n",
      "m−2\n",
      "\u0001\n",
      "terms. These patterns become increasingly complicated of course.\n",
      "It is instructive to note some simple results of the sum of Tr,S, for various\n",
      "forms of Tn, over all combinations of a given sample, as in equation (5.35).\n",
      "We will denote such a summation as\n",
      "X\n",
      "C\n",
      "Tr,S.\n",
      "Now, as an example, let\n",
      "Tn =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "Xi/n = X.\n",
      "Then\n",
      "T 2\n",
      "n = 1\n",
      "n2\n",
      "\n",
      "\n",
      "n\n",
      "X\n",
      "i=1\n",
      "X2\n",
      "i +\n",
      "X\n",
      "i̸=j\n",
      "XiXj\n",
      "\n",
      ",\n",
      "(5.40)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.2 U-Statistics\n",
      "409\n",
      "X\n",
      "C\n",
      "Tr,S =\n",
      "\u0012n −1\n",
      "r −1\n",
      "\u0013n\n",
      "r Tn,\n",
      "(5.41)\n",
      "X\n",
      "C\n",
      "TnTr,S = Tn\n",
      "X\n",
      "C\n",
      "Tr,S\n",
      "=\n",
      "\u0012n −1\n",
      "r −1\n",
      "\u0013n\n",
      "r T 2\n",
      "n,\n",
      "(5.42)\n",
      "and\n",
      "X\n",
      "C\n",
      "T 2\n",
      "r,S =\n",
      "\u0012n −2\n",
      "r −2\n",
      "\u0013 1\n",
      "r2\n",
      "\n",
      "\n",
      "n\n",
      "X\n",
      "i=1\n",
      "X2\n",
      "i +\n",
      "X\n",
      "i̸=j\n",
      "XiXj\n",
      "\n",
      "\n",
      "+\n",
      "\u0012\u0012n −1\n",
      "r −1\n",
      "\u0013\n",
      "−\n",
      "\u0012n −2\n",
      "r −2\n",
      "\u0013\u0013 1\n",
      "r2\n",
      "n\n",
      "X\n",
      "i=1\n",
      "X2\n",
      "i\n",
      "=\n",
      "\u0012n −2\n",
      "r −2\n",
      "\u0013 1\n",
      "r2\n",
      " \n",
      "n2T 2\n",
      "n + n −r\n",
      "r −1\n",
      "n\n",
      "X\n",
      "i=1\n",
      "X2\n",
      "i\n",
      "!\n",
      ".\n",
      "(5.43)\n",
      "U-Statistic as a Conditional Expectation of the Kernel\n",
      "Notice that a U-statistic could be deﬁned in terms of a conditional expectation\n",
      "of the kernel, given a suﬃcient statistic, say the order statistics. That is, if U is\n",
      "as given in equation (5.35), X1, . . ., Xn is a random sample and X(1), . . ., X(n)\n",
      "are the order statistics from the given distribution, and h is an mth order kernel\n",
      "(with m ≤n), then\n",
      "U = E\n",
      "\u0000h(X1, . . ., Xm)|X(1), . . ., X(n)\n",
      "\u0001\n",
      ".\n",
      "(5.44)\n",
      "Example 5.13 shows that the kernel is not unique; that is, the same U-\n",
      "statistic could be formed from diﬀerent kernels.\n",
      "Variations of the Order of the Kernel\n",
      "We informally deﬁned the order of the kernel as the “number of arguments” in\n",
      "the kernel, and by this we meant the number of sample items included in the\n",
      "kernel. Occasionally, the kernel will include some argument computed from\n",
      "the full sample; that is, an mth order kernel involves more than m items from\n",
      "the sample; hence the precise meaning of “order” breaks down somewhat. An\n",
      "example of such a kernel is one that is a function of a single observation as well\n",
      "as of the sample mean, h(xi, x). Such a kernel obviously cannot be symmetric.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "410\n",
      "5 Unbiased Point Estimation\n",
      "Example 5.20 variance\n",
      "Writing the variance of the random variable X as\n",
      "V(X) = E ((X −E(X))2)\n",
      "may suggest the kernel\n",
      "h(xi, ¯x) = (xi −x)2.\n",
      "(5.45)\n",
      "At ﬁrst glance, we might think that the expected value of this kernel is σ2.\n",
      "Because Xi is included in X, however, we have\n",
      "E \u0000h(Xi, X)\u0001 = E\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(n −1)Xi/n −\n",
      "X\n",
      "j̸=i\n",
      "Xj/n\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "= E\n",
      "\n",
      "(n −1)2X2\n",
      "i /n2 −2(n −1)Xi\n",
      "X\n",
      "j̸=i\n",
      "Xj/n2\n",
      "+\n",
      "X\n",
      "j̸=k̸=i\n",
      "XjXk/n2 +\n",
      "X\n",
      "j̸=i\n",
      "X2\n",
      "j /n2\n",
      "\n",
      "\n",
      "= (n −1)2µ2/n2 + (n −1)2σ2/n2 −2(n −1)(n −1)µ2/n2\n",
      "+(n −1)(n −2)µ2/n2 + (n −1)µ2/n2 + (n −1)σ2/n2\n",
      "= n −1\n",
      "n\n",
      "σ2,\n",
      "and the U-statistic associated with this kernel of course also has expectation\n",
      "n−1\n",
      "n σ2. On more careful thought, we would expect the expected value of the\n",
      "kernel to be less than σ2, because the expectation of (Xi −µ)2, which does\n",
      "not have (n −1)Xi/n subtracted out, is σ2.\n",
      "This is not an example of a U-statistic that is “biased”. A U-statistic is\n",
      "always (tautologically) unbiased for its expectation, if it exists. It we want a\n",
      "U-statistic for the variance, we have started with the wrong kernel!.\n",
      "If instead of the kernel h above, we used the kernel\n",
      "g(Xi, X) =\n",
      "n\n",
      "n −1(Xi −X)2,\n",
      "(5.46)\n",
      "we would have an expectation functional of interest; that is, one such that\n",
      "E(g(X1, . . ., Xm)) is something of interest, namely σ2.\n",
      "Example 5.21 jackknife variance estimator\n",
      "The jackknife variance estimator (3.166)\n",
      "PN\n",
      "j=1(T ∗\n",
      "j −T)2\n",
      "r(r −1)\n",
      ".\n",
      "is a U-statistic whose kernel is of order n −d, but the kernel also involves all\n",
      "n observations.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.2 U-Statistics\n",
      "411\n",
      "Generalized U-Statistics\n",
      "We can generalize U-statistics in an obvious way to independent random sam-\n",
      "ples from more than one population. The sample sizes can be diﬀerent. We do\n",
      "not even require that the number of elements used as arguments to the kernel\n",
      "be the same.\n",
      "Example 5.22 two-sample Wilcoxon statistic\n",
      "A common U-statistic involving two populations is the two-sample Wilcoxon\n",
      "statistic. For this, we assume that we have two samples X11, . . ., X1n1 and\n",
      "X21, . . ., X2n2. The kernel is h(x1i, x2j) = I]−∞,0](x2j −x1i). The two-sample\n",
      "Wilcoxon statistic is\n",
      "U =\n",
      "1\n",
      "n1n2\n",
      "n1\n",
      "X\n",
      "i=1\n",
      "n2\n",
      "X\n",
      "j=1\n",
      "I]−∞,0](X2j −X1i).\n",
      "(5.47)\n",
      "This is an unbiased estimator of Pr(X11 ≤X21).\n",
      "The more familiar form of this statistic is n1n2U, and in this form it is\n",
      "called the Mann-Whitney statistic.\n",
      "The two sample Wilcoxon statistic or the Mann-Whitney statistic can be\n",
      "used to test that the distributions of two populations are the same versus\n",
      "the alternative that a realization from one distribution is typically smaller\n",
      "(or larger) than a realization from the other distribution. Although the two\n",
      "sample Wilcoxon statistic is sometimes used to test whether one population\n",
      "has a larger median than that of another population, if the distributions have\n",
      "quite diﬀerent shapes, a typical value from the ﬁrst population may tend to\n",
      "be smaller than a typical value from the second population.\n",
      "5.2.3 Properties of U-Statistics\n",
      "U-statistics have a number of interesting properties. U-statistics are often\n",
      "useful in nonparametric inference because, among other reasons, they are\n",
      "asymptotically the same as the plug-in estimator that is based on the em-\n",
      "pirical CDF. Some of the important statistics used in modern computational\n",
      "statistical methods are U-statistics.\n",
      "By conditioning on the order statistics, we can show that a UMVUE can\n",
      "be expressed as a U-statistic.\n",
      "Theorem 5.3\n",
      "Let X1, . . ., Xn be a random sample from a distribution with parameter θ and\n",
      "with ﬁnite variance. Let T = T(X1, . . ., Xn) be unbiased for θ. Then there is\n",
      "a U-statistic, U, that is also unbiased for θ, and\n",
      "V(U) ≤V(T).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "412\n",
      "5 Unbiased Point Estimation\n",
      "Proof.\n",
      "We ﬁrst deﬁne an nth order expectation kernel to be associated with T; in\n",
      "fact, it is the function itself:\n",
      "h(Xi1, . . ., Xin) = T(Xi1, . . ., Xin).\n",
      "The associated U-statistic is\n",
      "U = 1\n",
      "n!\n",
      "X\n",
      "C\n",
      "T(Xi1, . . ., Xin)\n",
      "Now, as in equation (5.44), we write\n",
      "U = E\n",
      "\u0000T(Xi1, . . ., Xin)|X(1), . . ., X(n)\n",
      "\u0001\n",
      ".\n",
      "Hence,\n",
      "E \u0000U 2\u0001 = E\n",
      "\u0010\u0000E \u0000T|X(1), . . ., X(n)\n",
      "\u0001\u00012\u0011\n",
      "≤E\n",
      "\u0000E\n",
      "\u0000T 2|X(1), . . ., X(n)\n",
      "\u0001\u0001\n",
      "= E\n",
      "\u0000T 2\u0001\n",
      ".\n",
      "with equality if and only if E\n",
      "\u0000T|X(1), . . ., X(n)\n",
      "\u0001\n",
      "is degenerate and equals T\n",
      "with probability 1.\n",
      "We will assume E(h(X1, . . ., Xm)2) < ∞. We ﬁrst introduce some addi-\n",
      "tional notation for convenience.\n",
      "(The notation hr problem ***)\n",
      "For k = 1, . . ., m, let\n",
      "hk(x1, . . ., xk) = E(h(X1, . . ., Xm)|X1 = x1, . . ., Xk = xk)\n",
      "= E(h(x1, . . ., xk, Xk+1, . . ., Xm)).\n",
      "(5.48)\n",
      "We have hm = h and\n",
      "hk(x1, . . ., xk) = E(hk+1(x1, . . ., xk, Xk+1, . . ., Xm)).\n",
      "(5.49)\n",
      "Now, we deﬁne the centered versions of the h: for k = 1, . . ., m,\n",
      "˜hk = hk −E(h(X1, . . ., Xm)),\n",
      "(5.50)\n",
      "and let\n",
      "˜h = ˜hm\n",
      "We see that the corresponding centered U-statistic is\n",
      "U −E(U) =\n",
      "1\n",
      "\u0000 n\n",
      "m\n",
      "\u0001\n",
      "X\n",
      "C\n",
      "˜h(Xi1, . . ., Xim)\n",
      "(5.51)\n",
      "This notation is convenient in the demonstration that a sequence of ad-\n",
      "justed kernels forms a martingale (see Serﬂing (1980), page 177).\n",
      "It is also a simple matter to work out the variance of the corresponding\n",
      "U-statistic.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.2 U-Statistics\n",
      "413\n",
      "Theorem 5.4 (Hoeﬀding’s Theorem)\n",
      "Let U be a U-statistic with mth order kernel h with E(h(X1, . . ., Xm)2) < ∞.\n",
      "Then\n",
      "V(U) =\n",
      "1\n",
      "\u0000 n\n",
      "m\n",
      "\u0001\n",
      "m\n",
      "X\n",
      "k=1\n",
      "\u0012m\n",
      "k\n",
      "\u0013\u0012n −m\n",
      "m −k\n",
      "\u0013\n",
      "ζk\n",
      "(5.52)\n",
      "where\n",
      "ζk = V(hk(X1, . . ., Xk)).\n",
      "(5.53)\n",
      "Proof.\n",
      "MS2 p. 176.\n",
      "Projections of U-Statistics\n",
      "One method of working out the asymptotic distribution of a U-statistic is by\n",
      "use of projections\n",
      "We ﬁrst relate Theorem 1.65 on page 118 to the U-statistic,\n",
      "Un =\n",
      "1\n",
      "\u0000n\n",
      "m\n",
      "\u0001\n",
      "X\n",
      "all combinations\n",
      "h(Xi1, . . ., Xim).\n",
      "Let eUn be the projection of Un onto X1, . . ., Xn. (See Section 1.5.3 beginning\n",
      "on page 115.) Recall, as in equation (5.48),\n",
      "h1(x1) = E(h(X1, X2, . . ., Xm)|X1 = x1)\n",
      "= E(h(x1, X2, . . ., Xm)).\n",
      "and\n",
      "˜h1 = h1 −E(h(X1, . . ., Xm)),\n",
      "Then, starting with the deﬁnition of eUn as a projection, we have\n",
      "eUn = E(Un) +\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(E(Un|Xi) −E(Un))\n",
      "= E(Un) + m\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "˜h1(Xi).\n",
      "This yields\n",
      "V(eUn) = m2\n",
      "n ζ1,\n",
      "where, in the notation of equation (5.53), ζ1 = V(h1(X1)).\n",
      "Hence, by Hoeﬀding’s theorem (actually a corollary of it), and Theo-\n",
      "rem 1.65, we have\n",
      "E((Un −eUn)2) ∈O(n−2).\n",
      "If ζ1 > 0, this yields\n",
      "√n(Un −E(Un)) d→N(0, m2ζ1).\n",
      "(Theorem 3.5(i) in MS2.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "414\n",
      "5 Unbiased Point Estimation\n",
      "Computational Complexity\n",
      "Evaluating a U-statistic can be computationally intensive, with the number of\n",
      "arithmetic operations of O(nm). As we discussed on page 303 for the delete-d\n",
      "jackknife, we may reduce the number of computations by using only some\n",
      "of the possible combinations. There are various ways that the combinations\n",
      "could be chosen, including, of course, just a random sampling. The U-statistic\n",
      "would be approximated by an average of the kernel evaluated only over the\n",
      "random sampling of the subsets.\n",
      "5.3 Asymptotically Unbiased Estimation\n",
      "A sequence of estimators that are unbiased for any ﬁnite sample size is unbi-\n",
      "ased in the limit and is asymptotically unbiased. There are, however, many\n",
      "situations when an unbiased estimator in a ﬁnite sample does not exist, or\n",
      "when we cannot form one easily, or when a biased estimator has better MSE\n",
      "for any ﬁnite sample than an unbiased estimator. A biased estimator that\n",
      "is asymptotically unbiased, and for which there is no dominating unbiased\n",
      "estimator, is often considered optimal.\n",
      "Sometimes, by studying the nature of the bias, it may be possible to iden-\n",
      "tify a correction, as in the following example that uses the jackknife (see\n",
      "Section 3.6.1 on page 301).\n",
      "Example 5.23 Jackknife Bias Reduction\n",
      "Suppose that we can represent the bias of T as a power series in n−1; that is,\n",
      "Bias(T) = E(T) −θ\n",
      "=\n",
      "∞\n",
      "X\n",
      "q=1\n",
      "aq\n",
      "nq ,\n",
      "(5.54)\n",
      "where the aq do not involve n. If all aq = 0, the estimator is unbiased. If\n",
      "a1 ̸= 0, the order of the bias is n−1. (Such an estimator is sometimes called\n",
      "“second-order accurate”. “First-order” accuracy implies a bias of order n−1/2.)\n",
      "Using the power series representation for the bias of T, we see that the\n",
      "bias of the jackknife estimator is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.3 Asymptotically Unbiased Estimation\n",
      "415\n",
      "Bias(J(T)) = E(J(T)) −θ\n",
      "= n(E(T) −θ) −n −1\n",
      "n\n",
      "n\n",
      "X\n",
      "j=1\n",
      "E(T(−j) −θ)\n",
      "= n\n",
      "∞\n",
      "X\n",
      "q=1\n",
      "aq\n",
      "nq −(n −1)\n",
      " ∞\n",
      "X\n",
      "q=1\n",
      "aq\n",
      "(n −1)q\n",
      "!\n",
      "= a2\n",
      "\u0012 1\n",
      "n −\n",
      "1\n",
      "n −1\n",
      "\u0013\n",
      "+ a3\n",
      "\u0012 1\n",
      "n2 −\n",
      "1\n",
      "(n −1)2\n",
      "\u0013\n",
      "+ . . .\n",
      "= −a2\n",
      "\u0012\n",
      "1\n",
      "n(n −1)\n",
      "\u0013\n",
      "+ a3\n",
      "\u0012 1\n",
      "n2 −\n",
      "1\n",
      "(n −1)2\n",
      "\u0013\n",
      "+ . . .;\n",
      "(5.55)\n",
      "that is, the bias of the jackknife estimator, Bias(J(T)), is at most of order\n",
      "n−2. If aq = 0 for q = 2, . . ., the jackknife estimator is unbiased.\n",
      "This reduction in the bias is a major reason for using the jackknife. Any\n",
      "explicit analysis of the bias reduction, however, depends on a representation\n",
      "of the bias in a power series in n−1 with constant coeﬃcients. This may not\n",
      "be possible, of course.\n",
      "From\n",
      "E(J(T)) −θ = E(T) −θ + (n −1)\n",
      "\n",
      "E(T) −1\n",
      "n\n",
      "n\n",
      "X\n",
      "j=1\n",
      "E(T(−j))\n",
      "\n",
      ",\n",
      "we have the jackknife estimator of the bias in T,\n",
      "BJ = (n −1)\n",
      "\u0000T (•) −T\n",
      "\u0001\n",
      ",\n",
      "(5.56)\n",
      "and the jackknife bias-corrected estimator of θ,\n",
      "TJ = nT −(n −1)T (•).\n",
      "(5.57)\n",
      "Example 5.24 Higher-Order Bias Corrections\n",
      "Suppose that we pursue the bias correction to higher orders by using a second\n",
      "application of the jackknife. The pseudovalues are\n",
      "T ∗∗\n",
      "j\n",
      "= nJ(T) −(n −1)J(T(−j)).\n",
      "(5.58)\n",
      "Assuming the same series representations for the bias as before, a second-order\n",
      "jackknife estimator,\n",
      "J2(T) =\n",
      "n2J(T) −(n −1)2 Pn\n",
      "j=1 J(T)(−j)/n\n",
      "n2 −(n −1)2\n",
      ",\n",
      "(5.59)\n",
      "is unbiased to order O(n−3).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "416\n",
      "5 Unbiased Point Estimation\n",
      "There are two major diﬀerences between this estimator and the ﬁrst-order\n",
      "jackknifed estimator. For the ﬁrst-order jackknife, J(T) diﬀers from T by a\n",
      "quantity of order n−1; hence, if T has variance of order n−1 (as we usually\n",
      "hope), the variance of J(T) is asymptotically the same as that of T. In other\n",
      "words, the bias reduction carries no penalty in increased variance. This is not\n",
      "the case for higher-order bias correction of J2(T).\n",
      "The other diﬀerence is that in the bias expansion,\n",
      "E(T) −θ =\n",
      "∞\n",
      "X\n",
      "q=1\n",
      "aq/nq,\n",
      "if aq = 0 for q ≥2, then the ﬁrst-order jackknifed estimator is unbiased. For\n",
      "the second-order jackknifed estimator, even if aq = 0 for q ≥3, the estimator\n",
      "may not be unbiased. Its bias is\n",
      "Bias(J2(T)) =\n",
      "a2\n",
      "(n −1)(n −2)(2n −1);\n",
      "(5.60)\n",
      "that is, it is still of order n−3.\n",
      "We will consider four general kinds of estimators that may be of this type:\n",
      "estimators based on the method of moments, functions of unbiased estimators,\n",
      "V-statistics, and quantile estimators. Some of these estimators arise as plug-in\n",
      "statistics in the ECDF, such as those based on the method of moments, and\n",
      "others from a general plug-in rule, in which individual estimators are used in\n",
      "diﬀerent parts of the formula for the estimand, such as ratio estimators.\n",
      "We would like for such biased estimators to have either limiting bias or\n",
      "asymptotic bias of zero.\n",
      "5.3.1 Method of Moments Estimators\n",
      "If the estimand is written as a functional of the CDF, θ = Θ(P ), an estimator\n",
      "formed by applying Θ to the ECDF, bθ = Θ(Pn) is call a plug-in estimator.\n",
      "If Θ is an expectation functional of the form\n",
      "R\n",
      "xrdP (x), that is, if Θ is\n",
      "a raw moment, then the plug-in estimator Θ(Pn) is unbiased for Θ. Central\n",
      "moments are more often of interest interest. A plug-in estimator of a central\n",
      "moment, just as the central moment itself, can be written as a function of the\n",
      "corresponding raw moment and the ﬁrst moment. Such estimators are called\n",
      "method of moments estimators.\n",
      "An example of an estimator based on the method of moments is eS2 =\n",
      "(n−1)S2/n as an estimator of the population variance, σ2. This is the second\n",
      "central moment of the sample, just as σ2 is the second central moment of the\n",
      "population. We have seen that, in certain conditions, the MSE of eS2 is less\n",
      "than that of S2, and while it is biased, its limiting and asymptotic bias is zero\n",
      "and is of order 1/n.\n",
      "Although the second central sample moment is biased, the raw sample\n",
      "moments are unbiased for the corresponding raw population moments, if they\n",
      "exist.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.3 Asymptotically Unbiased Estimation\n",
      "417\n",
      "5.3.2 Ratio Estimators\n",
      "Ratio estimators, that is, estimators composed of the ratio of two separate\n",
      "estimators, arise often in sampling applications. Another situation is when\n",
      "an estimator is based on a linear combination of observations with diﬀerent\n",
      "variances. If we have some way of estimating the variances so we can form\n",
      "a weighted linear combination, the resulting estimator will be biased, but its\n",
      "MSE may be better than the unweighted estimator. Also, it is often the case\n",
      "that the biased estimator is asymptotically normal and unbiased.\n",
      "5.3.3 V-Statistics\n",
      "As we have seen, a U-statistic is an unbiased estimator of an expectation func-\n",
      "tional; speciﬁcally, if Θ(P ) = E(h(X1, . . ., Xm)) the U-statistic with kernel h\n",
      "is unbiased for Θ(P ). Applying the functional Θ to the ECDF Pn, we have\n",
      "Θ(Pn) =\n",
      "1\n",
      "nm\n",
      "n\n",
      "X\n",
      "i1=1\n",
      "· · ·\n",
      "n\n",
      "X\n",
      "im=1\n",
      "h(Xi1, . . ., Xim)\n",
      "= V\n",
      "(say),\n",
      "(5.61)\n",
      "which we call the V-statistic associated with the kernel h, or equivalently\n",
      "associated with the U-statistic with kernel h. Recalling that Θ(Pn) in general\n",
      "is not unbiased for Θ(P ), we do not expect a V-statistic to be unbiased in\n",
      "general. However, in view of the asymptotic properties of Pn, we might expect\n",
      "V-statistics to have good asymptotic properties.\n",
      "A simple example is the variance, for which the U-statistic in equa-\n",
      "tion (5.39) is unbiased. The V-statistic with the same kernel is\n",
      "V =\n",
      "1\n",
      "2n2\n",
      "n\n",
      "X\n",
      "i=1\n",
      "n\n",
      "X\n",
      "j=1\n",
      "(Xi −Xj)2\n",
      "(5.62)\n",
      "=\n",
      "1\n",
      "2n2\n",
      "n\n",
      "X\n",
      "i=1\n",
      "n\n",
      "X\n",
      "j=1\n",
      "(X2\n",
      "i + X2\n",
      "j −2XiXj)\n",
      "= n −1\n",
      "n\n",
      "S2,\n",
      "where S2 is the sample variance. This V-statistic is the same as the plug-in\n",
      "estimator of the population variance, and as with the plug-in estimator, no\n",
      "particular underlying distribution is assumed. It is also the same as the MLE\n",
      "estimator given an assumed underlying normal distribution. The V-statistic\n",
      "is biased for the population variance; but as we have seen, it has a smaller\n",
      "MSE than the unbiased U-statistic.\n",
      "The development of V-statistics can be based on the idea of applying the\n",
      "same functional to the ECDF Fn as the functional that deﬁnes the estimand\n",
      "when applied to the CDF F , and which is the basis for the U-statistics. Since\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "418\n",
      "5 Unbiased Point Estimation\n",
      "the ECDF assigns probability 1/n to each point of the values X1, . . ., Xn, any\n",
      "m independent variables with CDF Fn take on each of the possible m-tuples\n",
      "(Xi1, . . ., Xim) with probability 1/nm. The plug-in estimator, call it V, of θ\n",
      "is therefore\n",
      "V =\n",
      "1\n",
      "nm\n",
      "n\n",
      "X\n",
      "i1=1\n",
      "· · ·\n",
      "n\n",
      "X\n",
      "im=1\n",
      "h(Xi1, . . ., Xim).\n",
      "Notice for m = 1, V is a U-statistic; but consider m = 2, as above. We have\n",
      "U =\n",
      "1\n",
      "n(n −1)\n",
      "X\n",
      "i\n",
      "X\n",
      "j̸=i\n",
      "h(Xi, Xj),\n",
      "however\n",
      "V = 1\n",
      "n2\n",
      "n\n",
      "X\n",
      "i=1\n",
      "n\n",
      "X\n",
      "j=1\n",
      "h(Xi, Xj)\n",
      "= 1\n",
      "n2\n",
      "X\n",
      "i\n",
      "X\n",
      "j̸=i\n",
      "h(Xi, Xj) + 1\n",
      "n2\n",
      "n\n",
      "X\n",
      "i=1\n",
      "h(Xi, Xi)\n",
      "While, as we have seen U is unbiased for θ, we see that V is biased:\n",
      "E(V ) = n −1\n",
      "n\n",
      "θ + 1\n",
      "nE(h(X1, X1))\n",
      "= θ + 1\n",
      "n (E(h(X1, X1)) −θ) .\n",
      "An example of a V-statistic with m = 2 uses h(x1, x2) = (x1 −x2)2/2, as\n",
      "in equation (5.62), and results in (n −1)S2/n as an estimator of σ2, which is\n",
      "of course asymptotically unbiased.\n",
      "Theorem 3.16 in MS2 shows that under certain general conditions, V-\n",
      "statistics have limiting normal distributions and are asymptotically unbiased.\n",
      "5.3.4 Estimation of Quantiles\n",
      "plug-in\n",
      "ﬁnite sample properties – Harrel-Davis estimator\n",
      "asymptotic normality (ch 1)\n",
      "5.4 Asymptotic Eﬃciency\n",
      "Often a statistical procedure does not have some desirable property for any\n",
      "ﬁnite sample size, but the procedure does have that property asymptotically.\n",
      "The asymptotic properties that are of most interest are those deﬁned in terms\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.4 Asymptotic Eﬃciency\n",
      "419\n",
      "of a sequence that has a limiting standard normal distribution, N(0, 1), or more\n",
      "generally, Nk(0, Ik). A standard normal distribution of a statistic is desirable\n",
      "because in that case, it is easy to associate statements of probabilities with\n",
      "values of the statistic. It is also desirable because it is often easy to work out\n",
      "the distribution of functions of a statistic that has a normal distribution.\n",
      "It is important to remember the diﬀerence in an asymptotic property and\n",
      "a limiting property. An asymptotic distribution is the same as a limiting distri-\n",
      "bution, but other asymptotic properties are deﬁned, somewhat arbitrarily, in\n",
      "terms of a limiting distribution of some function of the sequence of statistics\n",
      "and of a ﬁnite divergent or convergent sequence, an. This seems to mean that\n",
      "a particular asymptotic property, such as, say, the asymptotic variance, de-\n",
      "pends on what function of the sequence of statistics that we choose. Although\n",
      "there may be some degree of arbitrariness in “an” asymptotic expectation,\n",
      "there is a certain uniqueness, as expressed in Proposition 2.3 in MS2.\n",
      "5.4.1 Asymptotic Relative Eﬃciency\n",
      "We assume a family of distributions P, a sequence of estimators {Tn} of\n",
      "g(θ), and a sequence of constants {an} with limn→∞an = ∞or with\n",
      "limn→∞an = a > 0, and such that anTn(X)\n",
      "d→T and 0 < E(T) < ∞.\n",
      "We deﬁne the asymptotic mean squared error of {Tn} for estimating g(θ) wrt\n",
      "P as an asymptotic expectation of (Tn −g(θ))2; that is, E((T −g(θ))2)/an,\n",
      "which we denote as AMSE(Tn, g(θ), P).\n",
      "For comparing two estimators, we may use the asymptotic relative eﬃ-\n",
      "ciency, which for the estimators Sn and Tn of g(θ) wrt P is\n",
      "ARE(Sn, Tn, P) = AMSE(Sn, g(θ), P)/AMSE(Tn, g(θ), P).\n",
      "5.4.2 Asymptotically Eﬃcient Estimators\n",
      "Relative eﬃciency is a useful concept for comparing two estimators, whether or\n",
      "not they are unbiased. When we restrict our attention to unbiased estimators\n",
      "we use the phrase Fisher eﬃcient to refer to an estimator that attains its\n",
      "Cram´er-Rao lower bound (Deﬁnition 3.8). Again, notice the slight diﬀerence in\n",
      "“eﬃciency” and “eﬃcient”; while one meaning of “eﬃciency” is a relative term\n",
      "that is not restricted to unbiased estimators (or other unbiased procedures, as\n",
      "we will see later), “eﬃcient” is absolute. “Eﬃcient” only applies to unbiased\n",
      "estimators, and an estimator either is or is not eﬃcient. The state of being\n",
      "eﬃcient, of course is called “eﬃciency”. This is another meaning of the term.\n",
      "The phrase “Fisher eﬃciency” helps to emphasis this diﬀerence.\n",
      "We consider the problem of estimating the k-vector θ based on a random\n",
      "sample X1, . . ., Xn. We denote the sequence of estimators as {ˆθn}. Suppose\n",
      "(Vn(θ))−1\n",
      "2\n",
      "\u0010\n",
      "ˆθn −θ\n",
      "\u0011 d→Nk(0, Ik),\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "420\n",
      "5 Unbiased Point Estimation\n",
      "where, for each n, Vn(θ) is a k×k positive deﬁnite matrix. From the deﬁnition\n",
      "of asymptotic expectation of\n",
      "\u0010\n",
      "ˆθn −θ\n",
      "\u00112\n",
      ", Vn(θ) is the asymptotic variance-\n",
      "covariance matrix of ˆθn. Note that this matrix may depend on θ. We should\n",
      "note that for any ﬁxed n, Vn(θ) is not necessarily the variance-covariance\n",
      "matrix of ˆθn; that is, it is possible that Vn(θ) ̸= V(ˆθn).\n",
      "Just as we have deﬁned Fisher eﬃciency for an unbiased estimator of ﬁxed\n",
      "size, we deﬁne a sequence to be asymptotically Fisher eﬃcient if the sequence\n",
      "is asymptotically unbiased, the Fisher information matrix In(θ) exists and is\n",
      "positive deﬁnite for each n, and Vn(θ) = (In(θ))−1 for each n. The deﬁnition\n",
      "of asymptotically (Fisher) eﬃciency is often limited even further so as to apply\n",
      "only to estimators that are asymptotically normal. (MS2 uses the restricted\n",
      "deﬁnition.)\n",
      "Being asymptotically eﬃcient does not mean for any ﬁxed n that ˆθn is\n",
      "eﬃcient. First of all, for ﬁxed n, ˆθn may not even be unbiased; even if it is\n",
      "unbiased, however, it may not be eﬃcient.\n",
      "As we have emphasized many times, asymptotic properties are diﬀerent\n",
      "from limiting properties. As a striking example of this, consider a very simple\n",
      "example from Romano and Siegel (1986).\n",
      "Example 5.25 Asymptotic and Limiting Properties\n",
      "Let X1, . . ., Xn\n",
      "iid\n",
      "∼N1(µ, 1), and consider a randomized estimator ˆµn of µ\n",
      "deﬁned by\n",
      "ˆµn =\n",
      "\n",
      "\n",
      "\n",
      "Xn with probability 1 −1\n",
      "n\n",
      "n2 with probability 1\n",
      "n.\n",
      "It is clear that n1/2(ˆµn−µ)\n",
      "d→N(0, 1), and furthermore, the Fisher information\n",
      "for µ is n−1/2. The estimator ˆµn is therefore asymptotically Fisher eﬃcient.\n",
      "The bias of ˆµn, however, is\n",
      "E(ˆµn −µ) = µ\n",
      "\u0012\n",
      "1 −1\n",
      "n\n",
      "\u0013\n",
      "+ n −µ = n −µ/n,\n",
      "which tends to inﬁnity, and the variance is\n",
      "V(ˆµn) = E(ˆµ2) −(E(ˆµ))2\n",
      "=\n",
      "\u0012\n",
      "1 −1\n",
      "n\n",
      "\u0013 1\n",
      "n +\n",
      "\u0012 1\n",
      "n\n",
      "\u0013\n",
      "n4 −\n",
      "\u0012\n",
      "µ\n",
      "\u0012\n",
      "1 −1\n",
      "n\n",
      "\u0013\n",
      "+ n\n",
      "\u00132\n",
      "= n3 + O(n2),\n",
      "which also tends to inﬁnity. Hence, we have an asymptotically Fisher eﬃcient\n",
      "estimator whose limiting bias and limiting variance are both inﬁnite.\n",
      "The example can be generalized to any estimator Tn of g(θ) such that\n",
      "V(Tn) = 1/n and n1/2(Tn −g(θ))\n",
      "d→N(0, 1). From Tn form the estimator\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.4 Asymptotic Eﬃciency\n",
      "421\n",
      "eTn =\n",
      "\n",
      "\n",
      "\n",
      "Tn with probability 1 −1\n",
      "n\n",
      "n2 with probability 1\n",
      "n.\n",
      "The estimator eTn is also asymptotically Fisher eﬃcient but has inﬁnite limit-\n",
      "ing bias and inﬁnite limiting variance.\n",
      "Asymptotic Eﬃciency and Consistency\n",
      "Although asymptotic eﬃciency implies that the estimator is asymptotically\n",
      "unbiased, even if the limiting variance is zero, asymptotic eﬃciency does not\n",
      "imply consistency. The counterexample above shows this.\n",
      "Likewise, of course, consistency does not imply asymptotic eﬃciency. There\n",
      "are many reasons. First, asymptotic eﬃciency is usually only deﬁned in the\n",
      "case of asymptotic normality (of course, it is unlikely that a consistent esti-\n",
      "mator would not be asymptotically normal). More importantly, the fact that\n",
      "both the bias and the variance go to zero as required by consistency, is not\n",
      "very strong. There are many ways both of these can go to zero without re-\n",
      "quiring asymptotic unbiasedness or that the asymptotic variance satisfy the\n",
      "asymptotic version of the information inequality.\n",
      "The Asymptotic Variance-Covariance Matrix\n",
      "In the problem of estimating the k-vector θ based on a random sample\n",
      "X1, . . ., Xn with the sequence of estimators as {ˆθn}, if\n",
      "(Vn(θ))−1\n",
      "2\n",
      "\u0010\n",
      "ˆθn −θ\n",
      "\u0011 d→Nk(0, Ik),\n",
      "where, for each n, Vn(θ) is a k × k positive deﬁnite matrix, then Vn(θ) is the\n",
      "asymptotic variance-covariance matrix of ˆθn. As we have noted, for any ﬁxed\n",
      "n, Vn(θ) is not necessarily the variance-covariance matrix of ˆθn.\n",
      "If Vn(θ) = V(ˆθn), then under the information inequality regularity condi-\n",
      "tions that yield the CRLB, we know that\n",
      "Vn(θ) ⪰(In(θ))−1 ,\n",
      "where In(θ) is the Fisher information matrix.\n",
      "Supereﬃciency\n",
      "Although if Vn(θ) ̸= V(ˆθn), the CRLB says nothing about the relation-\n",
      "ship between Vn(θ) and (In(θ))−1, we might expect that Vn(θ) ⪰(In(θ))−1.\n",
      "That this is not necessarily the case is shown by a simple example given by\n",
      "Joseph Hodges in a lecture in 1951, published in Le Cam (1953) (see also\n",
      "Romano and Siegel (1986)).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "422\n",
      "5 Unbiased Point Estimation\n",
      "Example 5.26 Hodges’ Supereﬃcient Estimator\n",
      "Let X1, . . ., Xn\n",
      "iid\n",
      "∼N1(µ, 1), and consider an estimator ˆµn of µ deﬁned by\n",
      "ˆµn =\n",
      "\n",
      "\n",
      "\n",
      "Xn if |Xn| ≥n−1/4\n",
      "tXn otherwise,\n",
      "for some ﬁxed t with |t| < 1.\n",
      "We have\n",
      "√n(Tn −g(θ))\n",
      "d→N(0, v(θ)),\n",
      "where v(θ) = 1 if θ ̸= 0 and v(θ) = t2 if θ = 0. (This takes a little working\n",
      "out; consider the two parts.)\n",
      "Notice that I(θ) = 1 and g′(θ) = 1, hence, at θ = 0, with |t| < 1, we have\n",
      "v(θ) < (g′(θ))2\n",
      "I(θ)\n",
      ".\n",
      "The estimator ˆµn is sometimes called “Hodges’ supereﬃcient estimator”.\n",
      "What gives Example 5.26 its kick is the dependence of the asymptotic\n",
      "distribution of ˆµn on µ. If µ ̸= 0, ˆµn has the same asymptotic distribution\n",
      "as Xn, and obeys the CRLB, both in its variance for ﬁnite n (even though\n",
      "it is biased) and in its asymptotic variance. However, if µ = 0, ˆµn is still\n",
      "asymptotically unbiased, but the asymptotic variance of ˆµn is t2/n, which is\n",
      "smaller than the inverse of asymptotic Fisher information, 1/n.\n",
      "A point in the parameter space at which this anomaly occurs is called\n",
      "a point of supereﬃciency. Le Cam has shown that under certain regularity\n",
      "conditions (that are slightly more stringent than the information inequality\n",
      "regularity conditions, see page 169) the number of points of supereﬃciency\n",
      "is countable. I list all of these regularity conditions in the statement of the\n",
      "following theorem, which is due to Le Cam (1953).\n",
      "Theorem 5.5\n",
      "Let X1, . . ., Xn be iid *****************\n",
      "Proof.\n",
      "Supereﬃciency is not important in applications (that is, where n is ﬁnite)\n",
      "any decrease in mean squared error at a point of supereﬃciency is accompanied\n",
      "by an increase in mean squared error at nearby points (and, of course, if we\n",
      "knew the parameter was a point of supereﬃciency, we would probably not be\n",
      "estimating it.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "423\n",
      "5.5 Applications\n",
      "Many methods of statistical inference rely on samples of identically distributed\n",
      "random variables. Two major areas of application of the methods are in anal-\n",
      "ysis of linear models and sampling of ﬁnite populations.\n",
      "5.5.1 Estimation in Linear Models\n",
      "In a simple variation on the requirement of identical distributions, we assume\n",
      "a model with two components, one “systematic” and one random, and the\n",
      "distributions of the observable random variables depend on the systematic\n",
      "component.\n",
      "Systematic and Random Components\n",
      "The most common form of linear model is one in which a random variable Y\n",
      "is the sum of a systematic component that determines its expected value and\n",
      "random component that is the value of an underlying unobservable random\n",
      "variable that has an expected value of 0. The systematic component may be\n",
      "a function of some additional variables x and parameters θ. If we represent\n",
      "the underlying unobservable random with expectation 0, as ϵ, we have\n",
      "Y = f(x, θ) + ϵ.\n",
      "(5.63)\n",
      "In this setup the mean of the random variable Y is determined by the param-\n",
      "eter θ and the values of the x variables, which are covariates (also called re-\n",
      "gressors, carriers, or independent variables). We generally treat the covariates\n",
      "as ﬁxed variables, that is, whether or not we could also model the covariates\n",
      "as random variables, in the simplest cases, we will use their observed values\n",
      "without regard to their origin.\n",
      "Regression Models\n",
      "The model above is a regression model. In the simplest variation, the observ-\n",
      "able random variables are independent, and have distributions in the same\n",
      "location family: P = {Pf(x,θ),Pϵ}. The family Pϵ of distributions Pϵ of the\n",
      "random component may be a parametric family, such as N(0, σ2), or it may\n",
      "be a nonparametric family. Whatever other assumptions on Pϵ, we assume\n",
      "E(ϵ) = 0.\n",
      "Linear Models\n",
      "Often we assume that the systematic component is a linear combination of\n",
      "the covariates. This setup is called a linear model, and is usually written in\n",
      "the form\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "424\n",
      "5 Unbiased Point Estimation\n",
      "Y = xTβ + E,\n",
      "(5.64)\n",
      "where Y is the observable random variable, x is an observable p-vector of\n",
      "covariates, β is an unknown and unobservable p-vector of parameters, and E\n",
      "is an unobservable random variable with E(E) = 0 and V(E) = σ2I. The\n",
      "parameter space for β is B ⊆IRp.\n",
      "An item of a random sample from this model may be denoted\n",
      "Yi = xT\n",
      "i β + Ei,\n",
      "(5.65)\n",
      "and a random sample be written in the vector-matrix form\n",
      "Y = Xβ + E,\n",
      "(5.66)\n",
      "where Y and E are n-vectors, X is an n × p matrix whose rows are the xT\n",
      "i ,\n",
      "and β is the p-vector above. A sample of realizations may be written in the\n",
      "vector-matrix form\n",
      "y = Xβ + ϵ.\n",
      "(5.67)\n",
      "where y and ϵ are n-vectors. This is the most commonly used notation.\n",
      "Inference in a Linear Model\n",
      "For estimation in a linear model, rather than formulating a decision problem\n",
      "and seeking a minimum risk estimator, we usually begin with a diﬀerent ap-\n",
      "proach. Estimation in a linear model is most commonly developed based on\n",
      "two simple heuristics: least squares and unbiasedness.\n",
      "The degree of β is p, meaning that the minimum number of observations\n",
      "required for unbiased estimation of β is p. Inferences about characteristics\n",
      "of the distribution of ϵ require additional observations, however, and so we\n",
      "assume n > p in the following.\n",
      "In statistical inference, we can think of β either as an unobservable random\n",
      "variable or as an unknown constant. If we think of it as an unknown constant\n",
      "and we want to determine a value of it that optimizes some objective function\n",
      "(such as a likelihood or a sum of squares), then we ﬁrst must substitute a\n",
      "variable for the constant. Although we often skip over this step, it is important\n",
      "conceptually.\n",
      "Least Squares Solutions of Overdetermined Linear Systems\n",
      "Having substituted the variable b is in place of the unknown model parameter\n",
      "β, we have an overdetermined linear system\n",
      "y ≈Xb,\n",
      "(5.68)\n",
      "where y and X are given, b is unknown, and y ∈IRn, X ∈IRn×p, and b ∈IRp.\n",
      "Solving for b in this system is a common problem in linear algebra. It is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "425\n",
      "one aspect of the statistical problem of ﬁtting the model (5.66), in which we\n",
      "assume that y is a realization of a random variable Y with E(Y ) = Xβ, but\n",
      "for the time being we will just consider the algebraic issues in solving, or\n",
      "“ﬁtting”, the overdetermined system.\n",
      "Fitting an overdetermined system y ≈Xb involves a choice of a criterion\n",
      "for the goodness of the approximation. A common choice is the squared error;\n",
      "that is, a solution is a vector b that minimizes ∥y −Xb∥2. This follows the\n",
      "approach to statistical inference discussed in Section 3.2.3. The solution to\n",
      "the linear algebra problem (5.68) is often called an “estimator” even though\n",
      "there is no underlying probability distribution.\n",
      "We deﬁne a least squares estimator (LSE) of b or of β in equation (5.66)\n",
      "as\n",
      "b∗= arg min\n",
      "b∈B\n",
      "∥y −Xb∥2,\n",
      "(5.69)\n",
      "where ∥c∥= ∥c∥2 =\n",
      "√\n",
      "cTc =\n",
      "pPp\n",
      "i=1 c2\n",
      "i for the p-vector c.\n",
      "A least squares estimator of β may or may not be unique. Whether or not\n",
      "b∗is unique,\n",
      "∥y −Xb∗∥2\n",
      "(5.70)\n",
      "is unique. This is because the objective function is convex and bounded below.\n",
      "The least squares estimator is obtained by direct minimization of\n",
      "s(b) = ∥y −Xb∥2\n",
      "= yTy −2bTXTy + bTXTXb.\n",
      "First of all, we note that s(b) is diﬀerentiable, and\n",
      "∂2\n",
      "∂b2 s(b) = XTX\n",
      "is nonnegative deﬁnitive. We therefore know that at the minimum, we have\n",
      "the estimating equation\n",
      "∂s(b)/∂b = 0.\n",
      "(5.71)\n",
      "The estimating equation leads to the normal equations:\n",
      "XTXb = XTy.\n",
      "(5.72)\n",
      "The coeﬃcient matrix in these equations has a special form; it is a Gramian\n",
      "matrix. We may use b∗to denote any solution to the normal equations formed\n",
      "from the linear system y = Xb, that is\n",
      "b∗= (XTX)−XTy.\n",
      "(5.73)\n",
      "Notice that if X is not of full rank, b∗is not unique.\n",
      "A unique solution to these equations is\n",
      "bβ = (XTX)+XTy;\n",
      "(5.74)\n",
      "that is, the solution arising from the Moore-Penrose inverse (see page 784).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "426\n",
      "5 Unbiased Point Estimation\n",
      "LSE in a Probability Model\n",
      "The mechanical aspects of least squares ﬁtting do not rely on any probability\n",
      "distributions.\n",
      "An LSE of β yields LSEs of other quantities. In general, for an estimand\n",
      "θ that can be expressed as\n",
      "θ = Eg(Y, bβ),\n",
      "(5.75)\n",
      "we call bθ = g(y, bβ) the LSE of θ. Notice that this deﬁnition preserves unbi-\n",
      "asedness if the relationships are linear.\n",
      "If the quantities in the equations correspond to n observations that follow\n",
      "the model (5.64), then we form an LSE of lTβ, for given l ∈IRp, as\n",
      "lT bβ.\n",
      "(5.76)\n",
      "While this quantity may not be unique, the quantity\n",
      "∥Y −X bβ∥2/(n −p)\n",
      "(5.77)\n",
      "is unique; it is the LSE of V(ϵ) = σ2; and furthermore, it is unbiased for σ2\n",
      "(exercise).\n",
      "Linear U-Estimability\n",
      "One of the most important questions for statistical inference involves esti-\n",
      "mating or testing some linear combination of the elements of the parameter\n",
      "β; for example, we may wish to estimate β1 −β2 or to test the hypothesis\n",
      "that β1 −β2 = c1 for some constant c1. In general, we will consider the linear\n",
      "combination lTβ. Whether or not it makes sense to estimate such a linear\n",
      "combination depends on whether there is a function of the observable random\n",
      "variable Y such that\n",
      "g(E(Y )) = lTβ.\n",
      "(5.78)\n",
      "We generally restrict our attention to linear functions of E(Y ) and formally\n",
      "deﬁne a linear combination lTβ to be (linearly) U-estimable if and only if there\n",
      "exists a vector t such that\n",
      "tTE(Y ) = lTβ\n",
      "(5.79)\n",
      "for any β.\n",
      "It is clear that if X is of full column rank, then lTβ is linearly estimable\n",
      "for any l. More generally, it is easy to see that lTβ is linearly estimable for\n",
      "any l ∈span(XT). (The t vector in equation (5.79) is just the normalized\n",
      "coeﬃcients expressing l in terms of the columns of X.)\n",
      "Estimability depends only on the simplest distributional assumption about\n",
      "the model; that is, that E(ϵ) = 0.\n",
      "Theorem 5.6\n",
      "Let Y = Xβ + ϵ where E(ϵ) = 0. Let lTβ be a linearly estimable function and\n",
      "let bβ = (XTX)+XTY . Then lT bβ is unbiased for lTβ.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "427\n",
      "Proof.\n",
      "Because l ∈span(XT) = span(XTX), we can write\n",
      "l = XTX˜t,\n",
      "(5.80)\n",
      "for some vector ˜t Now, we have\n",
      "E(lT bβ) = E(lT(XTX)+XTY )\n",
      "= ˜tTXTX(XTX)+XTXβ\n",
      "= ˜tTXTXβ\n",
      "= lTβ.\n",
      "(5.81)\n",
      "Although we have been taking bβ to be (XTX)+XTY , the equations above\n",
      "follow for other least squares ﬁts, b∗= (XTX)−XTY , for any generalized\n",
      "inverse. In fact, the estimator of lTβ is invariant to the choice of the generalized\n",
      "inverse.\n",
      "Theorem 5.7\n",
      "Let Y = Xβ + ϵ where E(ϵ) = 0. Let lTβ be a linearly estimable function, let\n",
      "bβ = (XTX)+XTY and let b∗= (XTX)−XTY . Then lTb∗= lT bβ.\n",
      "Proof.\n",
      "If b∗= (XTX)−XTY , we have XTXb∗= XTY , and so\n",
      "lT bβ −lTb∗= ˜tTXTX(bβ −b∗) = ˜tT(XTY −XTY ) = 0.\n",
      "Gauss-Markov Theorem\n",
      "The Gauss-Markov theorem provides a restricted optimality property for es-\n",
      "timators of estimable functions of β under the condition that E(ϵ) = 0 and\n",
      "V(ϵ) = σ2I; that is, in addition to the assumption of zero expectation, which\n",
      "we have used above, we also assume that the elements of ϵ have constant\n",
      "variance and that their covariances are zero. Note that we do not assume\n",
      "independence or normality.\n",
      "The Gauss-Markov theorem states that lT bβ is the unique best linear un-\n",
      "biased estimator (BLUE) of the estimable function lTβ. (Recall that Theo-\n",
      "rem 5.7 tells us that the inner product is invariant to the choice of the general-\n",
      "ized inverse; that is, lTb∗= lT bβ, where b∗and bβ are given in equations (5.73)\n",
      "and (5.74) respectively.) “Linear” estimator in this context means a linear\n",
      "combination of X; that is, an estimator in the form aTX. It is clear that lT bβ\n",
      "is linear, and we have already seen that it is unbiased for lTβ. “Best” in this\n",
      "context means that its variance is no greater than any other estimator that\n",
      "ﬁts the requirements.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "428\n",
      "5 Unbiased Point Estimation\n",
      "Theorem 5.8 (Gauss-Markov theorem)\n",
      "Let Y = Xβ + ϵ where E(ϵ) = 0 and V(ϵ) = σ2I, and assume lTβ is linearly\n",
      "estimable. Let bβ = (XTX)+XTY . Then lT bβ is the a.s. unique BLUE of lTβ.\n",
      "Proof.\n",
      "Let aTY be any unbiased estimator of lTβ, and write l = XTY ˜t as in equa-\n",
      "tion (5.80) above. Because aTY is unbiased for any β, as we saw above, it\n",
      "must be the case that aTX = lT. Recalling that XTX bβ = XTY , we have\n",
      "V(aTY ) = V(aTY −lT bβ + lT bβ)\n",
      "= V(aTY −˜tTXTY + lT bβ)\n",
      "= V(aTY −˜tTXTY ) + V(lT bβ) + 2Cov(aTY −˜tTXTY, ˜tTXTY ).\n",
      "Now, under the assumptions on the variance-covariance matrix of ϵ, which is\n",
      "also the (conditional, given X) variance-covariance matrix of Y , we have\n",
      "Cov(aTY −˜tTXTY, lT bβ) = (aT −˜tTXT)σ2IX˜t\n",
      "= (aTX −˜tTXTX)σ2I˜t\n",
      "= (lT −lT)σ2I˜t\n",
      "= 0;\n",
      "that is,\n",
      "V(aTY ) = V(aTY −˜tTXTY ) + V(lT bβ).\n",
      "This implies that\n",
      "V(aTY ) ≥V(lT bβ);\n",
      "that is, lT bβ has minimum variance among the linear unbiased estimators of\n",
      "lTβ.\n",
      "To see that it is unique, we consider the case in which V(aTY ) = V(lT bβ);\n",
      "that is, V(aTY −˜tTXTY ) = 0. For this variance to equal 0, it must be the\n",
      "case that aT −˜tTXT = 0 or aTY = ˜tTXTY = lT bβ a.s.; that is, lT bβ is the\n",
      "a.s. unique linear unbiased estimator that achieves the minimum variance.\n",
      "If we assume further that ϵ ∼Nn(0, σ2I), we see that lT bβ is the uniformly\n",
      "minimum variance unbiased estimator (UMVUE) for lTβ. This is because\n",
      "(XTY, (Y −X bβ)T(Y −X bβ)) is complete and suﬃcient for (β, σ2). This line\n",
      "of reasoning also implies that (Y −X bβ)T(Y −X bβ)/(n−r), where r = rank(X),\n",
      "is UMVUE for σ2.\n",
      "****** biased estimator with smaller MSE\n",
      "Example 5.27 Inadmissibility of the LSE in the Linear Model\n",
      "inadmissible under squared-error loss regularization; see page 252\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "429\n",
      "Optimal Properties of the Moore-Penrose Inverse\n",
      "The solution corresponding to the Moore-Penrose inverse is unique because\n",
      "that generalized inverse is unique. That solution is interesting for another\n",
      "reason.\n",
      "Theorem 5.9\n",
      "Let b∗be any solution to the normal equations (5.72), that is,\n",
      "b∗= (XTX)−XTY,\n",
      "and let\n",
      "bβ = (XTX)+XTY\n",
      "then\n",
      "∥bβ∥2 ≤∥b∗∥2.\n",
      "Proof.\n",
      "To see that this solution has minimum norm, ﬁrst factor Z, as\n",
      "X = QRU T,\n",
      "and form the Moore-Penrose inverse as\n",
      "X+ = U\n",
      "\u0014\n",
      "R−1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "\u0015\n",
      "QT.\n",
      "Now let\n",
      "bβ = X+Y.\n",
      "This is a least squares solution (that is, we have chosen a speciﬁc least squares\n",
      "solution).\n",
      "Now, let\n",
      "QTY =\n",
      "\u0012 c1\n",
      "c2\n",
      "\u0013\n",
      ",\n",
      "where c1 has exactly r elements and c2 has n −r elements, and let\n",
      "U Tb =\n",
      "\u0012 t1\n",
      "t2\n",
      "\u0013\n",
      ",\n",
      "where b is the variable in the norm ∥Y −Xb∥2 that we seek to minimize, and\n",
      "where t1 has r elements.\n",
      "Because multiplication by an orthogonal matrix does not change the norm,\n",
      "we have\n",
      "∥Y −Xb∥2 = ∥QT(Y −XUU Tb)∥2\n",
      "=\n",
      "\f\f\f\f\n",
      "\f\f\f\f\n",
      "\u0012 c1\n",
      "c2\n",
      "\u0013\n",
      "−\n",
      "\u0014R1 0\n",
      "0 0\n",
      "\u0015 \u0012t1\n",
      "t2\n",
      "\u0013\f\f\f\f\n",
      "\f\f\f\f\n",
      "2\n",
      "=\n",
      "\f\f\f\f\n",
      "\f\f\f\f\n",
      "\u0012 c1 −R1t1\n",
      "c2\n",
      "\u0013\f\f\f\f\n",
      "\f\f\f\f\n",
      "2\n",
      ".\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "430\n",
      "5 Unbiased Point Estimation\n",
      "The residual norm is minimized for t1 = R−1\n",
      "1 c1 and t2 arbitrary. However, if\n",
      "t2 = 0, then ∥t∥2 is also minimized. Because U Tb = t and U is orthogonal,\n",
      "∥b∥2 = ∥t∥2 = ∥t1∥2 + ∥t2∥2, and so with t2 = 0, that is, with b = bβ, ∥bβ∥2 is\n",
      "the minimum among the norms of all least squares solutions, ∥b∗∥2.\n",
      "Quadratic Forms\n",
      "Quadratic forms in nonnegative deﬁnite or positive deﬁnite matrices arise\n",
      "often in statistical applications, especially in the analysis of linear models. The\n",
      "analysis often involves the decomposition of a quadratic form in the positive\n",
      "deﬁnite matrix A, yTAy, into a sum, yTA1y + yTA2y, where A1 + A2 = A\n",
      "and A1 and A2 are nonnegative deﬁnite matrices.\n",
      "Cochran’s Theorems\n",
      "There are various facts that are sometimes called Cochran’s theorem. The\n",
      "simplest one concerns k symmetric idempotent n × n matrices, A1, . . ., Ak\n",
      "that sum to the identity matrix.\n",
      "Theorem 5.10 (Cochran’s theorem I)\n",
      "Let A1, . . ., Ak be symmetric idempotent n × n matrices such that\n",
      "In = A1 + · · · + Ak.\n",
      "Then\n",
      "AiAj = 0 for all i ̸= j.\n",
      "Proof.\n",
      "For an arbitrary j, for some matrix V , we have\n",
      "V TAjV = diag(Ir, 0),\n",
      "where r = rank(Aj). Now\n",
      "In = V TInV\n",
      "=\n",
      "k\n",
      "X\n",
      "i=1\n",
      "V TAiV\n",
      "= diag(Ir, 0) +\n",
      "X\n",
      "i̸=j\n",
      "V TAiV,\n",
      "which implies\n",
      "X\n",
      "i̸=j\n",
      "V TAiV = diag(0, In−r).\n",
      "Now for each i, V TAiV is idempotent, and because the diagonal elements of\n",
      "a symmetric idempotent matrix are all nonnegative, and hence the equation\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "431\n",
      "implies implies that for each i ̸= j, the ﬁrst r diagonal elements are 0. Fur-\n",
      "thermore, since these diagonal elements are 0, all elements in the ﬁrst r rows\n",
      "and columns are 0. We have, therefore, for each i ̸= j,\n",
      "V TAiV = diag(0, Bi)\n",
      "for some (n −r) × (n −r) symmetric idempotent matrix Bi. Now, for any\n",
      "i ̸= j, consider AiAj and form V TAiAjV . We have\n",
      "V TAiAjV = (V TAiV )(V TAjV )\n",
      "= diag(0, Bi)diag(Ir, 0)\n",
      "= 0.\n",
      "Because V is nonsingular, this implies the desired conclusion; that is, that\n",
      "AiAj = 0 for any i ̸= j.\n",
      "We can now extend this result to an idempotent matrix in place of I; that\n",
      "is, for an idempotent matrix A with A = A1 + · · · + Ak.\n",
      "Theorem 5.11 (Cochran’s theorem II)\n",
      "Let A1, . . ., Ak be n × n symmetric matrices and let\n",
      "A = A1 + · · · + Ak.\n",
      "Then any two of the following conditions imply the third one:\n",
      "(a). A is idempotent.\n",
      "(b). Ai is idempotent for i = 1, . . ., k.\n",
      "(c). AiAj = 0 for all i ̸= j.\n",
      "(The theorem also applies to nonsymmetric matrices if condition (c) is aug-\n",
      "mented with the requirement that rank(A2\n",
      "i ) = rank(Ai) for all i. We will\n",
      "restrict our attention to symmetric matrices, however, because in most appli-\n",
      "cations of these results, the matrices are symmetric.)\n",
      "Proof.\n",
      "First, if we assume properties (a) and (b), we can show that property (c)\n",
      "follows for the special case A = I.\n",
      "Now, let us assume properties (b) and (c) and show that property (a)\n",
      "holds. With properties (b) and (c), we have\n",
      "AA = (A1 + · · · + Ak) (A1 + · · · + Ak)\n",
      "=\n",
      "k\n",
      "X\n",
      "i=1\n",
      "AiAi +\n",
      "X\n",
      "i̸=j\n",
      "k\n",
      "X\n",
      "j=1\n",
      "AiAj\n",
      "=\n",
      "k\n",
      "X\n",
      "i=1\n",
      "Ai\n",
      "= A.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "432\n",
      "5 Unbiased Point Estimation\n",
      "Hence, we have property (a); that is, A is idempotent.\n",
      "Finally, let us assume properties (a) and (c). Property (b) follows imme-\n",
      "diately from\n",
      "A2\n",
      "i = AiAi = AiA = AiAA = A2\n",
      "i A = A3\n",
      "i\n",
      "and the fact that Ap+1 = Ap =⇒A is idempotent.\n",
      "Theorem 5.12 (Cochran’s theorem IIa)\n",
      "Any two of the properties (a) through (c) also imply a fourth property:\n",
      "(d). rank(A) = rank(A1) + · · · + rank(Ak).\n",
      "Proof.\n",
      "We ﬁrst note that any two of properties (a) through (c) imply the third one,\n",
      "so we will just use properties (a) and (b). Property (a) gives\n",
      "rank(A) = tr(A) = tr(A1 + · · · + Ak) = tr(A1) + · · · + tr(Ak),\n",
      "and property (b) states that the latter expression is rank(A1)+· · ·+rank(Ak),\n",
      "thus yielding property (d).\n",
      "There is also a partial converse: properties (a) and (d) imply the other\n",
      "properties.\n",
      "One of the most important special cases of Cochran’s theorem is when\n",
      "A = I in the sum:\n",
      "In = A1 + · · · + Ak.\n",
      "The identity matrix is idempotent, so if rank(A1) + · · · + rank(Ak) = n, all\n",
      "the properties above hold. (See Gentle (2007), pages 283–285.)\n",
      "In applications of linear models, a quadratic form involving Y is often\n",
      "partitioned into a sum of quadratic forms. The most important statistical\n",
      "application of Cochran’s theorem is for the distribution of quadratic forms of\n",
      "normally distributed random vectors.\n",
      "Theorem 5.13 (Cochran’s theorem III)\n",
      "Assume that Y is distributed as Nd(µ, Id), and for i = 1, . . .k, let Ai be a d×d\n",
      "symmetric matrix with rank ri such that P\n",
      "i Ai = Id. This yields a partition\n",
      "of the total sum of squares Y TY into k components:\n",
      "Y TY = Y TA1Y + · · · + Y TAkY.\n",
      "Then the Y TAiY have independent noncentral chi-squared distributions χ2\n",
      "ri(δi)\n",
      "with δi = µTAiµ if and only if P\n",
      "i ri = d.\n",
      "Proof.\n",
      "This follows from the results above and the multivariate normal distribution.\n",
      "(See Gentle (2007), pages 324–325.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "433\n",
      "The “Sum of Squares” Quadratic Form\n",
      "In statistical analysis, we often compare the variability within various sub-\n",
      "samples with the overall variability of the full sample. This is the basic idea in\n",
      "the common method called analysis of variance (AOV). The variability within\n",
      "any sample is usually measured by the sum of squares of the elements in the\n",
      "sample from their overall mean, P(yi −¯y)2.\n",
      "This sum of squares can be expressed as a quadratic form in an idempotent\n",
      "matrix. We can develop this matrix by use of the expressions for recursive\n",
      "computation of the variance. The basic matrix is the Helmert matrix (see\n",
      "Gentle (2007), page 308):\n",
      "Hn =\n",
      "\n",
      "\n",
      "1/√n\n",
      "1/√n\n",
      "1/√n\n",
      "· · ·\n",
      "1/√n\n",
      "1/\n",
      "√\n",
      "2\n",
      "−1/\n",
      "√\n",
      "2\n",
      "0\n",
      "· · ·\n",
      "0\n",
      "1/\n",
      "√\n",
      "6\n",
      "1/\n",
      "√\n",
      "6\n",
      "−2/\n",
      "√\n",
      "6 · · ·\n",
      "0\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "1\n",
      "√\n",
      "n(n−1)\n",
      "1\n",
      "√\n",
      "n(n−1)\n",
      "1\n",
      "√\n",
      "n(n−1) · · · −\n",
      "(n−1)\n",
      "√\n",
      "n(n−1)\n",
      "\n",
      "\n",
      "(5.82)\n",
      "Note that the Helmert matrix is orthogonal:\n",
      "HT\n",
      "n Hn = HnHT\n",
      "n = In.\n",
      "The (n−1)×n matrix below the ﬁrst row of the Helmert matrix is of particular\n",
      "interest. Let\n",
      "Hn =\n",
      "\n",
      "\n",
      "1/√n 1T\n",
      "n\n",
      ".........\n",
      "Kn−1\n",
      "\n",
      ".\n",
      "(5.83)\n",
      "First note that the two partitions are orthogonal to each other:\n",
      "1/√n 1T\n",
      "nKn−1 = 0.\n",
      "(5.84)\n",
      "(This also follows from the orthogonality of Hn of course.)\n",
      "Now let\n",
      "A = KT\n",
      "n−1Kn−1,\n",
      "(5.85)\n",
      "that is,\n",
      "A =\n",
      "\n",
      "\n",
      "n−1\n",
      "n\n",
      "−1\n",
      "n\n",
      "· · ·\n",
      "−1\n",
      "n\n",
      "−1\n",
      "n\n",
      "n−1\n",
      "n\n",
      "· · ·\n",
      "−1\n",
      "n\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "−1\n",
      "n\n",
      "−1\n",
      "n\n",
      "· · ·\n",
      "n−1\n",
      "n\n",
      "\n",
      "\n",
      "(5.86)\n",
      "Note that, for a sample of size n, A is the matrix of the quadratic form\n",
      "that yields P(xi −¯x)2:\n",
      "yTAy =\n",
      "X\n",
      "(yi −¯y)2.\n",
      "(5.87)\n",
      "We can form similar matrices for subsamples so as to decompose a sum of\n",
      "squares.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "434\n",
      "5 Unbiased Point Estimation\n",
      "Example 5.28 one-way ﬁxed-eﬀects AOV model\n",
      "Consider the linear model\n",
      "Yij = µ + αi + ϵij,\n",
      "i = 1, . . ., m;\n",
      "j = 1, . . ., n,\n",
      "(5.88)\n",
      "where we assume that E(ϵij) = 0 and V(ϵij) = σ2 for all i, j, and Cov(ϵij, ϵi′j′) =\n",
      "0 if i ̸= i′ or j ̸= j′. This can be expressed in the form of the linear\n",
      "model (5.66), Y = Xβ + E, where β = (µ, α1, . . ., αm) and\n",
      "X =\n",
      "\n",
      "\n",
      "1 1 0 · · · 0\n",
      "... ... ...\n",
      "...\n",
      "...\n",
      "1 1 0 · · · 0\n",
      "1 0 1 · · · 0\n",
      "... ... ...\n",
      "...\n",
      "...\n",
      "1 0 1 · · · 0\n",
      "... ... ...\n",
      "...\n",
      "...\n",
      "1 0 0 · · · 1\n",
      "... ... ...\n",
      "...\n",
      "...\n",
      "1 0 0 · · · 1\n",
      "\n",
      "\n",
      "(5.89)\n",
      "Letting\n",
      "Y i =\n",
      "n\n",
      "X\n",
      "j=1\n",
      "Yij/n\n",
      "(5.90)\n",
      "and\n",
      "Y =\n",
      "m\n",
      "X\n",
      "i=1\n",
      "Y i/m,\n",
      "(5.91)\n",
      "we may form two sums of squares\n",
      "SSA = n\n",
      "m\n",
      "X\n",
      "i=1\n",
      "(Y i −Y )2\n",
      "(5.92)\n",
      "and\n",
      "SSE =\n",
      "m\n",
      "X\n",
      "i=1\n",
      "n\n",
      "X\n",
      "j=1\n",
      "(Yij −Y i)2,\n",
      "(5.93)\n",
      "which have the property that\n",
      "m\n",
      "X\n",
      "i=1\n",
      "n\n",
      "X\n",
      "j=1\n",
      "(Yij −Y )2 = SSA + SSE.\n",
      "(5.94)\n",
      "Both SSA and SSE can be expressed as quadratic forms in matrices similar\n",
      "to KT\n",
      "n−1Kn−1, where Kn−1 is given in equation (5.83). This is what you are\n",
      "asked to do in Exercise 5.7.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "435\n",
      "The question of interest in a model such is this is whether the αi are\n",
      "diﬀerent from one another; that is, whether or not it is meaningful to group\n",
      "the Yij based on the i.\n",
      "Example 5.29 estimating the eﬀects in a one-way ﬁxed-eﬀects AOV\n",
      "model (continuation of Example 5.28)\n",
      "The individual αi are not U-estimable. We can see this because in this case,\n",
      "l = (0, . . ., 1, . . .) and so l is not in the row space of X in (5.89). (This argument\n",
      "follows from the condition in equation (5.79).) We see that l = (1, . . ., 1, . . .)\n",
      "and so l is not in the row space of X and so µ + αi is estimable, and its\n",
      "UMVUE is Y i. Also, αi −αj for i ̸= j is estimable because it corresponds to\n",
      "an l with ﬁrst element 0, and all other elements 0 except for two, one of which\n",
      "is 1 the other is −1. Such vectors are called contrasts.\n",
      "For any linear combination of β = (µ, α1, . . ., αm) that is estimable, say\n",
      "lTβ, we see that the a.s. unique UMVUE is lT bβ, where bβ = (XTX)+XTY\n",
      "(equation (5.74)).\n",
      "Although the form of the AOV model (5.88) is the one that is commonly\n",
      "used, we see that a closely related model could be formed by restricting this\n",
      "model so that P\n",
      "i αi = 0. This related model is Yij = θi + ϵij. The θi in this\n",
      "restricted model are U-estimable.\n",
      "Notice that so far we have not assumed any speciﬁc family of distributions\n",
      "for the AOV model. We have unique UMVUEs. To answer the question posed\n",
      "above of whether the αi are actually diﬀerent from one another, however,\n",
      "we need a basis for a statistical test. We might attempt some kind of non-\n",
      "parametric test based on rankings, but in the next example, we will make the\n",
      "common assumption that the random components have a normal distribution.\n",
      "Note that the previous assumption of 0 covariances gives independence if we\n",
      "assume normality. Cochran’s theorem tells us what the distributions are.\n",
      "Example 5.30 distribution of the sums of squares in a one-way ﬁxed-\n",
      "eﬀects AOV model (continuation of Example 5.28)\n",
      "If we assume that ϵij ∼N(0, 1), we know the distributions of functions of SSA\n",
      "and SSE, and on that basis we can assess the signiﬁcance of the αi. We have\n",
      "1\n",
      "σ2\n",
      "\n",
      "SSA −\n",
      "n\n",
      "m −1\n",
      "m\n",
      "X\n",
      "i=1\n",
      " \n",
      "αi −\n",
      "m\n",
      "X\n",
      "i=1\n",
      "αi/m\n",
      "!2\n",
      "∼χ2\n",
      "m−1\n",
      "(5.95)\n",
      "and\n",
      "1\n",
      "σ2 SSE ∼χ2\n",
      "m(n−1).\n",
      "(5.96)\n",
      "(Exercise 5.8.)\n",
      "The UMVUE of σ2 is SSE/(m(n −1)). Note that the UMVUE of σ2 is\n",
      "the same as the general result given in equation (5.77). (Exercise 5.9.) The\n",
      "UMVUE is consistent in n for m ﬁxed, and is consistent in m for n ﬁxed.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "436\n",
      "5 Unbiased Point Estimation\n",
      "You are to show this in Exercise 5.10. Compare this with the MLE of σ2 in\n",
      "Example 6.27 in Chapter 6.\n",
      "The model in equation (5.88) is called the one-way AOV model. If the αi\n",
      "in this model are assumed to be constants, it is called a “ﬁxed-eﬀects model”.\n",
      "A ﬁxed-eﬀects model is also sometimes called “model I”. Now let’s consider a\n",
      "variant of this called a “random-eﬀects model” or “model II”, because the αi\n",
      "in this model are assumed to be iid random variables.\n",
      "Example 5.31 UMVUEs of the variances in the one-way random-\n",
      "eﬀects AOV model\n",
      "Consider the linear model\n",
      "Yij = µ + δi + ϵij,\n",
      "i = 1, . . ., m;\n",
      "j = 1, . . ., n,\n",
      "(5.97)\n",
      "where the δi are identically distributed with E(δi) = 0, V(δi) = σ2\n",
      "δ, and\n",
      "Cov(δi, δ˜i) = 0 for i ̸= ˜i, and the ϵij are independent of the δi and are\n",
      "identically distributed with with E(ϵij) = 0, V(ϵij) = σ2\n",
      "ϵ, and Cov(ϵij, ϵ˜i˜j) = 0\n",
      "for either i ̸= ˜i or j ̸= ˜j.\n",
      "An important diﬀerence in the random-eﬀects model and the ﬁxed-eﬀects\n",
      "model is that in the random-eﬀects model, we do not have independence of\n",
      "the observables. We have\n",
      "Cov(Yij, Y˜i˜j) =\n",
      "\n",
      "\n",
      "\n",
      "σ2\n",
      "δ + σ2\n",
      "ϵ for i = ˜i, j = ˜j,\n",
      "σ2\n",
      "δ\n",
      "for i = ˜i, j ̸= ˜j,\n",
      "0\n",
      "for i ̸= ˜i.\n",
      "(5.98)\n",
      "A model such as this may be appropriate when there are a large number\n",
      "of possible treatments and m of them are chosen randomly and applied to\n",
      "experimental units whose responses Yij are observed. While in the ﬁxed-eﬀects\n",
      "model (5.88), we are interested in whether α1 = · · · = αm = 0, in the random-\n",
      "eﬀects model, we are interested in whether σ2\n",
      "δ = 0, which would result in a\n",
      "similar practical decision about the treatments.\n",
      "In the model (5.97) the variance of each Yij is σ2\n",
      "δ + σ2\n",
      "ϵ, and our interest in\n",
      "using the model is to make inference on the relative sizes of the components of\n",
      "the variance σ2\n",
      "δ and σ2\n",
      "ϵ. The model is sometimes called a “variance components\n",
      "model”.\n",
      "Let us suppose now that δi\n",
      "iid\n",
      "∼N(0, σ2\n",
      "δ), where σ2\n",
      "δ ≥0, and ϵij\n",
      "iid\n",
      "∼N(0, σ2\n",
      "ϵ),\n",
      "where as usual σ2 > 0. This will allow us to determine exact sampling distri-\n",
      "butions of the relevant statistics.\n",
      "We transform the model using Helmert matrices Hm and Hn as in equa-\n",
      "tion (5.82).\n",
      "Let\n",
      "Y =\n",
      "\n",
      "\n",
      "Y11 · · · Y1n\n",
      "...\n",
      "...\n",
      "Ym1 · · · Ymn\n",
      "\n",
      "; δ =\n",
      "\n",
      "\n",
      "δ1\n",
      "...\n",
      "δm\n",
      "\n",
      "; and ϵ =\n",
      "\n",
      "\n",
      "ϵ11 · · · ϵ1n\n",
      "...\n",
      "...\n",
      "ϵm1 · · · ϵmn\n",
      "\n",
      ".\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "437\n",
      "We now write the original model as\n",
      "Y = δ1T\n",
      "n + ϵ.\n",
      "Now, for the transformations. Let\n",
      "Z = HmXHT\n",
      "n ,\n",
      "eδ = Hmδ,\n",
      "and\n",
      "eϵ = HmϵHT\n",
      "n .\n",
      "We ﬁrst of all note that the transformations are all nonsingular and\n",
      "Z = H1T\n",
      "n + eϵ.\n",
      "Next, we see because of the orthonormality of the Helmert matrices that\n",
      "the distributions of eδ and eϵ are the same as those of δ and ϵ and they are still\n",
      "independent. Furthermore, the Zij are independent, and we have\n",
      "Zi1\n",
      "iid\n",
      "∼N(0, σ2\n",
      "a + σ2),\n",
      "for i = 1, . . ., m\n",
      "and\n",
      "Zij\n",
      "iid\n",
      "∼N(0, σ2),\n",
      "for i = 1, . . ., m; j = 2, . . ., n0.\n",
      "To continue with the analysis, we follow the same steps as in Example 5.28,\n",
      "and get the same decomposition of the “adjusted total sum of squares” as in\n",
      "equation (5.94):\n",
      "m\n",
      "X\n",
      "i=1\n",
      "n\n",
      "X\n",
      "j=1\n",
      "(Zij −Z)2 = SSA + SSE.\n",
      "(5.99)\n",
      "Again, we get chi-squared distributions, but the distribution involving SSA\n",
      "is not the same as in expression (5.95) for the ﬁxed-eﬀects model.\n",
      "Forming\n",
      "MSA = SSA/(m −1)\n",
      "and\n",
      "MSE = SSE/(m(n −1)),\n",
      "we see that\n",
      "E(MSA) = nσ2\n",
      "δ + σ2\n",
      "ϵ\n",
      "and\n",
      "E(MSE) = σ2\n",
      "ϵ.\n",
      "Unbiased estimators of σ2\n",
      "δ and σ2\n",
      "ϵ are therefore\n",
      "s2\n",
      "δ = (MSA −MSE)/n\n",
      "(5.100)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "438\n",
      "5 Unbiased Point Estimation\n",
      "and\n",
      "s2\n",
      "ϵ = MSE,\n",
      "(5.101)\n",
      "and we can also see that these are UMVUEs.\n",
      "Now we note something that might at ﬁrst glance be surprising: s2\n",
      "δ in\n",
      "equation (5.100) may be negative. This occurs if (m −1)MSA/m < MSE.\n",
      "This will be the case if the variation among Yij for a ﬁxed i is relatively large\n",
      "compared to the variation among Y i (or similarly, if the variation among Zij\n",
      "for a ﬁxed i is relatively large compared to the variation among Zi).\n",
      "Compare this with the MLEs in Example 6.29 in Chapter 6\n",
      "Predictions in the Linear Model\n",
      "Given a vector x0, use of bβ in equation (5.64), with E set to E(E), we have\n",
      "the predicted value of Y given x0:\n",
      "bY0 = bβTx0\n",
      "= ((XTX)+XTy)Tx0.\n",
      "(5.102)\n",
      "If x0 ∈span(X), then from Theorem 5.7, (b∗)Tx0 = bβTx0, so in this case the\n",
      "predicted value of Y is invariant to choice of the generalized inverse.\n",
      "In the model (5.66) corresponding to a set of n observations on the\n",
      "model (5.64), we have predicted values of the response Y at all rows within\n",
      "X:\n",
      "bY = X bβ\n",
      "= X(XTX)+XTY.\n",
      "(5.103)\n",
      "From equation (3.42), we see that this has the minimum MSE of any function\n",
      "of X.\n",
      "The idempotent projection matrix X(XTX)+XT is called the “hat ma-\n",
      "trix” because given Y , it provides bY . (See page 795 for properties of projection\n",
      "matrices.)\n",
      "We see from Deﬁnition 1.46 page 116 that bY is the projection of Y onto\n",
      "the column space of X. (This is a slightly diﬀerent meaning of the word\n",
      "“projection”, but obviously the meanings are related.) From Theorem 1.64\n",
      "we see that the “residual vector” Y −bY is orthogonal to the columns of X;\n",
      "that is, Cov(Y −bY , x) = 0 for any column x of X, and since bY is a linear\n",
      "combination of the columns of X, Cov(Y −bY , bY ) = 0. If we assume a normal\n",
      "distribution for ϵ, then 0 covariance implies independence.\n",
      "5.5.2 Estimation in Survey Samples of Finite Populations\n",
      "A substantial proportion of all applications of statistics deal with sample sur-\n",
      "veys in ﬁnite populations. Some aspects of this kind of application distinguish\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "439\n",
      "it from other areas of applied statistics. S¨arndal et al. (1997) provide a general\n",
      "coverage of the theory and methods. Valliant et al. (2000) provide a diﬀerent\n",
      "perspective on some of the particular issues of inference in ﬁnite populations.\n",
      "Finite Populations\n",
      "We think of a ﬁnite population as being a ﬁnite set P = {(1, y1), . . ., (N, yN)}.\n",
      "Our interest will be in making inferences about the population using a sample\n",
      "S = {(L1, X1), . . ., (Ln, Xn)}. We will also refer to X = {X1, . . ., Xn} as\n",
      "the “sample”. In discussions of sampling it is common to use n to denote\n",
      "the size of the sample and N to denote the size of the population. Another\n",
      "common notation used in sampling is Y to denote the population total, Y =\n",
      "PN\n",
      "i=1 yi. Estimation of the total is one of the most basic objectives in sampling\n",
      "applications.\n",
      "The parameter that characterizes the population is θ = (y1, . . ., yN). The\n",
      "parameter space, Θ, is the subspace of IRN containing all possible values of\n",
      "the yi.\n",
      "There are two approaches to the analysis of the problem. In one, which\n",
      "is the more common and which we will follow, P is essentially the sample\n",
      "space. In another approach P or θ is thought of as some random sample from\n",
      "a sample space or parameter space, called a “superpopulation”.\n",
      "The sample is completely determined by the set LS = {i1, . . ., in} of in-\n",
      "dexes of P that correspond to elements in X. For analysis of sampling meth-\n",
      "ods, we deﬁne an indicator\n",
      "Ii =\n",
      "\u001a 1 if i ∈LS\n",
      "0 othersise.\n",
      "“Sampling” can be thought of as selecting the elements of LS, that is, the\n",
      "labels of the population elements.\n",
      "Probability-based inferences about P are determined by the method of\n",
      "selection of S. This determines the probability of getting any particular S,\n",
      "which we will denote by p(S). If p(S) is constant for all S, we call the selected\n",
      "sample a simple random sample.\n",
      "A sample may be collected without replacement or with replacement. (The\n",
      "meanings of these are just what the words mean. In sampling without replace-\n",
      "ment, the elements of S are distinct.) Sampling with replacement is generally\n",
      "easier to analyze, because it is the same as taking a random sample from a\n",
      "discrete uniform distribution. Sampling without replacement is more common\n",
      "and it is what we will assume throughout.\n",
      "There are many variations on the method of collecting a sample. Both a\n",
      "general knowledge of the population and some consideration of the mechani-\n",
      "cal aspects of collecting the sample may lead to the use of stratiﬁed sampling,\n",
      "cluster sampling, multi-stage sampling, systematic sampling, or other varia-\n",
      "tions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "440\n",
      "5 Unbiased Point Estimation\n",
      "Estimation\n",
      "We are interested in “good” estimators, speciﬁcally UMVUEs, of estimable\n",
      "functions of θ. An interesting estimable function of θ is Y = PN\n",
      "i=1 θi.\n",
      "One of the most important results is the following theorem.\n",
      "Theorem 5.14\n",
      "(i) if p(S) > 0 for all S, then the set of order statistics X(1), . . ., X(n) is\n",
      "complete for all θ ∈Θ.\n",
      "and\n",
      "(ii) if p(S) is constant for all S, then the order statistics X(1), . . ., X(n) are\n",
      "suﬃcient for all θ ∈Θ.\n",
      "This theorem is somewhat similar to Corollary 3.1.1, which applied to the\n",
      "family of distributions dominated by Lebesgue measure. The suﬃciency is\n",
      "generally straightforward, and we expect it to hold in any iid case.\n",
      "The completeness is a little more complicated, and the proof of Theorem\n",
      "3.13 in MS2 is worth looking at. The set of order statistics may be complete\n",
      "in some family, such as the family of distributions dominated by Lebesgue\n",
      "measure, but may not be complete in some subfamily, such as the family of\n",
      "normal distributions with mean 0.\n",
      "After we have (i) and (ii), we have\n",
      "(iii): For any estimable function of θ, its unique UMVUE is the unbiased\n",
      "estimator T(X1, . . ., Xn) that is symmetric in its arguments. (The symmetry\n",
      "makes the connection to the order statistics.)\n",
      "Example 5.32 UMVUE of population total using simple random\n",
      "sample\n",
      "Consider estimation of Y = g(θ) = PN\n",
      "i=1 yi from the simple random sample\n",
      "X1, . . ., Xn. We ﬁrst note that\n",
      "bY = N\n",
      "n\n",
      "X\n",
      "i∈LS\n",
      "yi\n",
      "= N\n",
      "n\n",
      "N\n",
      "X\n",
      "i=1\n",
      "Iiyi\n",
      "is unbiased for Y :\n",
      "E(bY ) = N\n",
      "n\n",
      "N\n",
      "X\n",
      "i=1\n",
      "yiE(Ii)\n",
      "=\n",
      "N\n",
      "X\n",
      "i=1\n",
      "yi.\n",
      "From Theorem 5.14, we can see easily that bY = Ny is the UMVUE of Y .\n",
      "Now we consider the variance of bY . First, note that\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "5.5 Applications\n",
      "441\n",
      "V(Ii) = n\n",
      "N\n",
      "\u0010\n",
      "1 −n\n",
      "N\n",
      "\u0011\n",
      "(it’s Bernoulli), and for i ̸= j,\n",
      "Cov(Ii, Ij) = E(IiIj) −E(Ii)E(Ij)\n",
      "= n(n −1)\n",
      "N(N −1) −n2\n",
      "N 2 .\n",
      "Hence,\n",
      "V(bY ) = N 2\n",
      "n2 V\n",
      " N\n",
      "X\n",
      "i=1\n",
      "Iiyi\n",
      "!\n",
      "= N 2\n",
      "n2\n",
      "\n",
      "\n",
      "N\n",
      "X\n",
      "i=1\n",
      "y2\n",
      "i V(Ii) + 2\n",
      "X\n",
      "1≤i≤j≤N\n",
      "yiyjCov(Ii, Ij)\n",
      "\n",
      "\n",
      "= N\n",
      "n\n",
      "\u0010\n",
      "1 −n\n",
      "N\n",
      "\u0011\n",
      "\n",
      "\n",
      "N\n",
      "X\n",
      "i=1\n",
      "y2\n",
      "i −\n",
      "2\n",
      "N −1\n",
      "X\n",
      "1≤i≤j≤N\n",
      "yiyj\n",
      "\n",
      "\n",
      "= N 2\n",
      "n\n",
      "\u0010\n",
      "1 −n\n",
      "N\n",
      "\u0011\n",
      "1\n",
      "N −1\n",
      "N\n",
      "X\n",
      "i=1\n",
      "\u0012\n",
      "yi −Y\n",
      "N\n",
      "\u00132\n",
      ".\n",
      "(5.104)\n",
      "We see that the variance of bY is composed of three factors, an expansion\n",
      "factor N 2/n, a ﬁnite population correction factor (1−n/N), and the variance\n",
      "of a selection from a ﬁnite population,\n",
      "σ2 =\n",
      "1\n",
      "N −1\n",
      "N\n",
      "X\n",
      "i=1\n",
      "\u0012\n",
      "yi −Y\n",
      "N\n",
      "\u00132\n",
      ".\n",
      "(5.105)\n",
      "The sample variance S2 is unbiased for σ2, and so from this we have\n",
      "immediately the UMVUE of V(bY ) (Exercise 5.11).\n",
      "Horvitz-Thompson Estimation\n",
      "The properties of any statistic derived from a sample X1, . . ., Xn depend on\n",
      "the sampling design; that is, on how the items in the sample were selected. The\n",
      "two main properties of the design are the probability that a speciﬁc population\n",
      "item, say yi, is selected, and the probability that two speciﬁc population items,\n",
      "say yi and yj are both selected. Probabilities of combinations of larger sets\n",
      "may also be of interest, but we can work out simple expectations and variances\n",
      "just based on these two kinds of probabilities.\n",
      "Let πi be the probability that yi is included in the sample, and let πij be\n",
      "the probability that both yi and yj are included.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "442\n",
      "5 Unbiased Point Estimation\n",
      "If πi > 0 for all i, the Horvitz-Thompson estimator of the population total\n",
      "is\n",
      "bYHT =\n",
      "X\n",
      "i∈LS\n",
      "yi\n",
      "πi\n",
      ".\n",
      "(5.106)\n",
      "It is easy to see that bYHT is unbiased for Y :\n",
      "E\n",
      "\u0010\n",
      "bYHT\n",
      "\u0011\n",
      "= E\n",
      " X\n",
      "i∈LS\n",
      "yi\n",
      "πi\n",
      "!\n",
      "=\n",
      "N\n",
      "X\n",
      "i=1\n",
      "\u0012 yi\n",
      "πi\n",
      "πi\n",
      "\u0013\n",
      "=\n",
      "N\n",
      "X\n",
      "i=1\n",
      "yi.\n",
      "The variance of the Horvitz-Thompson estimator depends on the πij as\n",
      "well as the πi:\n",
      "V\n",
      "\u0010\n",
      "bYHT\n",
      "\u0011\n",
      "=\n",
      "N\n",
      "X\n",
      "i=1\n",
      "N\n",
      "X\n",
      "j=i+1\n",
      "(πiπj −πij)\n",
      "\u0012 yi\n",
      "πi\n",
      "−yj\n",
      "πj\n",
      "\u00132\n",
      "(5.107)\n",
      "(Exercise 5.12). Expressions for other sampling estimators are often shown in\n",
      "a similar manner.\n",
      "An important approximation for working out variances of more compli-\n",
      "cated sampling estimators is linearization, especially when the estimator in-\n",
      "volves a ratio.\n",
      "Notes and Further Reading\n",
      "Most of the material in this chapter is covered in MS2 Chapter 3 and Section\n",
      "4.5, and in TPE2 Chapter 2.\n",
      "Unbiasedness\n",
      "The property of unbiasedness for point estimators was given a solid but pre-\n",
      "liminary treatment by Halmos (1946).\n",
      "Unbiasedness has a heuristic appeal, although people will sometimes ques-\n",
      "tion its relevance by pointing out that it provides no guarantee of the goodness\n",
      "of an estimator in a single set of data. That argument, however, could apply to\n",
      "most measures of the quality of an estimator. Similar types of arguments could\n",
      "bring into question any consideration of asymptotic properties of statistical\n",
      "procedures.\n",
      "Unbiasedness is particularly useful when the loss is squared-error, because\n",
      "in that case unbiasedness may lead to uniformly minimum risk estimators. For\n",
      "absolute-error loss functions, a corresponding approach would be to require\n",
      "median unbiasedness.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "443\n",
      "Fisher Eﬃcient Estimators and Exponential Families\n",
      "Fisher eﬃcient estimators occur only in exponential families, and there is\n",
      "always one in an exponential family. This fact had been know for some time,\n",
      "but the ﬁrst rigorous proof was given by Wijsman (1973).\n",
      "U-Statistics\n",
      "The fundamental paper by Hoeﬀding (1948) considered the asymptotic nor-\n",
      "mality of certain unbiased point estimators and introduced the class of es-\n",
      "timators that he named U-statistics. Serﬂing (1980) provides an extensive\n",
      "discussion of U-statistics, as well as V-statistics. The statement and proof\n",
      "Theorem 5.3 and the use of the conditional kernels hk as in equation (5.48)\n",
      "follow Serﬂing. Kowalski and Tu (2008) consider several applications of U-\n",
      "statistics in a variety of settings.\n",
      "Exercises\n",
      "5.1. Show that the estimator (5.3) in Example 5.1 is the UMVUE of π. (Note\n",
      "that there are three things to show: (t −1)/(N −1) is unbiased, it has\n",
      "minimum variance among all unbiased estimators, and it is unique —\n",
      "“the” implies uniqueness.)\n",
      "5.2. Consider the problem of using a sample of size 1 for estimating g(θ) = e−3θ\n",
      "where θ is the parameter in a Poisson distribution.\n",
      "a) Show that T(X) = (−2)X is unbiased for g(θ).\n",
      "b) Show that T(X) = (−2)X is a UMVUE g(θ).\n",
      "c) What is wrong with this estimator?\n",
      "5.3. Show that the estimators (5.11) and (5.7) are the same.\n",
      "5.4. Show that the h(T)s in Example 5.6 are unbiased for the g(θ)s given.\n",
      "5.5. Deﬁne an alternative kernel for U-statistic that is unbiased for the covari-\n",
      "ance in Example 5.19; that is, instead of the kernel in equation (5.37),\n",
      "give a kernel similar to that in equation (5.46). Show that the resulting\n",
      "U-statistic is unbiased for the covariance.\n",
      "5.6. In the setup of model (5.64), show that the LSE ∥Y −X bβ∥2/(n −p) is\n",
      "unbiased for σ2.\n",
      "5.7. Let Xij = µ + αi + ϵij,\n",
      "i = 1, . . ., m,\n",
      "j = 1, . . ., n, where αi’s and ϵij’s\n",
      "are independent random variables, αi ∼N(0, σ2\n",
      "α), ϵij ∼N(0, σ2\n",
      "ϵ), and µ,\n",
      "σ2\n",
      "α, and σ2\n",
      "ϵ are unknown parameters. Let\n",
      "Xi =\n",
      "n\n",
      "X\n",
      "j=1\n",
      "Xij/n,\n",
      "X =\n",
      "m\n",
      "X\n",
      "i=1\n",
      "Xi/m,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "444\n",
      "5 Unbiased Point Estimation\n",
      "MSA = n\n",
      "m\n",
      "X\n",
      "i=1\n",
      "(Xi −X)2/(m −1),\n",
      "and\n",
      "MSE =\n",
      "m\n",
      "X\n",
      "i=1\n",
      "n\n",
      "X\n",
      "j=1\n",
      "(Xij −Xi)2/(m(n −1)).\n",
      "Express MSA and MSE as quadratic forms using parts of Helmert matrices\n",
      "and use Chochran’s theorem to show that they are independent.\n",
      "5.8. Show that the quantities in expressions (5.95) and (5.96) have the chi-\n",
      "squared distributions claimed.\n",
      "5.9. Show that the UMVUE of σ2, SSE/(m(n −1)), given in Example 5.28\n",
      "is the same as the UMVUE of σ2 for the general linear model given in\n",
      "equation (5.77).\n",
      "Hint: Write the model given in equation (5.88) in the form of the general\n",
      "linear model in equation (5.67).\n",
      "5.10. Suppose Xij\n",
      "iid\n",
      "∼N(µi, σ2) for i = 1, . . ., m and j = 1, . . ., n. (Compare the\n",
      "one-way AOV model of Examples 5.28, 5.29, and 5.30.)\n",
      "a) Determine the UMVUE Tmn(X) of σ2.\n",
      "b) Show that Tmn(X) is consistent in mean squared error for σ2 as m →\n",
      "∞and n remains ﬁxed.\n",
      "c) Show that Tmn(X) is consistent in mean squared error for σ2 as n →\n",
      "∞and m remains ﬁxed.\n",
      "5.11. Show that the sample variance S2 is the UMVUE of σ2 in equation (5.105)\n",
      "of Example 5.32. Hence, determine the UMVUE of V(bY ).\n",
      "5.12. Show that the variance of the Horvitz-Thompson estimator is as shown in\n",
      "equation (5.107), for given πi and πij. This is tedious, but it requires very\n",
      "little other than “advanced arithmetic” and simple properties of variances\n",
      "of sums.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6\n",
      "Statistical Inference Based on Likelihood\n",
      "The concepts of probability theory can be applied to statistical analyses in a\n",
      "very straightforward manner: we assume that observed events are governed\n",
      "by some data-generating process that depends on a probability distribution\n",
      "P , and our observations of those events can be used to make inferences about\n",
      "the probability distribution. The various ways that we use the observations\n",
      "to make those inferences constitute the main body of statistical theory. One\n",
      "of the general approaches that I outlined in Section 3.2 involves the use of a\n",
      "likelihood function. We considered this approach brieﬂy in Section 3.2.1. In\n",
      "this chapter, we will explore the use of likelihood in statistical inference more\n",
      "fully. In this chapter, the emphasis will be on estimation, and in Chapter 7,\n",
      "we will consider use of likelihood in testing statistical hypotheses.\n",
      "Although methods based on the likelihood may not have the logical ap-\n",
      "peal of methods based on a decision-theoretic approach, they do have an intu-\n",
      "itive appeal. More importantly, estimators and tests based on this approach\n",
      "have a number of desirable mathematical properties, especially asymptotic\n",
      "properties. Methods based on maximizing the likelihood are grounded on the\n",
      "likelihood principle.\n",
      "We begin with some general deﬁnitions and notation for methods based\n",
      "on the likelihood principle, and then look at speciﬁc applications.\n",
      "6.1 The Likelihood Function and Its Use in Statistical\n",
      "Inference\n",
      "Deﬁnition 6.1 (likelihood function)\n",
      "Given a sample x1, . . ., xn from distributions with probability densities pi(x)\n",
      "with respect to a common σ-ﬁnite measure, the likelihood function is deﬁned\n",
      "as\n",
      "Ln(pi ; x) = c\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "pi(xi),\n",
      "(6.1)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "446\n",
      "6 Statistical Inference Based on Likelihood\n",
      "where c ∈IR+ is any constant independent of the pi.\n",
      "It is common to speak of Ln(pi ; X) with c = 1 as “the” likelihood function,\n",
      "and in the following, we will not write the c.\n",
      "Methods based on the likelihood function are often chosen because of their\n",
      "asymptotic properties, and so it is common to use the n subscript as in equa-\n",
      "tion (6.1); in the following, however, we will usually ﬁnd it convenient to drop\n",
      "the n.\n",
      "As we generally do in discussing methods of statistical inference, in some\n",
      "cases, we will view the sample x1, . . ., xn as a set of constants. In cases when\n",
      "we want to consider the probabilistic or statistical properties of the statistical\n",
      "methods, we will view the observations as a vector of random variables.\n",
      "In equation (6.1), the domain of the likelihood function is some class of\n",
      "distributions speciﬁed by their probability densities, P = {pi(x)}, where all\n",
      "PDFs are dominated by a common σ-ﬁnite measure. In applications, often\n",
      "the PDFs are of a common parametric form, so equivalently, we can think of\n",
      "the domain of the likelihood function as being a parameter space, say Θ. In\n",
      "that case, the family of densities can be written as P = {pθ(x)} where θ ∈Θ,\n",
      "the known parameter space. It is usually more convenient to write pθ(x) as\n",
      "p(x ; θ), and we often write the likelihood function (6.1) as\n",
      "L(θ ; x) =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "p(xi ; θ).\n",
      "(6.2)\n",
      "Although in equation (6.2), we have written L(θ ; x), the expression\n",
      "L(pθ ; x) may be more appropriate because it reminds us of an essential in-\n",
      "gredient in the likelihood, namely a PDF.\n",
      "What Likelihood Is Not\n",
      "The diﬀerences in a likelihood and a PDF are illustrated clearly in Example 1.5\n",
      "on page 20. A likelihood is neither a probability nor a probability density.\n",
      "Notice, for example, that while the deﬁnite integrals over IR+ of both PDFs\n",
      "in in Example 1.5 are 1, the deﬁnite integrals over IR+ of the likelihood (1.21)\n",
      "in Example 1.5 are not the same, as we can easily see from the plots on the\n",
      "right side of Figure 1.2.\n",
      "It is not appropriate to refer to the “likelihood of an observation”. We use\n",
      "the term “likelihood” in the sense of the likelihood of a model or the likelihood\n",
      "of a distribution given observations.\n",
      "The Log-Likelihood Function\n",
      "The log-likelihood function,\n",
      "lL(θ ; x) = log L(θ ; x),\n",
      "(6.3)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.1 The Likelihood Function\n",
      "447\n",
      "is a sum rather than a product. We often denote the log-likelihood without\n",
      "the “L” subscript. The notation for the likelihood and the log-likelihood varies\n",
      "with authors. My own choice of an uppercase “L” for the likelihood and a\n",
      "lowercase “l” for the log-likelihood is long-standing, and not based on any\n",
      "notational optimality consideration. Because of the variation in the notation\n",
      "for the log-likelihood, I will often use the “lL” notation because this expression\n",
      "is suggestive of the meaning.\n",
      "We will often work with either the likelihood or the log-likelihood as if\n",
      "there is only one observation.\n",
      "Likelihood Principle\n",
      "According to the likelihood principle in statistical inference all of the informa-\n",
      "tion that the data provide concerning the relative merits of two hypotheses is\n",
      "contained in the likelihood ratio of those hypotheses and the data; that is, if\n",
      "for x and y,\n",
      "L(θ ; x)\n",
      "L(θ ; y) = c(x, y)\n",
      "∀θ,\n",
      "(6.4)\n",
      "where c(x, y) is constant for given x and y, then any inference about θ based\n",
      "on x should be in agreement with any inference about θ based on y.\n",
      "Although at ﬁrst glance, we may think that the likelihood principle is so\n",
      "obviously the right way to make decisions, Example 6.1 may cause us to think\n",
      "more critically about this principle.\n",
      "The likelihood principle asserts that for making inferences about a proba-\n",
      "bility distribution, the overall data-generating process need not be considered;\n",
      "only the observed data are relevant.\n",
      "Example 6.1 The likelihood principle in sampling from a Bernoulli\n",
      "distribution\n",
      "In Example 3.12 we considered the problem of making inferences on the pa-\n",
      "rameter π in a family of Bernoulli distributions.\n",
      "One approach was to take a random sample of size n, X1, . . ., Xn from the\n",
      "Bernoulli(π), and then use T = P Xi, which has a binomial distribution with\n",
      "parameters n and π.\n",
      "Another approach was to take a sequential sample, X1, X2, . . ., until a\n",
      "ﬁxed number t of 1’s have occurred. The size of the sample N is random and\n",
      "the random variable N has a negative binomial distribution with parameters\n",
      "t and π.\n",
      "Now, suppose we take the ﬁrst approach with n = n0 and we observe\n",
      "T = t0; and then we take the second approach with t = t0 and we observe\n",
      "N = n0. Using the PDFs in equations 3.43 and 3.44 we get the likelihoods\n",
      "LB(π) =\n",
      "\u0012n0\n",
      "t0\n",
      "\u0013\n",
      "πt0(1 −π)n0−t0\n",
      "(6.5)\n",
      "and\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "448\n",
      "6 Statistical Inference Based on Likelihood\n",
      "LNB(π) =\n",
      "\u0012n0 −1\n",
      "t0 −1\n",
      "\u0013\n",
      "πt0(1 −π)9.\n",
      "(6.6)\n",
      "Because LB(π)/LNB(π) does not involve π, the maxima of the likelihoods\n",
      "will occur at the same point. A maximum likelihood estimator of π based on\n",
      "a binomial observation of t0 out of n0 is the same as a maximum likelihood\n",
      "estimator of π based on a negative binomial observation of n0 for t0 1’s because\n",
      "the maximum of the likelihood occurs at the same place, t0/n0. The estimators\n",
      "conform to the likelihood principle. Recall that the UMVU estimators are\n",
      "diﬀerent. (Example 5.1 and follow-up in Example 5.5 and Exercise 5.1.)\n",
      "Further comments on Example 6.1\n",
      "We see that the likelihood principle allows the likelihood function to be de-\n",
      "ﬁned as any member of an equivalence class {cL : c ∈IR+}, as in the deﬁni-\n",
      "tion (3.45).\n",
      "The likelihood principle, however, is stronger than just the requirement\n",
      "that the estimator be invariant. It says that because LB(π)/LNB(π) does not\n",
      "involve π, any decision about π based on a binomial observation of 3 out of\n",
      "12 should be the same as any decision about π based on a negative binomial\n",
      "observation of 12 for 3 1’s. Because the variance of ˆπ does depend on whether a\n",
      "binomial distribution or a negative binomial distribution is assumed, the fact\n",
      "that the estimators are the same does not imply that the inference follows the\n",
      "likelihood principle. See Example 6.9.\n",
      "We will revisit this example again in Example 7.12 on page 539, where we\n",
      "wish to test a statistical hypothesis concerning π. We get diﬀerent conclusions\n",
      "in a signiﬁcance test.\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "Let us assume a parametric model; that is, a family of densities P = {p(x ; θ)}\n",
      "where θ ∈Θ, a known parameter space.\n",
      "For a sample X1, . . ., Xn from a distribution with probability density\n",
      "p(x ; θ), we write the likelihood function as a function of a variable in place\n",
      "of the parameter:\n",
      "L(t ; x) =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "p(xi ; t).\n",
      "(6.7)\n",
      "Note the reversal in roles of variables and parameters. While I really like to\n",
      "write the likelihood as a function of a variable of something other than the\n",
      "parameter, which I think of as ﬁxed, I usually write it like everyone else; that\n",
      "is, I write\n",
      "L(θ ; x) =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "p(xi ; θ).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "449\n",
      "In the likelihood function the data, that is, the realizations of the vari-\n",
      "ables in the density function, are considered as ﬁxed, and the parameters are\n",
      "considered as variables of the optimization problem,\n",
      "max\n",
      "θ\n",
      "L(θ ; x).\n",
      "(6.8)\n",
      "For given x, the relative values of L(θ ; x) are important. For given x1 and\n",
      "x2, the relative values of L(θ ; x1) and L(θ ; x2) are not relevant. Notice in\n",
      "Example 1.5, while L(θ ; 5) ≤L(θ ; 1) for all θ, max L(θ ; 5) occurs at θ = 5,\n",
      "and maxL(θ ; 1) occurs at θ = 1. Notice also in Example 6.1, while LB(π)\n",
      "in equation (6.5) is uniformly less than LNB(π) in equation (6.6), they both\n",
      "achieve their maximum at the same point, π = 1/4.\n",
      "Closure of the Parameter Space\n",
      "It is important to specify the domain of the likelihood function. If Θ is the\n",
      "domain of L in equation (6.7), we want to maximize L for t ∈Θ; that is,\n",
      "maximum likelihood often involves a constrained optimization problem.\n",
      "There may be diﬃculties with this maximization problem (6.8), however,\n",
      "because of open sets. The ﬁrst kind of problem is because the parameter space\n",
      "may be open. We address that problem in our deﬁnition the optimal estimator\n",
      "below. See Example 6.4. The second kind of open set may be the region over\n",
      "which the likelihood function is positive. This problem may arise because the\n",
      "support of the distribution is open and is dependent on the parameter to be\n",
      "estimated. We address that problem by adding a zero-probability set to the\n",
      "support (see Example 6.5 below).\n",
      "For certain properties of statistics that are derived from a likelihood ap-\n",
      "proach, it is necessary to consider the parameter space Θ to be closed (see,\n",
      "for example, Wald (1949)). Often in a given probability model, such as the\n",
      "exponential or the binomial, we do not assume Θ to be closed. If Θ is not\n",
      "a closed set, however, the maximum in (6.8) may not exist, so we consider\n",
      "the closure of Θ, Θ. (If Θ is closed Θ is the same set, so we can always just\n",
      "consider Θ.)\n",
      "6.2.1 Deﬁnition and Examples\n",
      "Deﬁnition 6.2 (maximum likelihood estimate; estimator)\n",
      "Let L(θ ; x) be the likelihood of θ ∈Θ for the observations x from a distri-\n",
      "bution with PDF with respect to a σ-ﬁnite measure ν. A maximum likelihood\n",
      "estimate, or MLE, of θ, written bθ, is deﬁned as\n",
      "bθ = arg max\n",
      "θ∈Θ\n",
      "L(θ ; x),\n",
      "(6.9)\n",
      "if it exists. There may be more than solution; any one is an MLE. If x is\n",
      "viewed as a random variable, then bθ is called a maximum likelihood estimator\n",
      "of θ.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "450\n",
      "6 Statistical Inference Based on Likelihood\n",
      "While I like to use the “hat” notation to mean an MLE, I also sometimes use\n",
      "it to mean any estimate or estimator.\n",
      "The estimate (or estimator) bθ is a Borel function of the observations or of\n",
      "the random variables.\n",
      "We use “MLE” to denote either a maximum likelihood estimate or estima-\n",
      "tor, or to denote the method of maximum likelihood estimation. The proper\n",
      "meaning can be determined from the context. If the term MLE is used in a\n",
      "statement about a maximum likelihood estimate or estimator, the statement\n",
      "can be assumed to apply to both the estimate and the estimator.\n",
      "If bθ in (6.9) exists, we also have\n",
      "bθ = arg max\n",
      "θ∈Θ\n",
      "lL(θ ; x),\n",
      "(6.10)\n",
      "that is, the MLE can be identiﬁed either from the likelihood function or from\n",
      "the log-likelihood.\n",
      "The Likelihood Equations\n",
      "Notice that ﬁnding an MLE means to solve a constrained optimization prob-\n",
      "lem. In simple cases, the constraints may not be active. In even simpler cases,\n",
      "the likelihood is diﬀerentiable, and the MLE occurs at a stationary point in\n",
      "the interior of the constraint space. In these happy cases, the MLE can be\n",
      "identiﬁed by diﬀerentiation.\n",
      "If the likelihood function or the log-likelihood function is diﬀerentiable\n",
      "within Θ◦, we call\n",
      "∇L(θ ; x) = 0\n",
      "(6.11)\n",
      "or\n",
      "∇lL(θ ; x) = 0\n",
      "(6.12)\n",
      "the likelihood equations.\n",
      "If θr ∈Θ◦is a root of the likelihood equations and if the Hessian HL(θr)\n",
      "evaluated at θr is negative deﬁnite, then θr ∈Θ◦is a local optimizer of L\n",
      "(and of lL). (See Theorem 0.0.13.)\n",
      "If the maximum occurs within Θ◦, then every MLE is a root of the like-\n",
      "lihood equations. There may be other roots within Θ◦, of course. Any such\n",
      "root of the likelihood equation, called an RLE, may be of interest.\n",
      "Example 6.2 MLE in the exponential family (continuation of Ex-\n",
      "ample 1.5)\n",
      "In the exponential family of Example 1.5, with a sample x1, . . ., xn, the like-\n",
      "lihood in equation (1.21) becomes\n",
      "L(θ ; x) = θ−ne−Pn\n",
      "i=1 xi/θIIR+(θ),\n",
      "whose derivative wrt θ is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "451\n",
      " \n",
      "−nθ−n−1e−Pn\n",
      "i=1 xi/θ + θ−n−2\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xie−Pn\n",
      "i=1 xi/θ\n",
      "!\n",
      "IIR+(θ).\n",
      "Equating this to zero, we obtain\n",
      "bθ =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xi/n\n",
      "as a stationary point. Checking the second derivative, we ﬁnd it is negative\n",
      "at bθ, and so we conclude that bθ is indeed the MLE of θ, and it is the only\n",
      "maximizer. Also, from the plot on the right side of Figure 1.2, we have visual\n",
      "conﬁrmation. Of course, Figure 1.2 is for a sample of size one.\n",
      "We can easily see that for a sample of size n this graph would be similar,\n",
      "but it would have a sharper peak; see Figure 6.1.\n",
      "0\n",
      "5\n",
      "10\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "θ\n",
      "likelihood\n",
      "Sample of Size 1\n",
      "0\n",
      "5\n",
      "10\n",
      "0.000\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "θ\n",
      "likelihood\n",
      "Sample of Size 5\n",
      "Figure 6.1.\n",
      "Likelihood for Diﬀerent Sample Sizes\n",
      "The fact that the likelihood has a sharper peak is in agreement with our\n",
      "expectation that the estimate should be “better” if we have a larger sample.\n",
      "Example 6.3 MLE in the exponential family with right censoring\n",
      "In use of the exponential family for modeling “lifetimes”, say of lightbulbs, it\n",
      "is often the case that the experiment is terminated before all of the random\n",
      "variables are realized; that is, we may have a potential sample x1, . . ., xn,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "452\n",
      "6 Statistical Inference Based on Likelihood\n",
      "but actually we only have values for the xi < tc, where tc is some ﬁxed\n",
      "and known value. It might be called the “censoring time”. This setup is yields\n",
      "censored data, in particular, it is right censored data, because the larger values\n",
      "are censored. Suppose that t1, . . ., tr observations are obtained, leaving n −r\n",
      "unobserved values of the potential sample. In this setup, the time tc is ﬁxed,\n",
      "and so r is a random variable. We could also contemplate an experimental\n",
      "setup in which r is chosen in advance, and so the censoring time tc is a\n",
      "random variable. (These two data-generating processes are similar to the two\n",
      "experiments we described for Bernoulli data in Example 3.12, and to which we\n",
      "have alluded in other examples.) The ﬁrst method is called “Type I censoring”\n",
      "(upper bound on the observation ﬁxed) and the other method is called “Type\n",
      "II censoring” (ﬁxed number of observed values to be taken).\n",
      "Censoring is diﬀerent from a situation in which the distribution is trun-\n",
      "cated, as in Exercise 2.14 on page 203.\n",
      "For right censored data with n, r, and tc as described above from any\n",
      "distribution with PDF f(x; θ) and CDF F (x; θ), the likelihood function is\n",
      "L(θ; x) =\n",
      "r\n",
      "Y\n",
      "i=1\n",
      "f(ti; θ)(1 −F (tc; θ))n−r.\n",
      "We may note in passing that the likelihood is the same for type I and type\n",
      "II censoring, just as we saw it to be in the binomial and negative binomial\n",
      "distributions arising from Bernoulli data in Example 3.12.\n",
      "Now, for the case where the distribution is exponential with parameter θ,\n",
      "we have the likelihood function\n",
      "L(θ; x) 1\n",
      "θr e\n",
      "Pr\n",
      "i=1 ti/θe(n−r)tc/θ.\n",
      "The maximum, which we can ﬁnd by diﬀerentiation, occurs at\n",
      "bθ = T/r,\n",
      "where T = Pr\n",
      "i=1 ti + (n −r)tc is called the “total time on test”.\n",
      "MLE in ∂Θ\n",
      "If Θ is open, and if the maximizer in equation (6.9) is in ∂Θ, the distribution\n",
      "deﬁned by the MLE may be degenerate, as can be the case in the following\n",
      "example.\n",
      "Example 6.4 MLE of Bernoulli parameter\n",
      "Consider the Bernoulli family of distributions with parameter π. In the usual\n",
      "deﬁnition of this family, π ∈Π =]0, 1[. Suppose we take a random sample\n",
      "X1, . . ., Xn. The log-likelihood is\n",
      "lL(π ; x) =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xi log(π) +\n",
      " \n",
      "n −\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xi\n",
      "!\n",
      "log(1 −π).\n",
      "(6.13)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "453\n",
      "This is a concave diﬀerentiable function, so we can get the maximum by\n",
      "diﬀerentiating and setting the result to zero. We obtain\n",
      "bπ =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xi/n.\n",
      "(6.14)\n",
      "If Pn\n",
      "i=1 xi = 0 or if Pn\n",
      "i=1 xi = n, bπ /∈Π, but bπ ∈Π so bπ is the MLE of π.\n",
      "Note that in this case, the MLE corresponds to the Bayes estimator with\n",
      "loss function (4.52) and uniform prior (see page 360) and to the UMVUE (see\n",
      "page 394).\n",
      "Further comments on Example 6.4\n",
      "In Example 6.1 we considered the problem of making inferences on the pa-\n",
      "rameter π in a family of Bernoulli distributions, and considered two diﬀerent\n",
      "approaches. One approach was to take a random sample of size n, X1, . . ., Xn\n",
      "from the Bernoulli(π), and then use T = PXi, which has a binomial distri-\n",
      "bution with parameters n and π. Another approach was to take a sequential\n",
      "sample, X1, X2, . . ., until a ﬁxed number t of 1’s have occurred. The likeli-\n",
      "hood principle tells us that if the data are the same, we should reach the\n",
      "same conclusions. In Example 6.1 we wrote the likelihood functions based on\n",
      "these two diﬀerent approaches. One was the same as in equation (6.13) and\n",
      "so the MLE under that setup would be that given in equation (6.14). After\n",
      "canceling constants, the other log-likelihood in Example 6.1 was also\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xi log(π) +\n",
      " \n",
      "n −\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xi\n",
      "!\n",
      "log(1 −π),\n",
      "so a that sampling scheme yields the same MLE, if n and Pn\n",
      "i=1 xi are the\n",
      "same.\n",
      "Of course making inferences about a parameter involves more than just\n",
      "obtaining a good estimate of it. We will consider the problem again in Exam-\n",
      "ples 6.9 and 7.12.\n",
      "Allowing an MLE to be in Θ−Θ is preferable to saying that an MLE does\n",
      "not exist. It does, however, ignore the question of continuity of L(θ ; x) over\n",
      "Θ, and it allows an estimated PDF that is degenerate.\n",
      "We have encountered this situation before in the case of UMVUEs; see\n",
      "Example 5.5.\n",
      "While the open parameter space in Example 6.4 would lead to a problem\n",
      "with existence of the MLE if its deﬁnition was as a maximum over the pa-\n",
      "rameter space instead of its closure, an open support can likewise lead to a\n",
      "problem. Consider a distribution with Lebesgue PDF\n",
      "pX(x) = h(x, θ)IS(θ)(x)\n",
      "(6.15)\n",
      "where S(θ) is open. In this case, the likelihood has the form\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "454\n",
      "6 Statistical Inference Based on Likelihood\n",
      "L(θ ; x) = h(x, θ)IR(x)(θ),\n",
      "(6.16)\n",
      "where R(x) is open. It is quite possible that sup L(θ ; x) will occur on R(x) −\n",
      "R(x).\n",
      "Example 6.5 MLE in U(0, θ); closed support\n",
      "Consider X1, . . ., Xn\n",
      "iid\n",
      "∼U(0, θ), with θ ∈Θ = IR+. The PDF is\n",
      "pX(x) = 1\n",
      "θ I[0,θ](x).\n",
      "(6.17)\n",
      "The likelihood is\n",
      "L(θ ; x) = 1\n",
      "θ I[x(n),∞[(θ).\n",
      "(6.18)\n",
      "The MLE is easily seen to be bθ = x(n). In Example 5.8, we saw that the\n",
      "UMVUE of θ is (1 + 1/n)x(n).\n",
      "Suppose we take the support to be the open interval ]0, θ[. (Despite Def-\n",
      "inition 1.12, such a support is often assumed.) The likelihood function then\n",
      "is\n",
      "L(θ ; x) = 1\n",
      "θ I]x(n),∞[(θ).\n",
      "This is discontinuous and it does not have a maximum, as we see in Figure 6.2.\n",
      "θ\n",
      "x(n)\n",
      "L\n",
      "]\n",
      "(\n",
      "Figure 6.2.\n",
      "Discontinuous Likelihood with No Maximum\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "455\n",
      "In this case the maximum of the likelihood does not exist, but the supre-\n",
      "mum of the likelihood occurs at x(n) and it is ﬁnite. We would like to call x(n)\n",
      "the MLE of θ.\n",
      "We can reasonably do this by modifying the deﬁnition of the family of\n",
      "distributions by adding a zero-probability set to the support. We redeﬁne the\n",
      "family in equation (6.15) to have the Lebesgue PDF\n",
      "pX(x) = 1\n",
      "θ I[0,θ](x).\n",
      "(6.19)\n",
      "Now, the open interval ]x(n), ∞[ where the likelihood was positive before be-\n",
      "comes a half-closed interval [x(n), ∞[, and the maximum of the likelihood\n",
      "occurs at x(n).\n",
      "This is one reason why we deﬁne the support to be closed.\n",
      "This approach is cleaner than solving the logical problem by deﬁning the\n",
      "MLE in terms of the sup rather than the max. A deﬁnition in terms of the sup\n",
      "may not address problems that could arise due to various types of discontinuity\n",
      "of L(θ ; x) at the boundary of S(θ).\n",
      "MLE of More than One Parameter\n",
      "It is usually more diﬃcult to determine the MLE of more than one parameter.\n",
      "The likelihood equation in that case is a system of equations. Also, of course,\n",
      "the likelihood equation, whether a single equation or a system, may not be\n",
      "easy to solve, as the following example shows.\n",
      "Example 6.6 MLE of the parameters in a gamma distribution\n",
      "Consider the gamma family of distributions with parameters α and β. Given\n",
      "a random sample x1, . . ., xn, the log-likelihood of α and β is\n",
      "lL(α, β ; x) = −nα log(β)−n log(Γ(α)))+(α−1)\n",
      "X\n",
      "log(xi)−1\n",
      "β\n",
      "X\n",
      "xi. (6.20)\n",
      "This yields the likelihood equations\n",
      "−n log(β) −nΓ′(α)\n",
      "Γ(α) +\n",
      "X\n",
      "log(xi) = 0\n",
      "(6.21)\n",
      "and\n",
      "−nα\n",
      "β + 1\n",
      "β2\n",
      "X\n",
      "xi = 0.\n",
      "(6.22)\n",
      "Checking the Hessian (at any point in the domain), we see that a root of the\n",
      "likelihood equations is a local minimizer.\n",
      "At the solution we have\n",
      "bβ =\n",
      "X\n",
      "xi/(nbα)\n",
      "(6.23)\n",
      "and\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "456\n",
      "6 Statistical Inference Based on Likelihood\n",
      "log(bα) −Γ′(bα)\n",
      "Γ(bα) +\n",
      "X\n",
      "log(xi)/n −log(\n",
      "X\n",
      "xi/n) = 0.\n",
      "(6.24)\n",
      "There is no closed form solution. A numerical method must be used; see\n",
      "Example 6.14.\n",
      "Sometimes in multiple-parameter models, the parameters are functionally\n",
      "independent and the optimization can be performed on diﬀerent parts of the\n",
      "separable likelihood function. This is the case for a normal distribution, as we\n",
      "see in Example 6.25.\n",
      "Example 6.7 MLE in the exponential family with range dependency\n",
      "Consider the two-parameter exponential family, that is, a shifted version of the\n",
      "exponential family of distributions. This family is the subject of Example 5.9\n",
      "on page 397. The Lebesgue PDF is\n",
      "θ−1e−(x−α)/θI]α,∞[(x)\n",
      "Suppose we have observations X1, X2, . . ., Xn. The likelihood function is\n",
      "L(α, θ; X) = θ−n exp\n",
      "\u0010\n",
      "−\n",
      "X\n",
      "(Xi −α)/θ\n",
      "\u0011\n",
      "I]0,X(1)](α)I]0,∞[(θ).\n",
      "This is 0 when α > X(1), but it is increasing in α on ]0, X(1)] independently\n",
      "of θ.\n",
      "Hence, the MLE of α is X(1).\n",
      "Now, we substitute this back into L(α, θ; X) and maximize wrt θ, that is,\n",
      "we solve\n",
      "max\n",
      "θ\n",
      "\u0010\n",
      "θ−n exp\n",
      "\u0010\n",
      "−\n",
      "X\n",
      "(Xi −X(1))/θ\n",
      "\u0011\u0011\n",
      ".\n",
      "We do this by forming and solving the likelihood equation, noting that it\n",
      "yields a maximum within the parameter space. We get\n",
      "bθ = 1\n",
      "n\n",
      "X\n",
      "(Xi −X(1)).\n",
      "In Example 5.9, we found the UMVUEs:\n",
      "Tα = X(1) −\n",
      "1\n",
      "n(n −1)\n",
      "X\n",
      "(Xi −X(1))\n",
      "and\n",
      "Tθ =\n",
      "1\n",
      "n −1\n",
      "X\n",
      "(Xi −X(1)).\n",
      "(Recall that we ﬁnd a complete suﬃcient statistic and then manipulate it to\n",
      "be unbiased.) Notice the similarity of these to the MLEs, which are biased.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "457\n",
      "6.2.2 Finite Sample Properties of MLEs\n",
      "Any approach to estimation may occasionally yield very poor or meaningless\n",
      "estimators. In addition to the possibly negative UMVUEs for variance com-\n",
      "ponents in Example 5.31, we have seen in Exercise 5.2 that a UMVUE of\n",
      "g(θ) = e−3θ in a Poisson distribution is not a very good estimator. While in\n",
      "some cases the MLE is more reasonable (see Exercise 6.1), in other cases the\n",
      "MLE may be very poor.\n",
      "As we have mentioned, MLEs have a nice intuitive property. In Section 6.3\n",
      "we will see that they also often have good asymptotic properties.\n",
      "We now consider some other properties; some useful and some less desir-\n",
      "able.\n",
      "Relation to Suﬃcient Statistics\n",
      "Theorem 6.1\n",
      "If there is a suﬃcient statistic and an MLE exists, then an MLE is a function\n",
      "of the suﬃcient statistic.\n",
      "Proof.\n",
      "This follows directly from the factorization theorem.\n",
      "Relation to Eﬃcient Statistics\n",
      "Given the three Fisher information regularity conditions (see page 168) we\n",
      "have deﬁned “Fisher eﬃcient estimators” as unbiased estimators that achieve\n",
      "the lower bound on their variance.\n",
      "Theorem 6.2\n",
      "Assume the FI regularity conditions for a family of distributions {Pθ} with\n",
      "the additional Le Cam-type requirement that the Fisher information matrix\n",
      "I(θ) is positive deﬁnite for all θ. Let T(X) be a Fisher eﬃcient estimator of\n",
      "θ. Then T(X) is an MLE of θ.\n",
      "Proof.\n",
      "Let pθ(x) be the PDF. We have\n",
      "∂\n",
      "∂θ log(pθ(x)) = I(θ)(T(x) −θ)\n",
      "for any θ and x. Clearly, for θ = T(x), this equation is 0 (hence, T(X) is an\n",
      "RLE). Because I(θ), which is the negative of the Hessian of the likelihood, is\n",
      "positive deﬁnite for all θ, the likelihood is convex in θ and T(x) maximizes\n",
      "the likelihood.\n",
      "Notice that without the additional requirement of a positive deﬁnite in-\n",
      "formation matrix, Theorem 6.2 would yield only the conclusion that T(X) is\n",
      "an RLE.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "458\n",
      "6 Statistical Inference Based on Likelihood\n",
      "Equivariance of MLEs\n",
      "If bθ is a good estimator of θ, it would seem to be reasonable that g(bθ) is a good\n",
      "estimator of g(θ), where g is a Borel function. “Good”, of course, is relative\n",
      "to some criterion. In a decision-theoretic approach, we seek L-invariance; that\n",
      "is, invariance of the loss function (see page 266). Even if the loss function is\n",
      "invariant, other properties may not be preserved. If the criterion is UMVU,\n",
      "then the estimator in general will not have this equivariance property; that\n",
      "is, if bθ is a UMVUE of θ, then g(bθ) may not be a UMVUE of g(θ). (It is not\n",
      "even unbiased in general.)\n",
      "We now consider the problem of determining the MLE of g(θ) when we\n",
      "have an MLE bθ of θ. Following the deﬁnition of an MLE, the MLE of g(θ)\n",
      "should be the maximizer of the likelihood function of g(θ). If the function g\n",
      "is not one-to-one, the likelihood function of g(θ) may not be well-deﬁned. We\n",
      "therefore introduce the induced likelihood.\n",
      "Deﬁnition 6.3 (induced likelihood)\n",
      "Let {pθ : θ ∈Θ} with Θ ⊆IRd be a family of PDFs wrt a common σ-\n",
      "ﬁnite measure, and let L(θ) be the likelihood associated with this family,\n",
      "given observations. Now let g be a Borel function from Θ to Λ ⊆IRd1 where\n",
      "1 ≤d1 ≤d. Then\n",
      "eL(λ) =\n",
      "sup\n",
      "{θ : θ∈Θ and g(θ)=λ}\n",
      "L(θ)\n",
      "(6.25)\n",
      "is called the induced likelihood function for the transformed parameter.\n",
      "The induced likelihood provides an appropriate MLE for g(θ) in the sense of\n",
      "the following theorem.\n",
      "Theorem 6.3\n",
      "Suppose {pθ : θ ∈Θ} with Θ ⊆IRd is a family of PDFs wrt a common\n",
      "σ-ﬁnite measure with associated likelihood L(θ). Let bθ be an MLE of θ. Now\n",
      "let g be a Borel function from Θ to Λ ⊆IRd1 where 1 ≤d1 ≤d and let eL(λ)\n",
      "be the resulting induced likelihood. Then g(bθ) maximizes eL(λ).\n",
      "Proof.\n",
      "Follows directly from deﬁnitions, but it is an exercise to ﬁll in the details.\n",
      "Usually when we consider reparametrizations, as in Section 2.6, with one-\n",
      "to-one functions. This provides a clean approach to the question of the MLE\n",
      "of g(θ) without having to introduce an induced likelihood.\n",
      "Given the distribution Pθ for the random variable X, suppose we seek\n",
      "an MLE of ˜g(θ). If ˜g is not one-to-one, then ˜g(θ) does not provide enough\n",
      "information to deﬁne the distribution P˜g(θ) for X. Therefore, we cannot deﬁne\n",
      "the likelihood for ˜g(θ).\n",
      "If ˜g(θ) is one-to-one, let g(θ) = ˜g(θ), otherwise, deﬁne\n",
      "g(θ) = (˜g(θ), h(θ))\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "459\n",
      "in such a way that g(θ) is one-to-one. The function h is not unique, but g−1\n",
      "is unique; the likelihood is well-deﬁned; g(bθ) is an MLE of g(θ); and so ˜g(bθ)\n",
      "is an MLE of ˜g(θ). Compare this with the results of Theorem 6.3 above.\n",
      "Example 6.8 MLE of the variance in a Bernoulli distribution\n",
      "Consider the Bernoulli family of distributions with parameter π. The vari-\n",
      "ance of a Bernoulli distribution is g(π) = π(1 −π). Given a random sample\n",
      "x1, . . ., xn, the MLE of π is\n",
      "bπ =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xi/n,\n",
      "as we saw in Example 6.4, hence the MLE of the variance is\n",
      "1\n",
      "n\n",
      "X\n",
      "xi\n",
      "\u0010\n",
      "1 −\n",
      "X\n",
      "xi/n\n",
      "\u0011\n",
      ".\n",
      "(6.26)\n",
      "Note that this estimator is biased and that it is the same estimator as that of\n",
      "the variance in a normal distribution from Example 3.13:\n",
      "X\n",
      "(xi −¯x)2/n.\n",
      "As we saw in Example 5.7, the UMVUE of the variance in a Bernoulli\n",
      "distribution is, as in equation (5.11),\n",
      "1\n",
      "n −1\n",
      "X\n",
      "xi\n",
      "\u0010\n",
      "1 −\n",
      "X\n",
      "xi/n\n",
      "\u0011\n",
      ".\n",
      "The diﬀerence in the MLE and the UMVUE of the variance in the Bernoulli\n",
      "distribution is the same as the diﬀerence in the estimators of the variance in\n",
      "the normal distribution that we encountered in Example 3.13 and Exam-\n",
      "ple 5.6. How do the MSEs of the estimators of the variance in a Bernoulli\n",
      "distribution compare? (Exercise 6.6.)\n",
      "Whenever the variance of a distribution can be expressed as a function\n",
      "of other parameters g(θ), as in the case of the Bernoulli distribution, the\n",
      "estimator of the variance is g(bθ), where bθ is an MLE of θ. The MLE of the\n",
      "variance of the gamma distribution, for example, is bαbβ2, where bα and bβ are\n",
      "the MLEs in Example 6.6. The plug-in estimator of the variance of the gamma\n",
      "distribution, given the sample, X1, X2 . . ., Xn, as always, is\n",
      "1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "\u0000Xi −X\u00012 .\n",
      "Example 6.9 The likelihood principle in sampling from a Bernoulli\n",
      "distribution\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "460\n",
      "6 Statistical Inference Based on Likelihood\n",
      "In Example 6.1 we considered the problem of making inferences on the param-\n",
      "eter π in a family of Bernoulli distributions by either taking a random sample\n",
      "of size n and using T = P Xi, which has a binomial distribution, or by taking\n",
      "a sample, X1, X2, . . ., until a ﬁxed number t of 1’s have occurred and observing\n",
      "the size of the sample N, which has a negative binomial distribution. Given\n",
      "T = t or N = n, either way, we get the MLE\n",
      "ˆπ = t/n.\n",
      "To make inferences on π using ˆπ we need the variance V(ˆπ). Under the bino-\n",
      "mial distribution, we need the variance of T/n, which is π(1 −π)/n, whose\n",
      "MLE as in Example 6.8 is ˆπ(1 −ˆπ)/n. Under the negative binomial distri-\n",
      "bution, we need the variance of t/N. The variance of N is t(1 −π)/π2 and\n",
      "its MLE is the same with ˆπ in place of π. The variance of t/N cannot be\n",
      "expressed in closed form. (See Stephan (1945).)\n",
      "Although we have seen in equations (6.5) and (6.6) that the ratio of the\n",
      "likelihoods does not involve π and the MLEs based on the two data-generating\n",
      "processes conform to the likelihood principle, the variances of the MLEs are\n",
      "diﬀerent.\n",
      "Other Properties of MLEs\n",
      "Some properties of MLEs are not always desirable.\n",
      "First of all, we note that an MLE may be biased. The most familiar ex-\n",
      "ample of this is the MLE of the variance, as seen in Examples 6.8 and 3.13.\n",
      "Another example is the MLE of the location parameter in the uniform distri-\n",
      "bution in Example 6.5.\n",
      "Although the MLE approach is usually an intuitively logical one, it is not\n",
      "based on a formal decision theory, so it is not surprising that MLEs may not\n",
      "possess certain desirable properties that are formulated from that perspective.\n",
      "An example of a likelihood function that is not very useful without some\n",
      "modiﬁcation is in nonparametric probability density estimation. Suppose we\n",
      "assume that a sample comes from a distribution with continuous PDF p(x).\n",
      "The likelihood is Qn\n",
      "i=1 p(xi). Even under the assumption of continuity, there\n",
      "is no solution. We will discuss this problem in Chapter 8.\n",
      "C. R. Rao cites another example in which the likelihood function is not\n",
      "very meaningful.\n",
      "Example 6.10 a meaningless MLE\n",
      "Consider an urn containing N balls labeled 1, . . ., N and also labeled with\n",
      "distinct real numbers θ1, . . ., θN (with N known). For a sample without re-\n",
      "placement of size n < N where we observe (xi, yi) = (label, θlabel), what is the\n",
      "likelihood function? It is either 0, if the label and θlabel for at least one ob-\n",
      "servation is inconsistent, or \u0000N\n",
      "n\n",
      "\u0001−1, otherwise; and, of course, we don’t know!\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "461\n",
      "This likelihood function is not informative, and could not be used, for exam-\n",
      "ple, for estimating θ = θ1 + · · · + θN. (There is a pretty good estimator of θ;\n",
      "it is N(P yi)/n.)\n",
      "There are other interesting examples in which MLEs do not have desirable\n",
      "(or expected) properties.\n",
      "•\n",
      "An MLE may be discontinuous in the data. This is obviously the case for\n",
      "a discrete distribution, but it can also occur in a contaminated continuous\n",
      "distribution as, for example, in the case of ϵ-mixture distribution family\n",
      "with CDF\n",
      "Pxc,ϵ(x) = (1 −ϵ)P (x) + ϵI[xc,∞[(x),\n",
      "(6.27)\n",
      "where 0 ≤ϵ ≤1.\n",
      "•\n",
      "An MLE may not be a function of a suﬃcient statistic (if the MLE is not\n",
      "unique).\n",
      "•\n",
      "An MLE may not satisfy the likelihood equation as, for example, when the\n",
      "likelihood function is not diﬀerentiable at its maximum, as in Example 6.5.\n",
      "•\n",
      "The likelihood equation may have a unique root, yet no MLE exists. While\n",
      "there are examples in which the roots of the likelihood equations occur\n",
      "at minima of the likelihood, this situation does not arise in any realistic\n",
      "distribution (that I am aware of). Romano and Siegel (1986) construct a\n",
      "location family of distributions with support on\n",
      "IR −{x1 + θ, x2 + θ : x1 < x2},\n",
      "where x1 and x2 are known but θ is unknown, with a Lebesgue density\n",
      "p(x) that rises as x ↗x1 to a singularity at x1 and rises as x ↙x2 to a\n",
      "singularity at x2 and that is continuous and strictly convex over ]x1, x2[\n",
      "and singular at both x1 and x2. With a single observation, the likelihood\n",
      "equation has a root at the minimum of the convex portion of the density\n",
      "between x1 + θ and x2 + θ, but the likelihood increases without bound at\n",
      "both x1 + θ and x2 + θ.\n",
      "•\n",
      "An MLE may diﬀer from an MME; in particular an MLE of the population\n",
      "mean may not be the sample mean.\n",
      "Note that Theorem 6.1 hints at two other issues: nonuniqueness of an MLE\n",
      "and existence of an MLE. We now consider these.\n",
      "Nonuniqueness\n",
      "There are many cases in which the MLEs are not unique (and I’m not just\n",
      "referring to RLEs). The following examples illustrate this.\n",
      "Example 6.11 likelihood in a Cauchy family\n",
      "Consider the Cauchy distribution with location parameter θ. The likelihood\n",
      "equation is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "462\n",
      "6 Statistical Inference Based on Likelihood\n",
      "n\n",
      "X\n",
      "i=1\n",
      "2(xi −θ)\n",
      "1 + (xi −θ)2 = 0.\n",
      "This may have multiple roots (depending on the sample), and so the one\n",
      "yielding the maximum would be the MLE. Depending on the sample, however,\n",
      "multiple roots can yield the same value of the likelihood function.\n",
      "Another example in which the MLE is not unique is U(θ −1/2, θ + 1/2).\n",
      "Example 6.12 likelihood in a uniform family with ﬁxed range\n",
      "Given the sample x1, . . ., xn, the likelihood function for U(θ −1/2, θ + 1/2)\n",
      "is\n",
      "I[x(n)−1/2, x(1)+1/2](θ).\n",
      "It is maximized at any value between x(n) −1/2 and x(1) + 1/2.\n",
      "Nonexistence and Other Properties\n",
      "We have already mentioned situations in which the likelihood approach does\n",
      "not seem to be the logical way, and have seen that sometimes in nonparametric\n",
      "problems, the MLE does not exist. This often happens when there are more\n",
      "“things to estimate” than there are observations. This can also happen in\n",
      "parametric problems. It may happen that the maximum does not exist because\n",
      "the likelihood is unbounded from above. In this case the argmax does not exist,\n",
      "and the maximum likelihood estimate does not exist.\n",
      "Example 6.13 nonexistence of MLE\n",
      "Consider the normal family of distributions with parameters µ and σ2. Sup-\n",
      "pose we have one observation x. The log-likelihood is\n",
      "lL(µ, σ2 ; x) = −1\n",
      "2 log(2πσ2) −(x −µ)2\n",
      "2σ2\n",
      ",\n",
      "which is unbounded when µ = x and σ2 approaches zero. It is therefore clear\n",
      "that no MLE of σ2 exists. Strictly speaking, we could also say that no MLE\n",
      "of µ exists either; however, for any ﬁxed value of σ2 in the (open) parameter\n",
      "space, µ = x maximizes the likelihood, so it is reasonable to call x the MLE\n",
      "of µ.\n",
      "Recall from Example 5.14 that the degree of the variance functional is 2.\n",
      "In this case, some people prefer to say that the likelihood function does not\n",
      "exist; that is, they suggest that the deﬁnition of a likelihood function include\n",
      "boundedness.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "463\n",
      "6.2.3 The Score Function and the Likelihood Equations\n",
      "In several of the preceding examples, we found the MLEs by diﬀerentiating\n",
      "the likelihood and equating the derivative to zero. In many cases, of course,\n",
      "we cannot ﬁnd an MLE by just diﬀerentiating the likelihood; Example 6.5\n",
      "is such a case. We will discuss methods of ﬁnding an MLE in Section 6.2.4\n",
      "beginning on page 465.\n",
      "In the following we will generally consider only the log-likelihood, and we\n",
      "will assume that it is diﬀerentiable within Θ◦.\n",
      "The derivative of the log-likelihood is the score function sn(θ ; x) (equa-\n",
      "tion (3.57) on page 244). The score function is important in computations for\n",
      "determining an MLE, as we see in Section 6.2.4, but it is also important in\n",
      "studying properties of roots of the likelihood equation, especially asymptotic\n",
      "properties, as we see in Section 6.3.\n",
      "The score function is an estimating function and leads to the likelihood\n",
      "equation ∇lL(θ ; x) = 0 or\n",
      "sn(θ ; x) = 0,\n",
      "(6.28)\n",
      "which is an estimating equation, similar to the estimating equation (5.71) for\n",
      "least squares estimators. Generalizations of these equations are called “gener-\n",
      "alized estimating equations”, or GEEs; see Section 3.2.5.\n",
      "Any root of the likelihood equations, which is called an RLE, may be an\n",
      "MLE. A theorem from functional analysis, usually proved in the context of\n",
      "numerical optimization, states that if θ∗is an RLE and HlL(θ∗; x) is negative\n",
      "deﬁnite, then there is a local maximum at θ∗. This may allow us to determine\n",
      "that an RLE is an MLE. There are, of course, other ways of determining\n",
      "whether an RLE is an MLE. In MLE, the determination that an RLE is\n",
      "actually an MLE is an important step in the process.\n",
      "The Log-Likelihood Function and the Score Function in Regular\n",
      "Families\n",
      "In the regular case satisfying the three Fisher information regularity conditions\n",
      "(see page 168), the likelihood function and consequently the log-likelihood\n",
      "are twice diﬀerentiable within Θ◦, and the operations of diﬀerentiation and\n",
      "integration can be interchanged. In this case, the score estimating function is\n",
      "unbiased (see Deﬁnition 3.7):\n",
      "Eθ(sn(θ ; X)) =\n",
      "Z\n",
      "X\n",
      "∂\n",
      "∂θ lL(θ ; x)p(x; θ)dx\n",
      "=\n",
      "Z\n",
      "X\n",
      "∂\n",
      "∂θ p(x; θ)dx\n",
      "= ∂\n",
      "∂θ\n",
      "Z\n",
      "X\n",
      "p(x; θ)dx\n",
      "= 0.\n",
      "(6.29)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "464\n",
      "6 Statistical Inference Based on Likelihood\n",
      "The derivatives of the log-likelihood function relate directly to useful con-\n",
      "cepts in statistical inference. If it exists, the derivative of the log-likelihood\n",
      "is the relative rate of change, with respect to the parameter placeholder θ, of\n",
      "the probability density function at a ﬁxed observation. If θ is a scalar, some\n",
      "positive function of the derivative, such as its square or its absolute value, is\n",
      "obviously a measure of the eﬀect of change in the parameter, or of change\n",
      "in the estimate of the parameter. More generally, an outer product of the\n",
      "derivative with itself is a useful measure of the changes in the components of\n",
      "the parameter:\n",
      "∇lL\n",
      "\u0000θ(k) ; x\u0001 \u0010\n",
      "∇lL\n",
      "\u0000θ(k) ; x\u0001\u0011T\n",
      ".\n",
      "Notice that the average of this quantity with respect to the probability density\n",
      "of the random variable X,\n",
      "I(θ1 ; X) = Eθ1\n",
      "\u0012\n",
      "∇lL\n",
      "\u0000θ(k) ; X\n",
      "\u0001 \u0010\n",
      "∇lL\n",
      "\u0000θ(k) ; X\n",
      "\u0001\u0011T\u0013\n",
      ",\n",
      "(6.30)\n",
      "is the information matrix for an observation on Y about the parameter θ.\n",
      "If θ is a scalar, the square of the ﬁrst derivative is the negative of the\n",
      "second derivative,\n",
      "\u0012 ∂\n",
      "∂θ lL(θ ; x)\n",
      "\u00132\n",
      "= −∂2\n",
      "∂θ2 lL(θ ; x),\n",
      "or, in general,\n",
      "∇lL\n",
      "\u0000θ(k) ; x\n",
      "\u0001 \u0010\n",
      "∇lL\n",
      "\u0000θ(k) ; x\n",
      "\u0001\u0011T\n",
      "= −HlL\n",
      "\u0000θ(k) ; x\n",
      "\u0001\n",
      ".\n",
      "(6.31)\n",
      "MLEs in Exponential Families\n",
      "If X has a distribution in the exponential class and we write its density in the\n",
      "natural or canonical form, the likelihood has the form\n",
      "L(η ; x) = exp(ηTT(x) −ζ(η))h(x).\n",
      "(6.32)\n",
      "The log-likelihood equation is particularly simple:\n",
      "T(x) −∂ζ(η)\n",
      "∂η\n",
      "= 0.\n",
      "(6.33)\n",
      "Newton’s method for solving the likelihood equation is\n",
      "η(k) = η(k−1) −\n",
      "\u0012 ∂2ζ(η)\n",
      "∂η(∂η)T\n",
      "\f\fη=η(k−1)\n",
      "\u0013−1 \u0012\n",
      "T(x) −∂ζ(η)\n",
      "∂η\n",
      "\f\fη=η(k−1)\n",
      "\u0013\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "465\n",
      "Note that the second term includes the Fisher information matrix for η.\n",
      "(The expectation is constant.) (Note that the FI matrix is not for a distribu-\n",
      "tion; it is for a parametrization of a distribution.)\n",
      "We have\n",
      "V(T(X)) = ∂2ζ(η)\n",
      "∂η(∂η)T |η=η .\n",
      "Note that the variance is evaluated at the true η (even though in an expression\n",
      "such as ∂η it must be a variable).\n",
      "If we have a full-rank member of the exponential class then V is positive\n",
      "deﬁnite, and hence there is a unique maximum.\n",
      "If we write\n",
      "µ(η) = ∂ζ(η)\n",
      "∂η\n",
      ",\n",
      "in the full-rank case, µ−1 exists and so we have the solution to the likelihood\n",
      "equation:\n",
      "bη = µ−1(T(x)).\n",
      "(6.34)\n",
      "So maximum likelihood estimation is very nice for the exponential class.\n",
      "6.2.4 Finding an MLE\n",
      "Notice that the problem of obtaining an MLE is a constrained optimization\n",
      "problem; that is, an objective function is to be optimized subject to the con-\n",
      "straints that the solution be within the closure of the parameter space.\n",
      "In some cases the MLE occurs at a stationary point, which can be identiﬁed\n",
      "by diﬀerentiation. That is not always the case, however. A standard example\n",
      "in which the MLE does not occur at a stationary point is a distribution in\n",
      "which the range depends on the parameter, and the simplest such distribution\n",
      "is the uniform U(0, θ), which was the subject of Example 6.5.\n",
      "In this section, we will discuss some standard methods of maximizing a\n",
      "likelihood function and also some methods that are useful in more complicated\n",
      "situations.\n",
      "Computations\n",
      "If the log-likelihood is twice diﬀerentiable and if the range does not depend\n",
      "on the parameter, Equation (6.31) is interesting because the second deriva-\n",
      "tive, or an approximation of it, is used in a Newton-like method to solve the\n",
      "maximization problem (6.10). Newton’s equation\n",
      "HlL(θ(k−1) ; x) d(k) = ∇lL(θ(k−1) ; x)\n",
      "(6.35)\n",
      "is used to determine the step direction in the kth iteration. A quasi-Newton\n",
      "method uses a matrix eHlL(θ(k−1)) in place of the Hessian HlL(θ(k−1)). (See\n",
      "notes on optimization in Appendix 0.4.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "466\n",
      "6 Statistical Inference Based on Likelihood\n",
      "In terms of the score function, and taking the step length to be 1, equa-\n",
      "tion (6.35) gives the iteration\n",
      "θ(k) = θ(k−1) −\n",
      "\u0010\n",
      "∇sn(θ(k−1) ; x)\n",
      "\u0011−1\n",
      "sn(θ(k−1) ; x).\n",
      "(6.36)\n",
      "Fisher Scoring on the Log-Likelihood\n",
      "In “Fisher scoring”, the Hessian in Newton’s method is replaced by its ex-\n",
      "pected value. The iterates then are\n",
      "bθk+1 = bθk −H−1\n",
      "l\n",
      "(bθk|x)∇l(bθk|x).\n",
      "Example 6.14 Computing the MLE of the parameters in a gamma\n",
      "distribution (continuation of Example 6.6)\n",
      "The likelihood equations for the gamma(α, β) distribution in Example 6.6 led\n",
      "to the two equations\n",
      "bβ = ¯x/bα\n",
      "and\n",
      "log(bα) −Γ′(bα)\n",
      "Γ(bα) +\n",
      "X\n",
      "log(xi)/n −log(¯x) = 0.\n",
      "(6.37)\n",
      "The two unknowns in these equations are separable, so we merely need to\n",
      "solve (6.37) in one unknown. The Hessian and gradient in equation (6.35)\n",
      "or (6.36) are scalars.\n",
      "The function\n",
      "Ψ(α) = Γ′(α)\n",
      "Γ(α)\n",
      "(6.38)\n",
      "is called the psi function or the digamma function (see Olver et al. (2010)).\n",
      "The R software package has a function for evaluation of the digamma function.\n",
      "If Newton’s method (see Appendix 0.4) is to be used to solve equation (6.37),\n",
      "we also need Ψ′(α). This is called the trigamma function, and it is also avail-\n",
      "able in R.\n",
      "To see how we may compute this, let us generate some artiﬁcial data and\n",
      "solve the likelihood equations using the iterations in equation (6.36).\n",
      "#\n",
      "Generate artificial data\n",
      "alpha <- 2\n",
      "beta\n",
      "<- 5\n",
      "n <- 10\n",
      "x <- rgamma(n,alpha,scale=beta)\n",
      "#\n",
      "Define functions to solve likelihood equation\n",
      "sna <- function(meanlog,logmean,a0){\n",
      "log(a0)-digamma(a0)+meanlog-logmean\n",
      "}\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "467\n",
      "snaprime <- function(meanlog,logmean,a0){\n",
      "1/a0 -trigamma(a0)\n",
      "}\n",
      "#\n",
      "Initialize data for algorithm\n",
      "n <- 10\n",
      "meanlog <- sum(log(x))/n\n",
      "logmean <- log(mean(x))\n",
      "#\n",
      "Initialize starting value, set tolerance, loop\n",
      "tol <- 10e-7\n",
      "ak <- 3; akp1 <- ak+3*tol\n",
      "iter <- 100\n",
      "i <- 0\n",
      "while (abs(akp1-ak)>tol&i<iter){\n",
      "i <- i+1\n",
      "ak <- max(tol,akp1)\n",
      "akp1 <- ak -\n",
      "sna(meanlog,logmean,ak)/snaprime(meanlog,logmean,ak)\n",
      "}\n",
      "bk <- mean(x)/ak\n",
      "ak\n",
      "bk\n",
      "i\n",
      "Depending on the sample, this code will converge in 5 to 10 iterations.\n",
      "One-Step MLE\n",
      "Perhaps surprisingly, one iteration of equation (6.36) often gives a good ap-\n",
      "proximation using any “reasonable” starting value. The result of one iteration\n",
      "is called a “one-step MLE”.\n",
      "Example 6.15 Computing the one-step MLEs of the parameters in\n",
      "a gamma distribution (continuation of Example 6.14)\n",
      "#\n",
      "Generate artificial data and initialize for algorithm\n",
      "x <- rgamma(n,alpha,scale=beta)\n",
      "n <- 10\n",
      "meanlog <- sum(log(x))/n\n",
      "logmean <- log(mean(x))\n",
      "#\n",
      "Initialize starting value, set tolerance, loop\n",
      "tol <- 10e-7\n",
      "ak <- 3; akp1 <- ak+3*tol\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "468\n",
      "6 Statistical Inference Based on Likelihood\n",
      "iter <- 100\n",
      "i <- 0\n",
      "while (abs(akp1-ak)>tol&i<iter){\n",
      "i <- i+1\n",
      "ak <- max(tol,akp1)\n",
      "akp1 <- ak -\n",
      "sna(meanlog,logmean,ak)/snaprime(meanlog,logmean,ak)\n",
      "}\n",
      "bk <- mean(x)/ak\n",
      "ak\n",
      "bk\n",
      "ak <- 3; akp1 <- ak+3*tol; ak1 <- akp1\n",
      "akp1 <- ak1 -\n",
      "sna(meanlog,logmean,ak1)/snaprime(meanlog,logmean,ak1)\n",
      "ak1 <- akp1\n",
      "bk1 <- mean(x)/ak1\n",
      "ak1\n",
      "bk1\n",
      "Here are some results from several runs on artiﬁcial data. The one-step MLE\n",
      "is generally close to the MLE.\n",
      "converged 3.017 4.001 4.297 1.687 2.703 2.499 4.955\n",
      "one-step\n",
      "3.017 3.746 3.892 0.584 2.668 2.393 4.161\n",
      "Nondiﬀerentiable Likelihood Functions\n",
      "The deﬁnition of MLEs does not depend on the existence of a likelihood\n",
      "equation. The likelihood function may not be diﬀerentiable with respect to\n",
      "the parameter, as in the following example in which the parameter space is\n",
      "countable.\n",
      "Example 6.16 MLE in the hypergeometric distribution\n",
      "Consider a common problem in quality assurance. A batch of N items contains\n",
      "an unknown number M of defective items. We take a random sample of n items\n",
      "from the batch, and observing that the sample contains x defective items, we\n",
      "wish to estimate M. The likelihood is\n",
      "L(M, N, n; x) =\n",
      "\u0012M\n",
      "x\n",
      "\u0013\u0012N −M\n",
      "n −x\n",
      "\u0013\n",
      "/\n",
      "\u0012N\n",
      "n\n",
      "\u0013\n",
      ",\n",
      "(6.39)\n",
      "over the appropriate ranges of N, M, n, and x, which are all nonnegative\n",
      "integers. This is a single-parameter distribution, because we assume N and n\n",
      "are known due to the setup of the problem. The parameter space is\n",
      "M = {0, 1, . . ., N},\n",
      "with M = 0 yielding a degenerate distribution.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "469\n",
      "The likelihood is not diﬀerentiable in M (it is not even continuous in\n",
      "M), so there is no likelihood equation. The function does have a maximum,\n",
      "however, and so an MLE exits.\n",
      "Even if a function is not diﬀerentiable, we can seek a maximum by iden-\n",
      "tifying a point of change from increasing to decreasing. We approximate a\n",
      "derivative:\n",
      "L(M)\n",
      "L(M −1) =\n",
      "This is larger than 1, that is, the function is increasing, so long as M <\n",
      "(N + 1)x/n and greater than 1 otherwise. Hence, the MLE is\n",
      "c\n",
      "M = ⌈(N + 1)x/n⌉.\n",
      "Note that this is biased. The UMVUE of M is Nx/n.\n",
      "EM Methods\n",
      "Expectation Maximization methods are iterative methods for ﬁnding an MLE.\n",
      "Although EM methods do not rely on missing data, they can be explained\n",
      "most easily in terms of a random sample that consists of two components, one\n",
      "observed and one unobserved or missing. EM methods can also be used for\n",
      "other applications.\n",
      "Missing Data\n",
      "A simple example of missing data occurs in life-testing, when, for example,\n",
      "a number of electrical units are switched on and the time when each fails is\n",
      "recorded.\n",
      "In such an experiment, it is usually necessary to curtail the recordings\n",
      "prior to the failure of all units.\n",
      "The failure times of the units still working are unobserved, but the num-\n",
      "ber of censored observations and the time of the censoring obviously provide\n",
      "information about the distribution of the failure times.\n",
      "Mixtures\n",
      "Another common example that motivates the EM algorithm is a ﬁnite mixture\n",
      "model.\n",
      "Each observation comes from an unknown one of an assumed set of distri-\n",
      "butions. The missing data is the distribution indicator.\n",
      "The parameters of the distributions are to be estimated. As a side beneﬁt,\n",
      "the class membership indicator is estimated.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "470\n",
      "6 Statistical Inference Based on Likelihood\n",
      "Applications of EM Methods\n",
      "The missing data can be missing observations on the same random variable\n",
      "that yields the observed sample, as in the case of the censoring example; or the\n",
      "missing data can be from a diﬀerent random variable that is related somehow\n",
      "to the random variable observed.\n",
      "Many common applications of EM methods involve missing-data problems,\n",
      "but this is not necessary.\n",
      "Often, an EM method can be constructed based on an artiﬁcial “missing”\n",
      "random variable to supplement the observable data.\n",
      "Example 6.17 MLE in a multinomial model\n",
      "One of the simplest examples of the EM method was given by Dempster et al.\n",
      "(1977).\n",
      "Consider the multinomial distribution with four outcomes, that is, the\n",
      "multinomial with probability function,\n",
      "p(x1, x2, x3, x4) =\n",
      "n!\n",
      "x1!x2!x3!x4!πx1\n",
      "1 πx2\n",
      "2 πx3\n",
      "3 πx4\n",
      "4 ,\n",
      "with n = x1+x2 +x3 +x4 and 1 = π1+π2 +π3 +π4. Suppose the probabilities\n",
      "are related by a single parameter, θ, with 0 ≤θ ≤1:\n",
      "π1 = 1\n",
      "2 + 1\n",
      "4θ\n",
      "π2 = 1\n",
      "4 −1\n",
      "4θ\n",
      "π3 = 1\n",
      "4 −1\n",
      "4θ\n",
      "π4 = 1\n",
      "4θ.\n",
      "Given an observation (x1, x2, x3, x4), the log-likelihood function is\n",
      "l(θ) = x1 log(2 + θ) + (x2 + x3) log(1 −θ) + x4 log(θ) + c\n",
      "and\n",
      "dl(θ)/dθ =\n",
      "x1\n",
      "2 + θ −x2 + x3\n",
      "1 −θ\n",
      "+ x4\n",
      "θ .\n",
      "The objective is to estimate θ.\n",
      "Dempster, Laird, and Rubin used n = 197 and x = (125, 18, 20, 34). (For\n",
      "this simple problem, the MLE of θ can be determined by solving a simple\n",
      "polynomial equation, but let’s proceed with an EM formulation.)\n",
      "To use the EM algorithm on this problem, we can think of a multinomial\n",
      "with ﬁve classes, which is formed from the original multinomial by splitting\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "471\n",
      "the ﬁrst class into two with associated probabilities 1/2 and θ/4. The original\n",
      "variable x1 is now the sum of u1 and u2. Under this reformulation, we now\n",
      "have a maximum likelihood estimate of θ by considering u2 + x4 (or x2 + x3)\n",
      "to be a realization of a binomial with n = u2 + x4 + x2 + x3 and π = θ\n",
      "(or 1 −θ). However, we do not know u2 (or u1). Proceeding as if we had a\n",
      "ﬁve-outcome multinomial observation with two missing elements, we have the\n",
      "log-likelihood for the complete data,\n",
      "lc(θ) = (u2 + x4) log(θ) + (x2 + x3) log(1 −θ),\n",
      "and the maximum likelihood estimate for θ is\n",
      "u2 + x4\n",
      "u2 + x2 + x3 + x4\n",
      ".\n",
      "The E-step of the iterative EM algorithm ﬁlls in the missing or unobserv-\n",
      "able value with its expected value given a current value of the parameter, θ(k),\n",
      "and the observed data. Because lc(θ) is linear in the data, we have\n",
      "E (lc(θ)) = E(u2 + x4) log(θ) + E(x2 + x3) log(1 −θ).\n",
      "Under this setup, with θ = θ(k),\n",
      "Eθ(k)(u2) = 1\n",
      "4x1θ(k) /\n",
      "\u00121\n",
      "2 + 1\n",
      "4x1θ(k)\n",
      "\u0013\n",
      "= u(k)\n",
      "2 .\n",
      "We now maximize Eθ(k) (lc(θ)). This maximum occurs at\n",
      "θ(k+1) = (u(k)\n",
      "2\n",
      "+ x4)/(u(k)\n",
      "2\n",
      "+ x2 + x3 + x4).\n",
      "The following R statements execute a single iteration, after tk has been\n",
      "initialized to some value between 0 and 1.\n",
      "u2kp1 <- x[1]*tk/(2+tk)\n",
      "tk <- (u2kp1 + x[4])/(sum(x)-x[1]+u2kp1)\n",
      "Within just a few iterations, tk settles to approximately 0.62682.\n",
      "Example 6.18 MLE in a variation of the life-testing experiment\n",
      "Consider an experiment described by Flury and Zopp`e (2000). It is assumed\n",
      "that the lifetime of light bulbs follows the exponential distribution with mean\n",
      "θ. To estimate θ, n light bulbs were tested until they all failed. Their failure\n",
      "times were recorded as x1, . . ., xn. In a separate experiment, m bulbs were\n",
      "tested, but the individual failure times were not recorded. Only the number\n",
      "of bulbs, r, that had failed at time t was recorded. This is a slightly diﬀerent\n",
      "setup as in Example 6.3.\n",
      "The missing data are the failure times of the bulbs in the second experi-\n",
      "ment, u1, . . ., um. We have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "472\n",
      "6 Statistical Inference Based on Likelihood\n",
      "lc(θ ; x, u) = −n(log θ + ¯x/θ) −\n",
      "m\n",
      "X\n",
      "i=1\n",
      "(log θ + ui/θ).\n",
      "The expected value for a bulb still burning is\n",
      "t + θ\n",
      "and the expected value of one that has burned out is\n",
      "θ −\n",
      "te−t/θ(k)\n",
      "1 −e−t/θ(k) .\n",
      "Therefore, using a provisional value θ(k), and the fact that r out of m\n",
      "bulbs have burned out, we have EU|x,θ(k)(lc) as\n",
      "q(k)(x, θ) = −(n + m) log(θ)\n",
      "−1\n",
      "θ\n",
      "\u0010\n",
      "n¯x + (m −r)(t + θ(k)) + r(θ(k) −th(k))\n",
      "\u0011\n",
      ",\n",
      "where h(k) is given by\n",
      "h(k) =\n",
      "e−t/θ(k)\n",
      "1 −e−t/θ(k) .\n",
      "The kth M step determines the maximum with respect to the variable θ,\n",
      "which, given θ(k), occurs at\n",
      "θ(k+1) =\n",
      "1\n",
      "n + m\n",
      "\u0010\n",
      "n¯x + (m −r)(t + θ(k)) + r(θ(k) −th(k))\n",
      "\u0011\n",
      ".\n",
      "(6.40)\n",
      "Starting with a positive number θ(0), equation (6.40) is iterated until conver-\n",
      "gence. The expectation q(k) does not need to be updated explicitly.\n",
      "To see how this works, let’s generate some artiﬁcial data and try it out.\n",
      "Some R code to implement this is:\n",
      "# Generate data from the exponential with theta=2,\n",
      "# and with the second experiment truncated at t=3.\n",
      "# Note that R uses a form of the exponential in\n",
      "# which the parameter is a multiplier; i.e., the R\n",
      "# parameter is 1/theta.\n",
      "# Set the seed, so computations are reproducible.\n",
      "set.seed(4)\n",
      "n <- 100\n",
      "m <- 500\n",
      "theta <- 2\n",
      "t <- 3\n",
      "x <- rexp(n,1/theta)\n",
      "r<-min(which(sort(rexp(m,1/theta))>=3))-1\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "473\n",
      "Some R code to implement the EM algorithm:\n",
      "# We begin with\n",
      "theta=1.\n",
      "# (Note theta.k is set to theta.kp1 at\n",
      "#\n",
      "the beginning of the loop.)\n",
      "theta.k<-.01\n",
      "theta.kp1<-1\n",
      "# Do some preliminary computations.\n",
      "n.xbar<-sum(x)\n",
      "# Then loop and test for convergence\n",
      "theta.k <- theta.kp1\n",
      "theta.kp1 <- (n.xbar +\n",
      "(m-r)*(t+theta.k) +\n",
      "r*(theta.k-\n",
      "t*exp(-t/theta.k)/(1-exp(-t/theta.k))\n",
      ")\n",
      ")/(n+m)\n",
      "The value of θ stabilizes to less than 0.1% change at 1.912 in 6 iterations.\n",
      "This example is interesting because if we assume that the distribution of\n",
      "the light bulbs is uniform, U(0, θ) (such bulbs are called “heavybulbs”!), the\n",
      "EM algorithm cannot be applied.\n",
      "Maximum likelihood methods must be used with some care whenever the\n",
      "range of the distribution depends on the parameter.\n",
      "In this case, however, there is another problem. It is in computing\n",
      "q(k)(x, θ), which does not exist for θ < θ(k−1).\n",
      "Example 6.19 MLE in a normal mixtures model\n",
      "A two-component normal mixture model can be deﬁned by two normal distri-\n",
      "butions, N(µ1, σ2\n",
      "1) and N(µ2, σ2\n",
      "2), and the probability that the random variable\n",
      "(the observable) arises from the ﬁrst distribution is w.\n",
      "The parameter in this model is the vector θ = (w, µ1, σ2\n",
      "1, µ2, σ2\n",
      "2). (Note\n",
      "that w and the σs have the obvious constraints.)\n",
      "The pdf of the mixture is\n",
      "p(y; θ) = wp1(y; µ1, σ2\n",
      "1) + (1 −w)p2(y; µ2, σ2\n",
      "2),\n",
      "where pj(y; µj, σ2\n",
      "j ) is the normal pdf with parameters µj and σ2\n",
      "j . (I am\n",
      "just writing them this way for convenience; p1 and p2 are actually the same\n",
      "parametrized function of course.)\n",
      "In the standard formulation with C = (X, U), X represents the observed\n",
      "data, and the unobserved U represents class membership.\n",
      "Let U = 1 if the observation is from the ﬁrst distribution and U = 0 if the\n",
      "observation is from the second distribution.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "474\n",
      "6 Statistical Inference Based on Likelihood\n",
      "The unconditional E(U) is the probability that an observation comes from\n",
      "the ﬁrst distribution, which of course is w.\n",
      "Suppose we have n observations on X, x1, . . ., xn.\n",
      "Given a provisional value of θ, we can compute the conditional expected\n",
      "value E(U|x) for any realization of X. It is merely\n",
      "E(U|x, θ(k)) =\n",
      "w(k)p1(x; µ(k)\n",
      "1 , σ2(k)\n",
      "1\n",
      ")\n",
      "p(x; w(k), µ(k)\n",
      "1 , σ2(k)\n",
      "1\n",
      ", µ(k)\n",
      "2 , σ2(k)\n",
      "2\n",
      ")\n",
      "The M step is just the familiar MLE of the parameters:\n",
      "w(k+1) = 1\n",
      "n\n",
      "X\n",
      "E(U|xi, θ(k))\n",
      "µ(k+1)\n",
      "1\n",
      "=\n",
      "1\n",
      "nw(k+1)\n",
      "X\n",
      "q(k)(xi, θ(k))xi\n",
      "σ2(k+1)\n",
      "1\n",
      "=\n",
      "1\n",
      "nw(k+1)\n",
      "X\n",
      "q(k)(xi, θ(k))(xi −µ(k+1)\n",
      "1\n",
      ")2\n",
      "µ(k+1)\n",
      "2\n",
      "=\n",
      "1\n",
      "n(1 −w(k+1))\n",
      "X\n",
      "q(k)(xi, θ(k))xi\n",
      "σ2(k+1)\n",
      "2\n",
      "=\n",
      "1\n",
      "n(1 −w(k+1))\n",
      "X\n",
      "q(k)(xi, θ(k))(xi −µ(k+1)\n",
      "2\n",
      ")2\n",
      "**** variations **** relate to AOV\n",
      "****** rewrite all this:\n",
      "In maximum likelihood estimation, the objective function is the likelihood,\n",
      "LX(θ; x) or the log-likelihood, lX(θ; x). (Recall that a likelihood depends on\n",
      "a known distributional form for the data; that is why we use the notation\n",
      "LX(θ; x) and lX(θ; x), where “X” represents the random variable of the dis-\n",
      "tribution.)\n",
      "The variable for the optimization is θ; thus in an iterative algorithm, we\n",
      "ﬁnd θ(1), θ(2), . . ..\n",
      "One type of alternating method is based on conditional optimization\n",
      "and a conditional bounding function alternates between updating θ(k) using\n",
      "maximum likelihood and conditional expected values. This method is called\n",
      "the EM method because the alternating steps involve an expectation and a\n",
      "maximization.\n",
      "Given θ(k−1) we seek a function qk(x, θ) that has a known relationship\n",
      "with lX(θ; x), and then we determine θ(k) to maximize qk(x, θ) (subject to\n",
      "any constraints on acceptable values of θ).\n",
      "The minorizing function qk(x, θ) is formed as a conditional expectation\n",
      "of a joint likelihood. In addition to the data we have observed, call it X, we\n",
      "assume we have some unobserved data U.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "475\n",
      "Thus, we have “complete” data C = (X, U) given the actual observed data\n",
      "X, and the other component, U, of C that is not observed.\n",
      "Let LC(θ; c) be the likelihood of the complete data, and let LX(θ; x) be the\n",
      "likelihood of the observed data, with similar notation for the log-likelihoods.\n",
      "We refer to LC(θ; c) as the “complete likelihood”.\n",
      "There are thus two likelihoods, one based on the complete (but unknown)\n",
      "sample, and one based only on the observed sample.\n",
      "We wish to estimate the parameter θ, which ﬁgures in the distribution of\n",
      "both components of C.\n",
      "The conditional likelihood of C given X is\n",
      "LC|X(θ; c|x) = LC(θ; x, u)/LX(θ; x),\n",
      "or\n",
      "lC|X(θ; c|x) = lC(θ; x, u) −lX(θ; x).\n",
      "Note that the conditional of C given X is the same as the conditional of\n",
      "U given X, and we may write it either way, either C|X or U|X.\n",
      "Because we do not have all the observations, LC|X(θ; c|x) and LC(θ; c)\n",
      "have\n",
      "•\n",
      "unknown variables (the unobserved U)\n",
      "•\n",
      "the usual unknown parameter.\n",
      "Hence, we cannot follow the usual approach of maximizing the likelihood with\n",
      "given data.\n",
      "We concentrate on the unobserved or missing data ﬁrst.\n",
      "We use a provisional value of θ(k−1) to approximate the complete likelihood\n",
      "based on the expected value of U given X = x.\n",
      "The expected value of the likelihood, which will generally be a function of\n",
      "both θ and θ(k−1), minorizes the objective function of interest, LX(θ; x), as\n",
      "we will see.\n",
      "We then maximize this minorizing function with respect to θ to get θ(k).\n",
      "Let LC(θ ; x, u) and lC(θ ; x, u) denote, respectively, the likelihood and\n",
      "the log-likelihood for the complete sample. The objective function, that is, the\n",
      "likelihood for the observed X, is\n",
      "LX(θ ; x) =\n",
      "Z\n",
      "LC(θ ; x, u) du,\n",
      "and lX(θ ; x) = logLX(θ ; x).\n",
      "After representing the function of interest, LX(θ ; x), as an integral, the\n",
      "problem is to determine this function; that is, to average over U. (This is what\n",
      "the integral does, but we do not know what to integrate.) The average over\n",
      "U is the expected value with respect to the marginal distribution of U.\n",
      "This is a standard problem in statistics: we estimate an expectation using\n",
      "observed data.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "476\n",
      "6 Statistical Inference Based on Likelihood\n",
      "In this case, however, even the values that we average to estimate the\n",
      "expectation depends on θ, so we use a provisional value of θ.\n",
      "We begin with a provisional value of θ, call it θ(0).\n",
      "Given any provisional value θ(k−1), we will compute a provisional value\n",
      "θ(k) that increases (or at least does not decrease) the conditional expected\n",
      "value of the complete likelihood.\n",
      "The EM approach to maximizing LX(θ ; x) has two alternating steps. The\n",
      "steps are iterated until convergence.\n",
      "E step : compute qk(x, θ) = EU|x,θ(k−1)\n",
      "\u0000lC(θ; x, U)\n",
      "\u0001\n",
      ".\n",
      "M step : determine θ(k) to maximize qk(x, θ), or at least to increase it (subject\n",
      "to any constraints on acceptable values of θ).\n",
      "Convergence of the EM Method\n",
      "Is lX(θ(k); x) ≥lX(θ(k−1); x)?\n",
      "(If it is, of course, then LX(θ(k); x) ≥LX(θ(k−1); x), because the log is\n",
      "monotone increasing.)\n",
      "The sequence θ(1), θ(2), . . . converges to a local maximum of the observed-\n",
      "data likelihood L(θ ; x) under fairly general conditions. (It can be very slow\n",
      "to converge, however.)\n",
      "Why EM Works\n",
      "The real issue is whether the EM sequence\n",
      "{θ(k)} →arg max\n",
      "θ\n",
      "lX(θ; x)\n",
      "(= arg max\n",
      "θ\n",
      "LX(θ; x)).\n",
      "If lX(·) is bounded (and it better be!), this is essentially equivalent to to asking\n",
      "if\n",
      "lX(θ(k); x) ≥lX(θ(k−1); x).\n",
      "(So long as in a suﬃcient number of steps the inequality is strict.)\n",
      "Using an equation from before, we ﬁrst write\n",
      "lX(θ; X) = lC(θ; (X, U)) −lU|X(θ; U|X),\n",
      "and then take the conditional expectation of functions of U given x and under\n",
      "the assumption that θ has the provisional value θ(k−1):\n",
      "lX(θ; X) = EU|x,θ(k−1)\n",
      "\u0000lC(θ; (x, U))\u0001 −EU|x,θ(k−1)\n",
      "\u0000lU|X(θ; U|x)\u0001\n",
      "= qk(x, θ) −hk(x, θ),\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "477\n",
      "where\n",
      "hk(x, θ) = EU|x,θ(k−1)\n",
      "\u0000lU|X(θ; U|x)\n",
      "\u0001\n",
      ".\n",
      "Now, consider\n",
      "lX(θ(k); X) −lX(θ(k−1); X).\n",
      "This has two parts:\n",
      "qk(x, θ(k)) −qk(x, θ(k−1))\n",
      "and\n",
      "−\n",
      "\u0010\n",
      "hk(x, θ(k)) −hk(x, θ(k−1))\n",
      "\u0011\n",
      ".\n",
      "The ﬁrst part is nonnegative from the M part of the kth step.\n",
      "What about the second part? We will show that it is nonnegative also (or\n",
      "without the minus sign it is nonpositive).\n",
      "For the other part, for given θ(k−1) and any θ, ignoring the minus sign,\n",
      "...\n",
      "hk(x, θ) −hk(x, θ(k−1))\n",
      "= EU|x,θ(k−1)\n",
      "\u0000lU|X(θ; U|x)\n",
      "\u0001\n",
      "−EU|x,θ(k−1)\n",
      "\u0000lU|X(θ(k−1); U|x)\n",
      "\u0001\n",
      "= EU|x,θ(k−1)\n",
      "\u0000log\n",
      "\u0000LU|x(θ; U|x)/LU|X(θ(k−1); U|x)\n",
      "\u0001\u0001\n",
      "≤log\n",
      "\u0000EU|x,θ(k−1)\n",
      "\u0000LU|X(θ; U|x)/LU|X(θ(k−1); U|x)\n",
      "\u0001\u0001\n",
      "(by Jensen′s inequality)\n",
      "= log\n",
      "Z\n",
      "D(U)\n",
      "LU|X(θ; U|x)\n",
      "LU|X(θ(k−1); U|x) LU|X(θ(k−1); U|x) du\n",
      "= log\n",
      "Z\n",
      "D(U)\n",
      "LU|X(θ; U|x) du\n",
      "= 0.\n",
      "So the second term is also nonnegative, and hence,\n",
      "lX(θ(k); x) ≥lX(θ(k−1); x).\n",
      "A Minorizing Function in EM Algorithms\n",
      "With lX(θ; x) = qk(x, θ) −hk(x, θ), and hk(x, θ) ≤hk(x, θ(k−1)) from the\n",
      "previous pages, we have\n",
      "lX(θ(k−1); x) −qk(x, θ(k−1)) ≤lX(θ; x) −qk(x, θ);\n",
      "and so\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "478\n",
      "6 Statistical Inference Based on Likelihood\n",
      "qk(x, θ) + c(x, θ(k−1)) ≤lX(θ; x),\n",
      "where c(x, θ(k−1)) is constant with respect to θ.\n",
      "Therefore for given θ(k−1) and any x,\n",
      "g(θ) = lX(θ(k−1); X) −qk(x, θ(k−1))\n",
      "is a minorizing function for lX(θ; x).\n",
      "Alternative Ways of Performing the Computations\n",
      "There are two kinds of computations that must be performed in each iteration:\n",
      "•\n",
      "E step : compute qk(x, θ) = EU|x,θ(k−1)\n",
      "\u0000lc(θ; x, U)\n",
      "\u0001\n",
      ".\n",
      "•\n",
      "M step : determine θ(k) to maximize qk(x, θ), subject to any constraints\n",
      "on acceptable values of θ.\n",
      "There are obviously various ways to perform each of these computations.\n",
      "A number of papers since 1977 have suggested speciﬁc methods for the\n",
      "computations.\n",
      "For each speciﬁcation of a method for doing the computations or each little\n",
      "modiﬁcation, a new name is given, just as if it were a new idea:\n",
      "GEM, ECM, ECME, AECM, GAECM, PXEM, MCEM, AEM, EM1, SEM\n",
      "E Step\n",
      "There are various ways the expectation step can be carried out.\n",
      "In the happy case of a “nice” distribution, the expectation can be com-\n",
      "puted in closed form. Otherwise, computing the expectation is a numerical\n",
      "quadrature problem. There are various procedures for quadrature, including\n",
      "Monte Carlo.\n",
      "Some people have called an EM method that uses Monte Carlo to evalu-\n",
      "ate the expectation an MCEM method. (If a Newton-Cotes method is used,\n",
      "however, we do not call it an NCEM method!) The additional Monte Carlo\n",
      "computations add a lot to the overall time required for convergence of the EM\n",
      "method.\n",
      "An additional problem in using Monte Carlo in the expectation step may\n",
      "be that the distribution of C is diﬃcult to simulate. The convergence criterion\n",
      "for optimization methods that involve Monte Carlo generally should be tighter\n",
      "than for deterministic methods.\n",
      "M Step\n",
      "For the maximization step, there are even more choices.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.2 Maximum Likelihood Parametric Estimation\n",
      "479\n",
      "The ﬁrst thing to note, as we mentioned earlier for alternating algorithms\n",
      "generally, is that rather than maximizing qk, we can just require that the\n",
      "overall sequence increase.\n",
      "Dempster et al. (1977) suggested requiring only an increase in the expected\n",
      "value; that is, take θ(k) so that\n",
      "qk(u, θ(k)) ≥qk−1(u, θ(k−1)).\n",
      "They called this a generalized EM algorithm, or GEM. (Even in the paper\n",
      "that introduced the “EM” acronym, another acronym was suggested for a\n",
      "variation.) If a one-step Newton method is used to do this, some people have\n",
      "called this a EM1 method.\n",
      "Meng and Rubin (1993) describe a GEM algorithm in which the M-step\n",
      "is an alternating conditional maximization; that is, if θ = (θ1, θ2), ﬁrst θ(k)\n",
      "1\n",
      "is\n",
      "determined to maximize q subject to the constraint θ2 = θ(k−1)\n",
      "2\n",
      "; then θ(k)\n",
      "2\n",
      "is\n",
      "determined to maximize qk subject to the constraint θ1 = θ(k)\n",
      "1 . This sometimes\n",
      "simpliﬁes the maximization problem so that it can be done in closed form.\n",
      "They call this an expectation conditional maximization method, ECM.\n",
      "Alternate Ways of Terminating the Computations\n",
      "In any iterative algorithm, we must have some way of deciding to terminate\n",
      "the computations. (The generally-accepted deﬁnition of “algorithm” requires\n",
      "that it terminate. In any event, of course, we want the computations to cease\n",
      "at some point.)\n",
      "One way of deciding to terminate the computations is based on conver-\n",
      "gence; if the computations have converged we quit. In addition, we also have\n",
      "some criterion by which we decide to quit anyway.\n",
      "In an iterative optimization algorithm, there are two obvious ways of de-\n",
      "ciding when convergence has occurred. One is when the decision variables\n",
      "(the estimates in MLE) are no longer changing appreciably, and the other is\n",
      "when the value of the objective function (the likelihood) is no longer changing\n",
      "appreciably.\n",
      "Convergence\n",
      "It is easy to think of cases in which the objective function converges, but the\n",
      "decision variables do not. All that is required is that the objective function\n",
      "is ﬂat over a region at its maximum. In statistical terms, the corresponds to\n",
      "unidentiﬁability.\n",
      "The Variance of Estimators Deﬁned by the EM Method\n",
      "As is usual for estimators deﬁned as solutions to optimization problems, we\n",
      "may have some diﬃculty in determining the statistical properties of the esti-\n",
      "mators.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "480\n",
      "6 Statistical Inference Based on Likelihood\n",
      "Louis (1982) suggested a method of estimating the variance-covariance\n",
      "matrix of the estimator by use of the gradient and Hessian of the complete-\n",
      "data log-likelihood, lLc(θ ; u, v). Kim and Taylor (1995) also described ways\n",
      "of estimating the variance-covariance matrix using computations that are part\n",
      "of the EM steps.\n",
      "It is interesting to note that under certain assumptions on the distribution,\n",
      "the iteratively reweighted least squares method can be formulated as an EM\n",
      "method (see Dempster et al. (1980)).\n",
      "Missing Data\n",
      "Although EM methods do not rely on missing data, they can be explained\n",
      "most easily in terms of a random sample that consists of two components, one\n",
      "observed and one unobserved or missing.\n",
      "A simple example of missing data occurs in life-testing, when, for example,\n",
      "a number of electrical units are switched on and the time when each fails is\n",
      "recorded.\n",
      "In such an experiment, it is usually necessary to curtail the recordings\n",
      "prior to the failure of all units.\n",
      "The failure times of the units still working are unobserved, but the num-\n",
      "ber of censored observations and the time of the censoring obviously provide\n",
      "information about the distribution of the failure times.\n",
      "Mixtures\n",
      "Another common example that motivates the EM algorithm is a ﬁnite mixture\n",
      "model.\n",
      "Each observation comes from an unknown one of an assumed set of distri-\n",
      "butions. The missing data is the distribution indicator.\n",
      "The parameters of the distributions are to be estimated. As a side beneﬁt,\n",
      "the class membership indicator is estimated.\n",
      "Applications of EM Methods\n",
      "The missing data can be missing observations on the same random variable\n",
      "that yields the observed sample, as in the case of the censoring example; or the\n",
      "missing data can be from a diﬀerent random variable that is related somehow\n",
      "to the random variable observed.\n",
      "Many common applications of EM methods involve missing-data problems,\n",
      "but this is not necessary.\n",
      "Often, an EM method can be constructed based on an artiﬁcial “missing”\n",
      "random variable to supplement the observable data.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.3 Asymptotic Properties of MLEs, RLEs, and GEE Estimators\n",
      "481\n",
      "6.3 Asymptotic Properties of MLEs, RLEs, and GEE\n",
      "Estimators\n",
      "The argmax of the likelihood function, that is, the MLE of the argument of\n",
      "the likelihood function, is obviously an important statistic.\n",
      "In many cases, a likelihood equation exists, and often in those cases, the\n",
      "MLE is a root of the likelihood equation. In some cases there are roots of the\n",
      "likelihood equation (RLEs) that may or may not be an MLE.\n",
      "6.3.1 Asymptotic Distributions of MLEs and RLEs\n",
      "We recall that asymptotic expectations are deﬁned as expectations in asymp-\n",
      "totic distributions (rather than as limits of expectations). The ﬁrst step in\n",
      "studying asymptotic properties is to determine the asymptotic distribution.\n",
      "Example 6.20 asymptotic distribution of the MLE of the variance\n",
      "in a Bernoulli family\n",
      "In Example 6.8 we determined the MLE of the variance g(π) = π(1 −π) in\n",
      "the Bernoulli family of distributions with parameter π. The MLE of g(π) is\n",
      "Tn = X(1 −X).\n",
      "From Example 1.25 on page 94, we get its asymptotic distributions as\n",
      "√n(g(π) −Tn) →N(0, π(1 −π)(1 −2π)2),\n",
      "if π ̸= 1/2, and if π = 1/2,\n",
      "4n(g(π) −Tn) d→χ2\n",
      "1.\n",
      "6.3.2 Asymptotic Eﬃciency of MLEs and RLEs\n",
      "One of the most important properties of roots of the likelihood equation, given\n",
      "the Le Cam regularity conditions (see page 169), is asymptotic eﬃciency. The\n",
      "regularity conditions are the same as those for Le Cam’s theorem on the\n",
      "countability of supereﬃcient estimators (see page 421). ******************\n",
      "ﬁx\n",
      "For distributions that satisfy the Le Cam regularity conditions (these con-\n",
      "ditions are essentially the FI regularity conditions plus a condition on the\n",
      "FI matrix), there is a nice sequence of the likelihood equation (6.12) that is\n",
      "formed from the sequence of score functions,\n",
      "sn(θ) = ∇lLn(θ ; x).\n",
      "(6.41)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "482\n",
      "6 Statistical Inference Based on Likelihood\n",
      "Theorem 6.4\n",
      "Assume the Le Cam regularity conditions for a family of distributions {Pθ},\n",
      "and let sn(θ) be the score function for a sample of size n. There is a sequence\n",
      "of estimators bθn such that\n",
      "Pr(sn(bθn) = 0) →1,\n",
      "and\n",
      "bθn\n",
      "p→θ.\n",
      "Proof.\n",
      "For a sequence in a Le Cam regular family that satisﬁes the conclusion of\n",
      "Theorem 6.4, there is an even more remarkable fact. We have seen in Theo-\n",
      "rem 6.2 that, with the FI regularity conditions, if there is an eﬃcient estimator,\n",
      "then that estimator is an MLE.\n",
      "Theorem 6.5\n",
      "Assume the Le Cam regularity conditions for a family of distributions {Pθ},\n",
      "and let sn(θ) be the score function for a sample of size n. Any consistent\n",
      "sequence of RLEs, that is, any consistent sequence bθn that satisﬁes\n",
      "sn(bθn) = 0,\n",
      "is asymptotically eﬃcient.\n",
      "Proof.\n",
      "Notice the diﬀerences in Theorems 6.2 and 6.5. Theorem 6.2 for ﬁnite sam-\n",
      "ple eﬃciency requires only the FI regularity conditions for RLEs (or with the\n",
      "additional requirement of a positive deﬁnite information matrix for an MLE),\n",
      "but is predicated on the existence of an eﬃcient estimator. As is often the case\n",
      "in asymptotic eﬃciency, Theorem 6.5 requires the Le Cam regularity condi-\n",
      "tions but it gives a stronger result: consistency yields asymptotic eﬃciency.\n",
      "It is important to be clear on what these theorems say. They apply to\n",
      "RLEs, which may be MLEs of the parameter, which as a variable is the variable\n",
      "of diﬀerentiation, say θ, in the score function. If bθ is the MLE of θ, then by\n",
      "deﬁnition, g(bθ) is the MLE of g(θ). If bθ is asymptotically eﬃcient for estimating\n",
      "θ, that does not mean that g(bθ) is asymptotically eﬃcient for estimating g(θ).\n",
      "Example 6.21 an MLE that is not asymptotically eﬃcient\n",
      "In Example 6.8 we determined that the MLE of the variance g(π) = π(1 −π)\n",
      "in the Bernoulli family of distributions with parameter π is Tn = X(1 −X),\n",
      "and in Example 6.20 we determined its asymptotic distribution.\n",
      "When π ̸= 1/2, we have\n",
      "√n(g(π) −Tn) →N(0, π(1 −π)(1 −2π)2),\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.3 Asymptotic Properties of MLEs, RLEs, and GEE Estimators\n",
      "483\n",
      "and when π = 1/2, we have\n",
      "4n(g(π) −Tn) d→χ2\n",
      "1.\n",
      "Hence, Tn is asymptotically biased.\n",
      "Asymptotic Relative Eﬃciency\n",
      "Remember that the ARE is the ratio of two asymptotic expectations — not\n",
      "the asymptotic expectation of a ratio, and certainly not the limit of a ratio;\n",
      "although of course sometimes these three things are the same.\n",
      "Example 6.22 ARE(MLE, UNVUE) in the exponential family with\n",
      "range dependency\n",
      "In Examples 5.9 and 6.7 we considered the two parameter exponential\n",
      "distribution with Lebesgue PDF\n",
      "θ−1e−(x−α)/θI]α,∞[(x).\n",
      "In Example 5.9, we found the UMVUEs:\n",
      "Tαn = X(1) −\n",
      "1\n",
      "n(n −1)\n",
      "X\n",
      "(Xi −X(1))\n",
      "and\n",
      "Tθn =\n",
      "1\n",
      "n −1\n",
      "X\n",
      "(Xi −X(1)).\n",
      "In Example 6.7, we found the MLEs:\n",
      "bαn = X(1)\n",
      "and\n",
      "bθn = 1\n",
      "n\n",
      "X\n",
      "(Xi −X(1)).\n",
      "The distributions for bθ and Tθ are relatively easy. We worked out the\n",
      "distribution of Tθ in Example 1.18, and bθ is just a scalar multiple of Tθ.\n",
      "Because of the relationship between bθ and Tθ, however, we do not even need\n",
      "the asymptotic distributions.\n",
      "In Example 1.11 we found that the distribution of bα is a two-parameter\n",
      "exponential distribution with parameters α and θ/n; hence,\n",
      "n(X(1) −α) d→exponential(0, θ).\n",
      "Now let us consider the ARE of the MLE to the UMVUE for these two\n",
      "parameters.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "484\n",
      "6 Statistical Inference Based on Likelihood\n",
      "•\n",
      "ARE(MLE,UMVUE) for θ.\n",
      "This is an easy case, because the estimators always diﬀer by the ratio\n",
      "n/(n −1). We do not even need the asymptotic distributions. The ARE is\n",
      "1.\n",
      "•\n",
      "ARE(MLE,UMVUE) for α.\n",
      "We have found the asymptotic distributions of U = bα−α and V = Tα −α,\n",
      "so we just work out the asymptotic expectations of U 2 and V 2. We get\n",
      "E(V 2) = θ and E(U 2) = θ + θ2. Therefore, the ARE is θ/(θ + θ2).\n",
      "6.3.3 Inconsistent MLEs\n",
      "In previous sections, we have seen that sometimes MLEs do not have some\n",
      "statistical properties that we usually expect of good estimators.\n",
      "The discussion in this section has focused on MLEs (or RLEs) that are\n",
      "consistent. It is not necessarily the case that MLEs are consistent, however.\n",
      "The following example is from Romano and Siegel (1986).\n",
      "Example 6.23 rational, irrational estimand\n",
      "Let X1, . . ., Xn be a sample from N(θ, 1). Deﬁne the estimand g(θ) as\n",
      "g(θ) =\n",
      "\u001a−θ if θ is irrational\n",
      "θ if θ is rational.\n",
      "Because Xn is the MLE of θ, g(Xn) is the MLE of g(θ). Now Xn ∼N(θ, 1/n)\n",
      "and so is almost surely irrational; hence, g(Xn) = −Xn a.s. Now, by the\n",
      "SLLN, we have g(Xn) = −θ a.s. Hence, if θ is a rational number ̸= 0, then\n",
      "g(Xn) a.s.\n",
      "→−θ ̸= θ = g(θ).\n",
      "While that example may seem somewhat contrived, consider an example\n",
      "due to Ferguson.\n",
      "Example 6.24 mixtures\n",
      "Let X1, . . ., Xn be a sample from from the distribution with PDF wrt\n",
      "Lebesgue measure\n",
      "pX(x; θ) = (1 −θ)pT(x; θ, δ(θ)) + θpU(x),\n",
      "where θ ∈[0, 1], δ(θ) is a continuous decreasing function of θ with δ(0) = 1\n",
      "and 0 < δ(θ) ≤1 −θ for 0 < θ < 1, and\n",
      "pT(x; θ, δ(θ)) =\n",
      "1\n",
      "δ(θ)\n",
      "\u0012\n",
      "1 −|x −θ|\n",
      "δ(θ)\n",
      "\u0013\n",
      "I[θ−δ(θ),θ+δ(θ)](x)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.3 Asymptotic Properties of MLEs, RLEs, and GEE Estimators\n",
      "485\n",
      "and\n",
      "pU(x) = 1\n",
      "2I[−1,1](x).\n",
      "The distribution is a mixture of a triangular distribution centered on θ and\n",
      "the U(−1, 1) distribution.\n",
      "Note that the densities are continuous in θ for any x and is deﬁned on\n",
      "[0, 1] and therefore an MLE exists.\n",
      "Let bθn = bθ(X1, . . ., Xn) denote any MLE of θ. Now, if θ < 1, then\n",
      "pX(x; θ) ≤(1 −θ)/δ(θ) + θ/2 < 1/δ(θ) + 1\n",
      "2,\n",
      "and so for any α < 1\n",
      "max\n",
      "0≤θ≤α\n",
      "ln(θ)\n",
      "n\n",
      "≤log\n",
      "\u0012 1\n",
      "δ(θ) + 1\n",
      "2\n",
      "\u0013\n",
      "< ∞.\n",
      "Now, if we could choose δ(θ) so that\n",
      "max\n",
      "0≤θ≤1\n",
      "ln(θ)\n",
      "n\n",
      "a.s.\n",
      "→∞,\n",
      "then bθn will eventually be greater than α for any α < 1, and so the MLE is\n",
      "not consistent.\n",
      "So, can we choose such a δ(θ)?\n",
      "Let\n",
      "Mn = max(X1, . . ., Xn),\n",
      "hence Mn\n",
      "a.s.\n",
      "→∞, and\n",
      "max\n",
      "0≤θ≤1\n",
      "ln(θ)\n",
      "n\n",
      "≥ln(Mn)\n",
      "n\n",
      "≥n −1\n",
      "n\n",
      "log\n",
      "\u0012Mn\n",
      "2\n",
      "\u0013\n",
      "+ 1\n",
      "n log\n",
      "\u00121 −Mn\n",
      "δ(Mn)\n",
      "\u0013\n",
      ",\n",
      "and so\n",
      "lim inf\n",
      "n\n",
      "max\n",
      "0≤θ≤1\n",
      "ln(θ)\n",
      "n\n",
      "≥log\n",
      "\u00121\n",
      "2\n",
      "\u0013\n",
      "+ lim inf\n",
      "n\n",
      "log\n",
      "\u00121 −Mn\n",
      "δ(Mn)\n",
      "\u0013\n",
      "a.s.\n",
      "So we need to choose δ(θ) so that the last limit is inﬁnite a.s. Now, ∀θMn\n",
      "a.s.\n",
      "→\n",
      "∞, and the slowest rate is for θ = 1, because that distribution has the smallest\n",
      "mass in a suﬃciently small neighborhood of 1. Therefore, all we need to do is\n",
      "choose δ(θ) →0 as θ →1 fast enough so that the limit is inﬁnite a.s. when\n",
      "θ = 0.\n",
      "So now for 0 < ϵ < 1,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "486\n",
      "6 Statistical Inference Based on Likelihood\n",
      "X\n",
      "n\n",
      "Prθ=0(n1/4(1 −Mn) > ϵ) =\n",
      "X\n",
      "n\n",
      "Prθ=0(Mn < 1 −ϵn−1/4)\n",
      "=\n",
      "X\n",
      "n\n",
      "\u0012\n",
      "1 −ϵ2 n−1/4\n",
      "2\n",
      "\u0013n\n",
      "≤\n",
      "X\n",
      "n\n",
      "exp\n",
      "\u0012\n",
      "−ϵ2 n−1/4\n",
      "2\n",
      "\u0013\n",
      "< ∞.\n",
      "Hence, by the Borel-Cantelli lemma, n1/4(1 −Mn) →0 a.s. Finally, choosing\n",
      "δ(θ) = (1 −θ) exp\n",
      "\u0000−(1 −θ)−4 + 1\n",
      "\u0001\n",
      ",\n",
      "we have a function that satisﬁes the requirements above (it is continuous\n",
      "decreasing with δ(0) = 1 and 0 < δ(θ) ≤1 −θ for 0 < θ < 1) and it is such\n",
      "that\n",
      "1\n",
      "n log\n",
      "\u00121 −Mn\n",
      "δ(Mn)\n",
      "\u0013\n",
      "=\n",
      "1\n",
      "n(1 −Mn)4 −1\n",
      "n\n",
      "a.s.\n",
      "→∞.\n",
      "This says that any MLE of θ must tend to 1 a.s., and so cannot be consistent.\n",
      "In addition to these examples, we recall the Neyman-Scott problem in\n",
      "Example 6.27, where the ordinary MLE of the variance is not consistent,\n",
      "but we were able to reformulate the problem so as to obtain an MLE of the\n",
      "variance that is consistent.\n",
      "6.3.4 Properties of GEE Estimators\n",
      "Consistency of GEE Estimators\n",
      "The roots of a generalized estimating equation\n",
      "sn(γ) = 0\n",
      "often have good asymptotic properties.\n",
      "If the GEE is chosen so that\n",
      "Eθ(sn(θ)) = 0,\n",
      "or else so that the asymptotic expectation of {xn} is zero *****************\n",
      "The class of estimators arising from the generalized estimating equa-\n",
      "tions (3.77) and (3.79), under very general assumptions have an asymptotic\n",
      "normal distribution. This is Theorem 5.13 in MS2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.4 Application: MLEs in Generalized Linear Models\n",
      "487\n",
      "√n(bθn −θ)\n",
      "d→N(0, σ2\n",
      "F),\n",
      "where {bθn} is a sequence of GEE estimators and\n",
      "σ2\n",
      "F =\n",
      "Z\n",
      "(ψ(x, θ))2dF (x)/(ψ′(x, θ))2.\n",
      "6.4 Application: MLEs in Generalized Linear Models\n",
      "6.4.1 MLEs in Linear Models\n",
      "In the case of underlying normal probability distribution, estimation of\n",
      "the mean based on least squares is the same as MLE. Consider a linear\n",
      "model (5.66) as discussed in Section 5.5.1.\n",
      "Example 6.25 MLE in a linear model\n",
      "Let\n",
      "Y = Xβ + E,\n",
      "(6.42)\n",
      "where Y and E are n-vectors with E(E) = 0 and V(E) = σ2In, X is an\n",
      "n × p matrix whose rows are the xT\n",
      "i , and β is the p-vector parameter. In\n",
      "Section 5.5.1 we studied a least squares estimator of β; that is,\n",
      "b∗= arg min\n",
      "b∈B\n",
      "∥Y −Xb∥2\n",
      "(6.43)\n",
      "= (XTX)−XTY.\n",
      "(6.44)\n",
      "Even if X is not of full rank, in which case the least squares estimator is\n",
      "not unique, we found that the least squares estimator has certain optimal\n",
      "properties for estimable functions.\n",
      "Of course at this point, we could not use MLE — we do not have a distri-\n",
      "bution. We could deﬁne a least squares estimator without an assumption on\n",
      "the distribution of Y or E, but for an MLE we need an assumption on the\n",
      "distribution.\n",
      "After we considered the least-squares estimator without a speciﬁc distri-\n",
      "bution, next in Section 5.5.1, we considered the additional assumption in the\n",
      "model that\n",
      "E ∼Nn(0, σ2In),\n",
      "or\n",
      "Y ∼Nn(Xβ, σ2In).\n",
      "In that case, we found that the least squares estimator yielded the unique\n",
      "UMVUE for any estimable function of β and for σ2. Again, let us assume\n",
      "Y ∼Nn(Xβ, σ2In),\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "488\n",
      "6 Statistical Inference Based on Likelihood\n",
      "yielding, for an observed y, the log-likelihood\n",
      "lL(β, σ2 y, X) = −n\n",
      "2 log(2πσ2) −\n",
      "1\n",
      "2σ2 (y −Xβ)T(y −Xβ).\n",
      "(6.45)\n",
      "Maximizing this function with respect to β is the same as minimizing the\n",
      "expression in equation (6.43), and so an MLE of β is the same as a least\n",
      "squares estimator of β.\n",
      "Estimation of σ2, however, is diﬀerent. In the case of least squares estima-\n",
      "tion on page 426, with no speciﬁc assumptions about the distribution of E in\n",
      "the model (6.42), we had no basis for forming an objective function of squares\n",
      "to minimize. With an assumption of normality, however, instead of explicitly\n",
      "forming a least squares problem for estimating σ2, using a least squares es-\n",
      "timator of β, b∗, we merely used the distribution of (y −Xb∗)T(y −Xb∗) to\n",
      "form a UMVUE of σ2,\n",
      "s2 = (Y −Xb∗)T(Y −Xb∗)/(n −r),\n",
      "(6.46)\n",
      "where r = rank(X).\n",
      "In the case of maximum likelihood, we directly determine the value of σ2\n",
      "that maximizes the expression in equation (6.45). This is an easy optimization\n",
      "problem. The solution is\n",
      "bσ2 = (y −X bβ)T(y −X bβ)/n\n",
      "(6.47)\n",
      "where bβ = b∗is an MLE of β. Compare the MLE of σ2 with the least squares\n",
      "estimator, and note that the MLE is biased. Recall that we have encountered\n",
      "these two estimators in the simpler cases of Example 3.13 (MLE) and 5.6\n",
      "(UMVUE). See also equation (3.55).\n",
      "In Examples 5.28, 5.29 and 5.30 (starting on page 434), we considered\n",
      "UMVUE in a special case of the linear model called the ﬁxed-eﬀects one-way\n",
      "AOV model. We now consider MLE in this model.\n",
      "Example 6.26 MLE in the one-way ﬁxed-eﬀects AOV model\n",
      "We consider the model\n",
      "Yij = µ + αi + ϵij,\n",
      "i = 1, . . ., m;\n",
      "j = 1, . . ., n.\n",
      "(6.48)\n",
      "For least squares or UMVUE we do not need to assume any particular distri-\n",
      "bution; all we need assume is that E(ϵij) = 0 and V(ϵij) = σ2 for all i, j, and\n",
      "Cov(ϵij, ϵi′j′) = 0 if i ̸= i′ or j ̸= j′. For MLE, however, we need to assume\n",
      "a distribution, and so we will assume each ϵij has a normal distribution with\n",
      "the additional assumptions about expected values.\n",
      "Proceeding to write the likelihood under the normal assumption, we see\n",
      "that an MLE is bβ = (XTX)−XTY for any generalized inverse of (XTX),\n",
      "which is the same as the least squares estimator obtained in equation (5.73).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.4 Application: MLEs in Generalized Linear Models\n",
      "489\n",
      "Example 6.27 ML estimation of the variance in the one-way ﬁxed-\n",
      "eﬀects AOV model\n",
      "In Example 5.30, we assumed a normal distribution for the residuals, and\n",
      "obtained the distribution of the sum of squares\n",
      "SSE =\n",
      "m\n",
      "X\n",
      "i=1\n",
      "n\n",
      "X\n",
      "j=1\n",
      "(Yij −Y i)2,\n",
      "and from that we obtained the UMVUE of σ2 as SSE/m(n −1).\n",
      "From maximization of the likelihood, we obtain the MLE of σ2 as\n",
      "c\n",
      "σ2 =\n",
      "1\n",
      "nm\n",
      "m\n",
      "X\n",
      "i=1\n",
      "n\n",
      "X\n",
      "j=1\n",
      "(Yij −Y i)2\n",
      "(6.49)\n",
      "(exercise).\n",
      "While the MLE of σ2 is consistent in mean squared error as n →∞\n",
      "and m remains ﬁxed, it is not consistent as m →∞and n remains ﬁxed\n",
      "(Exercise 6.3).\n",
      "There are interesting ways of getting around the lack of consistency of the\n",
      "variance estimator in Example 6.27. In the next example, we will illustrate\n",
      "an approach that is a simple use of a more general method called REML, for\n",
      "“residual maximum likelihood” (also called “restricted maximum likelihood”).\n",
      "Example 6.28 REML estimation of the variance in the one-way\n",
      "ﬁxed-eﬀects AOV model\n",
      "In the preceding examples suppose there are only two observations per group;\n",
      "that is, the model is\n",
      "Yij = µ + αi + ϵij,\n",
      "i = 1, . . ., m;\n",
      "j = 1, 2,\n",
      "with all of the other assumptions made above.\n",
      "The MLE of σ2 in equation (6.49) can be written as\n",
      "c\n",
      "σ2 =\n",
      "1\n",
      "4m\n",
      "m\n",
      "X\n",
      "i=1\n",
      "2\n",
      "X\n",
      "j=1\n",
      "(Yi1 −Yi2)2.\n",
      "(6.50)\n",
      "We see that the limiting expectation of c\n",
      "σ2 as m →∞is σ/2; that is, the\n",
      "estimator is not consistent. (This particular setup is called the “Neyman-Scott\n",
      "problem”. In a ﬁxed sample, of course, the estimator is biased, and there is\n",
      "no reason to expect any change unless n instead of m were to increase.)\n",
      "We see that the problem is caused by the unknown means, and as m\n",
      "increases the number of unknown parameters increases linearly in m. We can,\n",
      "however, reformulate the problem so as to focus on σ2. For i = 1, . . ., m,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "490\n",
      "6 Statistical Inference Based on Likelihood\n",
      "let Zi = Yi1 −Yi2. Now, using Zi, the likelihood is based on the N(0, 2σ2)\n",
      "distribution. Under the likelihood for this setup, which we call REML, we get\n",
      "the maximum likelihood estimate\n",
      "c\n",
      "σ2REML =\n",
      "1\n",
      "2m\n",
      "m\n",
      "X\n",
      "i=1\n",
      "2\n",
      "X\n",
      "j=1\n",
      "(Zi)2.\n",
      "(6.51)\n",
      "which is consistent in m for σ2.\n",
      "In the next example, we will consider a random-eﬀects model.\n",
      "Example 6.29 MLE in the one-way random-eﬀects AOV model\n",
      "Consider the linear model in Example 5.31 on page 436,\n",
      "Yij = µ + δi + ϵij,\n",
      "i = 1, . . ., m;\n",
      "j = 1, . . ., n,\n",
      "(6.52)\n",
      "where the δi are identically distributed with E(δi) = 0, V(δi) = σ2\n",
      "δ, and\n",
      "Cov(δi, δ˜i) = 0 for i ̸= ˜i, and the ϵij are independent of the δi and are\n",
      "identically distributed with with E(ϵij) = 0, V(ϵij) = σ2\n",
      "ϵ, and Cov(ϵij, ϵ˜i˜j) = 0\n",
      "for either i ̸= ˜i or j ̸= ˜j.\n",
      "In order to use a likelihood approach, of course, we need to make assump-\n",
      "tions about the distributions of the random variables. Let us suppose now\n",
      "that δi\n",
      "iid\n",
      "∼N(0, σ2\n",
      "δ), where σ2\n",
      "δ ≥0, and ϵij\n",
      "iid\n",
      "∼N(0, σ2\n",
      "ϵ), where as usual σ2 > 0.\n",
      "Our interest in using the model is to make inference on the relative sizes\n",
      "of the components of the variance σ2\n",
      "δ and σ2\n",
      "ϵ.\n",
      "In Example 5.31, we obtained the UMVUEs of σ2\n",
      "δ and σ2\n",
      "ϵ, and noted that\n",
      "the unbiased estimator of σ2\n",
      "δ may be negative.\n",
      "Now we consider the MLE of σ2\n",
      "δ and σ2\n",
      "ϵ. In the case of the model (6.52) with\n",
      "the assumption of normality and independence, using the PDF obtained in Ex-\n",
      "ample exa:onewayAOVmodel2, it is relatively easy to write the log-likelihood,\n",
      "lL(µ, σ2\n",
      "δ, σ2\n",
      "ϵ ; y) = −1\n",
      "2\n",
      "\u0000mn log(2π) + m(n −1) log(σ2\n",
      "ϵ) + m log(σ2\n",
      "δ)\n",
      "+SSE/σ2\n",
      "ϵ + SSA/(σ2\n",
      "ϵ + nσ2\n",
      "δ) + mn(¯y −µ)2/(σ2\n",
      "ϵ + nσ2\n",
      "δ)\n",
      "\u0001\n",
      ".\n",
      "(6.53)\n",
      "The MLEs must be in the closure of the parameter space, which for σ2\n",
      "δ and\n",
      "σ2\n",
      "ϵ is ¯IR+. From this we have the MLEs\n",
      "•\n",
      "if (m −1)MSA/m ≥MSE\n",
      "bσ2\n",
      "δ = 1\n",
      "n\n",
      "\u0012m −1\n",
      "m\n",
      "MSA −MSE\n",
      "\u0013\n",
      "(6.54)\n",
      "bσ2\n",
      "ϵ = MSE\n",
      "(6.55)\n",
      "•\n",
      "if (m −1)MSA/m < MSE\n",
      "bσ2\n",
      "δ = 0\n",
      "(6.56)\n",
      "bσ2\n",
      "ϵ = m −1\n",
      "m\n",
      "MSE.\n",
      "(6.57)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.4 Application: MLEs in Generalized Linear Models\n",
      "491\n",
      "Again we should note that we are dealing with a special model in Exam-\n",
      "ple 6.29; it is “balanced”; that is, for each i, there is a constant number of\n",
      "j’s. If, instead, we had j = 1, . . ., ni, we would not be able to write out the\n",
      "log-likelihood so easily, and the MLEs would be very diﬃcult to determine.\n",
      "6.4.2 MLEs in Generalized Linear Models\n",
      "Regression models such as (3.5)\n",
      "Y = f(X ; θ) + E\n",
      "are very useful in statistical applications. In this form, we assume indepen-\n",
      "dent observations (Y1, x1), . . ., (Yn, xn) and Y to represent an n-vector, X to\n",
      "represent an n × p matrix whose rows are the xT\n",
      "i , and E to represent an\n",
      "unobservable n-vector of random variables∼Pτ, with unknown Pτ, but with\n",
      "E(E) = 0 and V(E) = σ2I.\n",
      "The expression “f(·)” represents a systematic eﬀect related to the values\n",
      "of “X”, and “E” represents a random eﬀect, an unexplained eﬀect, or simply\n",
      "a “residual” that is added to the systematic eﬀect.\n",
      "A model in which the parameters are additively separable and with an\n",
      "additive random eﬀect is sometimes called an additive model:\n",
      "Y = f(X)θ + ϵ.\n",
      "A simple version of this is called a linear (additive) model:\n",
      "Y = Xβ + ϵ,\n",
      "(6.58)\n",
      "where β is a p-vector of parameters. We have considered speciﬁc instances of\n",
      "this model in Examples 6.25 and 5.31.\n",
      "Either form of the additive model can be generalized with a “link function”\n",
      "to be a generalized additive model.\n",
      "In the following, we will concentrate on the linear model, Y = Xβ +ϵ, and\n",
      "we will discuss the link function and the generalization of the linear model,\n",
      "which is called a generalized linear model (GLM or GLIM).\n",
      "Let us assume that the distribution of the residual has a ﬁrst moment and\n",
      "that it is known. In that case, we can take its mean to be 0, otherwise, we can\n",
      "incorporate it into Xβ. (If the ﬁrst moment does not exist, we can work with\n",
      "the median.) Hence, assuming the mean of the residual exists, the model can\n",
      "be written as\n",
      "E(Y ) = Xβ,\n",
      "that is, the expected value of Y is the systematic eﬀect in the model. More\n",
      "generally, we can think of the model as being a location family with PDF\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "492\n",
      "6 Statistical Inference Based on Likelihood\n",
      "pϵ(ϵ) = pϵ(y −Xβ),\n",
      "(6.59)\n",
      "wrt a given σ-ﬁnite measure.\n",
      "In the linear model (6.58), if ϵ ∼N(0, σ2), as we usually assume, we can\n",
      "easily identify ηi, T(yi), and ζ(ηi) in equation (6.32), and of course, h(yi) ≡1.\n",
      "This is a location-scale family.\n",
      "Generalized Linear Models\n",
      "A model as in equation (6.58) has limitations. Suppose, for example, that we\n",
      "are interested in modeling a response that is binary, for example, two states of\n",
      "a medical patient, “diseased” or “disease-free”. As usual, we set up a random\n",
      "variable to map the sample space to IR:\n",
      "Y : {disease-free,diseased} 7→{0, 1}.\n",
      "The linear model X = Zβ + ϵ does not make sense. It is continuous and\n",
      "unbounded.\n",
      "A more useful model may address Pr(X = 0).\n",
      "To make this more concrete, consider the situation in which several groups\n",
      "of subjects are each administered a given dose of a drug, and the number\n",
      "responding in each group is recorded. The data consist of the counts yi re-\n",
      "sponding in the ith group, which received a level xi of the drug.\n",
      "A basic model is\n",
      "P(Yi = 0|xi) = 1 −πi\n",
      "P(Yi = 1|xi) = πi\n",
      "(6.60)\n",
      "The question is how does π depend on x?\n",
      "A linear dependence, π = β0+β1x does not ﬁt well in this kind of situation\n",
      "– unless we impose restrictions, π would not be between 0 and 1.\n",
      "We can try a transformation to [0, 1].\n",
      "Suppose we impose an invertible function on\n",
      "η = β0 + β1x\n",
      "that will map it into [0, 1]:\n",
      "π = h(η),\n",
      "(6.61)\n",
      "or\n",
      "g(π) = η.\n",
      "(6.62)\n",
      "We call this a link function.\n",
      "A common model following this setup is\n",
      "πx = Φ(β0 + β1x),\n",
      "(6.63)\n",
      "where Φ is the normal cumulative distribution function, and β0 and β1 are\n",
      "unknown parameters to be estimated. This is called a probit model. The link\n",
      "function in this case is Φ−1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.4 Application: MLEs in Generalized Linear Models\n",
      "493\n",
      "The related logit model, in which the log odds ratio log(π/(1 −π)) is of\n",
      "interest, has as link function\n",
      "η = log\n",
      "\u0012\n",
      "π\n",
      "1 −π\n",
      "\u0013\n",
      ".\n",
      "(6.64)\n",
      "Other possibilities are the complementary log-log function\n",
      "η = log(−log(1 −π)),\n",
      "(6.65)\n",
      "and the log-log function,\n",
      "η = −log(−log(π)).\n",
      "(6.66)\n",
      "Link Functions\n",
      "The link function relates the systematic component to the mean of the random\n",
      "variable.\n",
      "In the case of the linear model, let ηi be the systematic component for a\n",
      "given value of the independent variable,\n",
      "ηi = β0 + β1x1i + · · ·βpxpi,\n",
      "and let µi = E(Y ), as before. Let g be the link function:\n",
      "ηi = g(µi).\n",
      "In this case, the link function is linear in a set of parameters, βj, and it is\n",
      "usually more natural to think in terms of these parameters rather than θ,\n",
      "g\n",
      "\u0012 d\n",
      "dθb(θi)\n",
      "\u0013\n",
      "= g(µi) = ηi = xT\n",
      "i β.\n",
      "The generalized linear model can now be thought of as consisting of three\n",
      "parts:\n",
      "1. the systematic component\n",
      "2. the random component\n",
      "3. the link between the systematic and random components.\n",
      "In the context of generalized linear models, a standard linear model has a\n",
      "systematic component of\n",
      "β0 + β1x1i + · · · βmxmi,\n",
      "a random component that is an identical and independent normal distribution\n",
      "for each observation, and a link function that is the identity.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "494\n",
      "6 Statistical Inference Based on Likelihood\n",
      "Fitting Generalized Linear Models\n",
      "Our initial objective is to ﬁt the model, that is, to determine estimates of the\n",
      "βj.\n",
      "The model parameters are usually determined either by a maximum likeli-\n",
      "hood method or by minimizing some function of the residuals. One approach\n",
      "is to use the link function and do a least squares ﬁt of η using the residuals\n",
      "yi −µi. It is better, however, to maximize the likelihood or, alternatively, the\n",
      "log-likelihood,\n",
      "l(θ, φ|y) =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "yiθi −b(θi)\n",
      "a(φ)\n",
      "+ c(yi, φ).\n",
      "The most common method of optimizing this function is “Fisher scoring”,\n",
      "which is a method like Newton’s method, except that some quantities are\n",
      "replaced by their expected values.\n",
      "In the generalized linear model, where the likelihood is linked to the pa-\n",
      "rameters that are really of interest, this still must be cast in terms that will\n",
      "yield values for bβ.\n",
      "Analysis of Deviance\n",
      "Our approach to modeling involves using the observations (including the real-\n",
      "izations of the random variables) as ﬁxed values and treating the parameters\n",
      "as variables (not random variables, however). The original model was then\n",
      "encapsulated into a likelihood function, L(θ|y), and the principle of ﬁtting\n",
      "the model was maximization of the likelihood with respect to the parameters.\n",
      "The log likelihood, l(θ|x), is usually used.\n",
      "In model ﬁtting an important issue is how well does the model ﬁt the data?\n",
      "How do we measure the ﬁt? Maybe use residuals. (Remember, some methods\n",
      "of model ﬁtting work this way; they minimize some function of the residuals.)\n",
      "We compare diﬀerent models by means of the measure of the ﬁt based on\n",
      "the residuals. We make inference about parameters based on changes in the\n",
      "measure of ﬁt.\n",
      "Using the likelihood approach, we make inference about parameters based\n",
      "on changes in the likelihood. Likelihood ratio tests are based on this principle.\n",
      "A convenient way of comparing models or making inference about the\n",
      "parameters is with the deviance function, which is a likelihood ratio:\n",
      "D(y|bθ) = 2\n",
      "\u0010\n",
      "l(θmax|y) −l(bθ|y)\n",
      "\u0011\n",
      ",\n",
      "where bθ is the ﬁt of a potential model.\n",
      "For generalized linear models the analysis of deviance plays a role similar\n",
      "to that of the analysis of sums of squares (analysis of “variance”) in linear\n",
      "models.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.4 Application: MLEs in Generalized Linear Models\n",
      "495\n",
      "Under appropriate assumptions, when θ1 is a subvector of θ2, the diﬀerence\n",
      "in deviances of two models, D(y| bθ2) −D(y| bθ1) has an asymptotic chi-squared\n",
      "distribution with degrees of freedom equal to the diﬀerence in the number of\n",
      "parameters.\n",
      "******************* repeat below For models with a binary response\n",
      "variable, we need a diﬀerent measure of residuals. Because we are measuring\n",
      "the model ﬁt in terms of the deviance, D, we may think of the observations as\n",
      "each contributing a quantity di, such that P di = D. (Exactly what that value\n",
      "is depends on the form of the systematic component and the link function that\n",
      "are in the likelihood.) The quantity\n",
      "ri = sign(yi −bµi)\n",
      "p\n",
      "di\n",
      "increases in (yi −bµi) and P r2\n",
      "i = D. We call ri the deviance residual.\n",
      "For the logit model,\n",
      "ri = sign(yi −bµi)\n",
      "p\n",
      "−2 (yi log( bπi) + (1 −yi) log(1 −bπi))\n",
      "Generalized Additive Models\n",
      "The mechanical process of dealing with generalized additive models parallels\n",
      "that of dealing with generalized linear models. There are some very important\n",
      "diﬀerences, however. The most important is probably that the distribution of\n",
      "the deviances is not worked out.\n",
      "The meaning of degrees of freedom is also somewhat diﬀerent.\n",
      "So, ﬁrst, we work out an analogous concept for degrees of freedom.\n",
      "The response variable is Bernoulli (or binomial). We model the log odds\n",
      "ratios as\n",
      "log\n",
      "\u0012\n",
      "πi\n",
      "1 −πi\n",
      "\u0013\n",
      "= ηi\n",
      "= β0 + β1x1i + · · · + β6x6i\n",
      "= xT\n",
      "i β.\n",
      "For a binomial with number mi, we write the log-likelihood,\n",
      "l(π|y) =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(yi log(πi/(1 −πi)) + mi log(1 −πi)) ,\n",
      "where a constant involving mi and yi has been omitted. Substituting, we have,\n",
      "l(β|y) =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "yixT\n",
      "i β −\n",
      "n\n",
      "X\n",
      "i=1\n",
      "mi log\n",
      "\u00001 + exp(xT\n",
      "i β)\n",
      "\u0001\n",
      ".\n",
      "The log likelihood depends on y only through XTy.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "496\n",
      "6 Statistical Inference Based on Likelihood\n",
      "∂l\n",
      "∂πi\n",
      "= yi −miπi\n",
      "πi(1 −πi)\n",
      "Using the chain rule, we have\n",
      "∂l\n",
      "∂βj\n",
      "=\n",
      "n\n",
      "X\n",
      "i=1\n",
      "yi −miπi\n",
      "πi(1 −πi)\n",
      "∂πi\n",
      "∂βj\n",
      "=\n",
      "n\n",
      "X\n",
      "i=1\n",
      "yi −miπi\n",
      "πi(1 −πi)\n",
      "dπi\n",
      "dηi\n",
      "xij\n",
      "The Fisher information is\n",
      "−E\n",
      "\u0012\n",
      "∂2l\n",
      "∂βj∂βk\n",
      "\u0013\n",
      "=\n",
      "n\n",
      "X\n",
      "i=1\n",
      "mi\n",
      "πi(1 −πi)\n",
      "∂πi\n",
      "∂βj\n",
      "∂πi\n",
      "∂βk\n",
      "=\n",
      "n\n",
      "X\n",
      "i=1\n",
      "mi(dπi/dηi)2\n",
      "πi(1 −πi)\n",
      "xijxik\n",
      "= (XTWX)jk,\n",
      "where W is a diagonal matrix of weights,\n",
      "mi(dπi/dηi)2\n",
      "πi(1 −πi)\n",
      "Notice\n",
      "dπi\n",
      "dηi\n",
      "= πi(1 −πi),\n",
      "so we have the simple expression,\n",
      "∂l\n",
      "∂β = XT(y −mπ)\n",
      "in matrix notation, and for the weights we have,\n",
      "miπi(1 −πi)\n",
      "Use Newton’s method,\n",
      "bβ(k+1) = bβ(k) −H−1\n",
      "l\n",
      "(bβ(k))∇l(bβ(k)),\n",
      "in which Hl is replaced by\n",
      "E\n",
      "\u0012\n",
      "−\n",
      "∂2l\n",
      "∂β∂βT .\n",
      "\u0013\n",
      "Using bβ(k), we form bπ(k) and bη(k), and then, an adjusted y(k),\n",
      "y(k)\n",
      "i\n",
      "= bη(k) + (y −mibπ(k)\n",
      "i\n",
      ")\n",
      "mi\n",
      "dηi\n",
      "dπi\n",
      "This leads to\n",
      "bβ(k+1) = (XTW (k)X)−1XTW (k)y(k),\n",
      "and it suggests an iteratively reweighted least squares (IRLS) algorithm.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.4 Application: MLEs in Generalized Linear Models\n",
      "497\n",
      "Residuals\n",
      "For models with a binary response variable, we need a diﬀerent measure of\n",
      "residuals. Because we are measuring the model ﬁt in terms of the deviance, D,\n",
      "we may think of the observations as each contributing a quantity di, such that\n",
      "Pdi = D. (Exactly what that value is depends on the form of the systematic\n",
      "component and the link function that are in the likelihood.) The quantity\n",
      "rD\n",
      "i = sign(yi −bµi)\n",
      "p\n",
      "di\n",
      "increases in (yi −bµi) and P(rD\n",
      "i )2 = D. We call rD\n",
      "i the deviance residual.\n",
      "For the logit model,\n",
      "rD\n",
      "i = sign(yi −bµi)\n",
      "p\n",
      "−2 (yi log( bπi) + (1 −yi) log(1 −bπi)).\n",
      "Another kind of residual is called the “working” residual. It is\n",
      "rW\n",
      "i\n",
      "= (yi −bµi) ∂bηi\n",
      "∂bµi\n",
      ",\n",
      "where the derivatives are evaluated at the ﬁnal iteration of the scoring algo-\n",
      "rithm.\n",
      "In the logistic regression model, these working residuals are\n",
      "yi −bπi\n",
      "bπi(1 −bπi)\n",
      "Residuals can be standardized by taking into account their diﬀerent stan-\n",
      "dard deviations that result from the inﬂuence.\n",
      "This is the same kind of concept as inﬂuence in linear models. Here, how-\n",
      "ever, we have\n",
      "bβ(k+1) = (XTW (k)X)−1XTW (k)y(k),\n",
      "where the weights are\n",
      "mibπ(k)\n",
      "i\n",
      "(1 −bπ(k)\n",
      "i\n",
      ").\n",
      "One measure is the diagonal of the hat matrix:\n",
      "W\n",
      "1\n",
      "2 X(XTWX)−1XTW\n",
      "1\n",
      "2\n",
      "In the case of generalized linear models, the hat matrix is only the predic-\n",
      "tion transformation matrix for the linear, systematic component.\n",
      "Data consisting of counts, for example, the number of certain events within\n",
      "a ﬁxed period of time, give rise naturally to a Poisson model. The relationship\n",
      "between the mean and the covariates is often assumed to be multiplicative,\n",
      "giving rise to a log-linear model,\n",
      "log(µ) = η = xTβ.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "498\n",
      "6 Statistical Inference Based on Likelihood\n",
      "Another possibility for count data is that the covariates have an additive\n",
      "eﬀect and the direct relation\n",
      "µ = xTβ\n",
      "can be used.\n",
      "Notice that the mean of the binomial and the Poisson distributions deter-\n",
      "mine the variance.\n",
      "In practice the variance of discrete response data, such as binomial or Pois-\n",
      "son data, is observed to exceed the nominal variance that would be determined\n",
      "by the mean.\n",
      "This phenomenon is referred to as “over-dispersion”. There may be logical\n",
      "explanations for over-dispersion, such as additional heterogeneity over and\n",
      "above what is accounted for by the covariates, or some more complicated\n",
      "variance structure arising from correlations among the responses.\n",
      "6.5 Variations on the Likelihood\n",
      "There are situations in which a likelihood equation either cannot be written\n",
      "or else it is not solvable. This may happen because of too many parameters,\n",
      "for example. In such cases an approximate likelihood equation may be more\n",
      "appropriate. In other cases, there may be a nuisance parameter that compli-\n",
      "cates the computation of the MLE for the parameter of interest. In both kind\n",
      "of these situations, we use approximate likelihood methods.\n",
      "6.5.1 Quasi-likelihood Methods\n",
      "Another way we deal with nuisance parameters in maximum likelihood estima-\n",
      "tion is by making some simplifying approximations. One type of simpliﬁcation\n",
      "is to reduce the dimensionality of the nuisance parameters by assuming some\n",
      "relationship among them. This yields a “quasi-likelihood” function. This may\n",
      "allow us to solve what otherwise might be a very diﬃcult problem. In some\n",
      "cases it may not aﬀect the MLE for the parameters of interest. A common\n",
      "application in which quasi-likelihood methods are useful is in estimation of\n",
      "parameters in a generalized linear model.\n",
      "Quasi-likelihood Methods in Generalized Linear Models\n",
      "Over-dispersion in the generalized linear model can often be accounted for\n",
      "by the nuisance parameter φ in the likelihood. For example, we modify the\n",
      "simple binomial model so the variance is\n",
      "V(yi|xi) = φπi(1 −πi)\n",
      "ni\n",
      ".\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.5 Variations on the Likelihood\n",
      "499\n",
      "Notice the multiplier φ is constant, while π depends on the covariates and\n",
      "n depends on the group size. This of course leads to a more complicated\n",
      "likelihood function, but it may not be necessary to use the actual likelihood.\n",
      "Quasi-likelihood and need not correspond to any particular distribution;\n",
      "rather quasi can be used to combine any available link and variance function.\n",
      "Wedderburn (1974) introduced a quasi-likelihood function to allow\n",
      "E(y|x) = µ = h(xTβ)\n",
      "and\n",
      "V(y|x) = σ2(µ) = φv(µ),\n",
      "where φ is the (nuisance) dispersion parameter in the likelihood and v(µ) is\n",
      "a variance function that is entirely separate from the likelihood.\n",
      "Quasi-likelihood methods require only speciﬁcation of a relationship be-\n",
      "tween the mean and variance of the response.\n",
      "In a multiparameter case, θ = (θ1, θ2), we may be interested in only some\n",
      "of the parameters, or in some function of the parameters, perhaps a transfor-\n",
      "mation into a lower-dimensional space. There are various ways of approaching\n",
      "this.\n",
      "6.5.2 Nonparametric and Semiparametric Models\n",
      "Empirical Likelihood\n",
      "Proﬁle Likelihood\n",
      "If θ = (θ1, θ2) and our interest is only in θ1, the simplest way of handling this\n",
      "is just to consider θ2 to be ﬁxed, perhaps at several diﬀerent values, one at a\n",
      "time. If θ2 is ﬁxed, the likelihood L(θ1 ; θ2, x) is called a proﬁle likelihood or\n",
      "concentrated likelihood of θ1 for given θ2 and x.\n",
      "*** build up Example 6.7 ... 2-D, then proﬁle ******** the derivative is\n",
      "not useful in ﬁnding the MLE is in a parametric-support family. For example,\n",
      "assume X1, . . ., Xn\n",
      "iid\n",
      "∼exponential(α, 1). The likelihood is\n",
      "L(α ; x) = e−P(xi−α)I]−∞,x(1)](α).\n",
      "Setting the derivative to 0 is not a useful way to ﬁnd a stationary point. (Note\n",
      "that the derivative of the indicator function is the Dirac delta function.) In\n",
      "fact, the max does not occur at a stationary point. The MLE of α is x(1).\n",
      "******************** relate this to Examples 1.5 and 6.5\n",
      "****** make 2-D plot\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "500\n",
      "6 Statistical Inference Based on Likelihood\n",
      "Exponential(α,1)\n",
      "(\n",
      "]\n",
      "α\n",
      "x(1)\n",
      "L(α;x)\n",
      "In some cases, it turns out that the estimation of a subset of the param-\n",
      "eters does not depend on the value of some other subset. A good method of\n",
      "estimation of β in a linear model X = Zβ + ϵ where the residuals ϵ have\n",
      "a common variance σ2 and zero correlation can be performed equally well\n",
      "no matter what the value of σ2 is. (The Gauss-Markov theorem tells us that\n",
      "the least-squares method yields a good estimator.) If the residuals are inde-\n",
      "pendently distributed as normals with a common variance, we can formulate\n",
      "the problem as a problem in maximum likelihood estimation. The MLE for β\n",
      "(which just happens to be the same as the LSE) can be thought of in terms of\n",
      "a proﬁle likelihood, because a particular value of σ2 could be chosen a priori.\n",
      "(This is of course not necessary because the maximum or the likelihood with\n",
      "respect to β occurs at the same point regardless of the value of σ2.)\n",
      "Conditional Likelihood\n",
      "When there is a nuisance parameter for which we have a suﬃcient statistic,\n",
      "a simple approach is to use the PDF conditional on the suﬃcient statistic to\n",
      "form the likelihood function for the parameter of interest. After doing this, the\n",
      "MLE procedure continues as in the usual case. If the PDFs can be factored\n",
      "so that one factor includes θ2 and some function of the sample, S(x), and the\n",
      "other factor, given S(x), is free of θ2, then this factorization can be carried\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "6.5 Variations on the Likelihood\n",
      "501\n",
      "into the likelihood. Such a likelihood is called a conditional likelihood of θ1\n",
      "given S(x).\n",
      "Conditional likelihood methods often arise in applications in which the pa-\n",
      "rameters of two diﬀerent distributions are to be compared; that is, when only\n",
      "their relative values are of interest. Suppose µ = (µ1, µ2) and let θ1 = µ1/µ2.\n",
      "Although our interest is in θ1, we may not be able to write the likelihood as a\n",
      "function of θ1. If, however, we can ﬁnd θ2 for which we have a suﬃcient statis-\n",
      "tic, T2(X), and we can factor the likelihood using the factorization theorem\n",
      "so that the factor corresponding to conditional distribution of X given T2(X)\n",
      "does not depend on θ2. This factor, as a function of θ1, is the conditional\n",
      "likelihood function.\n",
      "Sometimes a proﬁle likelihood can be thought of as a particularly sim-\n",
      "ple conditional likelihood. The linear model estimation problem referred to\n",
      "above could be formulated as a conditional likelihood. The actual form of the\n",
      "likelihood would be more complicated, but the solution is equivalent to the\n",
      "solution in which we think of the likelihood as a proﬁle likelihood.\n",
      "Conditional Likelihood for the Exponential Class\n",
      "If X has a distribution in the exponential class with θ = (η1, η2), and its\n",
      "likelihood can be written in the form\n",
      "L(θ ; x) = exp(ηT\n",
      "1 T1(x) + ηT\n",
      "2 T2(x) −ζ(η1, η2))h(x),\n",
      "or, in the log-likelihood form,\n",
      "lL(θ ; x) = ηT\n",
      "1 T1(x) + ηT\n",
      "2 T2(x) −ζ(η1, η2) + c(x),\n",
      "we can easily write the conditional log-likelihood:\n",
      "lL(η1 ; x ; T2) = ηT\n",
      "1 T1(x) + ˜ζ(η1, T2) + c(x).\n",
      "Notice that this decomposition can be achieved iﬀη1 is a linear function of θ.\n",
      "If our interest is only in η1, we only determine the argument that maximizes\n",
      "the function\n",
      "ηT\n",
      "1 T1(x) + ˜ζ(η1, T2),\n",
      "which is does not depend on η2.\n",
      "*********************\n",
      "Partial Likelihood\n",
      "The idea of partial likelihood is somewhat similar to conditional likelihood.\n",
      "The most common area of application is in semiparametric models. These are\n",
      "models of the form\n",
      "f(x; θ) = g(x; θ)h(x),\n",
      "(6.67)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "502\n",
      "6 Statistical Inference Based on Likelihood\n",
      "where x is observable, θ is unknown and unobservable, g is a function of known\n",
      "form, but f and h are of unknown form. The estimation problem has two\n",
      "components: the estimation of parameter θ and the nonparametric estimation\n",
      "of the function h.\n",
      "In the setup of equation (6.67) when f(x; θ) is the PDF of the observable,\n",
      "we form a partial likelihood function based on g(x; θ). This partial likelihood\n",
      "is an likelihood in the sense that it is a constant multiple (wrt θ) of the full\n",
      "likelihood function. The parameter θ can be estimated using the ordinary\n",
      "method for MLE.\n",
      "The most common example of this kind of problem in statistical inference\n",
      "is estimation of the proportional hazards model. Rather than discuss partial\n",
      "likelihood further here, I will postpone consideration of this semiparametric\n",
      "problem to Section 8.4 beginning on page 576.\n",
      "Notes and Further Reading\n",
      "Most of the material in this chapter is covered in MS2 Section 4.4, Section\n",
      "4.5,and Section 5.4, and in TPE2 Chapter 6.\n",
      "Likelihood and Probability\n",
      "Although it is natural to think of the distribution that yields the largest\n",
      "lieklihood as the “most probable” distribution that gave rise to an observed\n",
      "sample, it is important not to think of the likelihood function, even if it could\n",
      "be properly normalized, as a probability density. In the likelihood approach to\n",
      "statistical inference, there is no posterior conditional probability distribution\n",
      "as there is in a Bayesian approach. The book by Edwards (1992) provides a\n",
      "good discussion of the fundamental concept of likelihood.\n",
      "EM Methods\n",
      "EM methods were ﬁrst discussed systematically by Dempster et al. (1977). A\n",
      "general reference for EM methods is Ng et al. (2012).\n",
      "Computations\n",
      "The R function fitdistr in the MASS library computes the MLEs for a number\n",
      "of common distributions.\n",
      "Multiple RLEs\n",
      "There are interesting open questions associated with determining if an RLE\n",
      "yields a global maximum. See, for example, Biernacki (2005).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "503\n",
      "Maximum Likelihood in Linear Models\n",
      "Variance Components\n",
      "The problem of estimation of variance components in linear models received\n",
      "considerable attention during the later heyday of the development of statisti-\n",
      "cal methods for analysis of variance. The MLEs for the between-variance, σ2\n",
      "δ,\n",
      "and the residual variance, σ2\n",
      "ϵ, in the balanced one-way random-eﬀects model\n",
      "(equations (6.54) through (6.57)) were given by Herbach (1959). Thompson Jr.\n",
      "(1962) suggested a restricted maximum likelihood approach in which the es-\n",
      "timator is required to be equivariant. This method has come to be the most\n",
      "commonly used method of variance components estimation. This method is a\n",
      "more general form of REML, which we used in a special case in Example 6.27.\n",
      "As we mentioned, there are great diﬀerences in methods of estimation\n",
      "of variance components depending on whether the data are balanced, as in\n",
      "Example 5.31, or unbalanced. Various problems of variance component es-\n",
      "timation in unbalanced one-way models were discussed by Harville (1969).\n",
      "Searle et al. (1992) provide a thorough coverage of the various ways of esti-\n",
      "mating variance components and the underlying theory.\n",
      "Unbiasedness and Consistency\n",
      "While many MLEs are biased, most of the ones encountered in common situa-\n",
      "tions are at least consistent in mean squared error. Neyman and Scott (1948)\n",
      "give an example, which is a simpliﬁed version of an example due to Wald, of\n",
      "an MLEs that is not consistent. The problem is the standard one-way ANOVA\n",
      "model with two observations per class. The asymptotics are in the number\n",
      "of classes, and hence, of course in the number of observations. The model is\n",
      "Xij ∼N(µj, σ2) with i = 1, 2 and j = 1, 2, . . .. The asymptotic (and constant)\n",
      "expectation of the MLE of σ2 is σ2/2. This example certainly shows that\n",
      "MLEs may behave very poorly, but its special property should be recognized.\n",
      "The dimension of the parameter space is growing at the same rate as the\n",
      "number of observations.\n",
      "Quasilikelihood\n",
      "The idea of a quasilikelihood began with the work of Wedderburn (1974) on\n",
      "generalized linear models. This work merged with the earlier work of Durbin\n",
      "(1960) and Godambe (1960) on estimating functions. Heyde (1997) covers the\n",
      "important topics in the area.\n",
      "Empirical Likelihood\n",
      "The initial studies of empirical likelihood were in the application of likelihood\n",
      "ratio methods in nonparametric inference in the 1970s. Owen (2001) provides\n",
      "and introduction and summary.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "504\n",
      "6 Statistical Inference Based on Likelihood\n",
      "Exercises\n",
      "6.1. Consider the problem in Exercise 5.2 of using a sample of size 1 for esti-\n",
      "mating g(θ) = e−3θ where θ is the parameter in a Poisson distribution.\n",
      "What is the MLE of g(θ)?\n",
      "6.2. Show that the MLE of σ2 in the one-way ﬁxed-eﬀects model is as given\n",
      "in equation (6.49) on page 489.\n",
      "6.3. Suppose Xij\n",
      "iid\n",
      "∼N(µi, σ2) for i = 1, . . ., m and j = 1, . . ., n. (Compare\n",
      "Exercise 5.10 on page 444.)\n",
      "a) Show that the MLE of σ2 is not consistent in mean squared error as\n",
      "m →∞and n remains ﬁxed.\n",
      "b) Show that the MLE of σ2 is consistent in mean squared error as n →∞\n",
      "and m remains ﬁxed.\n",
      "6.4. a) Show that the log-likelihood given in equation (6.53) is correct.\n",
      "b) Show that the MLEs given in equations (6.54) through (6.57) maxi-\n",
      "mize the likelihood over (¯IR+)2.\n",
      "6.5. Fill in the details for the proof of Theorem 6.3.\n",
      "6.6. Given a Bernoulli distribution with parameter π. We wish to estimate the\n",
      "variance g(π) = π(1 −π). Compare the MSE of the UMVUE in equa-\n",
      "tion (5.11) with the MLE in equation (6.26).\n",
      "6.7. Determine the MLE of µ for the distribution with CDF given in equa-\n",
      "tion (6.27) if P (x) is the CDF of the distribution N(µ, σ2). At what point\n",
      "is it discontinuous in the data?\n",
      "6.8. Computations for variations on Example 6.19. Use a computer program,\n",
      "maybe R to generate some artiﬁcial data to use to experiment with\n",
      "the EM method in some variations of the normal mixtures model. Take\n",
      "θ = (0.7, 0, 1, 1, 2). The following R code will generate 300 observations\n",
      "from such a model.\n",
      "# Generate data from normal mixture.\n",
      "# Note that R uses sigma, rather than sigma^2 in rnorm.\n",
      "#\n",
      "Set the seed, so computations are reproducible.\n",
      "set.seed(4)\n",
      "n <- 300\n",
      "w <- 0.7\n",
      "mu1 <- 0\n",
      "sigma21 <- 1\n",
      "mu2 <- 5\n",
      "sigma22 <- 2\n",
      "x <- ifelse(runif(n)<w,\n",
      "rnorm(n,mu1,sqrt(sigma21)),rnorm(n,mu2,sqrt(sigma22)))\n",
      "a) Assume that µ1, σ2\n",
      "1, µ2, and σ2\n",
      "2 are all known and use EM to estimate\n",
      "θ1 = w.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "505\n",
      "b) Assume that σ2\n",
      "1 and σ2\n",
      "2 are known and use EM to estimate θ1, θ2, and\n",
      "θ4.\n",
      "c) Assume that all are unknown and use EM to estimate θ.\n",
      "6.9. Consider another variation on the normal mixture in Example 6.19. As-\n",
      "sume that w is known and σ2\n",
      "1 = σ2\n",
      "2 = σ2, but µ1, µ2, and σ2 are unknown.\n",
      "If w = 1/2, this setup is similar to the two-class one-way ﬁxed eﬀects AOV\n",
      "model in Example 6.26. What are the diﬀerences? Compare the estimators\n",
      "in the two setups.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7\n",
      "Statistical Hypotheses and Conﬁdence Sets\n",
      "In a frequentist approach to statistical hypothesis testing, the basic problem\n",
      "is to decide whether or not to reject a statement about the distribution of\n",
      "a random variable. The statement must be expressible in terms of member-\n",
      "ship in a well-deﬁned class. The hypothesis can therefore be expressed by\n",
      "the statement that the distribution of the random variable X is in the class\n",
      "PH = {Pθ : θ ∈ΘH}. An hypothesis of this form is called a statistical hy-\n",
      "pothesis.\n",
      "The basic paradigm of statistical hypothesis testing was described in Sec-\n",
      "tion 3.5.1, beginning on page 290. We ﬁrst review some of those ideas in Sec-\n",
      "tion 7.1, and then in Section 7.2 we consider the issue of optimality of tests.\n",
      "We ﬁrst consider the Neyman-Pearson Fundamental Lemma, which identiﬁes\n",
      "the optimal procedure for testing one simple hypothesis versus another simple\n",
      "hypothesis. Then we discuss tests that are uniformly optimal in Section 7.2.2.\n",
      "As we saw in the point estimation problem, it is often not possible to de-\n",
      "velop a procedure that is uniformly optimal, so just as with the estimation\n",
      "problem, we can impose restrictions, such as unbiasedness or invariance, or\n",
      "we can deﬁne uniformity in terms of some global risk. Because hypothesis\n",
      "testing is essentially a binary decision problem, a minimax criterion usually\n",
      "is not relevant, but use of global averaging may be appropriate. (This is done\n",
      "in the Bayesian approaches described in Section 4.5, and we will not pursue\n",
      "it further in this chapter.)\n",
      "If we impose restrictions on certain properties of the acceptable tests, we\n",
      "then proceed to ﬁnd uniformly most powerful tests under those restrictions.\n",
      "We discuss unbiasedness of tests in Section 7.2.3, and we discuss uniformly\n",
      "most powerful unbiased tests in Section 7.2.4. In Section 7.3, we discuss gen-\n",
      "eral methods for constructing tests based on asymptotic distributions. Next\n",
      "we consider additional topics in testing statistical hypotheses, such as non-\n",
      "parametric tests, multiple tests, and sequential tests.\n",
      "Conﬁdence sets are closely related to hypothesis testing. In general, rejec-\n",
      "tion of an hypothesis is equivalent to the hypothesis corresponding to a set\n",
      "of parameters or of distributions outside of a conﬁdence set constructed at\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "508\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "a level of conﬁdence that corresponds to the level of signiﬁcance of the test.\n",
      "The basic ideas of conﬁdence sets were discussed in Section 3.5.2, beginning on\n",
      "page 296. The related concept of credible sets was described in Section 4.6.1,\n",
      "beginning on page 372. Beginning in Section 7.1 of the present chapter, we\n",
      "discuss conﬁdence sets in somewhat more detail.\n",
      "The Decisions in Hypothesis Testing\n",
      "It is in hypothesis testing more than in any other type of statistical inference\n",
      "that the conﬂict among various fundamental philosophies come into sharpest\n",
      "focus.\n",
      "Neyman-Pearson; two\n",
      "Fisher signiﬁcance test; one\n",
      "one, where the other is “all others”\n",
      "evidence as measured by likelihood\n",
      "7.1 Statistical Hypotheses\n",
      "A problem in statistical hypothesis testing is set in the context of a given\n",
      "broad family of distributions, P = {Pθ : θ ∈Θ}. As in other problems in\n",
      "statistical inference, the objective is to decide whether the given observations\n",
      "arose from some subset of distributions PH ⊆P.\n",
      "The statistical hypothesis is a statement of the form “the family of distri-\n",
      "butions is PH”, where PH ⊆P, or perhaps “θ ∈ΘH”, where ΘH ⊆Θ.\n",
      "The full statement consists of two pieces, one part an assumption, “assume\n",
      "the distribution of X is in the class”, and the other part the hypothesis,\n",
      "“θ ∈ΘH, where ΘH ⊆Θ.” Given the assumptions, and the deﬁnition of ΘH,\n",
      "we often denote the hypothesis as H, and write it as\n",
      "H : θ ∈ΘH.\n",
      "(7.1)\n",
      "Two Hypotheses\n",
      "While, in general, to reject the hypothesis H would mean to decide that\n",
      "θ /∈ΘH, it is generally more convenient to formulate the testing problem as\n",
      "one of deciding between two statements:\n",
      "H0 : θ ∈Θ0\n",
      "(7.2)\n",
      "and\n",
      "H1 : θ ∈Θ1,\n",
      "(7.3)\n",
      "where Θ0∩Θ1 = ∅. These two hypotheses could also be expressed as “the fam-\n",
      "ily of distributions is P0” and “the family of distributions is P1”, respectively,\n",
      "with the obvious meanings of P0 and P1.\n",
      "We do not treat H0 and H1 symmetrically; H0 is the hypothesis (or “null\n",
      "hypothesis”) to be tested and H1 is the alternative. This distinction is impor-\n",
      "tant in developing a methodology of testing.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.1 Statistical Hypotheses\n",
      "509\n",
      "Tests of Hypotheses\n",
      "To test the hypotheses means to choose one hypothesis or the other; that is,\n",
      "to make a decision, d, where d is a real number that indicates the hypothesis\n",
      "accepted. As usual in statistical inference, we have a sample X from the\n",
      "relevant family of distributions and a statistic T(X) on which we base our\n",
      "decision.\n",
      "A nonrandomized test procedure is a rule δ(X) that assigns two decisions\n",
      "to two disjoint subsets, C0 and C1, of the range of T(X) that we call the test\n",
      "statistic. We equate those two decisions with the real numbers 0 and 1, so\n",
      "δ(X) is a real-valued function,\n",
      "δ(x) =\n",
      "\u001a0 for T(x) ∈C0\n",
      "1 for T(x) ∈C1.\n",
      "(7.4)\n",
      "Note for i = 0, 1,\n",
      "Pr(δ(X) = i) = Pr(T(X) ∈Ci).\n",
      "(7.5)\n",
      "We call C1 the critical region, and generally denote it by just C.\n",
      "A test δ(X) is associated with a critical region C. We may use the term\n",
      "“critical region” either to denote a set of values of a statistic T(X) or just of\n",
      "the sample X itself.\n",
      "If δ(X) takes the value 0, the decision is not to reject; if δ(X) takes the\n",
      "value 1, the decision is to reject. If the range of δ(X) is {0, 1}, the test is a\n",
      "nonrandomized test.\n",
      "Although occasionally it may be useful to choose the range of δ(X) as\n",
      "some other set of real numbers, such as {d0, d1} or even a set with cardinality\n",
      "greater than 2, we generally deﬁne the decision rule so that δ(X) ∈[0, 1]. If\n",
      "the range is taken to be the closed interval [0, 1], we can interpret a value of\n",
      "δ(X) as the probability that the null hypothesis is rejected.\n",
      "If it is not the case that δ(X) equals 0 or 1 a.s., we call the test a randomized\n",
      "test.\n",
      "Errors in Decisions Made in Testing\n",
      "There are four possibilities in a test of an hypothesis: the hypothesis may be\n",
      "true, and the test may or may not reject it, or the hypothesis may be false,\n",
      "and the test may or may not reject it. The result of a statistical hypothesis\n",
      "test can be incorrect in two distinct ways: it can reject a true hypothesis or\n",
      "it can fail to reject a false hypothesis. We call rejecting a true hypothesis a\n",
      "“type I error”, and failing to reject a false hypothesis a “type II error”.\n",
      "Our standard approach in hypothesis testing is to control the level of the\n",
      "probability of a type I error under the assumptions, and to try to ﬁnd a test\n",
      "subject to that level that has a small probability of a type II error.\n",
      "We call the maximum allowable probability of a type I error the “signiﬁ-\n",
      "cance level”, and usually denote it by α.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "510\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "We call the probability of rejecting the null hypothesis the power of the\n",
      "test, and will denote it by β. If the alternate hypothesis is the true state of\n",
      "nature, the power is one minus the probability of a type II error.\n",
      "It is clear that we can easily decrease the probability of one type of error\n",
      "(if its probability is positive) at the cost of increasing the probability of the\n",
      "other.\n",
      "In a common approach to hypothesis testing under the given assumptions\n",
      "on X (and using the notation above), we choose α ∈]0, 1[ and require that\n",
      "δ(X) be such that\n",
      "Pr(δ(X) = 1 | θ ∈Θ0) ≤α.\n",
      "(7.6)\n",
      "and, subject to this, ﬁnd δ(X) so as to minimize\n",
      "Pr(δ(X) = 0 | θ ∈Θ1).\n",
      "(7.7)\n",
      "Optimality of a test T is deﬁned in terms of this constrained optimization\n",
      "problem.\n",
      "Notice that the restriction on the type I error applies ∀θ ∈Θ0. We call\n",
      "sup\n",
      "θ∈Θ0\n",
      "Pr(δ(X) = 1 | θ)\n",
      "(7.8)\n",
      "the size of the test. If the size of the test is less than the signiﬁcance level, then\n",
      "the test can be modiﬁed, possibly by use of an auxiliary random mechanism.\n",
      "In common applications, Θ0 ∪Θ1 forms a convex region in IRk, and Θ0\n",
      "contains the set of common closure points of Θ0 and Θ1 and Pr(δ(X) = 1 | θ)\n",
      "is a continuous function of θ; hence the sup in equation (7.8) is generally a\n",
      "max. (The set of common closure points, that is, the boundary between Θ0\n",
      "and Θ1, will have a prominent role in identifying optimal tests.)\n",
      "If the size is less than the level of signiﬁcance, the test is said to be con-\n",
      "servative, and in that case, we often refer to α as the “nominal size”.\n",
      "Example 7.1 Testing in the exponential family\n",
      "Suppose we have observations X1, . . ., Xn\n",
      "iid\n",
      "∼exponential(θ). The Lebesgue\n",
      "PDF is\n",
      "pθ(x) = θ−1e−x/θI]0,∞[(x),\n",
      "with θ ∈]0, ∞[. Suppose now we wish to test\n",
      "H0 : θ ≤θ0\n",
      "versus\n",
      "H1 : θ > θ0.\n",
      "We know that X is suﬃcient for θ. If H1 is true, X is likely to be larger\n",
      "than if H0 is true, so a reasonable test may be to reject H0 if T(X) = X > cα,\n",
      "where cα is some ﬁxed positive constant; that is,\n",
      "δ(X) = I]cα,∞[(T(X)).\n",
      "We choose cα so as to control the probability of a type I error. We call T(X)\n",
      "the test statistic.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.1 Statistical Hypotheses\n",
      "511\n",
      "Knowing the distribution of X to be gamma(n, θ/n), we can now work out\n",
      "Pr(δ(X) = 1 | θ) = Pr(T(X) > cα | θ),\n",
      "which, for θ < θ0 is the probability of a type I error. We set up the testing\n",
      "procedure so as to limit the probability of this type of error to be no more\n",
      "than α.\n",
      "For θ ≥θ0\n",
      "1 −Pr(δ(X) = 1 | θ)\n",
      "is the probability of a type II error.\n",
      "Over the full range of θ, we identify the power of the test as\n",
      "β(θ) = Pr(δ(X) = 1 | θ).\n",
      "These probabilities for n = 1, as a function of θ, are shown in Figure 7.1.\n",
      "Performance of Test\n",
      "0\n",
      "θ\n",
      "θ0\n",
      "α\n",
      "1 −α\n",
      "0.5\n",
      "]\n",
      "(\n",
      "H0\n",
      "H1\n",
      "β(θ)\n",
      "type I error\n",
      "type II error\n",
      "correct rejection\n",
      "Figure 7.1.\n",
      "Probabilities of Type I and Type II Errors\n",
      "Now, for a given signiﬁcance level α, we choose cα so that\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "512\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "Pr(T(X) > cα | θ ≤θ0) ≤α.\n",
      "This is satisﬁed for cα if Pr(Y > cα) = α, where Y is a random variable with\n",
      "the gamma(n, θ0/n) distribution.\n",
      "p-Values\n",
      "Note that there is a diﬀerence in choosing the test procedure, and in using\n",
      "the test. To use the test, the question of the choice of α comes back. Does it\n",
      "make sense to choose α ﬁrst, and then proceed to apply the test just to end\n",
      "up with a decision d0 or d1? It is not likely that this rigid approach would\n",
      "be very useful for most objectives. In statistical data analysis our objectives\n",
      "are usually broader than just deciding which of two hypotheses appears to be\n",
      "true based on some arbitrary standard for “truth”. On the other hand, if we\n",
      "have a well-developed procedure for testing the two hypotheses, the decision\n",
      "rule in this procedure could be very useful in data analysis.\n",
      "One common approach is to use the functional form of the rule, but not to\n",
      "pre-deﬁne the critical region. Then, given the same setup of null hypothesis and\n",
      "alternative, to collect data X = x, and to determine the smallest value ˆα(x)\n",
      "at which the null hypothesis would be rejected. The value ˆα(x) is called the p-\n",
      "value of x associated with the hypotheses. The p-value indicates the strength\n",
      "of the evidence of the data against the null hypothesis. If the alternative\n",
      "hypothesis is “everything else”, a test based a p-value is a signiﬁcance test.\n",
      "Although use of p-values represents a fundamentally diﬀerent approach\n",
      "to hypothesis testing than an approach based on a pre-selected signiﬁcance\n",
      "level, the p-value does correspond to the “smallest” signiﬁcance under which\n",
      "the null hypothesis would be rejected. Because of practical considerations,\n",
      "computer software packages implementing statistical hypothesis testing pro-\n",
      "cedures report p-values instead of “reject” or “do not reject”.\n",
      "Example 7.2 Testing in the exponential family; p-value\n",
      "Consider again the problem in Example 7.1, where we had observations\n",
      "X1, . . ., Xn\n",
      "iid\n",
      "∼exponential(θ), and wished to test\n",
      "H0 : θ ≤θ0\n",
      "versus\n",
      "H1 : θ > θ0.\n",
      "Our test was based on T(X) = X > c, where c was some ﬁxed positive con-\n",
      "stant chosen so that Pr(Y > c) = α, where Y is a random variable distributed\n",
      "as gamma(n, θ0/n).\n",
      "Suppose instead of choosing c, we merely compute Pr(Y > ¯x), where ¯x is\n",
      "the mean of the set of observations. This is the p-value for the null hypothesis\n",
      "and the given data.\n",
      "If the p-value is less than a prechosen signiﬁcance level α, then the null\n",
      "hypothesis is rejected.\n",
      "***** tests\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.1 Statistical Hypotheses\n",
      "513\n",
      "Power of a Statistical Test\n",
      "We call the probability of rejecting H0 the power of the test, and denote it by\n",
      "β, or for the particular test δ(X), by βδ. The power is deﬁned over the full\n",
      "set of distributions in the union of the hypotheses. For hypotheses concerning\n",
      "the parameter θ, as in Example 7.1, the power can be represented as a curve\n",
      "β(θ), as shown in Figure 7.1. We see that the power function of the test, for\n",
      "any given θ ∈Θ as\n",
      "βδ(θ) = Eθ(δ(X)).\n",
      "(7.9)\n",
      "The power in the case that H1 is true is 1 minus the probability of a\n",
      "type II error. Thus, minimizing the error in equation (7.7) is equivalent to\n",
      "maximizing the power within Θ1.\n",
      "The probability of a type II error is generally a function of the true distri-\n",
      "bution of the sample Pθ, and hence so is the power, which we may emphasize\n",
      "by the notation βδ(Pθ) or βδ(θ). In much of the following, we will assume\n",
      "that θ ∈Θ ⊆IRk; that is, the statistical inference is “parametric”. This setup\n",
      "is primarily one of convenience, because most concepts carry over to more\n",
      "general nonparametric situations. There are some cases, however, when they\n",
      "do not, as for example, when we speak of continuity wrt θ. We now can focus\n",
      "on the test under either hypothesis (that is, under either subset of the family\n",
      "of distributions) in a uniﬁed fashion.\n",
      "Because the power is generally a function of θ, what does maximizing the\n",
      "power mean? That is, maximize it for what values of θ? Ideally, we would\n",
      "like a procedure that yields the maximum for all values of θ; that is, one that\n",
      "is most powerful for all values of θ. We call such a procedure a uniformly\n",
      "most powerful or UMP test. For a given problem, ﬁnding such procedures, or\n",
      "establishing that they do not exist, will be one of our primary objectives.\n",
      "In some cases, βδ(θ) may be a continuous function of θ. Such cases may\n",
      "allow us to apply analytic methods for identifying most powerful tests within a\n",
      "class of tests satisfying certain desirable restrictions. (We do this on page 525.)\n",
      "Randomized Tests\n",
      "We deﬁned a randomized test (page 293) as one whose range is not a.s. {0, 1}.\n",
      "Because in this deﬁnition, a randomized test does not yield a “yes/no” decision\n",
      "about the hypothesis being tested, a test with a random component is more\n",
      "useful.\n",
      "Given a randomized test δ(X) that maps X onto {0, 1} ∪DR, we can con-\n",
      "struct a test with a random component using the rule that if if δ(X) ∈DR,\n",
      "then the experiment R is performed with δR(X) chosen so that the overall\n",
      "probability of a type I error is the desired level. The experiment R is indepen-\n",
      "dent of the random variable about whose distribution the hypothesis applies\n",
      "to. As a practical matter a U(0, 1) random variable can be used to deﬁne\n",
      "the random experiment. The random variable itself is often simulated on a\n",
      "computer.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "514\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "A test with a random component may be useful for establishing properties\n",
      "of tests or as counterexamples to some statement about a given test. (We\n",
      "often use randomized estimators in this way; see Example 5.25.)\n",
      "Another use of tests with random components is in problems with count-\n",
      "able sample spaces when a critical region within the sample space cannot be\n",
      "constructed so that the test has a speciﬁed size.\n",
      "While randomized estimators rarely have application in practice, random-\n",
      "ized test procedures can actually be used to increase the power of a conser-\n",
      "vative test. Use of a randomized test in this way would not make much sense\n",
      "in real-world data analysis, but if there are regulatory conditions to satisfy, it\n",
      "might needed to achieve an exact size.\n",
      "7.2 Optimal Tests\n",
      "Testing statistical hypotheses involves making a decision whether or not to\n",
      "reject a null hypothesis. If the decision is not to reject we may possibly make\n",
      "a secondary decision as to whether or not to continue collecting data, as we\n",
      "discuss in Section 7.6. For the moment, we will ignore the sequential testing\n",
      "problem and address the more basic question of optimality in testing. We ﬁrst\n",
      "need a measure or criterion.\n",
      "A general approach to deﬁning optimality is to deﬁne a loss function that\n",
      "increases in the “badness” of the statistical decision, and to formulate the risk\n",
      "as the expected value of that loss function within the context of the family\n",
      "of probability models being considered. Optimal procedures are those that\n",
      "minimize the risk. The decision-theoretic approach formalizes these concepts.\n",
      "Decision-Theoretic Approach\n",
      "The decision space in a testing problem is usually {0, 1}, which corresponds\n",
      "respectively to not rejecting and rejecting the hypothesis. (We may also allow\n",
      "for another alternative corresponding to “making no decision”.) As in the\n",
      "decision-theoretic setup, we seek to minimize the risk:\n",
      "R(P, δ) = E\u0000L(P, δ(X))\u0001.\n",
      "(7.10)\n",
      "In the case of the 0-1 loss function and the four possibilities, the risk is\n",
      "just the probability of either type of error.\n",
      "We want a test procedure that minimizes the risk, but rather than taking\n",
      "into account the total expected loss in the risk (7.10), we generally prefer to\n",
      "restrict the probability of a type I error as in inequality (7.6) and then, subject\n",
      "to that, minimize the probability of a type II error as in equation (7.7), which\n",
      "is equivalent to maximizing the power under the alternative hypothesis. This\n",
      "approach is minimizes the risk subject to a restriction that the contribution\n",
      "to the risk from one type of loss is no greater than a speciﬁed amount.\n",
      "The issue of a uniformly most powerful test is similar to the issue of a\n",
      "uniformly minimum risk test subject to a restriction.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.2 Optimal Tests\n",
      "515\n",
      "An Optimal Test in a Simple Situation\n",
      "First, consider the problem of picking the optimal critical region C in a prob-\n",
      "lem of testing the hypothesis that a discrete random variable has the prob-\n",
      "ability mass function p0(x) versus the alternative that it has the probability\n",
      "mass function p1(x). We will develop an optimal test for any given signiﬁcance\n",
      "level based on one observation.\n",
      "For x ∋p0(x) > 0, let\n",
      "r(x) = p1(x)\n",
      "p0(x),\n",
      "(7.11)\n",
      "and label the values of x for which r is deﬁned so that\n",
      "r(xr1) ≥r(xr2) ≥· · · .\n",
      "Let N be the set of x for which p0(x) = 0 and p1(x) > 0. Assume that\n",
      "there exists a j such that\n",
      "j\n",
      "X\n",
      "i=1\n",
      "p0(xri) = α.\n",
      "If S is the set of x for which we reject the test, we see that the signiﬁcance\n",
      "level is\n",
      "X\n",
      "x∈S\n",
      "p0(x).\n",
      "and the power over the region of the alternative hypothesis is\n",
      "X\n",
      "x∈S\n",
      "p1(x).\n",
      "Then it is clear that if C = {xr1, . . ., xrj} ∪N, then P\n",
      "x∈S p1(x) is maximized\n",
      "over all sets C subject to the restriction on the size of the test.\n",
      "If there does not exist a j such that Pj\n",
      "i=1 p0(xri) = α, the rule is to put\n",
      "xr1, . . ., xrj in C so long as\n",
      "j\n",
      "X\n",
      "i=1\n",
      "p0(xri) = α∗< α.\n",
      "We then deﬁne a randomized auxiliary test R\n",
      "Pr(R = d1) = δR(xrj+1)\n",
      "= (α −α∗)/p0(xrj+1)\n",
      "It is clear in this way that P\n",
      "x∈S p1(x) is maximized subject to the restriction\n",
      "on the size of the test.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "516\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "Example 7.3 Testing between two discrete distributions\n",
      "Consider two distributions with support on a subset of {0, 1, 2, 3, 4, 5}. Let\n",
      "p0(x) and p1(x) be the probability mass functions. Based on one observation,\n",
      "we want to test H0 : p0(x) is the mass function versus H1 : p1(x) is the\n",
      "mass function.\n",
      "Suppose the distributions are as shown in Table 7.1, where we also show\n",
      "the values of r and the labels on x determined by r.\n",
      "Table 7.1. Two Probability Distributions\n",
      "x\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "p0\n",
      ".05 .10 .15 0\n",
      ".50 .20\n",
      "p1\n",
      ".15 .40 .30 .05 .05 .05\n",
      "r\n",
      "3\n",
      "4\n",
      "2\n",
      "-\n",
      "1/10 2/5\n",
      "label 2\n",
      "1\n",
      "3\n",
      "-\n",
      "5\n",
      "4\n",
      "Thus, for example, we see xr1 = 1 and xr2 = 0. Also, N = {3}.\n",
      "For given α, we choose C such that\n",
      "X\n",
      "x∈C\n",
      "p0(x) ≤α\n",
      "and so as to maximize\n",
      "X\n",
      "x∈C\n",
      "p1(x).\n",
      "We ﬁnd the optimal C by ﬁrst ordering r(xi1) ≥r(xi2) ≥· · · and then\n",
      "satisfying P\n",
      "x∈C p0(x) ≤α. The ordered possibilities for C in this example\n",
      "are\n",
      "{1} ∪{3},\n",
      "{1, 0} ∪{3},\n",
      "{1, 0, 2} ∪{3},\n",
      "· · · .\n",
      "Notice that including N in the critical region does not cost us anything (in\n",
      "terms of the type I error that we are controlling).\n",
      "Now, for any given signiﬁcance level, we can determine the optimum test\n",
      "based on one observation.\n",
      "•\n",
      "Suppose α = .10. Then the optimal critical region is C = {1, 3}, and the\n",
      "power for the null hypothesis is βδ(p1) = .45.\n",
      "•\n",
      "Suppose α = .15. Then the optimal critical region is C = {0, 1, 3}, and the\n",
      "power for the null hypothesis is βδ(p1) = .60.\n",
      "•\n",
      "Suppose α = .05. We cannot put 1 in C, with probability 1, but if we put\n",
      "1 in C with probability 0.5, the α level is satisﬁed, and the power for the\n",
      "null hypothesis is βδ(p1) = .25.\n",
      "•\n",
      "Suppose α = .20. We choose C = {0, 1, 3} with probability 2/3 and C =\n",
      "{0, 1, 2, 3} with probability 1/3. The α level is satisﬁed, and the power for\n",
      "the null hypothesis is βδ(p1) = .75.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.2 Optimal Tests\n",
      "517\n",
      "All of these tests are most powerful based on a single observation for the given\n",
      "values of α.\n",
      "We can extend this idea to tests based on two observations. We see imme-\n",
      "diately that the ordered critical regions are\n",
      "C1 = {1, 3} × {1, 3},\n",
      "C1 ∪{1, 3} × {0, 3},\n",
      "· · · .\n",
      "Extending this direct enumeration would be tedious, but, at this point we\n",
      "have grasped the implication: the ratio of the likelihoods is the basis for the\n",
      "most powerful test. This is the Neyman-Pearson Fundamental Lemma.\n",
      "7.2.1 The Neyman-Pearson Fundamental Lemma\n",
      "Example 7.3 illustrates the way we can approach the problem of testing any\n",
      "simple hypothesis against another simple hypothesis so long as we have a\n",
      "PDF. Notice the pivotal role played by ratio r in equation (7.11). This is a\n",
      "ratio of likelihoods.\n",
      "Thinking of the hypotheses in terms of a parameter θ that indexes these\n",
      "two PDFs by θ0 and θ1, for a sample X = x, we have the likelihoods associated\n",
      "with the two hypotheses as L(θ0; x) and L(θ1; x). We may be able to deﬁne an\n",
      "α-level critical region for nonrandomized tests in terms of the ratio of these\n",
      "likelihoods: Let us assume that a positive number k exists such that there is\n",
      "a subset of the sample space C with complement with respect to the sample\n",
      "space Cc, such that\n",
      "L(θ1; x)\n",
      "L(θ0; x) ≥k\n",
      "∀x ∈C\n",
      "L(θ1; x)\n",
      "L(θ0; x) ≤k\n",
      "∀x ∈Cc\n",
      "(7.12)\n",
      "and\n",
      "α = Pr(X ∈C | H0).\n",
      "(Notice that such a k and C may not exist.) For testing H0 that the distribu-\n",
      "tion of X is P0 versus the alternative H1 that the distribution of X is P1, we\n",
      "can see that C is the best critical region of size α for testing H0 versus H1;\n",
      "that is, if A is any critical region of size α, then\n",
      "Z\n",
      "C\n",
      "L(θ1) −\n",
      "Z\n",
      "A\n",
      "L(θ1) ≥0\n",
      "(7.13)\n",
      "(exercise).\n",
      "The critical region deﬁned in equation (7.12) illustrates the basic concepts,\n",
      "but it leaves some open questions. Following these ideas, the Neyman-Pearson\n",
      "Fundamental Lemma precisely identiﬁes the most powerful test for a simple\n",
      "hypothesis versus another simple hypothesis and furthermore shows that the\n",
      "test is a.s. unique.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "518\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "Theorem 7.1 (Neyman-Pearson Fundamental Lemma)\n",
      "Let P0 and P1 be distributions with PDFs that are deﬁned with respect to a\n",
      "common σ-ﬁnite measure. For given data X, let L(P0; X) and L(P1; X) be\n",
      "the respective likelihood functions.\n",
      "To test H0 : P0 versus H1 : P1 at the level α ∈]0, 1[,\n",
      "(i) there exists a test δ such that\n",
      "EP0(δ(X)) = α\n",
      "(7.14)\n",
      "and\n",
      "δ(X) =\n",
      "\n",
      "\n",
      "\n",
      "1 L(P1; X) > cL(P0; X)\n",
      "γ L(P1; X) = cL(P0; X)\n",
      "0 L(θ1; X) < cL(P0; X)\n",
      ";\n",
      "(7.15)\n",
      "(ii) δ is most powerful test\n",
      "(iii) if ˜δ is a test that is as powerful as δ, then ˜δ(X) = δ(X) a.e. µ.\n",
      "Proof.\n",
      "Example 7.4 Testing hypotheses about the parameter in a Bernoulli\n",
      "distribution\n",
      "Suppose we assume a Bernoulli(π) distribution for the independently observed\n",
      "random variables X1, . . ., Xn, and we wish to test H0 : π = 1/4 versus\n",
      "H1 : π = 3/4 at the level α = 0.05. If X is the number of 1s; that is, if\n",
      "X = Pn\n",
      "i=1 Xi, then X has a binomial distribution under either H0 or H1 and\n",
      "so the likelihoods in equation (7.15) are based on binomial PDFs. The optimal\n",
      "test δ(X), following the Neyman-Pearson setup, is based on the relationship\n",
      "of the ratio L(P1, x)/L(P0, x) to a constant c as in equation (7.15) such that\n",
      "EP0(δ(X)) = 0.05.\n",
      "Now, suppose n = 30 and the number of 1s observed is x. The ratio of the\n",
      "likelihoods is\n",
      "L(P1, x)\n",
      "L(P0, x) = 32x−30,\n",
      "(7.16)\n",
      "and larger values of x yield a value of 1 for δ(X). Since x can take on only the\n",
      "values 0, 1, . . ., 30, the ratio can take on only 31 diﬀerent values. If the value\n",
      "of c is chosen so that the test rejects H0 if x ≥12, then\n",
      "EP0(δ(X)) = Pr(X ≥12) = 0.05065828 = α−,\n",
      "(7.17)\n",
      "say, while if c is chosen so that the test rejects H0 if x ≥13, then\n",
      "EP0(δ(X)) = 0.02159364 = α+.\n",
      "(7.18)\n",
      "From equation (7.16), we see that c = 3−4 and from equations (7.17)\n",
      "and (7.18) we see that γ = (α−α−)/(α+−α−) = 0.9773512 in equation (7.15).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.2 Optimal Tests\n",
      "519\n",
      "Use of Suﬃcient Statistics\n",
      "It is a useful fact that if there is a suﬃcient statistic S(X) for θ, and ˜δ(X) is\n",
      "an α-level test for an hypothesis specifying values of θ, then there exists an\n",
      "α-level test for the same hypothesis, δ(S) that depends only on S(X), and\n",
      "which has power at least as great as that of ˜δ(X). We see this by factoring\n",
      "the likelihoods.\n",
      "Nuisance Parameters and Similar Regions\n",
      "It is often the case that there are additional parameters not speciﬁed by\n",
      "the hypotheses being tested. In this situation we have θ = (θs, θu), and the\n",
      "hypothesis may be of the form H0\n",
      ":\n",
      "θs = θs0 or more generally, for the\n",
      "sample space,\n",
      "H0 : Θ = Θ0,\n",
      "where Θ0 does not restrict some of the parameters. The hypothesis speciﬁes\n",
      "the family of distributions as P0 = {Pθ ; θ ∈Θ0}.\n",
      "The problem is that the performance of the test, that is, E(δ(X)) may\n",
      "depend on the value of θu, even though we are not interested in θu. There\n",
      "is nothing we can do about this over the full parameter space, but since we\n",
      "think it is important to control the size of the test, we require, for given α,\n",
      "EH0(δ(X)) = α.\n",
      "(Strictly speaking, we may only require that this expectation be bounded\n",
      "above by α.) Hence, we seek a procedure δ(X) such that E(δ(X)) = α over\n",
      "the subspace θ = (θs0, θu).\n",
      "Is this possible? It certainly is if α = 1; that is, if the rejection region is\n",
      "the entire sample space. Are there regions similar to the sample space in this\n",
      "regard? Maybe.\n",
      "If a critical region R is such that PrH0(X ∈R) = α for all values of θ, the\n",
      "region is called an α-level similar region with respect to θ = (θs0, θu), or with\n",
      "respect to H0, or with respect to P0.\n",
      "A test δ(X) such that Eθ(δ(X)) = α for all θ ∈H0 is called an α-level\n",
      "similar test with respect to θ = (θs0, θu).\n",
      "Now, suppose S is a suﬃcient statistic for the family P0 = {Pθ ; θ ∈Θ0}.\n",
      "Let δ(X) be a test that satisﬁes\n",
      "E(δ(X)|S) = α\n",
      "a.e. P0.\n",
      "In this case, we have\n",
      "EH0(δ(X)) = EH0(E(δ(X)|S))\n",
      "= α;\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "520\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "hence, the test is similar wrt P0. This condition on the critical region is called\n",
      "Neyman structure.\n",
      "The concepts of similarity and Neyman structure are relevant for unbiased\n",
      "tests, which we will consider in Section 7.2.3.\n",
      "Now suppose that U is boundedly complete suﬃcient for θu. If\n",
      "E(EH0(δ(X)|U)) = α,\n",
      "then the test has Neyman structure. While the power may still depend on θu,\n",
      "this fact may allow us to determine optimal tests of given size without regard\n",
      "to the nuisance parameters.\n",
      "7.2.2 Uniformly Most Powerful Tests\n",
      "The probability of a type I error is limited to α or less. We seek a procedure\n",
      "that yields the minimum probability of a type II error, given that bound\n",
      "on the probability of a type I error. This would be a “most powerful” test.\n",
      "Ideally, the test would be most powerful for all values of θ ∈Θ1. We call\n",
      "such a procedure a uniformly most powerful or UMP α-level test. For a given\n",
      "problem, ﬁnding such tests, or establishing that they do not exist, will be\n",
      "one of our primary objectives. The Neyman-Pearson Lemma gives us a way\n",
      "of determining whether a UMP test exists, and if so how to ﬁnd one. The\n",
      "main issue is the likelihood ratio as a function of the parameter in the region\n",
      "speciﬁed by a composite H1. If the likelihood ratio is monotone, then we have\n",
      "a UMP based on the ratio.\n",
      "Generalizing the Optimal Test to Hypotheses of Intervals: UMP\n",
      "Tests\n",
      "Although it applies to a simple alternative (and hence “uniform” properties\n",
      "do not make much sense), the Neyman-Pearson Lemma gives us a way of\n",
      "determining whether a uniformly most powerful (UMP) test exists, and if so\n",
      "how to ﬁnd one. We are often interested in testing hypotheses in which either\n",
      "or both of Θ0 and Θ1 are convex regions of IR (or IRk).\n",
      "We must look at the likelihood ratio as a function both of a scalar pa-\n",
      "rameter θ and of a scalar function of x. The question is whether, for given\n",
      "θ0 and any θ1 > θ0 (or equivalently any θ1 < θ0), the likelihood is monotone\n",
      "in some scalar function of x; that is, whether the family of distributions of\n",
      "interest is parameterized by a scalar in such a way that it has a monotone\n",
      "likelihood ratio (see page 167 and Exercise 2.5). In that case, it is clear that\n",
      "we can extend the test in (7.15) to be uniformly most powerful for testing\n",
      "H0 : θ = θ0 against an alternative H1 : θ > θ0 (or θ1 < θ0).\n",
      "Example 7.5 Testing hypotheses about the parameter in a Bernoulli\n",
      "distribution (continuation of Example 7.4)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.2 Optimal Tests\n",
      "521\n",
      "Syppose we assume a Bernoulli(π) distribution, and we wish to test H0 : π =\n",
      "1/4 versus H1 : π > 1/4 at the level α = 0.05. The alternative hypothesis is\n",
      "certainly more reasonable than the one in Example 7.4.\n",
      "************* modify Example 7.4\n",
      "****** also mention randomization *** don’t do it\n",
      "In this case, we do not have the problem of conﬂicting evidence mentioned\n",
      "on page 541.\n",
      "The exponential class of distributions is important because UMP tests are\n",
      "easy to ﬁnd for families of distributions in that class. Discrete distributions\n",
      "are especially simple, but there is nothing special about them. Example 7.1\n",
      "developed a test for H0 : θ ≤θ0 versus the alternative H1 : θ > θ0 in a\n",
      "one-parameter exponential distribution, that is clearly UMP, as we can see\n",
      "by using the formulation (7.15) in a pointwise fashion. (The one-parameter\n",
      "exponential distribution, with density over the positive reals θ−1e−x/θ is a\n",
      "member of the exponential class. Recall that the two-parameter exponential\n",
      "distribution used is not a member of the exponential family.)\n",
      "Let us ﬁrst identify some classes of hypotheses.\n",
      "•\n",
      "simple versus simple\n",
      "H0 : θ = θ0\n",
      "versus\n",
      "H1 : θ = θ1.\n",
      "(7.19)\n",
      "•\n",
      "one-sided\n",
      "H0 : θ ≤θ0\n",
      "versus\n",
      "H1 : θ > θ0.\n",
      "(7.20)\n",
      "•\n",
      "two-sided; null on extremes\n",
      "H0 : θ ≤θ1 or θ ≥θ2\n",
      "versus\n",
      "H1 : θ1 < θ < θ2.\n",
      "(7.21)\n",
      "•\n",
      "two-sided; null in center\n",
      "H0 : θ1 ≤θ ≤θ2\n",
      "versus\n",
      "H1 : θ < θ1 or θ > θ2.\n",
      "(7.22)\n",
      "We can examine tests of these hypotheses in the context of the exponential\n",
      "family of Example 7.1.\n",
      "Example 7.6 Testing in the exponential family\n",
      "Suppose we have observations X1, . . ., Xn\n",
      "iid\n",
      "∼exponential(θ). In Example 7.1,\n",
      "we developed a “reasonable” test of the hypothesis H0 :\n",
      "θ ≤θ0 versus\n",
      "H1 : θ > θ0. We will now use the test of equation (7.15) for these hypotheses,\n",
      "as well as the other hypotheses listed above.\n",
      "We will consider a test at the α level for each type of hypothesis. We will\n",
      "develop a test based on the statistic T(X) = X, which is suﬃcient for θ.\n",
      "For the given observations, the likelihood is L(θ; X) = θ−ne−P Xi/θI]0,∞[(θ).\n",
      "•\n",
      "First, we wish to test\n",
      "H0 : θ = θ0\n",
      "versus\n",
      "H1 : θ = θ1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "522\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "We must ﬁnd a cα such that\n",
      "Prθ0\n",
      "\u0000θ−n\n",
      "1\n",
      "e−P Xi/θ1 > cαθ−n\n",
      "0\n",
      "e−P Xi/θ0\u0001\n",
      "+λPrθ0\n",
      "\u0000θ−n\n",
      "1\n",
      "e−P Xi/θ1 = cαθ−n\n",
      "0\n",
      "e−P Xi/θ0\u0001\n",
      "= α.\n",
      "Because the second probability is 0, we have\n",
      "Prθ0\n",
      "\u0010\n",
      "θ−n\n",
      "1\n",
      "e−nX/θ1 > cαθ−n\n",
      "0\n",
      "e−nX/θ0\u0011\n",
      "= α;\n",
      "that is,\n",
      "Prθ0\n",
      "\u0000n log(θ1) + nX/θ1 < −log(cα) + n log(θ0) + nX/θ0\n",
      "\u0001\n",
      "= α.\n",
      "•\n",
      "We wish to test\n",
      "H0 : θ ≤θ0\n",
      "versus\n",
      "H1 : θ > θ0.\n",
      "These are the hypotheses of Example 7.1. A reasonable test may be to\n",
      "reject H0 if T(X) = X > cα, where cα is some ﬁxed positive constant;\n",
      "that is,\n",
      "δ(X) = I]cα,∞[(T(X)).\n",
      "We choose cα so as to control the probability of a type I error.\n",
      "Knowing the distribution of X to be gamma(n, θ/n), we can now work out\n",
      "Pr(δ(X) = 1 | θ) = Pr(T(X) > cα | θ),\n",
      "which, for θ < θ0 is the probability of a type I error. We set up the testing\n",
      "procedure so as to limit the probability of this type of error to be no more\n",
      "than α.\n",
      "For θ ≥θ0\n",
      "1 −Pr(δ(X) = 1 | θ)\n",
      "is the probability of a type II error.\n",
      "Over the full range of θ, we identify the power of the test as\n",
      "β(θ) = Pr(δ(X) = 1 | θ).\n",
      "•\n",
      "We wish to test\n",
      "H0 : θ ≤θ1 or θ ≥θ2\n",
      "versus\n",
      "H1 : θ1 < θ < θ2.\n",
      "This we can do in the same manner as above.\n",
      "•\n",
      "We wish to test\n",
      "H0 : θ1 ≤θ ≤θ2\n",
      "versus\n",
      "H1 : θ < θ1 or θ > θ2.\n",
      "We cannot do this as above; in fact, there is no UMP test. We will explore\n",
      "our options in the next section.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.2 Optimal Tests\n",
      "523\n",
      "Nonexistence of UMP Tests\n",
      "One of the most interesting cases in which a UMP test cannot exist is when the\n",
      "alternative hypothesis is two-sided as in hypothesis (7.22). Hypothesis (7.22)\n",
      "is essentially equivalent to the pair of hypotheses with a simple null:\n",
      "H0 : θ = θ0\n",
      "versus\n",
      "H1 : θ ̸= θ0.\n",
      "If Θ = IRd, it is easy to see that in most cases of practical interest no UMP test\n",
      "can exist for these hypotheses, and you should reason through this statement\n",
      "to see that it is true.\n",
      "So what can we do?\n",
      "This is similar to the problem in point estimation when we realized we\n",
      "could not have an estimator that would uniformly minimize the risk. In that\n",
      "case, we added a requirement of unbiasedness or invariance, or else we added\n",
      "some global property of the risk, such as minimum averaged risk or minimum\n",
      "maximum risk. We might introduce similar criteria for the testing problem.\n",
      "First, let’s consider a desirable property of tests that we will call unbiased-\n",
      "ness.\n",
      "7.2.3 Unbiasedness of Tests\n",
      "Recall that there are a couple of standard deﬁnitions of unbiasedness.\n",
      "•\n",
      "If a random variable X has a distribution with parameter θ, for a point\n",
      "estimator T(X) of an estimand g(θ) to be unbiased means that\n",
      "Eθ(T(X)) = g(θ).\n",
      "Although no loss function is speciﬁed in this meaning of unbiasedness,\n",
      "we know that such an estimator minimizes the risk based on a squared-\n",
      "error loss function. (This last statement is not iﬀ. Under squared-error loss\n",
      "the conditions of minimum risk and unbiasedness deﬁned in this way are\n",
      "equivalent if g is continuous and not constant over any open subset of the\n",
      "parameter space and if Eθ(T(X)) is a continuous function of θ.)\n",
      "•\n",
      "Another deﬁnition of unbiasedness is given with direct reference to a loss\n",
      "function. This is sometimes called L-unbiasedness. The estimator (or more\n",
      "generally, the procedure) T(X) is said to be L-unbiased under the loss\n",
      "function L, if for all θ and ˜θ,\n",
      "Eθ(L(θ, T(X))) ≤Eθ(L(˜θ, T(X))).\n",
      "Notice the subtle diﬀerences in this property and the property of an esti-\n",
      "mator that may result from an approach in which we seek a minimum-risk\n",
      "estimator; that is, an approach in which we seek to solve the minimization\n",
      "problem,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "524\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "min\n",
      "T\n",
      "Eθ(L(θ, T(X)))\n",
      "for all θ. This latter problem does not have a solution. (Recall the approach\n",
      "is to add other restrictions on T(X).)\n",
      "L-unbiasedness under a squared-error also leads to the previous deﬁnition\n",
      "of unbiasedness.\n",
      "Unbiasedness in hypothesis testing is the property that the test is more\n",
      "likely to reject the null hypothesis at any point in the parameter space speciﬁed\n",
      "by the alternative hypothesis than it is at any point in the parameter space\n",
      "speciﬁed by the null hypothesis.\n",
      "Deﬁnition 7.1 (unbiased test)\n",
      "The α-level test δ with power function βδ(θ) = Eθ(δ(X)) for the hypothesis\n",
      "H0 : θ ∈ΘH0 versus H1 : θ ∈ΘH1 is said to be unbiased if\n",
      "βδ(θ) ≤α\n",
      "∀θ ∈ΘH0\n",
      "and\n",
      "βδ(θ) ≥α\n",
      "∀θ ∈ΘH1.\n",
      "Notice that this unbiasedness depends not only on the hypotheses, but also\n",
      "on the signiﬁcance level.\n",
      "This deﬁnition of unbiasedness for a test is L-unbiasedness if the loss\n",
      "function is 0-1.\n",
      "In many cases of interest, the power function βδ(θ) is a continuous function\n",
      "of θ. In such cases, we may be particularly interested in the power on any\n",
      "common boundary point of ΘH0 and ΘH1, that is,\n",
      "B = ∂ΘH0 ∩∂ΘH1.\n",
      "The condition of unbiasedness of Deﬁnition 7.1 implies that βδ(θ) = α for any\n",
      "θ ∈B. We recognize this condition in terms of the similar regions that we\n",
      "have previously deﬁned, and we immediately have\n",
      "Theorem 7.2 An unbiased test with continuous power function is similar on\n",
      "the boundary.\n",
      "7.2.4 UMP Unbiased (UMPU) Tests\n",
      "We will be interested in UMP tests that are unbiased; that is, in UMPU tests.\n",
      "We ﬁrst note that if an α-level UMP test exists, it is unbiased, because\n",
      "its power is at least as great as the power of the constant test (for all x),\n",
      "δ(x) = α. Hence, any UMP test is automatically UMPU.\n",
      "Unbiasedness becomes relevant when no UMP exists, such as when the\n",
      "alternative hypothesis is two-sided:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.2 Optimal Tests\n",
      "525\n",
      "H0 : θ = θ0\n",
      "versus\n",
      "H1 : θ ̸= θ0.\n",
      "Hence, we may restrict our attention to tests with the desirable property of\n",
      "unbiasedness.\n",
      "In the following we consider the hypothesis H0 : θ ∈ΘH0 versus H1 : θ ∈\n",
      "ΘH1, and we seek a test that is UMP within the restricted class of unbiased\n",
      "tests.\n",
      "We will also restrict our attention to hypotheses in which\n",
      "B = ∂ΘH0 ∩∂ΘH1 ̸= ∅,\n",
      "(7.23)\n",
      "and to tests with power functions that are continuous in θ.\n",
      "Theorem 7.3 Let δ(X) be an α-level test of hypotheses satisfying (7.23) that\n",
      "is similar on B and that has continuous power function in θ ∈ΘH0 ∪ΘH1. If\n",
      "δ∗(X) is uniformly most powerful among such tests, then δ∗(X) is a UMPU\n",
      "test.\n",
      "Proof. Because δ∗(X) is uniformly at least as powerful as δ(X) ≡α, δ∗(X)\n",
      "is unbiased, and hence δ∗(X) is a UMPU test.\n",
      "Use of Theorem 7.3, when it applies, is one of the simplest ways of deter-\n",
      "mining a UMPU test, or given a test, to show that it is UMPU. This theorem\n",
      "has immediate applications in tests of hypotheses in exponential families. The-\n",
      "orem 6.4 in MS2 summarizes those results.\n",
      "Similar UMPU tests remain so in the presence of nuisance parameters.\n",
      "***************** more on Neyman structure, similarity\n",
      "7.2.5 UMP Invariant (UMPI) Tests\n",
      "We generally want statistical procedures to be invariant to various transfor-\n",
      "mations of the problem. For example, if the observables X are transformed\n",
      "in some way, it should be possible to transform a “good” test for a certain\n",
      "hypothesis in some obvious way so that the test remains “good” using the\n",
      "transformed data. (This of course means that the hypothesis is also trans-\n",
      "formed.)\n",
      "To address this issue more precisely, we consider transformation groups G,\n",
      "G,**** ﬁx notation and G∗, deﬁned and discussed beginning on page 280.\n",
      "We are often able to deﬁne optimal tests under the restriction of invariance.\n",
      "A test δ is said to be invariant under G, whose domain is the sample space\n",
      "X , if for all x ∈X and g ∈G,\n",
      "δ(g(x)) = δ(x).\n",
      "(7.24)\n",
      "(This is just the deﬁnition of an invariant function, equation (0.1.103).)\n",
      "We seek most powerful invariant tests. (They are invariant because the ac-\n",
      "cept/reject decision does not change.) Because of the meaning of “invariance”\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "526\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "in this context, the most powerful invariant test is uniformly most powerful\n",
      "(UMPI), just as we saw in the case of the equivariant minimum risk estimator.\n",
      "The procedure for ﬁnding UMPI (or just MPI) tests is similar to the proce-\n",
      "dure used in the estimation problem. For a given class of transformations, we\n",
      "ﬁrst attempt to characterize the form of φ, and then to determine the most\n",
      "powerful test of that form. Because of the relationship of invariant functions\n",
      "to a maximal invariant function, we may base our procedure on a maximal\n",
      "invariant function.\n",
      "As an example, consider the group G of translations, for x = (x1, . . ., xn):\n",
      "g(x) = (x1 + c, . . ., xn + c).\n",
      "Just as before, we see that for n > 1, the set of diﬀerences\n",
      "yi = xi −xn\n",
      "for i = 1, . . ., n −1,\n",
      "is invariant under G. This function is also maximal invariant. For x and ˜x, let\n",
      "y(x) = y(˜x). So we have for i = 1, . . ., n −1,\n",
      "˜xi −˜\n",
      "xn = xi −xn\n",
      "= (xi + c) −(xn + c)\n",
      "= g(x),\n",
      "and therefore the function is maximal invariant. Now, suppose we have\n",
      "the sample X = (X1, . . ., Xn) and we wish to test the hypothesis that\n",
      "the density of X is p0(x1 −θ, . . . , xn −θ) versus the alternative that it is\n",
      "p1(x1 −θ, . . . , xn −θ). This testing problem is invariant under the group G of\n",
      "translations, with the induced group of transformations G of the parameter\n",
      "space (which are translations also). Notice that there is only one orbit of G,\n",
      "the full parameter space. The most powerful invariant test will be based on\n",
      "Y = (X1 −Xn, . . ., Xn−1 −Xn). The density of Y under the null hypothesis\n",
      "is given by\n",
      "Z\n",
      "p0(y1 + z, . . ., yn−1 + z, z)dz,\n",
      "and the density of Y under the alternate hypothesis is similar. Because\n",
      "both densities are independent of θ, we have two simple hypotheses, and the\n",
      "Neyman-Pearson lemma gives us the UMP test among the class of invariant\n",
      "tests. The rejection criterion is\n",
      "R p1(y1 + u, . . ., yn + u)du\n",
      "R p0(y1 + u, . . ., yn + u)du > c,\n",
      "for some c.\n",
      "As we might expect, there are cases in which invariant procedures do not\n",
      "exist. For n = 1 there are no invariant functions under G in the translation\n",
      "example above. In such situations, obviously, we cannot seek UMP invariant\n",
      "tests.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.2 Optimal Tests\n",
      "527\n",
      "7.2.6 Equivariance, Unbiasedness, and Admissibility\n",
      "In some problems, the principles of invariance and unbiasedness are com-\n",
      "pletely diﬀerent; and in some cases, one may be relevant and the other totally\n",
      "irrelevant. In other cases there is a close connection between the two.\n",
      "For the testing problem, the most interesting relationship between invari-\n",
      "ance and unbiasedness is that if a unique up to sets of measure zero UMPU\n",
      "test exists, and a UMPI test up to sets of measure zero exists, then the two\n",
      "tests are the same up to sets of measure zero:\n",
      "Theorem 7.4\n",
      "equivalence of UMPI and UMPU\n",
      "Proof.\n",
      "Admissibility of a statistical procedure means that there is no procedure\n",
      "that is at least as “good” as the given procedure everywhere, and better\n",
      "than the given procedure somewhere. In the case of testing “good” means\n",
      "“powerful”, and, of course, everything depends on the level of the test.\n",
      "A UMPU test is admissible, but a UMPI test is not necessarily admissible.\n",
      "7.2.7 Asymptotic Tests\n",
      "We develop various asymptotic tests based on asymptotic distributions of tests\n",
      "and test statistics. For example, the asymptotic distribution of a maximum\n",
      "of a likelihood is a chi-squared and the ratio of two is asymptotically an F .\n",
      "We assume a family of distributions P, a sequence of statistics {δn} based\n",
      "on a random sample X1, . . ., Xn. In hypothesis testing, the standard setup is\n",
      "that we have an observable random variable with a distribution in the family\n",
      "P. Our hypotheses concern a speciﬁc member P ∈P. We want to test\n",
      "H0 : P ∈P0\n",
      "versus\n",
      "H1 : P ∈P1,\n",
      "where P0 ⊆P, P1 ⊆P, and P0 ∩P1 = ∅.\n",
      "We consider a sequence of tests {δn}, with power function β(δn, P ).\n",
      "For use of asymptotic approximations for hypothesis testing, we ﬁrst need\n",
      "the concepts of asymptotic signiﬁcance and limiting size, as discussed on\n",
      "page 314. These concepts apply to the asymptotic behavior of the test un-\n",
      "der the null hypothesis. We also must consider the consistency and uniform\n",
      "consistency, which concern the asymptotic behavior of the test under the al-\n",
      "ternative hypothesis. These properties ensure that the the propbability of a\n",
      "type II error goes to zero. We may also be interested in a diﬀerent type of\n",
      "asymptotic behavior under the null hypothesis, in which we reqiure that the\n",
      "propbability of a type I error go to zero. The concept of Chernoﬀconsistency\n",
      "is relevant here.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "528\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "Deﬁnition 7.2 (Chernoﬀconsistency)\n",
      "The sequence of tests {δn} with power function β(δ(Xn), P ) is Chernoﬀ-\n",
      "consistent for the test iﬀδn is consistent and furthermore,\n",
      "lim\n",
      "n→∞β(δ(Xn), P ) = 0 ∀P ∈P0.\n",
      "(7.25)\n",
      "7.3 Likelihood Ratio Tests, Wald Tests, and Score Tests\n",
      "We see that the Neyman-Pearson Lemma leads directly to use of the ratio of\n",
      "the likelihoods in constructing tests. Now we want to generalize this approach\n",
      "and to study the properties of tests based on that ratio.\n",
      "There are two types of tests that arise from likelihood ratio tests. These\n",
      "are called Wald tests and score tests. Score tests are also called Rao test or\n",
      "Lagrange multiplier tests.\n",
      "The Wald tests and score tests are asymptotically equivalent. They are\n",
      "consistent under the Le Cam regularity conditions, and they are Chernoﬀ-\n",
      "consistent if α is chosen so that as n →∞, α →0 and χ2\n",
      "r,αn ∈o(n).\n",
      "7.3.1 Likelihood Ratio Tests\n",
      "Although as we have emphasized, the likelihood is a function of the distri-\n",
      "bution rather than of the random variable, we want to study its properties\n",
      "under the distribution of the random variable. Using the idea of the ratio as\n",
      "in the test (7.12) of H0 : θ ∈Θ0, but inverting that ratio and including both\n",
      "hypotheses in the denominator, we deﬁne the likelihood ratio as\n",
      "λ(X) = supθ∈Θ0 L(θ; X)\n",
      "supθ∈Θ L(θ; X) .\n",
      "(7.26)\n",
      "The test, similarly to (7.12), rejects H0 if λ(X) ≤cα, where cα is some value\n",
      "in [0, 1]. Tests such as this are called likelihood ratio tests. (We should note\n",
      "that there are other deﬁnitions of a likelihood ratio; in particular, in TSH3\n",
      "its denominator is the sup over the alternative hypothesis. If the alternative\n",
      "hypothesis does not specify Θ −Θ0, such a deﬁnition requires speciﬁcation\n",
      "of both H0, and H1; whereas (7.26) requires speciﬁcation only of H0. Also,\n",
      "the direction of the inequality depends on the ratio; it may be inverted —\n",
      "compare the ratios in (7.12) and (7.26).)\n",
      "The likelihood ratio may not exist, but if it is well deﬁned, clearly it is\n",
      "in the interval [0, 1], and values close to 1 provide evidence that the null\n",
      "hypothesis is true, and values close to 0 provide evidence that it is false.\n",
      "If there is no cα such that\n",
      "Pr(λ(X) ≤cα|H0) = α,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.3 Likelihood Ratio Tests, Wald Tests, and Score Tests\n",
      "529\n",
      "then it is very unlikely that the likelihood ratio test is UMP. In such cases cα\n",
      "is chosen so that Pr(λ(X) ≤cα|H0) < α and a randomization procedure is\n",
      "used to raise the probability of rejection.\n",
      "Example 7.7 Likelihood ratio test in the exponential family (con-\n",
      "tinuation of Example 7.1)\n",
      "We have observations X1, . . ., Xn\n",
      "iid\n",
      "∼exponential(θ). The likelihood is\n",
      "L(θ, ; x) = θ−ne−n¯x/θI]0,∞[(θ).\n",
      "Suppose as before, we wish to test\n",
      "H0 : θ ≤θ0\n",
      "versus\n",
      "H1 : θ > θ0.\n",
      "From equation (7.26), we have\n",
      "λ(X) = max0<θ≤θ0 L(θ; X)\n",
      "max0<θ L(θ; X)\n",
      "= ∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗.\n",
      "From the analysis in Example 7.1, we know that *****************\n",
      "Asymptotic Likelihood Ratio Tests\n",
      "Some of the most important properties of LR tests are asymptotic ones.\n",
      "There are various ways of using the likelihood to build practical tests.\n",
      "Some are asymptotic tests that use MLEs (or RLEs).\n",
      "Regularity Conditions\n",
      "The interesting asymptotic properties of LR tests depend on the Le Cam reg-\n",
      "ularity conditions, which go slightly beyond the Fisher information regularity\n",
      "conditions. (See page 169.)\n",
      "These are the conditions to ensure that supereﬃciency can only occur\n",
      "over a set of Lebesgue measure 0 (Theorem 5.5, page 422), the asymptotic\n",
      "eﬃciency of RLEs (Theorem 6.5, page 482), and the chi-squared asymptotic\n",
      "distribution of the likelihood ratio (Theorem 7.5 below).\n",
      "Asymptotic Signiﬁcance of LR Tests\n",
      "We consider a general form of the null hypothesis,\n",
      "H0 : R(θ) = 0\n",
      "(7.27)\n",
      "versus the alternative\n",
      "H1 : R(θ) ̸= 0,\n",
      "(7.28)\n",
      "for a continuously diﬀerentiable function R(θ) from IRk to IRr. (The notation\n",
      "of MS2, H0 : θ = g(ϑ) where ϑ is a (k −r)-vector, although slightly diﬀerent,\n",
      "is equivalent.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "530\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "Theorem 7.5\n",
      "assuming the Le Cam regularity conditions, says that under H0,\n",
      "−2 log(λn)\n",
      "d→χ2\n",
      "r,\n",
      "where χ2\n",
      "r is a random variable with a chi-squared distribution with r degrees\n",
      "of freedom and r is the number of elements in R(θ). (In the simple case, r is\n",
      "the number of equations in the null hypothesis.)\n",
      "Proof.\n",
      "This allows us to determine the asymptotic signiﬁcance of an LR test. It\n",
      "is also the basis for constructing asymptotically correct conﬁdence sets, as we\n",
      "discuss beginning on page 551.\n",
      "7.3.2 Wald Tests\n",
      "For the hypostheses (7.27) and (7.28), the Wald test uses the test statistic\n",
      "Wn =\n",
      "\u0010\n",
      "R(ˆθ)\n",
      "\u0011T \u0012\u0010\n",
      "S(ˆθ)\n",
      "\u0011T \u0010\n",
      "In(ˆθ)\n",
      "\u0011−1\n",
      "S(ˆθ)\n",
      "\u0013−1\n",
      "R(ˆθ),\n",
      "(7.29)\n",
      "where S(θ) = ∂R(θ)/∂θ and In(θ) is the Fisher information matrix, and these\n",
      "two quantities are evaluated at an MLE or RLE ˆθ. The test rejects the null\n",
      "hypothesis when this value is large.\n",
      "Notice that for the simple hypothesis H0 : θ = θ0, S(θ) = 1, and so this\n",
      "simpliﬁes to\n",
      "(ˆθ −θ0)TIn(ˆθ)(ˆθ −θ0).\n",
      "(7.30)\n",
      "An asymptotic test can be constructed because Wn\n",
      "d→Y , where Y ∼χ2\n",
      "r\n",
      "and r is the number of elements in R(θ). This is proved in Theorem 6.6 of\n",
      "MS2, page 434.\n",
      "The test rejects at the α level if Wn > χ2\n",
      "r,1−α, where χ2\n",
      "r,1−α is the 1 −α\n",
      "quantile of the chi-squared distribution with r degrees of freedom. (Note that\n",
      "MS2 denotes this quantity as χ2\n",
      "r,α.)\n",
      "7.3.3 Score Tests\n",
      "A related test is the Rao score test, sometimes called a Lagrange multiplier\n",
      "test. It is based on a MLE or RLE ˜θ under the restriction that R(θ) = 0\n",
      "(whence the Lagrange multiplier), and rejects H0 when the following is large:\n",
      "Rn = (sn(˜θ))T \u0010\n",
      "In(˜θ)\n",
      "\u0011−1\n",
      "sn(˜θ),\n",
      "(7.31)\n",
      "where sn(θ) = ∂lL(θ)/∂θ, and is called the score function (see page 244).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.3 Likelihood Ratio Tests, Wald Tests, and Score Tests\n",
      "531\n",
      "An asymptotic test can be constructed because Rn\n",
      "d→Y , where Y ∼χ2\n",
      "r\n",
      "and r is the number of elements in R(θ). This is proved in Theorem 6.6 (ii)\n",
      "of MS2.\n",
      "The test rejects at the α level if Rn > χ2\n",
      "r,1−α, where χ2\n",
      "r,1−α is the 1 −α\n",
      "quantile of the chi-squared distribution with r degrees of freedom.\n",
      "7.3.4 Examples\n",
      "Example 7.8 tests in a binomial model\n",
      "*****\n",
      "H0 : π = π0 versus H0 : π ̸= π0\n",
      "Wald – uses estimated values\n",
      "Wn =\n",
      "ˆπ −π0\n",
      "q\n",
      "ˆπ(1−ˆπ)\n",
      "n\n",
      "problems when x = 0\n",
      "asymptotically valid of course, but not good for ﬁnite (especially for small)\n",
      "n\n",
      "Score – uses hypothesized values as well as estimated values\n",
      "Rn =\n",
      "ˆπ −π0\n",
      "q\n",
      "π0(1−π0)\n",
      "n\n",
      "no problems when x = 0\n",
      "usually better than Wald for ﬁnite (especially for small) n\n",
      "Example 7.9 tests in a linear model\n",
      "Consider a general regression model:\n",
      "Xi = f(zi, β) + ϵi,\n",
      "where ϵi\n",
      "iid\n",
      "∼N(0, σ2).\n",
      "(7.32)\n",
      "For given k × r matrix L, we want to test\n",
      "H0 : Lβ = β0.\n",
      "(7.33)\n",
      "Let X be the sample (it’s an n-vector). Let Z be the matrix whose rows\n",
      "are the zi.\n",
      "The log likelihood is\n",
      "log ℓ(β; X) = c(σ2) −\n",
      "1\n",
      "2σ2 (X −f(Z, β))T(X −f(Z, β)).\n",
      "The MLE is the LSE, ˆβ.\n",
      "Let ˜β be the maximizer of the log likelihood under the restriction Lβ = β0.\n",
      "The likelihood ratio is the same as the diﬀerence in the log likelihoods.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "532\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "The maximum of the unrestricted log likelihood (minus a constant) is the\n",
      "minimum of the residuals:\n",
      "1\n",
      "2σ2 (X −f(Z, ˆβ))T(X −f(Z, ˆβ)) =\n",
      "1\n",
      "2σ2 SSE(ˆβ)\n",
      "and likewise, for the restricted:\n",
      "1\n",
      "2σ2 (X −f(Z, ˜β))T(X −f(Z, ˜β)) =\n",
      "1\n",
      "2σ2 SSE(˜β).\n",
      "Now, the diﬀerence,\n",
      "SSE(ˆβ) −SSE(˜β)\n",
      "σ2\n",
      ",\n",
      "has an asymptotic χ2(r) distribution. (Note that the 2 goes away.)\n",
      "We also have that\n",
      "SSE(ˆβ)\n",
      "σ2\n",
      "has an asymptotic χ2(n −k) distribution.\n",
      "So for the likelihood ratio test we get an F -type statistic:\n",
      "(SSE(ˆβ) −SSE(˜β))/r\n",
      "SSE(ˆβ)/(n −k)\n",
      ".\n",
      "(7.34)\n",
      "Use unrestricted MLE ˆβ and consider Lˆβ −β0.\n",
      "V(ˆβ) →\n",
      "\u0010\n",
      "JT\n",
      "f( ˆβ)Jf( ˆβ)\n",
      "\u0011−1\n",
      "σ2,\n",
      "and so\n",
      "V(Lˆβ) →L\n",
      "\u0010\n",
      "JT\n",
      "f( ˆβ)Jf( ˆβ)\n",
      "\u0011−1\n",
      "LTσ2,\n",
      "where Jf( ˆβ) is the n × k Jacobian matrix.\n",
      "Hence, we can write an asymptotic χ2(r) statistic as\n",
      "(Lˆβ −β0)T\n",
      "\u0012\n",
      "L\n",
      "\u0010\n",
      "JT\n",
      "f( ˆβ)Jf( ˆβ)\n",
      "\u0011−1\n",
      "LTs2\n",
      "\u0013−1\n",
      "(Lˆβ −β0)\n",
      "We can form a Wishart-type statistic from this.\n",
      "If r = 1, L is just a vector (the linear combination), and we can take the\n",
      "square root and from a “pseudo t”:\n",
      "LT ˆβ −β0\n",
      "s\n",
      "r\n",
      "LT\n",
      "\u0010\n",
      "JT\n",
      "f( ˆβ)Jf( ˆβ)\n",
      "\u0011−1\n",
      "L\n",
      ".\n",
      "Get MLE with the restriction Lβ = β0 using a Lagrange multiplier, λ of\n",
      "length r.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.3 Likelihood Ratio Tests, Wald Tests, and Score Tests\n",
      "533\n",
      "Minimize\n",
      "1\n",
      "2σ2 (X −f(Z, β))T(X −f(Z, β)) + 1\n",
      "σ2 (Lβ −β0)Tλ.\n",
      "Diﬀerentiate and set = 0:\n",
      "−JT\n",
      "f( ˆβ)(X −f(Z, ˆβ)) + LTλ = 0\n",
      "Lˆβ −β0 = 0.\n",
      "JT\n",
      "f( ˆβ)(X −f(Z, ˆβ)) is called the score vector. It is of length k.\n",
      "Now V(X −f(Z, ˆβ)) →σ2In, so the variance of the score vector, and\n",
      "hence, also of LTλ, goes to σ2JT\n",
      "f(β)Jf(β).\n",
      "(Note this is the true β in this expression.)\n",
      "Estimate the variance of the score vector with ˜σ2JT\n",
      "f( ˜β)Jf( ˜β),\n",
      "where ˜σ2 = SSE(˜β)/(n −k + r).\n",
      "Hence, we use LT˜λ and its estimated variance.\n",
      "Get\n",
      "1\n",
      "˜σ2 ˜λTL\n",
      "\u0010\n",
      "JT\n",
      "f( ˜β)Jf( ˜β)\n",
      "\u0011−1\n",
      "LT˜λ\n",
      "(7.35)\n",
      "It is asymptotically χ2(r).\n",
      "This is the Lagrange multiplier form.\n",
      "Another form:\n",
      "Use JT\n",
      "f( ˜β)(X −f(Z, ˜β)) in place of LT˜λ.\n",
      "Get\n",
      "1\n",
      "˜σ2 (X −f(Z, ˜β))TJf( ˜β)\n",
      "\u0010\n",
      "JT\n",
      "f( ˜β)Jf( ˜β)\n",
      "\u0011−1\n",
      "JT\n",
      "f( ˜β)(X −f(Z, ˜β))\n",
      "(7.36)\n",
      "This is the score form. Except for the method of computing it, it is the\n",
      "same as the Lagrange multiplier form.\n",
      "This is the SSReg in the AOV for a regression model.\n",
      "Example 7.10 an anomalous score test\n",
      "Morgan et al. (2007) illustrate some interesting issues using a simple example\n",
      "of counts of numbers of stillbirths in each of a sample of litters of laboratory\n",
      "animals. They suggest that a zero-inﬂated Poisson is an appropriate model.\n",
      "This distribution is an ω mixture of a point mass at 0 and a Poisson distribu-\n",
      "tion. The CDF (in a notation we will use often later) is\n",
      "P0,ω(x|λ) = (1 −ω)P (x|λ) + ωI[0,∞[(x),\n",
      "where P (x) is the Poisson CDF with parameter λ.\n",
      "(Write the PDF (under the counting measure). Is this a reasonable prob-\n",
      "ability model? What are the assumptions? Do the litter sizes matter?)\n",
      "If we denote the number of litters in which the number of observed still-\n",
      "births is i by ni, the log-likelihood function is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "534\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "l(ω, λ) = n0 log \u0000ω + (1 −ω)e−λ\u0001+\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "ni log(1−ω)−\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "niλ+\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "ini log(λ)+c.\n",
      "Suppose we want to test the null hypothesis that ω = 0.\n",
      "The score test has the form\n",
      "sTJ−1s,\n",
      "where s is the score vector and J is either the observed or the expected\n",
      "information matrix. For each we substitute ω = 0 and λ = ˆλ0, where ˆλ0 =\n",
      "P∞\n",
      "i=1 ini/n with n = P∞\n",
      "i=0 ni, which is the MLE when ω = 0.\n",
      "Let\n",
      "n+ =\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "ni\n",
      "and\n",
      "d =\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "ini.\n",
      "The frequency of 0s is important. Let\n",
      "f0 = n0/n.\n",
      "Taking the derivatives and setting ω = 0, we have\n",
      "∂l\n",
      "∂ω = n0eλ −n,\n",
      "∂l\n",
      "∂λ = −n + d/λ,\n",
      "∂2l\n",
      "∂ω2 = −n −n0e2λ + n0eλ,\n",
      "∂2l\n",
      "∂ωλ = n0eλ,\n",
      "and\n",
      "∂2l\n",
      "∂λ2 = −d/λ2.\n",
      "So, substituting the observed data and the restricted MLE, we have ob-\n",
      "served information matrix\n",
      "O(0, ˆλ0) = n\n",
      "\"\n",
      "1 + f0e2ˆλ0 −2f0eˆλ0\n",
      "−f0eˆλ0\n",
      "−f0eˆλ0\n",
      "1/ˆλ0\n",
      "#\n",
      ".\n",
      "Now, for the expected information matrix when ω = 0, we ﬁrst observe that\n",
      "E(n0) = ne−λ, E(d) = nλ, and E(n+) = n(1 −e−λ); hence\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.4 Nonparametric Tests\n",
      "535\n",
      "I(0, ˆλ0) = n\n",
      "\"\n",
      "eˆλ0 −1\n",
      "−1\n",
      "−1\n",
      "1/ˆλ0\n",
      "#\n",
      ".\n",
      "Hence, the score test statistic can be written as\n",
      "κ(ˆλ0)(n0e\n",
      "ˆλ0 −n)2,\n",
      "where κ(ˆλ0) is the (1,1) element of the inverse of either O(0, ˆλ0) or I(0, ˆλ0).\n",
      "Inverting the matrices (they are 2 × 2), we have as the test statistic for\n",
      "the score test, either\n",
      "sI =\n",
      "ne−ˆλ0(1 −θ)2\n",
      "1 −e−ˆλ0 −ˆλ0e−ˆλ0\n",
      "or\n",
      "sO =\n",
      "ne−ˆλ0(1 −θ)2\n",
      "e−ˆλ0 + θ −2θe−ˆλ0θ2ˆλ0e−ˆλ0 ,\n",
      "where θ = f0eˆλ0, which is the ratio of the observed proportion of 0 counts to\n",
      "the estimated probability of a zero count under the Poisson model. (If n0 is\n",
      "actually the number expected under the Poisson model, then θ = 1.)\n",
      "Now consider the actual data reported by Morgan et al. (2007) for still-\n",
      "births in each litter of a sample of 402 litters of laboratory animals.\n",
      "No. stillbirths\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "No. litters\n",
      "314\n",
      "48 20\n",
      "7\n",
      "5\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "For these data, we have n = 402, d = 185, ˆλ0 = 0.4602, e−ˆλ0 = 0.6312,\n",
      "and θ = 1.2376.\n",
      "What is interesting is the diﬀerence in sI and sO.\n",
      "In this particular example, if all ni for i ≥1 are held constant at the\n",
      "observed values, but diﬀerent values of n0 are considered, as n0 increases the\n",
      "ratio sI/sO increases from about 1/4 to 1 (when the n0 is the expected number\n",
      "under the Poisson model; i.e., θ = 1), and then decreases, actually becoming\n",
      "negative (around n0 = 100).\n",
      "This example illustrates an interesting case. The score test is inconsistent\n",
      "because the observed information generates negative variance estimates at the\n",
      "MLE under the null hypothesis. (The score test can also be inconsistent if the\n",
      "expected likelihood equation has spurious roots.)\n",
      "7.4 Nonparametric Tests\n",
      "7.4.1 Permutation Tests\n",
      "For i = 1, 2, given the random Xi1, . . ., Xini from a a distribution with con-\n",
      "tinuous CDF Fi, we wish to test\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "536\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "H0 : F1 = F2\n",
      "versus\n",
      "H0 : F1 ̸= F2.\n",
      "Π(X), where X = {Xij : i = 1, 2; j = 1, . . ., ni}\n",
      "Π({X1, . . ., Xk}), for any Π, where Π(A) denotes a permutation of the\n",
      "elements of the set A.\n",
      "***************\n",
      "7.4.2 Sign Tests and Rank Tests\n",
      "We have a sample X1, . . ., Xn and the associated ranks of the absolute values,\n",
      "eR(X). We denote the subvector of eR(X) that corresponds to positive values\n",
      "of X as R+(X), and let n∗be the number of positive values of X. We let\n",
      "Ro\n",
      "+(X) be the vector of the elements of R+(X) in increasing order. We let J\n",
      "be a continuous and strictly increasing function on [0, 1], and let\n",
      "W(Ro\n",
      "+) = J(Ro\n",
      "+1/n) + · · · + J(Ro\n",
      "+n∗/n)\n",
      "(7.37)\n",
      "7.4.3 Goodness of Fit Tests\n",
      "Kolmogorov-Smirnov (KS) Test\n",
      "If P1 and P2 are CDFs, the L∞norm of their diﬀerence is called the Kol-\n",
      "mogorov distance between the two distributions; that is, the Kolmogorov dis-\n",
      "tance between two CDFs P1 and P2, written as ρK(P1, P2), is deﬁned as\n",
      "ρK(P1, P2) = sup |P1 −P2|.\n",
      "(7.38)\n",
      "Because a CDF is bounded by 0 and 1, it is clear that\n",
      "ρK(P1, P2) ≤1,\n",
      "(7.39)\n",
      "and if the supports S1 and S2 are such that x1 ∈S1 and x2 ∈S2 implies x1 ≤\n",
      "x2 with at least one x1 ∈S1 is less than some x2 ∈S2, then ρK(P1, P2) = 1.\n",
      "If one or both of P1 and P2 are ECDFs we can compute the Kolmogorov\n",
      "distance fairly easily using the order statistics.\n",
      "*** distribution\n",
      "Cram´er von Mises Test\n",
      "7.4.4 Empirical Likelihood Ratio Tests\n",
      "7.5 Multiple Tests\n",
      "In many applications, we test several hypotheses using only one set of obser-\n",
      "vations. For example in the one-way ﬁxed-eﬀects AOV model\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.5 Multiple Tests\n",
      "537\n",
      "Yij = µ + αi + ϵij,\n",
      "i = 1, . . ., m,\n",
      "j = 1, . . ., n,\n",
      "the usual hypothesis of interest is\n",
      "H0 : α1 = · · · = αm = 0.\n",
      "This can be thought of as several separate hypotheses, H0i : αi = 0; and the\n",
      "researcher is not just interested in whether all αi = 0. Which eﬀects are larger,\n",
      "or which set of eﬀects diﬀer for other eﬀects, and similar questions would be\n",
      "of interest.\n",
      "In other types of applications, diﬀerent populations may be tested in the\n",
      "same way and in some tests the null hypothesis is rejected and in others it\n",
      "is not. An example of such a situation is in seeking to identify human genes\n",
      "associated with a particular condition, say a certain type of illness. A common\n",
      "way of doing this is by use of DNA microarrays. The observations come from\n",
      "a set of subjects with the condition and from another set of subjects without\n",
      "the condition. A large number of genes from each subject are compared across\n",
      "the two groups. An hypothesis of “no diﬀerence” is to be tested for each gene.\n",
      "What is the probability that a gene will erroneously be identiﬁed as diﬀerent\n",
      "across the two groups; that is, what is the probability that a type I error\n",
      "occurs in such a situation?\n",
      "In such cases of multiple testing, the question of errors is not so straight-\n",
      "forward. We begin by considering a single error within a family of tests. The\n",
      "family is any group of tests that we want to consider together. In the one-way\n",
      "model, the family would be the multiple comparisons among the eﬀects. The\n",
      "probability of a type I error in any test in the family is called the family wise\n",
      "error rate, or FWER.\n",
      "There are several ways to measure the errors. Letting m be the number of\n",
      "tests, F be the number of false positives, T be the number of true positives, and\n",
      "S = F + T be the total number of “discoveries”, or rejected null hypotheses,\n",
      "we deﬁne various error measures. The family wise error rate, as above, is\n",
      "FWER = Pr(F ≥1).\n",
      "The per comparison error rate is\n",
      "PCER = E(F )/m.\n",
      "The false discovery rate is\n",
      "FDR = E(F/S).\n",
      "The false nondiscovery rate is\n",
      "FNR = E(T/S).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "538\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "The Benjamini-Hochberg (BH) method for controlling FDR works as fol-\n",
      "lows. First, order the m p-values from the tests: p1 ≤· · · ≤pm. Then de-\n",
      "termine a threshold value for rejection by ﬁnding the largest integer j such\n",
      "that pj ≤jα/m. Finally, reject any hypothesis whose p-value is smaller than\n",
      "or equal to pj. Benjamini and Hochberg (1995) prove that this procedure is\n",
      "guaranteed to force FDR ≤α. Genovese and Wasserman (2002), however,\n",
      "showed that this procedure does not minimize FNR subject to FDR ≤α for\n",
      "a given α.\n",
      "Example 7.11 Variable selection in a linear regression model\n",
      "One of the most common instances of multiple hypotheses testing is in the\n",
      "variable selection problem in linear regression.\n",
      "7.6 Sequential Tests\n",
      "In the simplest formulation of statistical hypothesis testing, corresponding\n",
      "to the setup of the Neyman-Pearson lemma, we test a given hypothesized\n",
      "distribution versus another given distribution. After setting some ground rules\n",
      "regarding the probability of falsely rejecting the null hypothesis, and then\n",
      "determining the optimal test in the case of simple hypotheses, we determined\n",
      "more general optimal tests in cases for which they exist, and for other cases,\n",
      "we determined optimal tests among classes of tests that had certain desirable\n",
      "properties. In some cases, the tests involved regions within the sample space\n",
      "in which the decision between the two hypotheses was made randomly; that is,\n",
      "based on a random process over and above the randomness of the distributions\n",
      "of interest.\n",
      "Another logical approach to take when the data generated by the process of\n",
      "interest does not lead to a clear decision is to decide to take more observations.\n",
      "Recognizing at the outset that this is a possibility, we may decide to design\n",
      "the test as a sequential procedure. We take a small number of observations,\n",
      "and if the evidence is strong enough either to accept the null hypothesis or\n",
      "to reject it, the test is complete and we make the appropriate decision. On\n",
      "the other hand, if the evidence from the small sample is not strong enough,\n",
      "we take some additional observations and perform the test again. We repeat\n",
      "these steps as necessary.\n",
      "Sequential procedures always present issues that may aﬀect the inference.\n",
      "There are various kinds of sequential procedures. Example 7.12, which revis-\n",
      "its an inference problem in a Bernoulli distribution that has been considered\n",
      "in Examples 3.12 and 6.1, sets up an experiment in which Bernoulli random\n",
      "variables are to be observed until a speciﬁed number of successes are observed.\n",
      "Although in some sense this is a sequential procedure (and it does raise ques-\n",
      "tions about the principles underlying our statistical inference), the stopping\n",
      "rule is not dependent on the inference.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.7 The Likelihood Principle and Tests of Hypotheses\n",
      "539\n",
      "In a sequential testing procedure, at any point, the question of whether\n",
      "or not to continue observing random variables depends on the inference that\n",
      "could be made at that point. If the hypothesis can be rejected or if it is\n",
      "very unlikely that it can be rejected, the decision is made, and the test is\n",
      "terminated; otherwise, the test continues. When we refer to a “sequential\n",
      "test”, this is the type of situation we have in mind.\n",
      "7.6.1 Sequential Probability Ratio Tests\n",
      "Let us again consider the test of a simple null hypothesis against a simple\n",
      "alternative. Thinking of the hypotheses in terms of a parameter θ that indexes\n",
      "these two PDFs by θ0 and θ1, for a sample of size n, we have the likelihoods\n",
      "associated with the two hypotheses as Ln(θ0; x) and Ln(θ1; x). The best test\n",
      "indicates that we should reject if\n",
      "Ln(θ0; x)\n",
      "Ln(θ1; x) ≤k,\n",
      "for some appropriately chosen k.\n",
      "deﬁne and show optimality\n",
      "7.6.2 Sequential Reliability Tests\n",
      "7.7 The Likelihood Principle and Tests of Hypotheses\n",
      "*** introduce; refer to likelihood in N-P\n",
      "Tests of Hypotheses that Depend on the Data-Generating Process\n",
      "***\n",
      "Example 7.12 Sampling in a Bernoulli distribution; p-values and\n",
      "the likelihood principle revisited\n",
      "In Examples 3.12 and 6.1, we have considered the family of Bernoulli distri-\n",
      "butions that is formed from the class of the probability measures Pπ({1}) = π\n",
      "and Pπ({0}) = 1 −π on the measurable space (Ω= {0, 1}, F = 2Ω). Suppose\n",
      "now we wish to test\n",
      "H0 : π ≥0.5\n",
      "versus\n",
      "H1 : π < 0.5.\n",
      "As we indicated in Example 3.12 there are two ways we could set up an\n",
      "experiment to make inferences on π. One approach is to take a random sample\n",
      "of size n, X1, . . ., Xn from the Bernoulli(π), and then use some function of\n",
      "that sample as an estimator. An obvious statistic to use is the number of 1’s\n",
      "in the sample, that is, T = P Xi. To assess the performance of an estimator\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "540\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "using T, we would ﬁrst determine its distribution and then use the properties\n",
      "of that distribution to decide what would be a good estimator based on T.\n",
      "A very diﬀerent approach is to take a sequential sample, X1, X2, . . ., until\n",
      "a ﬁxed number t of 1’s have occurred. This yields N, the number of trials\n",
      "until t 1’s have occurred.\n",
      "The distribution of T is binomial with parameters n and π; its PDF is\n",
      "pT (t ; n, π) =\n",
      "\u0012n\n",
      "t\n",
      "\u0013\n",
      "πt(1 −π)n−t,\n",
      "t = 0, 1, . . ., n.\n",
      "(7.40)\n",
      "The distribution of N is the negative binomial with parameters t and π,\n",
      "and its PDF is\n",
      "pN(n ; t, π) =\n",
      "\u0012n −1\n",
      "t −1\n",
      "\u0013\n",
      "πt(1 −π)n−t,\n",
      "n = t, t + 1, . . ..\n",
      "(7.41)\n",
      "Suppose we do this both ways. We choose n = 12 for the ﬁrst method\n",
      "and t = 3 for the second method. Now, suppose that for the ﬁrst method, we\n",
      "observe T = 3 and for the second method, we observe N = 12. The ratio of\n",
      "the likelihoods satisﬁes equation (6.4); that is, it does not involve π, so by the\n",
      "likelihood principle, we should make the same conclusions about π.\n",
      "Let us now compute the respective p-values for a one-sided test. For the\n",
      "binomial setup we get p = 0.073 (using the R function pbinom(3,12,0.5)),\n",
      "but for the negative binomial setup we get p = 0.033 (using the R function\n",
      "1-pnbinom(8,3,0.5) in which the ﬁrst argument is the number of “failures”\n",
      "before the number of “successes” speciﬁed in the second argument). The p-\n",
      "values are diﬀerent, and in fact, if we had decided to perform the test at the\n",
      "α = 0.05 signiﬁcance level, in one case we would reject the null hypothesis\n",
      "and in the other case we would not.\n",
      "Further comments on Example 7.12\n",
      "The problem in a signiﬁcance test as we have just described is determining\n",
      "what is “more extreme”. In the sampling design that speciﬁes a stopping\n",
      "rule based on n that leads to a binomial distribution, “more extreme” means\n",
      "T ∈{0, . . ., t}, and in a data-generating process that depends on a speciﬁed\n",
      "number of 1’s, “more extreme” means N ∈{n, n + 1, . . .}.\n",
      "Notice the key element in the example: in one case, the experiment that\n",
      "was conducted to gather information is completely independent of what is\n",
      "observed, while in the other case the experiment itself depended on what is\n",
      "observed. The latter type of experiment is often a type of Markov process in\n",
      "which there is a stopping time as in equation (1.262).\n",
      "This example illustrates a basic conﬂict between the likelihood principle\n",
      "and signiﬁcance testing. Observance of the likelihood principle while making\n",
      "inferences about a probability distribution leads us to ignore the overall data-\n",
      "generating process. The likelihood principle states that only the observed data\n",
      "are relevant.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.8 Conﬁdence Sets\n",
      "541\n",
      "Monte Carlo simulation would be an appropriate way to study this sit-\n",
      "uation. The data could be obtained by a process that involves a stopping\n",
      "rule, and the tests could be performed in a manner that ignores the process.\n",
      "Whether or not the test is valid could be assessed by evaluating the p-value\n",
      "under the null hypothesis.\n",
      "You are asked to explore this issue in Exercise 7.5.\n",
      "Evidence Supporting Hypotheses\n",
      "Example 7.4 is a good illustration of the Neyman-Pearson solution to a simple\n",
      "hypothesis testing problem. We might look at this problem in a slightly diﬀer-\n",
      "ent context, however, as suggested on page 318. This particular example, in\n",
      "fact, is used in Royall (1997) to show how the data actually provide evidence\n",
      "that might contradict our decision based on the Neyman-Pearson hypothesis\n",
      "testing approach.\n",
      "Example 7.13 Critique of the hypothesis test of Example 7.4\n",
      "Let’s consider the Bernoulli distribution of Example 7.4 and the two hypothe-\n",
      "ses regarding the parameter π, H0 : π = 1/4 and H1 : π = 3/4. The test is\n",
      "based on x, the total number of 1s in n trials. When n = 30, the Neyman-\n",
      "Pearson testing procedure at the level α = 0.05 rejects H0 in favor of H1 if\n",
      "x ≥13.\n",
      "Looking at the problem of choosing H0 or H1 based on the evidence in the\n",
      "data, however, we might ask what evidence x = 13 provides. The likelihood\n",
      "ratio in equation 7.16 is\n",
      "L(P1, x)\n",
      "L(P0, x) = 1/81,\n",
      "which would seem to be rather compelling evidence in favor of H0 over H1.\n",
      "An additional problem with the test in Example 7.4 is the manner in which\n",
      "a decision is made if x = 12. In order to achieve an exact size of α = 0.05,\n",
      "a randomization procedure that does not depend on the evidence of the data\n",
      "is required. This kind of randomization procedure does not seem to be a\n",
      "reasonable way to make a statistical decision.\n",
      "The alternative approach to hypothesis testing involves a comparison of\n",
      "the evidence from the data in favor of each of the competing hypotheses.\n",
      "This approach is similar to the use of the Bayes factor discussed in Sec-\n",
      "tion 4.5.3.\n",
      "7.8 Conﬁdence Sets\n",
      "For statistical conﬁdence sets, the basic problem is to use a random sample\n",
      "X from an unknown distribution P to determine a random subfamily A(X)\n",
      "of a given family of distributions P such that\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "542\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "PrP (PS ∋P ) ≥1 −α\n",
      "∀P ∈P,\n",
      "(7.42)\n",
      "for some given α ∈]0, 1[. The set PS is called a 1−α conﬁdence set or conﬁdence\n",
      "set. The “conﬁdence level” is 1 −α, so we sometimes call it a “level 1 −α\n",
      "conﬁdence set”. Notice that α is given a priori. We call\n",
      "inf\n",
      "P∈P PrP (PS ∋P )\n",
      "(7.43)\n",
      "the conﬁdence coeﬃcient of PS.\n",
      "If the conﬁdence coeﬃcient of PS is > 1 −α, then PS is said to be a\n",
      "conservative 1 −α conﬁdence set.\n",
      "We generally wish to determine a region with a given conﬁdence coeﬃcient,\n",
      "rather than with a given signiﬁcance level.\n",
      "If the distributions are characterized by a parameter θ in a given parameter\n",
      "space Θ an equivalent 1 −α conﬁdence set for θ is a random subset ΘS such\n",
      "that\n",
      "Prθ (ΘS ∋θ) ≥1 −α\n",
      "∀θ ∈Θ.\n",
      "(7.44)\n",
      "The basic paradigm of statistical conﬁdence sets was described in Sec-\n",
      "tion 3.5.2, beginning on page 296. We ﬁrst review some of those basic ideas,\n",
      "starting ﬁrst with simple interval conﬁdence sets. Then in Section 7.9 we dis-\n",
      "cuss optimality of conﬁdence sets.\n",
      "As we have seen in other problems in statistical inference, it is often not\n",
      "possible to develop a procedure that is uniformly optimal. As with the estima-\n",
      "tion problem, we can impose restrictions, such as unbiasedness or equivariance.\n",
      "We can deﬁne optimality in terms of a global averaging over the family\n",
      "of distributions of interest. If the the global averaging is considered to be a\n",
      "true probability distribution, then the resulting conﬁdence intervals can be\n",
      "interpreted diﬀerently, and it can be said that the probability that the distri-\n",
      "bution of the observations is in some ﬁxed family is some stated amount. The\n",
      "HPD Bayesian credible regions discussed in Section 4.6.2 can also be thought\n",
      "of as optimal sets that address similar applications in which conﬁdence sets\n",
      "are used.\n",
      "Because determining an exact 1 −α conﬁdence set requires that we know\n",
      "the exact distribution of some statistic, we often have to form approximate\n",
      "conﬁdence sets. There are three common ways that we do this as discussed\n",
      "in Section 3.1.4. In Section 7.10 we discuss asymptotic conﬁdence sets, and in\n",
      "Section 7.11, bootstrap conﬁdence sets.\n",
      "Our usual notion of a conﬁdence interval relies on a frequency approach to\n",
      "probability, and it leads to the deﬁnition of a 1−α conﬁdence interval for the\n",
      "(scalar) parameter θ as the random interval [TL, TU], that has the property\n",
      "Pr (TL ≤θ ≤TU) = 1 −α.\n",
      "This is also called a (1 −α)100% conﬁdence interval. The interval [TL, TU ] is\n",
      "not uniquely determined.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.8 Conﬁdence Sets\n",
      "543\n",
      "The concept extends easily to vector-valued parameters. A simple exten-\n",
      "sion would be merely to let TL and TU, and let the conﬁdence set be hyper-\n",
      "rectangle deﬁned by the cross products of the intervals. Rather than taking\n",
      "vectors TL and TU, however, we generally deﬁne other types of regions; in\n",
      "particular, we often take an ellipsoidal region whose shape is determined by\n",
      "the covariances of the estimators.\n",
      "A realization of the random interval, say [tL, tU], is also called a conﬁdence\n",
      "interval. Although it may seem natural to state that the “probability that θ\n",
      "is in [tL, tU] is 1 −α”, this statement can be misleading unless a certain\n",
      "underlying probability structure is assumed.\n",
      "In practice, the interval is usually speciﬁed with respect to an estimator\n",
      "of θ, T(X). If we know the sampling distribution of T −θ, we may determine\n",
      "c1 and c2 such that\n",
      "Pr (c1 ≤T −θ ≤c2) = 1 −α;\n",
      "and hence\n",
      "Pr (T −c2 ≤θ ≤T −c1) = 1 −α.\n",
      "If either TL or TU is inﬁnite or corresponds to a bound on acceptable values\n",
      "of θ, the conﬁdence interval is one-sided. For two-sided conﬁdence intervals,\n",
      "we may seek to make the probability on either side of T to be equal. This\n",
      "is called an equal-tail conﬁdence interval.\n",
      "We may, rather, choose to make\n",
      "c1 = −c2, and/or to minimize |c2 −c1| or |c1| or |c2|. This is similar in spirit\n",
      "to seeking an estimator with small variance.\n",
      "Prediction Sets\n",
      "We often want to identify a set in which a future observation on a random\n",
      "variable has a high probability of occurring. This kind of set is called a pre-\n",
      "diction set.\n",
      "For example, we may assume a given sample X1, . . ., Xn is from a N(µ, σ2)\n",
      "and we wish to determine a measurable set C(X) such that for a future ob-\n",
      "servation Xn+1\n",
      "inf\n",
      "P∈P PrP (Xn+1 ∈C(X)) ≥1 −α.\n",
      "More generally, instead of Xn+1, we could deﬁne a prediction interval for\n",
      "any random variable Y .\n",
      "The diﬀerence in this and a conﬁdence set for µ is that there is an addi-\n",
      "tional source of variation. The prediction set will be larger, so as to account\n",
      "for this extra variation.\n",
      "We may want to separate the statements about Xn+1 or Y and C(X). A\n",
      "tolerance set attempts to do this.\n",
      "Given a sample X, a measurable set S(X), and numbers δ and α in ]0, 1[,\n",
      "if\n",
      "inf\n",
      "P∈P(PrP (Y ∈S(X)|X) ≥δ) ≥1 −α,\n",
      "then S(X) is called a δ-tolerance set for Y with conﬁdence level 1 −α.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "544\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "Randomized conﬁdence Sets\n",
      "For discrete distributions, as we have seen, sometimes to achieve a test of a\n",
      "speciﬁed size, we had to use a randomized test.\n",
      "Conﬁdence sets may have exactly the same problem – and solution – in\n",
      "forming conﬁdence sets for parameters in discrete distributions. We form ran-\n",
      "domized conﬁdence sets. The idea is the same as in randomized tests, and\n",
      "we will discuss randomized conﬁdence sets in the context of hypothesis tests\n",
      "below.\n",
      "Pivotal Functions\n",
      "A straightforward way to form a conﬁdence interval is to use a function of the\n",
      "sample that also involves the parameter of interest, but that does not involve\n",
      "any nuisance parameters. This kind of function is called a pivotal function.\n",
      "The conﬁdence interval is then formed by separating the parameter from the\n",
      "sample values, as in Example 3.24 on page 298.\n",
      "Example 7.14 Conﬁdence Interval for a Quantile\n",
      "***distribution free\n",
      "For a given parameter and family of distributions there may be multiple\n",
      "pivotal values. For purposes of statistical inference, such considerations as\n",
      "unbiasedness and minimum variance may guide the choice of a pivotal value\n",
      "to use.\n",
      "Approximate Pivot Values\n",
      "It may not be possible to identify a pivotal quantity for a particular parameter.\n",
      "In that case, we may seek an approximate pivot. A function is asymptotically\n",
      "pivotal if a sequence of linear transformations of the function is pivotal in the\n",
      "limit as n →∞.\n",
      "*** nuisance parameters ***** ﬁnd consistent estimator\n",
      "If the distribution of T is known, c1 and c2 can be determined. If the\n",
      "distribution of T is not known, some other approach must be used. A common\n",
      "method is to use some numerical approximation to the distribution. Another\n",
      "method is to use bootstrap samples from the ECDF.\n",
      "Relation to Acceptance Regions of Hypothesis Tests\n",
      "A test at the α level has a very close relationship with a 1−α level conﬁdence\n",
      "set.\n",
      "When we test the hypothesis H0 :\n",
      "θ ∈ΘH0 at the α level, we form\n",
      "a critical region for a test statistic or rejection region for the values of the\n",
      "observable X. This region is such that the probability that the test statistic\n",
      "is in it is ≤α.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.8 Conﬁdence Sets\n",
      "545\n",
      "For any given θ0 ∈Θ, consider the nonrandomized test Tθ0 for testing the\n",
      "simple hypothesis H0 : θ = θ0, against some alternative H1. We let A(θ0) be\n",
      "the set of all x such that the test statistic is not in the critical region; that is,\n",
      "A(θ0) is the acceptance region.\n",
      "Now, for any θ and any value x in the range of X, we let\n",
      "C(x) = {θ : x ∈A(θ)}.\n",
      "For testing H0 : θ = θ0 at the α signiﬁcance level, we have\n",
      "sup Pr(X /∈A(θ0) | θ = θ0) ≤α;\n",
      "that is,\n",
      "1 −α ≤inf Pr(X ∈A(θ0) | θ = θ0) = inf Pr(C(X) ∋θ0 | θ = θ0).\n",
      "This holds for any θ0, so\n",
      "inf\n",
      "P∈P PrP (C(X) ∋θ) = inf\n",
      "θ0∈Θ inf PrP(C(X) ∋θ0 | θ = θ0)\n",
      "≥1 −α.\n",
      "Hence, C(X) is a 1 −α level conﬁdence set for θ.\n",
      "If the size of the test is α, the inequalities are equalities, and so the conﬁ-\n",
      "dence coeﬃcient is 1 −α.\n",
      "For example, suppose Y1, Y2, . . ., Yn is a random sample from a N(µ, σ2)\n",
      "distribution, and Y is the sample mean.\n",
      "To test H0 : µ = µ0, against the universal alternative, we form the test\n",
      "statistic\n",
      "T(X) =\n",
      "p\n",
      "n(n −1) (Y −µ0)\n",
      "qP \u0000Yi −Y\n",
      "\u00012\n",
      "which, under the null hypothesis, has a Student’s t distribution with n −1\n",
      "degrees of freedom.\n",
      "An acceptance region at the α level is\n",
      "\u0002\n",
      "t(α/2), t(1−α/2)\n",
      "\u0003\n",
      ",\n",
      "and hence, putting these limits on T(X) and inverting, we get\n",
      "h\n",
      "Y −t(1−α/2) s/√n,\n",
      "Y −t(α/2) s/√n\n",
      "i\n",
      ",\n",
      "which is a 1 −α level conﬁdence interval.\n",
      "The test has size α and so the conﬁdence coeﬃcient is 1 −α.\n",
      "Randomized Conﬁdence Sets\n",
      "To form a 1 −α conﬁdence level set, we form a nonrandomized conﬁdence\n",
      "set (which may be null) with 1 −α1 conﬁdence level, with 0 ≤α1 ≤α, and\n",
      "then we deﬁne a random experiment with some event that has a probability\n",
      "of α −α1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "546\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "7.9 Optimal Conﬁdence Sets\n",
      "Just as we refer to a test as being “valid” if the signiﬁcance level of the test is\n",
      "not exceeded (that is, if the probability of rejecting a true null hypothesis is\n",
      "bounded by the level of signiﬁcance), we refer to a a conﬁdence interval that\n",
      "has a probability of at least the conﬁdence level as being “correct”.\n",
      "We often evaluate a conﬁdence set using a family of distributions that does\n",
      "not include the true parameter.\n",
      "For example, “accuracy” refers to the (true) probability of the set including\n",
      "an incorrect value. A conﬁdence that is more accurate has a smaller probability\n",
      "of including a distribution that did not give rise to the sample. This is a general\n",
      "way relates to the size of the conﬁdence set.\n",
      "Size of Conﬁdence Sets\n",
      "The “volume” (or “length”) of a conﬁdence set is the Lebesgue measure of\n",
      "the set:\n",
      "vol(ΘS) =\n",
      "Z\n",
      "ΘS\n",
      "d˜θ.\n",
      "This may not be ﬁnite.\n",
      "If the volume is ﬁnite, we have (Theorem 7.6 in MS2)\n",
      "Eθ(vol(ΘS)) =\n",
      "Z\n",
      "θ̸=˜θ\n",
      "Prθ(ΘS ∋˜θ)d˜θ.\n",
      "We see this by a simple application of Fubini’s theorem to handle the integral\n",
      "over the product space, and then an interchange of integration:\n",
      "Want to minimize volume (if appropriate; i.e., ﬁnite.)\n",
      "7.9.1 Most Accurate Conﬁdence Set\n",
      "Accuracy of Conﬁdence Sets\n",
      "**********Want to maximize accuracy.???????????\n",
      "Conﬁdence sets can be thought of a a family of tests of hypotheses of the\n",
      "form θ ∈H0(˜θ) versus θ ∈H1(˜θ). A conﬁdence set of size 1 −α is equivalent\n",
      "to a critical region S(X) such that\n",
      "Pr\n",
      "\u0010\n",
      "S(X) ∋˜θ\n",
      "\u0011\n",
      "≥1 −α\n",
      "∀θ ∈H0\n",
      "\u0010\n",
      "˜θ\n",
      "\u0011\n",
      ".\n",
      "The power of the related tests is just\n",
      "Pr\n",
      "\u0010\n",
      "S(X) ∋˜θ\n",
      "\u0011\n",
      "for any θ. In testing hypotheses, we are concerned about maximizing this for\n",
      "θ ∈H1(˜θ).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.9 Optimal Conﬁdence Sets\n",
      "547\n",
      "This is called the accuracy of the conﬁdence set, and so in this terminology,\n",
      "we seek the most accurate conﬁdence set, and, of course, the uniformly most\n",
      "accurate conﬁdence region. Similarly to the case of UMP tests, the uniformly\n",
      "most accurate conﬁdence region may or may not exist.\n",
      "The question of existence of uniformly most accurate conﬁdence intervals\n",
      "also depends on whether or not there are nuisance parameters. Just as with\n",
      "UMP tests, in the presence of nuisance parameters, usually uniformly most\n",
      "accurate conﬁdence intervals do not exist. (We must add other restrictions on\n",
      "the intervals, as we see below.) The nonexistence of uniformly most accurate\n",
      "conﬁdence sets can also be addressed by imposing unbiasedness.\n",
      "Uniformly most accurate 1 −α level set:\n",
      "Prθ(ΘS ∋˜θ) is minimum among all 1 −α level sets and ∀˜θ ̸= θ.\n",
      "This deﬁnition of UMA may not be so relevant in the case of a one-sided\n",
      "conﬁdence interval.\n",
      "If eΘ is a subset of Θ that does not include θ, and\n",
      "Prθ(ΘS ∋˜θ) ≤Prθ(ΘeS ∋˜θ)\n",
      "for any 1 −α level set ΘeS and ∀˜θ ∈eΘ, then ΘS is said to be eΘ-uniformly\n",
      "most accurate.\n",
      "A conﬁdence set formed by inverting a nonrandomized UMP test is UMA.\n",
      "We see this easily from the deﬁnitions of UMP and UMA. (This is Theorem\n",
      "7.4 in MS2.)\n",
      "7.9.2 Unbiased Conﬁdence Sets\n",
      "With tests, sometimes no UMP exists, and hence we added a criterion, such\n",
      "as unbiasedness or invariance.\n",
      "Likewise, sometimes we cannot form a UMA conﬁdence interval, so we add\n",
      "some criterion.\n",
      "We deﬁne unbiasedness in terms of a subset eΘ that does not include the\n",
      "true θ.\n",
      "A 1 −α level conﬁdence set C(X) is said to be eΘ-unbiased if\n",
      "Prθ(ΘS ∋˜θ) ≤1 −α\n",
      "∀˜θ ∈eΘ.\n",
      "If eΘ = {θ}c, we call the set unbiased.\n",
      "A eΘ-unbiased set that is uniformly more accurate (“more” is deﬁned simi-\n",
      "larly to “most”) than any other eΘ-unbiased set is said to be a uniformly most\n",
      "accurate unbiased (UMAU) set. *********************\n",
      "The concept of unbiasedness in tests carries over immediately to conﬁdence\n",
      "sets. A family of conﬁdence sets of size 1 −α is said to be unbiased if\n",
      "Pr\n",
      "\u0010\n",
      "S(X) ∋˜θ\n",
      "\u0011\n",
      "≤1 −α\n",
      "∀θ ∈H1\n",
      "\u0010\n",
      "˜θ\n",
      "\u0011\n",
      ".\n",
      "In the case of nuisance parameters θu, unbiasedness means that this holds\n",
      "for all values of the nuisance parameters. In this case, similar regions and\n",
      "Neyman structure also are relevant, just as in the case of testing.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "548\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "Volume of a Conﬁdence Set\n",
      "If there are no nuisance parameters, the expected volume of a conﬁdence set\n",
      "is usually known a priori, e.g., for µ in N(µ, 1).\n",
      "What about a conﬁdence set for µ in N(µ, σ2), with σ2 unknown?\n",
      "The expected length is proportional to σ, and can be very long. (This is a\n",
      "consequence of the fact that two normal distributions N(µ1, σ2) and N(µ2, σ2)\n",
      "become indistinguishable as σ →∞.\n",
      "The length of the conﬁdence interval is inversely proportional to √n. How\n",
      "about a sequential procedure?\n",
      "A Sequential Procedure for a Conﬁdence Set\n",
      "Let X1, X2, . . . iid\n",
      "∼N(µ, σ2).\n",
      "Fix n0. Let ¯x0 = Pn0\n",
      "i=1 xi/n0 and s2\n",
      "0 = Pn0\n",
      "i=1(xi −¯x0)2/(n0 −1).\n",
      "Now, for measurable function of s, a and b, for n ≥n0, let\n",
      "a1 = · · · = an0 = a\n",
      "and\n",
      "an0+1 = · · · = an = b.\n",
      "Then\n",
      "Y =\n",
      "Pn\n",
      "i=1 ai(Xi −µ)\n",
      "p\n",
      "S2\n",
      "0\n",
      "Pn\n",
      "i=1 a2\n",
      "i\n",
      "has a Student’s t distribution with n0 −1 df.\n",
      "Controlling the Volume\n",
      "Now compute s2 from an initial sample of size n0. Let c be a given positive\n",
      "constant. Now choose n −n0 additional observations where\n",
      "n = max\n",
      "\u0012\n",
      "n0 + 1,\n",
      "\u0014s2\n",
      "c\n",
      "\u0015\n",
      "+ 1\n",
      "\u0013\n",
      ".\n",
      "Then there exists numbers a1, . . ., an with a1 = · · · = an0 and an0+1 = · · · =\n",
      "an such that Pn\n",
      "i=1 ai = 1 and Pn\n",
      "i=1 a2\n",
      "i = c/s2.\n",
      "And so (from above), Pn\n",
      "i=1 ai(Xi −µ)/√c has a Student’s t distribution\n",
      "with n0 −1 df.\n",
      "Therefore, given X1, . . ., Xn0 the expected length of the conﬁdence interval\n",
      "can be controlled.\n",
      "Example 7.15 Conﬁdence Interval in Inverse Regression\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.9 Optimal Conﬁdence Sets\n",
      "549\n",
      "Consider E(Y |x) = β0 + β1x. Suppose we want to estimate the point at\n",
      "which β0 + β1x has a preassigned value; for example ﬁnd dosage x = −β0/β1\n",
      "at which E(Y |x) = 0.\n",
      "This is equivalent to ﬁnding the value v = (x −¯x)/\n",
      "pP(xi −¯x)2 at which\n",
      "y = γ0 + γ1v = 0.\n",
      "So we want to ﬁnd v = −γ0/γ1.\n",
      "The most accurate unbiased conﬁdence sets for −γ0/γ1 can be obtained\n",
      "from UMPU tests of the hypothesis −γ0/γ1 = v0.\n",
      "Acceptance regions of these tests are given by\n",
      "|v0\n",
      "P viYi + ¯Y |/\n",
      "q\n",
      "1\n",
      "n + v2\n",
      "0\n",
      "q\u0000P(Yi −¯Y )2 −(P viYi)2\u0001\n",
      "/(n −2)\n",
      "≤c\n",
      "where\n",
      "Z c\n",
      "−c\n",
      "p(y)dy = 1 −α,\n",
      "where p is the PDF of t with n −2 df.\n",
      "So square and get quadratic inequalities in v:\n",
      "v2 \u0010\n",
      "c2s2 −(\n",
      "X\n",
      "viYi)2\u0011\n",
      "−2v ¯Y\n",
      "X\n",
      "viYi + 1\n",
      "n(c2x2 −n¯Y ) ≥0.\n",
      "Now let v and v be the roots of the equation.\n",
      "So the conﬁdence statement becomes\n",
      "v ≤γ0\n",
      "γ1\n",
      "≤v\n",
      "if\n",
      "| P viYi|\n",
      "s\n",
      "> c,\n",
      "γ0\n",
      "γ1\n",
      "< v\n",
      "or\n",
      "γ0\n",
      "γ1\n",
      "> v\n",
      "if\n",
      "| P viYi|\n",
      "s\n",
      "< c,\n",
      "and if = c, no solution.\n",
      "If y = γ0 + γ1v is nearly parallel to the v-axis, then the intercept with the\n",
      "v-axis will be large in absolute value and its sign is sensitive to a small change\n",
      "in the angle.\n",
      "Suppose in the quadratic that n¯Y 2 + (P viYi)2 < c2s2, then there is no\n",
      "real solution.\n",
      "For the conﬁdence levels to remain valid, the conﬁdence interval must be\n",
      "the whole real line.\n",
      "7.9.3 Equivariant Conﬁdence Sets\n",
      "The connection we have seen between a 1 −α conﬁdence set S(x), and the\n",
      "acceptance region of a α-level test, A(θ), that is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "550\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "S(x) ∋θ\n",
      "⇔\n",
      "x ∈A(θ),\n",
      "can often be used to relate UMP invariant tests to best equivariant conﬁdence\n",
      "sets.\n",
      "Equivariance for conﬁdence sets is deﬁned similarly to equivariance in\n",
      "other settings.\n",
      "Under the notation developed above, for the group of transformations G\n",
      "and the induced transformation groups G∗and G, a conﬁdence set S(x) is\n",
      "equivariant if for all x ∈X and g ∈G,\n",
      "g∗(S(x)) = S(g(x)).\n",
      "The uniformly most powerful property of the test corresponds to uniformly\n",
      "minimizing the probability that the conﬁdence set contains incorrect values,\n",
      "and the invariance corresponds to equivariance.\n",
      "An equivariant set that is eΘ-uniformly more accurate (“more” is deﬁned\n",
      "similarly to “most”) than any other equivariant set is said to be a uniformly\n",
      "most accurate equivariant (UMAE) set.\n",
      "There are situations in which there do not exist conﬁdence sets that have\n",
      "uniformly minimum probability of including incorrect values. In such cases,\n",
      "we may retain the requirement for equivariance, but impose some other crite-\n",
      "rion, such as expected smallest size (wrt Lebesgue measure) of the conﬁdence\n",
      "interval.\n",
      "7.10 Asymptotic Conﬁdence sets\n",
      "It is often diﬃcult to determine sets with a speciﬁed conﬁdence coeﬃcient or\n",
      "signiﬁcance level, or with other speciﬁed properties.\n",
      "In such cases it may be useful to determine a set that “approximately”\n",
      "meets the speciﬁed requirements.\n",
      "What does “approximately” mean?\n",
      "•\n",
      "uses numerical approximations\n",
      "•\n",
      "uses approximate distributions\n",
      "•\n",
      "uses a random procedure\n",
      "•\n",
      "uses asymptotics\n",
      "We assume a random sample X1, . . ., Xn from P ∈P\n",
      "An asymptotic signiﬁcance level of a conﬁdence set C(X) for g(θ) is 1 −α\n",
      "if\n",
      "lim inf\n",
      "n\n",
      "Pr(C(X) ∋θ) ≥1 −α\n",
      "for any P ∈P.\n",
      "The limiting conﬁdence coeﬃcient of a conﬁdence set C(X) for θ is\n",
      "lim\n",
      "n\n",
      "inf\n",
      "P∈P Pr(C(X) ∋θ)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.10 Asymptotic Conﬁdence sets\n",
      "551\n",
      "if it exists.\n",
      "Example (MS2). Suppose X1, . . ., X −n are iid from a distribution with\n",
      "CDF PX and ﬁnite mean µ and variance σ2. Suppose σ2 is known, and we\n",
      "want to form a 1−α level conﬁdence interval for µ. Unless PX is speciﬁed, we\n",
      "can only seek a conﬁdence interval with asymptotic signiﬁcance level 1−α. We\n",
      "have an asymptotic pivot T(X, µ) = (X −µ)/σ, and √nT has an asymptotic\n",
      "N(0, 1) distribution. We then form an interval\n",
      "C(X) = (C1(X), C2(X))\n",
      "= (X −σz1−α/2/√:n, X + σz1−α/2/√:n),\n",
      "where z1−α/2 = Φ−1(1−α/2) and Φ is the N(0, 1) CDF. Now consider Pr(µ ∈\n",
      "C(X)). We have\n",
      "Asymptotic Correctness and Accuracy\n",
      "A conﬁdence set C(X) for θ is 1 −α asymptotically correct if\n",
      "lim\n",
      "n Pr(C(X) ∋θ) = 1 −α.\n",
      "A conﬁdence set C(X) for θ is 1 −α lth-order asymptotically accurate if it\n",
      "is 1 −α asymptotically correct and\n",
      "lim\n",
      "n Pr(C(X) ∋θ) = 1 −α + O(n−l/2).\n",
      "Asymptotic Accuracy of Conﬁdence sets\n",
      "**************************\n",
      "Constructing Asymptotic Conﬁdence Sets\n",
      "There are two straightforward ways of constructing good asymptotic conﬁ-\n",
      "dence sets.\n",
      "One is based on an asymptotically pivotal function, that is one whose lim-\n",
      "iting distribution does not depend on any parameters other than the one of\n",
      "the conﬁdence set.\n",
      "Another method is to invert the acceptance region of a test. The properties\n",
      "of the test carry over to the conﬁdence set.\n",
      "The likelihood yields statistics with good asymptotic properties (for testing\n",
      "or for conﬁdence sets).\n",
      "See Example 7.24.\n",
      "Woodruﬀ’s interval\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "552\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "7.11 Bootstrap Conﬁdence Sets\n",
      "A method of forming a conﬁdence interval for a parameter θ is to ﬁnd a pivotal\n",
      "quantity that involves θ and a statistic T, f(T, θ), and then to rearrange the\n",
      "terms in a probability statement of the form\n",
      "Pr\n",
      "\u0000f(α/2) ≤f(T, θ) ≤f(1−α/2)\n",
      "\u0001\n",
      "= 1 −α.\n",
      "(7.45)\n",
      "When distributions are diﬃcult to work out, we may use bootstrap methods\n",
      "for estimating and/or approximating the percentiles, f(α/2) and f(1−α/2).\n",
      "Basic Intervals\n",
      "For computing conﬁdence intervals for a mean, the pivotal quantity is likely\n",
      "to be of the form T −θ.\n",
      "The simplest application of the bootstrap to forming a conﬁdence interval\n",
      "is to use the sampling distribution of T ∗−T0 as an approximation to the sam-\n",
      "pling distribution of T −θ; that is, instead of using f(T, θ), we use f(T ∗, T0),\n",
      "where T0 is the value of T in the given sample.\n",
      "The percentiles of the sampling distribution determine f(α/2) and f(1−α/2)\n",
      "in\n",
      "Pr\n",
      "\u0000f(α/2) ≤f(T, θ) ≤f(1−α/2)\n",
      "\u0001\n",
      "= 1 −α.\n",
      "Monte Carlo to Get Basic Intervals\n",
      "If we cannot determine the sampling distribution of T ∗−T, we can easily\n",
      "estimate it by Monte Carlo methods.\n",
      "For the case f(T, θ) = T −θ, the probability statement above is equivalent\n",
      "to\n",
      "Pr\n",
      "\u0000T −f(1−α/2) ≤θ ≤T −f(α/2)\n",
      "\u0001\n",
      "= 1 −α.\n",
      "(7.46)\n",
      "The f(π) may be estimated from the percentiles of a Monte Carlo sample\n",
      "of T ∗−T0.\n",
      "Bootstrap-t Intervals\n",
      "Methods of inference based on a normal distribution often work well even\n",
      "when the underlying distribution is not normal.\n",
      "A useful approximate conﬁdence interval for a location parameter can\n",
      "often be constructed using as a template the familiar conﬁdence interval for\n",
      "the mean of a normal distribution,\n",
      "\u0002\n",
      "Y −t(1−α/2) s/√n,\n",
      "Y −t(α/2) s/√n\n",
      "\u0003\n",
      ",\n",
      "where t(π) is a percentile from the Student’s t distribution, and s2 is the usual\n",
      "sample variance.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.11 Bootstrap Conﬁdence Sets\n",
      "553\n",
      "A conﬁdence interval for any parameter constructed in this pattern is\n",
      "called a bootstrap-t interval. A bootstrap-t interval has the form\n",
      "\u0014\n",
      "T −bt(1−α/2)\n",
      "q\n",
      "bV(T),\n",
      "T −bt(α/2)\n",
      "q\n",
      "bV(T)\n",
      "\u0015\n",
      ".\n",
      "(7.47)\n",
      "In the interval\n",
      "\u0014\n",
      "T −bt(1−α/2)\n",
      "q\n",
      "bV(T),\n",
      "T −bt(α/2)\n",
      "q\n",
      "bV(T)\n",
      "\u0015\n",
      "bt(π) is the estimated percentile from the studentized statistic,\n",
      "T ∗−T0\n",
      "q\n",
      "bV(T ∗)\n",
      ".\n",
      "For many estimators T, no simple expression is available for bV(T).\n",
      "The variance could be estimated using a bootstrap. This bootstrap nested\n",
      "in the bootstrap to determine bt(π) increases the computational burden multi-\n",
      "plicatively.\n",
      "If the underlying distribution is normal and T is a sample mean, the inter-\n",
      "val in expression (7.47) is an exact (1−α)100% conﬁdence interval of shortest\n",
      "length.\n",
      "If the underlying distribution is not normal, however, this conﬁdence in-\n",
      "terval may not have good properties. In particular, it may not even be of size\n",
      "(1 −α)100%. An asymmetric underlying distribution can have particularly\n",
      "deleterious eﬀects on one-sided conﬁdence intervals.\n",
      "If the estimators T and bV(T) are based on sums of squares of deviations,\n",
      "the bootstrap-t interval performs very poorly when the underlying distribution\n",
      "has heavy tails. This is to be expected, of course. Bootstrap procedures can\n",
      "be no better than the statistics used.\n",
      "Bootstrap Percentile Conﬁdence Intervals\n",
      "Given a random sample (y1, . . ., yn) from an unknown distribution with CDF\n",
      "P , we want an interval estimate of a parameter, θ = Θ(P ), for which we have\n",
      "a point estimator, T.\n",
      "If T ∗is a bootstrap estimator for θ based on the bootstrap sample\n",
      "(y∗\n",
      "1, . . ., y∗\n",
      "n), and if GT ∗(t) is the distribution function for T ∗, then the\n",
      "exact upper 1 −α conﬁdence limit for θ is the value t∗\n",
      "(1−α), such that\n",
      "GT ∗(t∗\n",
      "(1−α)) = 1 −α.\n",
      "This is called the percentile upper conﬁdence limit.\n",
      "A lower limit is obtained similarly, and an interval is based on the lower\n",
      "and upper limits.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "554\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "Monte Carlo for Bootstrap Percentile Conﬁdence Intervals\n",
      "In practice, we generally use Monte Carlo and m bootstrap samples to esti-\n",
      "mate these quantities.\n",
      "The probability-symmetric bootstrap percentile conﬁdence interval of size\n",
      "(1 −α)100% is thus\n",
      "h\n",
      "t∗\n",
      "(α/2),\n",
      "t∗\n",
      "(1−α/2)\n",
      "i\n",
      ",\n",
      "where t∗\n",
      "(π) is the [πm]th order statistic of a sample of size m of T ∗.\n",
      "(Note that we are using T and t, and hence T ∗and t∗, to represent estima-\n",
      "tors and estimates in general; that is, t∗\n",
      "(π) here does not refer to a percentile\n",
      "of the Student’s t distribution.)\n",
      "This percentile interval is based on the ideal bootstrap and may be esti-\n",
      "mated by Monte Carlo simulation.\n",
      "Conﬁdence Intervals Based on Transformations\n",
      "Suppose that there is a monotonically increasing transformation g and a con-\n",
      "stant c such that the random variable\n",
      "W = c(g(T ∗) −g(θ))\n",
      "(7.48)\n",
      "has a symmetric distribution about zero. Here g(θ) is in the role of a mean\n",
      "and c is a scale or standard deviation.\n",
      "Let H be the distribution function of W, so\n",
      "GT ∗(t) = H (c(g(t) −g(θ)))\n",
      "(7.49)\n",
      "and\n",
      "t∗\n",
      "(1−α/2) = g−1 \u0000g(t∗) + w(1−α/2)/c\u0001,\n",
      "(7.50)\n",
      "where w(1−α/2) is the (1−α/2) quantile of W. The other quantile t∗\n",
      "(α/2) would\n",
      "be determined analogously.\n",
      "Instead of approximating the ideal interval with a Monte Carlo sample, we\n",
      "could use a transformation to a known W and compute the interval that way.\n",
      "Use of an exact transformation g to a known random variable W, of course, is\n",
      "just as diﬃcult as evaluation of the ideal bootstrap interval. Nevertheless, we\n",
      "see that forming the ideal bootstrap conﬁdence interval is equivalent to using\n",
      "the transformation g and the distribution function H.\n",
      "Because transformations to approximate normality are well-understood\n",
      "and widely used, in practice, we generally choose g as a transformation to\n",
      "normality. The random variable W above is a standard normal random vari-\n",
      "able, Z. The relevant distribution function is Φ, the normal CDF. The normal\n",
      "approximations have a basis in the central limit property. Central limit ap-\n",
      "proximations often have a bias of order O(n−1), however, so in small samples,\n",
      "the percentile intervals may not be very good.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.11 Bootstrap Conﬁdence Sets\n",
      "555\n",
      "Bias in Intervals Due to Bias in the Estimator\n",
      "It is likely that the transformed statistic g(T ∗) in equation (7.48) is biased for\n",
      "the transformed θ, even if the untransformed statistic is unbiased for θ.\n",
      "We can account for the possible bias by using the transformation\n",
      "Z = c(g(T ∗) −g(θ)) + z0,\n",
      "and, analogous to equation (7.49), we have\n",
      "GT ∗(t) = Φ (c(g(t) −g(θ)) + z0) .\n",
      "The bias correction z0 is Φ−1 (GT ∗(t)).\n",
      "Bias in Intervals Due to Lack of Symmetry\n",
      "Even when we are estimating θ directly with T ∗(that is, g is the identity),\n",
      "another possible problem in determining percentiles for the conﬁdence interval\n",
      "is the lack of symmetry of the distribution about z0.\n",
      "We would therefore need to make some adjustments in the quantiles in-\n",
      "stead of using equation (7.50) without some correction.\n",
      "Correcting the Bias in Intervals\n",
      "Rather than correcting the quantiles directly, we may adjust their levels.\n",
      "For an interval of conﬁdence (1−α), instead of (t∗\n",
      "(α/2), t∗\n",
      "(1−α/2)), we take\n",
      "h\n",
      "t∗\n",
      "(α1),\n",
      "t∗\n",
      "(α2)\n",
      "i\n",
      ",\n",
      "where the adjusted probabilities α1 and α2 are determined so as to reduce the\n",
      "bias and to allow for the lack of symmetry.\n",
      "As we often do, even for a nonnormal underlying distribution, we relate\n",
      "α1 and α2 to percentiles of the normal distribution.\n",
      "To allow for the lack of symmetry—that is, for a scale diﬀerence below\n",
      "and above z0—we use quantiles about that point.\n",
      "Efron (1987), who developed this method, introduced an “acceleration”,\n",
      "a, and used the distance a(z0 + z(π)).\n",
      "Using values for the bias correction and the acceleration determined from\n",
      "the data, Efron suggested the quantile adjustments\n",
      "α1 = Φ\n",
      "\u0012\n",
      "bz0 +\n",
      "bz0 + z(α/2)\n",
      "1 −ba(bz0 + z(α/2))\n",
      "\u0013\n",
      "and\n",
      "α2 = Φ\n",
      "\u0012\n",
      "bz0 +\n",
      "bz0 + z(1−α/2)\n",
      "1 −ba(bz0 + z(1−α/2))\n",
      "\u0013\n",
      ".\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "556\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "Use of these adjustments to the level of the quantiles for conﬁdence inter-\n",
      "vals is called the bias-corrected and accelerated, or “BCa”, method.\n",
      "This method automatically takes care of the problems of bias or asymmetry\n",
      "resulting from transformations that we discussed above.\n",
      "Note that if ba = bz0 = 0, then α1 = Φ(z(α)) and α2 = Φ(z(1−α)). In this\n",
      "case, the BCa is the same as the ordinary percentile method.\n",
      "The problem now is to estimate the bias correction z0 and the acceleration\n",
      "a from the data.\n",
      "Estimating the Correction\n",
      "The bias-correction term z0 is estimated by correcting the percentile near the\n",
      "median of the m bootstrap samples:\n",
      "bz0 = Φ−1\n",
      "\n",
      "1\n",
      "m\n",
      "X\n",
      "j\n",
      "I]−∞,T ]\n",
      "\u0000T ∗j\u0001\n",
      "\n",
      ".\n",
      "The idea is that we approximate the bias of the median (that is, the bias of a\n",
      "central quantile) and then adjust the other quantiles accordingly.\n",
      "Estimating the Acceleration\n",
      "Estimating a is a little more diﬃcult. The way we proceed depends on the\n",
      "form the bias may take and how we choose to represent it.\n",
      "Because one cause of bias may be skewness, Efron (1987) adjusted for the\n",
      "skewness of the distribution of the estimator in the neighborhood of θ.\n",
      "The skewness is measured by a function of the second and third moments\n",
      "of T.\n",
      "We can use the jackknife to estimate the second and third moments of T.\n",
      "The expression is\n",
      "ba =\n",
      "P \u0000J(T) −T(i)\n",
      "\u00013\n",
      "6\n",
      "\u0010P\u0000J(T) −T(i)\n",
      "\u00012\u00113/2 .\n",
      "(7.51)\n",
      "Bias resulting from other departures from normality, such as heavy tails,\n",
      "is not addressed by this adjustment.\n",
      "There are R and S-Plus functions to compute BCa conﬁdence intervals.\n",
      "Comparisons of Bootstrap-t and BCa Intervals\n",
      "It is diﬃcult to make analytic comparisons between these two types of boot-\n",
      "strap conﬁdence intervals.\n",
      "In some Monte Carlo studies, it has been found that, for moderate and\n",
      "approximately equal sample sizes,the coverage of BCa intervals is closest to\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "7.12 Simultaneous Conﬁdence Sets\n",
      "557\n",
      "the nominal conﬁdence level; however, for samples with very diﬀerent sizes,\n",
      "the bootstrap-t intervals were better in the sense of coverage frequency.\n",
      "Because of the variance of the components in the BCa method, it gen-\n",
      "erally requires relatively large numbers of bootstrap samples. For location\n",
      "parameters, for example, we may need m = 1, 000.\n",
      "Other Bootstrap Conﬁdence Intervals\n",
      "Another method for bootstrap conﬁdence intervals is based on a delta method\n",
      "approximation for the standard deviation of the estimator.\n",
      "This method yields approximate bootstrap conﬁdence, or ABC, intervals.\n",
      "Terms in the Taylor series expansions are used for computing ba and bz0\n",
      "rather than using bootstrap estimates for these terms.\n",
      "As with the BCa method, bias resulting from other departures from nor-\n",
      "mality, such as heavy tails, is not addressed.\n",
      "There are R and S-Plus functions to compute ABC conﬁdence intervals.\n",
      "************************\n",
      "7.12 Simultaneous Conﬁdence Sets\n",
      "If θ = (θ1, θ2) a 1 −α level conﬁdence set for θ is a region in IR2, C(X), such\n",
      "that Prθ(C(X) ∋θ) ≥1 −α.\n",
      "Now consider the problem of separate intervals (or sets) in IR1, C(X) and\n",
      "C2(X), such that Prθ(C1(X) ∋θ1\n",
      "and\n",
      "C2(X) ∋θ2) ≥1 −α.\n",
      "These are called 1 −α simultaneous conﬁdence intervals.\n",
      "This is equivalent to C(X) = C1(X) × C2(X) in the case above. Or, in\n",
      "general × Ci(X).\n",
      "(Of course, we may want to minimize expected area or some other geo-\n",
      "metric measures of C(X).)\n",
      "There are several methods. In linear models, many methods depend on\n",
      "contrasts, e.g., Scheﬀ´e’s intervals or Tukey’s intervals.\n",
      "General conservative procedures depend depend on inequalities of proba-\n",
      "bilities of events.\n",
      "7.12.1 Bonferroni’s Conﬁdence Intervals\n",
      "A common conservative procedure called a Bonferroni method is based on the\n",
      "inequality\n",
      "Pr (∪Ai) ≤\n",
      "X\n",
      "Pr (Ai) ,\n",
      "for any events A1, . . ., Ak. For each component of θ, θt, we choose αt with\n",
      "Pαt = α, and we let Ct(X) be a level 1 −αt conﬁdence interval. It is easy\n",
      "to see that these are of level 1 −α because\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "558\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "inf Pr(Ct(X) ∋θt ∀t) = Pr(∩{Ct(X) ∋θt})\n",
      "= 1 −Pr ((∩{θt ∈Ct(X)})c)\n",
      "= 1 −Pr (∪{θt /∈Ct(X)})\n",
      "≥1 −\n",
      "X\n",
      "Pr ({θt /∈Ct(X)})\n",
      "≥1 −\n",
      "X\n",
      "αt\n",
      "= 1 −α.\n",
      "7.12.2 Scheﬀ´e’s Conﬁdence Intervals\n",
      "7.12.3 Tukey’s Conﬁdence Intervals\n",
      "Notes and Further Reading\n",
      "Most of the material in this chapter is covered in MS2, Chapters 6 and 7, and\n",
      "in TSH3, Chapters 3, 4, and 5.\n",
      "Foundations\n",
      "p-values, Fisher; objective posterior probabilities of hypotheses, Jeﬀreys; test-\n",
      "ing with ﬁxed error probabilities, Neyman.\n",
      "Berger (2003)\n",
      "Signiﬁcance Tests and the Likelihood Principle\n",
      "Example 7.12\n",
      "Lindley and Phillips (1976), a Bayesian view\n",
      "“Elementary statistics from an advanced standpoint”.\n",
      "suppose we have only the data from an experiment ...\n",
      "Edwards (1992) example of measurements taken with defective or limited\n",
      "instruments (voltmeter)\n",
      "Royall (1997) ... philosophy of science\n",
      "Tests for Monotone Likelihood Ratio\n",
      "Roosen and Hennessy (2004) tests for monotone likelihood ratio.\n",
      "Types of Tests and Their Relationships to Each Other\n",
      "Buse (1982) gives an interesting exposition of the three types of tests.\n",
      "Verbeke and Molenberghs (2007) and Freedman (2007) discussed the ex-\n",
      "ample of Morgan et al. (Example 7.10), as well as other anomalies of a score\n",
      "test.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "559\n",
      "Sequential Tests\n",
      "Wald (1945)\n",
      "Because of the binary nature of statistical hypothesis testing, it is rather\n",
      "straightforward to add a third choice to obtain more data before making a\n",
      "decision. Although sequential tests seem natural, the idea of statistical infer-\n",
      "ence that evolves through sequential sampling is not just limited to hypothesis\n",
      "testing. In any statistical procedure, some quantiﬁcation of the uncertainty\n",
      "should be made. Based on the level of uncertainty, the statistician can choose\n",
      "to continue sampling. Wald (1947b) described sequential variations on the\n",
      "general decision-theoretic approach to statistical inference. See also Section\n",
      "5.2 of Wald (1950).\n",
      "Multiple Tests\n",
      "Storey (2002) proposed use of the proportion of false positives for any hypoth-\n",
      "esis (feature) incurred, on average, when that feature deﬁnes the threshold\n",
      "value. The “q-value” can be calculated for each feature under investigation.\n",
      "Storey (2003) Bayesian perspective\n",
      "Exercises\n",
      "7.1. The p-value is a random variable whose distribution depends on the test\n",
      "statistic and the state of nature. When the null hypothesis is true, it is\n",
      "often the case that the distribution of the p-value is U(0, 1).\n",
      "a) State very clearly a set of conditions that ensures that under the\n",
      "null hypothesis, the distribution of the p-value is U(0, 1). Given those\n",
      "conditions, prove that the distribution is U(0, 1) under the null hy-\n",
      "pothesis.\n",
      "b) Give an example in which the distribution of the p-values is not uni-\n",
      "form, even though the null hypothesis is true.\n",
      "7.2. Prove expression (7.13).\n",
      "7.3. In the statement of Theorem 7.1, we assume PDFs f0 and f1 both deﬁned\n",
      "with respect to a common σ-ﬁnite measure µ. Does this limit the scope of\n",
      "the theorem; that is, might there be a situation in which we want to test\n",
      "between two distributions P0 and P1, yet there does not exist a σ-ﬁnite\n",
      "measure µ by which to deﬁne PDFs?\n",
      "7.4. Consider a case of multiple testing in which the distribution of the p-values\n",
      "p1, . . ., pm of each of the m tests is U(0, 1) under the null hypothesis. Now\n",
      "consider Q pi. Make a log transformation, and work out a chi-squared\n",
      "approximation that yields quantiles of the product.\n",
      "7.5. Consider the data-generating process described in Example 7.12. In that\n",
      "process Bernoulli(π) results are observed until t 1’s are observed, and the\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "560\n",
      "7 Statistical Hypotheses and Conﬁdence Sets\n",
      "number N of random Bernoulli variates is noted. Based on the observed\n",
      "value of N we wish to test the hypotheses\n",
      "H0 : π ≥0.5\n",
      "versus\n",
      "H1 : π < 0.5.\n",
      "using a test of size 0.05. A test with that exact size will require a random\n",
      "component.\n",
      "a) Deﬁne such a test (based on the negative binomial distribution), and\n",
      "sketch its power curve.\n",
      "b) Deﬁne a test for the same hypotheses and with the same size based on\n",
      "t being a realization of a binomial random variable with parameters\n",
      "N and π.\n",
      "c) Now, suppose we repeat the experiment as described and we obtain\n",
      "observations N1, . . ., Nm. What is the mean size of m tests based on\n",
      "the binomial distributions as in Exercise 7.5b?\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8\n",
      "Nonparametric and Robust Inference\n",
      "A major concern is how well the statistical model corresponds to the data-\n",
      "generating process. Analyses based on an inappropriate model are likely to\n",
      "yield misleading conclusions.\n",
      "One approach is to develop procedures for inference based on a minimal\n",
      "set of assumptions about the underlying probability distribution. This leads to\n",
      "what we call nonparametric inference, and includes a wide range of procedures,\n",
      "such as the nonparametric tests discussed in Section 7.4. In this chapter we\n",
      "will discuss general methods in other areas of statistical inference.\n",
      "Another approach is to consider the consequences on inferential procedures\n",
      "arising from diﬀerences in the model and the data-generating process. A major\n",
      "objective of the ﬁeld of robust statistics is to identify or develop procedures\n",
      "that yield useful conclusions even when the data-generating process diﬀers\n",
      "in certain ways from the statistical model. Such procedures are robust to\n",
      "departures within a certain class from the assumed model.\n",
      "8.1 Nonparametric Inference\n",
      "We have described statistical inference as the process of using observational\n",
      "data from a population that is in an assumed family of distributions P to\n",
      "identify a subfamily, PH ⊆P, that contains the population from which the\n",
      "data arose. If the assumed family of probability space is (Ω, F, Pθ) where the\n",
      "index on the probability measure is in some subset Θ ⊆IRd for some ﬁxed\n",
      "positive integer d and θ fully determines the measure, we call θ the parameter\n",
      "and the statistical inference is parametric inference. Statistical inference is\n",
      "a process of identifying a sub parameter space ΘH ⊆Θ. For example, we\n",
      "may assume that a given sample x1, . . ., xn is taken independently from some\n",
      "member of a family of distributions\n",
      "P = {N(µ, σ2) | µ ∈IR, σ2 ∈IR+},\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "562\n",
      "8 Nonparametric and Robust Inference\n",
      "and statistical inference in this situation may lead us to place the population\n",
      "giving rise to the observed sample in the family\n",
      "PH = {N(µ, σ2) | µ ∈[µ1, µ2], σ2 ∈IR+}\n",
      "of distributions.\n",
      "In a more general formulation of the inference problem, rather than iden-\n",
      "tifying a parameter space, we may make only very general assumptions about\n",
      "the family of probability distributions. For example, we may assume only\n",
      "that they have Lebesgue PDFs, or that the PDFs are symmetric, or that all\n",
      "moments of order less than some integer k are ﬁnite. We may be interested\n",
      "in estimating, or performing tests of hypotheses about, certain distributional\n",
      "measures. In nonparametric inference, the distributional measures of interest\n",
      "are those that are likely to exist even for quite “pathological” distributions,\n",
      "such as the Cauchy distribution. The Cauchy distribution, for example does\n",
      "not have a mean or variance. It does, however, have a median and an in-\n",
      "terquartile range. Nonparametric inference often concerns such things as the\n",
      "median or interquartile range, rather than the mean or variance. The instanti-\n",
      "ation of the basic paradigm of statistical inference may be to assume a family\n",
      "of distributions\n",
      "P = {distributions with Lebesgue PDF},\n",
      "and statistical inference leads us to place the population giving rise to the\n",
      "observed sample in the family\n",
      "PH = {distributions with Lebesgue PDF with median greater than 0}\n",
      "of distributions.\n",
      "We can also express the statistical inference problem as beginning with\n",
      "a given family of distribution P, and identify a subfamily based on values\n",
      "of some distributional measure expressed as a statistical function (see Sec-\n",
      "tion 1.1.9, beginning on page 51). The decision, for example, may be that the\n",
      "population at hand has a probability distribution in the family\n",
      "PH = {P | P ∈P and Υ(P ) = ΥH},\n",
      "where Υ is a functional. In this case, the statistical inference has focused on\n",
      "the distributional measure Υ(P ), which, of course, may be quite general.\n",
      "The methods in nonparametric inference are often based on functionals\n",
      "of the ECDF. The strong convergence of the ECDF, as shown, for example,\n",
      "in the Glivenko-Cantelli Theorem 1.71 or by the Dvoretzky/Kiefer/Wolfowitz\n",
      "inequality (1.289), suggest that this approach will result in procedures with\n",
      "good asymptotic properties.\n",
      "An example of nonparametric inference is the problem of testing that the\n",
      "distributions of two populations are the same versus the alternative that a\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.2 Inference Based on Order Statistics\n",
      "563\n",
      "realization from one distribution is typically smaller (or larger) than a real-\n",
      "ization from the other distribution. A U-statistic involving two populations\n",
      "is The two-sample Wilcoxon statistic U (which happens to be a U-statistic)\n",
      "discussed in Example 5.22 could be used as a test statistic for this common\n",
      "problem in nonparametric inference. This U-statistic is an unbiased estimator\n",
      "of Pr(X11 ≤X21). If the distributions have similar shapes and diﬀer primarily\n",
      "by a shift in location, U can be used as a test statistic for an hypothesis in-\n",
      "volving the medians of the two distributions instead of a two-sample t test for\n",
      "an hypothesis involving the means (and under the further assumptions that\n",
      "the distributions are normal with equal variances).\n",
      "8.2 Inference Based on Order Statistics\n",
      "8.2.1 Central Order Statistics\n",
      "Asymptotic Properties\n",
      "From equation (1.288) we have\n",
      "√n(Fn(x) −F (x)) d→N (0, F (x)(1 −F (x))) .\n",
      "Now, for a continuous CDF F with PDF f, consider a function g(t) deﬁned\n",
      "for 0 < t < 1 by\n",
      "g(t) = F −1(t).\n",
      "Then\n",
      "g′(t) =\n",
      "1\n",
      "f(F −1(t)),\n",
      "and so, using the delta method, we have\n",
      "√n(F −1(Fn(x)) −x) d→N\n",
      "\u0012\n",
      "0, F (x)(1 −F (x))\n",
      "(f(x))2\n",
      "\u0013\n",
      ".\n",
      "(8.1)\n",
      "F −1(Fn(x)) lies between the sample quantiles ***\n",
      "X(⌈nFn(x)⌉) ﬁx notation ****\n",
      "X(⌈nFn(x)⌉) −F −1(Fn(x))\n",
      "a.s.\n",
      "→0\n",
      "√n(X(⌈nFn(x)⌉) −x) d→N\n",
      "\u0012\n",
      "0, F (x)(1 −F (x))\n",
      "(f(x))2\n",
      "\u0013\n",
      ".\n",
      "location family F (x; θ) = F (x −θ; 0) F (0; 0) = 1/2 density f(x; θ) suppose\n",
      "f(0; 0) > 0\n",
      "e\n",
      "Xn sample median\n",
      "√n( e\n",
      "Xn −θ)\n",
      "d→N\n",
      "\u0012\n",
      "0,\n",
      "1\n",
      "4(f(0))2\n",
      "\u0013\n",
      "(8.2)\n",
      "ARE median vs mean normal 0.637 t3 1.62\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "564\n",
      "8 Nonparametric and Robust Inference\n",
      "8.2.2 Statistics of Extremes\n",
      "*** tail index Because a PDF must integrate to 1 over the full range of real\n",
      "numbers, the PDF of a distribution whose range is inﬁnite must approach 0\n",
      "suﬃciently fast in the tails, that is, as the argument of the PDF approaches\n",
      "−∞or ∞.\n",
      "We will just consider the positive side of the distribution, even if the range\n",
      "is inﬁnite on both sides. We can identify three general forms of the part of\n",
      "the PDF that determines the tail behavior (that is, the part of the PDF that\n",
      "contains the argument):\n",
      "e−|x|α\n",
      "with α > 0,\n",
      "(8.3)\n",
      "as in the normal, exponential, and double exponential;\n",
      "|x|−α\n",
      "with α > 1,\n",
      "(8.4)\n",
      "as in the Pareto (also the related “power distributions”) and the Cauchy (with\n",
      "some additional terms); and\n",
      "|x|αe−|x|β,\n",
      "(8.5)\n",
      "as in the gamma and Weibull.\n",
      "What happens as x →∞determines whether the moments of the distri-\n",
      "bution are ﬁnite.\n",
      "Consider ﬁrst the form (8.3). We will call this an exponential tail. (We\n",
      "sometimes call it a “right exponential tail”, because we are focusing only on\n",
      "the right side of the range.) Notice that for any α > 0\n",
      "E(|X|k) =\n",
      "Z ∞\n",
      "0\n",
      "γxke−|x|αdx < ∞;\n",
      "(8.6)\n",
      "that is, all moments are ﬁnite.\n",
      "We can see that some distributions with exponential tails have heavier tails\n",
      "than others. For example, the double exponential distribution has heavier tails\n",
      "than a normal distribution because\n",
      "e−|x|2 →0\n",
      "faster than\n",
      "e−|x| →0.\n",
      "Now consider the form (8.4). We will call this a polynomial tail or a Pareto\n",
      "tail because of the form of the Pareto PDF. We call α in expression (8.4) the\n",
      "tail index of the polynomial tail. (The tail index is also sometimes deﬁned as\n",
      "this quantity minus 1.) The larger is the tail index the more rapidly the PDF\n",
      "will approach 0. The moments E(|X|k) will be ﬁnite only for k < α. In the\n",
      "Pareto distribution, for example, the mean is ﬁnite only for α > 1 and the\n",
      "variance is ﬁnite only for α > 2.\n",
      "***\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.3 Nonparametric Estimation of Functions\n",
      "565\n",
      "8.3 Nonparametric Estimation of Functions\n",
      "An interesting problem in statistics is estimation of a continuous function.\n",
      "In one common type of this problem, the function expresses a relationship\n",
      "between one set of variables that are the argument of the function and another\n",
      "set of variables that are the function value. In one special instance of this kind\n",
      "of problem, the argument of the is a single variable representing time. The\n",
      "function in such a case is called a time series model. Statistical inference for\n",
      "these kinds of problems is based on data of the form (x1, y1), . . ., (xn, yn),\n",
      "where xi represents an observation on the argument of the function, and yi\n",
      "represents an observation on the corresponding value of the function. Both\n",
      "xi and yi could be vectors. Often what is actually observed is xi and yi with\n",
      "some error, which may be assumed to be realizations of a random variable. In\n",
      "the case of a time series, xi is a measure of time (and usually denoted as ti).\n",
      "In another important type of the problem of estimation of a continuous\n",
      "function, the function represents a probability density. Values of the argument\n",
      "of the function can be observed but the corresponding values of the function\n",
      "cannot be observed. A density cannot be observed directly. What is observed\n",
      "directly corresponds to the antiderivative of the function of interest. Figure 8.1\n",
      "illustrates the diﬀerence in these two situations. The panel on the left shows\n",
      "observations that consist of pairs of values, the argument and the function\n",
      "value plus, perhaps, random noise. The panel on the right shows a “rug” of\n",
      "observations; there are no observed values that correspond to the probability\n",
      "density function being estimated.\n",
      "In some common situations, the form of the function is assumed known\n",
      "and statistical inference involves the parameters that fully specify the func-\n",
      "tion. This is the common situation in linear or nonlinear regression and in\n",
      "some problems in time series analysis. If the function is a parametric prob-\n",
      "ability density function, the problem is the standard one of estimating the\n",
      "parameters.\n",
      "In this chapter we consider the problem of nonparametric estimation of\n",
      "functions; that is, we do not assume that the form of the function is known.\n",
      "Our objective will not be to develop an expression for the function, but rather\n",
      "to develop a rule such that, given a value of the argument, an estimate of the\n",
      "value of the function is provided. This problem, whether nonparametric re-\n",
      "gression analysis or nonparametric density estimation, is generally diﬃcult,\n",
      "and the statistical properties of an estimator of a function are more compli-\n",
      "cated than statistical properties of an estimator of a single parameter or even\n",
      "of a countable set of parameters.\n",
      "The usual optimality properties that we use in developing a theory of\n",
      "estimation of a ﬁnite-dimensional parameter must be extended for estimation\n",
      "of a general function. As we will see, two of the usual desirable properties of\n",
      "point estimators, namely unbiasedness and maximum likelihood, cannot be\n",
      "attained in general by estimators of functions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "566\n",
      "8 Nonparametric and Robust Inference\n",
      "0.0\n",
      "0.4\n",
      "0.8\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "x\n",
      "f^(x)\n",
      "x 1\n",
      "x 2x 3 x 4\n",
      "x 5\n",
      "Nonparametric Regression\n",
      "0.0\n",
      "0.4\n",
      "0.8\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.5\n",
      "2.0\n",
      "x\n",
      "p^(x)\n",
      "Density Estimation\n",
      "Figure 8.1. Nonparametric Function Estimation\n",
      "Notation\n",
      "We may denote a function by a single letter, f, for example, or by the func-\n",
      "tion notation, f(·) or f(x). When f(x) denotes a function, x is merely a\n",
      "placeholder. The notation f(x), however, may also refer to the value of the\n",
      "function at the point x. The meaning is usually clear from the context.\n",
      "Using the common “hat” notation for an estimator, we use bf or bf(x) to\n",
      "denote the estimator of f or of f(x). Following the usual terminology, we\n",
      "use the term “estimator” to denote a random variable, and “estimate” to\n",
      "denote a realization of the random variable. The hat notation is also used\n",
      "to denote an estimate, so we must determine from the context whether bf or\n",
      "bf(x) denotes a random variable or a realization of a random variable. The\n",
      "estimate or the estimator of the value of the function at the point x may\n",
      "also be denoted by bf(x). Sometimes, to emphasize that we are estimating the\n",
      "ordinate of the function rather than evaluating an estimate of the function,\n",
      "we use the notation d\n",
      "f(x). In this case also, we often make no distinction in\n",
      "the notation between the realization (the estimate) and the random variable\n",
      "(the estimator). We must determine from the context whether bf(x) or d\n",
      "f(x)\n",
      "denotes a random variable or a realization of a random variable. In most\n",
      "of the following discussion, the hat notation denotes a random variable that\n",
      "depends on the underlying random variable that yields the sample from which\n",
      "the estimator is computed.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.3 Nonparametric Estimation of Functions\n",
      "567\n",
      "Estimation or Approximation\n",
      "There are many similarities in estimation of functions and approximation of\n",
      "functions, but we must be aware of the fundamental diﬀerences in the two\n",
      "problems. Estimation of functions is similar to other estimation problems:\n",
      "we are given a sample of observations; we make certain assumptions about\n",
      "the probability distribution of the sample; and then we develop estimators.\n",
      "The estimators are random variables, and how useful they are depends on\n",
      "properties of their distribution, such as their expected values and their vari-\n",
      "ances. Approximation of functions is an important aspect of numerical analy-\n",
      "sis. Functions are often approximated to interpolate functional values between\n",
      "directly computed or known values. Functions are also approximated as a pre-\n",
      "lude to quadrature. Methods for estimating functions often use methods for\n",
      "approximating functions.\n",
      "8.3.1 General Methods for Estimating Functions\n",
      "In the problem of function estimation, we may have observations on the func-\n",
      "tion at speciﬁc points in the domain, or we may have indirect measurements\n",
      "of the function, such as observations that relate to a derivative or an integral\n",
      "of the function. In either case, the problem of function estimation has the\n",
      "competing goals of providing a good ﬁt to the observed data and predicting\n",
      "values at other points. In many cases, a smooth estimate satisﬁes this latter\n",
      "objective. In other cases, however, the unknown function itself is not smooth.\n",
      "Functions with diﬀerent forms may govern the phenomena in diﬀerent regimes.\n",
      "This presents a very diﬃcult problem in function estimation, and it is one that\n",
      "we will not consider in any detail here.\n",
      "There are various approaches to estimating functions. Maximum likelihood\n",
      "has limited usefulness for estimating functions because in general the likeli-\n",
      "hood is unbounded. A practical approach is to assume that the function is of\n",
      "a particular form and estimate the parameters that characterize the form. For\n",
      "example, we may assume that the function is exponential, possibly because of\n",
      "physical properties such as exponential decay. We may then use various esti-\n",
      "mation criteria, such as least squares, to estimate the parameter. An extension\n",
      "of this approach is to assume that the function is a mixture of other functions.\n",
      "The mixture can be formed by diﬀerent functions over diﬀerent domains or\n",
      "by weighted averages of the functions over the whole domain. Estimation of\n",
      "the function of interest involves estimation of various parameters as well as\n",
      "the weights.\n",
      "Another approach to function estimation is to represent the function of\n",
      "interest as a linear combination of basis functions, that is, to represent the\n",
      "function in a series expansion. The basis functions are generally chosen to be\n",
      "orthogonal over the domain of interest, and the observed data are used to\n",
      "estimate the coeﬃcients in the series.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "568\n",
      "8 Nonparametric and Robust Inference\n",
      "It is often more practical to estimate the function value at a given point.\n",
      "(Of course, if we can estimate the function at any given point, we can eﬀec-\n",
      "tively have an estimate at all points.) One way of forming an estimate of a\n",
      "function at a given point is to take the average at that point of a ﬁltering\n",
      "function that is evaluated in the vicinity of each data point. The ﬁltering\n",
      "function is called a kernel, and the result of this approach is called a kernel\n",
      "estimator.\n",
      "In the estimation of functions, we must be concerned about the properties\n",
      "of the estimators at speciﬁc points and also about properties over the full\n",
      "domain. Global properties over the full domain are often deﬁned in terms of\n",
      "integrals or in terms of suprema or inﬁma.\n",
      "Function Decomposition and Estimation of the Coeﬃcients in an\n",
      "Orthogonal Expansion\n",
      "We ﬁrst do a PDF decomposition of the function of interest with the proba-\n",
      "bility density function, p,\n",
      "f(x) = g(x)p(x).\n",
      "(8.7)\n",
      "We have\n",
      "ck = ⟨f, qk⟩\n",
      "=\n",
      "Z\n",
      "D\n",
      "qk(x)g(x)p(x)dx\n",
      "= E(qk(X)g(X)),\n",
      "(8.8)\n",
      "where X is a random variable whose probability density function is p.\n",
      "If we can obtain a random sample, x1, . . ., xn, from the distribution with\n",
      "density p, the ck can be unbiasedly estimated by\n",
      "bck = 1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "qk(xi)g(xi).\n",
      "The series estimator of the function for all x therefore is\n",
      "bf(x) = 1\n",
      "n\n",
      "j\n",
      "X\n",
      "k=0\n",
      "n\n",
      "X\n",
      "i=1\n",
      "qk(xi)g(xi)qk(x)\n",
      "(8.9)\n",
      "for some truncation point j.\n",
      "The random sample, x1, . . ., xn, may be an observed dataset, or it may be\n",
      "the output of a random number generator.\n",
      "Kernel Methods\n",
      "Another approach to function estimation and approximation is to use a ﬁl-\n",
      "ter or kernel function to provide local weighting of the observed data. This\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.3 Nonparametric Estimation of Functions\n",
      "569\n",
      "approach ensures that at a given point the observations close to that point\n",
      "inﬂuence the estimate at the point more strongly than more distant obser-\n",
      "vations. A standard method in this approach is to convolve the observations\n",
      "with a unimodal function that decreases rapidly away from a central point.\n",
      "This function is the ﬁlter or the kernel. A kernel has two arguments represent-\n",
      "ing the two points in the convolution, but we typically use a single argument\n",
      "that represents the distance between the two points.\n",
      "Some examples of univariate kernel functions are shown below.\n",
      "uniform:\n",
      "κu(t) = 0.5,\n",
      "for |t| ≤1,\n",
      "quadratic: κq(t) = 0.75(1 −t2), for |t| ≤1,\n",
      "normal:\n",
      "κn(t) =\n",
      "1\n",
      "√\n",
      "2πe−t2/2,\n",
      "for all t.\n",
      "The kernels with ﬁnite support are deﬁned to be 0 outside that range. Of-\n",
      "ten multivariate kernels are formed as products of these or other univariate\n",
      "kernels.\n",
      "In kernel methods, the locality of inﬂuence is controlled by a window\n",
      "around the point of interest. The choice of the size of the window is the most\n",
      "important issue in the use of kernel methods. In practice, for a given choice of\n",
      "the size of the window, the argument of the kernel function is transformed to\n",
      "reﬂect the size. The transformation is accomplished using a positive deﬁnite\n",
      "matrix, V , whose determinant measures the volume (size) of the window.\n",
      "To estimate the function f at the point x, we ﬁrst decompose f to have a\n",
      "factor that is a probability density function, p,\n",
      "f(x) = g(x)p(x).\n",
      "For a given set of data, x1, . . ., xn, and a given scaling transformation matrix\n",
      "V , the kernel estimator of the function at the point x is\n",
      "d\n",
      "f(x) = (n|V |)−1\n",
      "n\n",
      "X\n",
      "i=1\n",
      "g(x)κ\n",
      "\u0000V −1(x −xi)\n",
      "\u0001\n",
      ".\n",
      "(8.10)\n",
      "In the univariate case, the size of the window is just the width h. The\n",
      "argument of the kernel is transformed to s/h, so the function that is convolved\n",
      "with the function of interest is κ(s/h)/h. The univariate kernel estimator is\n",
      "d\n",
      "f(x) = 1\n",
      "nh\n",
      "n\n",
      "X\n",
      "i=1\n",
      "g(xi)κ\n",
      "\u0012x −xi\n",
      "h\n",
      "\u0013\n",
      ".\n",
      "8.3.2 Pointwise Properties of Function Estimators\n",
      "The statistical properties of an estimator of a function at a given point are\n",
      "analogous to the usual statistical properties of an estimator of a scalar pa-\n",
      "rameter. The statistical properties involve expectations or other properties of\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "570\n",
      "8 Nonparametric and Robust Inference\n",
      "random variables. In the following, when we write an expectation, E(·), or\n",
      "a variance, V(·), the expectations are usually taken with respect to the (un-\n",
      "known) distribution of the underlying random variable. Occasionally, we may\n",
      "explicitly indicate the distribution by writing, for example, Ep(·), where p is\n",
      "the density of the random variable with respect to which the expectation is\n",
      "taken.\n",
      "Bias\n",
      "The bias of the estimator of a function value at the point x is\n",
      "E\u0000 bf(x)\u0001 −f(x).\n",
      "If this bias is zero, we would say that the estimator is unbiased at the point\n",
      "x. If the estimator is unbiased at every point x in the domain of f, we say\n",
      "that the estimator is pointwise unbiased. Obviously, in order for bf(·) to be\n",
      "pointwise unbiased, it must be deﬁned over the full domain of f.\n",
      "Variance\n",
      "The variance of the estimator at the point x is\n",
      "V\n",
      "\u0000 bf(x)\n",
      "\u0001\n",
      "= E\n",
      "\u0012\u0010\n",
      "bf(x) −E\n",
      "\u0000 bf(x)\n",
      "\u0001\u00112\u0013\n",
      ".\n",
      "Estimators with small variance are generally more desirable, and an optimal\n",
      "estimator is often taken as the one with smallest variance among a class of\n",
      "unbiased estimators.\n",
      "Mean Squared Error\n",
      "The mean squared error, MSE, at the point x is\n",
      "MSE\u0000 bf(x)\u0001 = E\n",
      "\u0010\u0000 bf(x) −f(x)\u00012\u0011\n",
      ".\n",
      "(8.11)\n",
      "The mean squared error is the sum of the variance and the square of the bias:\n",
      "MSE\n",
      "\u0000 bf(x)\n",
      "\u0001\n",
      "= E\n",
      "\u0010\u0000 bf(x)\n",
      "\u00012 −2 bf(x)f(x) +\n",
      "\u0000f(x)\n",
      "\u00012\u0011\n",
      "= V\u0000 bf(x)\u0001 +\n",
      "\u0010\n",
      "E\u0000 bf(x)\u0001 −f(x)\n",
      "\u00112\n",
      ".\n",
      "(8.12)\n",
      "Sometimes, the variance of an unbiased estimator is much greater than\n",
      "that of an estimator that is only slightly biased, so it is often appropriate to\n",
      "compare the mean squared error of the two estimators. In some cases, as we\n",
      "will see, unbiased estimators do not exist, so rather than seek an unbiased\n",
      "estimator with a small variance, we seek an estimator with a small MSE.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.3 Nonparametric Estimation of Functions\n",
      "571\n",
      "Mean Absolute Error\n",
      "The mean absolute error, MAE, at the point x is similar to the MSE:\n",
      "MAE\n",
      "\u0000 bf(x)\n",
      "\u0001\n",
      "= E\n",
      "\u0010\f\f bf(x) −f(x)\n",
      "\f\f\n",
      "\u0011\n",
      ".\n",
      "(8.13)\n",
      "It is more diﬃcult to do mathematical analysis of the MAE than it is for the\n",
      "MSE. Furthermore, the MAE does not have a simple decomposition into other\n",
      "meaningful quantities similar to the MSE.\n",
      "Consistency\n",
      "Consistency of an estimator refers to the convergence of the expected value of\n",
      "the estimator to what is being estimated as the sample size increases without\n",
      "bound. A point estimator Tn, based on a sample of size n, is consistent for θ\n",
      "if\n",
      "E(Tn) →θ\n",
      "as n →∞.\n",
      "The convergence is stochastic, of course, so there are various types of con-\n",
      "vergence that can be required for consistency. The most common kind of\n",
      "convergence considered is weak convergence, or convergence in probability.\n",
      "In addition to the type of stochastic convergence, we may consider the\n",
      "convergence of various measures of the estimator. In general, if m is a function\n",
      "(usually a vector-valued function that is an elementwise norm), we may deﬁne\n",
      "consistency of an estimator Tn in terms of m if\n",
      "E(m(Tn −θ)) →0.\n",
      "(8.14)\n",
      "For an estimator, we are often interested in weak convergence in mean\n",
      "square or weak convergence in quadratic mean, so the common deﬁnition of\n",
      "consistency of Tn is\n",
      "E\u0000(Tn −θ)T(Tn −θ)\u0001 →0,\n",
      "where the type of convergence is convergence in probability. Consistency\n",
      "deﬁned by convergence in mean square is also called L2 consistency. 2\n",
      "consistence@L2 consistency\n",
      "If convergence does occur, we are interested in the rate of convergence. We\n",
      "deﬁne rate of convergence in terms of a function of n, say r(n), such that\n",
      "E(m(Tn −θ)) ∈O(r(n)).\n",
      "A common form of r(n) is nα, where α < 0. For example, in the simple case\n",
      "of a univariate population with a ﬁnite mean µ and ﬁnite second moment, use\n",
      "of the sample mean ¯x as the estimator Tn, and use of m(z) = z2, we have\n",
      "E(m(¯x −µ)) = E\n",
      "\u0000(¯x −µ)2\u0001\n",
      "= MSE(¯x)\n",
      "∈O\u0000n−1\u0001.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "572\n",
      "8 Nonparametric and Robust Inference\n",
      "In the estimation of a function, we say that the estimator bf of the function\n",
      "f is pointwise consistent if\n",
      "E\u0000 bf(x)\u0001 →f(x)\n",
      "(8.15)\n",
      "for every x the domain of f. Just as in the estimation of a parameter, there\n",
      "are various kinds of pointwise consistency in the estimation of a function. If\n",
      "the convergence in expression (8.15) is in probability, for example, we say that\n",
      "the estimator is weakly pointwise consistent. We could also deﬁne other kinds\n",
      "of pointwise consistency in function estimation along the lines of other types\n",
      "of consistency.\n",
      "8.3.3 Global Properties of Estimators of Functions\n",
      "Often we are interested in some measure of the statistical properties of an\n",
      "estimator of a function over the full domain of the function. An obvious way\n",
      "of deﬁning statistical properties of an estimator of an integrable function is\n",
      "to integrate the pointwise properties discussed in the previous section.\n",
      "Global properties of a function or the diﬀerence between two functions are\n",
      "often deﬁned in terms of a norm of the function or the diﬀerence.\n",
      "For comparing bf(x) and f(x), the Lp norm of the error is\n",
      "\u0012Z\n",
      "D\n",
      "\f\f bf(x) −f(x)\n",
      "\f\fp dx\n",
      "\u00131/p\n",
      ",\n",
      "(8.16)\n",
      "where D is the domain of f. The integral may not exist, of course. Clearly,\n",
      "the estimator bf must also be deﬁned over the same domain.\n",
      "Three useful measures are the L1 norm, also called the integrated absolute\n",
      "error, or IAE,\n",
      "IAE( bf ) =\n",
      "Z\n",
      "D\n",
      "\f\f\f bf(x) −f(x)\n",
      "\f\f\f dx,\n",
      "(8.17)\n",
      "the square of the L2 norm, also called the integrated squared error, or ISE,\n",
      "ISE( bf ) =\n",
      "Z\n",
      "D\n",
      "\u0010\n",
      "bf(x) −f(x)\n",
      "\u00112\n",
      "dx,\n",
      "(8.18)\n",
      "and the L∞norm, the sup absolute error, or SAE,\n",
      "SAE( bf ) = sup\n",
      "\f\f\f bf(x) −f(x)\n",
      "\f\f\f .\n",
      "(8.19)\n",
      "The L1 measure is invariant under monotone transformations of the coor-\n",
      "dinate axes, but the measure based on the L2 norm is not.\n",
      "The L∞norm, or SAE, is the most often used measure in general function\n",
      "approximation. In statistical applications, this measure applied to two cumu-\n",
      "lative distribution functions is the Kolmogorov distance. The measure is not\n",
      "so useful in comparing densities and is not often used in density estimation.\n",
      "Other useful measures of the diﬀerence in bf and f over the full range of x\n",
      "are the Kullback-Leibler measure and the Hellinger distance; see Section 0.1.9.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.3 Nonparametric Estimation of Functions\n",
      "573\n",
      "Integrated Bias and Variance\n",
      "We now want to develop global concepts of bias and variance for estimators of\n",
      "functions. Bias and variance are statistical properties that involve expectations\n",
      "of random variables. The obvious global measures of bias and variance are just\n",
      "the pointwise measures integrated over the domain. In the case of the bias,\n",
      "of course, we must integrate the absolute value, otherwise points of negative\n",
      "bias could cancel out points of positive bias.\n",
      "The estimator bf is pointwise unbiased if\n",
      "E\n",
      "\u0000 bf(x)\n",
      "\u0001\n",
      "= f(x)\n",
      "for all x ∈IRd.\n",
      "Because we are interested in the bias over the domain of the function, we\n",
      "deﬁne the integrated absolute bias as\n",
      "IAB\n",
      "\u0000 bf\n",
      "\u0001\n",
      "=\n",
      "Z\n",
      "D\n",
      "\f\f\fE\n",
      "\u0000 bf(x)\n",
      "\u0001\n",
      "−f(x)\n",
      "\f\f\f dx\n",
      "(8.20)\n",
      "and the integrated squared bias as\n",
      "ISB\u0000 bf \u0001 =\n",
      "Z\n",
      "D\n",
      "\u0010\n",
      "E\u0000 bf(x)\u0001 −f(x)\n",
      "\u00112\n",
      "dx.\n",
      "(8.21)\n",
      "If the estimator is unbiased, both the integrated absolute bias and inte-\n",
      "grated squared bias are 0. This, of course, would mean that the estimator is\n",
      "pointwise unbiased almost everywhere. Although it is not uncommon to have\n",
      "unbiased estimators of scalar parameters or even of vector parameters with a\n",
      "countable number of elements, it is not likely that an estimator of a function\n",
      "could be unbiased at almost all points in a dense domain. (“Almost” means\n",
      "all except possibly a set with a probability measure of 0.)\n",
      "The integrated variance is deﬁned in a similar manner:\n",
      "IV\n",
      "\u0000 bf\n",
      "\u0001\n",
      "=\n",
      "Z\n",
      "D\n",
      "V\n",
      "\u0000 bf(x)\n",
      "\u0001\n",
      "dx\n",
      "=\n",
      "Z\n",
      "D\n",
      "E\n",
      "\u0010\u0000 bf(x) −E\u0000 bf(x)\u0001\u00012\u0011\n",
      "dx.\n",
      "(8.22)\n",
      "Integrated Mean Squared Error and Mean Absolute Error\n",
      "As we suggested above, global unbiasedness is generally not to be expected. An\n",
      "important measure for comparing estimators of funtions is, therefore, based\n",
      "on the mean squared error.\n",
      "The integrated mean squared error is\n",
      "IMSE\n",
      "\u0000 bf\n",
      "\u0001\n",
      "=\n",
      "Z\n",
      "D\n",
      "E\n",
      "\u0010\u0000 bf(x) −f(x)\n",
      "\u00012\u0011\n",
      "dx\n",
      "= IV\u0000 bf \u0001 + ISB\u0000 bf \u0001\n",
      "(8.23)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "574\n",
      "8 Nonparametric and Robust Inference\n",
      "(compare equations (8.11) and (8.12)).\n",
      "If the expectation integration can be interchanged with the outer integra-\n",
      "tion in the expression above, we have\n",
      "IMSE\u0000 bf \u0001 = E\n",
      "\u0012Z\n",
      "D\n",
      "\u0010\n",
      "bf(x) −f(x)\n",
      "\u00112\n",
      "dx\n",
      "\u0013\n",
      "= MISE\u0000 bf \u0001,\n",
      "the mean integrated squared error. We will assume that this interchange leaves\n",
      "the integrals unchanged, so we will use MISE and IMSE interchangeably.\n",
      "Similarly, for the integrated mean absolute error, we have\n",
      "IMAE\u0000 bf \u0001 =\n",
      "Z\n",
      "D\n",
      "E\n",
      "\u0010\f\f bf(x) −f(x)\n",
      "\f\f\n",
      "\u0011\n",
      "dx\n",
      "= E\n",
      "\u0012Z\n",
      "D\n",
      "\f\f\f bf(x) −f(x)\n",
      "\f\f\f dx\n",
      "\u0013\n",
      "= MIAE\n",
      "\u0000 bf\n",
      "\u0001\n",
      ",\n",
      "the mean integrated absolute error.\n",
      "Mean SAE\n",
      "The mean sup absolute error, or MSAE, is\n",
      "MSAE\n",
      "\u0000 bf\n",
      "\u0001\n",
      "=\n",
      "Z\n",
      "D\n",
      "E\n",
      "\u0010\n",
      "sup\n",
      "\f\f bf(x) −f(x)\n",
      "\f\f\n",
      "\u0011\n",
      "dx.\n",
      "(8.24)\n",
      "This measure is not very useful unless the variation in the function f is rela-\n",
      "tively small. For example, if f is a density function, bf can be a “good” estima-\n",
      "tor, yet the MSAE may be quite large. On the other hand, if f is a cumulative\n",
      "distribution function (monotonically ranging from 0 to 1), the MSAE may be\n",
      "a good measure of how well the estimator performs. As mentioned earlier, the\n",
      "SAE is the Kolmogorov distance. The Kolmogorov distance (and, hence, the\n",
      "SAE and the MSAE) does poorly in measuring diﬀerences in the tails of the\n",
      "distribution.\n",
      "Large-Sample Statistical Properties\n",
      "The pointwise consistency properties are extended to the full function in the\n",
      "obvious way. In the notation of expression (8.14), consistency of the function\n",
      "estimator is deﬁned in terms of\n",
      "Z\n",
      "D\n",
      "E\n",
      "\u0010\n",
      "m\n",
      "\u0000 bf(x) −f(x)\n",
      "\u0001\u0011\n",
      "dx →0.\n",
      "The estimator of the function is said to be mean square consistent or L2\n",
      "consistent if the MISE converges to 0; that is,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.3 Nonparametric Estimation of Functions\n",
      "575\n",
      "Z\n",
      "D\n",
      "E\n",
      "\u0010\u0000 bf(x) −f(x)\u00012\u0011\n",
      "dx →0.\n",
      "If the convergence is weak, that is, if it is convergence in probability, we say\n",
      "that the function estimator is weakly consistent; if the convergence is strong,\n",
      "that is, if it is convergence almost surely or with probability 1, we say the\n",
      "function estimator is strongly consistent.\n",
      "The estimator of the function is said to be L1 consistent if the mean\n",
      "integrated absolute error (MIAE) converges to 0; that is,\n",
      "Z\n",
      "D\n",
      "E\n",
      "\u0010\f\f\f bf(x) −f(x)\n",
      "\f\f\f\n",
      "\u0011\n",
      "dx →0.\n",
      "As with the other kinds of consistency, the nature of the convergence in the\n",
      "deﬁnition may be expressed in the qualiﬁers “weak” or “strong”.\n",
      "As we have mentioned above, the integrated absolute error is invariant\n",
      "under monotone transformations of the coordinate axes, but the L2 measures\n",
      "are not. As with most work in L1, however, derivation of various properties\n",
      "of IAE or MIAE is more diﬃcult than for analogous properties with respect\n",
      "to L2 criteria.\n",
      "If the MISE converges to 0, we are interested in the rate of convergence.\n",
      "To determine this, we seek an expression of MISE as a function of n. We do\n",
      "this by a Taylor series expansion.\n",
      "In general, if bθ is an estimator of θ, the Taylor series for ISE(bθ), equa-\n",
      "tion (8.18), about the true value is\n",
      "ISE\n",
      "\u0000bθ\n",
      "\u0001\n",
      "=\n",
      "∞\n",
      "X\n",
      "k=0\n",
      "1\n",
      "k!\n",
      "\u0010\n",
      "bθ −θ\n",
      "\u0011k\n",
      "ISEk′(θ),\n",
      "(8.25)\n",
      "where ISEk′(θ) represents the kth derivative of ISE evaluated at θ.\n",
      "Taking the expectation in equation (8.25) yields the MISE. The limit of\n",
      "the MISE as n →∞is the asymptotic mean integrated squared error, AMISE.\n",
      "One of the most important properties of an estimator is the order of the\n",
      "AMISE.\n",
      "In the case of an unbiased estimator, the ﬁrst two terms in the Taylor\n",
      "series expansion are zero, and the AMISE is\n",
      "V(bθ) ISE′′(θ)\n",
      "to terms of second order.\n",
      "Other Global Properties of Estimators of Functions\n",
      "There are often other properties that we would like an estimator of a function\n",
      "to possess. We may want the estimator to weight given functions in some\n",
      "particular way. For example, if we know how the function to be estimated,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "576\n",
      "8 Nonparametric and Robust Inference\n",
      "f, weights a given function r, we may require that the estimate bf weight the\n",
      "function r in the same way; that is,\n",
      "Z\n",
      "D\n",
      "r(x) bf(x)dx =\n",
      "Z\n",
      "D\n",
      "r(x)f(x)dx.\n",
      "We may want to restrict the minimum and maximum values of the esti-\n",
      "mator. For example, because many functions of interest are nonnegative, we\n",
      "may want to require that the estimator be nonnegative.\n",
      "We may want to restrict the variation in the function. This can be thought\n",
      "of as the “roughness” of the function. A reasonable measure of the variation\n",
      "is\n",
      "Z\n",
      "D\n",
      "\u0012\n",
      "f(x) −\n",
      "Z\n",
      "D\n",
      "f(x)dx\n",
      "\u00132\n",
      "dx.\n",
      "If the integral R\n",
      "D f(x)dx is constrained to be some constant (such as 1 in the\n",
      "case that f(x) is a probability density), then the variation can be measured\n",
      "by the square of the L2 norm,\n",
      "S(f) =\n",
      "Z\n",
      "D\n",
      "\u0000f(x)\u00012dx.\n",
      "(8.26)\n",
      "We may want to restrict the derivatives of the estimator or the smooth-\n",
      "ness of the estimator. Another intuitive measure of the roughness of a twice-\n",
      "diﬀerentiable and integrable univariate function f is the integral of the square\n",
      "of the second derivative:\n",
      "R(f) =\n",
      "Z\n",
      "D\n",
      "\u0000f′′(x)\n",
      "\u00012dx.\n",
      "(8.27)\n",
      "Often, in function estimation, we may seek an estimator bf such that its rough-\n",
      "ness (by some deﬁnition) is small.\n",
      "8.4 Semiparametric Methods and Partial Likelihood\n",
      "In various contexts, we have considered estimation of probabilities of random\n",
      "variables being in speciﬁed intervals. This is estimation of a CDF evaluated at\n",
      "speciﬁed points. In this section, we will consider related problems of estimation\n",
      "of functional components of a CDF. These problems can be fully parametric\n",
      "or they can be semiparametric; that is, there are some “parameters”, but the\n",
      "form of the PDF or CDF may not be fully speciﬁed.\n",
      "We will focus on models of failure time data. The random variable of\n",
      "interest is the time to failure (from some arbitrary 0 time). We are interested\n",
      "in the distribution of the lifetimes of experimental units, for example, how\n",
      "long an electrical device will continue to operate. These problems may involve\n",
      "censoring, such as the setup in Example 6.3. A memoryless process, such as in\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.4 Semiparametric Methods and Partial Likelihood\n",
      "577\n",
      "that example, is often unrealistic because the survival rate does not depend\n",
      "on the age of the experimental units. In other settings we may assume the\n",
      "rate does depend on the age, and so we may be interested in the conditional\n",
      "rate given that the units have survived for some given time. Alternatively,\n",
      "or additionally, we may assume that the survival rate depends on observable\n",
      "covariates. (Note that we will speak of “survival rate” sometimes, and “failure\n",
      "rate” sometimes.)\n",
      "8.4.1 The Hazard Function\n",
      "The hazard function measures the instantaneous rate of failure.\n",
      "Deﬁnition 8.1 (hazard function)\n",
      "Let F be a CDF with associated PDF f. The hazard function is deﬁned at t,\n",
      "where F (t) < 1, as\n",
      "λ(t) = f(t)/(1 −F (t)),\n",
      "(8.28)\n",
      "and the cumulative hazard function is\n",
      "Λ(t) =\n",
      "Z t\n",
      "0\n",
      "λ(s)ds.\n",
      "(8.29)\n",
      "In applications, the basic function is the survival function S instead of the\n",
      "CDF F . The survival function is the denominator in the hazard function; that\n",
      "is, S(t) = 1 −F (t).\n",
      "Note that if the CDF is absolutely continuous, the hazard function is the\n",
      "derivative of the log of the survival function, and we have\n",
      "S(t) = exp(−Λ(t)).\n",
      "(8.30)\n",
      "The common probability models for failure time data are the exponential,\n",
      "Weibull, log-normal, and gamma families. The hazard function generally is a\n",
      "function both of the time t, and of the parameters in the probability model.\n",
      "In the case of the exponential(θ) family, we have\n",
      "λ(t) = 1\n",
      "θ e−t/θ/e−t/θ = 1\n",
      "θ .\n",
      "In applications of with failure time data, the parameter is often taken to be\n",
      "1/θ, and so the hazard rate is the parameter. In the exponential family the\n",
      "hazard function is a function only of the parameter in the probability model. It\n",
      "is constant with respect to time; this corresponds to the memoryless property.\n",
      "In other cases, the hazard rate may not be constant; see Exercise 8.3.\n",
      "If f(t) is interpreted as the instantaneous survival rate, then the hazard is\n",
      "the conditional survival rate, given survival to the point t.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "578\n",
      "8 Nonparametric and Robust Inference\n",
      "Theorem 8.1\n",
      "If λ is the hazard function associated with the random variable T, then\n",
      "λ(t) = lim\n",
      "ϵ↓0 ϵ−1Pr(t ≤T < t + ϵ|T ≥t).\n",
      "(8.31)\n",
      "Proof. Exercise 8.5.\n",
      "In applications we often assume that the hazard function is aﬀected by\n",
      "a p-vector of observable covariates x; that is, we have a function λ(t, x, θ, β),\n",
      "where I have written two sets of parameters, θ for those of the basic probability\n",
      "model (Weibull, for example), and β for parameters in a model of how the\n",
      "covariates x aﬀect the hazard function. If the eﬀects of the covariates are linear\n",
      "and additive, we may represent their overall eﬀect by βTx, just as in the linear\n",
      "models discussed elsewhere in this book. It is unlikely that their eﬀect is linear,\n",
      "but it is often the case that a function of the linear combination βTx, where\n",
      "β is a p-vector of unknown constants, seems to correspond well with observed\n",
      "data. In that case, we may write the conditional hazard function as λ(t; βTx),\n",
      "suppressing other model parameters.\n",
      "8.4.2 Proportional Hazards Models\n",
      "In a very useful class of hazard functions, the eﬀect of the covariates on\n",
      "the hazard function is multiplicative; that is, the function is of the form\n",
      "λ(t, x, β) = λ0(t)φ(x, β), where λ0(t) is the “baseline” hazard function if there\n",
      "is no eﬀect due to the covariates, and φ is some known function, and again we\n",
      "have suppressed other model parameters. Such models are called proportional\n",
      "hazards models.\n",
      "Note that in a proportional hazards model, we can identify a baseline\n",
      "cumulative hazard function Λ0(t) based only on λ0(t), and furthermore, the\n",
      "survival function can be written as\n",
      "1 −F (t) = exp(φ(x, β)Λ0(t)).\n",
      "(8.32)\n",
      "This is an important property of proportional hazards models. The parameters\n",
      "for the eﬀect of the covariates, that is, β, can be estimated by maximizing\n",
      "a partial likelihood without any consideration of the hazard function. The\n",
      "survival function is composed of a parametric component involving β and the\n",
      "component Λ0(t), which we may estimate without parameters.\n",
      "*** estimate β using partial likelihood **** refer to Chapter 6.\n",
      "*** estimate Λ0(t) nonparametrically. give both Breslow’s estimator and\n",
      "Horowitz’s estimator\n",
      "If the eﬀects of the covariates are linear and additive with respect to each\n",
      "other, we may represent the hazard function as λ0(t)φ(βTx). A simple case\n",
      "may have the form λ0(t)(1 + βTx). This form, of course, would require some\n",
      "restriction on βTx, similar in some ways to the restriction that partially mo-\n",
      "tivated the development of generalized linear models. Another form that does\n",
      "not require any restrictions is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "579\n",
      "λ(t; x) = λ0(t)eβTx.\n",
      "(8.33)\n",
      "This model of the hazard function is called the Cox proportional hazards\n",
      "model.\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "***************** Scott (1992) and Scott (2012)\n",
      "There are obviously many connections between estimation of a CDF and\n",
      "the corresponding PDF. We have seen that the ECDF is a strongly consistent\n",
      "estimator of the CDF (Theorem 1.71), but it is not immediately obvious how\n",
      "to use the ECDF to construct an estimator of the PDF. Nevertheless, in many\n",
      "cases, an estimate of the PDF is more useful than an estimate of the CDF.\n",
      "8.5.1 Nonparametric Probability Density Estimation\n",
      "Estimation of a probability density function is similar to the estimation of\n",
      "any function, and the properties of the function estimators that we have dis-\n",
      "cussed are relevant for density function estimators. A density function p(y) is\n",
      "characterized by two properties:\n",
      "•\n",
      "it is nonnegative everywhere;\n",
      "•\n",
      "it integrates to 1 (with the appropriate deﬁnition of “integrate”).\n",
      "In this chapter, we consider several nonparametric estimators of a den-\n",
      "sity; that is, estimators of a general nonnegative function that integrates to 1\n",
      "and for which we make no assumptions about a functional form other than,\n",
      "perhaps, smoothness.\n",
      "It seems reasonable that we require the density estimate to have the char-\n",
      "acteristic properties of a density:\n",
      "•\n",
      "bp(y) ≥0 for all y;\n",
      "•\n",
      "R\n",
      "IRd bp(y) dy = 1.\n",
      "A probability density estimator that is nonnegative and integrates to 1 is\n",
      "called a bona ﬁde estimator.\n",
      "Rosenblatt has shown that no unbiased bona ﬁde estimator can exist for\n",
      "all continuous p. Rather than requiring an unbiased estimator that cannot be\n",
      "a bona ﬁde estimator, we generally seek a bona ﬁde estimator with small mean\n",
      "squared error or a sequence of bona ﬁde estimators bpn that are asymptotically\n",
      "unbiased; that is,\n",
      "Ep(bpn(y)) →p(y)\n",
      "for all y ∈IRd as n →∞.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "580\n",
      "8 Nonparametric and Robust Inference\n",
      "The Likelihood Function\n",
      "Suppose that we have a random sample, y1, . . ., yn, from a population with\n",
      "density p. Treating the density p as a variable, we write the likelihood func-\n",
      "tional as\n",
      "L(p; y1, . . ., yn) =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "p(yi).\n",
      "The maximum likelihood method of estimation obviously cannot be used di-\n",
      "rectly because this functional is unbounded in p. We may, however, seek an\n",
      "estimator that maximizes some modiﬁcation of the likelihood. There are two\n",
      "reasonable ways to approach this problem. One is to restrict the domain of\n",
      "the optimization problem. This is called restricted maximum likelihood. The\n",
      "other is to regularize the estimator by adding a penalty term to the functional\n",
      "to be optimized. This is called penalized maximum likelihood.\n",
      "We may seek to maximize the likelihood functional subject to the con-\n",
      "straint that p be a bona ﬁde density. If we put no further restrictions on\n",
      "the function p, however, inﬁnite Dirac spikes at each observation give an un-\n",
      "bounded likelihood, so a maximum likelihood estimator cannot exist, subject\n",
      "only to the restriction to the bona ﬁde class. An additional restriction that\n",
      "p be Lebesgue-integrable over some domain D (that is, p ∈L1(D)) does not\n",
      "resolve the problem because we can construct sequences of ﬁnite spikes at\n",
      "each observation that grow without bound.\n",
      "We therefore must restrict the class further. Consider a ﬁnite dimensional\n",
      "class, such as the class of step functions that are bona ﬁde density estimators.\n",
      "We assume that the sizes of the regions over which the step function is constant\n",
      "are greater than 0.\n",
      "For a step function with m regions having constant values, c1, . . ., cm, the\n",
      "likelihood is\n",
      "L(c1, . . ., cm; y1, . . ., yn) =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "p(yi)\n",
      "=\n",
      "m\n",
      "Y\n",
      "k=1\n",
      "cnk\n",
      "k ,\n",
      "(8.34)\n",
      "where nk is the number of data points in the kth region. For the step function\n",
      "to be a bona ﬁde estimator, all ck must be nonnegative and ﬁnite. A maximum\n",
      "therefore exists in the class of step functions that are bona ﬁde estimators.\n",
      "If vk is the measure of the volume of the kth region (that is, vk is the\n",
      "length of an interval in the univariate case, the area in the bivariate case, and\n",
      "so on), we have\n",
      "m\n",
      "X\n",
      "k=1\n",
      "ckvk = 1.\n",
      "We incorporate this constraint together with equation (8.34) to form the La-\n",
      "grangian,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "581\n",
      "L(c1, . . ., cm) + λ\n",
      " \n",
      "1 −\n",
      "m\n",
      "X\n",
      "k=1\n",
      "ckvk\n",
      "!\n",
      ".\n",
      "Diﬀerentiating the Lagrangian function and setting the derivative to zero, we\n",
      "have at the maximum point ck = c∗\n",
      "k, for any λ,\n",
      "∂L\n",
      "∂ck\n",
      "= λvk.\n",
      "Using the derivative of L from equation (8.34), we get\n",
      "nkL = λc∗\n",
      "kvk.\n",
      "Summing both sides of this equation over k, we have\n",
      "nL = λ,\n",
      "and then substituting, we have\n",
      "nkL = nLc∗\n",
      "kvk.\n",
      "Therefore, the maximum of the likelihood occurs at\n",
      "c∗\n",
      "k = nk\n",
      "nvk\n",
      ".\n",
      "The restricted maximum likelihood estimator is therefore\n",
      "bp(y) = nk\n",
      "nvk\n",
      ", for y ∈region k,\n",
      "= 0,\n",
      "otherwise.\n",
      "(8.35)\n",
      "Instead of restricting the density estimate to step functions, we could con-\n",
      "sider other classes of functions, such as piecewise linear functions.\n",
      "We may also seek other properties, such as smoothness, for the estimated\n",
      "density. One way of achieving other desirable properties for the estimator is\n",
      "to use a penalizing function to modify the function to be optimized. Instead\n",
      "of the likelihood function, we may use a penalized likelihood function of the\n",
      "form\n",
      "Lp(p; y1, . . ., yn) =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "p(yi)e−T (p),\n",
      "(8.36)\n",
      "where T (p) is a transform that measures some property that we would like\n",
      "to minimize. For example, to achieve smoothness, we may use the transform\n",
      "R(p) of equation (8.27) in the penalizing factor. To choose a function ˆp to\n",
      "maximize Lp(p) we would have to use some ﬁnite series approximation to\n",
      "T (ˆp).\n",
      "For densities with special properties there may be likelihood approaches\n",
      "that take advantage of those properties.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "582\n",
      "8 Nonparametric and Robust Inference\n",
      "8.5.2 Histogram Estimators\n",
      "Let us assume ﬁnite support D, and construct a ﬁxed partition of D into a grid\n",
      "of m nonoverlapping bins Tk. (We can arbitrarily assign bin boundaries to one\n",
      "or the other bin.) Let vk be the volume of the kth bin (in one dimension, vk\n",
      "is a length and in this simple case is often denoted hk; in two dimensions, vk\n",
      "is an area, and so on). The number of such bins we choose, and consequently\n",
      "their volumes, depends on the sample size n, so we sometimes indicate that\n",
      "dependence in the notation: vn,k. For the sample y1, . . ., yn, the histogram\n",
      "estimator of the probability density function is deﬁned as\n",
      "bpH(y) =\n",
      "m\n",
      "X\n",
      "k=1\n",
      "1\n",
      "vk\n",
      "Pn\n",
      "i=1 ITk(yi)\n",
      "n\n",
      "ITk(y),\n",
      "for y ∈D,\n",
      "= 0,\n",
      "otherwise.\n",
      "The histogram is the restricted maximum likelihood estimator (8.35).\n",
      "Letting nk be the number of sample values falling into Tk,\n",
      "nk =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "ITk(yi),\n",
      "we have the simpler expression for the histogram over D,\n",
      "bpH(y) =\n",
      "m\n",
      "X\n",
      "k=1\n",
      "nk\n",
      "nvk\n",
      "ITk(y).\n",
      "(8.37)\n",
      "As we have noted already, this is a bona ﬁde estimator:\n",
      "bpH(y) ≥0\n",
      "and\n",
      "Z\n",
      "IRd bpH(y)dy =\n",
      "m\n",
      "X\n",
      "k=1\n",
      "nk\n",
      "nvk\n",
      "vk\n",
      "= 1.\n",
      "Although our discussion generally concerns observations on multivariate\n",
      "random variables, we should occasionally consider simple univariate observa-\n",
      "tions. One reason why the univariate case is simpler is that the derivative is a\n",
      "scalar function. Another reason why we use the univariate case as a model is\n",
      "because it is easier to visualize. The density of a univariate random variable\n",
      "is two-dimensional, and densities of other types of random variables are of\n",
      "higher dimension, so only in the univariate case can the density estimates be\n",
      "graphed directly.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "583\n",
      "In the univariate case, we assume that the support is the ﬁnite interval\n",
      "[a, b]. We partition [a, b] into a grid of m nonoverlapping bins Tk = [tn,k, tn,k+1)\n",
      "where\n",
      "a = tn,1 < tn,2 < . . . < tn,m+1 = b.\n",
      "The univariate histogram is\n",
      "bpH(y) =\n",
      "m\n",
      "X\n",
      "k=1\n",
      "nk\n",
      "n(tn,k+1 −tn,k)ITk(y).\n",
      "(8.38)\n",
      "If the bins are of equal width, say h (that is, tk = tk−1 +h), the histogram\n",
      "is\n",
      "bpH(y) = nk\n",
      "nh,\n",
      "for y ∈Tk.\n",
      "This class of functions consists of polynomial splines of degree 0 with ﬁxed\n",
      "knots, and the histogram is the maximum likelihood estimator over the class\n",
      "of step functions. Generalized versions of the histogram can be deﬁned with\n",
      "respect to splines of higher degree. Splines with degree higher than 1 may\n",
      "yield negative estimators, but such histograms are also maximum likelihood\n",
      "estimators over those classes of functions.\n",
      "The histogram as we have deﬁned it is sometimes called a “density his-\n",
      "togram”, whereas a “frequency histogram” is not normalized by the n.\n",
      "Some Properties of the Histogram Estimator\n",
      "The histogram estimator, being a step function, is discontinuous at cell bound-\n",
      "aries, and it is zero outside of a ﬁnite range. It is sensitive both to the bin size\n",
      "and to the choice of the origin.\n",
      "An important advantage of the histogram estimator is its simplicity, both\n",
      "for computations and for analysis. In addition to its simplicity, as we have\n",
      "seen, it has two other desirable global properties:\n",
      "•\n",
      "It is a bona ﬁde density estimator.\n",
      "•\n",
      "It is the unique maximum likelihood estimator conﬁned to the subspace\n",
      "of functions of the form\n",
      "g(t) = ck, for t ∈Tk,\n",
      "= 0, otherwise,\n",
      "and where g(t) ≥0 and R\n",
      "∪kTk g(t) dt = 1.\n",
      "Pointwise and Binwise Properties\n",
      "Properties of the histogram vary from bin to bin. From equation (8.37), the\n",
      "expectation of the histogram estimator at the point y in bin Tk is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "584\n",
      "8 Nonparametric and Robust Inference\n",
      "E(bpH(y)) = pk\n",
      "vk\n",
      ",\n",
      "(8.39)\n",
      "where\n",
      "pk =\n",
      "Z\n",
      "Tk\n",
      "p(t) dt\n",
      "(8.40)\n",
      "is the probability content of the kth bin.\n",
      "Some pointwise properties of the histogram estimator are the following:\n",
      "•\n",
      "The bias of the histogram at the point y within the kth bin is\n",
      "pk\n",
      "vk\n",
      "−p(y).\n",
      "(8.41)\n",
      "Note that the bias is diﬀerent from bin to bin, even if the bins are of\n",
      "constant size. The bias tends to decrease as the bin size decreases. We can\n",
      "bound the bias if we assume a regularity condition on p. If there exists γ\n",
      "such that for any y1 ̸= y2 in an interval\n",
      "|p(y1) −p(y2)| < γ∥y1 −y2∥,\n",
      "we say that p is Lipschitz-continuous on the interval, and for such a density,\n",
      "for any ξk in the kth bin, we have\n",
      "|Bias(bpH(y))| = |p(ξk) −p(y)|\n",
      "≤γk∥ξk −y∥\n",
      "≤γkvk.\n",
      "(8.42)\n",
      "•\n",
      "The variance of the histogram at the point y within the kth bin is\n",
      "V\n",
      "\u0000bpH(y)\n",
      "\u0001\n",
      "= V(nk)/(nvk)2\n",
      "= pk(1 −pk)\n",
      "nv2\n",
      "k\n",
      ".\n",
      "(8.43)\n",
      "This is easily seen by recognizing that nk is a binomial random variable\n",
      "with parameters n and pk. Notice that the variance decreases as the bin\n",
      "size increases. Note also that the variance is diﬀerent from bin to bin. We\n",
      "can bound the variance:\n",
      "V(bpH(y)) ≤pk\n",
      "nv2\n",
      "k\n",
      ".\n",
      "By the mean-value theorem, we have pk = vkp(ξk) for some ξk ∈Tk, so\n",
      "we can write\n",
      "V(bpH(y)) ≤p(ξk)\n",
      "nvk\n",
      ".\n",
      "Notice the tradeoﬀbetween bias and variance: as h increases the variance,\n",
      "equation (8.43), decreases, but the bound on the bias, equation (8.42), in-\n",
      "creases.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "585\n",
      "•\n",
      "The mean squared error of the histogram at the point y within the kth\n",
      "bin is\n",
      "MSE\n",
      "\u0000bpH(y)\n",
      "\u0001\n",
      "= pk(1 −pk)\n",
      "nv2\n",
      "k\n",
      "+\n",
      "\u0012pk\n",
      "vk\n",
      "−p(y)\n",
      "\u00132\n",
      ".\n",
      "(8.44)\n",
      "For a Lipschitz-continuous density, within the kth bin we have\n",
      "MSE\n",
      "\u0000bpH(y)\n",
      "\u0001\n",
      "≤p(ξk)\n",
      "nvk\n",
      "+ γ2\n",
      "kv2\n",
      "k.\n",
      "(8.45)\n",
      "We easily see that the histogram estimator is L2 pointwise consistent for\n",
      "a Lipschitz-continuous density if, as n →∞, for each k, vk →0 and\n",
      "nvk →∞. By diﬀerentiating, we see that the minimum of the bound on\n",
      "the MSE in the kth bin occurs for\n",
      "h∗(k) =\n",
      "\u0012p(ξk)\n",
      "2γ2\n",
      "kn\n",
      "\u00131/3\n",
      ".\n",
      "(8.46)\n",
      "Substituting this value back into MSE, we obtain the order of the optimal\n",
      "MSE at the point x,\n",
      "MSE∗\u0000bpH(y)\n",
      "\u0001\n",
      "∈O\n",
      "\u0000n−2/3\u0001\n",
      ".\n",
      "Asymptotic MISE (or AMISE) of Histogram Estimators\n",
      "Global properties of the histogram are obtained by summing the binwise prop-\n",
      "erties over all of the bins.\n",
      "The expressions for the integrated variance and the integrated squared bias\n",
      "are quite complicated because they depend on the bin sizes and the probability\n",
      "content of the bins. We will ﬁrst write the general expressions, and then we will\n",
      "assume some degree of smoothness of the true density and write approximate\n",
      "expressions that result from mean values or Taylor approximations. We will\n",
      "assume rectangular bins for additional simpliﬁcation. Finally, we will then\n",
      "consider bins of equal size to simplify the expressions further.\n",
      "First, consider the integrated variance,\n",
      "IV\n",
      "\u0000bpH\n",
      "\u0001\n",
      "=\n",
      "Z\n",
      "IRd V(bpH(t)) dt\n",
      "=\n",
      "m\n",
      "X\n",
      "k=1\n",
      "Z\n",
      "Tk\n",
      "V(bpH(t)) dt\n",
      "=\n",
      "m\n",
      "X\n",
      "k=1\n",
      "pk −p2\n",
      "k\n",
      "nvk\n",
      "=\n",
      "m\n",
      "X\n",
      "k=1\n",
      "\u0012 1\n",
      "nvk\n",
      "−\n",
      "P p(ξk)2vk\n",
      "n\n",
      "\u0013\n",
      "+ o(n−1)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "586\n",
      "8 Nonparametric and Robust Inference\n",
      "for some ξk ∈Tk, as before. Now, taking P p(ξk)2vk as an approximation\n",
      "to the integral R (p(t))2 dt, and letting S be the functional that measures the\n",
      "variation in a square-integrable function of d variables,\n",
      "S(g) =\n",
      "Z\n",
      "IRd(g(t))2 dt,\n",
      "(8.47)\n",
      "we have the integrated variance,\n",
      "IV\n",
      "\u0000bpH\n",
      "\u0001\n",
      "≈\n",
      "m\n",
      "X\n",
      "k=1\n",
      "1\n",
      "nvk\n",
      "−S(p)\n",
      "n ,\n",
      "(8.48)\n",
      "and the asymptotic integrated variance,\n",
      "AIV\u0000bpH\n",
      "\u0001 =\n",
      "m\n",
      "X\n",
      "k=1\n",
      "1\n",
      "nvk\n",
      ".\n",
      "(8.49)\n",
      "The measure of the variation, S(p), is a measure of the roughness of the\n",
      "density because the density integrates to 1.\n",
      "Now, consider the other term in the integrated MSE, the integrated\n",
      "squared bias. We will consider the case of rectangular bins, in which hk =\n",
      "(hk1, . . ., hkd) is the vector of lengths of sides in the kth bin. In the case of\n",
      "rectangular bins, vk = Πd\n",
      "j=1hkj.\n",
      "We assume that the density can be expanded in a Taylor series, and we\n",
      "expand the density in the kth bin about ¯tk, the midpoint of the rectangular\n",
      "bin. For ¯tk + t ∈Tk, we have\n",
      "p(¯tk + t) = p(¯tk) + tT∇p(¯tk) + 1\n",
      "2tTHp(¯tk)t + · · · ,\n",
      "(8.50)\n",
      "where Hp(¯tk) is the Hessian of p evaluated at ¯tk.\n",
      "The probability content of the kth bin, pk, from equation (8.40), can be\n",
      "expressed as an integral of the Taylor series expansion:\n",
      "pk =\n",
      "Z\n",
      "¯tk+t∈Tk\n",
      "p(¯tk + t) dt\n",
      "=\n",
      "Z hkd/2\n",
      "−hkd/2\n",
      "· · ·\n",
      "Z hk1/2\n",
      "−hk1/2\n",
      "\u0000p(¯tk) + tT∇p(¯tk) + . . .\u0001 dt1 · · · dtd\n",
      "= vkp(¯tk) + O\n",
      "\u0010\n",
      "hd+2\n",
      "k∗\n",
      "\u0011\n",
      ",\n",
      "(8.51)\n",
      "where hk∗= minj hkj. The bias at a point ¯tk + t in the kth bin, after substi-\n",
      "tuting equations (8.50) and (8.51) into equation (8.41), is\n",
      "pk\n",
      "vk\n",
      "−p(¯tk + t) = −tT∇p(¯tk) + O\n",
      "\u0000h2\n",
      "k∗\n",
      "\u0001\n",
      ".\n",
      "For the kth bin the integrated squared bias is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "587\n",
      "ISBk(bpH)\n",
      "=\n",
      "Z\n",
      "Tk\n",
      " \n",
      "\u0000tT∇p(¯tk)\n",
      "\u00012 −O\n",
      "\u0000h2\n",
      "k∗\n",
      "\u0001\n",
      "tT∇p(¯tk) + O\n",
      "\u0000h4\n",
      "k∗\n",
      "\u0001\n",
      "!\n",
      "dt\n",
      "=\n",
      "Z hkd/2\n",
      "−hkd/2\n",
      "· · ·\n",
      "Z hk1/2\n",
      "−hk1/2\n",
      "X\n",
      "i\n",
      "X\n",
      "j\n",
      "tkitkj∇ip(¯tk)∇jp(¯tk) dt1 · · ·dtd + O\n",
      "\u0010\n",
      "h4+d\n",
      "k∗\n",
      "\u0011\n",
      ".\n",
      "(8.52)\n",
      "Many of the expressions above are simpler if we use a constant bin size,\n",
      "v, or h1, . . ., hd. In the case of constant bin size, the asymptotic integrated\n",
      "variance in equation (8.49) becomes\n",
      "AIV\n",
      "\u0000bpH\n",
      "\u0001\n",
      "= m\n",
      "nv.\n",
      "(8.53)\n",
      "In this case, the integral in equation (8.52) simpliﬁes as the integration is\n",
      "performed term by term because the cross-product terms cancel, and the\n",
      "integral is\n",
      "1\n",
      "12(h1 · · · hd)\n",
      "d\n",
      "X\n",
      "j=1\n",
      "h2\n",
      "j\n",
      "\u0000∇jp(¯tk)\n",
      "\u00012.\n",
      "(8.54)\n",
      "This is the asymptotic squared bias integrated over the kth bin.\n",
      "When we sum the expression (8.54) over all bins, the \u0000∇jp(¯tk)\u00012 become\n",
      "S\n",
      "\u0000∇jp\n",
      "\u0001\n",
      ", and we have the asymptotic integrated squared bias,\n",
      "AISB\n",
      "\u0000bpH\n",
      "\u0001\n",
      "= 1\n",
      "12\n",
      "d\n",
      "X\n",
      "j=1\n",
      "h2\n",
      "jS\n",
      "\u0000∇jp\n",
      "\u0001\n",
      ".\n",
      "(8.55)\n",
      "Combining the asymptotic integrated variance, equation (8.53), and squared\n",
      "bias, equation (8.55), for the histogram with rectangular bins of constant size,\n",
      "we have\n",
      "AMISE\n",
      "\u0000bpH\n",
      "\u0001\n",
      "=\n",
      "1\n",
      "n(h1 · · ·hd) + 1\n",
      "12\n",
      "d\n",
      "X\n",
      "j=1\n",
      "h2\n",
      "jS\n",
      "\u0000∇jp\n",
      "\u0001\n",
      ".\n",
      "(8.56)\n",
      "As we have seen before, smaller bin sizes increase the variance but decrease\n",
      "the squared bias.\n",
      "Bin Sizes\n",
      "As we have mentioned and have seen by example, the histogram is very sen-\n",
      "sitive to the bin sizes, both in appearance and in other properties. Equa-\n",
      "tion (8.56) for the AMISE assuming constant rectangular bin size is often\n",
      "used as a guide for determining the bin size to use when constructing a his-\n",
      "togram. This expression involves S\n",
      "\u0000∇jp\n",
      "\u0001\n",
      "and so, of course, cannot be used\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "588\n",
      "8 Nonparametric and Robust Inference\n",
      "directly. Nevertheless, diﬀerentiating the expression with respect to hj and\n",
      "setting the result equal to zero, we have the bin width that is optimal with\n",
      "respect to the AMISE,\n",
      "hj∗= S\n",
      "\u0000∇jp\n",
      "\u0001−1/2\n",
      " \n",
      "6\n",
      "d\n",
      "Y\n",
      "i=1\n",
      "S\n",
      "\u0000∇ip\n",
      "\u00011/2\n",
      "!\n",
      "1\n",
      "2+d\n",
      "n−\n",
      "1\n",
      "2+d .\n",
      "(8.57)\n",
      "Substituting this into equation (8.56), we have the optimal value of the AMISE\n",
      "1\n",
      "4\n",
      " \n",
      "36\n",
      "d\n",
      "Y\n",
      "i=1\n",
      "S\u0000∇ip\u00011/2\n",
      "!\n",
      "1\n",
      "2+d\n",
      "n−\n",
      "2\n",
      "2+d .\n",
      "(8.58)\n",
      "Notice that the optimal rate of decrease of AMISE for histogram estimators\n",
      "is that of O(n−\n",
      "2\n",
      "2+d ). Although histograms have several desirable properties,\n",
      "this order of convergence is not good compared to that of some other bona\n",
      "ﬁde density estimators, as we will see in later sections.\n",
      "The expression for the optimal bin width involves S\n",
      "\u0000∇jp\n",
      "\u0001\n",
      ", where p is\n",
      "the unknown density. An approach is to choose a value for S\n",
      "\u0000∇jp\n",
      "\u0001\n",
      "that cor-\n",
      "responds to some good general distribution. A “good general distribution”,\n",
      "of course, is the normal with a diagonal variance-covariance matrix. For the\n",
      "d-variate normal with variance-covariance matrix Σ = diag(σ2\n",
      "1, . . ., σ2\n",
      "d),\n",
      "S\u0000∇jp\u0001 =\n",
      "1\n",
      "2d+1πd/2σ2\n",
      "j|Σ|1/2 .\n",
      "For a univariate normal density with variance σ2,\n",
      "S(p′) = 1/(4√πσ3),\n",
      "so the optimal constant one-dimensional bin width under the AMISE criterion\n",
      "is\n",
      "3.49σn−1/3.\n",
      "In practice, of course, an estimate of σ must be used. The sample standard\n",
      "deviation s is one obvious choice. A more robust estimate of the scale is based\n",
      "on the sample interquartile range, r. The sample interquartile range leads to\n",
      "a bin width of 2rn−1/3.\n",
      "The AMISE is essentially an L2 measure. The L∞criterion—that is, the\n",
      "sup absolute error (SAE) of equation (8.19)—also leads to an asymptotically\n",
      "optimal bin width that is proportional to n−1/3.\n",
      "One of the most commonly used rules is for the number of bins rather\n",
      "than the width. Assume a symmetric binomial model for the bin counts, that\n",
      "is, the bin count is just the binomial coeﬃcient. The total sample size n is\n",
      "m−1\n",
      "X\n",
      "k=0\n",
      "\u0012\n",
      "m −1\n",
      "k\n",
      "\u0013\n",
      "= 2m−1,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "589\n",
      "and so the number of bins is\n",
      "m = 1 + log2 n.\n",
      "Bin Shapes\n",
      "In the univariate case, histogram bins may vary in size, but each bin is an\n",
      "interval. For the multivariate case, there are various possibilities for the shapes\n",
      "of the bins. The simplest shape is the direct extension of an interval, that is a\n",
      "hyperrectangle. The volume of a hyperrectangle is just vk = Q hkj. There are,\n",
      "of course, other possibilities; any tessellation of the space would work. The\n",
      "objects may or may not be regular, and they may or may not be of equal size.\n",
      "Regular, equal-sized geometric ﬁgures such as hypercubes have the advantages\n",
      "of simplicity, both computationally and analytically. In two dimensions, there\n",
      "are three possible regular tessellations: triangles, squares, and hexagons.\n",
      "For hyperrectangles of constant size, the univariate theory generally ex-\n",
      "tends fairly easily to the multivariate case. The histogram density estimator\n",
      "is\n",
      "bpH(y) =\n",
      "nk\n",
      "nh1h2 · · ·hd\n",
      ",\n",
      "for y ∈Tk,\n",
      "where the h’s are the lengths of the sides of the rectangles. The variance within\n",
      "the kth bin is\n",
      "V(bpH(y)) =\n",
      "npk(1 −pk)\n",
      "(nh1h2 · · · hd)2 ,\n",
      "for y ∈Tk,\n",
      "and the integrated variance is\n",
      "IV(bpH) ≈\n",
      "1\n",
      "nh1h2 · · ·hd\n",
      "−S(f)\n",
      "n\n",
      ".\n",
      "Other Density Estimators Related to the Histogram\n",
      "There are several variations of the histogram that are useful as probability\n",
      "density estimators. The most common modiﬁcation is to connect points on the\n",
      "histogram by a continuous curve. A simple way of doing this in the univariate\n",
      "case leads to the frequency polygon. This is the piecewise linear curve that\n",
      "connects the midpoints of the bins of the histogram. The endpoints are usually\n",
      "zero values at the midpoints of two appended bins, one on either side.\n",
      "The histospline is constructed by interpolating knots of the empirical CDF\n",
      "with a cubic spline and then diﬀerentiating it. More general methods use\n",
      "splines or orthogonal series to ﬁt the histogram.\n",
      "As we have mentioned and have seen by example, the histogram is some-\n",
      "what sensitive in appearance to the location of the bins. To overcome the\n",
      "problem of location of the bins, a density estimator that is the average of sev-\n",
      "eral histograms with equal bin widths but diﬀerent bin locations can be used.\n",
      "This is called the average shifted histogram, or ASH. It also has desirable\n",
      "statistical properties, and it is computationally eﬃcient in the multivariate\n",
      "case.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "590\n",
      "8 Nonparametric and Robust Inference\n",
      "8.5.3 Kernel Estimators\n",
      "Kernel methods are probably the most widely used technique for building\n",
      "nonparametric probability density estimators. They are best understood by\n",
      "developing them as a special type of histogram. The diﬀerence is that the bins\n",
      "in kernel estimators are centered at the points at which the estimator is to\n",
      "be computed. The problem of the choice of location of the bins in histogram\n",
      "estimators does not arise.\n",
      "Rosenblatt’s Histogram Estimator; Kernels\n",
      "For the one-dimensional case, Rosenblatt deﬁned a histogram that is shifted\n",
      "to be centered on the point at which the density is to be estimated. Given the\n",
      "sample y1, . . ., yn, Rosenblatt’s histogram estimator at the point y is\n",
      "bpR(y) = #{yi s.t. yi ∈]y −h/2,\n",
      "y + h/2] }\n",
      "nh\n",
      ".\n",
      "(8.59)\n",
      "This histogram estimator avoids the ordinary histogram’s constant-slope con-\n",
      "tribution to the bias. This estimator is a step function with variable lengths\n",
      "of the intervals that have constant value.\n",
      "Rosenblatt’s centered histogram can also be written in terms of the ECDF:\n",
      "bpR(y) = Pn(y + h/2) −Pn(y −h/2)\n",
      "h\n",
      ",\n",
      "where, as usual, Pn denotes the ECDF. As seen in this expression, Rosenblatt’s\n",
      "estimator is a centered ﬁnite-diﬀerence approximation to the derivative of the\n",
      "empirical cumulative distribution function (which, of course, is not diﬀeren-\n",
      "tiable at the data points). We could, of course, use the same idea and form\n",
      "other density estimators using other ﬁnite-diﬀerence approximations to the\n",
      "derivative of Pn.\n",
      "Another way to write Rosenblatt’s shifted histogram estimator over bins\n",
      "of length h is\n",
      "bpR(y) = 1\n",
      "nh\n",
      "n\n",
      "X\n",
      "i=1\n",
      "κ\n",
      "\u0012y −yi\n",
      "h\n",
      "\u0013\n",
      ",\n",
      "(8.60)\n",
      "where κ(t) = 1 if |t| < 1/2 and = 0 otherwise. The function κ is a kernel or\n",
      "ﬁlter. In Rosenblatt’s estimator, it is a “boxcar” function, but other kernel\n",
      "functions could be used.\n",
      "The estimator extends easily to the multivariate case. In the general kernel\n",
      "estimator, we usually use a more general scaling of y −yi,\n",
      "V −1(y −yi),\n",
      "for some positive-deﬁnite matrix V . The determinant of V −1 scales the esti-\n",
      "mator to account for the scaling within the kernel function. The general kernel\n",
      "estimator is given by\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "591\n",
      "bpK(y) =\n",
      "1\n",
      "n|V |\n",
      "n\n",
      "X\n",
      "i=1\n",
      "κ \u0000V −1(y −yi)\u0001 ,\n",
      "(8.61)\n",
      "where the function κ is called the kernel, and V is the smoothing matrix. The\n",
      "determinant of the smoothing matrix is exactly analogous to the bin volume\n",
      "in a histogram estimator. The univariate version of the kernel estimator is the\n",
      "same as Rosenblatt’s estimator (8.60), but in which a more general function\n",
      "κ is allowed.\n",
      "In practice, V is usually taken to be constant for a given sample size, but,\n",
      "of course, there is no reason for this to be the case, and indeed it may be\n",
      "better to vary V depending on the number of observations near the point y.\n",
      "The dependency of the smoothing matrix on the sample size n and on y is\n",
      "often indicated by the notation Vn(y).\n",
      "Properties of Kernel Estimators\n",
      "The appearance of the kernel density estimator depends to some extent on the\n",
      "support and shape of the kernel. Unlike the histogram estimator, the kernel\n",
      "density estimator may be continuous and even smooth.\n",
      "It is easy to see that if the kernel satisﬁes\n",
      "κ(t) ≥0,\n",
      "(8.62)\n",
      "and\n",
      "Z\n",
      "IRd κ(t) dt = 1\n",
      "(8.63)\n",
      "(that is, if κ is a density), then bpK(y) is a bona ﬁde density estimator.\n",
      "There are other requirements that we may impose on the kernel either for\n",
      "the theoretical properties that result or just for their intuitive appeal. It also\n",
      "seems reasonable that in estimating the density at the point y, we would want\n",
      "to emphasize the sample points near y. This could be done in various ways,\n",
      "but one simple way is to require\n",
      "Z\n",
      "IRd tκ(t) dt = 0.\n",
      "(8.64)\n",
      "In addition, we may require the kernel to be symmetric about 0.\n",
      "For multivariate density estimation, the kernels are usually chosen as a\n",
      "radially symmetric generalization of a univariate kernel. Such a kernel can be\n",
      "formed as a product of the univariate kernels. For a product kernel, we have\n",
      "for some constant σ2\n",
      "κ,\n",
      "Z\n",
      "IRd ttTκ(t) dt = σ2\n",
      "κId,\n",
      "(8.65)\n",
      "where Id is the identity matrix of order d. We could also impose this as a\n",
      "requirement on any kernel, whether it is a product kernel or not. This makes\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "592\n",
      "8 Nonparametric and Robust Inference\n",
      "the expressions for bias and variance of the estimators simpler. The spread of\n",
      "the kernel can always be controlled by the smoothing matrix V , so sometimes,\n",
      "for convenience, we require σ2\n",
      "κ = 1.\n",
      "In the following, we will assume the kernel satisﬁes the properties in equa-\n",
      "tions (8.62) through (8.65).\n",
      "The pointwise properties of the kernel estimator are relatively simple to\n",
      "determine because the estimator at a point is merely the sample mean of n\n",
      "independent and identically distributed random variables. The expectation\n",
      "of the kernel estimator (8.61) at the point y is the convolution of the kernel\n",
      "function and the probability density function,\n",
      "E (bpK(y)) =\n",
      "1\n",
      "|V |\n",
      "Z\n",
      "IRd κ \u0000V −1(y −t)\u0001 p(t) dt\n",
      "=\n",
      "Z\n",
      "IRd κ(u)p(y −V u) du,\n",
      "(8.66)\n",
      "where u = V −1(y −t) (and, hence, du = |V |−1dt).\n",
      "If we approximate p(y −V u) about y with a three-term Taylor series,\n",
      "using the properties of the kernel in equations (8.62) through (8.65) and using\n",
      "properties of the trace, we have\n",
      "E (bpK(y)) ≈\n",
      "Z\n",
      "IRd κ(u)\n",
      "\u0012\n",
      "p(y) −(V u)T∇p(y) + 1\n",
      "2(V u)THp(y)V u\n",
      "\u0013\n",
      "du\n",
      "= p(y) −0 + 1\n",
      "2trace\n",
      "\u0000V THp(y)V\n",
      "\u0001\n",
      ".\n",
      "(8.67)\n",
      "To second order in the elements of V (that is, to terms in O(|V |2)), the bias\n",
      "at the point y is therefore\n",
      "1\n",
      "2trace \u0000V V THp(y)\u0001 .\n",
      "(8.68)\n",
      "Using the same kinds of expansions and approximations as in equa-\n",
      "tions (8.66) and (8.67) to evaluate E\n",
      "\u0000(bpK(y))2\u0001\n",
      "to get an expression of order\n",
      "O(|V |/n), and subtracting the square of the expectation in equation (8.67),\n",
      "we get the approximate variance at y as\n",
      "V (bpK(y)) ≈p(y)\n",
      "n|V |\n",
      "Z\n",
      "IRd(κ(u))2 du,\n",
      "or\n",
      "V (bpK(y)) ≈p(y)\n",
      "n|V |S(κ).\n",
      "(8.69)\n",
      "Integrating this, because p is a density, we have\n",
      "AIV\n",
      "\u0000bpK\n",
      "\u0001\n",
      "= S(κ)\n",
      "n|V | ,\n",
      "(8.70)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "593\n",
      "and integrating the square of the asymptotic bias in expression (8.68), we have\n",
      "AISB\n",
      "\u0000bpK\n",
      "\u0001\n",
      "= 1\n",
      "4\n",
      "Z\n",
      "IRd\n",
      "\u0000trace\n",
      "\u0000V THp(y)V\n",
      "\u0001\u00012 dy.\n",
      "(8.71)\n",
      "These expressions are much simpler in the univariate case, where the\n",
      "smoothing matrix V is the smoothing parameter or window width h. We have\n",
      "a simpler approximation for E (bpK(y)) than that given in equation (8.67),\n",
      "E (bpK(y)) ≈p(y) + 1\n",
      "2h2p′′(y)\n",
      "Z\n",
      "IR\n",
      "u2κ(u) du,\n",
      "and from this we get a simpler expression for the AISB. After likewise simpli-\n",
      "fying the AIV, we have\n",
      "AMISE\u0000bpK\n",
      "\u0001 = S(κ)\n",
      "nh + 1\n",
      "4σ4\n",
      "κh4R(p),\n",
      "(8.72)\n",
      "where we have left the kernel unscaled (that is, R u2κ(u) du = σ2\n",
      "K).\n",
      "Minimizing this with respect to h, we have the optimal value of the smooth-\n",
      "ing parameter\n",
      "h∗=\n",
      "\u0012\n",
      "S(κ)\n",
      "nσ4κR(p)\n",
      "\u00131/5\n",
      ";\n",
      "(8.73)\n",
      "that is, the optimal bandwidth is O(n−1/5).\n",
      "Substituting the optimal bandwidth back into the expression for the\n",
      "AMISE, we ﬁnd that its optimal value in this univariate case is\n",
      "5\n",
      "4R(p)(σκS(κ))4/5 n−4/5.\n",
      "(8.74)\n",
      "The AMISE for the univariate kernel density estimator is thus in O(n−4/5).\n",
      "Recall that the AMISE for the univariate histogram density estimator is in\n",
      "O(n−2/3).\n",
      "We see that the bias and variance of kernel density estimators have similar\n",
      "relationships to the smoothing matrix that the bias and variance of histogram\n",
      "estimators have. As the determinant of the smoothing matrix gets smaller\n",
      "(that is, as the window of inﬂuence around the point at which the estimator\n",
      "is to be evaluated gets smaller), the bias becomes smaller and the variance\n",
      "becomes larger. This agrees with what we would expect intuitively.\n",
      "Kernel-Based Estimator of CDF\n",
      "We can form an estimator of the CDF based on a kernel PDF estimator pK\n",
      "by using\n",
      "K(x) =\n",
      "Z x\n",
      "−∞\n",
      "κ(t)dt.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "594\n",
      "8 Nonparametric and Robust Inference\n",
      "A kernel-based estimator of the CDF is\n",
      "bPK(y) = 1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "K\n",
      "\u0012y −yi\n",
      "hn\n",
      "\u0013\n",
      ".\n",
      "Let us consider the convergence of { bPKn} to the true CDF P . We recall from\n",
      "the Glivenko-Cantelli theorem (Theorem 1.71) that the ECDF, Pn converges\n",
      "to P uniformly; that is, for any P , given ϵ and η, there exists an N independent\n",
      "of P such that ∀n ≥N\n",
      "Pr\n",
      "\u0012\n",
      "sup\n",
      "y∈IR\n",
      "|Pn(y) −P (y)| ≥ϵ\n",
      "\u0013\n",
      "≤η.\n",
      "This does not hold for { bPKn}. To see this, pick a point y0 for which P (y0) > 0.\n",
      "Now, assume that (1) for some i, 0 < K((y0 −yi)/hn) < 1, (2) for some\n",
      "t ∈]0, P (y0)[, K−1(t) < (y0 −yi)/hn, and (3) hn > 0∀n. (If the kernel is a\n",
      "PDF and if the kernel density estimator is ﬁnite, then these conditions hold.)\n",
      "*************** Zieli´nski (2007)\n",
      "Choice of Kernels\n",
      "Standard normal densities have these properties described above, so the kernel\n",
      "is often chosen to be the standard normal density. As it turns out, the kernel\n",
      "density estimator is not very sensitive to the form of the kernel.\n",
      "Although the kernel may be from a parametric family of distributions, in\n",
      "kernel density estimation, we do not estimate those parameters; hence, the\n",
      "kernel method is a nonparametric method.\n",
      "Sometimes, a kernel with ﬁnite support is easier to work with. In the\n",
      "univariate case, a useful general form of a compact kernel is\n",
      "κ(t) = κrs(1 −|t|r)sI[−1,1](t),\n",
      "where\n",
      "κrs =\n",
      "r\n",
      "2B(1/r, s + 1),\n",
      "for r > 0, s ≥0,\n",
      "and B(a, b) is the complete beta function.\n",
      "This general form leads to several simple speciﬁc cases:\n",
      "•\n",
      "for r = 1 and s = 0, it is the rectangular kernel;\n",
      "•\n",
      "for r = 1 and s = 1, it is the triangular kernel;\n",
      "•\n",
      "for r = 2 and s = 1 (κrs = 3/4), it is the “Epanechnikov” kernel, which\n",
      "yields the optimal rate of convergence of the MISE (see Epanechnikov,\n",
      "1969);\n",
      "•\n",
      "for r = 2 and s = 2 (κrs = 15/16), it is the “biweight” kernel.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.5 Nonparametric Estimation of PDFs\n",
      "595\n",
      "If r = 2 and s →∞, we have the Gaussian kernel (with some rescaling).\n",
      "As mentioned above, for multivariate density estimation, the kernels are\n",
      "often chosen as a product of the univariate kernels. The product Epanechnikov\n",
      "kernel, for example, is\n",
      "κ(t) = d + 2\n",
      "2cd\n",
      "(1 −tTt)I(tTt≤1),\n",
      "where\n",
      "cd =\n",
      "πd/2\n",
      "Γ(d/2 + 1).\n",
      "We have seen that the AMISE of a kernel estimator (that is, the sum of\n",
      "equations (8.70) and (8.71)) depends on S(κ) and the smoothing matrix V .\n",
      "As we mentioned above, the amount of smoothing (that is, the window of\n",
      "inﬂuence) can be made to depend on σκ. We can establish an approximate\n",
      "equivalence between two kernels, κ1 and κ2, by choosing the smoothing matrix\n",
      "to oﬀset the diﬀerences in S(κ1) and S(κ2) and in σκ1 and σκ2.\n",
      "Computation of Kernel Density Estimators\n",
      "If the estimate is required at one point only, it is simplest just to compute it\n",
      "directly. If the estimate is required at several points, it is often more eﬃcient\n",
      "to compute the estimates in some regular fashion.\n",
      "If the estimate is required over a grid of points, a fast Fourier transform\n",
      "(FFT) can be used to speed up the computations.\n",
      "8.5.4 Choice of Window Widths\n",
      "An important problem in nonparametric density estimation is to determine\n",
      "the smoothing parameter, such as the bin volume, the smoothing matrix, the\n",
      "number of nearest neighbors, or other measures of locality. In kernel density\n",
      "estimation, the window width has a much greater eﬀect on the estimator than\n",
      "the kernel itself does.\n",
      "An objective is to choose the smoothing parameter that minimizes the\n",
      "MISE. We often can do this for the AMISE, as in equation (8.57) on page 588.\n",
      "It is not as easy for the MISE. The ﬁrst problem, of course, is just to estimate\n",
      "the MISE.\n",
      "In practice, we use cross validation with varying smoothing parameters\n",
      "and alternate computations between the MISE and AMISE.\n",
      "In univariate density estimation, the MISE has terms such as hαS(p′) (for\n",
      "histograms) or hαS(p′′) (for kernels). We need to estimate the roughness of a\n",
      "derivative of the density.\n",
      "Using a histogram, a reasonable estimate of the integral S(p′) is a Riemann\n",
      "approximation,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "596\n",
      "8 Nonparametric and Robust Inference\n",
      "bS(p′) = h\n",
      "X\u0000bp′(tk)\u00012\n",
      "=\n",
      "1\n",
      "n2h3\n",
      "X\n",
      "(nk+1 −nk)2,\n",
      "where bp′(tk) is the ﬁnite diﬀerence at the midpoints of the kth and (k + 1)th\n",
      "bins; that is,\n",
      "bp′(tk) = nk+1/(nh) −nk/(nh)\n",
      "h\n",
      ".\n",
      "This estimator is biased. For the histogram, for example,\n",
      "E( bS(p′)) = S(p′) + 2/(nh3) + . . .\n",
      "A standard estimation scheme is to correct for the 2/(nh3) term in the bias and\n",
      "plug this back into the formula for the AMISE (which is 1/(nh) + h2S(r′)/12\n",
      "for the histogram).\n",
      "We compute the estimated values of the AMISE for various values of h\n",
      "and choose the one that minimizes the AMISE. This is called biased cross\n",
      "validation because of the use of the AMISE rather than the MISE.\n",
      "These same techniques can be used for other density estimators and for\n",
      "multivariate estimators, although at the expense of considerably more com-\n",
      "plexity.\n",
      "8.5.5 Orthogonal Series Estimators\n",
      "A continuous real function p(x), integrable over a domain D, can be repre-\n",
      "sented over that domain as an inﬁnite series in terms of a complete spanning\n",
      "set of real orthogonal functions {fk} over D:\n",
      "p(x) =\n",
      "X\n",
      "k\n",
      "ckfk(x).\n",
      "(8.75)\n",
      "The orthogonality property allows us to determine the coeﬃcients ck in\n",
      "the expansion (8.75):\n",
      "ck = ⟨fk, p⟩.\n",
      "(8.76)\n",
      "Approximation using a truncated orthogonal series can be particularly\n",
      "useful in estimation of a probability density function because the orthogonality\n",
      "relationship provides an equivalence between the coeﬃcient and an expected\n",
      "value. Expected values can be estimated using observed values of the random\n",
      "variable and the approximation of the probability density function. Assume\n",
      "that the probability density function p is approximated by an orthogonal series\n",
      "{qk} with weight function w(y):\n",
      "p(y) =\n",
      "X\n",
      "k\n",
      "ckqk(y).\n",
      "From equation (8.76), we have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.6 Perturbations of Probability Distributions\n",
      "597\n",
      "ck = ⟨qk, p⟩\n",
      "=\n",
      "Z\n",
      "D\n",
      "qk(y)p(y)w(y)dy\n",
      "= E(qk(Y )w(Y )),\n",
      "(8.77)\n",
      "where Y is a random variable whose probability density function is p.\n",
      "The ck can therefore be unbiasedly estimated by\n",
      "bck = 1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "qk(yi)w(yi).\n",
      "The orthogonal series estimator is therefore\n",
      "bpS(y) = 1\n",
      "n\n",
      "j\n",
      "X\n",
      "k=0\n",
      "n\n",
      "X\n",
      "i=1\n",
      "qk(yi)w(yi)qk(y)\n",
      "(8.78)\n",
      "for some truncation point j.\n",
      "Without some modiﬁcations, this generally is not a good estimator of the\n",
      "probability density function. It may not be smooth, and it may have inﬁnite\n",
      "variance. The estimator may be improved by shrinking the bck toward the\n",
      "origin. The number of terms in the ﬁnite series approximation also has a\n",
      "major eﬀect on the statistical properties of the estimator. Having more terms\n",
      "is not necessarily better. One useful property of orthogonal series estimators\n",
      "is that the convergence rate is independent of the dimension. This may make\n",
      "orthogonal series methods more desirable for higher-dimensional problems.\n",
      "There are several standard orthogonal series that could be used. The two\n",
      "most commonly used series are the Fourier and the Hermite. Which is prefer-\n",
      "able depends on the situation.\n",
      "The Fourier series is commonly used for distributions with bounded sup-\n",
      "port. It yields estimators with better properties in the L1 sense.\n",
      "For distributions with unbounded support, the Hermite polynomials are\n",
      "most commonly used.\n",
      "8.6 Perturbations of Probability Distributions\n",
      "If a task in statistical inference begins with the assumption that the underlying\n",
      "family of probability distributions is P, we use statistical methods that are\n",
      "optimized for this family. If, however, the true underlying family of probability\n",
      "distributions is Pϵ, where Pϵ ̸= P, our methods of inference may not be very\n",
      "good. If Pϵ ⊆P, our methods are likely to be valid but suboptimal; if Pϵ ⊇P,\n",
      "our methods are likely to be invalid; in more general cases when Pϵ ̸= P, we\n",
      "may have no knowledge of how the method performs. Our objective is to\n",
      "identify methods that are likely to be “good” so long as Pϵ is “relatively\n",
      "close” to P.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "598\n",
      "8 Nonparametric and Robust Inference\n",
      "We often measure the diﬀerence in functions by a norm or pseudonorm\n",
      "functional (see Section 0.1.9 beginning on page 744). The measure of the\n",
      "diﬀerence is called a metric, or pseudometric (see Section 0.1.9).\n",
      "Functionals of CDFs can be used as measures of the diﬀerences between\n",
      "two distributions. Because of the deﬁnition of a CDF, the functionals we use\n",
      "are true norms and true metrics.\n",
      "In Section 7.4.3, we discussed ways of measuring the distance between two\n",
      "diﬀerent distributions for the purpose of testing goodness of ﬁt, that is, for\n",
      "testing an hypothesis that a given sample came from some speciﬁed distribu-\n",
      "tion. Functionals used to measure diﬀerences between two distributions can\n",
      "also be used to evaluate the properties of statistical methods, especially if\n",
      "those methods are deﬁned in terms of functionals.\n",
      "Distances between Probability Distributions\n",
      "In Section 0.1.9 beginning on page 747 we discuss various general measures\n",
      "of the diﬀerence between two functions. The diﬀerence in two probability dis-\n",
      "tributions may be measured in terms of a distance between the cumulative\n",
      "distribution functions such as the Hellinger distance or the Kullback-Leibler\n",
      "measure as described on page 747, or it may be measured in terms of diﬀer-\n",
      "ences in probabilities or diﬀerences in expected values.\n",
      "Because we use samples to make inferences about the distances between\n",
      "probability distributions, the measures of interest are usually taken between\n",
      "two ECDFs. If we measure the distance between probability distributions\n",
      "in terms of a distance between the cumulative distribution functions, we can\n",
      "compare the distance between the ECDFs from the samples. If the comparison\n",
      "is between a distribution of a sample and some family of distributions, we use\n",
      "the ECDF from the sample and a CDF from the family; If the comparison is\n",
      "between the distributions of two samples, we use the ECDFs from the samples.\n",
      "It is important to note that even though the measure of the diﬀerence\n",
      "between two CDFs may be small, there may be very large diﬀerences in prop-\n",
      "erties of the probability distributions. For example, consider the diﬀerence\n",
      "between the CDF of a standard Cauchy and a standard normal. The sup\n",
      "diﬀerence is about 0.1256. (It occurs near ±1.85.) The sup dif between the\n",
      "ECDFs for samples of size 20 will often be between 0.2 and 0.3. (That is a\n",
      "signiﬁcance level of between 0.83 and 0.34 on a KS test.)\n",
      "The Kolmogorov Distance; An L∞Metric\n",
      "On page 536, we deﬁned the Kolmogorov distance between two CDFs P1 and\n",
      "P2, ρK(P1, P2), as the L∞norm of the diﬀerence between the two CDFs:\n",
      "ρK(P1, P2) = sup |P1 −P2|.\n",
      "(8.79)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.6 Perturbations of Probability Distributions\n",
      "599\n",
      "The L´evy Metric\n",
      "Another measure of the distance between two CDFs is the L´evy distance,\n",
      "deﬁned for the CDFs P1 and P2 as\n",
      "ρL(P1, P2) = inf{h, s.t. ∀x, P1(x −h) −h ≤P2(x) ≤P1(x + h) + h}. (8.80)\n",
      "Notice that h is both the deviation in the argument x and the deviation\n",
      "in the function values.\n",
      "It can be shown that ρL(P1, P2) is a metric over the set of distribution\n",
      "functions. It can also be shown that for any CDFs P1 and P2,\n",
      "ρL(P1, P2) ≤ρK(P1, P2).\n",
      "(8.81)\n",
      "Example 8.1 Kolmogorov and L´evy distances between distribu-\n",
      "tions\n",
      "Consider the U(0, 1) distribution and the Bernoulli distribution with param-\n",
      "eter π = 0.3, with CDFs P1 and P2 respectively. Figure 8.2 shows the CDFs\n",
      "and the Kolmogorov and L´evy distances between them.\n",
      "We see that if h were any smaller and x = 1, then P1(x −h) −h would be\n",
      "greater than P2(x). On the other hand, we see that this value of h will satisfy\n",
      "the inequalities in the deﬁnition of the L´evy distance at any point x.\n",
      "The Wasserstein-Mallows Metric\n",
      "Another useful measure of the distance between two CDFs is the Mallows\n",
      "distance or the Wasserstein-Mallows distance. This metric is also called by\n",
      "various other names, including the Renyi metric, and the “earth movers’ dis-\n",
      "tance”. We will brieﬂy describe this metric, but we will rarely use it in the\n",
      "following.\n",
      "For the CDFs P1 and P2, with random variables X1 having CDF P1 and\n",
      "X2 having CDF P2, if E(∥X1∥p) and E(∥X2∥p) are ﬁnite, this distance is\n",
      "ρMp(P1, P2) = inf(E(∥X1 −X2∥p))1/p,\n",
      "where the inﬁmum is taken over all joint distributions P (x1, x2) with marginals\n",
      "P1 and P2.\n",
      "If P1 and P2 are univariate ECDFs based on the same number of observa-\n",
      "tions, we have\n",
      "ρMp(Pn1, Pn2) =\n",
      "\u0012 1\n",
      "n\n",
      "X\n",
      "(|x1(i) −x2(i)|p)\n",
      "\u00131/p\n",
      ".\n",
      "For a scalar-valued random variable X with CDF P , if U ∼U(0, 1) then\n",
      "P −1(U) d= X (Corollary 1.7.1); that is,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "600\n",
      "8 Nonparametric and Robust Inference\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "1.2\n",
      "1.4\n",
      "x\n",
      "CDF\n",
      "[\n",
      "[\n",
      "P 2\n",
      "P 1\n",
      "ρK=.7\n",
      "x−h\n",
      "P 1(x−h)\n",
      "P 1(x−h)−h\n",
      "x+h\n",
      "P 1(x+h)+h\n",
      "P 1(x+h)\n",
      "ρL=.35\n",
      "Bernoulli\n",
      "Uniform\n",
      "Both\n",
      "Figure 8.2. Two CDFs and the Kolmogorov and L´evy Distances between Them\n",
      "ρMp(P1, P2) = (E(∥P −1\n",
      "1\n",
      "(U) −P −1\n",
      "2\n",
      "(U)∥p))1/p,\n",
      "The ﬁrst question we might consider given the deﬁnition of the Wasserstein-\n",
      "Mallows metric is whether the inﬁmum exists, and then it is not clear whether\n",
      "this is indeed a metric. (The triangle inequality is the only hard question.)\n",
      "Bickel and Freedman (1981) answered both of these questions in the aﬃrma-\n",
      "tive. The proof is rather complicated for vector-valued random variables; for\n",
      "scalar-valued random variables, there is a simpler proof in terms of the inverse\n",
      "CDF.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.6 Perturbations of Probability Distributions\n",
      "601\n",
      "A Useful Class of Perturbations\n",
      "In statistical applications using functionals deﬁned on the CDF, we are in-\n",
      "terested in how the functional varies for “nearby” CDFs in the distribution\n",
      "function space.\n",
      "A simple kind of perturbation of a given distribution is a mixture distribu-\n",
      "tion with the given distribution as one of the components of the mixture. We\n",
      "often consider a simple type of function in the neighborhood of the CDF. This\n",
      "kind of CDF results from adding a single mass point to the given distribution.\n",
      "For a given CDF P (x), we can deﬁne a simple perturbation as\n",
      "Pxc,ϵ(x) = (1 −ϵ)P (x) + ϵI[xc,∞[(x),\n",
      "(8.82)\n",
      "where 0 ≤ϵ ≤1. This is an ϵ-mixture family of distributions that we discussed\n",
      "on page 194. We will refer to the distribution with CDF P as the reference\n",
      "distribution. (The reference distribution is the distribution of interest, so I\n",
      "often refer to it without any qualiﬁcation.)\n",
      "A simple interpretation of the perturbation in equation (8.82) is that it\n",
      "is the CDF of a mixture of a distribution with CDF P and a degenerate\n",
      "distribution with a single mass point at xc, which may or may not be in the\n",
      "support of the distribution. The extent of the perturbation depends on ϵ; if\n",
      "ϵ = 0, the distribution is the reference distribution.\n",
      "If the distribution with CDF P is continuous with PDF p, the PDF of the\n",
      "mixture is\n",
      "dPxc,ϵ(x)/dx = (1 −ϵ)p(x) + ϵδ(xc −x),\n",
      "where δ(·) is the Dirac delta function. If the distribution is discrete, the prob-\n",
      "ability mass function has nonzero probabilities (scaled by (1 −ϵ)) at each\n",
      "of the mass points associated with P together with a mass point at xc with\n",
      "probability ϵ.\n",
      "The left-hand graph in Figure 8.3 shows the Lebesgue PDF of a continuous\n",
      "reference distribution (dotted line) and the PDF of an ϵ-mixture distribution\n",
      "(solid line together with the mass point at xc). Over part of the support\n",
      "the PDF of the mixture is a Lebesgue PDF and over another part of the\n",
      "support (the single point) it is a probability mass function. The right-hand\n",
      "graph shows the corresponding CDFs. The reference distribution is a standard\n",
      "normal, xc = 1, and ϵ = 0.3. (Such a large value of ϵ was used so that the\n",
      "graphs would look better. In most applications when an ϵ-mixture distribution\n",
      "is assumed, the value of ϵ is much smaller, often of the order of .05.)\n",
      "We will often analyze the sensitivity of statistical methods with respect to\n",
      "the perturbation of a reference distribution by xc and ϵ.\n",
      "Example 8.2 Kolmogorov and L´evy distances between standard\n",
      "normal and associated ϵ-mixture distribution\n",
      "Consider the N(0, 1) distribution and the associated ϵ-mixture distribution\n",
      "with xc = 1 and ϵ = 0.1. Figure 8.4 shows the CDFs and the Kolmogorov and\n",
      "L´evy distances between them.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "602\n",
      "8 Nonparametric and Robust Inference\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "x\n",
      "PDF\n",
      "ε\n",
      "x c\n",
      "Normal\n",
      "Mixture\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "x\n",
      "CDF\n",
      "[\n",
      "[\n",
      "ε\n",
      "x c\n",
      "Normal\n",
      "Mixture\n",
      "Figure 8.3. PDFs and the CDF of the ϵ-Mixture Distribution\n",
      "The Kolmogorov distance is slightly less than ϵ. The L´evy distance is the\n",
      "length of a side of the square shown. (The square does not appear to be a\n",
      "square because the scales of the axes are diﬀerent.)\n",
      "Although by both measures, the distributions are quite “close” to each\n",
      "other, and increasing the value of xc would not make these measures get larger,\n",
      "the eﬀect on statistical inference about, say, the mean of the distribution could\n",
      "be quite large.\n",
      "8.7 Robust Inference\n",
      "Robust inference is concerned with methods that are not greatly aﬀected by\n",
      "perturbations in the assumed family of distributions.\n",
      "Functionals of the CDF and Estimators Based on Statistical\n",
      "Functions\n",
      "While the cumulative distribution function is the most basic function for de-\n",
      "scribing a probability distribution or a family of distributions, there are a\n",
      "number of other, simpler descriptors of probability distributions that are use-\n",
      "ful. Many of these are expressed as functionals of the CDF. For example, the\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.7 Robust Inference\n",
      "603\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "x\n",
      "CDF\n",
      "ρK\n",
      "ρL\n",
      "xc\n",
      "Normal\n",
      "Mixture\n",
      "Figure 8.4. CDFs of a Standard Normal and Associated ϵ-Mixture Distribution\n",
      "and the Kolmogorov and L´evy Distances between Them\n",
      "mean of a distribution, if it exists, may be written as the functional M of the\n",
      "CDF P :\n",
      "M(P ) =\n",
      "Z\n",
      "y dP (y).\n",
      "(8.83)\n",
      "A natural way of estimating a distributional measure that is deﬁned in\n",
      "terms of a statistical function of the CDF is to use the same statistical func-\n",
      "tion on the ECDF. This leads us to plug-in estimators, as we discussed in\n",
      "Section 3.2.2 beginning on page 246.\n",
      "Estimators based on statistical functions play major roles throughout non-\n",
      "parametric and semiparametric inference. They are also important in robust\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "604\n",
      "8 Nonparametric and Robust Inference\n",
      "statistics. In robustness studies, we ﬁrst consider the sensitivity of the statis-\n",
      "tical function to perturbations in distribution functions. Statistical functions\n",
      "that are relatively insensitive to perturbations in distribution functions when\n",
      "applied to a ECDF should yield robust estimators.\n",
      "These kinds of plug-in estimators should generally have good asymptotic\n",
      "properties relative to the corresponding population measures because of the\n",
      "global asymptotic properties of the ECDF.\n",
      "Although the statistical functions we have considered have intuitive inter-\n",
      "pretations, the question remains as to what are the most useful distributional\n",
      "measures by which to describe a given distribution. In a simple case such as\n",
      "a normal distribution, the choices are obvious. For skewed distributions, or\n",
      "distributions that arise from mixtures of simpler distributions, the choices of\n",
      "useful distributional measures are not so obvious. A central concern in robust\n",
      "statistics is how a functional of a CDF behaves as the distribution is per-\n",
      "turbed. If a functional is rather sensitive to small changes in the distribution,\n",
      "then one has more to worry about if the observations from the process of\n",
      "interest are contaminated with observations from some other process.\n",
      "8.7.1 Sensitivity of Statistical Functions\n",
      "******\n",
      "One of the most interesting things about a function (or a functional) is\n",
      "how its value varies as the argument is perturbed. Two key properties are\n",
      "continuity and diﬀerentiability.\n",
      "For the case in which the arguments are functions, the cardinality of the\n",
      "possible perturbations is greater than that of the continuum. We can be precise\n",
      "in discussions of continuity and diﬀerentiability of a functional Υ at a point\n",
      "(function) F in a domain F by deﬁning another set D consisting of diﬀerence\n",
      "functions over F; that is the set the functions D = F1 −F2 for F1, F2 ∈F.\n",
      "Three kinds of functional diﬀerentials are deﬁned on page 760.\n",
      "Given a reference distribution P and an ϵ-mixture distribution Px,ϵ, a\n",
      "statistical function evaluated at Px,ϵ compared to the function evaluated at P\n",
      "allows us to determine the eﬀect of the perturbation on the statistical function.\n",
      "For example, we can determine the mean of the distribution with CDF Px,ϵ\n",
      "in terms of the mean µ of the reference distribution to be (1 −ϵ)µ + ϵx. This\n",
      "is easily seen by thinking of the distribution as a mixture. Formally, using the\n",
      "M in equation (8.83), we can write\n",
      "M(Px,ϵ) =\n",
      "Z\n",
      "y d((1 −ϵ)P (y) + ϵI[x,∞[(y))\n",
      "= (1 −ϵ)\n",
      "Z\n",
      "y dP (y) + ϵ\n",
      "Z\n",
      "yδ(y −x) dy\n",
      "= (1 −ϵ)µ + ϵx.\n",
      "(8.84)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.7 Robust Inference\n",
      "605\n",
      "For a discrete distribution we would follow the same steps using summations\n",
      "(instead of an integral of y times a Dirac delta function, we just have a point\n",
      "mass of 1 at x), and would get the same result.\n",
      "The π quantile of the mixture distribution, Ξπ(Px,ϵ) = P −1\n",
      "x,ϵ (π), is some-\n",
      "what more diﬃcult to work out. This quantile, which we will call q, is shown\n",
      "relative to the π quantile of the continuous reference distribution, yπ, for two\n",
      "cases in Figure 8.5. (In Figure 8.5, although the speciﬁcs are not important,\n",
      "the reference distribution is a standard normal, π = 0.7, so yπ = 0.52, and\n",
      "ϵ = 0.1. In the left-hand graph, x1 = −1.25, and in the right-hand graph,\n",
      "x2 = 1.25.)\n",
      "p(y)\n",
      "(1-ε)p(y)\n",
      "yπ\n",
      "ε\n",
      "x1\n",
      "q\n",
      "p(y)\n",
      "(1-ε)p(y)\n",
      "yπ\n",
      "ε\n",
      "x2\n",
      "q\n",
      "Figure 8.5. Quantile of the ϵ-Mixture Distribution\n",
      "We see that in the case of a continuous reference distribution (implying P\n",
      "is strictly increasing),\n",
      "P −1\n",
      "x,ϵ (π) =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P −1 \u0010\n",
      "π−ϵ\n",
      "1−ϵ\n",
      "\u0011\n",
      ", for (1 −ϵ)P (x) + ϵ < π,\n",
      "x,\n",
      "for (1 −ϵ)P (x) ≤π ≤(1 −ϵ)P (x) + ϵ,\n",
      "P −1 \u0010\n",
      "π\n",
      "1−ϵ\n",
      "\u0011\n",
      ", for π < (1 −ϵ)P (x).\n",
      "(8.85)\n",
      "The conditions in equation (8.85) can also be expressed in terms of x and quan-\n",
      "tiles of the reference distribution. For example, the ﬁrst condition is equivalent\n",
      "to x < yπ−ϵ\n",
      "1−ϵ .\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "606\n",
      "8 Nonparametric and Robust Inference\n",
      "The Inﬂuence Function\n",
      "The extent of the perturbation depends on ϵ, and so we are interested in the\n",
      "relative eﬀect; in particular, the relative eﬀect as ϵ approaches zero.\n",
      "The inﬂuence function for the functional Υ and the CDF P , deﬁned at x\n",
      "as\n",
      "φΥ,P(x) = lim\n",
      "ϵ↓0\n",
      "Υ(Px,ϵ) −Υ(P )\n",
      "ϵ\n",
      "(8.86)\n",
      "if the limit exists, is a measure of the sensitivity of the distributional measure\n",
      "deﬁned by Υ to a perturbation of the distribution at the point x. The inﬂuence\n",
      "function is also called the inﬂuence curve, and denoted by IC.\n",
      "The limit in equation (8.86) is the right-hand Gˆateaux derivative of the\n",
      "functional Υ at P and x.\n",
      "The inﬂuence function can also be expressed as the limit of the derivative\n",
      "of Υ(Px,ϵ) with respect to ϵ:\n",
      "φΥ,P(x) = lim\n",
      "ϵ↓0\n",
      "∂\n",
      "∂ϵΥ(Px,ϵ).\n",
      "(8.87)\n",
      "This form is often more convenient for evaluating the inﬂuence function.\n",
      "Some inﬂuence functions are easy to work out, for example, the inﬂuence\n",
      "function for the functional M in equation (8.83) that deﬁnes the mean of a\n",
      "distribution, which we denote by µ. The inﬂuence function for this functional\n",
      "operating on the CDF P at x is\n",
      "φµ,P (x) = lim\n",
      "ϵ↓0\n",
      "M(Px,ϵ) −M(P )\n",
      "ϵ\n",
      "= lim\n",
      "ϵ↓0\n",
      "(1 −ϵ)µ + ϵx −µ\n",
      "ϵ\n",
      "= x −µ.\n",
      "(8.88)\n",
      "We note that the inﬂuence function of a functional is a type of derivative of\n",
      "the functional, ∂M(Px,ϵ)/∂ϵ. The inﬂuence function for other moments can\n",
      "be computed in the same way as the steps in equation (8.88).\n",
      "Note that the inﬂuence function for the mean is unbounded in x; that is,\n",
      "it increases or decreases without bound as x increases or decreases without\n",
      "bound. Note also that this result is the same for multivariate or univariate\n",
      "distributions.\n",
      "The inﬂuence function for a quantile is more diﬃcult to work out. The\n",
      "problem arises from the diﬃculty in evaluating the quantile. As I informally\n",
      "described the distribution with CDF Px,ϵ, it is a mixture of some given dis-\n",
      "tribution and a degenerate discrete distribution. Even if the reference distri-\n",
      "bution is continuous, the CDF of the mixture, Px,ϵ, does not have an inverse\n",
      "over the full support (although for quantiles we will write P −1\n",
      "x,ϵ ).\n",
      "Let us consider a simple instance: a univariate continuous reference distri-\n",
      "bution, and assume p(yπ) > 0. We approach the problem by considering the\n",
      "PDF, or the probability mass function.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "8.7 Robust Inference\n",
      "607\n",
      "In the left-hand graph of Figure 8.5, the total probability mass up to the\n",
      "point yπ is (1 −ϵ) times the area under the curve, that is, (1 −ϵ)π, plus\n",
      "the mass at x1, that is, ϵ. Assuming ϵ is small enough, the π quantile of the\n",
      "ϵ-mixture distribution is the π −ϵ quantile of the reference distribution, or\n",
      "P −1(π −ϵ). It is also the π quantile of the scaled reference distribution; that\n",
      "is, it is the value of the function (1−ϵ)p(x) that corresponds to the proportion\n",
      "π of the total probability (1 −ϵ) of that component. Use of equation (8.85)\n",
      "directly in equation (8.86) is somewhat messy. It is more straightforward to\n",
      "diﬀerentiate P −1\n",
      "x1,ϵ and take the limit as in equation (8.87). For ﬁxed x < yπ,\n",
      "we have\n",
      "∂\n",
      "∂ϵP −1\n",
      "\u0012π −ϵ\n",
      "1 −ϵ\n",
      "\u0013\n",
      "=\n",
      "1\n",
      "p\n",
      "\u0010\n",
      "P −1\n",
      "\u0010\n",
      "π−ϵ\n",
      "1−ϵ\n",
      "\u0011\u0011 (π −1)(1 −ϵ)\n",
      "(1 −ϵ)2\n",
      ".\n",
      "Likewise, we take the derivatives for the other cases in equation (8.85), and\n",
      "then take limits. We get\n",
      "φΞπ,P(x) =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "π −1\n",
      "p(yπ), for x < yπ,\n",
      "0,\n",
      "for x = yπ,\n",
      "π\n",
      "p(yπ), for x > yπ.\n",
      "(8.89)\n",
      "Notice that the actual value of x is not in the inﬂuence function; only whether\n",
      "x is less than, equal to, or greater than the quantile. Notice also that, un-\n",
      "like inﬂuence function for the mean, the inﬂuence function for a quantile is\n",
      "bounded; hence, a quantile is less sensitive than the mean to perturbations\n",
      "of the distribution. Likewise, quantile-based measures of scale and skewness,\n",
      "as in equations (1.115) and (1.116), are less sensitive than the moment-based\n",
      "measures to perturbations of the distribution.\n",
      "The functionals LJ and Mρ deﬁned in equations (1.118) and (1.119), de-\n",
      "pending on J or ρ, can also be very insensitive to perturbations of the distri-\n",
      "bution.\n",
      "The mean and variance of the inﬂuence function at a random point are of\n",
      "interest; in particular, we may wish to restrict the functional so that\n",
      "E(φΥ,P(X)) = 0\n",
      "and\n",
      "E\n",
      "\u0000(φΥ,P(X))2\u0001\n",
      "< ∞.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "608\n",
      "8 Nonparametric and Robust Inference\n",
      "8.7.2 Robust Estimators\n",
      "If a distributional measure of interest is deﬁned on the CDF as Υ(P ), we\n",
      "are interested in the performance of the plug-in estimator Υ(Pn); speciﬁcally,\n",
      "we are interested in Υ(Pn) −Υ(P ). This turns out to depend crucially on\n",
      "the diﬀerentiability of Υ. If we assume Gˆateaux diﬀerentiability, from equa-\n",
      "tion (0.1.118), we can write\n",
      "√n (Υ(Pn) −Υ(P )) = ΛP (√n(Pn −P )) + Rn\n",
      "=\n",
      "1\n",
      "√n\n",
      "X\n",
      "i\n",
      "φΥ,P(Yi) + Rn\n",
      "where the remainder Rn →0.\n",
      "We are interested in the stochastic convergence. First, we assume\n",
      "E(φΥ,P(X)) = 0\n",
      "and\n",
      "E\n",
      "\u0000(φΥ,P(X))2\u0001\n",
      "< ∞.\n",
      "Then the question is the stochastic convergence of Rn. Gˆateaux diﬀerentiabil-\n",
      "ity does not guarantee that Rn converges fast enough. However, ρ-Hadamard\n",
      "diﬀerentiability, does imply that that Rn is in oP (1), because it implies that\n",
      "norms of functionals (with or without random arguments) go to 0. We can\n",
      "also get that Rn is in oP(1) by assuming Υ is ρ-Fr´echet diﬀerentiable and that\n",
      "√nρ(Pn, P ) is in OP (1). In either case, that is, given the moment properties\n",
      "of φΥ,P(X) and Rn is in oP (1), we have by Slutsky’s theorem (page 91),\n",
      "√n (Υ(Pn) −Υ(P )) d→N(0, σ2\n",
      "Υ,P),\n",
      "where σ2\n",
      "Υ,P = E\n",
      "\u0000(φΥ,P(X))2\u0001\n",
      ".\n",
      "For a given plug-in estimator based on the statistical function Υ, knowing\n",
      "E \u0000(φΥ,P(X))2\u0001 (and assuming E(φΥ,P(X)) = 0) provides us an estimator of\n",
      "the asymptotic variance of the estimator.\n",
      "The inﬂuence function is also very important in leading us to estimators\n",
      "that are robust; that is, to estimators that are relatively insensitive to depar-\n",
      "tures from the underlying assumptions about the distribution. As mentioned\n",
      "above, the functionals LJ and Mρ, depending on J or ρ, can be very insensi-\n",
      "tive to perturbations of the distribution; therefore estimators based on them,\n",
      "called L-estimators and M-estimators, can be robust.\n",
      "M-Estimators\n",
      "L-Estimators\n",
      "A class of L-estimators that are particularly useful are linear combinations of\n",
      "the order statistics. Because of the suﬃciency and completeness of the order\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Notes and Further Reading\n",
      "609\n",
      "statistics in many cases of interest, such estimators can be expected to exhibit\n",
      "good statistical properties.\n",
      "Another class of estimators similar to the L-estimators are those based on\n",
      "ranks, which are simpler than order statistics. These are not suﬃcient – the\n",
      "data values have been converted to their ranks – nevertheless they preserve a\n",
      "lot of the information. The fact that they lose some information can actually\n",
      "work in their favor; they can be robust to extreme values of the data.\n",
      "A functional to deﬁne even a simple linear combination of ranks is rather\n",
      "complicated. As with the LJ functional, we begin with a function J, which\n",
      "in this case we require to be strictly increasing, and also, in order to ensure\n",
      "uniqueness, we require that the CDF P be strictly increasing. The RJ func-\n",
      "tional is deﬁned as the solution to the equation\n",
      "Z\n",
      "J\n",
      "\u0012P (y) + 1 −P (2RJ(P ) −y)\n",
      "2\n",
      "\u0013\n",
      "dP (y) = 0.\n",
      "(8.90)\n",
      "A functional deﬁned as the solution to this optimization problem is called an\n",
      "RJ functional, and an estimator based on applying it to a ECDF is called an\n",
      "RJ estimator or just an R-estimator.\n",
      "Notes and Further Reading\n",
      "Much of the material in this chapter is covered in MS2 Sections 5.1, 5.2, 5.3.\n",
      "Nonparametric Statistics\n",
      "There are a number of books on nonparametric methods of statistical infer-\n",
      "ence. Most of the underlying theory for these methods is developed in the\n",
      "context of order statistics and ranks.\n",
      "Failure Time Data and Survival Analysis\n",
      "The analysis of failure time data has application in engineering reliability\n",
      "studies as well as in medical statistics. Many of the models used are parametric\n",
      "or semiparametric, but my brief discussion of it is included in this chapter on\n",
      "nonparametric methods. An indepth discussion of the theory and methods is\n",
      "given by Kalbﬂeisch and Prentice (2002).\n",
      "Expansions of Functionals and Their Sensitivity to Perturbations\n",
      "Small (2010)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "610\n",
      "8 Nonparametric and Robust Inference\n",
      "Robust Statistics\n",
      "The books by Staudte and Sheather (1990) and Huber and Ronchetti (2009)\n",
      "and the article by Davies and Gather (2012) provide a more complete coverage\n",
      "of the general topic of robust statistics.\n",
      "The Inﬂuence Function\n",
      "Davies and Gather (2012) discuss and give several examples of this kind of\n",
      "perturbation to study the sensitivity of a functional to perturbations of the\n",
      "CDF at a given point x.\n",
      "Adaptive Procedures\n",
      "Although the idea of adapting the statistical methods to the apparent charac-\n",
      "teristics of the data is appealing, there are many practical problems to contend\n",
      "with. Hogg (1974) and Hogg and Lenth (1984) review many of the issues and\n",
      "discuss several adaptive procedures for statistical inference.\n",
      "Exercises\n",
      "8.1. Consider the problem of estimating the function f(x) = θ−1e−x/θIIR+(x)\n",
      "based on a random sample of size n from a population with PDF f(x).\n",
      "Let bf be an estimator of f that is the given functional form of f with the\n",
      "sample mean in place of λ.\n",
      "a) What is the bias and the variance of bf at the point x?\n",
      "b) What is the asymptotic mean squared error, AMSE, of bf at the point\n",
      "x?\n",
      "8.2. Integrated measures in a parametric problem.\n",
      "Consider the U(0, θ) distribution, with θ unknown. The true probability\n",
      "density is p(x) = 1/θ over (0, θ) and 0 elsewhere. Suppose we have a sample\n",
      "of size n and we estimate the density as bp(x) = 1/x(n) over (0, x(n)) and\n",
      "0 elsewhere, where x(n) is the maximum order statistic.\n",
      "a) Determine the integrated squared error, ISE, of bp(x).\n",
      "b) Determine (that is, write an explicit expression for) the integrated\n",
      "squared bias, ISB, of bp(x).\n",
      "c) Determine the mean integrated squared error, MISE, of bp(x).\n",
      "d) Determine the asymptotic mean integrated squared error, AMISE, of\n",
      "bp(x).\n",
      "8.3. Determine the hazard function for\n",
      "(a) the Weibull(α, β) family;\n",
      "(b) the log-normal(µ, σ2) family;\n",
      "(c) the gamma(α, β) family.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Exercises\n",
      "611\n",
      "In each case, suggest a reparametrization that directly incorporates the\n",
      "hazard function, as in the example in the text when the θ parameter of\n",
      "the exponential family is replaced by 1/λ.\n",
      "8.4. Show that equations (8.30) and (8.32) are correct.\n",
      "8.5. Prove Theorem 8.1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0\n",
      "Statistical Mathematics\n",
      "Statistics is grounded in mathematics. Most of mathematics is important and\n",
      "it is diﬃcult to identify the particular areas of mathematics, and at what levels,\n",
      "that must be mastered by statisticians. Of course, statistics is a large ﬁeld,\n",
      "and statisticians working in diﬀerent areas need diﬀerent kinds and diﬀerent\n",
      "levels of mathematical competence. One of the best general references for\n",
      "mathematical topics is PlanetMath, http://planetmath.org/ (This is an\n",
      "outgrowth of MathWorld by Eric Weisstein.)\n",
      "The purpose of this chapter is to provide some general mathematical back-\n",
      "ground for the theory of probability and statistics. In Section 0.0 I start with\n",
      "some very basic material. This includes standard objects such as sets and var-\n",
      "ious structures built onto sets. There are many standard methods we use in\n",
      "mathematical statistics. It may seem that many methods are ad hoc, but it\n",
      "is useful to identify common techniques and have a ready tool kit of methods\n",
      "with general applicability. There are many standard mathematical techniques\n",
      "that every statistician should have in a toolkit, and this section surveys several\n",
      "of them.\n",
      "Beyond the general basics covered in Section 0.0, the statistician needs\n",
      "grounding in measure theory to the extent covered in Section 0.1, beginning on\n",
      "page 692, in stochastic calculus to the extent covered in Section 0.2, beginning\n",
      "on page 765, in linear algebra to the extent covered in Section 0.3, beginning on\n",
      "page 781, and in methods of optimization to the extent covered in Section 0.4\n",
      "beginning on page 822.\n",
      "The general development in this chapter is in the usual form of a mathe-\n",
      "matical development, moving from primitives to deﬁnitions to theorems, how-\n",
      "ever, occasionally it is assumed that the reader already is somewhat familiar\n",
      "with such concepts as diﬀerentiation, integration, and mathematical expec-\n",
      "tation before we give their formal deﬁnitions. References within the chapter,\n",
      "therefore, may be either forward or backward. Although generally this chap-\n",
      "ter is meant to provide background for the other chapters, occasionally some\n",
      "material in this chapter depends on concepts from other chapters; in particu-\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "614\n",
      "0 Statistical Mathematics\n",
      "lar, Section 0.2 on stochastic processes depends on material in Chapter 1 on\n",
      "probability.\n",
      "The attitudes and methods of mathematics pervade mathematical statis-\n",
      "tics. We study objects. These objects may be structures, such as groups and\n",
      "ﬁelds, or functionals, such as integrals, estimators, or tests. We want to under-\n",
      "stand the properties of these objects. We identify, describe, and name these\n",
      "properties in ﬁxed statements, with labels, such as the “Neyman-Pearson\n",
      "Lemma”, or the “Dominated Convergence Theorem”. We identify limits to\n",
      "the properties of an object or boundary points on the characteristics of the\n",
      "object by means of “counterexamples”.\n",
      "Our understanding and appreciation of a particular object is enhanced by\n",
      "comparing the properties of the given object with similar objects. The prop-\n",
      "erties of objects of the same class as the given object are stated in theorems\n",
      "(or “lemmas”, or “corollaries”, or “propositions” — unless you understand\n",
      "the diﬀerence, just call them all “theorems”; clearly, many otherwise compe-\n",
      "tent mathematical statisticians have no idea what these English words mean).\n",
      "The hypotheses of the theorems deﬁne various classes of objects to which the\n",
      "conclusions of the theorems apply. Objects that do not satisfy all of the hy-\n",
      "potheses of a given theorem provide us insight into these hypotheses. These\n",
      "kinds of objects are called counterexamples for the conclusions of the theorem.\n",
      "For example, the Lebesgue integral and the Riemann integral are similar ob-\n",
      "jects. How are they diﬀerent? First, we should look at the big picture: in the\n",
      "Lebesgue integral, we begin with a partitioning of the range of the function;\n",
      "in the Riemann integral, we begin with a partitioning of the domain of the\n",
      "function. What about some speciﬁc properties? Some important properties\n",
      "of the Lebesgue integral are codiﬁed in the Big Four Theorems: the bounded\n",
      "convergence theorem, Fatou’s lemma, the (Lebesgue) monotone convergence\n",
      "theorem, and the dominated convergence theorem. None of these hold for the\n",
      "Riemann integral; that is, the Riemann integral provides counterexamples for\n",
      "the conclusions of these theorems. To understand these two types of inte-\n",
      "grals, we need to be able to prove the four theorems (they’re related) for the\n",
      "Lebesgue integral, and to construct counterexamples to show that they do not\n",
      "hold for the Riemann integral. The speciﬁcs here are not as important as the\n",
      "understanding of the attitude of mathematics.\n",
      "Notation\n",
      "I must ﬁrst of all point out a departure from the usual notation and terminol-\n",
      "ogy in regard to the real numbers. I use IR to denote the scalar real number\n",
      "system in which the elements of the underlying set are singleton numbers.\n",
      "Much of the underlying theory is based on IR, but my main interest is usually\n",
      "IRd, for some ﬁxed positive integer d. The elements of the underlying set for\n",
      "IRd are d-tuples, or vectors. I sometimes emphasize the diﬀerence by the word\n",
      "“scalar” or “vector”. I do not, however, distinguish in the notation for these\n",
      "elements from the notation for the singleton elements of IR; thus, the symbol\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0 Statistical Mathematics\n",
      "615\n",
      "x may represent a scalar or a vector, and a “random variable” X may be a\n",
      "scalar random variable or a vector random variable.\n",
      "This uniﬁed approach requires a generalized interpretation for certain func-\n",
      "tions and relational operators. Many of these functions and operators are in-\n",
      "terpreted as applying to the individual elements of a vector; for example, |x|,\n",
      "|x|p, ex, and x < y. If x = (x1, . . ., xd) and y = (y1, . . ., yd), then\n",
      "|x| def\n",
      "= (|x1|, . . ., |xd|)\n",
      "(0.1)\n",
      "|x|p def\n",
      "= (|x1|p, . . ., |xd|p)\n",
      "(0.2)\n",
      "ex def\n",
      "= (ex1, . . ., exd)\n",
      "(0.3)\n",
      "and\n",
      "x < y\n",
      "⇐⇒\n",
      "x1 < y1, . . ., xd < yd,\n",
      "(0.4)\n",
      "that is, these functions and relations are applied elementwise. For more com-\n",
      "plicated objects, such as matrices, the indicated operations may have diﬀerent\n",
      "meanings.\n",
      "There are, of course, other functions and operators that apply to combi-\n",
      "nations of the elements of a vector; for example,\n",
      "∥x∥p\n",
      "def\n",
      "=\n",
      " d\n",
      "X\n",
      "i=1\n",
      "|xi|p\n",
      "!1/p\n",
      ".\n",
      "(0.5)\n",
      "This approach, however, results in some awkwardness in some contexts,\n",
      "such as for multiplication. If a and b are scalars, ab has a very simple mean-\n",
      "ing. Likewise, if a is a scalar and x is a vector, ax has a very simple meaning\n",
      "consistent with the elementwise operations deﬁned above. Deﬁnition of mul-\n",
      "tiplication of two vectors, however, is somewhat more complicated. First of\n",
      "all, the two operands must be vectors of the same length. There are three\n",
      "possibilities: the product of vectors x and y may be a scalar, a vector, or a\n",
      "matrix.\n",
      "The scalar product is an inner product, also called the dot product, and\n",
      "is denoted as ⟨x, y⟩or xTy. It is the sum of the elementwise products.\n",
      "The vector product is the vector of elementwise products. (If we have the\n",
      "luxury of working only in 3-space, there is another useful vector product, often\n",
      "called the cross product, useful in physics.) We will rarely have occasion to\n",
      "use vector products of either type.\n",
      "The matrix product is the outer product, denoted as xyT, and deﬁned as\n",
      "the matrix whose (i, j) element is xiyj.\n",
      "In the following, and in all of my writing, I try to be very consistent in\n",
      "use of notation. Occasionally, I will mention alternative notation in common\n",
      "usage. A more complete coverage of the notation I use can be found beginning\n",
      "on page 857.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "616\n",
      "0 Statistical Mathematics\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "We ﬁrst need to develop some basic deﬁnitions and properties of sets. Given\n",
      "simple operations on sets, we expand the concepts to include various functions\n",
      "and structures over sets. The most important set and the one for which we\n",
      "deﬁne many functions is the set of reals, which I denote as IR or IRd. First,\n",
      "however, we begin with abstract sets, that is, sets whose elements are not\n",
      "restricted to lie in special mathematical structures.\n",
      "In this section, I will often refer to objects or operations that are not\n",
      "deﬁned in this section, for example derivatives and integrals and the associated\n",
      "operations. These will be deﬁned and discussed rather carefully in Section 0.1.\n",
      "In addition to understanding standard objects and their properties, we\n",
      "need to become familiar with the methods of mathematical statistics. There\n",
      "are many standard methods we use in mathematical statistics. It may seem\n",
      "that many methods are ad hoc, but it is useful to identify common techniques\n",
      "and have a ready tool kit of methods with general applicability. Monte Carlo\n",
      "methods play important roles in statistics, both for inference in applications\n",
      "and in the development of statistical theory. We discuss Monte Carlo meth-\n",
      "ods in Section 0.0.7. Finally, in Section 0.0.9, I describe some examples that\n",
      "have general relevance and some standard mathematical techniques that every\n",
      "statistician should have in a toolkit.\n",
      "0.0.1 Sets\n",
      "We use the term set without a formal deﬁnition to denote a collection of\n",
      "things, called elements or points. If every element of a set A2 is also in a set\n",
      "A1, we say A2 is a subset of A1, and write A2 ⊆A1. If A2 is a subset of A1,\n",
      "but A1 is not a subset of A2, we say A2 is a proper subset of A1and write\n",
      "A2 ⊂A1.\n",
      "Given sets A1 and A2, their union, written A1 ∪A2, is the set consisting\n",
      "of all elements that are in A1 or A2; and their intersection, written A1 ∩A2,\n",
      "is the set consisting of all elements that are in both A1 and A2. Obviously,\n",
      "both union and intersection operations are commutative: A1 ∪A2 = A2 ∪A1\n",
      "and A1 ∩A2 = A2 ∩A1.\n",
      "In working with sets, it is useful to deﬁne an empty set. This is the set\n",
      "that contains no elements. We often denote it as ∅.\n",
      "The cardinality of a set is an indicator of how many elements the set\n",
      "contains. If the number of elements in a set is a ﬁnite integer, that number is\n",
      "the cardinality of the set. If the elements of a set can be put into a one-to-\n",
      "one correspondence with a sequence of positive integers, the set is said to be\n",
      "countable. If it is countable but its cardinality is not a ﬁnite integer, then the\n",
      "set is said to be countably inﬁnite. Any interval of IR is uncountably inﬁnite.\n",
      "Its cardinality is said to be the cardinality of the continuum.\n",
      "In any particular application, we can conceive of a set of “everything”, or a\n",
      "“universe of discourse”. In general, we call this the universal set. (Sometimes,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "617\n",
      "especially in applications of probability, we will call it the sample space.) If A\n",
      "is the universal set, then when we speak of the set A1, we imply A1 ⊆A.\n",
      "The concept of a universal set also leads naturally to the concept of the\n",
      "complement of a set. The complement of A1, written Ac\n",
      "1, is the set of all\n",
      "elements in the universal set A that are not in A1, which we can also write as\n",
      "A −A1. More generally, given the sets A1 and A2, we write A1 −A2 (some\n",
      "people write A1\\A2 instead) to represent diﬀerence of A1 and A2; that is,\n",
      "the complement of A2 in A1, A1 −A2 = A1 ∩Ac\n",
      "2. If A2 ⊆A1, the diﬀerence\n",
      "A1 −A2 is called the proper diﬀerence.\n",
      "The symmetric diﬀerence of A1 and A2, written A1∆A2, is the union of\n",
      "their diﬀerences:\n",
      "A1∆A2\n",
      "def\n",
      "= (A1 −A2) ∪(A2 −A1).\n",
      "(0.0.1)\n",
      "Obviously, the symmetric diﬀerence operation is commutative:\n",
      "A1∆A2 = A2∆A1.\n",
      "Two useful relationships, known as De Morgan’s laws, are\n",
      "(A1 ∪A2)c = Ac\n",
      "1 ∩Ac\n",
      "2\n",
      "(0.0.2)\n",
      "and\n",
      "(A1 ∩A2)c = Ac\n",
      "1 ∪Ac\n",
      "2.\n",
      "(0.0.3)\n",
      "These two equations can be extended to countable unions and intersections:\n",
      "(∪∞\n",
      "i=1Ai)c = ∩∞\n",
      "i=1Ac\n",
      "i\n",
      "(0.0.4)\n",
      "and\n",
      "(∩∞\n",
      "i=1Ai)c = ∪∞\n",
      "i=1Ac\n",
      "i.\n",
      "(0.0.5)\n",
      "We often are interested in the “smallest” subset of the universal set A that\n",
      "has a given property. By the smallest subset we mean the intersection of all\n",
      "subsets of A with the given property.\n",
      "Product Sets\n",
      "The cartesian product (or direct product or cross product) of two sets A and B,\n",
      "written A × B, is the set of all doubletons, (ai, bj), where ai ∈A and bj ∈B.\n",
      "The set A × B is called a product set.\n",
      "Obviously, A × B ̸= B × A unless B = A.\n",
      "By convention, we have ∅× A = A × ∅= ∅= ∅× ∅.\n",
      "The concept of product sets can be extended to more than two sets in a\n",
      "natural way.\n",
      "One statement of the Axiom of Choice is that the cartesian product of any\n",
      "non-empty collection of non-empty sets is non-empty.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "618\n",
      "0 Statistical Mathematics\n",
      "Collections of Sets\n",
      "Collections of sets are usually called “collections”, rather than “sets”. We\n",
      "usually denote collections of sets with upper-case calligraphic letters, e.g., B,\n",
      "F, etc.\n",
      "The usual set operators and set relations are used with collections of sets,\n",
      "and generally have the same meaning. Thus if F1 is a collection of sets that\n",
      "contains the set A, we write A ∈F1, and if F2 is also a collection of sets, we\n",
      "denote the collection of all sets that are in either F1, or F2 as F1 ∪F2.\n",
      "The collection of all subsets of a given set is called the power set of the\n",
      "given set. An axiom of naive set theory postulates the existence of the power\n",
      "set for any given set. We denote the power set for a set S as 2S.\n",
      "Partitions; Disjoint Sets\n",
      "A partition of a set S is a collection of disjoint subsets of S whose union is S.\n",
      "Partitions of sets play an important role.\n",
      "A simple example is a partition of a set S that is the union two sets;\n",
      "S = A1 ∪A2. One partition of the union is just {A1, A2 −A1}. Given any\n",
      "union of sets ∪i=1Ai, we can obtain similar partitions. For a sequence of sets\n",
      "{An} the following theorem gives a sequence of disjoint sets {Dn} whose union\n",
      "is the same as that of {An}.\n",
      "Theorem 0.0.1 Let A1, A2, . . . be a sequence of subsets of the universal set\n",
      "A. Let the sequence of {Dn} be deﬁned as\n",
      "D1 = A1\n",
      "Dn = An −∪n−1\n",
      "i=1 Ai−1\n",
      "for n = 2, 3, . . .\n",
      "(0.0.6)\n",
      "Then the sets in the sequence {Dn} are disjoint, and\n",
      "∪∞\n",
      "i=1Di = ∪∞\n",
      "i=1Ai.\n",
      "Proof.\n",
      "Let {Dn} be as given; then Dn = An ∩\u0000∩n−1\n",
      "i=1 Ac\n",
      "i−1\n",
      "\u0001 and Dn ⊆An. Now let n\n",
      "and m be distinct integers. Without loss, suppose n < m. Then\n",
      "Dn ∩Dm ⊆An ∩Dm\n",
      "= An ∩Am ∩· · · ∩Ac\n",
      "n ∩· · ·\n",
      "= An ∩Ac\n",
      "n ∩· · ·\n",
      "= ∅.\n",
      "Also, because Di ⊆Ai,\n",
      "∪∞\n",
      "i=1Di ⊆∪∞\n",
      "i=1Ai.\n",
      "Now, let x ∈∪∞\n",
      "i=1Ai. Then x must belong to a least one Ai. Let n be the\n",
      "smallest integer such that x ∈An. Then x ∈Dn (by the deﬁnition of Dn),\n",
      "and so x ∈∪∞\n",
      "i=1Di. Hence,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "619\n",
      "∪∞\n",
      "i=1Ai ⊆∪∞\n",
      "i=1Di.\n",
      "Because each is a subset of the other,\n",
      "∪∞\n",
      "i=1Di = ∪∞\n",
      "i=1Ai.\n",
      "A partition of the full universal set S is formed by {Dn} and D0 = S −\n",
      "∪∞\n",
      "i=1Di.\n",
      "Sequences of nested intervals are important. The following corollary, which\n",
      "follows immediately from Theorem 0.0.1, applies to such sequences.\n",
      "Corollary 0.0.1.1\n",
      "Let {An} be a sequence of sets such that A1 ⊆A2 ⊆. . .. Let the sequence of\n",
      "{Dn} be deﬁned as\n",
      "Dn = An+1 −An\n",
      "for n = 1, 2, . . .\n",
      "(0.0.7)\n",
      "Then the sets in the sequence {Dn} are disjoint, and\n",
      "∪∞\n",
      "i=1Di = ∪∞\n",
      "i=1Ai.\n",
      "Notice that ∪i\n",
      "j=1Dj = Ai. (Notice also that there is an oﬀset in the indices of\n",
      "the sequence (0.0.7) from those of the sequence (0.0.6).)\n",
      "These ideas of partitioning a union of a sequence of sets may also be\n",
      "applied to an intersection of a sequence by using De Morgan’s laws. For the\n",
      "intersection, instead of an increasing sequence, A1 ⊆A2 ⊆. . ., our interest is\n",
      "usually in a decreasing sequence A1 ⊃A2 ⊃. . ..\n",
      "Another useful partition of the union S = A1 ∪A2 uses three sets:\n",
      "{A1 −(A1 ∩A2), A2 −(A1 ∩A2), (A1 ∩A2)}.\n",
      "(0.0.8)\n",
      "We call this partitioning “disjointiﬁcation”, and expression (0.0.8) is called\n",
      "the “inclusion-exclusion formula”.\n",
      "&%\n",
      "'$\n",
      "&%\n",
      "'$\n",
      "A1\n",
      "A2\n",
      "Disjointiﬁcation\n",
      "Disjointiﬁcation leads to the inclusion-exclusion formula for measures of\n",
      "sets that has common applications in deriving properties of measures (see, for\n",
      "example, equation (0.1.11) on page 707).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "620\n",
      "0 Statistical Mathematics\n",
      "Covering Sets\n",
      "A collection of sets A is said to cover a set S if S ⊆∪Ai∈AAi.\n",
      "Given a collection A = {A1, A2, . . .} that covers a set S, a partition of S\n",
      "can be formed by removing some of the intersections of sets in A. For example,\n",
      "if S ⊆A1 ∪A2, then {A1 ∩S, (A2 ∩S) −(A1 ∩A2)} is a partition of S.\n",
      "It is often of interest to determine the “smallest” partition of the universal\n",
      "set formed by sets in a given collection that does not necessarily cover the\n",
      "universal set. (If the number of sets is ﬁnite, the smallest partition has the\n",
      "smallest number of sets; otherwise, we may be able to give meaning to “small-\n",
      "est” in terms of intersections of collections of sets. In some cases, there is no\n",
      "reasonable interpretation of “smallest”.) For example, consider the collection\n",
      "A = {A1, A2}. If neither A1 nor A2 is a subset of the other, then the partition\n",
      "{A1 ∩A2, A1 −A2, A2 −A1, (A1 ∪A2)c}\n",
      "consists of the “smallest” collection of subsets that can be identiﬁed with\n",
      "operations on A1 and A2, and whose union is the universal set.\n",
      "If A1 ⊆A2, then A1 −A2 = ∅, and so the smallest partition is\n",
      "{A1, A2 −A1, Ac\n",
      "2}.\n",
      "Ordered Sets\n",
      "A set A is said to be partially ordered if there exists a relation ≤on A × A\n",
      "such that:\n",
      "•\n",
      "∀a ∈A, a ≤a (it is reﬂexive)\n",
      "•\n",
      "for a, b, c ∈A, a ≤b, b ≤c ⇒a ≤c (it is transitive)\n",
      "•\n",
      "for a, b ∈A, a ≤b, b ≤a ⇒a = b (it is antisymmetric)\n",
      "The relation is called an ordering. A set together with a partial ordering is\n",
      "called a poset.\n",
      "A set A is called linearly ordered (or totally ordered) if it is partially ordered\n",
      "and every pair of elements a, b ∈A can be compared with each other by the\n",
      "partial ordering relation. In that case, the relation is called a linear ordering\n",
      "or total ordering.\n",
      "The real numbers IR are linearly ordered using the common inequality\n",
      "relation. This relation can be used elementwise to deﬁne a partial ordering in\n",
      "a set S ∈IR2. Such an ordering, however, is not a linear ordering because, for\n",
      "example, a = (1, 2) and b = (2, 1) cannot be compared by that ordering.\n",
      "A set A is called well-ordered if it is an ordered set for which every non-\n",
      "empty subset contains a smallest element. By the Axiom of Choice, every set\n",
      "(even IRd) can be well-ordered. The positive integers are well-ordered by the\n",
      "usual inequality relation, but neither the set of all integers or the reals are\n",
      "well-ordered by the usual inequality relation.\n",
      "For structures built on sets, such as a ﬁeld (Deﬁnition 0.0.3), we may deﬁne\n",
      "diﬀerent kinds of orderings that preserve the structure, as in Deﬁnition 0.0.4,\n",
      "for example.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "621\n",
      "0.0.2 Sets and Spaces\n",
      "In Section 0.0.9 beginning on page 676, we discussed some basics of sets and\n",
      "operations on sets. Now we consider mathematical structures that are built on\n",
      "sets together with other objects or methods such as an operation on the given\n",
      "set or on subsets of the set. We often refer to these structures as “spaces”.\n",
      "Spaces\n",
      "In any application it is generally useful to deﬁne some “universe of discourse”\n",
      "that is the set of all elements that will be considered in a given problem. Given\n",
      "a universe or universal set, which we often denote by the special symbol Ω\n",
      "(note the font), we then deﬁne various mathematical structures on Ω. These\n",
      "structures, or “spaces”, are formed by specifying certain types of collections\n",
      "of subsets of Ωand/or by deﬁning operations on the elements of Ωor on the\n",
      "subsets in the special collection of subsets. In probability and statistics, we\n",
      "will call the universal set the sample space.\n",
      "Some of the general structures that we will ﬁnd useful are topological\n",
      "spaces, which are deﬁned in terms of the type of collection of subsets of the\n",
      "universal set, and metric spaces and linear spaces, which are deﬁned in terms\n",
      "of operations on elements of the universal set. We will discuss these below,\n",
      "and then in Section 0.0.5, we will discuss some properties of the special spaces\n",
      "in which the universal set is the set of real numbers. In Section 0.1, we will\n",
      "discuss various types of collections of subsets of the universal set, and then\n",
      "for a particular type of collection, called a σ-ﬁeld, we will discuss a special\n",
      "type of space, called a measurable space, and then, with the addition of a\n",
      "real-valued set function, we will deﬁne a measure space. A particular type of\n",
      "measure space is a probability space.\n",
      "Topologies\n",
      "One of the simplest structures based on the nonempty universal set Ωis a\n",
      "topological space or a topology, which is formed by any collection T of subsets\n",
      "of Ωwith the following properties:\n",
      "(t1) ∅, Ω∈T , and\n",
      "(t2) A, B ∈T ⇒A ∩B ∈T , and\n",
      "(t3) A ⊆T ⇒∪{A : A ∈A} ∈T .\n",
      "We denote a topological space by a double of the form (Ω, T ).\n",
      "We may use the term “topology” to denote either the space or the collec-\n",
      "tion of subsets that deﬁnes it.\n",
      "Properties of Ωthat can be expressed in terms of a topology are called its\n",
      "topological properties. Without imposing any additional structure on a topo-\n",
      "logical space, we can deﬁne several useful concepts, but because the collection\n",
      "of subsets that deﬁne a topology is arbitrary, many terms that relate to a\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "622\n",
      "0 Statistical Mathematics\n",
      "topology are too general for our use in developing a theory of probability for\n",
      "real-valued random variables.\n",
      "Given a topological space (Ω, T ), we can deﬁne a subspace topology as any\n",
      "set S ⊆Ωtogether with the collection of subsets\n",
      "TS = {S ∩U | U ∈T }.\n",
      "Open and Closed Sets in a Topology\n",
      "Let (Ω, T ) be a topological space. Members of T are called open sets. A set\n",
      "A ⊆Ωis said to be closed iﬀΩ∩Ac ∈T . Notice that this deﬁnition means\n",
      "that some sets, for example, ∅and Ω, are both open and closed. Such sets are\n",
      "sometimes said to be clopen. Also, notice that some subsets of Ωare neither\n",
      "open nor closed.\n",
      "For the set A ⊆Ω, the closure of A is the set\n",
      "A = ∩{B : B is closed, and A ⊆B ⊆Ω}.\n",
      "(Notice that every y ∈A is a point of closure of A, and that A is closed iﬀ\n",
      "A = A.) For the set A ⊆Ω, the interior of A is the set\n",
      "A◦= ∪{U : U is open, andU ⊆A}.\n",
      "The boundary of the set A ⊆Ωis the set\n",
      "∂A = A ∩Ac.\n",
      "A set A ⊆Ωsuch that A = Ωis said to be dense in Ω.\n",
      "A set is said to be separable if it contains a countable dense subset. Obvi-\n",
      "ously, any countable set is itself separable. (This term is usually applied only\n",
      "to the universal set.)\n",
      "Any subcollection A1, A2, . . . of T such that ∪iAi = Ωis called an open\n",
      "cover of Ω. A topological space for which each open cover contains a ﬁnite open\n",
      "cover is said to be compact. A set A in a topological space is said to be compact\n",
      "if each collection of open sets that covers A contains a ﬁnite subcollection of\n",
      "open sets that covers A.\n",
      "The following properties of unions and intersections of open and closed\n",
      "sets are easy to show from the deﬁnitions:\n",
      "•\n",
      "The intersection of a ﬁnite collection of open sets is open.\n",
      "•\n",
      "The union of a countable collection of open sets is open.\n",
      "•\n",
      "The union of a ﬁnite collection of closed sets is closed.\n",
      "•\n",
      "The intersection of a countable collection of closed sets is closed.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "623\n",
      "Point Sequences in a Topology\n",
      "A sequence {xn} in the topological space (Ω, T ) is said to converge to the\n",
      "point x, or to have a limit x, if given any open set T containing x, there is an\n",
      "integer N such that xn ∈T ∀n ≥N.\n",
      "A point x is said to be an accumulation point or cluster point of the se-\n",
      "quence {xn} if given any open set T containing x and any integer N, there is\n",
      "an integer n ≥N ∋xn ∈T. This means that x is an accumulation point if\n",
      "{xn} has a subsequence that converges to x. In an arbitrary topological space,\n",
      "however, it is not necessarily the case that if x is an accumulation point of\n",
      "{xn} that there is a subsequence of {xn} that converges to x.\n",
      "Neighborhoods Deﬁned by Open Sets\n",
      "Given a topological space (Ω, T ), a neighborhood of a point ω ∈Ωis any set\n",
      "U ∈T such that x ∈U. Notice that Ωis a neighborhood of each point.\n",
      "The space (Ω, T ) is called a Hausdorﬀspace iﬀeach pair of distinct points\n",
      "of Ωhave disjoint neighborhoods. For x ∈Ωand A ⊆Ω, we say that x is a\n",
      "limit point of A iﬀfor each neighborhood U of x, U ∩{x}c ∩A ̸= ∅.\n",
      "The topological space (Ω, T ) is said to be connected iﬀthere do not exist\n",
      "two disjoint open sets A and B such that A ∪B = Ω. We can also speak of a\n",
      "subset of Ωas being connected, using this same condition.\n",
      "In a space endowed with a metric, which we deﬁne below, open sets can be\n",
      "deﬁned by use of the metric. The corresponding topology can then be deﬁned\n",
      "as the collection of all open sets according to the deﬁnition of openness in that\n",
      "context, and the topological properties follow in the usual way. In a metric\n",
      "topological space, we can also deﬁne the topological properties in terms of the\n",
      "metric. These alternate deﬁnitions based on a metric are the ones we will use\n",
      "for sets of real numbers.\n",
      "Metrics\n",
      "A useful structure can be formed by introduction of a function that maps the\n",
      "product set Ω× Ωinto the nonnegative reals. It is called a metric.\n",
      "Deﬁnition 0.0.1 (metric)\n",
      "Given a space Ω, a metric over Ωis a function ρ such that for x, y, z ∈Ω\n",
      "•\n",
      "ρ(x, y) ≥0\n",
      "•\n",
      "ρ(x, y) = 0 if and only if x = y\n",
      "•\n",
      "ρ(x, y) = ρ(y, x)\n",
      "•\n",
      "ρ(x, y) ≤ρ(x, z) + ρ(z, x).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "624\n",
      "0 Statistical Mathematics\n",
      "The structure (Ω, ρ) is called a metric space.\n",
      "A common example of a metric space is the set IR together with ρ(x, y) =\n",
      "|x −y|, where | · | denotes the ordinary absolute value.\n",
      "The concept of a metric allows us to redeﬁne the topological properties\n",
      "introduced above in terms of the metric. The deﬁnitions in terms of a metric\n",
      "are generally more useful, and also a metric allows us to deﬁne additional\n",
      "important properties, such as continuity. Rather than deﬁne special sets for\n",
      "general metric spaces here, we will discuss these sets and their properties in\n",
      "the context of IR in Section 0.0.5.\n",
      "Neighborhoods Deﬁned by Metrics\n",
      "We have deﬁned neighborhoods in general topological spaces, but the concept\n",
      "of a metric allows us to give a more useful deﬁnition of a neighborhood of a\n",
      "point in a set. For a point x ∈Ω, a metric ρ on Ω, and any positive number ϵ,\n",
      "an ϵ-neighborhood of x, denoted by Nρ(x, ϵ), is the set of y ∈Ωwhose distance\n",
      "from x is less than ϵ; that is,\n",
      "Nρ(x, ϵ) def\n",
      "= {y : ρ(x, y) < ϵ}.\n",
      "(0.0.9)\n",
      "Notice that the meaning of a neighborhood depends on the metric, but in\n",
      "any case it is an open set, in the sense made more precise below. Usually, we\n",
      "assume that a metric is given and just denote the neighborhood as N(x, ϵ)\n",
      "or, with the size in place of the metric, as Nϵ(x). We also often refer to some\n",
      "(unspeciﬁed) ϵ-neighborhood of x just as a “neighborhood” and denote it as\n",
      "N(x).\n",
      "The concept of a neighborhood allows us to give a more meaningful deﬁni-\n",
      "tion of open sets and to deﬁne such things as continuity. These deﬁnitions are\n",
      "consistent with the deﬁnitions of the same concepts in a general topological\n",
      "space, as discussed above.\n",
      "Theorem 0.0.2 Every metric space is a Hausdorﬀspace.\n",
      "Proof. Exercise.\n",
      "Open and Closed Sets in a Metric Space\n",
      "The speciﬁcation of a topology deﬁnes the open sets of the structure and\n",
      "consequently neighborhoods of points. It is often a more useful approach,\n",
      "however, ﬁrst to deﬁne a metric, then to deﬁne neighborhoods as above, and\n",
      "ﬁnally to deﬁne open sets in terms of neighborhoods. In this approach, a\n",
      "subset G of Ωis said to be open if each member of G has a neighborhood that\n",
      "is contained in G.\n",
      "Note that with each metric space (Ω, ρ), we can associate a topological\n",
      "space (Ω, T ), where T is the collection of open sets in (Ω, ρ) that are deﬁned\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "625\n",
      "in terms of the metric. The topology provides the deﬁnition of a closed set, as\n",
      "above; that is, a set A ⊆Ωis said to be closed iﬀΩ∩Ac ∈T , where T is the\n",
      "collection of open sets deﬁned in terms of the metric. As with the deﬁnitions\n",
      "above for general topological spaces, some sets are both open and closed, and\n",
      "such sets are said to be clopen.\n",
      "We note that (IR, ρ) is a Hausdorﬀspace because, given x, y ∈IR and\n",
      "x ̸= y we have ρ(x, y) > 0 and so N(x, ρ(x, y)/2) and N(y, ρ(x, y)/2) are\n",
      "disjoint open sets.\n",
      "We also note that IR is connected, as is any interval in IR. (Connectedness\n",
      "is a topological property that is deﬁned on page 623.)\n",
      "We will defer further discussion of openness and related concepts to\n",
      "page 645 in Section 0.0.5 where we discuss the real number system.\n",
      "Relations and Functions\n",
      "A relation is a set of doubletons, or pairs of elements; that is, a relation is a\n",
      "subset of a cartesian product of two sets. We use “relation” and “mapping”\n",
      "synonymously.\n",
      "A function is a relation in which no two diﬀerent pairs have the same ﬁrst\n",
      "element.\n",
      "To say that f is a function from Ωto Λ, written\n",
      "f : Ω7→Λ,\n",
      "means that for every ω ∈Ωthere is a pair in f whose ﬁrst member is ω. We\n",
      "use the notation f(ω) to represent the second member of the pair in f whose\n",
      "ﬁrst member is ω, and we call ω the argument of the function. We call Ωthe\n",
      "domain of the function and we call {λ|λ = f(ω) for some ω ∈Ω} the range\n",
      "of the function.\n",
      "Variations include functions that are onto, meaning that for every λ ∈Λ\n",
      "there is a pair in f whose second member is λ; and functions that are one-to-\n",
      "one, often written as 1 : 1, meaning that no two pairs have the same second\n",
      "member. A function that is one-to-one and onto is called a bijection.\n",
      "A function f that is one-to-one has an inverse, written f−1, that is a\n",
      "function from Λ to Ω, such that if f(ω0) = λ0, then f−1(λ0) = ω0.\n",
      "If (a, b) ∈f, we may write a = f−1(b), although sometimes this notation is\n",
      "restricted to the cases in which f is one-to-one. If f is not one-to-one and if the\n",
      "members of the pairs in f are reversed, the resulting relation is not a function.\n",
      "We say f−1 does not exist; yet for convenience we may write a = f−1(b), with\n",
      "the meaning above.\n",
      "If A ⊆Ω, the image of A, denoted by f[A], or just by f(A), is the set of all\n",
      "λ ∈Λ for which λ = f(ω) for some ω ∈Ω. (The notation f[A] is preferable,\n",
      "but we will often just use f(A).) Similarly, if C is a collection of sets (see\n",
      "below), the notation f[C] denotes the collection of sets {f[C] : C ∈C}.\n",
      "For the function f that maps from Ωto Λ, Ωis called the domain of the\n",
      "function and and f[Ω] is called the range of the function.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "626\n",
      "0 Statistical Mathematics\n",
      "For a subset B of Λ, the inverse image or the preimage of B, denoted by\n",
      "f−1[B], or just by f−1(B), is the set of all ω ∈Ωsuch that f(ω) ∈B. The\n",
      "notation f−1 used in this sense must not be confused with the inverse function\n",
      "f−1 (if the latter exists). We use this notation for the inverse image whether\n",
      "or not the inverse of the function exists.\n",
      "We also write f[f−1[B]] as f ◦f−1[B]. The set f[f−1[B]] may be a proper\n",
      "subset of B; that is, there may be an element λ in B for which there is no\n",
      "ω ∈Ωsuch that f(ω) = λ. In this case the inverse image of a set may not\n",
      "generate the set. If f is bijective, then f[f−1[B]] = B.\n",
      "We will discuss functions, images, and preimages further in Section 0.1.2,\n",
      "beginning on page 701.\n",
      "Continuous Functions\n",
      "A function f from the metric space Ωwith metric ρ to the metric space Λ\n",
      "with metric τ is said to be continuous at the point ω0 ∈Ωif for any ϵ > 0\n",
      "there is a δ > 0 such that f maps Nρ(ω0, ϵ) into Nτ(f(ω0), δ). Usually, we\n",
      "assume that the metrics are given and, although they may be diﬀerent, we\n",
      "denote the neighborhood without explicit reference to the metrics. Thus, we\n",
      "write f[N(ω0, ϵ)] ⊆N(f(ω0), δ).\n",
      "We will discuss various types of continuity of real-valued functions over\n",
      "real domains in Section 0.1.5 beginning on page 720.\n",
      "Sequences of Sets; lim sup and lim inf\n",
      "De Morgan’s laws (0.0.2) and (0.0.3) express important relationships between\n",
      "unions, intersections, and complements.\n",
      "Two important types of unions and intersections of sequences of sets are\n",
      "called the lim sup and the lim inf and are deﬁned as\n",
      "lim sup\n",
      "n\n",
      "An\n",
      "def\n",
      "= ∩∞\n",
      "n=1 ∪∞\n",
      "i=n Ai\n",
      "(0.0.10)\n",
      "and\n",
      "lim inf\n",
      "n\n",
      "An\n",
      "def\n",
      "= ∪∞\n",
      "n=1 ∩∞\n",
      "i=n Ai.\n",
      "(0.0.11)\n",
      "We sometimes use the alternative notation A∗or limn for lim sup:\n",
      "A∗def\n",
      "= limnAn\n",
      "def\n",
      "= lim sup\n",
      "n\n",
      "An,\n",
      "(0.0.12)\n",
      "and A∗or limn for lim inf:\n",
      "A∗\n",
      "def\n",
      "= limnAn\n",
      "def\n",
      "= lim inf\n",
      "n\n",
      "An.\n",
      "(0.0.13)\n",
      "We deﬁne convergence of a sequence of sets in terms of lim sup and lim\n",
      "inf.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "627\n",
      "The sequence of sets {An} is said to converge if\n",
      "lim sup\n",
      "n\n",
      "An = lim inf\n",
      "n\n",
      "An,\n",
      "(0.0.14)\n",
      "and this set is said to be the limit of the sequence, written simply as limn An.\n",
      "A sequence of sets {An} is said to be increasing if\n",
      "An ⊆An+1 ∀n,\n",
      "(0.0.15)\n",
      "and is said to be decreasing if\n",
      "An+1 ⊆An ∀n.\n",
      "(0.0.16)\n",
      "In either case, the sequence is said to be monotone.\n",
      "An increasing sequence {An} converges to ∪∞\n",
      "n=1An.\n",
      "A decreasing sequence {An} converges to ∩∞\n",
      "n=1An.\n",
      "Some Basic Facts about lim sup and lim inf\n",
      "Two simple relationships that follow immediately from the deﬁnitions:\n",
      "lim sup\n",
      "n\n",
      "An ⊆∪∞\n",
      "i=kAi\n",
      "∀k ≥1\n",
      "(0.0.17)\n",
      "and\n",
      "∩∞\n",
      "i=k Ai ⊆lim inf\n",
      "n\n",
      "An\n",
      "∀k ≥1,\n",
      "(0.0.18)\n",
      "which of course leads to\n",
      "lim inf\n",
      "n\n",
      "An ⊆lim sup\n",
      "n\n",
      "An.\n",
      "(0.0.19)\n",
      "To see the latter fact directly, consider any ω ∈lim infn An:\n",
      "ω ∈∪∞\n",
      "n=1 ∩∞\n",
      "i=n Ai\n",
      "⇐⇒\n",
      "∃n such that ∀i ≥n, ω ∈Ai,\n",
      "so ω ∈lim supn An.\n",
      "By De Morgan’s laws, we have the useful relationships between lim sup\n",
      "and lim inf and sequences of complementary sets:\n",
      "\u0010\n",
      "lim inf\n",
      "n\n",
      "An\n",
      "\u0011c\n",
      "= lim sup\n",
      "n\n",
      "Ac\n",
      "n\n",
      "(0.0.20)\n",
      "and\n",
      "\u0012\n",
      "lim sup\n",
      "n\n",
      "An\n",
      "\u0013c\n",
      "= lim inf\n",
      "n\n",
      "Ac\n",
      "n.\n",
      "(0.0.21)\n",
      "We can interpret lim sup and lim inf in intuitive terms. From the deﬁnition\n",
      "lim supn An = ∩∞\n",
      "n=1 ∪∞\n",
      "i=n Ai, we have immediately\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "628\n",
      "0 Statistical Mathematics\n",
      "lim sup\n",
      "n\n",
      "An = {ω | ω ∈Ai for inﬁnitely many i},\n",
      "(0.0.22)\n",
      "and\n",
      "ω ∈lim sup\n",
      "n\n",
      "An\n",
      "⇐⇒\n",
      "∀n ∃i ≥n ∋ω ∈Ai.\n",
      "(0.0.23)\n",
      "From the deﬁnition lim infn An = ∪∞\n",
      "n=1 ∩∞\n",
      "i=n Ai, similarly we have\n",
      "lim inf\n",
      "n\n",
      "An = {ω | ω ∈Ai for all but a ﬁnite number of i},\n",
      "(0.0.24)\n",
      "and\n",
      "ω ∈lim inf\n",
      "n\n",
      "An\n",
      "⇐⇒\n",
      "∃n ∋∀i ≥n, ω ∈Ai.\n",
      "(0.0.25)\n",
      "While at ﬁrst glance, equations (0.0.22) and (0.0.24) may seem to say the\n",
      "same thing, they are very diﬀerent, and in fact characterize lim sup and lim\n",
      "inf respectively. They could therefore be used as deﬁnitions of lim sup and lim\n",
      "inf. Exercise 0.0.3 asks you to prove these two equations.\n",
      "Examples\n",
      "Example 0.0.1 (Alternating-constant series)\n",
      "Consider the alternating-constant series of abstract sets:\n",
      "A2n = B and A2n+1 = C.\n",
      "Then\n",
      "lim sup\n",
      "n\n",
      "An = B ∪C\n",
      "and\n",
      "lim inf\n",
      "n\n",
      "An = B ∩C.\n",
      "Example 0.0.2 (Alternating series of increasing and decreasing intervals)\n",
      "Now let the sample space be IR, and consider the intervals\n",
      "A2n =] −n, n[ and A2n+1 =]0, 1/n[.\n",
      "Then\n",
      "lim sup\n",
      "n\n",
      "An = IR\n",
      "and\n",
      "lim inf\n",
      "n\n",
      "An = ∅.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "629\n",
      "Example 0.0.3\n",
      "Now, again in IR, consider the sequence of intervals\n",
      "An =\n",
      "\n",
      "\n",
      "\n",
      "\u0003 1\n",
      "n,\n",
      "3\n",
      "4 −1\n",
      "n\n",
      "\u0002\n",
      "for n = 1, 3, 5, . . .\n",
      "\u0003 1\n",
      "4 −1\n",
      "n, 1 + 1\n",
      "n\n",
      "\u0002\n",
      "for n = 2, 4, 6, . . .\n",
      "In this case,\n",
      "lim sup\n",
      "n\n",
      "An =]0, 1],\n",
      "and\n",
      "lim inf\n",
      "n\n",
      "An =\n",
      "\u00141\n",
      "4, 3\n",
      "4\n",
      "\u0014\n",
      ".\n",
      "0.0.3 Binary Operations and Algebraic Structures\n",
      "In a given set S we may ﬁnd it useful to deﬁne a binary operation; that is, a\n",
      "way of combining two elements of the set to form a single entity. If x, y ∈S,\n",
      "we may denote the binary operation as ◦and we denote the result of the\n",
      "combination of x and y under ◦as x ◦y.\n",
      "Given a set S and a binary operation ◦deﬁned on it, various properties of\n",
      "the operation may be of interest:\n",
      "closure We say S is closed wrt ◦iﬀ\n",
      "x, y ∈S =⇒x ◦y ∈S.\n",
      "commutativity We say ◦deﬁned in S is commutative iﬀ\n",
      "x, y ∈S =⇒x ◦y = y ◦x.\n",
      "associativity We say ◦deﬁned in S is associative iﬀ\n",
      "x, y, z ∈S =⇒x ◦(y ◦z) = (x ◦y) ◦z.\n",
      "Groups\n",
      "One of the most useful algebraic structures is a group, which is a set and an\n",
      "operation (S, ◦) with special properties.\n",
      "Deﬁnition 0.0.2 (group)\n",
      "Let S be a nonempty set and let ◦be a binary operation. The structure (S, ◦)\n",
      "is called a group if the following conditions hold.\n",
      "•\n",
      "x1, x2 ∈S ⇒x1 ◦x2 ∈S (closure);\n",
      "•\n",
      "∃e ∈S ∋∀x ∈S, e ◦x = x (identity);\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "630\n",
      "0 Statistical Mathematics\n",
      "•\n",
      "∀x ∈S ∃x−1 ∈S ∋x−1 ◦x = e (inverse);\n",
      "•\n",
      "x1, x2, x3 ∈S ⇒x1 ◦(x2 ◦x3) = (x1 ◦x2) ◦x3 (associativity).\n",
      "If a group contains only one element, it is called a trivial group, and obvi-\n",
      "ously is not of much interest.\n",
      "Notice that the binary operation need not be commutative, but some spe-\n",
      "ciﬁc pairs commute under the operation. In particular, we can easily see that\n",
      "x ◦e = e◦x and x ◦x−1 = x−1 ◦x (exercise). We can also see that e is unique,\n",
      "and for a given x, x−1 is unique (exercise).\n",
      "If the binary operation in the group (S, ◦) is commutative, then the group\n",
      "is called a commutative group or an Abelian group.\n",
      "A simple example of an Abelian group is the set of all real numbers together\n",
      "with the binary operation of ordinary addition.\n",
      "We often denote a group by a single symbol, say G, for example, and we\n",
      "use the phrase x ∈G to refer to an element in the set that is part of the\n",
      "structure G; that is, if G = (S, ◦), then x ∈G ⇔x ∈S.\n",
      "A very important group is formed on a set of transformations.\n",
      "Example 0.0.4 Group of bijections\n",
      "Let X be a set and let G be a set of bijective functions g on X. For g1, g2 ∈G,\n",
      "let g1 ◦g2 represent function composition; that is, for x ∈X, g1 ◦g2(x) =\n",
      "g1(g2(x)). We see that\n",
      "•\n",
      "g1 ◦g2 is a bijection on X, so G is closed with respect to ◦;\n",
      "•\n",
      "the function ge(x) = x is a bijection, and ∀g ∈G, ge ◦g = g (identity);\n",
      "•\n",
      "∀g ∈G ∃g−1 ∈G ∋g−1 ◦g = ge (inverse);\n",
      "•\n",
      "g1, g2, g3 ∈G ⇒g1◦(g2◦g3) = (g1◦g2)◦g3 (associativity); that is, ∀x ∈X,\n",
      "either grouping of the operations is g1(g2(g3(x))).\n",
      "Therefore, (G, ◦) is a group. The group of bijections in general is not Abelian.\n",
      "In this Example 0.0.4, we began with bijective functions and showed that\n",
      "they formed a group. We could have begun with a group of functions (and\n",
      "obviously they must be onto functions), and showed that they must be 1:1\n",
      "(Exercise 0.0.5).\n",
      "Subgroups and Generating Sets\n",
      "Any subset of the set on which the group is deﬁned that is closed and contains\n",
      "the identity and all inverses forms a group with the same operation as the\n",
      "original group. This subset together with the operation is called a subgroup.\n",
      "We use the standard terminology of set operations for operations on groups.\n",
      "A set G1 together with an operation ◦deﬁned on G1 generates a group G\n",
      "that is the smallest group (G, ◦) such that G1 ⊆G. If G1 and G2 are groups\n",
      "over G1 and G2 with a common operation ◦, the group generated by G1 and\n",
      "G2 is (G, ◦), where G is the smallest set containing G1 and G2 so that (G, ◦)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "631\n",
      "is a group. Notice that the G may contain elements that are in neither G1 nor\n",
      "G2.\n",
      "Homomorphisms\n",
      "It is often of interest to consider relationships between two groups. The sim-\n",
      "plest and most useful is a morphism, or homomorphism. (The two words are\n",
      "synonymous; I will generally use the latter.) Given two groups G = (G, ◦) and\n",
      "G∗= (G∗, ⋄), a homomorphism from G to G∗is a function f from G to G∗\n",
      "such that for g1, g2 ∈G,\n",
      "f(g1 ◦g2) = f(g1) ⋄f(g2).\n",
      "Often in applications, G and G∗are sets of the same kind of objects, say\n",
      "functions for example, and the operations ◦and ⋄are the same, say function\n",
      "composition.\n",
      "If the homomorphism from G to G∗is a bijection, then the homomorphism\n",
      "is called an isomorphism, and since it has an inverse, we say the two groups\n",
      "are isomorphic. Isomorphic groups of transformations are the basic objects\n",
      "underlying the concept of equivariant and invariant statistical procedures.\n",
      "Structures with Two Binary Operators\n",
      "Often it is useful to deﬁne two diﬀerent binary operators, say + and ◦over\n",
      "the same set. An important type of relationship between the two operators is\n",
      "called distributivity:\n",
      "distributivity We say ◦is distributive over + in S iﬀ\n",
      "x, y, z ∈S =⇒x ◦(y + z) = (x ◦y) + (x ◦z) ∈S.\n",
      "Another very useful algebraic structure is a ﬁeld, which is a set and two\n",
      "binary operations (S, +, ◦) with special properties.\n",
      "Deﬁnition 0.0.3 (ﬁeld)\n",
      "Let S be a set with two distinct elements and let + and ◦be binary operations.\n",
      "The structure (S, +, ◦) is called a ﬁeld if the following conditions hold.\n",
      "(f1) x1, x2 ∈S ⇒x1 + x2 ∈S (closure of +);\n",
      "(f2) ∃0 ∈S ∋∀x ∈S, 0 + x = x (identity for +);\n",
      "(f3) ∀x ∈S ∃−x ∈S ∋−x ◦x = e (inverse wrt +);\n",
      "(f4) x1, x2 ∈S ⇒x1 + x2 = x2 + x1 (commutativity of +);\n",
      "(f5) x1, x2, x3 ∈S ⇒x1 ◦(x2 ◦x3) = (x1 ◦x2) ◦x3 (associativity of +).\n",
      "(f6) x1, x2 ∈S ⇒x1 ◦x2 ∈S (closure of ◦);\n",
      "(f7) ∃1 ∈S ∋∀x ∈S, 1 ◦x = x (identity for ◦);\n",
      "(f8) ∀x ̸= 0 ∈S ∃x−1 ∈S ∋x−1 ◦x = 1 (inverse wrt ◦);\n",
      "(f9) x1, x2 ∈S ⇒x1 ◦x2 = x2 ◦x1 (commutativity of ◦);\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "632\n",
      "0 Statistical Mathematics\n",
      "(f10) x1, x2, x3 ∈S ⇒x1 ◦(x2 ◦x3) = (x1 ◦x2) ◦x3 (associativity of ◦).\n",
      "(f11) x1, x2, x3 ∈S ⇒x1 ◦(x2 + x3) = (x1 ◦x2) + (x1 ◦x3) (distributivity of\n",
      "◦over +).\n",
      "Although a ﬁeld is a structure consisting of a set and two binary operations,\n",
      "(S, +, ◦), we often refer to the ﬁeld by the name of the set, S, in this case.\n",
      "Notice that the word “ﬁeld” is also used in a slightly diﬀerent sense to refer\n",
      "to a structure consisting of a collection of sets and the unary operation of set\n",
      "complementation and the binary operation of set union (see Deﬁnition 0.1.3\n",
      "on page 693).\n",
      "For x1, x2 in the ﬁeld (S, +, ◦), we will adopt the notation x1 −x2 to mean\n",
      "the element d ∈S, such that d + x2 = x1, and if x2 ̸= 0, we will adopt the\n",
      "notation x1/x2 to mean the element q ∈S, such that qx2 = x1.\n",
      "If x is an element in a ﬁeld (S, +, ◦), and n is an integer (not necessarily\n",
      "an element of the ﬁeld), then by the expression nx we mean “x + · · · + x” n\n",
      "times if n is positive, we mean “0” if n is zero, and we mean “−x −· · ·−x” n\n",
      "times if n is negative. Thus, we have a multiplication-type operation between\n",
      "an integer and any element of a ﬁeld, and the operation is closed within the\n",
      "ﬁeld. Also, by the expression xn we mean “x ◦· · · ◦x” n times if n is positive,\n",
      "we mean “1” if n is zero, and we mean “(1/x) ◦· · · ◦(1/x)” n times if n is\n",
      "negative.\n",
      "A ring is a structure having all properties of a ﬁeld except, possibly, no\n",
      "inverses wrt the second operation, that is, lacking property (f8), and possibly\n",
      "having only one element (that is, a ring can be trivial). Sometimes a ring in\n",
      "which the second operation is commutative (as we have required and as is\n",
      "always required for a ﬁeld) is called a commutative ring, and a structure that\n",
      "may possibly lack that commutativity is called a ring.\n",
      "The concept of homomorphisms between ﬁelds or between rings follows\n",
      "similar development as that of homomorphisms between groups.\n",
      "Examples\n",
      "The set of integers together with the operations of ordinary addition and\n",
      "multiplication form a ring, but do not form a ﬁeld.\n",
      "The set of rational numbers (which are numbers that can be represented\n",
      "in the form a/b, where a and b are integers) is a ﬁeld. Other common examples\n",
      "of ﬁelds are the sets of all real numbers, of all complex numbers, and of all\n",
      "p-adic numbers, each together with the binary operations of ordinary addition\n",
      "and multiplication.\n",
      "For our purposes, the most important of these is the ﬁeld of real numbers.\n",
      "We denote this ﬁeld as IR; that is, IR = (S, +, ◦), where S is the set of all real\n",
      "numbers, + is ordinary addition, and ◦is ordinary multiplication. (Of course,\n",
      "we also use the symbol IR to denote just the set of all real numbers.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "633\n",
      "While I generally attempt to give due attention to IRd for d = 1, 2, . . .,\n",
      "because for d > 1, there is only one useful operation that takes IRd × IRd into\n",
      "IRd (that is, addition), there is no useful ﬁeld deﬁned on IRd for d > 1.\n",
      "Note that the extended reals IR together with the extensions of the deﬁ-\n",
      "nitions of + and ◦for −∞and ∞(see page 640) is not a ﬁeld. (Operations\n",
      "for some pairs are not deﬁned, and additional elements fail to have inverses.)\n",
      "In at least two areas of statistics (design of experiments and random num-\n",
      "ber generation), ﬁelds with a ﬁnite number of elements are useful. A ﬁnite\n",
      "ﬁeld is sometimes called a Galois ﬁeld. We often denote a Galois ﬁeld with m\n",
      "elements as IG(m).\n",
      "A ﬁeld can be deﬁned by means of an addition table and a multiplication\n",
      "table. For example a IG(5) over the set {0, 1, 2, 3, 4} together with the “addi-\n",
      "tion” operation + with identity 0 and the “multiplication” operation ◦with\n",
      "identity 1 can be deﬁned by giving the tables below.\n",
      "Table 0.1. Operation Tables for IG(5)\n",
      "+ 0 1 2 3 4\n",
      "0 0 1 2 3 4\n",
      "1 1 2 3 4 0\n",
      "2 2 3 4 0 1\n",
      "3 3 4 0 1 2\n",
      "4 4 0 1 2 3\n",
      "◦0 1 2 3 4\n",
      "0 0 0 0 0 0\n",
      "1 0 1 2 3 4\n",
      "2 0 2 4 1 3\n",
      "3 0 3 1 4 2\n",
      "4 0 4 3 2 1\n",
      "Because the tables are symmetric, we know that the operations are com-\n",
      "mutative. We see that each row in the addition table contains the additive\n",
      "identity; hence, each element has an additive inverse. We see that each row\n",
      "in the multiplication table, except for the row corresponding to additive iden-\n",
      "tity contains the multiplicative identity; hence, each element except for the\n",
      "additive identity has a multiplicative inverse.\n",
      "Also we see, for example, that the additive inverse of 3, that is, −3, is 1,\n",
      "and the multiplicative inverse of 3, that is, 3−1, is 2.\n",
      "In ﬁelds whose elements are integers, the smallest positive integer k such\n",
      "that k ◦1 = 0 is called the characteristic of the ﬁeld. It is clear that no such\n",
      "integer exists for an inﬁnite ﬁeld, so we deﬁne the characteristic of an inﬁnite\n",
      "ﬁeld to be 0.\n",
      "The number of elements of any ﬁnite ﬁeld, called its order, is of the form\n",
      "pn, where p is a prime number and n is a positive integer; conversely, for\n",
      "every prime number p and positive integer n, there exists a ﬁnite ﬁeld with\n",
      "pn elements. (See Hewitt and Stromberg (1965) for a proof.) In the case of\n",
      "a ﬁeld of order pn, p is the characteristic of the ﬁeld. The term “order” has\n",
      "other meanings in regard to ﬁelds, as we see below.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "634\n",
      "0 Statistical Mathematics\n",
      "Ordered Fields\n",
      "We have deﬁned ordered sets in Section 0.0.1 in terms of the existence of a\n",
      "binary relation. We now deﬁne ordered ﬁelds in terms of the ﬁeld operations.\n",
      "Deﬁnition 0.0.4 (ordered ﬁeld)\n",
      "A ﬁeld S is said to be ordered if there is a subset P of S such that\n",
      "•\n",
      "P ∩(−P ) = ∅;\n",
      "•\n",
      "P ∪{0} ∪(−P ) = S;\n",
      "•\n",
      "x, y ∈P ⇒x + y, x ◦y ∈P .\n",
      "Notice that this is only a partial ordering; it is neither a linear ordering nor\n",
      "a well-ordering. Applying this deﬁnition to the real numbers, we can think of\n",
      "P as the positive reals, and the elements of −P as the negative reals. Notice\n",
      "that 1 must be in P (because a ∈P ⇒a2 ∈P and b ∈−P ⇒b2 ∈P ) and so\n",
      "an ordered ﬁeld must have characteristic 0; in particular, a ﬁnite ﬁeld cannot\n",
      "be ordered.\n",
      "We deﬁne the binary relations “≤”, “<”,“≥”, and “>” in the ordered ﬁeld\n",
      "S in a way that is consistent with our previous use of those symbols. For\n",
      "example, for x, y ∈S, x < y or y > x implies y −x ∈P .\n",
      "We now deﬁne a stronger ordering.\n",
      "Deﬁnition 0.0.5 (Archimedean ordered ﬁeld)\n",
      "An ordered ﬁeld S deﬁned by the subset P is said to be Archimedean ordered\n",
      "if for all x ∈S and all y ∈P , there exists a positive integer n such that\n",
      "ny > x.\n",
      "An Archimedean ordered ﬁeld must be dense, in the sense that we can ﬁnd\n",
      "an element between any two given elements.\n",
      "Theorem 0.0.3\n",
      "Let S be an Archimedean ordered ﬁeld, and let x, y ∈S such that x < y. Then\n",
      "there exists integers m and n such that m/n ∈S and\n",
      "x < m\n",
      "n < y.\n",
      "Proof.\n",
      "Exercise.\n",
      "The practical application of Theorem 0.0.3 derives from its implication\n",
      "that there exists a rational number between any two real numbers.\n",
      "0.0.4 Linear Spaces\n",
      "An interesting class of spaces are those that have a closed commutative and\n",
      "associative addition operation for all elements, an additive identity, and each\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "635\n",
      "element has an additive inverse (that is, the spaces are Abelian groups under\n",
      "that addition operation), and for which we deﬁne a multiplication of real\n",
      "numbers and elements of the space. Such spaces are called linear spaces, and\n",
      "we will formally deﬁne them below. Linear spaces are also called vector spaces,\n",
      "especially if the elements are n-tuples of real numbers, called real vectors.\n",
      "We denote the addition operation by “+”, the additive identity by “0”,\n",
      "and the multiplication of a real number and an element of the space by juxta-\n",
      "position. (Note that the symbol “+” also denotes the addition in IR and “0”\n",
      "also denotes the additive identity in IR.)\n",
      "Deﬁnition 0.0.6 (linear space)\n",
      "A structure (Ω, +) in which the operator + is commutative and associative,\n",
      "and for which the the multiplication of a real number a and an element of Ω\n",
      "x is deﬁned as a closed operation in Ω(whose value is denoted as ax) is called\n",
      "a linear space if for any x, y ∈Ωand any a, b ∈IR,\n",
      "•\n",
      "a(x + y) = ax + ay,\n",
      "•\n",
      "(a + b)x = ax + bx,\n",
      "•\n",
      "(ab)x = a(bx),\n",
      "•\n",
      "1x = x, where 1 is the multiplicative identity in IR.\n",
      "The “axpy operation”, ax+y, is the fundamental operation in linear spaces.\n",
      "The two most common linear spaces that we will encounter are those whose\n",
      "elements are real vectors and those whose elements are real-valued functions.\n",
      "The linear spaces consisting of real vectors are subspaces of IRd, for some\n",
      "integer d ≥1.\n",
      "Linear spaces, however, may be formed from various types of elements.\n",
      "The elements may be sets, for example. In this case, x ∈Ωwould mean\n",
      "that x = {xi | i ∈I}, “+” may represent set union, and “0” may be ∅.\n",
      "The multiplication between real numbers and elements of the space could\n",
      "be deﬁned in various ways; if the sets in Ωare sets of real numbers, then\n",
      "“ax” above could be interpreted as ordinary multiplication of each element:\n",
      "ax = {axi | i ∈I}.\n",
      "Two synonyms for “linear spaces” are “vector spaces” and “linear mani-\n",
      "folds”, although “vector space” is often often restricted to linear spaces over\n",
      "IRd, and “linear manifold” is used diﬀerently by diﬀerent authors.\n",
      "Linear Combinations, Linear Independence, and Basis Sets\n",
      "Given x1, x2, . . . ∈Ωand c1, c2, . . . ∈IR, P\n",
      "i cixi is called a linear combination.\n",
      "A set of elements x1, x2, . . . ∈Ωare said to be linearly independent if\n",
      "P\n",
      "i cixi = 0 for c1, c2, . . . ∈IR implies that c1 = c2 = · · · = 0.\n",
      "Given a linear space Ωand a set B = {bi} of linearly independent elements\n",
      "of Ωif for any element x ∈Ω, there exist c1, c2, . . . ∈IR such that x = P\n",
      "i cibi,\n",
      "then B is called a basis set of Ω.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "636\n",
      "0 Statistical Mathematics\n",
      "Subsets of Linear Spaces\n",
      "Interesting subsets of a given linear space Ωare formed as {x ; x ∈Ω, g(x) =\n",
      "0}, for example, the plane in IR3 deﬁned by c1x1 + c2x2 + c3x3 = c0 for some\n",
      "constants c0, c1, c2, c3 ∈IR and where (x1, x2, x3) ∈IR3.\n",
      "Many subsets of IRd are of interest because of their correspondence to\n",
      "familiar geometric objects, such as lines, planes, and structures of ﬁnite extent\n",
      "such as cubes, and spheres. An object in higher dimensions that is analogous\n",
      "to a common object in IR3 is often called by the name of the three-dimensional\n",
      "object preceded by “hyper-”; for example, hypersphere.\n",
      "A subset of a linear space may or may not be a linear space. For example,\n",
      "a hyperplane is a linear space only if it goes through the origin. Other subsets\n",
      "of a linear space may have very little in common with a linear space, for\n",
      "example, a hypersphere.\n",
      "A hyperplane in IRd is often of interest in statistics because of its use\n",
      "as a model for how a random variable is aﬀected by covariates. Such a linear\n",
      "manifold in IRd is determined by d points that have the property of aﬃne inde-\n",
      "pendence. A set of elements x1, . . ., xd ∈Ωare said to be aﬃnely independent\n",
      "if x2 −x1, . . ., xd −x1 are linearly independent. Aﬃne independence in this\n",
      "case insures that the points do not lie in a set of less than d −1 dimensions.\n",
      "Inner Products\n",
      "We now deﬁne a useful real-valued binary function on linear spaces.\n",
      "Deﬁnition 0.0.7 (inner product)\n",
      "If Ωis a linear space, an inner product on Ωis a real-valued function, denoted\n",
      "by ⟨x, y⟩for all x and y in Ω, that satisﬁes the following three conditions for\n",
      "all x, y, and z in Ω.\n",
      "1. Nonnegativity and mapping of the identity:\n",
      "if x ̸= 0, then ⟨x, x⟩> 0 and ⟨0, x⟩= ⟨x, 0⟩= ⟨0, 0⟩= 0.\n",
      "2. Commutativity:\n",
      "⟨x, y⟩= ⟨y, x⟩.\n",
      "3. Factoring of scalar multiplication in inner products:\n",
      "⟨ax, y⟩= a⟨x, y⟩for real a.\n",
      "4. Relation of vector addition to addition of inner products:\n",
      "⟨x + y, z⟩= ⟨x, z⟩+ ⟨y, z⟩.\n",
      "Inner products are often called dot products, although “dot product” is\n",
      "often used to mean a speciﬁc inner product.\n",
      "A linear space together with an inner product, (Ω, ⟨·, ·⟩), is called an inner\n",
      "product space.\n",
      "A useful property of inner products is the Cauchy-Schwarz inequality:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "637\n",
      "⟨x, y⟩≤⟨x, x⟩\n",
      "1\n",
      "2 ⟨y, y⟩\n",
      "1\n",
      "2 .\n",
      "(0.0.26)\n",
      "The proof of this is a classic:\n",
      "ﬁrst form the nonnegative polynomial in t,\n",
      "0 ≤⟨tx + y, tx + y⟩= ⟨x, x⟩t2 + 2⟨x, y⟩t + ⟨y, y⟩,\n",
      "and then, because there can be at most one real root in t, require that the\n",
      "discriminant\n",
      "(2⟨x, y⟩)2 −4⟨x, x⟩⟨y, y⟩\n",
      "be nonpositive. Finally, rearrange to get inequality (0.0.26).\n",
      "Inner product spaces give rise to the very useful concept of orthogonality.\n",
      "If (Ω, ⟨·, ·⟩) is an inner product space, the elements x, y ∈Ωif ⟨x, y⟩= 0 we\n",
      "write x ⊥y and say that x and y are orthogonal. There are many instances in\n",
      "which we mention orthogonality, but we defer more discussion to Section 0.3.\n",
      "Norms\n",
      "We now deﬁne a useful function from a linear space to the nonnegative reals.\n",
      "Deﬁnition 0.0.8 (norm)\n",
      "If Ωis a linear space, a norm on Ωis a function, denoted by ∥· ∥, from Ωto\n",
      "¯IR+ that satisﬁes the following three conditions for all x and y in Ω.\n",
      "1. Nonnegativity and mapping of the identity:\n",
      "if x ̸= 0, then ∥x∥> 0, and ∥0∥= 0\n",
      "2. Relation of scalar multiplication to real multiplication:\n",
      "∥ax∥= |a| ∥x∥for real a\n",
      "3. Triangle inequality:\n",
      "∥x + y∥≤∥x∥+ ∥y∥\n",
      "A linear space together with a norm, (Ω, ∥· ∥), is called a normed linear\n",
      "space.\n",
      "Normed linear spaces give rise to the very useful concept of projections.\n",
      "Deﬁnition 0.0.9 (projection)\n",
      "Let (Ω, ∥· ∥) be a normed linear space and let (Λ, ∥· ∥) be a normed linear\n",
      "subspace; that is, (Λ, ∥· ∥) is a normed linear space and Λ ⊆Ω. For x ∈Ω,\n",
      "the projection of x onto Λ is xp such that\n",
      "∥x −xp∥= inf\n",
      "y∈Λ ∥x −y∥.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "638\n",
      "0 Statistical Mathematics\n",
      "We can show that such a xp exists and that it is unique. (See, for example,\n",
      "Bachman and Narici (2000), Section 10.4.) There are many instances in which\n",
      "we mention projections. They arise commonly in linear algebra and we will\n",
      "discuss them in more detail in Section 0.3. Projections are also important in\n",
      "probability and statistics. We discuss them in this context in Section 1.5.3 on\n",
      "page 115.\n",
      "Pseudonorms\n",
      "If, in the ﬁrst condition in Deﬁnition 0.0.8, the requirement if x ̸= 0, then\n",
      "∥x∥> 0 is replaced by\n",
      "1. if x ̸= 0, then ∥x∥≥0 and ∥0∥= 0,\n",
      "the resulting function is called a pseudonorm, or a seminorm. (Here, I am\n",
      "considering these two terms to be synonyms. Some people use one or the\n",
      "other of these terms as I have deﬁned it and use the other term for some\n",
      "diﬀerent functional.)\n",
      "In some linear spaces whose elements are sets, we deﬁne a real nonnega-\n",
      "tive “measure” function on subsets of the space (see Section 0.1). A set whose\n",
      "measure is 0 is negligible for most purposes of interest In some cases, however,\n",
      "∅is not the only set with measure 0. In such contexts, the requirement on\n",
      "a norm that ∥x∥= 0 ⇒x = ∅is generally too restrictive to be useful. One\n",
      "alternative is just to use a pseudonorm in such a context, but the pseudonorm\n",
      "is not restrictive enough to be very useful. The best way out of this dilemma\n",
      "is to deﬁne equivalence classes of sets such that two sets whose diﬀerence has\n",
      "measure zero are equivalent. In such equivalence classes the sets are said to\n",
      "be the same “almost everywhere”. (See page 710 for a more careful develop-\n",
      "ment of these ideas.) Following this approach, we may qualify the implication\n",
      "implicit in the ﬁrst condition in the list above by “almost everywhere”; that\n",
      "is, ∥x∥= 0 implies that the measure of x is 0.\n",
      "Norms and Metrics Induced by an Inner Product\n",
      "A norm can be deﬁned simply in terms of an inner product.\n",
      "Theorem 0.0.4\n",
      "Let ⟨·, ·⟩be an inner product on Ω. Then\n",
      "p\n",
      "⟨x, x⟩, for x in Ω, is a norm on\n",
      "Ω.\n",
      "Proof.\n",
      "Exercise. (Using the properties of ⟨x, x⟩given in Deﬁnition 0.0.7, show that\n",
      "p\n",
      "⟨x, x⟩satisﬁes the properties of Deﬁnition 0.0.8. The only one that is non-\n",
      "trivial is the triangle inequality; Exercise 0.0.10.)\n",
      "Given the inner product, ⟨·, ·⟩, ∥x∥=\n",
      "p\n",
      "⟨x, x⟩, is called the norm induced\n",
      "by that inner product.\n",
      "A metric can be deﬁned simply in terms of a norm.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "639\n",
      "Theorem 0.0.5\n",
      "Let ∥· ∥be a norm on Ω. Then ∥x −y∥, for x, y ∈Ω, is a metric on Ω.\n",
      "Proof.\n",
      "Exercise 0.0.11. (This is easily seen from the deﬁnition of metric, Deﬁni-\n",
      "tion 0.0.1, on page 623.)\n",
      "This metric is said to be induced by the norm ∥· ∥. A pseudonorm induces\n",
      "a pseudometric in the same way.\n",
      "Notice also that we have a sort of converse to Theorem 0.0.5: that is, a\n",
      "metric induces a norm. Given the metric ρ(x, y), ∥x∥= ρ(x, 0) is a norm.\n",
      "(Here, in general, 0 is the additive identity of the linear space.)\n",
      "The terms “normed linear space” and “linear metric space” are therefore\n",
      "equivalent, and we will use them interchangeably.\n",
      "Countable Sequences and Complete Spaces\n",
      "Countable sequences of elements of a linear metric space, {xi}, for i = 1, 2, . . .,\n",
      "are often of interest. The limit of the sequence, that is, limi→∞xi, is of interest.\n",
      "The ﬁrst question, of course, is whether it exists. An oscillating sequence such\n",
      "as −1, +1, −1, +1, . . . does not have a limit. Likewise, a divergent sequence\n",
      "such as 1, 2, 3, . . . does not have a limit. If the sequence has a ﬁnite limit, we\n",
      "say the sequence converges, but the next question is whether it converges to\n",
      "a point in the given linear space.\n",
      "Let A = {xi} be a countable sequence of elements of a linear metric space\n",
      "with metric ρ(·, ·). If for every ϵ > 0, there exists a constant nϵ such that\n",
      "ρ(xn, xm) < ϵ\n",
      "∀m, n > nϵ,\n",
      "(0.0.27)\n",
      "then A is called a Cauchy sequence. Notice that the deﬁnition of a Cauchy\n",
      "sequence depends on the metric.\n",
      "A sequence of elements of a linear metric space converges only if it is a\n",
      "Cauchy sequence.\n",
      "A normed linear space is said to be complete if every Cauchy sequence in\n",
      "the space converges to a point in the space.\n",
      "A complete normed linear space is called a Banach space.\n",
      "A Banach space whose metric arises from an inner product is called a\n",
      "Hilbert space. Equivalently, a Hilbert space is the same as a complete inner\n",
      "product space.\n",
      "Given a metric space (Ω, ρ), the space (Ωc, ρc) is called the completion of\n",
      "(Ω, ρ) if\n",
      "•\n",
      "Ωis dense in Ωc\n",
      "•\n",
      "ρc(x, y) = ρ(x, y) ∀x, y ∈Ω.\n",
      "For any given metric space, the completion exists. We show this by ﬁrst\n",
      "deﬁning an equivalence class of sequences to consist of all sequences {xn} and\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "640\n",
      "0 Statistical Mathematics\n",
      "{ym} such that ρ(xn, ym) →0, and then deﬁning ρc over pairs of equivalence\n",
      "classes.\n",
      "In the foregoing we have assumed very little about the nature of the point\n",
      "sets that we have discussed. Lurking in the background, however, was the very\n",
      "special set of real numbers. Real numbers are involved in the deﬁnition of a\n",
      "linear space no matter the nature of the elements of the space. Real numbers\n",
      "are also required in the deﬁnitions of inner products, norms, and metrics over\n",
      "linear spaces no matter the nature of the elements of the spaces.\n",
      "We now turn to the special linear space, the real number system, and\n",
      "provide speciﬁc instantiations of some the concepts discussed previously. We\n",
      "also introduce additional concepts, which could be discussed in the abstract,\n",
      "but whose relevance depends so strongly on the real number system that\n",
      "abstraction would be a distraction.\n",
      "0.0.5 The Real Number System\n",
      "The most important sets we will work with are sets of real numbers or product\n",
      "sets of real numbers. The set of real numbers together with ordinary addition\n",
      "and multiplication is a ﬁeld (page 631); it is a Hausdorﬀspace (page 623);\n",
      "it is a complete metric space (page 639); it is a connected topological space\n",
      "(page 623); it is a Banach space (page 639); it is a Hilbert space (with the\n",
      "Euclidean norm) (page 639); et cetera, et cetera.\n",
      "We denote the full set of real numbers, that is, the “reals”, by IR, the set\n",
      "of positive real numbers by IR+, and the set of negative real numbers by IR−.\n",
      "For a positive integer d, we denote the product set Qd\n",
      "i=1 IR as IRd. The reals\n",
      "themselves and these three sets formed from them are all clopen.\n",
      "The Extended Reals\n",
      "The reals do not include the two special elements ∞and −∞, but we some-\n",
      "times speak of the “extended reals”, which we denote and deﬁne by\n",
      "IR\n",
      "def\n",
      "= IR ∪{−∞, ∞}.\n",
      "(0.0.28)\n",
      "This notation may seem to imply that IR is the closure of IR, and to some\n",
      "extent, that is the case; however, we should recall that\n",
      "IR =] −∞, ∞[\n",
      "is itself a closed set. (It is also open; that is, it is clopen.)\n",
      "We deﬁne operations with the two elements ∞and −∞as follows (where\n",
      "“×” means multiplication):\n",
      "•\n",
      "∀x ∈IR, −∞< x < ∞\n",
      "•\n",
      "∀x ∈IR, x ± ∞= ±∞+ x = ±∞\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "641\n",
      "•\n",
      "∀x ∈IR ∪{∞}, x × ±∞= ±∞× x = ±∞\n",
      "•\n",
      "∀x ∈IR ∪{−∞}, x × ±∞= ±∞× x = ∓∞\n",
      "•\n",
      "∀x ∈IR, x/ ± ∞= 0\n",
      "•\n",
      "∀x ∈IR+, ±∞/x = ±∞\n",
      "•\n",
      "∀x ∈IR−, ±∞/x = ∓∞.\n",
      "Other operations may or may not be deﬁned, depending on the context.\n",
      "For example, often in probability theory or measure theory, we may use the\n",
      "deﬁnition\n",
      "•\n",
      "0 × ±∞= ±∞× 0 = 0.\n",
      "In numerical mathematics, we may use the deﬁnition\n",
      "•\n",
      "∀x ∈IR, x/0 = sign(x)∞.\n",
      "The other cases,\n",
      "∞−∞,\n",
      "±∞/ ± ∞,\n",
      "±∞/ ∓∞\n",
      "are almost aways undeﬁned; that is, they are considered indeterminate forms.\n",
      "Notice that the extended reals is not a ﬁeld, but in general, all of the laws\n",
      "of the ﬁeld IR hold so long as the operations are deﬁned.\n",
      "The ﬁnite reals without ∞and −∞, are generally more useful. By not\n",
      "including the inﬁnities in the reals, that is, by working with the ﬁeld of reals,\n",
      "we often make the discussions simpler.\n",
      "Important Subsets of the Reals\n",
      "We denote the full set of integers by ZZ, and the set of positive integers by\n",
      "ZZ+. The positive integers are also called the natural numbers. Integers are\n",
      "reals and so ZZ ⊆IR and ZZ+ ⊆IR+. We often seem implicitly to include ∞as\n",
      "an integer in expressions such as P∞\n",
      "i=1. Such an expression, however, should\n",
      "be interpreted as limn→∞\n",
      "Pn\n",
      "i=1.\n",
      "The set of numbers that can be represented in the form a/b, where a, b ∈ZZ,\n",
      "are the rational numbers. The rationals can be mapped to the integers; hence\n",
      "the set of rationals is countable.\n",
      "The closure (page 622) of the set of rational numbers is IR; hence, the\n",
      "rationals are dense in IR (page 622). Because the rationals are countable and\n",
      "are dense in IR, IR is separable (page 622).\n",
      "Norms and Metrics on the Reals\n",
      "The simplest and most commonly used norms on IRd are the Lp norms, which\n",
      "for p ≥1 and x ∈IRd are deﬁned by\n",
      "∥x∥p =\n",
      " d\n",
      "X\n",
      "i=1\n",
      "|xi|p\n",
      "!1/p\n",
      ".\n",
      "(0.0.29)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "642\n",
      "0 Statistical Mathematics\n",
      "For this to be a norm, for any x, y ∈IRd and 1 ≤p, we must have\n",
      " d\n",
      "X\n",
      "i=1\n",
      "|xi + yi|p\n",
      "!1/p\n",
      "≤\n",
      " d\n",
      "X\n",
      "i=1\n",
      "|xi|p\n",
      "!1/p\n",
      "+\n",
      " d\n",
      "X\n",
      "i=1\n",
      "|yi|p\n",
      "!1/p\n",
      ".\n",
      "(0.0.30)\n",
      "This inequality is called Minkowski’s inequality, and hence the norm itself is\n",
      "sometimes called the Minkowski norm.\n",
      "A straightforward proof of Minkowski’s inequality is based on H¨older’s\n",
      "inequality. H¨older’s inequality, using the same notation as above, except with\n",
      "the requirement 1 < p, is\n",
      "d\n",
      "X\n",
      "i=1\n",
      "|xiyi| ≤\n",
      " d\n",
      "X\n",
      "i=1\n",
      "|xi|p\n",
      "!1/p\n",
      "+\n",
      " d\n",
      "X\n",
      "i=1\n",
      "|yi|p/(p−1)\n",
      "!(p−1)/p\n",
      ".\n",
      "(0.0.31)\n",
      "To prove H¨older’s inequality, we use the concavity property of the log\n",
      "function. For 0 < a1, a2 and 1 < p, because 1/p+(p−1)/p = 1, the concavity\n",
      "of the log function yields\n",
      "log(a1)/p + log(a2)(p −1)/p ≤log(a1/p + a2(p −1)/p),\n",
      "or\n",
      "a1/p\n",
      "1\n",
      "a(p−1)/p\n",
      "2\n",
      "≤a1/p + a2(p −1)/p.\n",
      "(0.0.32)\n",
      "Now, if |xi| > 0 and |yi| > 0, let\n",
      "a1 =\n",
      "|xi|p\n",
      "Pd\n",
      "i=1 |xi|p ,\n",
      "a2 =\n",
      "|yi|p/(p−1)\n",
      "Pd\n",
      "i=1 |yi|p/(p−1).\n",
      "Substituting these in equation (0.0.32) and then summing over i, we have\n",
      "d\n",
      "X\n",
      "i=1\n",
      " \n",
      "|xi|p\n",
      "Pd\n",
      "i=1 |xi|p\n",
      "!1/p  \n",
      "|yi|p/(p−1)\n",
      "Pd\n",
      "i=1 |yi|p/(p−1)\n",
      "!(p−1)/p\n",
      "≤\n",
      "d\n",
      "X\n",
      "i=1\n",
      " \n",
      "1\n",
      "p\n",
      "|xi|p\n",
      "Pd\n",
      "i=1 |xi|p + p −1\n",
      "p\n",
      "|yi|p/(p−1)\n",
      "Pd\n",
      "i=1 |yi|p/(p−1)\n",
      "!\n",
      "= 1.\n",
      "Writing the summands in the numerators as |xi| and |yi| and simplifying the\n",
      "resulting expression, we have H¨older’s inequality for the case where all xi and\n",
      "yi are nonzero. We next consider the case where some are zero, and see that\n",
      "the same proof holds; we just omit the zero terms. For the case that all xi\n",
      "and/or yi are zero, the inequality holds trivially.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "643\n",
      "For Minkowski’s inequality, we ﬁrst observe that it holds if p = 1, and it\n",
      "holds trivially if either Pd\n",
      "i=1 |xi| = 0 or Pd\n",
      "i=1 |yi| = 0. For p > 1, we ﬁrst\n",
      "obtain from the left side of inequality (0.0.30)\n",
      "d\n",
      "X\n",
      "i=1\n",
      "|xi + yi|p =\n",
      "d\n",
      "X\n",
      "i=1\n",
      "|xi + yi||xi + yi|p−1\n",
      "≤\n",
      "d\n",
      "X\n",
      "i=1\n",
      "|xi||xi + yi|p−1 +\n",
      "d\n",
      "X\n",
      "i=1\n",
      "|yi||xi + yi|p−1.\n",
      "We next apply H¨older’s inequality, and simplify under the assumption that\n",
      "neither Pd\n",
      "i=1 |xi| = 0 nor Pd\n",
      "i=1 |yi| = 0.\n",
      "Because H¨older’s inequality implies Minkowski’s inequality, the Lp norm is\n",
      "also called the H¨older norm. (As with many things in mathematics, the impli-\n",
      "cations of eponyms are ambiguous. Otto H¨older stated what we call H¨older’s\n",
      "inequality in an open publication before Hermann Minkowski published what\n",
      "we call Minkowski’s inequality. But who knows?)\n",
      "We see from the deﬁnition that ∥x∥p is a nonincreasing function in p, that\n",
      "is,\n",
      "1 ≤p1 < p2 ⇒∥x∥p1 ≥∥x∥p2.\n",
      "(0.0.33)\n",
      "(Exercise.)\n",
      "In the case of d = 1, the Lp is just the absolute value for any p; that is,\n",
      "for x ∈IR, ∥x∥= |x|.\n",
      "Because\n",
      "lim\n",
      "p→∞∥x∥p = max({|xi|}),\n",
      "(0.0.34)\n",
      "we formally deﬁne the L∞norm of the vector x = (x1, . . ., xd) as max({|xi|}).\n",
      "It is clear that it satisﬁes the properties of a norm.\n",
      "Metrics are often deﬁned in terms of norms of diﬀerences (Theorem 0.0.5).\n",
      "Because the Lp norm is a common norm on IRd, a common metric on IRd is\n",
      "the Lp metric, ρp(x, y) = ∥x −y∥p.\n",
      "The Lp metric is also called the Minkowski metric or the Minkowski dis-\n",
      "tance.\n",
      "The most common value of p is 2, in which case we have the Euclidean\n",
      "metric (or Euclidean distance):\n",
      "∥x −y∥2 =\n",
      " d\n",
      "X\n",
      "i=1\n",
      "(xi −yi)2\n",
      "!1/2\n",
      ".\n",
      "(0.0.35)\n",
      "We often write the Euclidean distance between x and y as ∥x −y∥.\n",
      "Any of these metrics allows us to deﬁne neighborhoods and open sets, and\n",
      "to deﬁne convergence of sequences.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "644\n",
      "0 Statistical Mathematics\n",
      "Ordering the Reals\n",
      "The ﬁeld of real numbers IR is complete and Archimedean ordered; in fact,\n",
      "the reals can be deﬁned as a (any) complete Archimedean ordered ﬁeld. The\n",
      "ordering properties correspond to our intuitive understanding of ordering.\n",
      "The more useful ordering of the reals is the linear ordering that results\n",
      "from the usual inequality relation. On the other hand, IRd cannot be linearly\n",
      "ordered in an intuitive or universally useful way. An ordering of x, y ∈IRd\n",
      "based on notions of x < y or x ≤y are rarely useful in statistical applications\n",
      "(see Section 0.0.1 on page 620). Most useful orderings are based on relations\n",
      "between ∥x∥and ∥y∥, but their usefulness depends on the application. (See\n",
      "Gentle (2009), pages 538 to 549, for some discussion of this.)\n",
      "In order to simplify the following discussion, we will focus on subsets of\n",
      "IR. Because the reals are not a well-ordered set using the usual inequality\n",
      "relation, we ﬁnd it convenient to deﬁne limiting maxima and minima based\n",
      "on that relation.\n",
      "For X ⊆IR, the supremum of X or least upper bound of X, is the number\n",
      "x∗= sup(X),\n",
      "(0.0.36)\n",
      "deﬁned by\n",
      "∀x ∈X, x ≤x∗\n",
      "and\n",
      "∀ϵ > 0, ∃x ∈X ∋x > x∗−ϵ.\n",
      "The inﬁmum of X, that is, the greatest lower bound of X, is written as\n",
      "x∗= inf(X)\n",
      "(0.0.37)\n",
      "and is deﬁned similarly, with the inequalities reversed.\n",
      "Examples:\n",
      "•\n",
      "Let A = {x}. Then sup(A) = inf(A) = x.\n",
      "•\n",
      "Let A = {x | x = 1/i, i ∈ZZ+}. Then sup(A) = 1 and inf(A) = 0. Notice\n",
      "that inf(A) /∈A.\n",
      "•\n",
      "Let A = {x | i, i ∈ZZ+}. Then sup(A) = ∞and inf(A) = 1. Alternatively,\n",
      "we may say that sup(A) does not exist. In any event, sup(A) /∈A.\n",
      "An important fundamental property of the reals is that every bounded\n",
      "set of reals has a supremum that is a real number. This property is often\n",
      "called Dedekind completeness. In the usual axiomatic development of the reals,\n",
      "Dedekind completeness is an axiom.\n",
      "The maximum of a well-ordered set is the largest element of the set, if it\n",
      "exists; likewise, the minimum of a well-ordered set is the smallest element of\n",
      "the set, if it exists. The maximum and/or the minimum may not exist if the\n",
      "set has an inﬁnite number of elements. This can happen in two ways: one, the\n",
      "set may have no bound; and another, the bound may not be in the set.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "645\n",
      "Sets of Reals; Open, Closed, Compact\n",
      "For sets of reals we can redeﬁne various topological properties in a way that\n",
      "is simpler but yet consistent with our discussion of general topologies on\n",
      "page 621. These concepts yield important kinds of sets, such as open, closed,\n",
      "compact, and convex sets (see page 658).\n",
      "A set A of reals is called open if for each x ∈A, there exists a δ > 0 such\n",
      "that for each y with ∥x −y∥< δ belongs to A.\n",
      "If A is a set of reals and if for a given x ∈A, there exists a δ > 0 such that\n",
      "for each y with ∥x −y∥< δ belongs to A, then x is called an interior point of\n",
      "A.\n",
      "We denote the set of all interior points of A as\n",
      "A◦\n",
      "and call it the interior of A. Clearly A◦is open, and, in fact, it is the union\n",
      "of all open subsets of A.\n",
      "A real number (vector) x is called a point of closure of a set A of real\n",
      "numbers (vectors) if for every δ > 0 there exists a y in A such that ∥x−y∥< δ.\n",
      "(Notice that every y ∈A is a point of closure of A.)\n",
      "We denote the set of points of closure of A by\n",
      "A,\n",
      "and a set A is called closed if A = A. (This is the same deﬁnition and notation\n",
      "as for any topology.)\n",
      "The boundary of the set A, denoted ∂A, is the set of points of closure of\n",
      "A that are not interior points of A; that is,\n",
      "∂A = A −A◦.\n",
      "(0.0.38)\n",
      "As we have mentioned above, in any topological space, a set A is said\n",
      "to be compact if each collection of open sets that covers A contains a ﬁnite\n",
      "subcollection of open sets that covers A. Compactness of subsets of the reals\n",
      "can be characterized simply, as in the following theorem.\n",
      "Theorem 0.0.6 (Heine-Borel theorem)\n",
      "A set of real numbers is compact iﬀit is closed and bounded.\n",
      "For a proof of this theorem, see a text on real analysis, such as Hewitt and Stromberg\n",
      "(1965). Because of the Heine-Borel theorem, we often take closed and bounded\n",
      "as the deﬁnition of a compact set of reals.\n",
      "Intervals in IR\n",
      "A very important type of set is an interval in IR, which is a connected subset\n",
      "of IR. Intervals are the basis for building important structures on IR.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "646\n",
      "0 Statistical Mathematics\n",
      "We denote an open interval with open square brackets; for example ]a, b[\n",
      "is the set of all real x such that a < x < b. We denote a closed interval with\n",
      "closed square brackets; for example [a, b] is the set of all real x such that\n",
      "a ≤x ≤b. We also have “half-open” or “half-closed” intervals, with obvious\n",
      "meanings.\n",
      "The main kinds of intervals have forms such as\n",
      "] −∞, a[,\n",
      "] −∞, a],\n",
      "]a, b[,\n",
      "[a, b],\n",
      "]a, b],\n",
      "[a, b[,\n",
      "]b, ∞[,\n",
      "and [b, ∞[.\n",
      "Of these,\n",
      "•\n",
      "] −∞, a[,\n",
      "]a, b[,\n",
      "and ]b, ∞[ are open;\n",
      "•\n",
      "[a, b] is closed and ]a, b[ = [a, b];\n",
      "•\n",
      "]a, b] and [a, b[ are neither (they are “half-open”);\n",
      "•\n",
      "] −∞, a] and [b, ∞[ are closed, although in a special way that sometimes\n",
      "requires special treatment.\n",
      "A ﬁnite closed interval is compact (by the Heine-Borel theorem); but an\n",
      "open or half-open interval is not, as we see below.\n",
      "The following facts for real intervals are special cases of the properties of\n",
      "unions and intersections of open and closed sets we listed above, which can\n",
      "be shown from the deﬁnitions:\n",
      "•\n",
      "∩n\n",
      "i=1]ai, bi[=]a, b[ (that is, some open interval)\n",
      "•\n",
      "∪∞\n",
      "i=1]ai, bi[ is an open set\n",
      "•\n",
      "∪n\n",
      "i=1[ai, bi] is a closed set\n",
      "•\n",
      "∩∞\n",
      "i=1[ai, bi] = [a, b] (that is, some closed interval).\n",
      "Two types of interesting intervals are\n",
      "\u0015\n",
      "a −1\n",
      "i , b + 1\n",
      "i\n",
      "\u0014\n",
      "(0.0.39)\n",
      "and\n",
      "\u0014\n",
      "a + 1\n",
      "i , b −1\n",
      "i\n",
      "\u0015\n",
      ".\n",
      "(0.0.40)\n",
      "Sequences of intervals of these two forms are worth remembering because\n",
      "they illustrate interesting properties of intersections and unions of inﬁnite\n",
      "sequences. Inﬁnite intersections and unions behave diﬀerently with regard to\n",
      "collections of open and closed sets. For ﬁnite intersections and unions we know\n",
      "that ∩n\n",
      "i=1]ai, bi[ is an open interval, and ∪n\n",
      "i=1[ai, bi] is a closed set.\n",
      "First, observe that\n",
      "lim\n",
      "i→∞\n",
      "\u0015\n",
      "a −1\n",
      "i , b + 1\n",
      "i\n",
      "\u0014\n",
      "= [a, b]\n",
      "(0.0.41)\n",
      "and\n",
      "lim\n",
      "i→∞\n",
      "\u0014\n",
      "a + 1\n",
      "i , b −1\n",
      "i\n",
      "\u0015\n",
      "= [a, b].\n",
      "(0.0.42)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "647\n",
      "Now for ﬁnite intersections of the open intervals and ﬁnite unions of the\n",
      "closed intervals, that is, for ﬁnite k, we have\n",
      "k\\\n",
      "i=1\n",
      "\u0015\n",
      "a −1\n",
      "i , b + 1\n",
      "i\n",
      "\u0014\n",
      "is open\n",
      "and\n",
      "k[\n",
      "i=1\n",
      "\u0014\n",
      "a + 1\n",
      "i , b −1\n",
      "i\n",
      "\u0015\n",
      "is closed.\n",
      "Inﬁnite intersections and unions behave diﬀerently with regard to collec-\n",
      "tions of open and closed sets. With the open and closed intervals of the special\n",
      "forms, for inﬁnite intersections and unions, we have the important facts:\n",
      "∞\n",
      "\\\n",
      "i=1\n",
      "\u0015\n",
      "a −1\n",
      "i , b + 1\n",
      "i\n",
      "\u0014\n",
      "= [a, b]\n",
      "(0.0.43)\n",
      "and\n",
      "∞\n",
      "[\n",
      "i=1\n",
      "\u0014\n",
      "a + 1\n",
      "i , b −1\n",
      "i\n",
      "\u0015\n",
      "=]a, b[.\n",
      "(0.0.44)\n",
      "These equations follow from the deﬁnitions of intersections and unions. To see\n",
      "equation (0.0.44), for example, we note that a ∈∪Ai iﬀa ∈Ai for some i;\n",
      "hence, if a ̸∈Ai for any i, then a ̸∈∪Ai.\n",
      "Likewise, we have\n",
      "∞\n",
      "[\n",
      "i=1\n",
      "\u0014\n",
      "a + 1\n",
      "i , b\n",
      "\u0015\n",
      "=\n",
      "∞\n",
      "\\\n",
      "i=1\n",
      "\u0015\n",
      "a, b + 1\n",
      "i\n",
      "\u0014\n",
      "= ]a, b].\n",
      "(0.0.45)\n",
      "From this we see that\n",
      "lim\n",
      "n→∞\n",
      "n\n",
      "[\n",
      "i=1\n",
      "\u0014\n",
      "a + 1\n",
      "i , b −1\n",
      "i\n",
      "\u0015\n",
      "̸=\n",
      "[\n",
      "lim\n",
      "i→∞\n",
      "\u0014\n",
      "a + 1\n",
      "i , b −1\n",
      "i\n",
      "\u0015\n",
      ".\n",
      "(0.0.46)\n",
      "Equations (0.0.44) and (0.0.45) for ]a, b[ and ]a, b] above show that open\n",
      "intervals and half-open intervals are not compact, because no ﬁnite collection\n",
      "of sets in the unions cover the intervals.\n",
      "Intervals in IRd\n",
      "“Intervals” in IRd are merely product sets of intervals in IR; that is, they are\n",
      "hyperrectangles. Because of the possibilities of the types of endpoints of the\n",
      "intervals in IR, the intervals in IRd cannot always be speciﬁed using “[, ]” “], [”,\n",
      "and so on. In more restrictive cases in which all of the intervals in IR are of\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "648\n",
      "0 Statistical Mathematics\n",
      "the same types, we use the same notation as above to indicate the product\n",
      "sets. For example, given the vectors a = (a1, . . ., ad) and b = (b1, . . ., bd), we\n",
      "could write ]a, b] with the meaning\n",
      "]a, b] =]a1, b1] × · · · ×]ad, bd].\n",
      "Sequences of Reals, Limit Points, and Accumulation Points\n",
      "Because of the Heine-Borel theorem, bounded sequence in IRd are of interest.\n",
      "(A sequence {xn} is bounded if there exists an M ∈IRd ∋xn ≤M ∀n.) We\n",
      "let {xn} = x1, x2, . . . be a sequence in IRd. The Bolzano-Weierstrass theorem\n",
      "(see below) states that every bounded sequence has a convergent subsequence;\n",
      "that is, there is a subsequence that has a limit point. A point x ∈IRd is called\n",
      "an accumulation point, or a cluster point, of {xn} if there is a subsequence\n",
      "{xni} that converges to x.\n",
      "An important property of the reals is that a sequence of reals converges if\n",
      "and only if it is a Cauchy sequence. (The “if” part means that the reals are\n",
      "complete.) This is sometimes called the “Cauchy criterion”. See Exercise 0.0.6\n",
      "for an outline of a proof or see Hewitt and Stromberg (1965).\n",
      "Notice that the ﬁeld of rationals is not complete for we can form a Cauchy\n",
      "sequence in the rationals that does not converge to a rational number. For\n",
      "example consider the rational sequence\n",
      "x1 = 1\n",
      "xn = xn−1/2 + 1/xn−1,\n",
      "n = 2, 3, . . .\n",
      "which is a Cauchy sequence (in the Euclidean metric). The sequence, however,\n",
      "converges to\n",
      "√\n",
      "2, which is not in the rationals. The ﬁeld of the reals can, in\n",
      "fact, be considered to be a completion of the rationals in the Euclidean norm.\n",
      "A linear space formed on IRd with the usual addition operation for vectors\n",
      "together with any metric is a Banach space. If the metric is taken to be the\n",
      "L2 metric then that linear space is a Hilbert space.\n",
      "Monotone Sequences in IR\n",
      "We will now limit the discussion to subsets and sequences in IR. This allows\n",
      "us to use a simple deﬁnition of monotonicity, based on the linear ordering of\n",
      "the reals.\n",
      "A useful theorem tells us that if a bounded sequence is monotone, then\n",
      "the sequence must converge:\n",
      "Theorem 0.0.7 (monotone convergence of sequence in IR)\n",
      "Let x1 ≤x2 ≤· · · be a sequence in IR. This sequence has a ﬁnite limit iﬀthe\n",
      "sequence is bounded.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "649\n",
      "Proof.\n",
      "First, obviously if limn→∞xn = x < ∞, then every xn ≤x, and the sequence\n",
      "is bounded.\n",
      "Now we assume x1 ≤x2 ≤· · · x < ∞. By Dedekind completeness of the\n",
      "reals, x∗= sup({xn}) exists and is ﬁnite. For every ϵ > 0, there exists xN\n",
      "such that xN > x∗−ϵ because otherwise x∗−ϵ would be an upper bound less\n",
      "than sup({xn}). So now, because {xn} is increasing, ∀n > N we have\n",
      "|x∗−xn| = x∗−xn ≤x∗−xN < ϵ;\n",
      "therefore, the limit of {xn} is sup({xn}).\n",
      "Theorem 0.0.8 (Bolzano-Weierstrass)\n",
      "Every bounded sequence in IR has a convergent subsequence.\n",
      "Proof.\n",
      "This theorem is an immediate result of the following lemma.\n",
      "Lemma 0.0.8.1\n",
      "Every sequence in IR has a monotone subsequence.\n",
      "Proof.\n",
      "Deﬁne a dominant term (or a peak) as a term xj in a sequence {xn} such that\n",
      "xj > xj+1, xj+2, . . .. The number of dominant terms must either be ﬁnite or\n",
      "inﬁnite.\n",
      "Suppose the number is inﬁnite. In that case, we can form a subsequence\n",
      "that contains only those dominant terms {xni}. This sequence is (strictly)\n",
      "monotonic decreasing, xni > xni+1 > · · ·.\n",
      "On the other hand, suppose the number of dominant terms is ﬁnite. If\n",
      "there is a dominant term, consider the next term after the last dominant\n",
      "term. Call it xn1. If there are no dominant terms, n1 = 1. Now, since xn1 is\n",
      "not a dominant term, there must be another term, say xn2, that is no greater\n",
      "than xn1, and since that term is not dominant, there must be a term xn3 that\n",
      "is no greater than xn2, and so on. The sequence {xni} formed in this way is\n",
      "monotonic nondecreasing.\n",
      "The Bolzano-Weierstrass theorem is closely related to the Heine-Borel the-\n",
      "orem. Either can be used in the proof of the other. A system for which the\n",
      "Bolzano-Weierstrass theorem holds is said to have the Bolzano-Weierstrass\n",
      "property.\n",
      "Some properties of sequences or subsequences in IR that we discuss that\n",
      "depend on xi ≤xj can often be extended easily to sequences in IRd using\n",
      "the partial ordering imposed by applying the IR ordering element by element.\n",
      "For example, if {xn} is a sequence in IRd, then Lemma 0.0.8.1 could ﬁrst be\n",
      "applied to the ﬁrst element in each vector xn to form a subsequence based\n",
      "on the ﬁrst element, and then applied to the second element in each vector\n",
      "in this subsequence to form a subsubsequence, and then applied to the third\n",
      "element of each vector in the subsubsequence, and so on. The deﬁnition of\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "650\n",
      "0 Statistical Mathematics\n",
      "the order in IRd may require some redeﬁnition of the order that arises from\n",
      "applying the IR order directly (see Exercise 0.0.7).\n",
      "Theorem 0.0.9 is an alternate statement of the Bolzano-Weierstrass theo-\n",
      "rem.\n",
      "Theorem 0.0.9 (Bolzano-Weierstrass (alternate))\n",
      "Every bounded sequence in IR has an accumulation point.\n",
      "We will prove this statement of the theorem directly, because in doing so we\n",
      "are led to the concept of a largest accumulation point, which has more general\n",
      "uses.\n",
      "Proof.\n",
      "For the bounded sequence {xn}, deﬁne the set of real numbers\n",
      "S = {x | there are inﬁnitely many xn > x}.\n",
      "Let x∗= sup(S). Because the sequence is bounded, x∗is ﬁnite. By deﬁnition\n",
      "of S and sup(S), for any ϵ > 0, only there are only ﬁnitely many xn such that\n",
      "xn ≥x∗+ ϵ, but there are inﬁnitely many xn such that xn ≥x∗−ϵ, so there\n",
      "are inﬁnitely many xn in the interval [x∗−ϵ, x∗+ ϵ].\n",
      "Now for i = 1, 2, . . ., consider the intervals Ii = [x∗−1/i, x∗+ 1/i] each of\n",
      "which contains inﬁnitely many xn, and form a monotone increasing sequence\n",
      "{ni} such that xni ∈Ii. (Such a sequence is not unique.) Now use the sequence\n",
      "{ni} to form a subsequence of {xn}, {xni}. The sequence {xni} converges to\n",
      "x∗; which is therefore an accumulation point of {xn}.\n",
      "lim sup and lim inf\n",
      "Because of the way S was deﬁned in the proof of Theorem 0.0.9, the accu-\n",
      "mulation point x∗= sup(S) is the largest accumulation point of {xn}. The\n",
      "largest accumulation point of a sequence is an important property of that se-\n",
      "quence. We call the largest accumulation point of the sequence {xn} the limit\n",
      "superior of the sequence and denote it as lim supn xn. If the sequence is not\n",
      "bounded from above, we deﬁne lim supn xn as ∞. We have\n",
      "lim sup\n",
      "n\n",
      "xn = lim\n",
      "n sup\n",
      "k≥n\n",
      "xk.\n",
      "(0.0.47)\n",
      "We see that\n",
      "lim sup\n",
      "n\n",
      "xn = sup{x | there are inﬁnitely many xn > x},\n",
      "(0.0.48)\n",
      "which is a characterization of lim sup for any nonincreasing real point sequence\n",
      "{xn}. (Compare this with equation (0.0.22) on page 628.)\n",
      "Likewise, for a bounded sequence, we deﬁne the smallest accumulation\n",
      "point of the sequence {xn} the limit inferior of the sequence and denote it as\n",
      "lim infn xn. If the sequence is not bounded from below, we deﬁne lim infn xn\n",
      "as −∞. We have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "651\n",
      "lim inf\n",
      "n\n",
      "xn = lim\n",
      "n inf\n",
      "k≥n xk.\n",
      "(0.0.49)\n",
      "We have\n",
      "lim inf\n",
      "n\n",
      "xn = inf{x | there are inﬁnitely many xn < x}.\n",
      "(0.0.50)\n",
      "The properties of lim sup and lim inf of sequences of sets discussed on\n",
      "page 627 have analogues for lim sup and lim inf of sequences of points.\n",
      "For a bounded sequence {xn}, it is clear that\n",
      "lim inf\n",
      "n\n",
      "xn ≤lim sup\n",
      "n\n",
      "xn,\n",
      "(0.0.51)\n",
      "and {xn} converges iﬀlim infn xn = lim supn xn, and in that case we write\n",
      "the quantity simply as limn xn.\n",
      "We also have\n",
      "lim sup\n",
      "n\n",
      "xn = inf\n",
      "n sup\n",
      "k≥n\n",
      "xk\n",
      "(0.0.52)\n",
      "and\n",
      "lim inf\n",
      "n\n",
      "xn = sup\n",
      "n\n",
      "inf\n",
      "k≥n xk.\n",
      "(0.0.53)\n",
      "The triangle inequalities also hold:\n",
      "lim sup\n",
      "n\n",
      "(xn + yn) ≤lim sup\n",
      "n\n",
      "xn + lim sup\n",
      "n\n",
      "yn.\n",
      "(0.0.54)\n",
      "lim inf\n",
      "n\n",
      "(xn + yn) ≥lim inf\n",
      "n\n",
      "xn + lim inf\n",
      "n\n",
      "yn.\n",
      "(0.0.55)\n",
      "Common Sequences of Reals\n",
      "There are some forms of sequences that arise often in applications; for ex-\n",
      "ample, xn = 1/n or xn = 1 + c/n. Having lists of convergent sequences or\n",
      "convergent series (see Sections 0.0.5 and 0.0.9) can be a useful aid in work in\n",
      "mathematics.\n",
      "A useful limit of sequences of reals that we will encounter from time to\n",
      "time is\n",
      "lim\n",
      "n→∞\n",
      "\u0010\n",
      "1 + c\n",
      "n\n",
      "\u0011n\n",
      "= ec.\n",
      "(0.0.56)\n",
      "We can prove this easily using some simple properties of the logarithm func-\n",
      "tion, which we deﬁne as L(t) = R t\n",
      "1 (1/x)dx for t > 0. We ﬁrst observe that\n",
      "L is continuous and increasing, L(1) = 0, that L′ exists at 1, L′(1) = 1, and\n",
      "nL(x) = L(xn). For a ﬁxed constant c ̸= 0 we can write the derivative at 1 as\n",
      "lim\n",
      "n→∞\n",
      "L(1 + c/n) −L(1)\n",
      "c/n\n",
      "= 1,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "652\n",
      "0 Statistical Mathematics\n",
      "which, because L(1) = 0, we can rewrite as limn→∞L((1 + c/n)n) = c. Since\n",
      "L is continuous and increasing limn→∞(1 + c/n)n exists and is the value of x\n",
      "such that L(x) = c; that is, it is ec.\n",
      "A related limit for a function g(n) that has the limit limn→∞= b is\n",
      "lim\n",
      "n→∞\n",
      "\u0012\n",
      "1 + cg(n)\n",
      "n\n",
      "\u0013n\n",
      "= ebc,\n",
      "(0.0.57)\n",
      "which can be shown easily by use of the limit above, and the bounds\n",
      "\u0012\n",
      "1 + c(b −ϵ)\n",
      "n\n",
      "\u0013n\n",
      "≤\n",
      "\u0012\n",
      "1 + cg(n)\n",
      "n\n",
      "\u0013n\n",
      "≤\n",
      "\u0012\n",
      "1 + c(b + ϵ)\n",
      "n\n",
      "\u0013n\n",
      ",\n",
      "for c > 0 and any ϵ > 0, which arise from the bounds b −ϵ < g(n) < b + ϵ for\n",
      "n suﬃciently large. Taking limits, we get\n",
      "ec(b−ϵ) ≤lim\n",
      "n→∞\n",
      "\u0012\n",
      "1 + cg(n)\n",
      "n\n",
      "\u0013n\n",
      "≤ec(b+ϵ),\n",
      "and since ϵ was arbitrary, we have the desired conclusion under the assumption\n",
      "that c > 0. We get the same result (with bounds reversed) for c < 0.\n",
      "Another related limit is for a function g(n) that has the limit limn→∞g(n) =\n",
      "0, and constants b and c with c ̸= 0 is\n",
      "lim\n",
      "n→∞\n",
      "\u0012\n",
      "1 + c\n",
      "n + g(n)\n",
      "n\n",
      "\u0013b\n",
      "n = ebc.\n",
      "(0.0.58)\n",
      "The Rate of Convergence; Big O and Little o Notation\n",
      "We are often interested in how quickly one sequence of real numbers or of real-\n",
      "valued functions converges to another sequence. We will distinguish two types\n",
      "limiting behavior, and the rate of convergence is measured by asymptotic\n",
      "ratios of the sequences.\n",
      "We consider a class determined by the rate of a given sequence {an} ∈IRd.\n",
      "We identify another sequence {bn} ∈IRd as belonging to the order class if its\n",
      "rate of convergence is similar. If these are sequences of functions, we assume\n",
      "a common domain for all functions in the sequences and our comparisons of\n",
      "an(x) and bn(x) are for all points x in the domain. We refer to one type of\n",
      "limiting behavior as “big O” and to another type as “little o”.\n",
      "Big O, written O(an).\n",
      "{bn} ∈O(an) means there exists some ﬁxed ﬁnite c such that ∥bn∥≤\n",
      "c∥an∥∀n.\n",
      "In particular, bn ∈O(1) means bn is bounded.\n",
      "Little o, written o(an).\n",
      "{bn} ∈o(an) means ∥bn∥/∥an∥→0 as n →∞.\n",
      "In particular, bn ∈o(1) means bn →0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "653\n",
      "Instead of “{bn} ∈O(an)” or “{bn} ∈o(an)”, most people write “{bn} =\n",
      "O(an)” or “{bn} = o(an)”. (From this, we could deduce such nonsense as\n",
      "n = n2, since it is clear, in this notation that n = O(n2) and n2 = O(n2).)\n",
      "I do not like this level of imprecision in notation. I so sometimes abuse this\n",
      "notation slightly, for example, by referring to a sequence as “being O(f(n))”\n",
      "rather than as “being in the order class O(f(n))”. In one very common case, I\n",
      "abuse the notation in this way. As most people, I may use O(f(n)) to represent\n",
      "some unspeciﬁed scalar or vector x ∈O(f(n)) in the case of a convergent\n",
      "series, for example,\n",
      "s = f1(n) + · · · + fk(n) + O(f(n)),\n",
      "where f1(n), . . ., fk(n) are constants. I also use o(f(n)) to represent some\n",
      "unspeciﬁed scalar or vector x ∈o(f(n)) in special case of a convergent series,\n",
      "as above:\n",
      "s = f1(n) + · · · + fd(n) + o(f(n)).\n",
      "We often write bn ∈O(an) or bn ∈O(an) instead of {bn} ∈O(an) or\n",
      "{bn} ∈O(an).\n",
      "We sometimes omit the arguments of functions; for example, we may write\n",
      "f ∈O(g), with the understanding that the limits are taken with respect to\n",
      "the arguments.\n",
      "The deﬁning sequence an is often a simple expression in n; for examples,\n",
      "an = n−2, an = n−1, an = n−1/2, and so on, or an = n, an = n log(n),\n",
      "an = n2, and so on. We have\n",
      "O(1) ⊆O(n−2) ⊆O(n−1) ⊆O(n) ⊆O(n log(n)) ⊆O(n2)\n",
      "etc.\n",
      "(0.0.59)\n",
      "Our interests in these orders are generally diﬀerent for decreasing functions of\n",
      "n than for increasing sequences in n. The former are often used to measure how\n",
      "quickly an error rate goes to zero, and the latter are often used to evaluate the\n",
      "speed of an algorithm as the problem size grows. In either case, it is important\n",
      "to recognize that the order expressed in big O (or little o) is a lower bound, as\n",
      "indicated in expression (0.0.59). (There are variations on the big O concept\n",
      "referred to as “big Ω” and “big Θ” that specify upper bounds and two-sided\n",
      "bounds.)\n",
      "Some additional properties of big O classes are the following.\n",
      "bn ∈O(an), dn ∈O(cn) =⇒bndn ∈O(ancn),\n",
      "(0.0.60)\n",
      "bn ∈O(an), dn ∈O(cn) =⇒bn + dn ∈O(∥an∥∥cn∥),\n",
      "(0.0.61)\n",
      "O(can) = O(an)\n",
      "for constant c.\n",
      "(0.0.62)\n",
      "The proofs of these are left as exercises. Similar results hold for little o classes.\n",
      "In probability and statistics, the sequences may involve random variables,\n",
      "in which case we may need to distinguish the type of convergence. If the con-\n",
      "vergence is almost sure, there is little diﬀerence whether or not the sequence\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "654\n",
      "0 Statistical Mathematics\n",
      "involves random variables. Weak convergence, however, results in diﬀerent\n",
      "types of measures for the rate of convergence, and we ﬁnd the related con-\n",
      "cepts of big O in probability, OP, and little o in probability, oP, useful; see\n",
      "page 83.\n",
      "Sums of Sequences of Reals\n",
      "Sums of countable sequences of real numbers {xi}, for i = 1, 2, . . ., are often\n",
      "of interest. A sum of a countable sequence of real numbers is called a (real)\n",
      "series. The usual question is what is limn→∞\n",
      "Pn\n",
      "i=1 xi. If this limit is ﬁnite,\n",
      "the series is called convergent, or the series is said to converge; otherwise, the\n",
      "series is called divergent, or the series is said to diverge. If limn→∞\n",
      "Pn\n",
      "i=1 |xi| is\n",
      "ﬁnite, the series is called absolutely convergent, or the series is said to converge\n",
      "absolutely.\n",
      "We often simply write Pxi to mean limn→∞\n",
      "Pn\n",
      "i=1 xi, and often we restrict\n",
      "the meaning of “series” to refer to this limit; however, I may occasionally use\n",
      "the word “series” to refer to a sum of a ﬁnite number of elements.\n",
      "A useful way to investigate sums of sequences of reals is by use of partial\n",
      "sums. When we are interested in P xi, we form the partial sum,\n",
      "Sk =\n",
      "k\n",
      "X\n",
      "i=1\n",
      "xi,\n",
      "where k is some integer. Clearly, assuming the xis are ﬁnite, Sk is ﬁnite. The\n",
      "use of partial sums can be illustrated by considering the geometric series,\n",
      "which is the sum of the geometric progression, a, ar, ar2, . . .. Let\n",
      "Sk =\n",
      "k\n",
      "X\n",
      "i=0\n",
      "ari.\n",
      "Multiplying both sides by r and subtracting the resulting equation, we have\n",
      "(1 −r)Sk = a(1 −rk+1),\n",
      "which yields for the partial sum\n",
      "Sk = a1 −rk+1\n",
      "1 −r\n",
      ".\n",
      "This formula is useful for ﬁnite sums, but its main use is for the series. If\n",
      "|r| < 1, then\n",
      "∞\n",
      "X\n",
      "i=0\n",
      "ari = lim\n",
      "k→∞Sk =\n",
      "a\n",
      "1 −r .\n",
      "If |r| > 1, then the series diverges.\n",
      "Another important fact about series, called Kronecker’s lemma, is useful\n",
      "in proofs of theorems about sums of independent random variables, such as\n",
      "the strong law of large numbers (Theorem 1.52, page 103):\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "655\n",
      "Theorem 0.0.10 (Kronecker’s Lemma) Let {xi | i = 1, 2, . . .} and {ai | i =\n",
      "1, 2, . . .} be sequences of real numbers such that P∞\n",
      "i=1 xi exists (and is ﬁnite),\n",
      "and 0 < a1 ≤a2 ≤... and an →∞. Then\n",
      "lim\n",
      "n→∞\n",
      "1\n",
      "an\n",
      "n\n",
      "X\n",
      "i=1\n",
      "aixi = 0.\n",
      "Proof. Form the partial sums in xi, Sk and Sn, with k < n. We have\n",
      "1\n",
      "an\n",
      "n\n",
      "X\n",
      "i=1\n",
      "aixi = Sn −1\n",
      "an\n",
      "n−1\n",
      "X\n",
      "i=1\n",
      "(ai+1 −ai)Sk.\n",
      "Let s = P∞\n",
      "i=1 xi, and for any ϵ > 0, let N be such that for n > N, |Sn−s| < ϵ.\n",
      "We can now write the left-hand side of the equation above as\n",
      "Sn −1\n",
      "an\n",
      "N−1\n",
      "X\n",
      "i=1\n",
      "(ai+1 −ai)Sk −1\n",
      "an\n",
      "n−1\n",
      "X\n",
      "i=N\n",
      "(ai+1 −ai)Sk\n",
      "= Sn −1\n",
      "an\n",
      "N−1\n",
      "X\n",
      "i=1\n",
      "(ai+1 −ai)Sk −1\n",
      "an\n",
      "n−1\n",
      "X\n",
      "i=N\n",
      "(ai+1 −ai)s −1\n",
      "an\n",
      "n−1\n",
      "X\n",
      "i=N\n",
      "(ai+1 −ai)(Sk −s)\n",
      "= Sn −1\n",
      "an\n",
      "N−1\n",
      "X\n",
      "i=1\n",
      "(ai+1 −ai)Sk −an −aN\n",
      "an\n",
      "s −1\n",
      "an\n",
      "n−1\n",
      "X\n",
      "i=N\n",
      "(ai+1 −ai)(Sk −s)\n",
      ".\n",
      "Now, consider limn→∞. The ﬁrst term goes to s, which cancels with the third\n",
      "term. The second term goes to zero (because the sum is a ﬁxed value). Since\n",
      "the sequence {ai} is nondecreasing, the last term is bounded by an−aN\n",
      "an\n",
      "ϵ, which\n",
      "is less than or equal to ϵ, which was any positive number.\n",
      "In Section 0.0.9 beginning on page 677, we list some additional ways of\n",
      "determining whether or not a series converges.\n",
      "Real Functions\n",
      "Real-valued functions over real domains are some of the most important math-\n",
      "ematical objects. Here we will discuss some of their simpler characteristics.\n",
      "In Section 0.1.5 we will consider some properties in more detail and in Sec-\n",
      "tions 0.1.6 through 0.1.13 we will consider important operations on real func-\n",
      "tions.\n",
      "We will often consider the domain of a function to be an interval [a, b], or\n",
      "if the domain is in IRk, to be a rectangle [a1, b1] × · · · × [ak, bk], and many\n",
      "concepts relating to a ﬁnite partitioning P of that domain. The partition may\n",
      "be deﬁned by the sets {Ii : i ∈P }, or especially in the case of [a, b] in IR, by\n",
      "(a = x0, x1, . . ., xn = b).\n",
      "Important properties of functions include continuity, diﬀerentiability, in-\n",
      "tegrability, and shape. The ﬁrst three of these properties, which are deﬁned\n",
      "in terms of limits, are essentially dichotomous, but they have various levels\n",
      "depending on whether they hold over certain subdomains of the function.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "656\n",
      "0 Statistical Mathematics\n",
      "Taylor’s Theorem\n",
      "One of the most important and useful facts in analysis is Taylor’s theorem.\n",
      "We state the theorem here for scalar-valued real functions of a scalar real\n",
      "variable, but similar results hold for more general functions.\n",
      "Theorem 0.0.11 (Taylor’s theorem)\n",
      "Let f be a function deﬁned on D ⊆IR, let n be a positive integer, suppose that\n",
      "the (n −1)th derivative of f is continuous on the interval [x0, x] ⊆D, and\n",
      "suppose that the nth derivative of f exists on the interval ]x0, x[. Then,\n",
      "f(x) = f(x0)+(x−x0)f′(x0)+(x −x0)2\n",
      "2!\n",
      "f′′(x0)+· · ·+(x −x0)n−1\n",
      "(n −1)!\n",
      "f(n−1)(x0)+Rn,\n",
      "(0.0.63)\n",
      "where the remainder Rn = (x−x0)n\n",
      "n!\n",
      "f(n)(ξ) with x0 < ξ < x.\n",
      "The proof starts with the identity\n",
      "f(x) = f(x0) +\n",
      "Z x\n",
      "x0\n",
      "f′(t)dt,\n",
      "and then proceeds iteratively by integrating by parts. This expression also\n",
      "suggests another form of the remainder. It clearly can be expressed as an\n",
      "integral over t of f(n)(x −t)n−1/n!.\n",
      "It is not necessary to restrict x0 and x as we have in the statement of\n",
      "Taylor’s theorem above. We could, for example, require that D is an interval\n",
      "and x0, x ∈D.\n",
      "Notice that for n = 1, Taylor’s theorem is the mean-value theorem (The-\n",
      "orem 0.0.19).\n",
      "The properties of the remainder term, Rn, are important in applications\n",
      "of Taylor’s theorem.\n",
      "Taylor Series and Analytic Functions\n",
      "As n →∞, if the nth derivative of f continues to exist on the open interval,\n",
      "and if the remainder Rn goes to zero, then we can use Taylor’s theorem to\n",
      "represent the function in terms of a Taylor series. (The common terminology\n",
      "has a possessive in the name of the theorem and a simple adjective in the\n",
      "name of the series.)\n",
      "As indicated in the comments above about Taylor’s theorem, the points x\n",
      "and x0 must be in a closed interval over which the derivatives exist and have\n",
      "ﬁnite values. The Taylor series is\n",
      "f(x) = f(x0) + (x −x0)f′(x0) + (x −x0)2\n",
      "2!\n",
      "f′′(x0) + · · ·\n",
      "(0.0.64)\n",
      "A Taylor series in the form of equation (0.0.64) is said to be an expansion\n",
      "of f about x0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "657\n",
      "Note that equation (0.0.64) implies that the function is inﬁnitely diﬀer-\n",
      "entiable and that the Taylor series converges to the function value. If equa-\n",
      "tion (0.0.64) for all real x and x0, the function f said to be analytic. If the\n",
      "Examples\n",
      "The are many examples of functions for which the Taylor series expansion\n",
      "does not hold. First of all, it only holds for inﬁnitely diﬀerentiable functions.\n",
      "For inﬁnitely diﬀerentiable functions, the question is whether the remain-\n",
      "der Rn goes to zero; that is, does the series converge, and if so, does it converge\n",
      "to f(x) and if so is the convergence over the whole domain?\n",
      "Example 0.0.5 (Convergence only at a single point)\n",
      "Consider\n",
      "f(x) =\n",
      "∞\n",
      "X\n",
      "n=0\n",
      "e−n cos n2x.\n",
      "The function is inﬁnitely diﬀerentiable but the Taylor series expansion about\n",
      "0 converges only for x = x0. (Exercise.)\n",
      "Example 0.0.6 (Convergence only over a restricted interval)\n",
      "Consider\n",
      "f(x) =\n",
      "1\n",
      "1 + x2 .\n",
      "The function is inﬁnitely diﬀerentiable but the Taylor series expansion about\n",
      "0 converges only for |x| < 1. (Exercise.)\n",
      "Example 0.0.7 (Convergence to the wrong value)\n",
      "Consider\n",
      "f(x) =\n",
      "\u001a\n",
      "e−1/x2 for x ̸= 0\n",
      "0\n",
      "for x = 0.\n",
      "The function is inﬁnitely diﬀerentiable and fn(0) = 0 for all n. A Taylor series\n",
      "expansion about 0 converges to 0, but f(x) ̸= 0 if x ̸= 0.\n",
      "Functions of Bounded Variation\n",
      "Another important class of real functions consists of those of bounded vari-\n",
      "ation. The function f deﬁned over [a, b] is said to be of bounded varia-\n",
      "tion on [a, b] if there exists a number M such that for any partition (a =\n",
      "x0, x1, . . ., xn = b)\n",
      "n\n",
      "X\n",
      "i=1\n",
      "|∆fi| ≤M,\n",
      "(0.0.65)\n",
      "where ∆fi = f(xi) −f(xi−1).\n",
      "A suﬃcient condition for a function f to be of bounded variation on [a, b]\n",
      "is that it is continuous on [a, b] and its derivative exists and is bounded on\n",
      "]a, b[.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "658\n",
      "0 Statistical Mathematics\n",
      "Convexity\n",
      "Another useful concept for real sets and for real functions of real numbers is\n",
      "convexity.\n",
      "Deﬁnition 0.0.10 (convex set)\n",
      "A set A ⊆IRd is convex iﬀfor x, y ∈A, ∀a ∈[0, 1], ax + (1 −a)y ∈A.\n",
      "Intervals, rectangles, and hyperrectangles are convex.\n",
      "Theorem 0.0.12\n",
      "If A is a set of real numbers that is convex, then both A and A◦are convex.\n",
      "Proof.\n",
      "Exercise; just use the deﬁnitions.\n",
      "Deﬁnition 0.0.11 (convex function)\n",
      "A function f : D ⊆IRd 7→IR, where D is convex, is convex iﬀfor x, y ∈D,\n",
      "∀a ∈[0, 1],\n",
      "f(ax + (1 −a)y) ≤af(x) + (1 −a)f(y).\n",
      "A function is strictly convex if the inequality above is strict.\n",
      "Deﬁnition 0.0.12 (concave function)\n",
      "A function f is (strictly) concave iﬀ−f is (strictly) convex.\n",
      "A useful theorem that characterizes convexity of twice diﬀerentiable func-\n",
      "tions is the following\n",
      "Theorem 0.0.13\n",
      "If the function f is twice diﬀerentiable over an open convex set D, then f is\n",
      "convex iﬀthe Hessian, Hf, is nonnegative deﬁnite at all points in D. Iﬀit is\n",
      "positive deﬁnite, f is strictly convex.\n",
      "For a proof of this theorem, see a text on continuous optimization, such as\n",
      "Griva et al. (2009).\n",
      "Theorem 0.0.14\n",
      "The composition of a convex function and a convex function is convex.\n",
      "Proof.\n",
      "Let f and g be any convex functions for which f ◦g is deﬁned. Now let a be\n",
      "any real number in [0, 1]. Then f ◦g(ax+(1−a)y) ≤f(ag(x)+(1−a)g(y)) ≤\n",
      "af ◦g(x) + (1 −a)f ◦g(y).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "659\n",
      "Subharmonic Functions\n",
      "Convexity of a function is deﬁned in terms of the average of the function at\n",
      "two points, compared to the function at the average of the two points. We can\n",
      "extend that basic idea to the average of the function over a sphere compared\n",
      "to the function at the sphere. (The average of the function over a sphere is\n",
      "deﬁned in terms of the ratio of a measure of the function image to a measure\n",
      "of the surface of the sphere. The measures are integrals.)\n",
      "Deﬁnition 0.0.13 (subharmonic function)\n",
      "A function f : D ⊆IRd 7→IR, where D is convex, is subharmonic over D, iﬀ\n",
      "for every point x0 ∈D and for every r > 0, the average of f over the surface\n",
      "of the sphere Sr(x0) = {x : ∥x −x0∥= r} is greater than or equal to f(x0).\n",
      "Deﬁnition 0.0.14 (superharmonic function)\n",
      "A function f is superharmonic if −f is subharmonic.\n",
      "Deﬁnition 0.0.15 (harmonic function)\n",
      "A function is harmonic if it is both superharmonic and subharmonic.\n",
      "In one dimension, a subharmonic function is convex and a superharmonic\n",
      "function is concave.\n",
      "A useful theorem that characterizes harmonicity of twice diﬀerentiable\n",
      "functions is the following:\n",
      "Theorem 0.0.15 If the function f is twice diﬀerentiable over an open convex\n",
      "set D, then f is subharmonic iﬀthe Laplacian, ∇2f, (which is just the trace\n",
      "of Hf) is nonnegative at all points in D. The function is harmonic if the\n",
      "Laplacian is 0, and superharmonic if the Laplacian is nonpositive.\n",
      "Proof. Exercise.\n",
      "The relatively simple Laplacian operator captures curvature only in the\n",
      "orthogonal directions corresponding to the principal axes; if the function is\n",
      "twice diﬀerentiable everywhere, however, this is suﬃcient to characterize the\n",
      "(sub-, super-) harmonic property. These properties are of great importance in\n",
      "multidimensional loss functions.\n",
      "Harmonicity is an important concept in potential theory. It arises in ﬁeld\n",
      "equations in physics. The basic equation ∇2f = 0, which implies f is har-\n",
      "monic, is called Laplace’s equation. Another basic equation in physics is\n",
      "∇2f = −cρ, where cρ is positive, which implies f is superharmonic. This\n",
      "is called Poisson’s equation, and is the basic equation in a potential (electri-\n",
      "cal, gravitational, etc.) ﬁeld. A superharmonic function is called a potential for\n",
      "this reason. These PDE’s, which are of the elliptical type, govern the diﬀusion\n",
      "of energy or mass as the domain reaches equilibrium. Laplace’s equation repre-\n",
      "sents a steady diﬀusion and Poisson’s equation models an unsteady diﬀusion,\n",
      "that is, diﬀusion with a source or sink.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "660\n",
      "0 Statistical Mathematics\n",
      "Example 0.0.8\n",
      "Consider f(x) = exp\n",
      "\u0010Pk\n",
      "j=1 x2\n",
      "j\n",
      "\u0011\n",
      ". This function is twice diﬀerentiable, and we\n",
      "have\n",
      "∇2 exp\n",
      "\n",
      "\n",
      "k\n",
      "X\n",
      "j=1\n",
      "x2\n",
      "j\n",
      "\n",
      "=\n",
      "k\n",
      "X\n",
      "i=1\n",
      "(4x2\n",
      "i −2) exp\n",
      "\n",
      "\n",
      "k\n",
      "X\n",
      "j=1\n",
      "x2\n",
      "j\n",
      "\n",
      ".\n",
      "The exponential term is positive, so the condition depends on Pk\n",
      "i=1(4x2\n",
      "i −2).\n",
      "If Pk\n",
      "i=1 x2\n",
      "i < 1/2, it is superharmonic; if Pk\n",
      "i=1 x2\n",
      "i = 1/2, it is harmonic; if\n",
      "Pk\n",
      "i=1 x2\n",
      "i > 1/2, it is subharmonic.\n",
      "0.0.6 The Complex Number System\n",
      "The complex number system, IC, can be developed most directly from the real\n",
      "number system by ﬁrst introducing an element, i, deﬁned as\n",
      "i2 = −1.\n",
      "We call i the imaginary unit, and deﬁne the ﬁeld IC on the set IR × IR as\n",
      "the set {x + iy ; x, y ∈IR}, together with the operations of addition and\n",
      "multiplication for x and y as in IR.\n",
      "The complex number system is very important in mathematical statistics\n",
      "in three areas: transformations (see Section 0.0.9); transforms, as in character-\n",
      "istic functions (see Section 1.1.7) and Fourier transforms, including “discrete”\n",
      "Fourier transforms; and eigenanalysis. In this section, we will just state some\n",
      "of the important properties of the complex number system.\n",
      "Operations and Elementary Functions\n",
      "The operations of complex addition and multiplication can be deﬁned directly\n",
      "in terms of the real operations. For example, if for z1, z2 ∈IC with z1 = x1+iy1\n",
      "and z2 = x2 + iy2, then we denote and deﬁne addition by\n",
      "z1 + z2 = x1 + x2 + i(y1 + y2),\n",
      "and multiplication by\n",
      "z1z2 = (x1x2 −y1y2) + i(x1y2 + x2y1).\n",
      "Both the sum and the product are in IC.\n",
      "Other operations such as powers and division can be deﬁned in terms of\n",
      "the expected results. For example, for an integer k and z ∈IC, zk is the element\n",
      "of IC that would result from the appropriate number of multiplications, and\n",
      "z1/k is the element of IC such that if it is raised to the kth power would yield\n",
      "z. We also note i−1 = −i.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "661\n",
      "The operation z1/k causes us to recognize another diﬀerence in the real\n",
      "and the complex number systems. With the reals, if k is even the operation is\n",
      "deﬁned only over a subset of IR and while there are two real numbers that could\n",
      "be considered as results of the operation, we deﬁne the operation so that it\n",
      "yields a single value. In the case of the complex number system, the operation\n",
      "of taking the kth root is deﬁned over all IC. The main point, however, is that\n",
      "the operation may yield multiple values and there may not be an immediately\n",
      "obvious one to call the result. In complex analysis, we develop ways of dealing\n",
      "with multi-valued operations and functions.\n",
      "There are other operations and functions whose deﬁnitions do not arise\n",
      "from simple extensions of the corresponding operation or function on the reals.\n",
      "For example, we deﬁne “modulus” as an extension of the absolute value: for\n",
      "z = x + iy, we deﬁne |z| as\n",
      "p\n",
      "x2 + y2.\n",
      "Other examples are the exponentiation operation, the functions log(·),\n",
      "sin(·), and so on. The standard way of deﬁning the elementary functions is\n",
      "through analytic continuation of a Taylor series expansion into IC. For example,\n",
      "for z ∈IC, we may deﬁne exp(z) in terms of the convergent series\n",
      "ez = 1 + z + z2\n",
      "2! + z3\n",
      "3! + z4\n",
      "4! + · · ·\n",
      "(0.0.66)\n",
      "The ratio test can be used to show that this is convergent over all IC (exercise).\n",
      "The deﬁnition of ez can be used to deﬁne the exponentiation operation zz2\n",
      "1\n",
      "in general.\n",
      "Complex Conjugates\n",
      "For z = x + iy ∈IC, we deﬁne deﬁne the complex conjugate as x −iy, and\n",
      "denote it as z. We deﬁne the modulus of z as\n",
      "√\n",
      "zz, and denote it as |z|. It is\n",
      "clear that |z| is real and nonnegative, and it corresponds to the absolute value\n",
      "of x if z = x + 0i.\n",
      "We have some simple relationships for complex conjugates:\n",
      "Theorem 0.0.16\n",
      "For all z, z1, z2 ∈IC, we have\n",
      "•\n",
      "z = z,\n",
      "•\n",
      "z1 + z2 = z1 + z2,\n",
      "•\n",
      "z1z2 = z1z2.\n",
      "Proof. Exercise.\n",
      "Euler’s Formula\n",
      "One of the most useful facts is given in Euler’s formula, for a real number x:\n",
      "eix = cos(x) + i sin(x).\n",
      "(0.0.67)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "662\n",
      "0 Statistical Mathematics\n",
      "This relationship can be derived in a number of ways. A straightforward\n",
      "method is to expand eix in a Taylor series about 0 and then reorder the\n",
      "terms:\n",
      "eix = 1 + ix + (ix)2\n",
      "2!\n",
      "+ (ix)3\n",
      "3!\n",
      "+ (ix)4\n",
      "4!\n",
      "+ · · ·\n",
      "=\n",
      "\u0012\n",
      "1 −x2\n",
      "2! + x4\n",
      "4! + x6\n",
      "6! + · · ·\n",
      "\u0013\n",
      "+ i\n",
      "\u0012\n",
      "x −x3\n",
      "3! + x5\n",
      "5! −x7\n",
      "7! + · · ·\n",
      "\u0013\n",
      "= cos(x) + i sin(x).\n",
      "We can do this because the series are absolutely convergent for x ∈IR.\n",
      "Euler’s formula has a number of applications and special cases. For exam-\n",
      "ples,\n",
      "cos(x) = eix + e−ix\n",
      "2\n",
      ",\n",
      "eiπ = −1,\n",
      "and\n",
      "\f\feix\f\f = 1.\n",
      "Other Properties of eix\n",
      "The function eix, where x ∈IR, has a number of interesting properties that\n",
      "we can derive from the Taylor expansion with integral remainder:\n",
      "eix =\n",
      "n\n",
      "X\n",
      "k=0\n",
      "(ix)k\n",
      "k!\n",
      "+ in+1\n",
      "n!\n",
      "Z x\n",
      "0\n",
      "(x −t)neitdt.\n",
      "(0.0.68)\n",
      "Now, following the ideas suggested for the proof of Theorem 0.0.11 (Taylor’s\n",
      "theorem), starting with n = 1, evaluation of\n",
      "R x\n",
      "0 (x −t)neitdt by integration by\n",
      "parts, and then recognizing the form of the resulting integrals, we let n be a\n",
      "positive integer and get\n",
      "Z x\n",
      "0\n",
      "(x −t)neitdt = xn+1\n",
      "n + 1 +\n",
      "i\n",
      "n + 1\n",
      "Z x\n",
      "0\n",
      "(x −t)n+1eitdt.\n",
      "(0.0.69)\n",
      "This gives another form of the remainder in the Taylor series:\n",
      "eix =\n",
      "n\n",
      "X\n",
      "k=0\n",
      "(ix)k\n",
      "k!\n",
      "+\n",
      "in\n",
      "(n −1)!\n",
      "Z x\n",
      "0\n",
      "(x −t)n(eit −1)dt.\n",
      "(0.0.70)\n",
      "Now, we see that the remainder in equation (0.0.68) is bounded from above\n",
      "by\n",
      "|x|n+1\n",
      "(n + 1)!,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "663\n",
      "and the remainder in equation (0.0.70) is bounded from above by\n",
      "2|x|n\n",
      "n! .\n",
      "Hence, we have a bound on the diﬀerence in eix and its approximation by the\n",
      "truncated series:\n",
      "\f\f\f\f\feix −\n",
      "n\n",
      "X\n",
      "k=0\n",
      "(ix)k\n",
      "k!\n",
      "\f\f\f\f\f ≤min\n",
      "\u00122|x|n\n",
      "n! , |x|n+1\n",
      "(n + 1)!\n",
      "\u0013\n",
      ".\n",
      "(0.0.71)\n",
      "Ordering the Complex Numbers\n",
      "We have discussed various orderings of sets (see Section 0.0.1), and for the IR\n",
      "we have seen that there is a useful linear ordering based on the usual inequality\n",
      "relation. This simple linear ordering is not possible for the complex numbers.\n",
      "Orderings of sets also sometimes carry over to a relation on a ﬁeld (see\n",
      "Section 0.0.3). While the reals constitute an Archimedean ordered ﬁeld, there\n",
      "can be no ordering on the complex ﬁeld.\n",
      "Theorem 0.0.17\n",
      "The ﬁeld IC cannot be ordered.\n",
      "Proof. In order to show by contradiction that this must be the case, assume\n",
      "the existence in IC of a subset P as in Deﬁnition 0.0.4 on page 634. Now i ̸= 0,\n",
      "so i ∈P or i ∈−P . But i /∈P because i ◦i ∈−P ; furthermore i /∈−P because\n",
      "i ◦i ∈P ; hence, there can be no such P as required in the deﬁnition of an\n",
      "ordered ﬁeld.\n",
      "0.0.7 Monte Carlo Methods\n",
      "Monte Carlo methods involve sampling, usually artiﬁcially, in the sense that\n",
      "the samples are generated on the computer. To sample from any given dis-\n",
      "tribution, we generally begin with samples from a U(0, 1) distribution (or an\n",
      "approximate U(0, 1) distribution, in the sense that the samples are generated\n",
      "on the computer). A raw sample of uniforms, U1, U2, . . ., is transformed into\n",
      "a sequence {Xj} of (pseudo)random variables from a distribution of interest.\n",
      "We often want the sequence {Xj} to be iid. As part of the transformation\n",
      "process, however, we may use a sequence {Yi} that has internal dependencies.\n",
      "The simplest type of transformation makes use of the inverse of the CDF\n",
      "of the random variable of interest.\n",
      "Inverse CDF Transformation\n",
      "Assume that the CDF of the distribution of interest is FX, and further, sup-\n",
      "pose that FX is continuous and strictly monotone.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "664\n",
      "0 Statistical Mathematics\n",
      "In that case, if X is a random variable with CDF FX, then U = FX(X)\n",
      "has a U(0, 1) distribution.\n",
      "In the inverse CDF method, we transform each Ui to an Xi by\n",
      "Xi = F −1\n",
      "X (Ui).\n",
      "If FX is not continuous or strictly monotone, we can modify this transfor-\n",
      "mation slightly.\n",
      "Acceptance/Rejection\n",
      "Often it is not easy to invert the CDF. In the case of Bayesian inference the\n",
      "posterior distribution may be known only proportionally. First, let us consider\n",
      "the problem in which the CDF is known fully.\n",
      "We will transform an iid sequence {Ui} of uniforms into an iid sequence\n",
      "{Xj} from a distribution that has a probability density p(·).\n",
      "We use an intermediate sequence {Yk} from a distribution that has a\n",
      "probability density g(·). (It could also be the uniform distribution.)\n",
      "Further, suppose for some constant c that h(x) = cg(x) is such that h(x) ≥\n",
      "p(x).\n",
      "1. Generate a variate y from the distribution having pdf g.\n",
      "2. Generate independently a variate u from the uniform(0,1) distribution.\n",
      "3. If u ≤p(y)/h(y), then accept y as the variate, otherwise, reject y and\n",
      "return to step 1.\n",
      "See Figure 0.1.\n",
      "To see that the accepted ys have the desired distribution, ﬁrst let X be\n",
      "the random variable delivered. For any x, because Y (from the density g) and\n",
      "U are independent, we have\n",
      "Pr(X ≤x) = Pr\n",
      "\u0012\n",
      "Y ≤x | U ≤p(Y )\n",
      "cg(Y )\n",
      "\u0013\n",
      "=\n",
      "R x\n",
      "−∞\n",
      "R p(t)/cg(t)\n",
      "0\n",
      "g(t) ds dt\n",
      "R ∞\n",
      "−∞\n",
      "R p(t)/cg(t)\n",
      "0\n",
      "g(t) ds dt\n",
      "=\n",
      "Z x\n",
      "−∞\n",
      "p(t) dt,\n",
      "the distribution function corresponding to p. Diﬀerentiating this quantity with\n",
      "respect to x yields p(x).\n",
      "Obviously, the closer cg(x) is to p(x), the faster the acceptance/rejection\n",
      "algorithm will be, if we ignore the time required to generate y from the dom-\n",
      "inating density g. A good majorizing function would be such that the l is\n",
      "almost as large as k.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "665\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "0.0\n",
      "0.5\n",
      "1.0\n",
      "1.5\n",
      "2.0\n",
      "x\n",
      "p(x)\n",
      "cg(x)\n",
      "(.8, 2.4)\n",
      "R\n",
      "A\n",
      "R\n",
      "A\n",
      "Figure 0.1.\n",
      "Acceptance/Rejection\n",
      "Often, g is chosen to be a very simple density, such as a uniform or\n",
      "a triangular density. When the dominating density is uniform, the accep-\n",
      "tance/rejection method is similar to the “hit-or-miss” method of Monte Carlo\n",
      "quadrature.\n",
      "Variations of Acceptance/Rejection\n",
      "There are many variations of the basic acceptance/rejection.\n",
      "One is called transformed rejection. In the transformed acceptance/rejection\n",
      "method, the steps of the algorithm are combined and rearranged slightly.\n",
      "There are various ways that acceptance/rejection can be used for discrete\n",
      "distributions.\n",
      "It is clear from the description of the algorithm that the acceptance/rejection\n",
      "method also applies to multivariate distributions. (The uniform random num-\n",
      "ber is still univariate, of course.)\n",
      "Use of Dependent Random Variables\n",
      "The methods described above use a sequence of iid variates from the majoriz-\n",
      "ing density. It is also possible to use a sequence from a conditional majorizing\n",
      "density.\n",
      "A method using a nonindependent sequence is called a Metropolis method,\n",
      "and there are variations of these, with their own names.\n",
      "There are two related cases:\n",
      "Suppose {Xj : j = 0, 1, 2, . . .} is such that for j = 1, 2, . . . we know the\n",
      "conditional distributions of Xj|X0, . . ., Xj−1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "666\n",
      "0 Statistical Mathematics\n",
      "Alternatively, suppose we know the functional form (up to the normaliz-\n",
      "ing constant) of the joint density of X1, X2, . . ., Xk, and that we know the\n",
      "distribution of at least one Xi|Xj(i ̸= j).\n",
      "Markov Chain Monte Carlo\n",
      "A Markov chain is a stochastic process X0, X1, . . . in which the conditional\n",
      "distribution of Xt given X0, X1, . . ., Xt−1 is the same as the conditional dis-\n",
      "tribution of Xt given only Xt−1. An aperiodic, irreducible, positive recurrent\n",
      "Markov chain is associated with a stationary distribution or invariant dis-\n",
      "tribution, which is the limiting distribution of the chain. See Section 1.6.3\n",
      "beginning on page 126 for description of these terms.\n",
      "If the density of interest, p, is the density of the stationary distribution of\n",
      "a Markov chain, correlated samples from the distribution can be generated by\n",
      "simulating the Markov chain.\n",
      "This appears harder than it is.\n",
      "A Markov chain is the basis for several schemes for generating random\n",
      "samples. The interest is not in the sequence of the Markov chain itself.\n",
      "The elements of the chain are accepted or rejected in such a way as to\n",
      "form a diﬀerent chain whose stationary distribution or limiting distribution is\n",
      "the distribution of interest.\n",
      "Convergence\n",
      "An algorithm based on a stationary distribution of a Markov chain is an\n",
      "iterative method because a sequence of operations must be performed until\n",
      "they converge; that is, until the chain has gone far enough to wash out any\n",
      "transitory phase that depends on where we start.\n",
      "Several schemes for assessing convergence have been proposed. For exam-\n",
      "ple, we could use multiple starting points and then use an ANOVA-type test\n",
      "to compare variances within and across the multiple streams.\n",
      "The Metropolis Algorithm\n",
      "For a distribution with density p, the Metropolis algorithm, introduced by\n",
      "Metropolis et al. (1953) generates a random walk and performs an accep-\n",
      "tance/rejection based on p evaluated at successive steps in the walk.\n",
      "In the simplest version, the walk moves from the point yi to a candidate\n",
      "point yi+1 = yi + s, where s is a realization from U(−a, a), and accepts yi+1\n",
      "if\n",
      "p(yi+1)\n",
      "p(yi)\n",
      "≥u,\n",
      "where u is an independent realization from U(0, 1).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "667\n",
      "This method is also called the “heat bath” method because of the context\n",
      "in which it was introduced.\n",
      "The random walk of Metropolis et al. is the basic algorithm of simulated\n",
      "annealing, which is currently widely used in optimization problems.\n",
      "If the range of the distribution is ﬁnite, the random walk is not allowed to\n",
      "go outside of the range.\n",
      "Example 0.0.9 Simulation of the von Mises Distribution with the\n",
      "Metropolis Algorithm\n",
      "Consider, for example, the von Mises distribution, with density,\n",
      "p(x) =\n",
      "1\n",
      "2πI0(c)ec cos(x),\n",
      "for −π ≤x ≤π,\n",
      "where I0 is the modiﬁed Bessel function of the ﬁrst kind and of order zero.\n",
      "The von Mises distribution is an easy one to simulate by the Metropolis\n",
      "algorithm. This distribution is often used by physicists in simulations of lattice\n",
      "gauge and spin models, and the Metropolis method is widely used in these\n",
      "simulations.\n",
      "It is not necessary to know the normalizing constant, because it is can-\n",
      "celed in the ratio. The fact that all we need is a nonnegative function that is\n",
      "proportional to the density of interest is an important property of this method.\n",
      "If c = 3, after a quick inspection of the amount of ﬂuctuation in p, we may\n",
      "choose a = 1. The R statements below implement the Metropolis algorithm\n",
      "to generate n −1 deviates from the von Mises distribution.\n",
      "Notice the simplicity of the algorithm in the R code. We did not need to\n",
      "determine a majorizing density, nor even evaluate the Bessel function that is\n",
      "the normalizing constant for the von Mises density.\n",
      "n <- 1000\n",
      "x <- rep(0,n)\n",
      "a <-1\n",
      "c <-3\n",
      "yi <-3\n",
      "j <-0\n",
      "i <- 2\n",
      "while (i < n) {\n",
      "i <- i + 1\n",
      "yip1 <- yi + 2*a*runif(1)- 1\n",
      "if (yip1 < pi & yip1 > - pi) {\n",
      "if (exp(c*(cos(yip1)-cos(yi))) > runif(1)) yi <- yip1\n",
      "else yi <- x[i-1]\n",
      "x[i] <- yip1\n",
      "}\n",
      "}\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "668\n",
      "0 Statistical Mathematics\n",
      "A histogram is not aﬀected by the sequence of the output in a large sample.\n",
      "The Markov chain samplers generally require a “burn-in” period; that is,\n",
      "a number of iterations before the stationary distribution is achieved.\n",
      "In practice, the variates generated during the burn-in period are discarded.\n",
      "The number of iterations needed varies with the distribution, and can be\n",
      "quite large, sometimes several hundred.\n",
      "The von Mises example is unusual; no burn-in is required. In general,\n",
      "convergence is much quicker for univariate distributions with ﬁnite ranges\n",
      "such as this one.\n",
      "It is important to remember what convergence means; it does not mean\n",
      "that the sequence is independent from the point of convergence forward. The\n",
      "deviates are still from a Markov chain.\n",
      "The Metropolis-Hastings Algorithm\n",
      "The Metropolis-Hastings algorithm uses a more general chain for the accep-\n",
      "tance/rejection step.\n",
      "To generate deviates from a distribution with density pX it uses deviates\n",
      "from a Markov chain with density gYt+1|Yt. The conditional density gYt+1|Yt is\n",
      "chosen so that it is easy to generate deviates from it.\n",
      "0. Set k = 0.\n",
      "1. Choose x(k) in the range of pX. (The choice can be arbitrary.)\n",
      "2. Generate y from the density gYt+1|Yt(y|x(k)).\n",
      "3. Set r:\n",
      "r = pX(y)\n",
      "gYt+1|Yt(x(k)|y)\n",
      "pX(x(k))gYt+1|Yt(y|x(k))\n",
      "4. If r ≥1, then\n",
      "4.a. set x(k+1) = y;\n",
      "otherwise\n",
      "4.b. generate u from uniform(0,1) and\n",
      "if u < r, then\n",
      "4.b.i. set x(k+1) = y,\n",
      "otherwise\n",
      "4.b.ii. set x(k+1) = x(k).\n",
      "5. If convergence has occurred, then\n",
      "5.a. deliver x = x(k+1);\n",
      "otherwise\n",
      "5.b. set k = k + 1, and go to step 2.\n",
      "Compare the Metropolis-Hastings algorithm with the basic acceptance/rejection\n",
      "method.\n",
      "The majorizing function in the Metropolis-Hastings algorithm is\n",
      "gYt+1|Yt(x|y)\n",
      "pX(x) gYt+1|Yt(y|x).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "669\n",
      "r is called the “Hastings ratio”, and step 4 is called the “Metropolis re-\n",
      "jection”. The conditional density, gYt+1|Yt(·|·) is called the “proposal density”\n",
      "or the “candidate generating density”. Notice that because the majorizing\n",
      "function contains pX as a factor, we only need to know pX to within a con-\n",
      "stant of proportionality. As we have mentioned already, this is an important\n",
      "characteristic of the Metropolis algorithms.\n",
      "As with the acceptance/rejection methods with independent sequences,\n",
      "the acceptance/rejection methods based on Markov chains apply immediately\n",
      "to multivariate random variables.\n",
      "We can see why this algorithm works by using the same method as we\n",
      "used to analyze the acceptance/rejection method; that is, determine the CDF\n",
      "and diﬀerentiate.\n",
      "The CDF is the probability-weighted sum of the two components corre-\n",
      "sponding to whether the chain moved or not. In the case in which the chain\n",
      "does move, that is, in the case of acceptance, for the random variable Z whose\n",
      "realization is y, we have\n",
      "Pr(Z ≤x) = Pr\n",
      "\u0012\n",
      "Y ≤x\n",
      "\f\f U ≤p(Y )\n",
      "g(xi|Y )\n",
      "p(xi)g(Y |xi)\n",
      "\u0013\n",
      "=\n",
      "R x\n",
      "−∞\n",
      "R p(t)g(xi|t)/(p(xi)g(t|xi))\n",
      "0\n",
      "g(t|xi) ds dt\n",
      "R ∞\n",
      "−∞\n",
      "R p(t)g(xi|t)/(p(xi)g(t|xi))\n",
      "0\n",
      "g(t|xi) ds dt\n",
      "=\n",
      "Z x\n",
      "−∞\n",
      "pX(t) dt.\n",
      "Gibbs Sampling\n",
      "An iterative method, somewhat similar to the use of marginals and condition-\n",
      "als, can also be used to generate multivariate observations. It was ﬁrst used\n",
      "for a a Gibbs distribution (Boltzmann distribution), and so is called the Gibbs\n",
      "method.\n",
      "In the Gibbs method, after choosing a starting point, the components of\n",
      "the d-vector variate are generated one at a time conditionally on all others.\n",
      "If pX is the density of the d-variate random variable X, we use the condi-\n",
      "tional densities pX1|X2,X3,··· ,Xd, pX2|X1,X3,··· ,Xd, and so on.\n",
      "At each stage the conditional distribution uses the most recent values of\n",
      "all the other components.\n",
      "As with other MCMC methods, it may require a number of iterations\n",
      "before the choice of the initial starting point is washed out.\n",
      "Gibbs sampling is often useful in higher dimensions. It depends on the\n",
      "convergence of a Markov chain to its stationary distribution, so a burn-in\n",
      "period is required.\n",
      "0. Set k = 0.\n",
      "1. Choose x(k) ∈S.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "670\n",
      "0 Statistical Mathematics\n",
      "2. Generate x(k+1)\n",
      "1\n",
      "conditionally on x(k)\n",
      "2 , x(k)\n",
      "3 , . . ., x(k)\n",
      "d ,\n",
      "Generate x(k+1)\n",
      "2\n",
      "conditionally on x(k+1)\n",
      "1\n",
      ", x(k)\n",
      "3 , . . ., x(k)\n",
      "d ,\n",
      ". . .\n",
      "Generate x(k+1)\n",
      "d−1\n",
      "conditionally on x(k+1)\n",
      "1\n",
      ", x(k+1)\n",
      "2\n",
      ", . . ., x(k)\n",
      "d ,\n",
      "Generate x(k+1)\n",
      "d\n",
      "conditionally on x(k+1)\n",
      "1\n",
      ", x(k+1)\n",
      "2\n",
      ", . . ., x(k+1)\n",
      "d−1 .\n",
      "3. If convergence has occurred, then\n",
      "3.a. deliver x = x(k+1);\n",
      "otherwise\n",
      "3.b. set k = k + 1, and go to step 2.\n",
      "Example 0.0.10 Gibbs Sampling to Generate Independent Normals\n",
      "Consider Xt+1 normal with a mean of Xt and a variance of σ2.\n",
      "We will generate an iid sample from a standard normal distribution; that\n",
      "is, a normal with a mean of 0 and a variance of 1. In this example, the target\n",
      "distribution is simpler than the proposal.\n",
      "We start with a x0, chosen arbitrarily.\n",
      "We take logs and cancel terms in the expression for r.\n",
      "The following simple Matlab statements generate the sample.\n",
      "x(1) = x0;\n",
      "while i < n\n",
      "i = i + 1;\n",
      "yip1 = yi + sigma*randn;\n",
      "lr2 = yi^2 - yip1^2;\n",
      "if lr2 > 0\n",
      "yi = yip1;\n",
      "else\n",
      "u = rand;\n",
      "if lr2 > log(u)*2\n",
      "yi = yip1;\n",
      "else\n",
      "yi = x(i-1);\n",
      "end\n",
      "end\n",
      "x(i) = yi;\n",
      "end\n",
      "plot (x)\n",
      "There are several variations of the basic Metropolis-Hastings algorithm.\n",
      "Two common related methods are Gibbs sampling and hit-and-run sampling.\n",
      "Those methods are particularly useful in multivariate simulation.\n",
      "Markov chain Monte Carlo has become one of the most important tools in\n",
      "statistics in recent years. Its applications pervade Bayesian analysis, as well\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "671\n",
      "as many Monte Carlo procedures in the frequentist approach to statistical\n",
      "analysis.\n",
      "Whenever a correlated sequence such as a Markov chain is used, variance\n",
      "estimation must be performed with some care. In the more common cases\n",
      "of positive autocorrelation, the ordinary variance estimators are negatively\n",
      "biased. The method of batch means or some other method that attempts to\n",
      "account for the autocorrelation should be used.\n",
      "Convergence\n",
      "Some of the most important issues in MCMC concern the rate of convergence,\n",
      "that is, the length of the burn-in, and the frequency with which the chain\n",
      "advances.\n",
      "In many applications of simulation, such as studies of waiting times in\n",
      "queues, there is more interest in transient behavior than in stationary behav-\n",
      "ior.\n",
      "This is usually not the case in use of MCMC methods. The stationary\n",
      "distribution is the only thing of interest.\n",
      "The issue of convergence is more diﬃcult to address in multivariate distri-\n",
      "butions. It is for multivariate distributions, however, that the MCMC method\n",
      "is most useful.\n",
      "This is because the Metropolis-Hastings algorithm does not require knowl-\n",
      "edge of the normalizing constants, and the computation of a normalizing con-\n",
      "stant may be more diﬃcult for multivariate distributions.\n",
      "Various diagnostics have been proposed to assess convergence. Most of\n",
      "them use multiple chains in one way or another. Use of batch means from\n",
      "separate streams can be used to determine when the variance has stabilized.\n",
      "A cusum plot on only one chain to help to identify convergence.\n",
      "Various methods have been proposed to speed up the convergence.\n",
      "Methods of assessing convergence is currently an area of active research.\n",
      "The question of whether convergence has practically occurred in a ﬁnite\n",
      "number of iterations is similar in the Gibbs method to the same question in\n",
      "the Metropolis-Hastings method.\n",
      "In either case, to determine that convergence has occurred is not a simple\n",
      "problem.\n",
      "Once a realization is delivered in the Gibbs method, that is, once con-\n",
      "vergence has been deemed to have occurred, subsequent realizations can be\n",
      "generated either by starting a new iteration with k = 0 in step 0, or by\n",
      "continuing at step 1 with the current value of x(k).\n",
      "If the chain is continued at the current value of x(k), we must remember\n",
      "that the subsequent realizations are not independent.\n",
      "Eﬀects of Dependence\n",
      "This aﬀects variance estimates (second order sample moments), but not means\n",
      "(ﬁrst order moments).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "672\n",
      "0 Statistical Mathematics\n",
      "In order to get variance estimates we may use means of batches of subse-\n",
      "quences or use just every mth (for some m > 1) deviate in step 3. (The idea\n",
      "is that this separation in the sequence will yield subsequences or a systematic\n",
      "subsample with correlations nearer 0.)\n",
      "If we just want estimates of means, however, it is best not to subsample the\n",
      "sequence; that is, the variances of the estimates of means (ﬁrst order sample\n",
      "moments) using the full sequence is smaller than the variances of the estimates\n",
      "of the same means using a systematic (or any other) subsample (so long as\n",
      "the Markov chain is stationary.)\n",
      "To see this, let ¯xi be the mean of a systematic subsample of size n consisting\n",
      "of every mth realization beginning with the ith realization of the converged\n",
      "sequence. Now, we observe that\n",
      "|Cov(¯xi, ¯xj)| ≤V(¯xl)\n",
      "for any positive i, j, and l less than or equal to m. Hence if ¯x is the sample\n",
      "mean of a full sequence of length nm, then\n",
      "V(¯x) = V(¯xl)/m +\n",
      "m\n",
      "X\n",
      "i̸=j;i,j=1\n",
      "Cov(¯xi, ¯xj)/m2\n",
      "≤V(¯xl)/m + m(m −1)V(¯xl)/m\n",
      "= V(¯xl).\n",
      "In the Gibbs method the components of the d-vector are changed system-\n",
      "atically, one at a time. The method is sometimes called alternating conditional\n",
      "sampling to reﬂect this systematic traversal of the components of the vector.\n",
      "Ordinary Monte Carlo and Iterative Monte Carlo\n",
      "The acceptance/rejection method can be visualized as choosing a subsequence\n",
      "from a sequence of iid realizations from the distribution with density gY in\n",
      "such a way the subsequence has density pX.\n",
      "iid from gY yi yi+1 yi+2 yi+3 · · · yi+k · · ·\n",
      "accept?\n",
      "no yes\n",
      "no\n",
      "yes · · · yes · · ·\n",
      "iid from pX\n",
      "xj\n",
      "xj+1 · · · xj+l · · ·\n",
      "A Markov chain Monte Carlo method can be visualized as choosing a\n",
      "subsequence from a sequence of realizations from a random walk with density\n",
      "gYi+1|Yi in such a way that the subsequence selected has density pX.\n",
      "random walk yi yi+1 =\n",
      "yi+3 =\n",
      "yi+2 =\n",
      "yi + si+1\n",
      "yi+1 + si+2\n",
      "yi+2 + si+3 · · ·\n",
      "accept?\n",
      "no\n",
      "yes\n",
      "no\n",
      "yes\n",
      "· · ·\n",
      "iid from pX\n",
      "xj\n",
      "xj+1\n",
      "· · ·\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "673\n",
      "The general objective in Monte Carlo simulation is to calculate the expec-\n",
      "tation of some function g of a random variable X. In ordinary Monte Carlo\n",
      "simulation, the method relies on the fact that for independent, identically\n",
      "distributed realizations X1, X2, . . . from the distribution P of X,\n",
      "1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "g(Xi) →Eg(X)\n",
      "almost surely as n goes to inﬁnity. This convergence is a simple consequence\n",
      "of the law of large numbers.\n",
      "In Monte Carlo simulation, the sample is simulated with a random num-\n",
      "ber generator. When X is multivariate or a complicated stochastic process,\n",
      "however, it may be diﬃcult or impossible to simulate independent realizations.\n",
      "Monte Carlo Applications\n",
      "Whether the random number generation is direct or iterative, there are gener-\n",
      "ally two kinds of objectives in Monte Carlo applications. One is just to under-\n",
      "stand a probability distribution better. This may involve merely simulating\n",
      "random observations from the distribution and examining the distribution of\n",
      "the simulated sample.\n",
      "The other main application of Monte Carlo methods is to evaluate some\n",
      "constant. No matter how complicated the problem is, it can always be formu-\n",
      "lated as the problem of evaluating a deﬁnite integral\n",
      "Z\n",
      "D\n",
      "f(x)dx.\n",
      "Using a PDF decomposition (0.0.95) f(x) = g(x)p(x), by equation (0.0.96),\n",
      "we see that the evaluation of the integral is the same as the evaluation of the\n",
      "expected value of g(X) where X is a random variable whose distribution has\n",
      "PDF p with support D.\n",
      "The problem now is to estimate E(g(X)). If we have a sample x1, . . ., xn,\n",
      "the standard way of estimating E(g(X)) is to use\n",
      "\\\n",
      "E(g(X)) = 1\n",
      "n\n",
      "n\n",
      "X\n",
      "i=1\n",
      "g(xi).\n",
      "(0.0.72)\n",
      "0.0.8 Mathematical Proofs\n",
      "A mathematical system consists of a body of statements, which may be deﬁni-\n",
      "tions, axioms, or propositions. A proposition is a conditional statement, which\n",
      "has the form “if A then B”, or “A ⇒B”, where A and B are simple declarative\n",
      "statements or conditional statements. A conditional statement may be true\n",
      "or false, or neither. Our interest in mathematics is to establish the truth or\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "674\n",
      "0 Statistical Mathematics\n",
      "falsity of a conditional statement; that is, to prove or disprove the statement.\n",
      "A proposition that has a proof is sometimes called a “lemma”, a “theorem”,\n",
      "or a “corollary”. While these terms have meanings, the meanings are rather\n",
      "vague or subjective, and many authors’ usage of the diﬀerent terms serves\n",
      "no purpose other than to annoy the reader. If a proposition has no known\n",
      "proof, it is sometimes called a “conjecture”. We usually do not use the term\n",
      "“proposition” to refer to a conditional statement that has been disproved.\n",
      "A declarative statement has one of two mutually exclusive states: “true”\n",
      "or “false”. We denote the negation or falsiﬁcation of the statement A by ¬A.\n",
      "With a basic proposition, such as\n",
      "A ⇒B,\n",
      "there are associated four related propositions:\n",
      "contrapositive,\n",
      "¬B ⇒¬A,\n",
      "inverse,\n",
      "¬A ⇒¬B,\n",
      "converse,\n",
      "B ⇒A,\n",
      "and contradiction,\n",
      "¬(A ⇒B).\n",
      "If a proposition is true, then its contraposition is also true, but its contra-\n",
      "diction is not true. The inverse is the contrapositive of the converse. The\n",
      "contradiction of the contradiction of a proposition is the proposition. Within\n",
      "any mathematical system there are propositions which are neither true nor\n",
      "false.\n",
      "There are various types of proofs for propositions. Some are “better” than\n",
      "others. (See Aigner and Ziegler (2010) for discussions of diﬀerent types of\n",
      "proof.) The “best” proof of a proposition is a direct proof, which is a sequence\n",
      "of statements “if A then A1, if A1 . . . , . . . then B”, where each statement in\n",
      "the sequence is an axiom or a previously proven proposition. A direct proof is\n",
      "called deductive, because each of the steps after the ﬁrst is deduced from the\n",
      "preceding step.\n",
      "Occasionally, the Axiom of Choice is used in a proof. This axiom, which\n",
      "we encountered on page 617, is outside the usual axiomatic basis of much of\n",
      "mathematics. The Axiom of Choice basically says that given any collection\n",
      "of sets, even an inﬁnite collection, it is possible to form a set consisting of\n",
      "exactly one element from each set in the collection. The Axiom of Choice is\n",
      "tautological for a ﬁnite collection.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "675\n",
      "Whenever the Axiom of Choice is used in a proof, that fact should be\n",
      "stated. Also, whenever an indirect method of proof is used, the type of the\n",
      "proof should be stated or described.\n",
      "Two useful types of indirect proofs are contradiction and induction. Al-\n",
      "though proofs of these types often appear very clever, they lack the simple\n",
      "elegance of a direct proof.\n",
      "In a proof of “A ⇒B” by contradiction, we assume “A”, and suppose\n",
      "“not B”. Then we ultimately arrive at a conclusion that contradicts an axiom\n",
      "or a previously proven proposition. The means that the supposition “not B”\n",
      "cannot be true, and hence that “B” is true. The proof that the Vitali set is\n",
      "not Lebesgue-measurable uses contradiction as well as the Axiom of Choice\n",
      "(see Example 0.1.5 on page 718.)\n",
      "A proof by induction may be appropriate when we can index a sequence of\n",
      "statements by n ∈ZZ+, that is, Sn, and the statement we wish to prove is that\n",
      "Sn is true for all n ≥m ∈ZZ+. We ﬁrst show that Sm is true. (Here is where\n",
      "a proof by induction requires some care; this statement must be nontrivial;\n",
      "that is, it must be a legitimate member of the sequence of statements.) Then\n",
      "we show that for n ≥m, Sn ⇒Sn+1, in which case we conclude that Sn is\n",
      "true for all n ≥m ∈ZZ+.\n",
      "As an example of mathematical induction, consider the statement that for\n",
      "any positive integer n,\n",
      "n\n",
      "X\n",
      "i=1\n",
      "i = 1\n",
      "2n(n + 1).\n",
      "We use induction to prove this by ﬁrst showing for n = 1,\n",
      "1 = 1\n",
      "2(2).\n",
      "Then we assume that for some k > 1,\n",
      "k\n",
      "X\n",
      "i=1\n",
      "i = 1\n",
      "2k(k + 1),\n",
      "and consider Pk+1\n",
      "i=1 i:\n",
      "k+1\n",
      "X\n",
      "i=1\n",
      "i =\n",
      "k\n",
      "X\n",
      "i=1\n",
      "i + k + 1\n",
      "= 1\n",
      "2k(k + 1) + k + 1\n",
      "= 1\n",
      "2(k + 1)((k + 1) + 1).\n",
      "Hence, we conclude that the statement is true for any positive integer n.\n",
      "Another useful type of deductive proof for “A ⇒B” is a contrapositive\n",
      "proof; that is, a proof of “not B ⇒not A”.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "676\n",
      "0 Statistical Mathematics\n",
      "There are some standard procedures often used in proofs. If the conclusion\n",
      "is that two sets A and B are equal, show that A ⊆B and B ⊆A. To do this\n",
      "(for the ﬁrst one), choose any x ∈A and show x ∈B. The same technique is\n",
      "used to show that two collections of sets, for example, two σ-ﬁelds, are equal.\n",
      "To show that a sequence converges, use partial sums and an ϵ bound.\n",
      "To show that a series converges, show that the sequence is a Cauchy se-\n",
      "quence.\n",
      "The standard procedures may not always work, but try them ﬁrst. In\n",
      "the next section, I describe several facts that are often used in mathematical\n",
      "proofs.\n",
      "0.0.9 Useful Mathematical Tools and Operations\n",
      "In deriving results or in proving theorems, there are a number of operations\n",
      "that occur over and over. It is useful to list some of these operations so that\n",
      "they will more naturally come to mind when they are needed. The following\n",
      "subsections list mathematical operations that should be in fast memory. None\n",
      "of these should be new to the reader. In some cases, we mention a speciﬁc\n",
      "operation such as completing the square; in other cases, we mention a speciﬁc\n",
      "formula such as De Morgan’s laws or the inclusion-exclusion formula.\n",
      "Working with Abstract Sets\n",
      "Two of the most useful relations are De Morgan’s laws, equations (0.0.2)\n",
      "and (0.0.3), and their extensions to countable unions and intersections.\n",
      "The inclusion-exclusion formula, equation (0.0.8), is particularly useful in\n",
      "dealing with collections of subsets of a sample space.\n",
      "For a general sequence of sets {An}, the disjoint sequence (0.0.6) {Dn} on\n",
      "page 618 that partitions their union is often useful.\n",
      "If the sequence {An} is increasing, that is, A1 ⊆A2 ⊆. . ., the intersection\n",
      "is trivial, but the union ∪∞\n",
      "n=1An may be of interest. In that case, the disjoint\n",
      "sequence (0.0.7) Dn = An+1 −An may be useful. Conversely, if the sequence\n",
      "{An} is decreasing, the union is trivial, but the intersection may be of interest.\n",
      "In that case, De Morgan’s laws may be used to change the decreasing sequence\n",
      "into an increasing one.\n",
      "Working with Real Sets\n",
      "There are many useful properties of real numbers that simplify operations\n",
      "on them. Recognizing common sequences of reals as discussed beginning on\n",
      "page 651 or sequences of real intervals discussed beginning on page 645 will\n",
      "aid in solving many problems in mathematics. The sequences of intervals\n",
      "Oi =\n",
      "\u0015\n",
      "a −1\n",
      "i , b + 1\n",
      "i\n",
      "\u0014\n",
      "(0.0.73)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "677\n",
      "and\n",
      "Ci =\n",
      "\u0014\n",
      "a + 1\n",
      "i , b −1\n",
      "i\n",
      "\u0015\n",
      ".\n",
      "(0.0.74)\n",
      "given in expressions (0.0.39) and (0.0.40) are worth remembering because\n",
      "∩∞\n",
      "i=1Oi = [a, b],\n",
      "that is, it is closed; and\n",
      "∪∞\n",
      "i=1Ci =]a, b[,\n",
      "that is, it is open. Note the nesting of the sequences; the sequence {Oi} is\n",
      "decreasing and the sequence {Ci} is decreasing.\n",
      "Other sequences of real numbers that may be useful are nested intervals\n",
      "each of the form of Oi or Ci above, for example,\n",
      "Iij =\n",
      "\u0015\n",
      "j + 1\n",
      "i , j −1\n",
      "i\n",
      "\u0014\n",
      ".\n",
      "(0.0.75)\n",
      "These kinds of sequences may be used to form an interesting sequence of\n",
      "unions or intersections; for example,\n",
      "Uj = ∪∞\n",
      "i=1Iij.\n",
      "Two useful set of integers are the increasing sequence\n",
      "Ai = {1, . . ., i}\n",
      "(0.0.76)\n",
      "and the decreasing sequence\n",
      "Bi = ZZ+ −{1, . . ., i}.\n",
      "(0.0.77)\n",
      "Note that ∪Ai = ZZ+ and ∩Bi = ∅.\n",
      "Working with Real Sequences and Series\n",
      "It is helpful to be familiar with a few standard sequences and series such as\n",
      "those we mentioned on page 651 and in Section 0.0.5. A question that arises\n",
      "often is whether or not a given series of real numbers converges. We discussed\n",
      "that issue brieﬂy in Section 0.0.5. Here we list some additional conditions are\n",
      "useful in addressing this question. In the following we write P xi to mean\n",
      "limn→∞\n",
      "Pn\n",
      "i=1 xi.\n",
      "Comparison Tests\n",
      "• If P|yi| converges and |xi| ≤|yi| then Pxi converges absolutely.\n",
      "• If P |yi| diverges and |xi| ≥|yi| then P |xi| diverges but P xi may converge.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "678\n",
      "0 Statistical Mathematics\n",
      "Ratio Test\n",
      "• If limi→∞|xi+1|/|xi| = α, then P xi converges absolutely if α < 1 and\n",
      "diverges if α > 1.\n",
      "Root Test\n",
      "• If limi→∞\n",
      "np\n",
      "|xi| = α, then Pxi converges absolutely if α < 1 and diverges\n",
      "if α > 1.\n",
      "Raabe’s Test\n",
      "• If limn→∞i (1 −|xi+1|/|xi|) = α, then P xi converges absolutely if α < 1\n",
      "and diverges if α > 1.\n",
      "Alternating Series Test\n",
      "• If xi ≥0, xi ≤xi+1, and limn→∞xi = 0, then P(−1)ixi converges.\n",
      "Use of Standard Inequalities\n",
      "Many mathematical inequalities lead to other interesting facts. I mention sev-\n",
      "eral useful inequalities in this chapter. In Appendix B I state versions of\n",
      "many of these in the setting of probabilities or expectations, and I also men-\n",
      "tion several additional ones in that appendix. Being familiar with these var-\n",
      "ious inequalities (the more, the better!) helps one to prove or disprove other\n",
      "propositions.\n",
      "The proofs of some of the standard inequalities themselves are templates of\n",
      "techniques that should be in the toolbox of mathematical statisticians. Two\n",
      "examples are the proof of the Cauchy-Schwarz inequality and the proof of\n",
      "H¨older’s inequality. In each case, the main fact used may not have an obvious\n",
      "relationship with the inequality itself. For the Cauchy-Schwarz inequality, we\n",
      "use a simple and wellknown fact from the theory of equations. See page 637.\n",
      "For H¨older’s inequality, we identify a relevant concave function and then use\n",
      "a property of such a function. See page 642 for a proof that uses a concave\n",
      "function, and see page 852 for a proof of a slightly diﬀerent version of H¨older’s\n",
      "inequality that uses the related convex function. The remembrance of how\n",
      "we get started on these two proofs can help when we are faced with a new\n",
      "proposition to prove or disprove.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "679\n",
      "Working with Real-Valued Functions\n",
      "When dealing with general real-valued functions, it is often useful to decom-\n",
      "pose the function into its nonnegative part and its negative part. In this way,\n",
      "the function f is written as\n",
      "f = f+ −f−.\n",
      "An example of this technique is in the deﬁnition of the Lebesgue integral,\n",
      "Deﬁnition 0.1.41.\n",
      "Use of Transformations\n",
      "Many problems are simpliﬁed by use of transformations of the variables. Some\n",
      "useful transformations are those between trigonometric and exponential func-\n",
      "tions, such as Euler’s formula,\n",
      "ei(nx) = cos(nx) + i sin(nx),\n",
      "(0.0.78)\n",
      "for integer n and real x.\n",
      "Euler’s formula yields de Moivre’s formula for multiples of angles,\n",
      "(cos(x) + i sin(x))n = cos(nx) + i sin(nx),\n",
      "(0.0.79)\n",
      "again for integer n and real x. (Note, in fact, that this formula does not hold\n",
      "for non-integer n.) There are many other formulas among the trigonometric\n",
      "functions that can be useful for transforming variables.\n",
      "Another very useful class of transformations are those that take cartesian\n",
      "coordinates into circular systems. In two dimensions, the “polar coordinates”\n",
      "ρ and θ in terms of the cartesian coordinates x1 and x2 are\n",
      "ρ =\n",
      "p\n",
      "x2\n",
      "1 + x2\n",
      "2\n",
      "θ =\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "if\n",
      "x1 = x2 = 0\n",
      "arcsin(x2/\n",
      "p\n",
      "x2\n",
      "1 + x2\n",
      "2)\n",
      "if\n",
      "x1 ≥0\n",
      "π −arcsin(x2/\n",
      "p\n",
      "x2\n",
      "1 + x2\n",
      "2) if\n",
      "x1 < 0\n",
      "(0.0.80)\n",
      "The extension of these kinds of transformations to higher dimensions is called\n",
      "“spherical coordinates”.\n",
      "Expansion in a Taylor Series\n",
      "One of the most useful tools in analysis is the Taylor series expansion of a\n",
      "function about a point a. For a scalar-valued function of a scalar variable, it\n",
      "is\n",
      "f(x) = f(a) + (x −a)f′ + 1\n",
      "2!(x −a)2f′′ + · · · ,\n",
      "(0.0.81)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "680\n",
      "0 Statistical Mathematics\n",
      "if the derivatives exist and the series is convergent. (The class of functions\n",
      "for which this is the case in some region that contains x and a is said to be\n",
      "analytic over that region; see page 656. An important area of analysis is the\n",
      "study of analyticity.)\n",
      "In applications, the series is usually truncated, and we call the series with\n",
      "k + 1 terms, the kth order Taylor expansion.\n",
      "For a function of m variables, it is a rather complicated expression:\n",
      "f(x1, . . ., xm) =\n",
      "∞\n",
      "X\n",
      "j=0\n",
      "\n",
      "1\n",
      "j!\n",
      " m\n",
      "X\n",
      "k=1\n",
      "(xk −ak) ∂\n",
      "∂xk\n",
      "!j\n",
      "f(x1, . . ., xm)\n",
      "\n",
      "\n",
      "(x1,...,xm)=(a1,...,am)\n",
      ".\n",
      "(0.0.82)\n",
      "The second order Taylor expansion for a function of an m-vector is the\n",
      "much simpler expression.\n",
      "f(x) ≈f(a) + (x −a)T∇f(a) + 1\n",
      "2(x −a)THf(a)(x −a),\n",
      "(0.0.83)\n",
      "where ∇f(a) is the vector of ﬁrst derivatives evaluated at a and Hf(a) is the\n",
      "matrix of second second derivatives (the Hessian) evaluated at a. This is the\n",
      "basis for Newton’s method in optimization, for example. Taylor expansions\n",
      "beyond the second order for vectors becomes rather messy (see the expres-\n",
      "sion on the right side of the convergence expression (1.200) on page 95, for\n",
      "example).\n",
      "Mean-Value Theorem\n",
      "Two other useful facts from calculus are Rolle’s theorem and the mean-value\n",
      "theorem, which we state here without proof. (Proofs are available in most\n",
      "texts on calculus.)\n",
      "Theorem 0.0.18 (Rolle’s theorem)\n",
      "Assume the function f(x) is continuous on [a, b] and diﬀerentiable on ]a, b[. If\n",
      "f(a) = f(b), then there exists a point x0 with a < x0 < b such that f′(x0) = 0.\n",
      "Theorem 0.0.19 (mean-value theorem)\n",
      "Assume the function f(x) is continuous on [a, b] and diﬀerentiable on ]a, b[.\n",
      "Then there exists a point x0 with a < x0 < b such that\n",
      "f(b) −f(a) = (b −a)f′(x0).\n",
      "Evaluation of Integrals\n",
      "There are many techniques that are useful in evaluation of a deﬁnite inte-\n",
      "gral. Before attempting to evaluate the integral, we should establish that the\n",
      "integral is ﬁnite. For example, consider the integral\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "681\n",
      "Z ∞\n",
      "−∞\n",
      "exp(−t2/2)dt.\n",
      "(0.0.84)\n",
      "A technique for evaluation of this integral is to re-express it as an iterated\n",
      "integral over a product space. (See Exercise 0.0.22; this is an application\n",
      "of Fubini’s theorem.) Before doing this, however, we might ask whether the\n",
      "integral is ﬁnite. Because exp(−t2/2) decreases rapidly in the tails, there is\n",
      "a good chance that the integral is ﬁnite. We can see it directly, however, by\n",
      "observing that\n",
      "0 < exp(−t2/2) < exp(−|t| + 1)\n",
      "−∞< t < ∞,\n",
      "and\n",
      "Z ∞\n",
      "−∞\n",
      "exp(−|t| + 1)dt = 2e.\n",
      "One of the simplest techniques for evaluation of an integral is to express\n",
      "the integral in terms of a known integral, as we discuss in Section 0.0.9.\n",
      "Use of Known Integrals and Series\n",
      "The standard families of probability distributions provide a compendium of\n",
      "integrals and series with known values.\n",
      "Integrals\n",
      "There are three basic continuous univariate distributions that every stu-\n",
      "dent of mathematical statistics should be familiar with. Each of these\n",
      "distributions is associated with an integral that is important in many\n",
      "areas of mathematics.\n",
      "•\n",
      "over IR; the normal integral:\n",
      "Z ∞\n",
      "−∞\n",
      "e−(x−µ)2/2σ2dx =\n",
      "√\n",
      "2πσ,\n",
      "(0.0.85)\n",
      "for σ > 0, and its multivariate extension,\n",
      "•\n",
      "over IR+; the gamma integral (called the complete gamma function):\n",
      "Z ∞\n",
      "0\n",
      "1\n",
      "γα xα−1e−x/γdx = Γ(α),\n",
      "(0.0.86)\n",
      "for α, γ > 0.\n",
      "•\n",
      "over ]0, 1[; the beta integral (called the complete beta function):\n",
      "Z 1\n",
      "0\n",
      "xα−1(1 −x)β−1dx = Γ(α)Γ(β)\n",
      "Γ(α + β) ,\n",
      "(0.0.87)\n",
      "for α, β > 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "682\n",
      "0 Statistical Mathematics\n",
      "Multivariate integrals\n",
      "Both the normal distribution and the beta distribution have important\n",
      "and straightforward multivariate extensions. These are associated with\n",
      "important multivariate integrals.\n",
      "•\n",
      "over IRd; Aitken’s integral:\n",
      "Z\n",
      "IRd e−(x−µ)TΣ−1(x−µ)/2 dx = (2π)d/2|Σ|1/2,\n",
      "(0.0.88)\n",
      "for positive deﬁnite Σ−1.\n",
      "•\n",
      "over ]0, 1[d; Dirichlet integral:\n",
      "Z\n",
      "]0,1[d\n",
      "d\n",
      "Y\n",
      "i=1\n",
      "xαi−1\n",
      "i\n",
      " \n",
      "1 −\n",
      "d\n",
      "X\n",
      "i=1\n",
      "xi\n",
      "!αd+1−1\n",
      "dx =\n",
      "Qd+1\n",
      "i=1 Γ(αi)\n",
      "Γ(Pd+1\n",
      "i=1 αi)\n",
      ".\n",
      "(0.0.89)\n",
      "Series\n",
      "There are four simple series that should also be immediately recognizable:\n",
      "•\n",
      "over 0, . . ., n; the binomial series:\n",
      "n\n",
      "X\n",
      "x=0\n",
      "Γ(n + 1)\n",
      "Γ(x + 1)Γ(n −x + 1)πx(1 −π)n−x = 1,\n",
      "(0.0.90)\n",
      "for 0 < π < 1 and n ≥1.\n",
      "•\n",
      "over max(0, N −L + M), . . ., min(N, M); the hypergeometric series:\n",
      "min(N,M)\n",
      "X\n",
      "x=max(0,N−L+M)\n",
      "\u0012M\n",
      "x\n",
      "\u0013\u0012L −M\n",
      "N −x\n",
      "\u0013\n",
      "=\n",
      "\u0012L\n",
      "n\n",
      "\u0013\n",
      ",\n",
      "(0.0.91)\n",
      "for 1 ≤L, 0 ≤N ≤L, and 0 ≤M ≤L.\n",
      "•\n",
      "over 0, 1, 2, . . .; the geometric series:\n",
      "∞\n",
      "X\n",
      "x=0\n",
      "(1 −π)x = π−1\n",
      "(0.0.92)\n",
      "for 0 < π < 1.\n",
      "•\n",
      "over 0, 1, 2, . . .; the Poisson series:\n",
      "∞\n",
      "X\n",
      "x=0\n",
      "θx\n",
      "x! = eθ,\n",
      "(0.0.93)\n",
      "for θ > 0.\n",
      "The beta integral and the binomial series have a natural connection\n",
      "through the relation\n",
      "Γ(n + 1)\n",
      "Γ(x + 1)Γ(n −x + 1) =\n",
      "\u0012n\n",
      "x\n",
      "\u0013\n",
      ".\n",
      "(0.0.94)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "683\n",
      "The Dirichlet integral, which is a generalization of the beta, has a similar\n",
      "relation to the multinomial series, which is a generalization of the binomial.\n",
      "For computing expected values or evaluating integrals or sums, the trick\n",
      "often is to rearrange the integral or the sum so that it is in the form of the\n",
      "original integrand or summand with diﬀerent parameters.\n",
      "As an example, consider the integral that is the qth raw moment of a\n",
      "gamma(α, β) random variable:\n",
      "Z ∞\n",
      "0\n",
      "1\n",
      "Γ(α)βα xqxα−1e−x/βdx.\n",
      "We use the known value of the integral of the density:\n",
      "Z ∞\n",
      "0\n",
      "1\n",
      "Γ(α)βα xα−1e−x/βdx = 1.\n",
      "So\n",
      "Z ∞\n",
      "0\n",
      "1\n",
      "Γ(α)βα xqxα−1e−x/βdx =\n",
      "Z ∞\n",
      "0\n",
      "1\n",
      "Γ(α)\n",
      "Γ(q + α)βq\n",
      "Γ(q + α)βq+α x(q+α)−1e−x/βdx\n",
      "= Γ(q + α)βq\n",
      "Γ(α)\n",
      "Z ∞\n",
      "0\n",
      "1\n",
      "Γ(q + α)βq+α x(q+α)−1e−x/βdx\n",
      "= Γ(q + α)βq\n",
      "Γ(α)\n",
      "Another example is a series of the form\n",
      "∞\n",
      "X\n",
      "x=0\n",
      "xq θx e−θ\n",
      "x! .\n",
      "We recognize in this the known series that corresponds to the probability\n",
      "function associated with the Poisson distribution:\n",
      "∞\n",
      "X\n",
      "x=0\n",
      "θx e−θ\n",
      "x! = 1,\n",
      "and realize that evaluation of the series involves a manipulation of xq and x!.\n",
      "For q = 1, we have\n",
      "∞\n",
      "X\n",
      "x=0\n",
      "x θx e−θ\n",
      "x! = θ\n",
      "∞\n",
      "X\n",
      "x=1\n",
      "θ(x−1)\n",
      "e−θ\n",
      "(x −1)!\n",
      "= θ.\n",
      "For q = 2, we form two sums so that we can get expressions involving the\n",
      "basic probability function:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "684\n",
      "0 Statistical Mathematics\n",
      "∞\n",
      "X\n",
      "x=0\n",
      "x2 θx e−θ\n",
      "x! =\n",
      "∞\n",
      "X\n",
      "x=2\n",
      "x(x −1) θx e−θ\n",
      "x! +\n",
      "∞\n",
      "X\n",
      "x=1\n",
      "x θx e−θ\n",
      "x!\n",
      "= θ2\n",
      "∞\n",
      "X\n",
      "x=2\n",
      "θ(x−2)\n",
      "e−θ\n",
      "(x −2)! + θ\n",
      "∞\n",
      "X\n",
      "x=1\n",
      "θ(x−1)\n",
      "e−θ\n",
      "(x −1)!\n",
      "= θ2 + θ.\n",
      "The PDF Decomposition\n",
      "It is often useful to decompose a given function f into a product of a PDF p\n",
      "and a function g:\n",
      "f(x) = g(x)p(x).\n",
      "(0.0.95)\n",
      "An appropriate PDF depends on the domain of f, of course. For continuous\n",
      "functions over a ﬁnite domain, a scaled PDF of a beta distribution is often\n",
      "useful; for a domain of the form [a, ∞[, a shifted gamma works well; and for\n",
      "] −∞, ∞[, a normal is often appropriate.\n",
      "The PDF decomposition yields the relation\n",
      "Z\n",
      "f(x) dx = E(g(X)),\n",
      "(0.0.96)\n",
      "where the expectation is taken wrt the distribution with PDF p.\n",
      "The PDF decomposition is useful in Monte Carlo applications. When the\n",
      "PDF is chosen appropriately, the technique is called “importance sampling”.\n",
      "The PDF decomposition is also used often in function estimation.\n",
      "Completing the Square\n",
      "Squared binomials occur frequently in statistical theory, often in a loss func-\n",
      "tion or as the exponential argument in the normal density function. Sometimes\n",
      "in an algebraic manipulation, we have an expression of the form ax2 +bx, and\n",
      "we want an expression for this same quantity in the form (cx + d)2 + e, where\n",
      "e does not involve x. This form can be achieved by adding and subtracting\n",
      "b2/(4a), so as to have\n",
      "ax2 + bx =\n",
      "\u0000√ax + b/(2√a)\n",
      "\u00012 −b2/(4a).\n",
      "(0.0.97)\n",
      "We have a similar operation for vectors and positive deﬁnite matrices. If\n",
      "A is a positive deﬁnite matrix (meaning that A−1\n",
      "2 exists) and x and b are\n",
      "vectors, we can complete the square of xTAx + xTb in a similar fashion: we\n",
      "add and subtract bTA−1b/4. This gives\n",
      "\u0010\n",
      "A\n",
      "1\n",
      "2 x + A−1\n",
      "2 b/2\n",
      "\u0011T \u0010\n",
      "A\n",
      "1\n",
      "2 x + A−1\n",
      "2 b/2\n",
      "\u0011\n",
      "−bTA−1b/4\n",
      "or\n",
      "\u0000x + A−1b/2\n",
      "\u0001T A\n",
      "\u0000x + A−1b/2\n",
      "\u0001\n",
      "−bTA−1b/4.\n",
      "(0.0.98)\n",
      "This is a quadratic form in a linear function of x, together with a quadratic\n",
      "form bTA−1b/4 that does not involve x.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "685\n",
      "The “Pythagorean Theorem” of Statistics\n",
      "We often encounter a mean or an expectation taken over a squared binomial.\n",
      "In this case, it may be useful to decompose the squared binomial into a sum\n",
      "of squared binomials or an expectation of a squared binomial plus a single\n",
      "squared binomial:\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(xi −m)2 =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(xi −¯x)2 + n(¯x −m)2,\n",
      "(0.0.99)\n",
      "where ¯x = Pxi/n, and\n",
      "E((X −m)2) = E((X −µ)2) + (µ −m)2,\n",
      "(0.0.100)\n",
      "where µ = E(X).\n",
      "This of course is also true for the d-vectors X, m, and µ:\n",
      "E(∥X −m∥2) = E(∥X −µ∥2) + ∥µ −m∥2,\n",
      "(0.0.101)\n",
      "where µ = E(X).\n",
      "Because the second term on the right-hand side in each of the equations\n",
      "above is positive, we can conclude that ¯x and µ are the respective minimizers\n",
      "of the left-hand side, wrt a variable m.\n",
      "Orthogonalizing Linearly Independent Elements of a Vector Space\n",
      "Given a set of nonnull, linearly independent vectors, x1, x2, . . ., it is easy to\n",
      "form orthonormal vectors, ˜x1, ˜x2, . . ., that span the same space. This can be\n",
      "done with respect to any inner product and the norm deﬁned by the inner\n",
      "product. The most common inner product for vectors of course is ⟨xi, xj⟩=\n",
      "xT\n",
      "i xj, and the Euclidean norm, ∥x∥=\n",
      "p\n",
      "⟨x, x⟩, which we often write without\n",
      "the subscript.\n",
      "˜x1 =\n",
      "x1\n",
      "∥x1∥\n",
      "˜x2 =\n",
      "(x2 −⟨˜x1, x2⟩˜x1)\n",
      "∥x2 −⟨˜x1, x2⟩˜x1)∥\n",
      "˜x3 =\n",
      "(x3 −⟨˜x1, x3⟩˜x1 −⟨˜x2, x3⟩˜x2)\n",
      "∥x3 −⟨˜x1, x3⟩˜x1 −⟨˜x2, x3⟩˜x2∥\n",
      "etc.\n",
      "These are called Gram-Schmidt transformations. These transformations also\n",
      "apply to other kinds of objects, such as functions, for which we can deﬁne\n",
      "an inner product. (Note: the third expression above, and similar expressions\n",
      "for subsequent vectors may be numerically unstable. See Gentle (2007), pages\n",
      "27–29 and 432, for a discussion of numerical issues.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "686\n",
      "0 Statistical Mathematics\n",
      "Expansion in Basis Elements of a Vector Space\n",
      "If q1, q2, . . . form an orthonormal basis for the real vector space V, then x ∈V\n",
      "can be expressed in the form\n",
      "x =\n",
      "X\n",
      "ciqi,\n",
      "(0.0.102)\n",
      "where c1, c2, . . . are real numbers, called the Fourier coeﬃcients with respect\n",
      "to the basis (q1, q2, . . .). (I have left oﬀthe limits, because the vector space\n",
      "may be inﬁnite dimensional.) The Fourier coeﬃcients satisfy\n",
      "ck = ⟨x, qk⟩.\n",
      "(0.0.103)\n",
      "If the inner product arises from a norm (the L2 norm!), then for any ﬁxed j,\n",
      "the approximation to x\n",
      "˜x =\n",
      "j\n",
      "X\n",
      "i=1\n",
      "ciqi,\n",
      "where c1, . . ., cj are the Fourier coeﬃcients is better than any other approxi-\n",
      "mation of the form Pj\n",
      "i=1 aiqi in the sense that\n",
      "x −\n",
      "j\n",
      "X\n",
      "i=1\n",
      "ciqi\n",
      " ≤\n",
      "x −\n",
      "j\n",
      "X\n",
      "i=1\n",
      "aiqi\n",
      " .\n",
      "(0.0.104)\n",
      "Discrete Transforms\n",
      "Operations on a vector can often be facilitated by ﬁrst forming an inner prod-\n",
      "uct with the given vector and another speciﬁc vector that has an additional\n",
      "argument. This inner product is a function in the additional argument. The\n",
      "function is called a transform of the given vector. Because of the linearity of\n",
      "inner products, these are linear transforms.\n",
      "One of the most useful transforms is the discrete Fourier transform (DFT),\n",
      "which is the weighted inner product of a given n vector with the vector\n",
      "f(s) = (e−2πis, e−4πis, . . ., e−2nπis);\n",
      "that is, for the n-vector x,\n",
      "d(s) =\n",
      "1\n",
      "√n⟨x, f(s)⟩\n",
      "=\n",
      "1\n",
      "√n\n",
      "∞\n",
      "X\n",
      "t=1\n",
      "xte−2tπis\n",
      "=\n",
      "1\n",
      "√n\n",
      " ∞\n",
      "X\n",
      "t=1\n",
      "xt, cos(−2tπs) −i\n",
      "∞\n",
      "X\n",
      "t=1\n",
      "xt, sin(−2tπs)\n",
      "!\n",
      ".\n",
      "(0.0.105)\n",
      "*** discuss uses\n",
      "*** describe FFT\n",
      "We will discuss continuous transforms and transforms generally in Sec-\n",
      "tion 0.1.12.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "687\n",
      "Diﬀerential Equations and Diﬀerence Equations\n",
      "Many processes of interest can be modeled by diﬀerential equations or by\n",
      "diﬀerence equations. This is because in many cases the change in a system\n",
      "may be easy to observe, it may follow some physical law, or else its behavior\n",
      "can be related in a natural way to other observable events or measurable\n",
      "variables.\n",
      "A diﬀerential equation is an equation that involves one or more derivatives.\n",
      "The variable(s) with respect to which the derivatives are taken are called “in-\n",
      "dependent variable(s)”. If all of the derivatives are taken with respect to the\n",
      "same variable, the equation is an ordinary diﬀerential equation or ODE; oth-\n",
      "erwise, it is a partial diﬀerential equation or PDE. A solution to a diﬀerential\n",
      "equation is an equation involving the variables of the diﬀerential equation\n",
      "that satisﬁes the diﬀerential equation identically, that is, for all values of the\n",
      "independent variables.\n",
      "In ordinary diﬀerential equations, we generally denote a derivative by a\n",
      "prime on the variable being diﬀerentiated: y′, y′′, etc. A diﬀerential equation\n",
      "has an indeﬁnite number of solutions; for example, the diﬀerential equation\n",
      "y′′ −y = 0,\n",
      "in which x is the independent variable; that is, in which y′ ≡dy/dx and\n",
      "y′′ ≡dy′/dx, has solutions\n",
      "y = c1ex + c2e−x,\n",
      "where c1 and c2 are any constants.\n",
      "***Deﬁne terms general solution initial value, boundary value particular\n",
      "solution order\n",
      "*** types of ODEs order and degree separable\n",
      "*** methods of solution\n",
      "*** diﬀerence equations types, solutions\n",
      "Optimization\n",
      "Many statistical methods depend on maximizing something (e.g., MLE), or\n",
      "minimizing something, generally a risk (e.g., UMVUE, MRE) or something\n",
      "that has an intuitive appeal (e.g., squared deviations from observed values,\n",
      "“least squares”).\n",
      "First of all, when looking for an optimal solution, it is important to consider\n",
      "the problem carefully, and not just immediately diﬀerentiate something and\n",
      "set it equal to 0.\n",
      "A practical optimization problem often has constraints of some kind.\n",
      "min\n",
      "α\n",
      "f(x, α)\n",
      "s.t. g(x, α) ≤b.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "688\n",
      "0 Statistical Mathematics\n",
      "If the functions are diﬀerentiable, and if the minimum occurs at an interior\n",
      "point, use of the Lagrangian is usually the way to solve the problem.\n",
      "With the dependence on x suppressed, the Lagrangian is\n",
      "L(α, λ) = f(α) + λT(g(α) −b).\n",
      "Diﬀerentiating the Lagrangian and setting to 0, we have a system of equations\n",
      "that deﬁnes a stationary point, α∗.\n",
      "For twice-diﬀerentiable functions, we check to insure that it is a minimum\n",
      "by evaluating the Hessian,\n",
      "∇∇f(α)\n",
      "\f\f\f\n",
      "α=α∗.\n",
      "If this is positive deﬁnite, there is a local minimum at α∗.\n",
      "There are many other techniques useful in optimization problems, such\n",
      "as EM methods. We discuss various methods of optimization further in Sec-\n",
      "tion 0.4 beginning on page 822.\n",
      "Some Useful Limits\n",
      "There are a number of general forms of expressions involving fractions, expo-\n",
      "nentials, or trigonometric functions that occur frequently in limit operations.\n",
      "It is helpful to be familiar with some of these standard forms, so that when\n",
      "they arise in the course of a proof or derivation, we can quickly evaluate them\n",
      "and move on. I list some useful limits below, in no particular order.\n",
      "lim\n",
      "n→∞\n",
      "\u0010\n",
      "1 + x\n",
      "n\n",
      "\u0011n\n",
      "= ex.\n",
      "(0.0.106)\n",
      "lim\n",
      "h→0\n",
      "1 −cos(hx)\n",
      "h2\n",
      "= 1\n",
      "2x2.\n",
      "(0.0.107)\n",
      "Notes and References for Section 0.0\n",
      "It is important that the student fully understand the concept of a mathe-\n",
      "matical proof. Solow (2003) discusses the basic ideas, and Aigner and Ziegler\n",
      "(2010), whose title comes from a favorite phrase of Paul Erd¨os, give many\n",
      "well-constructed proofs of common facts.\n",
      "Gelbaum and Olmsted (1990, 2003) have remarked that mathematics is\n",
      "built on two types of things: theorems and counterexamples. Counterexamples\n",
      "help us to understand the principles in a way that we might miss if we only\n",
      "considered theorems. Counterexamples delimit the application of a theorem.\n",
      "They help us understand why each part of the hypothesis of a theorem is\n",
      "important.\n",
      "The book by Romano and Siegel (1986) is replete with examples that il-\n",
      "lustrate the “edges” of statistical properties. Other books of this general type\n",
      "in various areas of mathematics are listed below.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "689\n",
      "Khuri (2003) describes the important facts and techniques in advanced\n",
      "calculus and other areas of applied mathematics that should be in every statis-\n",
      "tician’s toolbox.\n",
      "Sometimes a step in a proof or a derivation may seem to be a “trick”,\n",
      "and the student may ask “how could I have thought of that?” Often these\n",
      "tricks involve thinking of an appropriate expansion or recognizing a convergent\n",
      "series. Jolley (1961) provides a useful compendium of convergent series.\n",
      "A book that I encountered rather late in my life in mathematics is\n",
      "Graham et al. (1994). This beautiful book presents many “tricks”, and puts\n",
      "them in a context in which they appear to be the natural thing to think of.\n",
      "Exercises for Section 0.0\n",
      "0.0.1. Prove Theorem 0.0.2.\n",
      "0.0.2. Use De Morgan’s laws to prove equation (0.0.20).\n",
      "0.0.3. Prove equations (0.0.22) and (0.0.24).\n",
      "0.0.4. Let (S, ◦) be a group with identity e. Let x be any element of S. Prove:\n",
      "a) x ◦e = e ◦x.\n",
      "b) x ◦x−1 = x−1 ◦x.\n",
      "c) e is unique.\n",
      "d) for given x, x−1 is unique.\n",
      "0.0.5. Let (G, ◦) be a group of functions on X and ◦is function composition.\n",
      "Show that the functions must be bijections. (Compare Example 0.0.4.)\n",
      "0.0.6. a) Show that a sequence {xn} ∈IRd that converges to x ∈IRd is a\n",
      "Cauchy sequence. (In IRd convergence is usually deﬁned in terms of\n",
      "the Euclidean metric, but that is not necessary.)\n",
      "b) Show that each Cauchy sequence is bounded.\n",
      "c) Show that if a Cauchy sequence has a subsequence that converges to\n",
      "x, then the original sequence converges to x.\n",
      "d) Finally, prove the Cauchy criterion: There is a number x ∈IRd\n",
      "to which the sequence {xn} ∈IRd converges iﬀ{xn} is a Cauchy\n",
      "sequence.\n",
      "0.0.7. Consider the set D ⊆IR2 where xi ∈D is\n",
      "xi = (i, 1/i).\n",
      "Deﬁne a total order on D using the ordinary order relations in IR.\n",
      "0.0.8. Prove Theorem 0.0.3.\n",
      "Hint: Use the fact that the characteristic must be 0.\n",
      "0.0.9. Using Deﬁnition 0.0.6, show that in the linear space S, for any x ∈S,\n",
      "0x = 0s,\n",
      "where 0s is the additive identity in S.\n",
      "0.0.10. Let ⟨·, ·⟩be an inner product on Ω. Show that for x, y ∈Ω,\n",
      "⟨x + y, x + y⟩≤⟨x, x⟩+ ⟨y, y⟩.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "690\n",
      "0 Statistical Mathematics\n",
      "0.0.11. Prove Theorem 0.0.5.\n",
      "0.0.12. Prove statement (0.0.33).\n",
      "0.0.13. Let x ∈IRd.\n",
      "a) Prove\n",
      "lim\n",
      "p→∞∥x∥p = max({|xi|}).\n",
      "b) Prove that h(x) = max({|xi|}) is a norm over IRd.\n",
      "0.0.14. Prove equations (0.0.51) through (0.0.55).\n",
      "0.0.15. Prove statement (0.0.59).\n",
      "0.0.16. Suppose ∥g(n)∥/∥f(n)∥→c as n →∞, where c is a ﬁnite constant.\n",
      "a) Show that g(n) ∈O(f(n)).\n",
      "b) Suppose also that g(n) ∈O(h(n)). What can you say about O(f(n))\n",
      "and O(h(n))?\n",
      "0.0.17. a) Prove statements (0.0.60) through (0.0.62).\n",
      "b) Prove statements (0.0.60) through (0.0.62) with little o in place of big\n",
      "O.\n",
      "0.0.18. Why is equation (0.0.43) true?\n",
      "0.0.19. Taylor series of real univariate functions.\n",
      "a) Consider\n",
      "f(x) =\n",
      "∞\n",
      "X\n",
      "n=0\n",
      "e−n cos(n2x).\n",
      "Show that the function is inﬁnitely diﬀerentiable but the Taylor series\n",
      "expansion about 0 converges only for x = 0.\n",
      "b) Consider\n",
      "f(x) =\n",
      "1\n",
      "1 + x2 .\n",
      "Show that the function is inﬁnitely diﬀerentiable but the Taylor series\n",
      "expansion about 0 converges only for |x| < 1.\n",
      "c) Consider\n",
      "f(x) =\n",
      "\u001a\n",
      "e−1/x2 for x ̸= 0\n",
      "0\n",
      "for x = 0.\n",
      "(i) Show that the function is inﬁnitely diﬀerentiable but that the Tay-\n",
      "lor series expansion about 0 does not converge to f(x) if x ̸= 0.\n",
      "(ii) In the notation of equation (0.0.63), what is Rn in this case?\n",
      "d) Consider\n",
      "f(x) = ex.\n",
      "Write out the Taylor series expansion about 0 and show that it con-\n",
      "verges to f(x) ∀x ∈IR.\n",
      "0.0.20. Use the ratio test to show that the series in equation (0.0.66) is convergent.\n",
      "0.0.21. Prove Theorem 0.0.16.\n",
      "0.0.22. Evaluation of deﬁnite integrals.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.0 Some Basic Mathematical Concepts\n",
      "691\n",
      "a) Evaluate the integral (0.0.84):\n",
      "I =\n",
      "Z ∞\n",
      "−∞\n",
      "exp(−x2/2)dx.\n",
      "Hint:\n",
      "Write I2 as the iterated integral in x and y with integrand\n",
      "exp(−(x2 + y2)/2) and then change to polar coordinates.\n",
      "b) Evaluate the integral\n",
      "Z ∞\n",
      "0\n",
      "sin(x)\n",
      "x\n",
      "dx,\n",
      "interpreted in the Lebesgue sense as\n",
      "lim\n",
      "t→∞\n",
      "Z t\n",
      "0\n",
      "sin(x)/x dx.\n",
      "Hint:\n",
      "Write this as an integral in a product space and show that\n",
      "Fubini’s theorem applies. See Billingsley (1995), page 235.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "692\n",
      "0 Statistical Mathematics\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "Measure and integration and the probability theory built on those topics are\n",
      "major ﬁelds in mathematics. The objective of this section is just to get enough\n",
      "measure theory to support the probability theory necessary for a solid foun-\n",
      "dation in statistical inference.\n",
      "Most of the early development in this section is for abstract objects; for\n",
      "each of these, however, there is a concrete instance that is relevant in proba-\n",
      "bility theory. Althought the original development of these concepts generally\n",
      "involved real numbers and the ideas were later generalized, nowadays it is more\n",
      "satisfying to develop general ideas in the abstract, and then to specialize them\n",
      "to real numbers or to whatever structure is of interest.\n",
      "We begin with abstract measurable spaces in Section 0.1.1 and then mea-\n",
      "sures over general spaces in Section 0.1.3. A measurable space together with\n",
      "a measure is a measure space.\n",
      "In Section 0.1.4, we discuss an important measure space, namely the re-\n",
      "als, the Borel σ-ﬁeld on the reals, and Lebesgue measure. In Section 0.1.5\n",
      "we discuss real-valued functions over the reals. We then discuss integration\n",
      "and diﬀerentiation. In the ordinary calculus, diﬀerentiation is usually intro-\n",
      "duced before integration, and then the two are associated by means of the\n",
      "“Fundamental Theorem”. In analysis, the order is usually reversed, and so in\n",
      "Section 0.1.6 we discuss integration of real functions, and then in Section 0.1.7\n",
      "we deﬁne derivatives.\n",
      "Sections 0.1.8 through 0.1.13 cover some basics of real functional analysis,\n",
      "including a calculus over functionals.\n",
      "0.1.1 Basic Concepts of Measure Theory\n",
      "Analysis depends heavily on a primitive concept of a set, or a collection of\n",
      "“elements”. We also use “space” as a primitives, but it is usually just a set\n",
      "that may have some special properties. We generally avoid nesting “set”; that\n",
      "is, rather than a “set of sets”, we speak of a collections of sets.\n",
      "The Sample Space and Subsets of It\n",
      "A sample space is a nonempty set. It is the “universe of discourse” in a given\n",
      "problem. It is often denoted by Ω. Interesting structures are built on a given\n",
      "set Ωby deﬁning some important types of collections of subsets of Ω.\n",
      "Special properties of collections of subsets of the sample space deﬁne π-\n",
      "systems, rings, ﬁelds or algebras, and Dynkin systems, or Sierpinski systems,\n",
      "λ-systems. The collection of subsets that constitute a general ﬁeld is closed\n",
      "with respect to ﬁnite unions. A very important ﬁeld is one in which the collec-\n",
      "tion of subsets is also closed with respect to countable unions. This is called\n",
      "a σ-ﬁeld.\n",
      "We will now deﬁne these systems.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "693\n",
      "Deﬁnition 0.1.1 (π-system)\n",
      "A nonempty collection of subsets, P, is called a π-system iﬀ\n",
      "(π1) A, B ∈P ⇒A ∩B ∈P.\n",
      "The name π-system comes from the condition that the collection includes\n",
      "products or intersections.\n",
      "Deﬁnition 0.1.2 (ring)\n",
      "A nonempty collection of subsets, R, is called a ring iﬀ\n",
      "(r1) A, B ∈R ⇒A ∪B ∈R.\n",
      "(r2) A, B ∈R ⇒A −B ∈R.\n",
      "Deﬁnition 0.1.3 (ﬁeld)\n",
      "A collection of subsets, F is called a ﬁeld iﬀ\n",
      "(a1) Ω∈F, and\n",
      "(a2) A ∈F ⇒Ac ∈F, and\n",
      "(a3) A, B ∈F ⇒A ∪B ∈F.\n",
      "A ﬁeld of sets is also called an algebra of sets. (Compare the deﬁnition above\n",
      "with the deﬁnition of the algebraic structure given in Deﬁnition 0.0.3 on\n",
      "page 631.)\n",
      "Notice that property (a3) is equivalent to\n",
      "(a′\n",
      "3) A1, A2, . . .An ∈F ⇒∪n\n",
      "i=1Ai ∈F;\n",
      "that is, F is closed under ﬁnite unions. The next systems we describe are\n",
      "closed under countable unions.\n",
      "Notice that a ﬁeld is nonempty by deﬁnition, although “nonempty” is not\n",
      "speciﬁed explicitly, as it is for a ring. A ﬁeld contains at least one set, Ω, and\n",
      "because Ωis nonempty, it contains two sets, Ωand ∅.\n",
      "Deﬁnition 0.1.4 (λ-system)\n",
      "A collection of subsets, L, is called a λ-system iﬀ\n",
      "(λ1) Ω∈L, and\n",
      "(λ2) A ∈L ⇒Ac ∈L, and\n",
      "(λ3) A1, A2, . . . ∈L and Ai ∩Aj = ∅for i ̸= j ⇒∪iAi ∈L.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "694\n",
      "0 Statistical Mathematics\n",
      "The deﬁnitions of π-systems, rings, and ﬁelds have involved only ﬁnite num-\n",
      "bers of sets. The name λ-system comes from its property that involves an\n",
      "operation on an inﬁnite number of sets, that is, a limiting operation. A λ-\n",
      "system is also called a Dynkin system or a Sierpinski system.\n",
      "We can see that the ﬁrst and third properties of a λ-system imply that\n",
      "the second property is equivalent to\n",
      "(λ′\n",
      "2) A, B ∈L and A ⊆B ⇒B −A ∈L.\n",
      "To see this, ﬁrst assume the three properties that characterize a λ-system L,\n",
      "and A, B ∈L and A ⊆B. We ﬁrst see that this implies Bc ∈L and so the\n",
      "disjoint union A ∪Bc ∈L. This implies that the complement (A ∪Bc)c ∈\n",
      "L. But (A ∪Bc)c = B −A; hence, we have the alternative property (λ′\n",
      "2).\n",
      "Conversely, assume this alternative property together with the ﬁrst property\n",
      "(λ1). Hence, A ∈L ⇒Ω−A ∈L, but Ω−A = Ac; that is, Ac ∈L.\n",
      "In a similar manner, we can show that the third property is equivalent to\n",
      "(λ′\n",
      "3) A1, A2, . . . ∈L with A1 ⊆A2 ⊆· · · ⇒∪iAi ∈L.\n",
      "Now, we deﬁne the most important type of system of collections of subsets:\n",
      "Deﬁnition 0.1.5 (σ-ﬁeld)\n",
      "A collection of subsets, F, of a given sample space, Ω, is called a σ-ﬁeld iﬀ\n",
      "(σ1) Ω∈F\n",
      "(σ2) A ∈F ⇒Ac ∈F\n",
      "(σ3) A1, A2, . . . ∈F ⇒∪iAi ∈F.\n",
      "A σ-ﬁeld is also called a σ-algebra or a a σ-ring. (Notice, however, that it\n",
      "is much more than a simple extension of a ring.)\n",
      "A ﬁeld with the properties (σ1) and (σ2), but with (σ3) replaced with the\n",
      "property\n",
      "(δ3) A1, A2, . . . ∈F ⇒∩iAi ∈F\n",
      "is called a δ-ﬁeld, a δ-algebra or a δ-ring. It is clear, however, that δ-ﬁeld and\n",
      "σ-ﬁeld are equivalent concepts. We will use the latter term.\n",
      "The deﬁnition of a σ-ﬁeld immediately implies that the ﬁeld is closed with\n",
      "respect to set diﬀerences.\n",
      "Theorem 0.1.1\n",
      "Given the σ-ﬁeld F, if A1, A2, . . . ∈F, then lim supn An ∈F and lim infn An ∈\n",
      "F.\n",
      "Proof. The conclusion follows because for any n, ∪∞\n",
      "i=nAi ∈F and ∩∞\n",
      "i=nAi ∈\n",
      "F.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "695\n",
      "Notice that the deﬁnitions of a π-system and of a ring must specify that\n",
      "the collections are nonempty; the deﬁnitions of the other systems ensure that\n",
      "the collections are nonempty without saying so explicitly.\n",
      "The exact deﬁnitions of these systems can be modiﬁed in various simple\n",
      "ways. For example, in the deﬁnitions of a ﬁeld, a λ-system, and a σ-ﬁeld the\n",
      "requirement that Ωbe in the system could be replaced by the requirement\n",
      "that ∅be in the system, because closure with respect to complementation\n",
      "guarantees the inclusion of Ω. (The requirement that Ωbe in a λ-system,\n",
      "however, could not be replaced by the requirement that ∅be in the system.)\n",
      "The closure property for unions in a ﬁeld or a σ-ﬁeld could be replaced by\n",
      "the requirement that the system be closed for intersections of the same kind\n",
      "as the unions.\n",
      "Before concluding this subsection, we will deﬁne another type of collection\n",
      "of subsets that is often useful in statistics.\n",
      "Deﬁnition 0.1.6 (σ-lattice)\n",
      "A nonempty collection of subsets, L, of a given sample space, Ω, is called a\n",
      "σ-lattice iﬀ\n",
      "(σl1) A1, A2, . . . ∈L ⇒∪iAi ∈L\n",
      "(σl2) A1, A2, . . . ∈L ⇒∩iAi ∈L.\n",
      "The most useful of the systems we have deﬁned is a σ-ﬁeld, and in the\n",
      "following sections, we will focus our attention on σ-ﬁelds.\n",
      "σ-Field Generated by a Collection of Sets\n",
      "Given a sample space Ωand any collection C of subsets of Ω, the intersection\n",
      "of all σ-ﬁelds over Ωthat contain C is called the σ-ﬁeld generated by C, and\n",
      "is denoted by\n",
      "σ(C).\n",
      "It is the minimal σ-ﬁeld that contains C; that is, the “smallest” σ-ﬁeld over\n",
      "Ωof which C is a subset.\n",
      "Given A1, A2, . . . ⊆Ω, we may use a similar notation to that above to\n",
      "refer to generated σ-ﬁelds. We use σ(A1) and σ(A1, A2) to refer respectively\n",
      "to σ({A1}) and σ({A1, A2}). That is, the argument in the operator σ(·) may\n",
      "be either a set or a collection of sets.\n",
      "A σ-ﬁeld can contain a very large number of subsets. If k is the maximum\n",
      "number of sets that partition Ωthat can be formed by operations on the sets\n",
      "in C, then the number of sets in the σ-ﬁeld is 2k. (What is the “largest” σ-ﬁeld\n",
      "over Ω?)\n",
      "Other special collections of subsets can also be generated by a given col-\n",
      "lection. For example, given a collection C of subsets of a sample space Ω, we\n",
      "can form a π-system by adding (only) enough subsets to make the collection\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "696\n",
      "0 Statistical Mathematics\n",
      "closed with respect to intersections. This system generated by C is the minimal\n",
      "π-system that contains C. This π-system, denoted by π(C), is the intersection\n",
      "of all π-systems that contain C. Likewise, we deﬁne the λ-system generated\n",
      "by C as the minimal λ-system that contains C, and we denote it by λ(C).\n",
      "Example 0.1.1 (σ-ﬁelds)\n",
      "1. The “trivial σ-ﬁeld” is {∅, Ω}.\n",
      "2. For the sample space Ω, σ({Ω}) is the trivial σ-ﬁeld, {∅, Ω}.\n",
      "3. If A = {A} with respect to the sample space Ω, σ(A) = {∅, A, Ac, Ω}.\n",
      "If A = ∅or A = Ω, this is the trivial σ-ﬁeld; otherwise, it is the second\n",
      "simplest σ-ﬁeld.\n",
      "4. If A = {A1, A2} and neither A1 nor A2 is a subset of the other, with\n",
      "respect to the sample space Ω, there are 4 “smallest” sets that partition\n",
      "A. These are called atoms. They are\n",
      "{A1 ∩A2, A1 −A2, A2 −A1, (A1 ∪A2)c}.\n",
      "Hence, there are 24 = 16 sets in σ(A). These can be written simply as the\n",
      "binary combinations of all above: (0000), (0001), (0010), ... Following this\n",
      "order, using the partition above, the sets are (after simpliﬁcation):\n",
      "σ(A) = {∅, (A1 ∪A2)c, A2 −A1, Ac\n",
      "1,\n",
      "A1 −A2, Ac\n",
      "2, A1∆A2, (A1 ∩A2)c,\n",
      "A1 ∩A2, (A1∆A2)c, A2, (A1 −A2)c,\n",
      "A1, (A2 −A1)c, A1 ∪A2, Ω}.\n",
      "Notice that σ({A1}) ⊆σ({A1, A2}).\n",
      "5. If A = {A1, A2} and A1 ⊆A2, with respect to the sample space Ω, there\n",
      "are 8 sets in σ(A).\n",
      "6. For the sample space Ω, the power set 2Ωis a σ-ﬁeld. It is the “largest”\n",
      "σ-ﬁeld over Ω.\n",
      "Notice the notation in the example above. Why do we have the braces\n",
      "in σ({Ω})? We do often abuse the notation, however; if the argument of σ(˙)\n",
      "is a singleton, we sometimes omit the braces. For example, the second most\n",
      "trivial σ-ﬁeld is that generated by a single set, say A: σ(A) = {∅, A, Ac, Ω}.\n",
      "We may also abuse the notation even further by writing a collection of subsets\n",
      "without putting them in braces. So, for example, σ(A, B) may be used instead\n",
      "of σ({A.B}).\n",
      "Example 0.1.2 (A ﬁeld that is not a σ-ﬁeld)\n",
      "Let Ωbe a countably inﬁnite set, and let F consist of all ﬁnite subsets of Ω\n",
      "along with all subsets of Ωwhose complements are ﬁnite. We see immediately\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "697\n",
      "that F is a ﬁeld. To see that it is not a σ-ﬁeld, all we must do is choose a set\n",
      "A that is countably inﬁnite and has inﬁnite complement. One such set can be\n",
      "constructed from a sequence ω1, ω2, . . ., and let A = {ω1, ω3, . . .}. Therefore\n",
      "for ﬁxed i, the singleton {ω2i−1} ∈F, but A /∈F even though A = ∪i{ω2i−1}.\n",
      "The sets in Example 0.1.2 are not speciﬁed exactly. How can we just give\n",
      "some property, such as ﬁniteness, and then specify all sets with this property?\n",
      "While it may seem obvious that we can do this, in order to do it, we are relying\n",
      "on the Axiom of Choice (see pages 617 and 674, although even that fact is not\n",
      "obvious. Other interesting collections of sets similar to those in Example 0.1.2\n",
      "can be formed. Instead of ﬁniteness, we may focus on countability. We ﬁrst\n",
      "deﬁne a related term: A set A is said to be cocountable iﬀAc is countable.\n",
      "Example 0.1.3 (A σ-ﬁeld that does not contain certain sets)\n",
      "Let Ωbe the universal set, and let F consist of all countable and cocountable\n",
      "subsets of Ω. Then F is a σ-ﬁeld on Ω(exercise).\n",
      "If Ωis uncountable, then is contains a set A such that both A and Ac are\n",
      "countable. Such a set A (or, equivalently, Ac) is not in F.\n",
      "Borel σ-Fields\n",
      "There is a particularly interesting type of σ-ﬁeld, called a Borel σ-ﬁeld, that\n",
      "can be deﬁned in topological spaces.\n",
      "Deﬁnition 0.1.7 (Borel σ-ﬁeld) (general)\n",
      "Let (Ω, T ) be a topological space. The σ-ﬁeld generated by T is the Borel\n",
      "σ-ﬁeld on (Ω, T ).\n",
      "We often denote this Borel σ-ﬁeld as B(Ω, T ) or as B(Ω).\n",
      "The most interesting topological space is the set of reals together with the\n",
      "class of open intervals, (IR, C). We denote the Borel σ-ﬁeld on this space as\n",
      "B(IR) or just as B.\n",
      "Relations of σ-Fields to Other Structures\n",
      "A σ-ﬁeld is a π-system, a ﬁeld, and a λ-system.\n",
      "Theorem 0.1.2\n",
      "A class that is both a π-system and a λ-system is a σ-ﬁeld.\n",
      "Proof. Because it is a λ-system, the class contains ∅and is closed under\n",
      "formation of complements, and because it is a π-system, it is closed under\n",
      "ﬁnite intersections. It is therefore a ﬁeld. Now, suppose that it contains sets\n",
      "Ai, for i = 1, 2, . . .. The class then contains the sets Bi = Ai ∩Ac\n",
      "1 ∩· · ·∩Ac\n",
      "i−1,\n",
      "which are necessarily disjoint. Because it is a λ-system, it contains ∪iBi. But\n",
      "∪iBi = ∪iAi, and since it contains ∪iAi it is a σ-ﬁeld.\n",
      "A useful fact is known as the π-λ theorem.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "698\n",
      "0 Statistical Mathematics\n",
      "Theorem 0.1.3 (the π-λ theorem)\n",
      "If P is a π-system and L is a λ-system, and if P ⊆L, then\n",
      "σ(P) ⊆L.\n",
      "The π-λ theorem is also called Dynkin’s π-λ theorem or Sierpinski’s π-λ the-\n",
      "orem.\n",
      "Proof. We use the given notation and assume the hypothesis. Let LP be the\n",
      "λ-system generated by P; that is,\n",
      "LP = λ(P).\n",
      "LP is the intersection of every λ-system that contains P, and it is contained\n",
      "in every λ-system that contains P. Thus, we have\n",
      "P ⊆LP ⊆L.\n",
      "It will now suﬃce to show that LP is also a π-system, because from the result\n",
      "above, if it is both a π-system and a λ-system it is a σ-ﬁeld, and it contains\n",
      "P so it must be the case that σ(P) ⊆LP because σ(P) is the minimal σ-ﬁeld\n",
      "that contains P.\n",
      "Now deﬁne a collection of sets whose intersection with a given set is a\n",
      "member of LP. For any set A, let\n",
      "LA = {B : A ∩B ∈LP}.\n",
      "Later in the proof, for some given set B, we use the symbol “LB” to denote\n",
      "the collection of sets whose intersection with B is a member of LP.\n",
      "If A ∈LP, then LA is a λ-system, as we see by checking the conditions:\n",
      "(λ1) A ∩Ω= A ∈LP so Ω∈LA\n",
      "(λ′\n",
      "2) If B1, B2 ∈LA and B1 ⊆B2, then LP contains A ∩B1 and A ∩B2, and\n",
      "hence contains the diﬀerence (A ∩B2) −(A ∩B1) = A ∩(B2 −B1); that\n",
      "is, B2 −B1 ∈LA.\n",
      "(λ3) If B1, B2, . . . ∈LA and Bi ∩Bj = ∅for i ̸= j, then LP contains the\n",
      "disjoint sets (A ∩B1), (A ∩B2), . . . and hence their union A ∩(∪iBi),\n",
      "which in turn implies ∪iBi ∈LA.\n",
      "Now because P is a π-system,\n",
      "A, B ∈P ⇒A ∩B ∈P\n",
      "⇒B ∈LA\n",
      "⇒P ⊆LA\n",
      "⇒LP ⊆LA.\n",
      "(The last implication follows from the minimality of LP and because LA is a\n",
      "λ-system containing P.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "699\n",
      "Using a similar argument as above, we have A ∈P and B ∩B ∈LP also\n",
      "imply A ∈LB (here LB is in the role of LA above) and we have\n",
      "A ∈LB ⇐⇒B ∈LA.\n",
      "Continuing as above, we also have P ⊆LB and LP ⊆LB.\n",
      "Now, to complete the proof, let B, C ∈LP. This means that C ∈LB,\n",
      "which from the above means that B ∩C ∈LP; that is, LP is a π-system,\n",
      "which, as we noted above is suﬃcient to imply the desired conclusion: σ(P) ⊆\n",
      "LP ⊆L.\n",
      "The π-λ theorem immediately implies that if P is a π-system then\n",
      "σ(P) = λ(P).\n",
      "(0.1.1)\n",
      "Operations on σ-Fields\n",
      "The usual set operators and set relations are used with collections of sets,\n",
      "and generally have the same meaning. If the collections of sets are σ-ﬁelds,\n",
      "the operation on the collections may not yield a collection that is a σ-ﬁeld,\n",
      "however.\n",
      "Theorem 0.1.4\n",
      "Given σ-ﬁelds F1 and F2 deﬁned with respect to a common sample space, the\n",
      "intersection, F1 ∩F2, is a σ-ﬁeld.\n",
      "Proof. Exercise.\n",
      "The union, F1 ∪F2, however, may not be a σ-ﬁeld. A simple counterex-\n",
      "ample with Ω= {a, b, c} is\n",
      "F1 = {∅, {a}, {b, c}, Ω}\n",
      "and\n",
      "F2 = {∅, {b}, {a, c}, Ω}.\n",
      "(0.1.2)\n",
      "The notation σ(F1 ∪F2) refers to the smallest σ-ﬁeld that contains all of\n",
      "the sets in either F1 or F2. For F1 and F2 as given above, we have\n",
      "σ(F1 ∪F2) = {∅, {a}, {b}, {c}, {a, b}, {a, c}, {b, c}, Ω}.\n",
      "Sub-σ-Fields\n",
      "A subset of a σ-ﬁeld that is itself a σ-ﬁeld is called a sub-σ-ﬁeld.\n",
      "Increasing sequences of σ-ﬁelds, F1 ⊆F2 ⊆· · ·, are often of interest,\n",
      "especially in stochastic processes.\n",
      "Given a σ-ﬁeld F, an interesting sub-σ-ﬁeld can be formed by taking a\n",
      "speciﬁc set B in F, and forming its intersection with all of the other sets in\n",
      "F. We often denote this sub-σ-ﬁeld as FB:\n",
      "FB = {B ∩A : A ∈F}.\n",
      "(0.1.3)\n",
      "It is an exercise to verify the three deﬁning properties of a σ-ﬁeld for FB.\n",
      "By the deﬁnition of σ(C) for a given collection C of subsets of Ωas the\n",
      "intersection of all σ-ﬁelds over Ωthat contain C, we have the trivial result\n",
      "σ(C1) ⊆σ(C2)\n",
      "if C1 ⊆C2.\n",
      "(0.1.4)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "700\n",
      "0 Statistical Mathematics\n",
      "Measurable Space: The Structure (Ω, F)\n",
      "If Ωis a sample space, and F is a σ-ﬁeld over Ω, the double (Ω, F) is called\n",
      "a measurable space.\n",
      "Measurable spaces are fundamental objects in our development of a theory\n",
      "of measure and its extension to probability theory.\n",
      "Partitions of the sample space by sets in the σ-ﬁeld are often useful. Notice\n",
      "that this is always possible, and, in fact, a ﬁnite partition always exists for if\n",
      "A ∈F, then Ac ∈F, and A ∩Ac = ∅and A ∪Ac = Ω. This partitioning of\n",
      "the sample space, which is often called a decomposition of the sample space,\n",
      "is useful, especially in working with simple functions, as we will see later.\n",
      "Notice that no measure is required for a measurable space. (We will deﬁne\n",
      "“measure” below, Deﬁnition 0.1.10. It is a scalar extended-real-valued non-\n",
      "negative set function whose domain is a σ-ﬁeld with the properties that the\n",
      "measure of the null set is 0 and the measure of the union of any collection\n",
      "of disjoint sets is the sum of the measures of the sets. A measurable space\n",
      "together with a measure forms a structure called a measure space.) We will\n",
      "consider measures and measure spaces in Section 0.1.3. For now, we continue\n",
      "discussions of measurability without reference to a speciﬁc measure.\n",
      "Subspaces\n",
      "Given a measurable space (Ω, F), and a set B ∈F, we have seen how to form\n",
      "a sub-σ-ﬁeld FB. This immediately yields a sub measurable space (B, FB), if\n",
      "we take the sample space to be Ω∩B = B.\n",
      "Cartesian Products\n",
      "The cartesian product of two sets A and B, written A × B, is the set of\n",
      "all doubletons, (ai, bj), where ai ∈A and bj ∈B. The cartesian product of\n",
      "two collections of sets is usually interpreted as the collection consisting of all\n",
      "possible cartesian products of the elements of each, e.g., if A = {A1, A2} and\n",
      "B = {B1, B2}\n",
      "A × B = {A1 × B1, A1 × B2, A2 × B1, A2 × B2},\n",
      "that is,\n",
      "{{(a1i, b1j) | a1i ∈A1, b1j ∈B1}, {(a1i, b2j) | a1i ∈A1, b2j ∈B2},\n",
      "{(a2i, b1j) | a2i ∈A2, b1j ∈B1}, {(a2i, b2j) | a2i ∈A2, b2j ∈B2}}.\n",
      "The cartesian product of two collections of sets is not a very useful object,\n",
      "because, as we see below, important characteristics of the collections, such as\n",
      "being σ-ﬁelds do not carry over to the product.\n",
      "Two measurable spaces (Ω1, F1) and (Ω2, F2) can be used to form a carte-\n",
      "sian product measurable space with sample space Ω1 ×Ω2. The product of the\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "701\n",
      "σ-ﬁelds is not necessarily a σ-ﬁeld. A simple counterexample is the same as\n",
      "we have used before with Ω= {a, b, c}. Let\n",
      "F1 = {{a}, {b, c}, ∅, Ω}\n",
      "and\n",
      "F2 = {{b}, {a, c}, ∅, Ω}.\n",
      "The product F1 × F2 contains 8 sets of doubletons, two of which are {(a, b)}\n",
      "and {(b, b), (c, b)}; however, we see that their union {(a, b), (b, b), (c, b)} is not\n",
      "a member of F1 × F2; hence, F1 × F2 is not a σ-ﬁeld.\n",
      "As another example, let Ω= IR, let F = σ(IR+) = {∅, IR+, IR −IR+, IR},\n",
      "let G1 = σ(IR+ × IR+), and let G2 = σ({Fi × Fj : Fi, Fj ∈F}). We see that\n",
      "G1 ̸= G2, because, for example, IR+ × IR is in G2 but it is not in G1.\n",
      "Deﬁnition 0.1.8 (cartesian product measurable space)\n",
      "Given the measurable spaces (Ω1, F1) and (Ω2, F2), we deﬁne the cartesian\n",
      "product measurable space as\n",
      "(Ω1 × Ω2, σ(F1 × F2)).\n",
      "As noted above, the collection σ(F1 × F2) is not the same as F1 × F2.\n",
      "Product measure spaces provide us the basis for developing a probability\n",
      "theory for vectors and multivariate distributions.\n",
      "0.1.2 Functions and Images\n",
      "A function is a set of ordered pairs such that no two pairs have the same\n",
      "ﬁrst element. If (a, b) is an ordered pair in f, then a is called an argument of\n",
      "the function, b is called the corresponding value of the function, and we write\n",
      "b = f(a). The set of all arguments of the function is called the domain of the\n",
      "function, and the set of all values of the function is called the range of the\n",
      "function. If the arguments of the function are sets, the function is called a set\n",
      "function.\n",
      "We will be interested in a function, say f, that maps one measurable space\n",
      "(Ω, F) to another measurable space (Λ, G). We may write f : (Ω, F) 7→(Λ, G),\n",
      "or just f : Ω7→Λ because the argument of the function is an element of Ω\n",
      "(in fact, any element of Ω) and the value of the function is an element of Λ.\n",
      "It may not be the case that all elements of Λ are values of f. If it is the case\n",
      "that for every element λ ∈Λ, there is an element ω ∈Ωsuch that f(ω) = λ,\n",
      "then the function is said to be “onto” Λ. Such a function is called surjective.\n",
      "(Any function is surjective with respect to its range.)\n",
      "Note the convention that we are adopting here: the domain of the function\n",
      "is the sample space. If we wish to restrict the domain of deﬁnition of a function,\n",
      "we do so by redeﬁning the sample space.\n",
      "If f : Ω7→Λ and ∀x, y ∈Ω, f(x) = f(y) ⇒x = y, then the function is\n",
      "said to be one-to-one. If a function from Ωto Λ is one-to-one and surjective,\n",
      "it is said to be bijective and the function itself is called a bijection.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "702\n",
      "0 Statistical Mathematics\n",
      "If (a, b) ∈f, we may write a = f−1(b), although sometimes this notation\n",
      "is restricted to the cases in which f is one-to-one. (There are some subtleties\n",
      "here if f is not one-to-one. In that case, if the members of the pairs in f are\n",
      "reversed, the resulting set is not a function. We may then say f−1 does not\n",
      "exist; yet we may write a = f−1(b), with the meaning above. It is perhaps\n",
      "more appropriate to take f−1(b) to be an equivalence class, are if f(a) = b to\n",
      "say that a ∈f−1(b). We will not attempt to accommodate these subtleties,\n",
      "however.)\n",
      "If A ⊆Ω, the image of A, denoted by f[A], is the set of all λ ∈Λ for which\n",
      "λ = f(ω) for some ω ∈A. Likewise, if C is a collection of subsets of Ω, the\n",
      "image of C, denoted by f[C], or just by f(C), is the collection of all subsets\n",
      "of Λ that are images of the subsets of C. (While I prefer the notation “[·]”\n",
      "when the argument of the function is a set or a collection of sets — unless\n",
      "the function is a set function — in some cases I will do like most other people\n",
      "and just use the “(·)”, which actually applies more properly to an element.)\n",
      "For a subset B of Λ, the inverse image or the preimage of B, denoted by\n",
      "f−1[B], is the set of all ω ∈Ωsuch that f(ω) ∈B. We also write f[f−1[B]]\n",
      "as f ◦f−1[B]. The set f[f−1[B]] may be a proper subset of B; that is, there\n",
      "may be an element λ in B for which there is no ω ∈Ωsuch that f(ω) = λ. If\n",
      "there is no element ω ∈Ωsuch that f(ω) ∈B, then f−1[B] = ∅.\n",
      "We see from the foregoing that f−1[Λ] = Ω, although it may be the case\n",
      "that f[Ω] ̸= Λ. Because f is deﬁned at points in Ω(and only there), we see\n",
      "that f[∅] = ∅and f−1[∅] = ∅.\n",
      "The following theorems state useful facts about preimages.\n",
      "Theorem 0.1.5\n",
      "Let f : Ω7→Λ. For B ⊆Λ,\n",
      "f−1[Bc] = (f−1[B])c.\n",
      "(Here, Bc = Λ −B, and (f−1[B])c = Ω−f−1[B]).)\n",
      "Proof.\n",
      "We see this in the standard way by showing that each is a subset of the other.\n",
      "Let ω be an arbitrary element of Ω.\n",
      "Suppose ω ∈f−1[Bc]. Then f(ω) ∈Bc, so f(ω) /∈B, hence ω /∈f−1[B],\n",
      "and so ω ∈(f−1[B])c. We have f−1[Bc] ⊆(f−1[B])c.\n",
      "Now suppose ω ∈(f−1[B])c. Then ω /∈f−1[B], so f(ω) /∈B, hence f(ω) ∈\n",
      "Bc, and so ω ∈f−1[Bc]. We have (f−1[B])c ⊆f−1[Bc].\n",
      "Theorem 0.1.6\n",
      "Let f : Ω7→Λ, and let A1, A2 ⊆Λ with A1 ∩A2 = ∅. Then\n",
      "f−1[A1] ∩f−1[A2] = ∅.\n",
      "Proof. Exercise.\n",
      "Notice that the theorem does not hold in the other direction, unless f\n",
      "is bijective. That is, B1, B2 ⊆Ωwith B1 ∩B2 = ∅does not imply that\n",
      "f[B1] ∩f[B2] = ∅.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "703\n",
      "Theorem 0.1.7\n",
      "Let f : Ω7→Λ, and let A1, A2, . . . ⊆Λ. Suppose (∪∞\n",
      "i=1Ai) ⊆Λ, then\n",
      "f−1[∪∞\n",
      "i=1Ai] = ∪∞\n",
      "i=1f−1(Ai).\n",
      "Proof.\n",
      "We see this as above; again, let λ be an arbitrary element of Λ.\n",
      "Suppose λ ∈f−1[∪∞\n",
      "i=1Ai]. Then f(λ) ∈∪∞\n",
      "i=1Ai, so for some j, f(λ) ∈\n",
      "Aj and λ ∈f−1[Aj]; hence λ ∈∪∞\n",
      "i=1f−1[Ai]. We have f−1[∪∞\n",
      "i=1Ai] ⊆\n",
      "∪∞\n",
      "i=1f−1[Ai].\n",
      "Now suppose λ ∈∪∞\n",
      "i=1f−1[Ai]. Then for some j, λ ∈f−1[Aj], so f(λ) ∈\n",
      "Aj and f(λ) ∈∪∞\n",
      "i=1Ai; hence λ ∈f−1[∪∞\n",
      "i=1Ai]. We have ∪∞\n",
      "i=1f−1[Ai] ⊆\n",
      "f−1[∪∞\n",
      "i=1Ai], and so the two sets are the same.\n",
      "It is worth noting a ﬁnite-union version of this result:\n",
      "f−1[A1 ∪A2] = f−1[A1] ∪f−1[A2].\n",
      "For bijective functions, we have similar relationships for intersections and\n",
      "set diﬀerences, but in general, f−1[A1 ∩A2] ̸= f−1[A1] ∩f−1[A2].\n",
      "If f : (Ω, F) 7→(Λ, G), the σ-ﬁelds in the measurable spaces determine cer-\n",
      "tain properties of the function, the most important of which is measurability.\n",
      "Measurable Functions\n",
      "We have been discussing measurability without discussing a measure. We\n",
      "continue in this vein for one more concept; that of a measurable function. The\n",
      "importance of the concept of a measurable function from the measurable space\n",
      "(Ω, F) to the measurable space (Λ, G) is that it allows a measure deﬁned on\n",
      "F to be used immediately in G. We discuss measure formally in Section 0.1.3.\n",
      "Deﬁnition 0.1.9 (measurable function)\n",
      "If (Ω, F) and (Λ, G) are measurable spaces, and f is a mapping from Ωto Λ,\n",
      "with the property that ∀A ∈G, f−1[A] ∈F, then f is a measurable function\n",
      "with respect to F and G. It is also said to be measurable F/G.\n",
      "For a real-valued function, that is, a mapping from Ωto IR with σ-ﬁeld\n",
      "B(IR), or in other cases where there is an “obvious” σ-ﬁeld, we often just say\n",
      "that the function is measurable with respect to F. In any event, the role of F\n",
      "is somewhat more important. We use the notation f ∈F to denote the fact\n",
      "that f is measurable with respect to F. (Note that this is an abuse of the\n",
      "notation, because f is not one of the sets in the collection F.)\n",
      "Given the measurable spaces (Ω, F) and (Λ, G) and a mapping f from Ω\n",
      "to Λ, we also call call f a mapping from (Ω, F) to (Λ, G).\n",
      "Note that a measurable function f(·) does not depend on a measure. The\n",
      "domain of f(·) has no relationship to F, except through the range of f(·) that\n",
      "happens to be in the subsets in G.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "704\n",
      "0 Statistical Mathematics\n",
      "σ-Field Generated by a Measurable Function\n",
      "If f is a measurable function from (Ω, F) to (Λ, G), then we can see that\n",
      "f−1[G] is a sub-σ-ﬁeld of F (Exercise 0.1.5). We call this the σ-ﬁeld generated\n",
      "by f, and write it as σ(f). Now we have a third type that the argument in\n",
      "the operator σ(·) may be. It can be either a set, a collection of sets, or a\n",
      "measurable function.\n",
      "For measurable functions f and g from the same measurable space (Ω, F)\n",
      "to the same measurable space (Λ, G) we may write σ(f, g), with the meaning\n",
      "σ(f, g) = σ\n",
      "\u0000f−1[G] ∪g−1[G]\n",
      "\u0001\n",
      ".\n",
      "(0.1.5)\n",
      "As with σ-ﬁelds generated by collections of sets in equation (0.1.4), it is clear\n",
      "that\n",
      "σ(f) ⊆σ(f, g).\n",
      "(0.1.6)\n",
      "For measurable functions f and g from (Ω, F) to (Ω, F), it is clear (exer-\n",
      "cise) that\n",
      "σ(g ◦f) ⊆σ(f).\n",
      "(0.1.7)\n",
      "0.1.3 Measure\n",
      "A measure is a scalar extended-real-valued nonnegative set function whose\n",
      "domain is a σ-ﬁeld, with some useful properties, as stated next.\n",
      "Deﬁnition 0.1.10 (measure)\n",
      "Given a measurable space (Ω, F), a function ν deﬁned on F is a measure if\n",
      "1. ν(∅) = 0,\n",
      "2. ∀A ∈F, ν(A) ∈[0, ∞],\n",
      "3. if A1, A2, . . . ∈F are disjoint, then\n",
      "ν(∪∞\n",
      "i=1Ai) =\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "ν(Ai).\n",
      "(0.1.8)\n",
      "An immediate generalization is a vector measure, which is a similar func-\n",
      "tion whose range R is a Banach space and the series on the right of equa-\n",
      "tion (0.1.8) is convergent in the of the Banach space.\n",
      "Two generalizations of measure are signed measure and outer measure.\n",
      "Deﬁnition 0.1.11 (signed measure)\n",
      "Given a measurable space (Ω, F), a function σ deﬁned on F is a signed measure\n",
      "if\n",
      "1. σ(∅) = 0,\n",
      "2. ∀A ∈F, σ(A) ∈[−∞, ∞],\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "705\n",
      "3. if A1, A2, . . . ∈F are disjoint, then\n",
      "σ(∪∞\n",
      "i=1Ai) =\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "σ(Ai).\n",
      "(0.1.9)\n",
      "Deﬁnition 0.1.12 (outer measure)\n",
      "An outer measure is an extended-real-valued function ν on the power set of a\n",
      "given sample space Ωwith the properties\n",
      "1. ν(∅) = 0,\n",
      "2. ∀A ⊆Ω, ν(A) ∈[0, ∞],\n",
      "3. ∀A, B ⊆Ω, A ⊆B ⇒ν(A) ≤ν(B),\n",
      "4. if A1, A2, . . . ⊆Ω, then\n",
      "ν(∪∞\n",
      "i=1Ai) ≤\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "ν(Ai).\n",
      "(0.1.10)\n",
      "An outer measure is useful when it is inconvenient to work with disjoint\n",
      "sets as required in the deﬁnition of a measure.\n",
      "Properties of Measures\n",
      "Several properties of a measure are derived immediately from Deﬁnition 0.1.10.\n",
      "Theorem 0.1.8 (monotonicity)\n",
      "Let ν be a measure with domain F. If A1 ⊆A2 ∈F, then ν(A1) ≤ν(A2)\n",
      "Proof. Exercise.\n",
      "Theorem 0.1.9 (subadditivity)\n",
      "Let ν be a measure with domain F. If A1, A2, . . . ∈F, then ν(∪iAi) ≤\n",
      "P\n",
      "i ν(Ai).\n",
      "Proof. Exercise. (Hint:\n",
      "Use the sequence (0.0.6) in Theorem 0.0.1. Notice\n",
      "that you must show that each Di of that theorem is in F.)\n",
      "Theorem 0.1.10 (continuity from below)\n",
      "Let ν be a measure with domain F. If A1 ⊆A2 ⊆. . . ∈F, then ν(∪∞\n",
      "i=1Ai) =\n",
      "limi→∞ν(Ai).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "706\n",
      "0 Statistical Mathematics\n",
      "Proof.\n",
      "Let {Dn} be the sequence of disjoint sets deﬁned in equation (0.0.7); that is,\n",
      "Dj = Aj+1 −Aj and\n",
      "∪∞\n",
      "i=1Di = ∪∞\n",
      "i=1Ai.\n",
      "By the closure property for set diﬀerences in a σ-ﬁeld, each Dj is in F. We\n",
      "have\n",
      "ν (∪∞\n",
      "i=1Ai) = ν (∪∞\n",
      "i=1Di)\n",
      "=\n",
      "∞\n",
      "X\n",
      "i=1\n",
      "ν(Di)\n",
      "= lim\n",
      "i→∞\n",
      "i\n",
      "X\n",
      "j=1\n",
      "ν(Dj)\n",
      "= lim\n",
      "i→∞ν\n",
      "\u0000∪i\n",
      "j=1Dj\n",
      "\u0001\n",
      "= lim\n",
      "i→∞ν(Ai).\n",
      "Sequences of nested intervals are important. We denote a sequence A1 ⊆\n",
      "A2 ⊆. . . with A = ∪∞\n",
      "i=1Ai, as Ai ↗A. (This same notation is used for a\n",
      "sequence of real numbers xi such that x1 ≤x2 · · · and lim xi = x, where we\n",
      "write xi ↗x.)\n",
      "Continuity from below is actually a little stronger than what is stated\n",
      "above, because the sequence of values of the measure is also monotonic: for\n",
      "Ai ∈F, Ai ↗A ⇒ν(Ai) ↗ν(A).\n",
      "Although we deﬁned the continuity from below, we could likewise deﬁne\n",
      "continuity from above for a sequence A1 ⊃A2 ⊃. . . ∈F in which ν(A1) < ∞.\n",
      "We let A = ∩∞\n",
      "i=1Ai, and we denote this as Ai ↘A. Continuity from above\n",
      "is the fact that for such a sequence ν(Ai) ↘ν(A). The proof uses methods\n",
      "similar to those of the proof of Theorem 0.1.10 along with De Morgan’s laws\n",
      "with complementation being taken with respect to A1. The condition that\n",
      "ν(A1) < ∞is crucial, that is, certain measures may not be continuous from\n",
      "above (exercise). A probability measure, in which ν(Ω) = 1 (Deﬁnition 0.1.14),\n",
      "is continuous from above.\n",
      "Without qualifying the property as “from below” or “from above”, because\n",
      "both obtain, we can say the measure is continuous.\n",
      "Notice that the deﬁnition of a measure does not preclude the possibility\n",
      "that the measure is identically 0. This often requires us to specify “nonzero\n",
      "measure” in order to discuss nontrivial properties. Another possibility, of\n",
      "course, would be just to specify ν(Ω) > 0 (remember Ω̸= ∅in a measur-\n",
      "able space).\n",
      "To evaluate ν(∪iAi) we form disjoint sets by intersections. For example,\n",
      "we have ν(A1 ∪A2) = ν(A1) + ν(A2) −ν(A1 ∩A2). This is an application of\n",
      "the simplest form of the inclusion-exclusion formula (see page 619). If there\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "707\n",
      "are three sets, we take out all pairwise intersections and then add back in the\n",
      "triple intersection. We can easily extend this (the proof is by induction) so\n",
      "that, in general for n ≥4, we have\n",
      "ν(∪n\n",
      "i Ai) = P\n",
      "1≤i≤n ν(Ai) −P\n",
      "1≤i<j≤n ν(Ai ∩Aj)\n",
      "+ P\n",
      "1≤i<j<k≤n ν(Ai ∩Aj ∩Ak) −· · ·\n",
      "+(−1)n+1ν(A1 ∩· · · ∩An).\n",
      "(0.1.11)\n",
      "Some General Types of Measures\n",
      "There are some types of measures that deserve special attention. Some of\n",
      "these are general classes of measures such as ﬁnite measures and Radon mea-\n",
      "sures, and others are speciﬁc measures, such as the counting measure and the\n",
      "Lebesgue measure (see page 717). Some types are deﬁned for any measur-\n",
      "able space, and other types are deﬁned only for measurable spaces with some\n",
      "additional structure, such as a topology.\n",
      "Recall that measures take values in the extended nonnegative reals, [0, ∞].\n",
      "Deﬁnition 0.1.13 (ﬁnite measure)\n",
      "A measure ν such that ν(Ω) < ∞is called a ﬁnite measure.\n",
      "An important ﬁnite measure is a probability measure.\n",
      "Deﬁnition 0.1.14 (probability measure)\n",
      "A measure whose domain is a σ-ﬁeld deﬁned on the sample space Ωwith the\n",
      "property that ν(Ω) = 1 is called a probability measure. We often use P to\n",
      "denote such a measure.\n",
      "Probability measures and their applications are discussed in Chapter 1.\n",
      "Deﬁnition 0.1.15 (σ-ﬁnite measure)\n",
      "A measure ν is σ-ﬁnite on (Ω, F) iﬀthere exists a sequence A1, A2, . . . in F\n",
      "such that ∪iAi = Ωand ν(Ai) < ∞for all i.\n",
      "A ﬁnite measure is obviously σ-ﬁnite. In integration theory, many impor-\n",
      "tant results (for example Fubini’s theorem and the Radon-Nikodym theorem)\n",
      "depend on the measures being σ-ﬁnite.\n",
      "Deﬁnition 0.1.16 (complete measure)\n",
      "A measure ν deﬁned on the σ-ﬁeld F is said to be complete if A1 ⊆A ∈F\n",
      "and ν(A) = 0 implies A1 ∈F.\n",
      "Completeness of a measure means that all subsets of measurable sets with\n",
      "measure 0 and also measurable, and have measure 0. (For an A1 in the deﬁ-\n",
      "nition above, clearly, ν(A1) = 0.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "708\n",
      "0 Statistical Mathematics\n",
      "Deﬁnition 0.1.17 (Radon measure)\n",
      "In a topological measurable space (Ω, F), a measure µ such that for every\n",
      "compact set B ∈F, µ(B) < ∞is called a Radon measure.\n",
      "A Radon measure is σ-ﬁnite, although it is not necessarily ﬁnite.\n",
      "Deﬁnition 0.1.18 (Haar invariant measures)\n",
      "Let Ωbe a group, and let (Ω, F) be a topological measurable space. For B ∈F\n",
      "and x ∈Ω, let Bx = {yx : y ∈B} (where “xy” represents the element of Ω\n",
      "formed by the group operation on x and y), and let xBx = {xy : y ∈B}.\n",
      "a) Let µr be a measure such that for any B ∈F and x ∈Ω, if Bx ∈F then\n",
      "µr(Bx) = µr(B). Then µr is said to be a right invariant Haar measure.\n",
      "b) Let µl be a measure such that for any B ∈F and x ∈Ω, if xB ∈F then\n",
      "µl(Bx) = µl(B). Then µl is said to be a left invariant Haar measure.\n",
      "c) If µ is a right invariant Haar measure and a left invariant Haar measure,\n",
      "then µ is an invariant Haar measure.\n",
      "If Ωis Abelian, then both right and left invariant Haar measures are\n",
      "invariant Haar measures.\n",
      "Some Useful Speciﬁc Measures\n",
      "Deﬁnition 0.1.19 (Dirac measure)\n",
      "Let (Ω, F) be a measurable space, let A, B ∈F, and let ω ∈B. The Dirac\n",
      "measure of A concentrated at ω, usually denoted by δω, is deﬁned as\n",
      "δω(A) =\n",
      "\u001a 1 if ω ∈A\n",
      "0 otherwise.\n",
      "(0.1.12)\n",
      "It is clear from Deﬁnition 0.1.10 that δω is a measure, and further, it is a\n",
      "Radon measure (exercise).\n",
      "Deﬁnition 0.1.20 (counting measure)\n",
      "Let (Ω, F) be a measurable space, and assume that every A ∈F is countable.\n",
      "The counting measure is deﬁned as\n",
      "γ(A) = #(A),\n",
      "(0.1.13)\n",
      "where #(A) = ∞if A is countably inﬁnite.\n",
      "The counting measure is σ-ﬁnite. (Notice that the counting measure is only\n",
      "deﬁned for the case that Ωis countable.) If Ωis ﬁnite, the counting measure\n",
      "is ﬁnite.\n",
      "If the sets of F are all countable the most common measure in applications\n",
      "is the counting measure. The counting measure is the most useful measure over\n",
      "the ring of integers ZZ with the σ-ﬁeld 2ZZ.\n",
      "Other speciﬁc measures for metric spaces (in particular IR) are the Borel\n",
      "measure and the Lebesgue measure, which we will discuss on page 717.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "709\n",
      "Measure Space: The Structure (Ω, F, ν)\n",
      "If Ωis a sample space, F is a σ-ﬁeld over Ω, and ν is a measure with domain\n",
      "F, the triple (Ω, F, ν) is called a measure space (compare measurable space,\n",
      "above).\n",
      "The elements in the measure space can be any kind of objects. They do\n",
      "not need to be numbers.\n",
      "Deﬁnition 0.1.21 (complete measure space)\n",
      "If the measure ν in the measure space (Ω, F, ν) is complete, then we say that\n",
      "the measure space is a complete measure space.\n",
      "A complete measure space is a Banach space if the norm in the Banach\n",
      "space is deﬁned in terms of the measure of the measure space.\n",
      "Deﬁnition 0.1.22 (probability space; event)\n",
      "If P in the measure space (Ω, F, P ) is a probability measure, the triple\n",
      "(Ω, F, P ) is called a probability space. A set A ∈F is called an “event”.\n",
      "Restricted Measures and Sub-Measure Spaces\n",
      "If (Ω, F) is a measurable space with measure ν, and A ⊆F is a sub-σ-ﬁeld,\n",
      "then the function νA that is the same as ν on FB and undeﬁned elsewhere is\n",
      "a measure. We say that νA is the “measure ν restricted to A”.\n",
      "Because νA is a measure on A, (Ω, A, νA) is a measure space. It is a sub\n",
      "measure space of (Ω, F, ν), and it corresponds in a natural way to all the usual\n",
      "subsetting operations.\n",
      "If (Ω, F, ν) is a measure space, and for some set B ∈F, (B, FB) is a\n",
      "sub measurable space as described above, then the function νB, which is the\n",
      "same as ν on FB and undeﬁned elsewhere, is a measure (Exercise 0.1.17), and\n",
      "(B, FB, νB) is a measure space.\n",
      "We say that νB is the “measure ν restricted to FB”.\n",
      "Measurable Set\n",
      "If ν is a measure with domain F then every set in F is said to be ν-measurable,\n",
      "or just measurable.\n",
      "Note that unlike other terms above that involve “measurable”, this term\n",
      "is deﬁned in terms of a given measure.\n",
      "As usual, we can get a better feel for a concept if we consider situations\n",
      "in which the concept does not apply; hence, we look for some set that is not\n",
      "measurable. We will consider a simple example of a nonmeasurable set, called\n",
      "a Vitali set, in Section 0.1.4.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "710\n",
      "0 Statistical Mathematics\n",
      "Almost Everywhere (a.e.) and Negligible Sets\n",
      "Given a measure space, (Ω, F, ν), a property that holds for all elements of F\n",
      "the σ-ﬁeld with positive measure is said to hold ν-almost everywhere, or ν-a.e.\n",
      "This is also sometimes written as a.e.[ν]. Also, when the measure is obvious,\n",
      "we often use the phrase almost everywhere or a.e. without explicit reference\n",
      "to the measure.\n",
      "A set A ∈F such that ν(A) = 0 is said to be a ν-negligible set, or just a\n",
      "negligible set when the measure is obvious.\n",
      "As we have deﬁned it, “for all” or “everywhere” implies “almost every-\n",
      "where”. Almost everywhere is sometimes deﬁned in such a way that it requires\n",
      "there to be a set in F with zero measure over which the property does not\n",
      "hold. Although it is only in such cases that the distinction between “for all”\n",
      "and “almost everywhere” is needed, it does not seem necessary to require the\n",
      "existence of a measurable set over which the property does not hold in order\n",
      "to deﬁne almost everywhere.\n",
      "A property that holds a.e. with respect to a probability measure is said to\n",
      "hold almost surely, or a.s. (There is no essential diﬀerence in the two phrases.)\n",
      "Support of a Measure\n",
      "For a general measure space (Ω, F, ν), a “support” of the measure may be\n",
      "deﬁned as any A ∈F such that ν(Ac) = 0. If the measure is ﬁnite, A ∈F\n",
      "is a support iﬀν(A) = ν(Ω). This deﬁnition, which is used by some authors\n",
      "(Billingsley (1995), for example), is not very useful in practice; in particular,\n",
      "it does not lead to a practical concept in probability distributions. A more\n",
      "useful deﬁnition of support of a measure requires restrictions on the measure\n",
      "space. If the measure space (Ω, F, ν) is a topological space or a space with a\n",
      "metric (that is, if points in the space have neighborhoods) and if ν is deﬁned\n",
      "for some ϵ-neighborhood of every ω ∈Ω, then we deﬁne the topological support\n",
      "of ν as\n",
      "S(ν) = {ω ∈Ω| ν(N(ω)) > 0}.\n",
      "(0.1.14)\n",
      "We say ν is concentrated on S(ν). The topological support is also called just\n",
      "the support or the spectrum.\n",
      "The support of a measure, when it is deﬁned, has some interesting prop-\n",
      "erties, such as closure, but we will not pursue this topic here. We will deﬁne\n",
      "support of a probability distribution of a random variable later.\n",
      "Relations of One Measure to Another\n",
      "From the deﬁnition of measure, we see that the class of measures on a given\n",
      "measurable space (Ω, F) form a linear space; that is, if µ and ν are measures\n",
      "on (Ω, F) and a ∈IR, then aµ + ν is a measure on (Ω, F).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "711\n",
      "The are several kinds of relationships between measures on a given mea-\n",
      "surable space or related measurable spaces that are interesting. Some of these\n",
      "relationships are equivalence relations, but some are not symmetric. The in-\n",
      "teresting relationship are generally transitive, however, and so an ordering on\n",
      "the space of measures could be constructed.\n",
      "Deﬁnition 0.1.23 (dominating measure; absolute continuity; equivalence)\n",
      "Given measures ν and µ on the same measurable space, (Ω, F), if ∀A ∈F\n",
      "ν(A) = 0\n",
      "⇒\n",
      "µ(A) = 0,\n",
      "then µ is said to be dominated by ν and we denote this by\n",
      "µ ≪ν.\n",
      "In this case we also say that µ is absolutely continuous with respect to ν.\n",
      "If µ ≪ν and ν ≪µ, then µ and ν are equivalent, and we write\n",
      "µ ≡ν.\n",
      "The deﬁnition says that µ ≪ν iﬀevery ν-negligible set is a µ-negligible\n",
      "set.\n",
      "If µ is ﬁnite (that is, if µ(A) < ∞∀A ∈F), the absolute continuity of µ\n",
      "with respect to ν can be characterized by an ϵ-δ relationship as used in the\n",
      "deﬁnition of absolute continuity of functions (Deﬁnition 0.1.32): Given that\n",
      "µ is ﬁnite, µ is absolutely continuous with respect to ν iﬀfor any A ∈F and\n",
      "for any ϵ > 0, there exists a δ such that\n",
      "ν(A) < δ\n",
      "⇒\n",
      "µ(A) < ϵ.\n",
      "Absolute continuity is a linear relationship; that is, if λ, µ, and ν are\n",
      "measures on (Ω, F) and a ∈IR then\n",
      "λ ≪ν\n",
      "and\n",
      "µ ≪ν =⇒(aλ + µ) ≪ν.\n",
      "(0.1.15)\n",
      "(Exercise.)\n",
      "Deﬁnition 0.1.24 (singular measure)\n",
      "Given measures ν and µ on the same measurable space, (Ω, F), if there exists\n",
      "two disjoint sets A and B in F such that A ∪B = Ωand for any measurable\n",
      "set A1 ⊆A, ν(A1) = 0, while for any measurable set B1 ⊆B, µ(B1) = 0 then\n",
      "the pair of measures ν and µ is said to be singular . We denote this property\n",
      "as\n",
      "ν ⊥µ.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "712\n",
      "0 Statistical Mathematics\n",
      "If ν ⊥µ, it follows immediately from the deﬁnitions of singularity and of\n",
      "absolute continuity that neither ν nor µ can dominate the other.\n",
      "Singular measures rely on, or equivalently, deﬁne, a partition of the sample\n",
      "space.\n",
      "Singularity is a linear relationship; that is, if λ, µ, and ν are measures on\n",
      "(Ω, F) and a ∈IR then\n",
      "λ ⊥ν\n",
      "and\n",
      "µ ⊥ν =⇒(aλ + µ) ⊥ν.\n",
      "(0.1.16)\n",
      "(Exercise.)\n",
      "Induced Measure\n",
      "If (Ω, F, ν) is a measure space, (Λ, G) is a measurable space, and f is a function\n",
      "from Ωto Λ that is measurable with respect to F, then the domain and range\n",
      "of the function ν ◦f−1 is G and it is a measure (Exercise 0.1.19).\n",
      "The measure ν ◦f−1 is called an induced measure on G. (It is induced from\n",
      "the measure space (Ω, F, ν).) An induced measure is also called a pushforward\n",
      "measure.\n",
      "Completion of a Measure Space\n",
      "Given the measure space (Ω, F, ν) and A ∈F with ν(A) = 0, if A1 ⊆A, it\n",
      "would seem reasonable to say that ν(A1) = 0. If A1 /∈F, however, ν(A1) is\n",
      "not 0; it is not deﬁned. We can form a measure space (Ω, Fc, νc) that is related\n",
      "to (Ω, F, ν), but which allows us to say that the measure of any subset of a\n",
      "zero-measure set in F has zero measure. We form Fc as the σ-ﬁeld generated\n",
      "by F and Z, where Z is the collection of all sets in F with ν-measure 0. Now\n",
      "deﬁne\n",
      "νc(A1) = inf{ν(A) | A1 ⊆A ∈F}.\n",
      "(0.1.17)\n",
      "The measure space (Ω, Fc, νc) is complete. (Exercise.) It is called the com-\n",
      "pletion of the measure space (Ω, F, ν). The construction above proves the\n",
      "existence of the completion of a measure space.\n",
      "Notice that the completion of a measure space does not require addition of\n",
      "points to Ω; compare the completion of a metric space discussed on page 639.\n",
      "Every A ∈Fc constructed as above is of the form B ∪C where B ∈F and\n",
      "C ∈Z, and\n",
      "νc(B ∪C) = ν(B).\n",
      "(0.1.18)\n",
      "(Exercise.)\n",
      "Extensions of Measures\n",
      "In applications we may have a measure space (Ω, F1, ν) and wish to consider\n",
      "a diﬀerent σ-ﬁeld F2 over the same sample space and extend the measure to\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "713\n",
      "σ(F1, F2) while preserving its properties over F1. More generally we may have\n",
      "a measure deﬁned on any collection of subsets A of Ωand wish to extend it\n",
      "to some σ-ﬁeld of which σ(A) is a subset while preserving the properties of\n",
      "the measure over A. The Carath´eodory extension theorem tells us not only\n",
      "that we can do this, but that the extension is unique so long as the measure\n",
      "on A is σ-ﬁnite.\n",
      "Theorem 0.1.11 (Carath´eodory extension theorem)\n",
      "Given a collection A of subsets of a sample space Ωand a σ-ﬁnite measure\n",
      "ν0 on A. Then there exists a unique σ-ﬁnite measure ν on σ(A) such that for\n",
      "any A ∈A, ν(A) = ν0(A).\n",
      "This theorem is proved in Billingsley (1995) for probability measures on page\n",
      "36, and for general σ-ﬁnite measures on page 166.\n",
      "Product Measures\n",
      "Given measure spaces (Ω1, F1, ν1) and (Ω2, F2, ν2), we deﬁne the cartesian\n",
      "product measure space as (Ω1 × Ω2, σ(F1 × F2), ν1 × ν2), where the product\n",
      "measure ν1 × ν2 is deﬁned on the product σ-ﬁeld σ(F1 × F2) to have the\n",
      "property for A1 ∈F1 and A2 ∈F2\n",
      "ν1 × ν2(A1 × A2) = ν1(A1)ν2(A2).\n",
      "(0.1.19)\n",
      "It can be shown that the measure with this property is unique, see Billingsley\n",
      "(1995), for example.\n",
      "0.1.4 Sets in IR and IRd\n",
      "First, recall some important deﬁnitions:\n",
      "•\n",
      "A set A of real numbers is called open if for each x ∈A, there exists a\n",
      "δ > 0 such that for each y with |x −y| < δ belongs to A.\n",
      "•\n",
      "A real number x is called a point of closure of a set A of real numbers if\n",
      "for every δ > 0 there exists a y in A such that |x −y| < δ. (Notice that\n",
      "every y ∈A is a point of closure of A.)\n",
      "We denote the set of points of closure of A by A.\n",
      "•\n",
      "A set A is called closed if A = A.\n",
      "Some simple facts follow:\n",
      "•\n",
      "The intersection of a ﬁnite collection of open sets is open.\n",
      "•\n",
      "The union of a countable collection of open sets is open.\n",
      "•\n",
      "The union of a ﬁnite collection of closed sets is closed.\n",
      "•\n",
      "The intersection of a countable collection of closed sets is closed.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "714\n",
      "0 Statistical Mathematics\n",
      "Notice what is not said above (where we use the word “ﬁnite”).\n",
      "A very important type of set is an interval in IR. Intervals are the basis for\n",
      "building important structures on IR. All intervals are Borel sets. We discussed\n",
      "properties of real intervals and, in particular, sequences on real intervals be-\n",
      "ginning on page 645.\n",
      "The Borel σ-Field on the Reals\n",
      "On page 697, we have deﬁned a Borel σ-ﬁeld for a topological space as the\n",
      "σ-ﬁeld generated by the topology, that is, by the collection of open sets that\n",
      "deﬁne the topology. In a metric space, such as IR, we deﬁne open sets in terms\n",
      "of the metric, and then we deﬁne a Borel σ-ﬁeld as before in terms of those\n",
      "open sets. The most interesting topological space is the set of reals together\n",
      "with the class of open intervals, (IR, C).\n",
      "Deﬁnition 0.1.25 (Borel σ-ﬁeld)\n",
      "Let C be the collection of all open intervals in IR. The σ-ﬁeld σ(C) is called\n",
      "the Borel σ-ﬁeld over IR, and is denoted by B(IR).\n",
      "We often call this Borel σ-ﬁeld over IR just the Borel ﬁeld, and denote it\n",
      "just by B.\n",
      "Borel Sets\n",
      "Any set in B is called a Borel set. Such sets are said to be “Borel measurable”,\n",
      "from the fact that they are λ-measurable, for the Lebesgue measure λ in\n",
      "equation (0.1.20).\n",
      "Example 0.1.4 (Borel-measurable sets)\n",
      "The following are all Borel-measurable sets.\n",
      "1. IR\n",
      "2. ∅\n",
      "3. any countable set; in particular, any ﬁnite set, ZZ, ZZ+ (the natural num-\n",
      "bers), and the set of all rational numbers\n",
      "4. hence, from the foregoing, the set of all irrational numbers (which is un-\n",
      "countable)\n",
      "5. any interval, open, closed, or neither\n",
      "6. the Cantor set\n",
      "The Cantor set is ∩∞\n",
      "i=1Ci, where\n",
      "C1 = [0, 1/3]∪[2/3, 1],\n",
      "C2 = [0, 1/9]∪[2/9, 1/3]∪[2/3, 7/9]∪[8/9, 1],\n",
      ". . . ,\n",
      "We see that each of these is Borel, and hence, so is the intersection. A\n",
      "Cantor set has interesting properties; for example, its cardinality is the\n",
      "same as that of the interval [0, 1], yet it is nowhere dense in [0, 1]. (The\n",
      "particular set described here is the Cantor ternary set; other similar sets\n",
      "are also called Cantor sets.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "715\n",
      "7. the Smith-Volterra-Cantor set\n",
      "Instead of removing a ﬁxed percentage of the subintervals at each stage,\n",
      "as in the case of a Cantor set, we can form a “fat” Cantor set by removing\n",
      "at each stage a decreasing percentage. The Smith-Volterra-Cantor set is\n",
      "formed by ﬁrst removing the middle 1/4 open subinterval from [0, 1] (that\n",
      "is, leaving the set [0, 3/8] ∪[5/8, 1]), then at the kth stage, removing the\n",
      "middle 2−2k open subintervals from each of the 2k−1 subintervals. The\n",
      "Smith-Volterra-Cantor set, as the Cantor set, has cardinality the same as\n",
      "that of the interval [0, 1] and yet is nowhere dense.\n",
      "8. any union of any of the above\n",
      "So, are all subsets of IR Borel sets?\n",
      "No. Interestingly enough, the cardinality of B can be shown to be the same\n",
      "as that of IR, and the cardinality of the collection of all subsets of IR, that is,\n",
      "the cardinality of the power set, 2IR, is much larger – which means there are\n",
      "many subsets of IR that are not Borel sets.\n",
      "Equivalent Deﬁnitions of the Borel σ-Field\n",
      "The facts that unions of closed sets may be open and that intersections of\n",
      "open intervals may be closed allow us to characterize the Borel σ-ﬁeld B(IR)\n",
      "in various ways. The canonical deﬁnition is that B = σ(C), where C is the\n",
      "collection of all ﬁnite open intervals. This is a simple version of the general\n",
      "deﬁnition of a Borel σ-ﬁeld for a topological space. (In that deﬁnition, the\n",
      "generator is the collection of all open sets.) The following theorem list three\n",
      "other useful collections of subsets of IR that generate the Borel σ-ﬁeld.\n",
      "Theorem 0.1.12\n",
      "The Borel σ-ﬁeld B(IR) is generated by any of the following collections of\n",
      "subsets of IR:\n",
      "(i) the collection of all ﬁnite closed intervals [a, b] of IR,\n",
      "(ii) the collection of all semi-inﬁnite half-open intervals ] −∞, b] of IR, and\n",
      "(iii) the collection of all semi-inﬁnite open intervals ]a, −∞[ of IR.\n",
      "Proof.\n",
      "To show that the σ-ﬁelds generated by two collections C and D are the same,\n",
      "we use the fact that a σ-ﬁeld is closed with respect to countable intersec-\n",
      "tions (remember the usual deﬁnition requires that it be closed with respect\n",
      "to countable unions) and then we show that\n",
      "(1) C ∈C ⇒C ∈σ(D) and\n",
      "(2) D ∈D ⇒D ∈σ(C).\n",
      "Hence, to prove part (i), let D the collection of all ﬁnite closed intervals of\n",
      "IR.\n",
      "(1) assume D =[a,b]∈D. Now, consider the sequence of sets Bi =]a−1/i, b+\n",
      "1/i[. These open intervals are in B, and hence,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "716\n",
      "0 Statistical Mathematics\n",
      "∞\n",
      "\\\n",
      "i=1\n",
      "]a −1/i, b + 1/i[= [a, b] ∈B.\n",
      "Next,\n",
      "(2) let ]a, b[ be any set in the generator collection of B, and consider the\n",
      "sequence of sets Di = [a+1/i, b−1/i], which are in D. By deﬁnition, we have\n",
      "S∞\n",
      "i=1[a + 1/i, b −1/i] =]a, b[∈σ(D)\n",
      "Proofs of parts (ii) and (iii) are left as exercises.\n",
      "Likewise, the collections of semi-inﬁnite intervals of the form ]∞, b[ or\n",
      "[a, ∞[ generate B(IR).\n",
      "We also get the same Borel ﬁeld B(IR) by using the collection all open sets\n",
      "of IR, as in the general deﬁnition of a Borel σ-ﬁeld for a topological space.\n",
      "(Exercise.)\n",
      "The σ-Field B[0,1]\n",
      "We are often interested in some subspace of IRd, for example an interval (or\n",
      "rectangle). One of the most commonly-used intervals in IR is [0, 1].\n",
      "For the sample space Ω= [0, 1], the most useful σ-ﬁeld consists of the\n",
      "collection of all sets of the form [0, 1] ∩B, where B ∈B(IR). We often denote\n",
      "this σ-ﬁeld as B[0,1].\n",
      "The σ-ﬁeld formed in this way is the same as the σ-ﬁeld generated by\n",
      "all open intervals on [0, 1]; that is, B([0, 1]). (The reader should show this, of\n",
      "course.)\n",
      "Product Borel σ-Fields\n",
      "For the d-product measurable space generated by (IR, B), the σ-ﬁeld is σ(Bd),\n",
      "as stated in Deﬁnition 0.1.8, and so the measurable space of interest is\n",
      "(IRd, σ(Bd)).\n",
      "As pointed out on page 701, the σ-ﬁeld generated by the product of a\n",
      "number of σ-ﬁelds is not necessarily the same as the product of the σ-ﬁelds.\n",
      "It can be shown, however, that the σ-ﬁeld σ(Bd) is Bd. Furthermore, this is\n",
      "the σ-ﬁeld that would result from deﬁnition 0.1.25 by extending the collection\n",
      "C of all open intervals in IR to the collection Cd of all open intervals (or\n",
      "“hyperrectangles”) in IRd.\n",
      "The product measurable space of interest, therefore, is (IRd, Bd).\n",
      "Measures on (IR, B(IR))\n",
      "The various types of measures we discussed beginning on page 707 may all be\n",
      "deﬁned on (IR, B(IR)), but for this measurable space, the most common mea-\n",
      "sure is the Lebesgue measure. Lebesgue measure is the extension (see page 712)\n",
      "of Borel measure, which we now deﬁne.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "717\n",
      "Deﬁnition 0.1.26 (Borel measure on (IR, B))\n",
      "The Borel measure is deﬁned on (IR, B) by the relation\n",
      "λ(]a, b[) = b −a,\n",
      "(0.1.20)\n",
      "for given real numbers a ≤b.\n",
      "Lebesgue Measure on IR\n",
      "Although for most purposes, (IR, B) is the basic structure that we work with,\n",
      "as it turns out, (IR, B) is not complete wrt the measure λ deﬁned in equa-\n",
      "tion (0.1.20).\n",
      "*** describe extension *** add stuﬀon Carath´eodory\n",
      "*** Lebesgue σ-ﬁeld\n",
      "*** use λ to denote Lebesgue measure\n",
      "It is clear from the deﬁnition of a measure (Deﬁnition 0.1.10) that λ is a\n",
      "measure, and because λ is a measure, we see that\n",
      "λ([a, b]) = λ(]a, b[).\n",
      "(0.1.21)\n",
      "Although it is not ﬁnite, the Lebesgue measure is σ-ﬁnite, as can be seen\n",
      "from the sequence of open intervals ] −i, i[.\n",
      "The measurable space (IR, B) is a topological space, and the Lebesgue\n",
      "measure is a Radon measure (exercise). Furthermore, along with addition, IR\n",
      "is a group, and the Lebesgue measure is a Haar invariant measure wrt that\n",
      "group. (Note that the property of Haar invariance depends on the operation\n",
      "within a group). This latter fact is expressed by saying that Lebesgue measure\n",
      "is translation invariant.\n",
      "It can be shown that any σ-ﬁnite translation invariant measure µ on (IR, B)\n",
      "is equivalent to Lebesgue measure in the sense that there is a positive constant\n",
      "c, such that for any A ∈B, λ(A) = cµ(A).\n",
      "The space over which the Lebesgue measure is deﬁned is a linear space.\n",
      "The Lebesgue measure is translation invariant, as noted above, and it is also\n",
      "scale equivariant. Let A ∈B, and for b, x ∈IR with b > 0, let bA + x =\n",
      "{by + x | y ∈A}. We have\n",
      "λ(bA + x) = bλ(A).\n",
      "(0.1.22)\n",
      "A set A ⊆IRd such that λ(A) = 0 is called a null set (whether or not\n",
      "A ∈Bd). It is clear that all countable sets are null sets. For example, the set\n",
      "of rational numbers has measure 0. An example of an uncountable set that is\n",
      "a null set is the Cantor set, as we see by computing the measure of what is\n",
      "taken out of the interval [0, 1]:\n",
      "∞\n",
      "X\n",
      "k=1\n",
      "2k−13−k = 1\n",
      "3 + 2\n",
      "32 + 22\n",
      "33 + · · · = 1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "718\n",
      "0 Statistical Mathematics\n",
      "This implies that the measure of what is left, that is, the nowhere dense\n",
      "(ternary) Cantor set is 0.\n",
      "Another set that is uncountable but nowhere dense, is the Smith-Volterra-\n",
      "Cantor set. Its measure, however, is 1/2, as we see by summing the measures\n",
      "of what is taken out.\n",
      "∞\n",
      "X\n",
      "k=1\n",
      "2k−12−2k = 1\n",
      "2.\n",
      "Singular Measures on (IR, B(IR))\n",
      "Two other measures that are often useful in (IR, B) are the Dirac measure\n",
      "(equation (0.1.12)) and the counting measure (equation (0.1.13)).\n",
      "It is easy to see from the deﬁnitions that both the Dirac measure δ and\n",
      "the counting measure γ are singular with respect to the Lebesgue measure:\n",
      "δ ⊥λ\n",
      "(0.1.23)\n",
      "and\n",
      "γ ⊥λ.\n",
      "(0.1.24)\n",
      "Hence, neither of these measures is absolutely continuous with respect to the\n",
      "Lebesgue measure, and the Lebesgue measure is not absolutely continuous\n",
      "with respect to either of them.\n",
      "Note, for example, δ0({0}) = 1 but λ({0}) = 0, while δ0([1, 2]) = 0 but\n",
      "λ([1, 2]) = 1.\n",
      "A measure on (IR, B) that is singular with respect to the Lebesgue measure\n",
      "is called simply a singular measure.\n",
      "Example 0.1.5 (A non-Borel-measurable set)\n",
      "A simple example of a non-Borel-measurable set, called a Vitali set, can be\n",
      "constructed using the Axiom of Choice. We begin by deﬁning equivalence\n",
      "classes within IR by making x, y ∈IR equivalent, written x ∼y, iﬀx −y is ra-\n",
      "tional. For each x ∈IR, we identify the equivalence class Ex as {y | y ∼x}. The\n",
      "collection of these equivalence classes is a countable partition of IR. (Recall\n",
      "that the set of rationals is countable.) Now we form the Vitali set V by choos-\n",
      "ing exactly one member of each equivalence class in the interval [0, 1]. Next\n",
      "we show that the Vitali set is nonmeasurable by contradiction. Let q1, q2, . . .\n",
      "represent the distinct (countable) rationals in [−1, 1], and form the disjoint\n",
      "countable sequence of sets Vk = {v + qk | v ∈V }. (Why are the sets disjoint?)\n",
      "Now, assume V is Lebesgue measurable (that is, “Borel measurable”). Be-\n",
      "cause Lebesgue measure is translation invariant (equation (0.1.22)), if V is\n",
      "Lebesgue measurable, so is each Vk, and in fact, λ(Vk) = λ(V ). Note that\n",
      "[0, 1] ⊆\n",
      "[\n",
      "k\n",
      "Vk ⊆[−1, 2]\n",
      "(0.1.25)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "719\n",
      "(why?), and so\n",
      "1 ≤λ\n",
      " [\n",
      "k\n",
      "Vk\n",
      "!\n",
      "≤3.\n",
      "We also have\n",
      "λ\n",
      " [\n",
      "k\n",
      "Vk\n",
      "!\n",
      "=\n",
      "X\n",
      "k\n",
      "λ(Vk) =\n",
      "X\n",
      "k\n",
      "λ(V ),\n",
      "which must be either 0 or inﬁnite, in either case contradicting\n",
      "1 ≤\n",
      "X\n",
      "k\n",
      "λ(V ) ≤3,\n",
      "which follows only from the properties of measures and the assumption that\n",
      "V is Lebesgue measurable. We therefore conclude that V is not measurable.\n",
      "Borel Measurable Functions\n",
      "We will now consider real-valued functions; that is, mappings into IRd. The\n",
      "domains are not necessarily real-valued. We ﬁrst identify two useful types of\n",
      "real-valued functions.\n",
      "Deﬁnition 0.1.27 (indicator function)\n",
      "The indicator function, denoted IS(x) for a given set S, is deﬁned by IS(x) = 1\n",
      "if x ∈S and IS(x) = 0 otherwise.\n",
      "Notice that I−1\n",
      "S [A] = ∅if 0 /∈A and 1 /∈A; I−1\n",
      "S [A] = S if 0 /∈A and 1 ∈A;\n",
      "I−1\n",
      "S [A] = Sc if 0 ∈A and 1 /∈A; and I−1\n",
      "S [A] = Ωif 0 ∈A and 1 ∈A.\n",
      "Hence, σ(IS) is the second most trivial σ-ﬁeld we referred to earlier; i.e.,\n",
      "σ(S) = {∅, S, Sc, Ω}.\n",
      "Deﬁnition 0.1.28 (simple function)\n",
      "If A1, . . ., Ak are measurable subsets of Ωand a1, . . ., ak are constant real\n",
      "numbers, a function ϕ is a simple function if for ω ∈Ω,\n",
      "ϕ(ω) =\n",
      "k\n",
      "X\n",
      "i=1\n",
      "aiIAi(ω),\n",
      "(0.1.26)\n",
      "where IS(x) is the indicator function.\n",
      "Recall the convention for functions that we have adopted: the domain of the\n",
      "function is the sample space; hence, the subsets corresponding to constant\n",
      "values of the function form a ﬁnite partition of the sample space.\n",
      "Deﬁnition 0.1.29 (Borel measurable function)\n",
      "A measurable function from (Ω, F) to (IRd, Bd) is said to be Borel measurable\n",
      "with respect to F.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "720\n",
      "0 Statistical Mathematics\n",
      "A function that is Borel measurable is called a Borel function.\n",
      "Simple functions are Borel measurable. (Exercise.)\n",
      "The following theorem is useful because it allows us to build up any mea-\n",
      "surable real-valued function from a sequence of simple functions.\n",
      "Theorem 0.1.13\n",
      "Every measurable real-valued function can be represented at any point as the\n",
      "limit of a sequence of simple functions.\n",
      "Proof. Let f be real and measurable. Now, if f(ω) ≥0, there exists a sequence\n",
      "{fn} of simple functions such that\n",
      "0 ≤fn(ω) ↗f(ω)\n",
      "a.e.,\n",
      "and if f(ω) ≤0, there exists a sequence {fn} of simple functions such that\n",
      "0 ≥fn(ω) ↘f(ω)\n",
      "a.e.\n",
      "The sequence is\n",
      "fn(ω) =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "−n\n",
      "if\n",
      "f(ω) ≤−n,\n",
      "−(k −1)2−n if\n",
      "−k2−n < f(ω) ≤−(k −1)2−n, for 1 ≤k ≤n2−n,\n",
      "(k −1)2−n\n",
      "if\n",
      "(k −1)2−n < f(ω) < k2−n, for 1 ≤k ≤n2−n,\n",
      "n\n",
      "if\n",
      "n ≤f(ω).\n",
      "As a corollary of Theorem 0.1.13, we have that for a nonnegative random\n",
      "variable X, there exists a sequence of simple (degenerate) random variables\n",
      "{Xn} such that\n",
      "0 ≤Xn ↗X\n",
      "a.s.\n",
      "(0.1.27)\n",
      "0.1.5 Real-Valued Functions over Real Domains\n",
      "In the foregoing we have given special consideration to real-valued functions\n",
      "over arbitrary domains. In the following we consider real-valued functions\n",
      "over real domains. For such functions, we identify some additional properties,\n",
      "and then we deﬁne integrals and derivatives of real-valued functions over real\n",
      "domains.\n",
      "In most practical purposes, two functions are “equal” if they are equal\n",
      "almost everywhere. For real-valued functions over real domains, almost ev-\n",
      "erywhere usually means wrt Lebesgue measure, and when we use the phrase\n",
      "“almost everywhere” or “a.e.” without qualiﬁcation, that is what we mean.\n",
      "Continuous Real Functions\n",
      "Continuity is an important property of some functions. On page 626 we deﬁned\n",
      "continuous functions in general topological spaces. For real functions on a real\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "721\n",
      "domain, we equivalently deﬁne continuity in terms of the Euclidean distance\n",
      "between two points in the domain and the Euclidean distance between the\n",
      "corresponding function values.\n",
      "Deﬁnition 0.1.30 (continuous function)\n",
      "Let f be a real-valued function whose domain is a set D ⊆IRd. We say that\n",
      "f is continuous at the point x ∈D if f is deﬁned in an open neighborhood of\n",
      "x, and given ϵ > 0, ∃δ ∋∀y ∈D ∋∥x −y∥< δ, ∥f(x) −f(y)∥< ϵ.\n",
      "Here, the norms are the Euclidean norms. Notice that the order of f(x)\n",
      "may be diﬀerent from the order of x.\n",
      "The δ in the deﬁnition may depend on x as well as on ϵ.\n",
      "If f is continuous at each point in a subset of its domain, we say it is\n",
      "continuous on that subset. If f is continuous at each point in its domain, we\n",
      "say that f is continuous.\n",
      "We have an immediate useful fact about continuous functions:\n",
      "Theorem 0.1.14\n",
      "If f is a continuous function, the inverse image f−1 of an open set is open.\n",
      "Proof. Follows immediately from the deﬁnition.\n",
      "There are various types of continuity, and some examples will help to\n",
      "illustrate the diﬀerences.\n",
      "Example 0.1.6 (the Dirichlet function;) nowhere continuous function\n",
      "The indicator function of the rational numbers, called the Dirichlet function,\n",
      "is everywhere discontinuous.\n",
      "Example 0.1.7 (the Thomae function;) continuous on irrationals, dis-\n",
      "continuous on rationals\n",
      "Let f(x) be deﬁned as\n",
      "f(x) =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 if x = 0\n",
      "1\n",
      "q if x = p\n",
      "q is rational,\n",
      "where q is a positive integer and p is relatively prime to q\n",
      "0 if x is irrational\n",
      "Then f(x), called the Thomae function, is continuous at x if x is irrational\n",
      "and discontinuous at x if x is rational.\n",
      "We now consider three successively stronger types of continuity, and one\n",
      "modiﬁcation of the strong type.\n",
      "Deﬁnition 0.1.31 (uniformly continuous function)\n",
      "Let f be a real-valued function whose domain includes a set D ⊆IRd. We say\n",
      "that f is uniformly continuous over D if, given ϵ > 0, ∃δ ∋∀x, y ∈D with\n",
      "∥x −y∥< δ,\n",
      "∥f(x) −f(y)∥< ϵ.\n",
      "(0.1.28)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "722\n",
      "0 Statistical Mathematics\n",
      "Continuity is a point-wise property, while uniform continuity is a property\n",
      "for all points in some given set.\n",
      "Example 0.1.8 continuous but not uniformly continuous\n",
      "The function f(x) = 1/x is continuous on ]0, ∞[, but is not uniformly con-\n",
      "tinuous over that interval. This function is, however, uniformly continuous\n",
      "over any closed and bounded subinterval of ]0, ∞[. The Heine-Cantor theo-\n",
      "rem, in fact, states that any function that is continuous over a compact set is\n",
      "uniformly continuous over that set.\n",
      "If {xn} is a Cauchy sequence in the domain of a a uniformly continuous\n",
      "function f, then {f(xn)} is also a Cauchy sequence.\n",
      "If a function f is uniformly continuous over a ﬁnite interval ]a, b[, then f\n",
      "is bounded over ]a, b[.\n",
      "Deﬁnition 0.1.32 (absolutely continuous function)\n",
      "Let f be a real-valued function deﬁned on [a, b] (its domain may be larger).\n",
      "We say that f is absolutely continuous on [a, b] if, given ϵ > 0, there exists\n",
      "a δ such that for every ﬁnite collection of nonoverlapping open rectangles\n",
      "]xi, yi[⊆[a, b] with Pn\n",
      "i=1 ∥xi −yi∥< δ,\n",
      "n\n",
      "X\n",
      "i=1\n",
      "∥f(xi) −f(yi)∥< ϵ.\n",
      "(0.1.29)\n",
      "(We deﬁned absolute continuity of a measure with respect to another measure\n",
      "in Deﬁnition 0.1.23. Absolute continuity of a function f is a similar concept\n",
      "with respect to the Lebesgue measure over the domain and range of f.)\n",
      "We also speak of local absolute continuity of functions in the obvious way.\n",
      "If f is absolutely continuous over D, it is uniformly continuous on D, but\n",
      "the converse is not true.\n",
      "Example 0.1.9 (the Cantor function) uniformly continuous but not ab-\n",
      "solutely continuous\n",
      "The Cantor function, deﬁned over the interval [0, 1], is an example of a func-\n",
      "tion that is continuous everywhere, and hence, uniformly continuous on that\n",
      "compact set, but not absolutely continuous. The Cantor function takes diﬀer-\n",
      "ent values over the diﬀerent intervals used in the construction of the Cantor\n",
      "set (see page 714). Let f0(x) = x, and then for n = 0, 1, . . ., let\n",
      "fn+1(x) = 0.5fn(3x)\n",
      "for 0 ≤x < 1/3\n",
      "fn+1(x) = 0.5\n",
      "for 1/3 ≤x < 2/3\n",
      "fn+1(x) = 0.5 + 0.5fn(3(x −2/3))\n",
      "for 2/3 ≤x ≤1.\n",
      "The Cantor function is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "723\n",
      "f(x) = lim\n",
      "n→∞fn(x).\n",
      "(0.1.30)\n",
      "The Cantor function has a derivative of 0 almost everywhere, but has no\n",
      "derivative at any member of the Cantor set. (We deﬁne derivatives below,\n",
      "and in a more general way on page 739.) The properties of this function\n",
      "are discussed very carefully on pages 131–135 of Boas Jr. (1960), who uses a\n",
      "tertiary representation of the points in the set and a binary representation of\n",
      "the values of the function to demonstrate continuity and the derivatives or\n",
      "lack thereof.\n",
      "An absolutely continuous function is of bounded variation; it has a deriva-\n",
      "tive almost everywhere; and if the derivative is 0 a.e., the function is constant.\n",
      "A slightly stronger form of continuity is Lipschitz-continuity. It places an\n",
      "explicit bound on the amount by which the function can change.\n",
      "Deﬁnition 0.1.33 (Lipschitz-continuous function)\n",
      "Let f be a real-valued function whose domain is an interval D ⊆IRd. We say\n",
      "that f is Lipschitz-continuous if for any y1, y2 ∈D and y1 ̸= y2, there exists\n",
      "γ such that\n",
      "∥f(y1) −f(y2)∥≤γ∥y1 −y2∥.\n",
      "(0.1.31)\n",
      "The smallest γ for which the inequality holds is called the Lipschitz constant.\n",
      "We also speak of local Lipschitz continuity in the obvious way.\n",
      "Every Lipschitz-continuous function is absolutely continuous. Lipschitz\n",
      "continuity plays an important role in nonparametric function estimation.\n",
      "The graph of a scalar-valued Lipschitz-continuous function f over D ⊆\n",
      "IR has the interesting geometric property that the entire graph of f(x) lies\n",
      "between the lines y = f(c) ± γ(x −c) for any c ∈D.\n",
      "Example 0.1.10 absolutely continuous but not Lipschitz continuous\n",
      "The function f(x) = √x for x ∈[0, 1] is an example of a absolutely continuous\n",
      "everywhere on [0, 1], but is not Lipschitz continuous on that set. (The problem\n",
      "with Lipschitz continuity occurs at x = 0.)\n",
      "Finally, a slight modiﬁcation of Lipschitz-continuity yields another form of\n",
      "continuity called uniform Lipschitz-continuity of order α, or H¨older continuity\n",
      "of order α.\n",
      "Deﬁnition 0.1.34 (H¨older-continuous function)\n",
      "Let f be a real-valued function whose domain is an interval D ⊆IRd. We say\n",
      "that f is H¨older-continuous of order α where α > 0, if for any y1, y2 ∈D and\n",
      "y1 ̸= y2, there exists γ such that\n",
      "∥f(y1) −f(y2)∥≤γ∥y1 −y2∥α.\n",
      "(0.1.32)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "724\n",
      "0 Statistical Mathematics\n",
      "We also speak of local H¨older continuity in the obvious way.\n",
      "Depending on α, H¨older continuity may be stronger or weaker than Lip-\n",
      "schitz continuity. For α < 1, H¨older continuity does not guarantee diﬀeren-\n",
      "tiability, whereas uniform continuity, and a fortiori, Lipschitz continuity, does\n",
      "guarantee it, except on set of measure 0. (See Example 0.1.11.)\n",
      "Diﬀerentiability; Derivatives of Functions\n",
      "Continuity has to do with how function values change as the function argu-\n",
      "ment changes. A continuous function does not have abrupt changes. Diﬀeren-\n",
      "tiability is a related concept that has to do with the rate of change. Here we\n",
      "deﬁne a very useful type of diﬀerentiation. We deﬁne derivatives in a more\n",
      "general way on page 739. Unlike continuity, here we will deﬁne diﬀerentiabil-\n",
      "ity only for functions deﬁned on IR. The deﬁnition generalizes, but in IRd for\n",
      "d > 1 there are some additional important issues involving directions.\n",
      "Deﬁnition 0.1.35 (diﬀerentiable function)\n",
      "Let x be a point in IR and let f be a real-valued function deﬁned in an open\n",
      "neighborhood of x. We say that f is diﬀerentiable at the point x if the limit\n",
      "lim\n",
      "h→0\n",
      "f(x + h) −f(x)\n",
      "h\n",
      "(0.1.33)\n",
      "exists.\n",
      "If the limit (0.1.33) exists, it is called the derivative of f at the point x\n",
      "and is denoted as f′. Wherever it exists, the derivative is a function, and we\n",
      "often denote it as f′(x).\n",
      "Diﬀerentiability obviously depends on continuity, but does continuity guar-\n",
      "antee diﬀerentiability?\n",
      "Example 0.1.11 (the Weierstrass function) continuous everywhere but\n",
      "diﬀerentiable nowhere\n",
      "The Weierstrass function, deﬁned over the interval [−2, 2], is an example of a\n",
      "function that is continuous everywhere but diﬀerentiable nowhere. The Weier-\n",
      "strass function is\n",
      "f(x) =\n",
      "∞\n",
      "X\n",
      "n=0\n",
      "an cos(bnxπ),\n",
      "(0.1.34)\n",
      "where 0 < a < 1 and b is a positive odd integer such that ab > 1 + 3π/2.\n",
      "This example shows that H¨older continuity may not be suﬃcient to guar-\n",
      "antee diﬀerentiability. The Weierstrass function is H¨older continuous for all\n",
      "orders α < 1 (Exercise ??).\n",
      "***Function that is continuous but not diﬀerentiable Weierstrass, Wiener\n",
      "Uniform continuity is the weakest form that guarantees diﬀerentiability.\n",
      "A uniformly continuous function is diﬀerentiable almost everywhere. Even\n",
      "Lipschitz-continuity does not guarantee diﬀerentiability. For example f(x) =\n",
      "|x| is Lipschitz continuous over [−a, a], but it is not diﬀerentiable at x = 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "725\n",
      "Sequences of Functions; lim sup and lim inf\n",
      "We now consider some properties of sequences of functions, {fn}. We will\n",
      "limit our attention to Borel functions. Important properties of a sequence\n",
      "of functions are its lim sup and lim inf, which we deﬁne analogously to the\n",
      "meaning of lim sup and lim inf of a sequence of sets given in equations (0.0.10)\n",
      "and (0.0.11):\n",
      "lim sup\n",
      "n\n",
      "fn\n",
      "def\n",
      "= inf\n",
      "n sup\n",
      "i≥n\n",
      "fi\n",
      "(0.1.35)\n",
      "and\n",
      "lim inf\n",
      "n\n",
      "fn\n",
      "def\n",
      "= sup\n",
      "n inf\n",
      "i≥n fi.\n",
      "(0.1.36)\n",
      "We ﬁrst consider the σ-ﬁelds generated by a sequence of functions. An\n",
      "important result is the following.\n",
      "Theorem 0.1.15\n",
      "Let {fn} be a sequence of Borel functions on a measurable space (Ω, F). Then\n",
      "(i)\n",
      "σ(f1, f2, . . .) = σ(∪∞\n",
      "n=1σ(fn))\n",
      "(0.1.37)\n",
      "= σ(∪∞\n",
      "j=1σ(f1, . . ., fj))\n",
      "(0.1.38)\n",
      "(ii)\n",
      "σ(lim supfn) = σ(∩∞\n",
      "n=1σ(fn, fn+1, . . .)).\n",
      "(0.1.39)\n",
      "Proof. ***\n",
      "We identify two types of convergence of functions, pointwise convergence\n",
      "and uniform convergence, with a distinction reminiscent of that between con-\n",
      "tinuity and uniform continuity. We will then consider a stronger type of point-\n",
      "wise convergence, and show a relationship between strong pointwise conver-\n",
      "gence and uniform convergence.\n",
      "Deﬁnition 0.1.36 (pointwise convergence)\n",
      "Let {fn} be a sequence of real-valued function over a real domain D, and\n",
      "likewise let f be a real-valued function over D. We say {fn} converges to f\n",
      "at the point x ∈D iﬀfor\n",
      "∀ϵ > 0, ∃N ∋n ≥N ⇒∥fn(x) −f(x)∥< ϵ.\n",
      "We write fn(x) →f(x).\n",
      "The “N” in the deﬁnition may depend on x. In uniform convergence it does\n",
      "not.\n",
      "Deﬁnition 0.1.37 (uniform convergence)\n",
      "Let {fn} be a sequence of real-valued function over a real domain D, and like-\n",
      "wise let f be a real-valued function over D. We say {fn} converges uniformly\n",
      "to f iﬀfor\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "726\n",
      "0 Statistical Mathematics\n",
      "∀ϵ > 0, ∃N ∋n ≥N ⇒∥fn(x) −f(x)∥< ϵ ∀x ∈D.\n",
      "We write fn →f or fn(x)\n",
      "uniformly\n",
      "→\n",
      "f(x).\n",
      "Uniform convergence can also be limited to a subset of the domain, in which\n",
      "case we may call a function locally uniformly convergent.\n",
      "Deﬁnition 0.1.38 (almost everywhere (pointwise) convergence)\n",
      "*** We write fn(x) a.e.\n",
      "→f(x).\n",
      "Next, we consider a relationship between types convergence of functions.\n",
      "The most important and basic result is stated in the Severini-Egorov theorem,\n",
      "also called Egorov’s or Egoroﬀ’s theorem.\n",
      "Theorem 0.1.16 (Severini-Egorov theorem)\n",
      "Let {fn} be a sequence of Borel functions on a measure space (Ω, F, ν). For\n",
      "any A ∈F such that ν(A) < ∞, suppose that fn(ω) →f(ω)∀ω ∈A. Then\n",
      "∀ϵ > 0, ∃B ⊆A with ν(B) < ϵ ∋fn(ω) →f(ω) on A ∩Bc.\n",
      "Proof. ***\n",
      "The Severini-Egorov theorem basically states that pointwise convergence\n",
      "almost everywhere on A implies the stronger uniform convergence everywhere\n",
      "except on some subset B of arbitrarily small measure. This type of convergence\n",
      "is also called almost uniform convergence.\n",
      "This theorem is Littlewood’s principle of real analysis that states that\n",
      "every convergent sequence of functions is “nearly” uniformly convergent (see\n",
      "Littlewood (1944)). We will encounter this principle again in connection with\n",
      "the monotone convergence theorem on page 733.\n",
      "0.1.6 Integration\n",
      "Integrals are some of the most important functionals of real-valued functions.\n",
      "Integrals and the action of integration are deﬁned using measures. Although\n",
      "much of integration theory could be developed over abstract sets, we will\n",
      "generally assume that the domains of the functions are real and the functions\n",
      "are real-valued.\n",
      "Integrals of nonnegative functions are themselves measures. There are var-\n",
      "ious types of integrals, Lebesgue, Riemann, Riemann-Stieltjes, Ito, and so on.\n",
      "The most important in probability theory is the Lebesgue, and when we use\n",
      "the term “integral” without qualiﬁcation that will be the integral meant. We\n",
      "begin with the deﬁnition and properties of the Lebesgue integral. We brieﬂy\n",
      "discuss the Riemann integral in Section 0.1.6 on page 735, the Riemann-\n",
      "Stieltjes integral in Section 0.1.6 on page 736, and the Ito integral in Sec-\n",
      "tion 0.2.2 on page 775.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "727\n",
      "The Lebesgue Integral of a Function with Respect to a Given\n",
      "Measure: The Deﬁnition\n",
      "An integral of a function f with respect to a given measure ν, if it exists, is a\n",
      "functional whose value is an average of the function weighted by the measure.\n",
      "It is denoted by\n",
      "R\n",
      "f dν. The function f is called the integrand.\n",
      "The integral is deﬁned over the sample space of a given measure space, say\n",
      "(Ω, F, ν). This is called the domain of the integral. We often may consider in-\n",
      "tegrals over diﬀerent domains formed from a sub measure space, (D, FD, ν) for\n",
      "some set D ∈F, as described above. We often indicate the domain explicitly\n",
      "by notation such as\n",
      "R\n",
      "D f dν.\n",
      "If the domain is a real interval [a, b], we often write the restricted interval\n",
      "as R b\n",
      "a f dν. If ν is the Lebesgue measure, this integral is the same as the\n",
      "integral over the open interval ]a, b[.\n",
      "We also write an integral in various equivalent ways. For example if the\n",
      "integrand is a function of real numbers and our measure is the Lebesgue\n",
      "measure, we may write the integral over the interval ]a, b[ as R b\n",
      "a f(x) dx.\n",
      "We build the deﬁnition of an integral of a function in three steps: ﬁrst\n",
      "for nonnegative simple functions, then for nonnegative Borel functions, and\n",
      "ﬁnally for general Borel functions.\n",
      "Deﬁnition 0.1.39 (integral of a nonnegative simple function)\n",
      "If f is a simple function deﬁned as f(ω) = Pk\n",
      "i=1 aiIAi(ω), where the Ais are\n",
      "measurable with respect to ν, then\n",
      "Z\n",
      "f dν =\n",
      "k\n",
      "X\n",
      "i=1\n",
      "aiν(Ai).\n",
      "(0.1.40)\n",
      "Note that a simple function over measurable Ais is necessarily measurable.\n",
      "What about the case in which ν(Ai) = ∞, as for example when the domain\n",
      "of f is the real line and ν is the Lebesgue measure? We adopt the convention\n",
      "that\n",
      "∞+ ∞= ∞,\n",
      "∞· 0 = 0 · ∞= 0,\n",
      "and for c > 0,\n",
      "c · ∞= c + ∞= ∞,\n",
      "and so the integral (0.1.40) is always deﬁned, although it may be inﬁnite.\n",
      "We deﬁne the integral of a nonnegative Borel function in terms of the\n",
      "supremum of a collection of simple functions.\n",
      "Deﬁnition 0.1.40 (integral of a nonnegative Borel function)\n",
      "Let f be a nonnegative Borel function with respect to ν on Ω, and let Sf be\n",
      "the collection of all nonnegative simple functions such that\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "728\n",
      "0 Statistical Mathematics\n",
      "ϕ ∈Sf ⇒ϕ(ω) ≤f(ω)\n",
      "∀ω ∈Ω\n",
      "The integral of f with respect to ν is\n",
      "Z\n",
      "f dν = sup\n",
      "\u001aZ\n",
      "ϕ dν\n",
      "\f\f\f\f ϕ ∈Sf\n",
      "\u001b\n",
      ".\n",
      "(0.1.41)\n",
      "Another way of stating this deﬁnition in the measure space (Ω, F, ν) is to\n",
      "consider various ﬁnite partitions of Ωusing sets in F. (See the discussion on\n",
      "page 700.) If {Ai} is such a partition, we form the sum\n",
      "X\n",
      "i\n",
      "inf\n",
      "ω∈Ai f(ω)ν(Ai),\n",
      "(0.1.42)\n",
      "in which we adopt the conventions above so that if, in any addend in ex-\n",
      "pression (0.1.42), either factor is 0, then the addend is 0. The deﬁnition in\n",
      "equation (0.1.41) is therefore equivalent to\n",
      "Z\n",
      "f dν =\n",
      "sup\n",
      "all partitions\n",
      "X\n",
      "i\n",
      "inf\n",
      "ω∈Ai f(ω)ν(Ai)\n",
      "(0.1.43)\n",
      "and so again the integral (0.1.41) is always deﬁned, although it may be inﬁnite.\n",
      "Now consider general Borel functions. For a general Borel function f, we\n",
      "form two nonnegative Borel functions f+ and f−such that f = f+ −f−:\n",
      "f+(ω) = max{f(ω), 0}\n",
      "f−(ω) = max{−f(ω), 0}.\n",
      "Deﬁnition 0.1.41 (integral of a general Borel function)\n",
      "The integral of f with respect to ν is the diﬀerence of the integrals of the two\n",
      "nonnegative functions:\n",
      "Z\n",
      "f dν =\n",
      "Z\n",
      "f+ dν −\n",
      "Z\n",
      "f−dν,\n",
      "(0.1.44)\n",
      "so long as either\n",
      "R\n",
      "f+ dν or\n",
      "R\n",
      "f−dν is ﬁnite (because ∞−∞is not deﬁned).\n",
      "We can rewrite the deﬁnition in equation (0.1.44) in a manner similar to\n",
      "how we rewrote equation (0.1.42) above:\n",
      "Z\n",
      "f dν =\n",
      "sup\n",
      "all partitions\n",
      "X\n",
      "i\n",
      "\f\f\f\f inf\n",
      "ω∈Ai f(ω)\n",
      "\f\f\f\f ν(Ai).\n",
      "(0.1.45)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "729\n",
      "Note that, just as with the deﬁnitions for nonnegative functions above, the\n",
      "integral of a general Borel function may be inﬁnite; in fact, it may be ∞or\n",
      "−∞.\n",
      "For what kind of function would the Lebesgue integral not be deﬁned?\n",
      "The Lebesgue integral is not deﬁned for functions for which both the positive\n",
      "part and the negative of the negative part in equation (0.1.44) are ∞. The\n",
      "function f(x) = sin(x)/x over the positive real line is an example of such a\n",
      "function (but see the section beginning on page 738).\n",
      "Although the deﬁnition allows the integral to be inﬁnite, we use a special\n",
      "term for the case in which the integral is ﬁnite. If both R f+ dν and R f−dν are\n",
      "ﬁnite, the integral itself is ﬁnite, and in that case we say that f is integrable.\n",
      "Note that being Borel does not imply that a function is integrable.\n",
      "We deﬁne the integral over a domain A as\n",
      "Z\n",
      "A\n",
      "f dν =\n",
      "Z\n",
      "IAf dν.\n",
      "(0.1.46)\n",
      "Although we may not explicitly identify the underlying measure space, tech-\n",
      "nically there is one, say (Ω, F, ν), and A ∈F and so A ⊆Ω.\n",
      "Measures Deﬁned by Integrals\n",
      "The integral over a domain together with a nonnegative Borel function leads\n",
      "to an induced measure: If a given measure space (Ω, F, ν) and a given non-\n",
      "negative Borel function f, let λ(A) for A ⊆Ωbe deﬁned as\n",
      "λ(A) =\n",
      "Z\n",
      "A\n",
      "f dν.\n",
      "(0.1.47)\n",
      "Then λ(A) is a measure over (Ω, F) (exercise). Furthermore, because\n",
      "ν(A) = 0 ⇒λ(A) = 0,\n",
      "λ is absolutely continuous with respect to ν.\n",
      "If f ≡1 the integral with respect to a given measure deﬁnes the same\n",
      "measure. This leads to the representation of the probability of an event as an\n",
      "integral. Given a probability space (Ω, F, P ), R\n",
      "A dP is the probability of A,\n",
      "written P (A) or Pr(A).\n",
      "The properties of a measure deﬁned by an integral depend on the prop-\n",
      "erties of the underlying measure space and the function. For example, in IR\n",
      "with Lebesgue measure ν, the measure for Borel sets of positive reals deﬁned\n",
      "by\n",
      "λ(A) =\n",
      "Z\n",
      "A\n",
      "1\n",
      "x dν(x)\n",
      "(0.1.48)\n",
      "is a Haar measure (see Deﬁnition 0.1.18). More interesting Haar measures are\n",
      "those deﬁned over nonsingular n × n real matrices,\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "730\n",
      "0 Statistical Mathematics\n",
      "µ(D) =\n",
      "Z\n",
      "D\n",
      "1\n",
      "|det(X)|n dX,\n",
      "or over matrices in the orthogonal group. (See Gentle (2007), pages 169–171.)\n",
      "Properties of the Lebesgue Integral\n",
      "Lebesgue integrals have several useful and important properties. In this sec-\n",
      "tion, we will consider integrals of a ﬁnite number of functions (often just one)\n",
      "over a single measure space. (A ﬁnite number greater than one is essentially\n",
      "equivalent to two.)\n",
      "In Section 0.1.6 we will consider countable integrals and integrals of a\n",
      "countable number of functions over a single measure space.\n",
      "In Section 0.1.6 we will consider integrals over more than one measure\n",
      "space.\n",
      "The following theorems state several properties of integrals.\n",
      "Theorem 0.1.17\n",
      "Let f be a Borel function. Then\n",
      "ν({ω | f(ω) > 0}) > 0 =⇒\n",
      "Z\n",
      "fdν > 0.\n",
      "Proof. Exercise.\n",
      "Theorem 0.1.18\n",
      "Let f be a Borel function. Then\n",
      "Z\n",
      "fdν < ∞=⇒f < ∞a.e.\n",
      "Proof. Exercise.\n",
      "Theorem 0.1.19\n",
      "Let f and g be Borel functions. Then\n",
      "(i). f = 0 a.e. =⇒\n",
      "R\n",
      "fdν = 0;\n",
      "(ii). f ≤g a.e. =⇒\n",
      "R\n",
      "fdν ≤\n",
      "R\n",
      "gdν;\n",
      "(iii). f = g a.e. =⇒R fdν = R gdν.\n",
      "Proof. Exercise.\n",
      "One of the most important properties of the integral is the fact that it is\n",
      "a linear operator.\n",
      "Theorem 0.1.20 (linearity)\n",
      "For real a and Borel f and g,\n",
      "R\n",
      "(af + g) dν = a\n",
      "R\n",
      "f dν +\n",
      "R\n",
      "g dν.\n",
      "Proof. To see this, we ﬁrst show it for nonnegative functions f+ and g+,\n",
      "using the deﬁnition in equation (0.1.41). Then we use the deﬁnition in equa-\n",
      "tion (0.1.44) for general Borel functions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "731\n",
      "Theorem 0.1.21\n",
      "(i). R |f| dν ≥0\n",
      "(ii).\n",
      "R\n",
      "|f| dν = 0 ⇒f = 0 a.e.\n",
      "Proof. Follows immediately from the deﬁnition.\n",
      "This fact together with the linearity means that\n",
      "R\n",
      "|f| dν is a pseudonorm\n",
      "for functions. A more general pseudonorm based on the integral is (R |f|p dν)1/p\n",
      "for 1 ≤p. (Deviating slightly from the usual deﬁnition of a norm, it may seem\n",
      "reasonable to allow the implication of R |f| dν = 0 to be only almost every-\n",
      "where. Strictly speaking, however, without this weakened form of equality to\n",
      "0,\n",
      "R\n",
      "|f| dν is only a pseudonorm. See the discussion on page 638.)\n",
      "Another important property of the integral is its monotonicity. First, we\n",
      "state this for a ﬁnite number of functions and integrals (in fact, for just two be-\n",
      "cause in this case, two is eﬀectively any ﬁnite number). Later, in Section 0.1.6,\n",
      "we will consider analogous properties for an inﬁnitely countable number of\n",
      "functions.\n",
      "Theorem 0.1.22 (ﬁnite monotonicity)\n",
      "For integrable f and g, f ≤g a.e. ⇒R f dν ≤R g dν.\n",
      "Proof. Follows immediately from the deﬁnition.\n",
      "Limits of Functions and Limits of Integrals\n",
      "There are some conditions for interchange of an integration operation and a\n",
      "limit operation that are not so obvious. The following theorems address this\n",
      "issue and are closely related to each other. The fundamental theorems are the\n",
      "monotone convergence theorem, Fatou’s lemma, and Lebesgue’s dominated\n",
      "convergence theorem. These same three theorems provide important relation-\n",
      "ships between sequences of expectations and expectations of sequences, as we\n",
      "see on pages 89 and 113.\n",
      "We begin with a lemma to prove the monotone convergence theorem.\n",
      "Lemma 0.1.23.1\n",
      "Assume a measure space (Ω, F, ν), and Borel measurable functions fn and f.\n",
      "0 ≤fn(ω) ↗f(ω) ∀ω =⇒0 ≤\n",
      "Z\n",
      "fndν ↗\n",
      "Z\n",
      "fdν.\n",
      "Proof.\n",
      "First, we observe that\n",
      "R\n",
      "fndν is nondecreasing and is bounded above by\n",
      "R\n",
      "fdν.\n",
      "(This is Theorem 0.1.190.1.19.) So all we need to show is that limn\n",
      "R\n",
      "fndν ≥\n",
      "R fdν. That is, writing the latter integral in the form of equation (0.1.43), we\n",
      "need to show that\n",
      "lim\n",
      "n\n",
      "Z\n",
      "fndν ≥sup\n",
      "X\n",
      "i\n",
      "inf\n",
      "ω∈Ai f(ω)ν(Ai),\n",
      "(0.1.49)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "732\n",
      "0 Statistical Mathematics\n",
      "where the sup is taken over all ﬁnite partitions of Ω.\n",
      "We consider separately the two cases determined by whether the right-\n",
      "hand side is ﬁnite.\n",
      "First, suppose the right side is ﬁnite, and further, that each term in the\n",
      "sum is ﬁnite and positive; that is, each infω∈Ai f(ω) and each ν(Ai) are ﬁnite\n",
      "and positive. (Recall the convention on page 728 for the terms in this sum\n",
      "that ∞· 0 = 0 · ∞= 0.) In this case there exists an ϵ such that for each i,\n",
      "0 < ϵ < yi, where yi = infω∈Ai f(ω). Now, deﬁne the subset of Ai,\n",
      "Ain = {ω | ω ∈Ai, fn(ω) > yi −ϵ}.\n",
      "We now have a ﬁnite partition of Ω(as required in the deﬁnition of the\n",
      "integral) consisting of the Ain and the complement of their union, and so\n",
      "taking only some of the terms that represent the integral as a sum over that\n",
      "partition, we have\n",
      "Z\n",
      "fndν ≥\n",
      "X\n",
      "i\n",
      "(yi −ϵ)ν(Ain).\n",
      "(0.1.50)\n",
      "Because fn ↗f, we have for each i, Ain ↗Ai and the complement of their\n",
      "union goes to ∅. Because of ν is continuous from below, we have for the term\n",
      "on the right above,\n",
      "X\n",
      "i\n",
      "(yi −ϵ)ν(Ain) →\n",
      "X\n",
      "i\n",
      "(yi −ϵ)ν(Ai).\n",
      "Hence, from inequality (0.1.50),\n",
      "Z\n",
      "fndν ≥\n",
      "Z\n",
      "fdν −ϵ\n",
      "X\n",
      "i\n",
      "ν(Ai),\n",
      "and because all ν(Ai) are ﬁnite and ϵ can be arbitrarily close to 0, we have\n",
      "what we wanted to show, that is, inequality (0.1.49).\n",
      "Now, still assuming that the right side of (0.1.49) is ﬁnite, that is, that\n",
      "R fdν is ﬁnite, we allow some terms to be zero. (They all must still be ﬁnite\n",
      "as above, however.) Let us relabel the terms in the ﬁnite partition so that\n",
      "for i ≤m0, yiν(Ai) > 0 and for m0 < i ≤m, yiν(Ai) = 0. If m0 < 1, then\n",
      "all yiν(Ai) = 0, and we have inequality (0.1.49) immediately; otherwise for\n",
      "i ≤m0, both yi and ν(Ai) are positive and ﬁnite. In this case we proceed\n",
      "as before, but only for the positive terms; that is, for i ≤m0, we deﬁne\n",
      "Ain as above, form the inequality (0.1.50), and by the same steps establish\n",
      "inequality (0.1.49).\n",
      "Finally, suppose\n",
      "R\n",
      "fdν is inﬁnite. In this case, for some i0, both yi0 and\n",
      "ν(Ai0) are positive and at least one is inﬁnite. Choose positive constants δy\n",
      "and δA and bound them away from 0:\n",
      "0 < δy < yi0 ≤∞\n",
      "and\n",
      "0 < δA < ν(Ai0) ≤∞.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "733\n",
      "Now, similarly as before, deﬁne a subset of Ai0:\n",
      "Ai0n = {ω | ω ∈Ai0, fn(ω) > δy}.\n",
      "As before, fn ↗f =⇒Ai0n ↗Ai0, and so for some n0, for all n > n0,\n",
      "µ(Ai0n) > δA. Now, with the partition of Ωconsisting of Ai0n and its com-\n",
      "plement, we have\n",
      "Z\n",
      "fndν ≥δyδA,\n",
      "for n > n0,\n",
      "hence, limn\n",
      "R fndν ≥δyδA. Now, if yi0 = ∞, let δy →∞and if ν(Ai0) =\n",
      "∞, let δA →∞. Either way, we have limn\n",
      "R fndν = ∞and so we have\n",
      "inequality (0.1.49).\n",
      "Notice that this lemma applies in the case of pointwise convergence. If\n",
      "convergence is uniform, it would immediately apply in the case of convergence\n",
      "a.e. The next theorem provides the desired generalization.\n",
      "Theorem 0.1.23 (monotone convergence)\n",
      "Let 0 ≤f1 ≤f2 ≤· · ·, and f be Borel functions, and let limn→∞fn = f a.e.,\n",
      "then\n",
      "Z\n",
      "fn dν ↗\n",
      "Z\n",
      "f dν.\n",
      "(0.1.51)\n",
      "Proof.\n",
      "Assume the hypothesis: that is, fn ↗f for all ω ∈A where ν(Ac) =\n",
      "0. Now restrict each function to A, and observe that fnIA ↗fIA and\n",
      "R fnIA dν = R fn dν and R fIA dν = R f dν. Lemma 0.1.23.1 immediately\n",
      "implies\n",
      "R\n",
      "fn dν ↗\n",
      "R\n",
      "f dν.\n",
      "That Theorem 0.1.23 follows so readily from Lemma 0.1.23.1 is another\n",
      "illustration of a principle of real analysis stated by Littlewood that every con-\n",
      "vergent sequence of functions is “nearly” uniformly convergent (see page 761).\n",
      "In the hypotheses of the lemma, we have only pointwise convergence. Without\n",
      "needing uniform convergence, however, we extend the conclusion to the case\n",
      "of convergence a.e.\n",
      "Theorem 0.1.24 (Fatou’s lemma)\n",
      "For nonnegative integrable Borel fn,\n",
      "Z\n",
      "lim\n",
      "n inf fn dν ≤lim\n",
      "n inf\n",
      "Z\n",
      "fn dν.\n",
      "(0.1.52)\n",
      "Proof.\n",
      "Let gn = infk≥n fk and g =\n",
      "R\n",
      "limn inf fn. As in the monotone convergence\n",
      "theorem, gn is nonnegative and gn ↗g, so\n",
      "R\n",
      "gn dν ↗\n",
      "R\n",
      "g dν. Also, for each\n",
      "n,\n",
      "R\n",
      "fn dν ≥\n",
      "R\n",
      "gn dν; hence, we have the desired conclusion.\n",
      "The next theorem is the most powerful of the convergence theorems for\n",
      "integrals.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "734\n",
      "0 Statistical Mathematics\n",
      "Theorem 0.1.25 (Lebesgue’s dominated convergence)\n",
      "If limn→∞fn = f a.e. and there exists an integrable function g such that\n",
      "|fn| ≤g a.e., then\n",
      "lim\n",
      "n→∞\n",
      "Z\n",
      "fn dν =\n",
      "Z\n",
      "f dν.\n",
      "(0.1.53)\n",
      "Proof.\n",
      "***\n",
      "Corollary 0.1.25.1 (bounded convergence)\n",
      "Let {fn} be a sequence of measurable functions deﬁned on a set A, where\n",
      "ν(A) < ∞. If for some real number M, |fn(ω)| ≤M, and limn→∞fn(ω) =\n",
      "f(ω) for each ω ∈A then\n",
      "lim\n",
      "n→∞\n",
      "Z\n",
      "A\n",
      "fn dν =\n",
      "Z\n",
      "A\n",
      "f dν.\n",
      "(0.1.54)\n",
      "Integrals over More than One Measure Space\n",
      "So far the integrals we have discussed have been for functions over a single\n",
      "measure space. We now consider integrals over more than one measure space.\n",
      "We ﬁrst consider a space (Ω, F, ν) and a space (Λ, G, µ) together with a\n",
      "function f : (Ω, F) 7→(Λ, G) that deﬁnes the measure µ, that is, µ = ν ◦f−1.\n",
      "This is change of variables.\n",
      "We next consider the relation between integration in a product measure\n",
      "space to integration in each of the component measure spaces. Fubini’s theo-\n",
      "rem tells us that the integral over the product space is the same as the iterated\n",
      "integral.\n",
      "We use Fubini’s theorem in a somewhat surprising way to derive a useful\n",
      "formula for integration of products of functions called integration by parts.\n",
      "Later, in Section 0.1.7, we consider two diﬀerent measures on the same\n",
      "space and ﬁnd that if one measure dominates the other, there is a unique\n",
      "function whose integral wrt to the dominating measure deﬁnes a measure as\n",
      "in equation (0.1.47) that is the same as the dominated measure. This is the\n",
      "Radon-Nikodym theorem, and leads to a useful function, the Radon-Nikodym\n",
      "derivative.\n",
      "Change of Variables\n",
      "Consider two measurable spaces (Ω, F) and (Λ, G), let f be a measurable\n",
      "function from (Ω, F) to (Λ, G), and let ν be a measure on F. As we have seen,\n",
      "ν ◦f−1 is an induced measure on G. Now let g be a Borel function on (Λ, G).\n",
      "Then the integral of g ◦f over Ωwith respect to ν is the same as the integral\n",
      "of g over Λ with respect to ν ◦f−1:\n",
      "Z\n",
      "Ω\n",
      "g ◦f dν =\n",
      "Z\n",
      "Λ\n",
      "g d(ν ◦f−1)\n",
      "(0.1.55)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "735\n",
      "Integration in a Product Space; Fubini’s Theorem\n",
      "Given two measure spaces (Ω1, F1, ν1) and (Ω2, F2, ν2) with σ-ﬁnite measures\n",
      "and a Borel function f on Ω1 × Ω2, the integral over Ω1, if it exists, is a\n",
      "function of ω2 ∈Ω2 a.e., and likewise, the integral over Ω2, if it exists, is a\n",
      "function of ω2 ∈Ω1 a.e. Fubini’s theorem shows that if one of these marginal\n",
      "integrals, exists a.e., then the natural extension of an integral to a product\n",
      "space, resulting in the double integral, is the same as the iterated integral.\n",
      "Theorem 0.1.26 (Fubini’s theorem)\n",
      "Let (Ω1, F1, ν1) and (Ω2, F2, ν2) be measure spaces where the measures ν1 and\n",
      "ν2 are σ-ﬁnite. Let f be a Borel function on Ω1 × Ω2, such that the marginal\n",
      "integral\n",
      "g(ω2) =\n",
      "Z\n",
      "Ω1\n",
      "f(ω1, ω2) dν1\n",
      "exists a.e. Then\n",
      "Z\n",
      "Ω1×Ω2\n",
      "f(ω1, ω2) dν1 × dν2 =\n",
      "Z\n",
      "Ω2\n",
      "\u0012Z\n",
      "Ω1\n",
      "f(ω1, ω2) dν1\n",
      "\u0013\n",
      "dν2.\n",
      "(0.1.56)\n",
      "Proof. A proof is given in Billingsley (1995), page 233.\n",
      "Integration by Parts\n",
      "*** corollary *** If f and g are bounded on the interval [a, b] and have no\n",
      "common points of discontinuity in that interval, then\n",
      "Z\n",
      "[a,b]\n",
      "f(x)dg(x) = f(b)g(b) −f(a)g(a) −\n",
      "Z\n",
      "[a,b]\n",
      "g(x)df(x).\n",
      "(0.1.57)\n",
      "This is proved using Fubini’s theorem.\n",
      "The Riemann Integral\n",
      "The Riemann integral is one of the simplest integrals. The Riemann inte-\n",
      "gral is deﬁned over intervals of IR (or over rectangles in IRk). The Rie-\n",
      "mann integral over the interval [a, b] is deﬁned in terms of a partitioning\n",
      "{[x1, x0[, [x2, x1[, . . ., [xn −xn−1]}. We can deﬁne the Riemann integral of a\n",
      "real function f over the interval [a, b] in terms of the Lebesgue measure λ as\n",
      "the real number r such that for any ϵ > 0, there exists a δ such that\n",
      "\f\f\f\f\fr −\n",
      "X\n",
      "i∈P\n",
      "f(xi)λ(Ii)\n",
      "\f\f\f\f\f < ϵ\n",
      "(0.1.58)\n",
      "where {Ii\n",
      ":\n",
      "i ∈P } is any ﬁnite partition of ]a, b[ such that for each i,\n",
      "λ(Ii) < δ and xi ∈Ii. If the Riemann integral exists, it is the same as the\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "736\n",
      "0 Statistical Mathematics\n",
      "Lebesgue integral. We use the same notation for the Riemann integral as for\n",
      "the Lebesgue integral, that is, we write r as\n",
      "r =\n",
      "Z b\n",
      "a\n",
      "f(x)dx.\n",
      "(0.1.59)\n",
      "Because of the use of Lebesgue measure in the deﬁnition, the integral over\n",
      "[a, b] is the same as over ]a, b[.\n",
      "A classic example for which the Lebesgue integral exists, but the Riemann\n",
      "integral does not, is the Dirichlet function (Example 0.1.6) restricted to ]0, 1];\n",
      "that is, the function g deﬁned over ]0, 1] as g(x) = 1 if x is rational, and\n",
      "g(x) = 0 otherwise. The Lebesgue integral R 1\n",
      "0 g(x) dx exists and equals 0,\n",
      "because g(x) = 0 a.e. The Riemann integral, on the other had does not exist\n",
      "because for an arbitrary partition {Ii}, the integral is 1 if xi ∈Ii is taken as\n",
      "a rational, and the integral is 0 if xi ∈Ii is taken as an irrational number.\n",
      "The Riemann integral lacks the three convergence properties of the Lebesgue\n",
      "integral given on page 733.\n",
      "We will not develop the properties of the Riemann integral here. When the\n",
      "Riemann integral exists, it has the same properties as the Lebesgue integral,\n",
      "such as linearity. Hence, the separately important questions involve the exis-\n",
      "tence of the Riemann integral. We list some suﬃcient conditions for existence\n",
      "below. Proofs of these and other properties of the Riemann integral can be\n",
      "found in texts on advanced calculus, such as Khuri (2003).\n",
      "•\n",
      "If f(x) is continuous on [a, b], then it is Riemann integrable over [a, b].\n",
      "•\n",
      "If f(x) is monotone (increasing or decreasing) on [a, b], then it is Riemann\n",
      "integrable over [a, b]. (Notice that the function may not be continuous.)\n",
      "•\n",
      "If f(x) is of bounded variation on [a, b], then it is Riemann integrable over\n",
      "[a, b].\n",
      "The Riemann-Stieltjes Integral\n",
      "The Riemann-Stieltjes integral is a generalization of the Riemann integral in\n",
      "which dx is replaced by dg(x) and the interval lengths are replaced by changes\n",
      "in g(x). We write it as\n",
      "rs =\n",
      "Z b\n",
      "a\n",
      "f(x)dg(x).\n",
      "(0.1.60)\n",
      "To deﬁne the Riemann-Stieltjes integral we will handle the partitions\n",
      "slightly diﬀerently from how they were used in equation (0.1.58) for the\n",
      "Riemann integral. (Either way could be used for either integral, however.\n",
      "This is diﬀerent from integrals with respect to stochastic diﬀerentials, where\n",
      "the endpoints matter; see Section 0.2.2.) Form a partition of [a, b], call it\n",
      "P = (a = x0 < x1 < · · · < xn = b), and let ∆gi = g(xi) −g(xi−1). We now\n",
      "consider the sup and inf of f within each interval of the partition and the inf\n",
      "and sup of sums of over all partitions:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "737\n",
      "inf\n",
      "P\n",
      "n\n",
      "X\n",
      "i=1\n",
      "sup\n",
      "x∈[xi,xi−1]\n",
      "f(x)∆gi\n",
      "and\n",
      "sup\n",
      "P\n",
      "n\n",
      "X\n",
      "i=1\n",
      "inf\n",
      "x∈[xi,xi−1] f(x)∆gi.\n",
      "If these are equal, then the Riemann-Stieltjes integral is deﬁned as their com-\n",
      "mon value:\n",
      "Z b\n",
      "a\n",
      "f(x)dg(x) = inf\n",
      "P\n",
      "n\n",
      "X\n",
      "i=1\n",
      "sup\n",
      "x∈[xi,xi−1]\n",
      "f(x)∆gi = sup\n",
      "P\n",
      "n\n",
      "X\n",
      "i=1\n",
      "inf\n",
      "x∈[xi,xi−1] f(x)∆gi.\n",
      "(0.1.61)\n",
      "There is a simple connection with Riemann-Stieltjes integral and the Rie-\n",
      "mann integral whenever g′(x) exists and is continuous.\n",
      "Theorem 0.1.27\n",
      "Suppose that Riemann-Stieltjes integral\n",
      "R b\n",
      "a f(x)dg(x) exists and suppose the\n",
      "derivative of g, g′(x) exists and is continuous on [a, b]; then\n",
      "Z b\n",
      "a\n",
      "f(x)dg(x) =\n",
      "Z b\n",
      "a\n",
      "f(x)g′(x)dx.\n",
      "Proof. Exercise. (Hint: use the mean-value theorem together with the respec-\n",
      "tive deﬁnitions.)\n",
      "The existence of the Riemann-Stieltjes integral depends on f.\n",
      "Theorem 0.1.28\n",
      "If f(x) is continuous on [a, b], then Riemann-Stieltjes integrable on [a, b].\n",
      "Proof. Exercise. (Hint: just determine an appropriate g(x).)\n",
      "The Riemann-Stieltjes integral can exist for discontinuous f (under the\n",
      "same conditions as the Riemann integral), but may fail to exist when f and\n",
      "g are discontinuous at the same point.\n",
      "The Riemann-Stieltjes integral is often useful when g(x) is a step func-\n",
      "tion. We usually deﬁne step functions to be continuous from the right. This\n",
      "allows easy development and interpretation of impulse functions and transfer\n",
      "functions.\n",
      "Theorem 0.1.29\n",
      "Let g(x) be a step function on [a, b] such that for the partition\n",
      "P = (a = x0 < x1 < · · · < xn = b),\n",
      "g(x) is constant over each subinterval in the partition. For i = 1, . . ., n, let\n",
      "gi = g(xi−1) (this means g(x) = gi on [xi−1, xi[), and let gn+1 = g(b). Let\n",
      "∆gi = gi+1 −gi. If f(x) is bounded on [a, b] and is continuous at x1, · · · , xn,\n",
      "then\n",
      "Z b\n",
      "a\n",
      "f(x)dg(x) =\n",
      "n\n",
      "X\n",
      "i=1\n",
      "∆gif(xi).\n",
      "(0.1.62)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "738\n",
      "0 Statistical Mathematics\n",
      "Proof. Exercise.\n",
      "In a special case of this theorem, g is the Heaviside function H, and we\n",
      "have\n",
      "Z b\n",
      "a\n",
      "f(x)dH(x) =\n",
      "Z b\n",
      "a\n",
      "f(x)δ(x)dx = f(0),\n",
      "where δ(x) is the Dirac delta function.\n",
      "Improper Integrals\n",
      "Because of the restriction on the Lebesgue measure of the subintervals in\n",
      "the deﬁnitions of the Riemann and Riemann-Stieltjes integrals, if a = ∞or\n",
      "b = ∞, the integral is not deﬁned. We deﬁne an “improper” Riemann integral,\n",
      "however, as, for example,\n",
      "Z ∞\n",
      "a\n",
      "f(x)dx = lim\n",
      "b→∞\n",
      "Z b\n",
      "a\n",
      "f(x)dx.\n",
      "(0.1.63)\n",
      "Notice that an analogous deﬁnition for such a special case is not necessary\n",
      "for a Lebesgue integral.\n",
      "Adding the improper Riemann integral to the deﬁnition of the integral\n",
      "itself yields an instance where the Riemann integral that exists even though\n",
      "the Lebesgue integral does not. Recall (page 729) that the Lebesgue integral\n",
      "Z ∞\n",
      "0\n",
      "sin(x)/x dx\n",
      "(0.1.64)\n",
      "does not exist because both the positive part and the negative of the negative\n",
      "part are ∞.\n",
      "Whether the integral is interpreted in the Riemann sense or in the\n",
      "Lebesgue sense, we may be interested in\n",
      "lim\n",
      "t→∞\n",
      "Z t\n",
      "0\n",
      "sin(x)/x dx.\n",
      "(In the Riemann sense, we would just write that as\n",
      "R ∞\n",
      "0\n",
      "sin(x)/x dν(x), but it\n",
      "is not standard to use such notation for Lebesgue integrals unless they exist\n",
      "by Deﬁnition 0.1.41.) With some eﬀort (see Billingsley (1995), for example,\n",
      "in which Fubini’s theorem is used), we have\n",
      "lim\n",
      "t→∞\n",
      "Z t\n",
      "0\n",
      "sin(x)\n",
      "x\n",
      "dx = π\n",
      "2 .\n",
      "(0.1.65)\n",
      "This is the same value as the Riemann improper integral\n",
      "R ∞\n",
      "0\n",
      "sin(x)/x dx, but\n",
      "we do not write it that way when “R ” represents the Lebesgue integral.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "739\n",
      "0.1.7 The Radon-Nikodym Derivative\n",
      "Given a measure ν on (Ω, F) and an integrable function f, we have seen that\n",
      "λ(A) =\n",
      "Z\n",
      "A\n",
      "fdν\n",
      "∀A ∈F\n",
      "is also a measure on (Ω, F) and that λ ≪ν. The Radon-Nikodym theorem\n",
      "says that given two such measures, λ ≪ν, then a function f exists.\n",
      "Theorem 0.1.30 (Radon-Nikodym theorem)\n",
      "Given two measures ν and λ on the same measurable space, (Ω, F), such that\n",
      "λ ≪ν and ν is σ-ﬁnite. Then there exists a unique a.e. nonnegative Borel\n",
      "function f on Ωsuch that λ(A) =\n",
      "R\n",
      "A fdν ∀A ∈F.\n",
      "Proof. A proof is given in Billingsley (1995), page 422.\n",
      "Uniqueness a.e. means that if also, for some g, λ(A) = R\n",
      "A gdν ∀A ∈F\n",
      "then f = g a.e.\n",
      "Deﬁnition 0.1.42 (Radon-Nikodym derivative)\n",
      "Let ν and λ be σ-ﬁnite measures on the same measurable space and λ ≪ν.\n",
      "Let f be the function such that\n",
      "λ(A) =\n",
      "Z\n",
      "A\n",
      "fdν ∀A ∈F.\n",
      "Then f is called the Radon-Nikodym derivative of λ with respect to ν, and we\n",
      "write f = dλ/dν.\n",
      "Notice an important property of the derivative: If dλ/dν > 0 over A, but\n",
      "λ(A) = 0, then ν(A) = 0.\n",
      "With this deﬁnition of a derivative, we have the familiar properties for\n",
      "measures λ, λ1, λ2, µ, and ν on the same measurable space, (Ω, F):\n",
      "1. If λ ≪ν, with ν σ-ﬁnite, and f ≥0, then\n",
      "Z\n",
      "fdλ =\n",
      "Z\n",
      "f dλ\n",
      "dν dν.\n",
      "(0.1.66)\n",
      "2. If λ1 ≪ν and λ1 + λ2 ≪ν, with ν σ-ﬁnite, then\n",
      "d(λ1 + λ2)\n",
      "dν\n",
      "= dλ1\n",
      "dν + dλ2\n",
      "dν\n",
      "a.e. ν.\n",
      "(0.1.67)\n",
      "3. If λ ≪µ ≪ν, with µ and ν σ-ﬁnite, then\n",
      "dλ\n",
      "dν = dλ\n",
      "dµ\n",
      "dµ\n",
      "dν\n",
      "a.e. ν.\n",
      "(0.1.68)\n",
      "If λ ≡ν, then\n",
      "dλ\n",
      "dν =\n",
      "\u0012dν\n",
      "dλ\n",
      "\u0013−1\n",
      "a.e. ν and µ.\n",
      "(0.1.69)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "740\n",
      "0 Statistical Mathematics\n",
      "4. If (Ω1, F1) and (Ω2, F2) are measurable spaces, λ1 and ν1, with λ1 ≪ν1,\n",
      "are measures on (Ω1, F1), λ2 and ν2, with λ2 ≪ν2, are measures on\n",
      "(Ω2, F2), and ν1 and ν2 are σ-ﬁnite, then for ω1 ∈Ω1 and ω2 ∈Ω2\n",
      "d(λ1 + λ2)\n",
      "d(ν1 + ν2) (ω1, ω2) = dλ1\n",
      "dν1\n",
      "(ω1)dλ2\n",
      "dν2\n",
      "(ω2)\n",
      "a.e. ν1 × ν2.\n",
      "(0.1.70)\n",
      "The proofs of all of these are exercises.\n",
      "An absolutely continuous function has a derivative almost everywhere; and\n",
      "if the derivative is 0 a.e., the function is constant.\n",
      "0.1.8 Function Spaces\n",
      "Real-valued linear function spaces are sets of real-valued functions over a\n",
      "common domain that are closed with respect to the standard operations of\n",
      "pointwise addition and scalar multiplication of function values.\n",
      "In addition to the operations of pointwise addition and scalar multiplica-\n",
      "tion that are basic to a function space, there are other interesting operations\n",
      "on functions. One of the most common is function composition, often denoted\n",
      "by “◦”. For the functions f and g, the composition f ◦g is deﬁned by\n",
      "f ◦g(x) = f(g(x)).\n",
      "(0.1.71)\n",
      "Notice that the operation is not commutative and that the range of g must\n",
      "be a subset of the domain of f for the composition to exist.\n",
      "Two useful types of real function spaces are those that contain smooth\n",
      "functions and those that contain integrable functions. We ﬁrst describe spaces\n",
      "of smooth functions of various degrees, and then in Section 0.1.9 discuss spaces\n",
      "of integrable functions of various types and the kinds of operations that can\n",
      "be deﬁned on those spaces.\n",
      "Other useful operations on functions are correlation, convolution, inner\n",
      "product, and other transforms. Whether or not a given type of operation can\n",
      "be deﬁned on a function space may depend on the properties of functions, in\n",
      "particular, the integrability of the functions. We therefore defer discussion of\n",
      "these other operations to Section 0.1.9.\n",
      "Spaces of Smooth Real Functions\n",
      "Diﬀerentiability is a smoothness property.\n",
      "Deﬁnition 0.1.43 (Ck space)\n",
      "For an integer k ≥0, a function f such that all derivatives up to the kth\n",
      "derivative exist and are continuous is said to belong to the class Ck.\n",
      "The notation Ck does not specify the domain of the functions. Generally,\n",
      "without any further notation, for d-variate functions, the domain is taken to\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "741\n",
      "be IRd. A domain D can be speciﬁed by the notation Ck(D). For example,\n",
      "C0([0, 1]) refers to the class of all continuous functions over the unit interval\n",
      "[0, 1].\n",
      "The class C0 includes all continuous functions.\n",
      "If f ∈Ck then f ∈Cj for j ≤k.\n",
      "A Ck class of functions over the same domain is a linear space (exercise).\n",
      "The term “smooth” is used in connection with the Ck classes. In a relative\n",
      "sense, for j < k, a function in Ck is smoother than one in Cj and not in Ck.\n",
      "In an absolute sense, a function is said to be “smooth” if it is in C∞.\n",
      "Analytic Functions\n",
      "Not all functions in C∞have a convergent Taylor series at any point (see\n",
      "page 656). The special ones that do are said to be analytic over the region in\n",
      "which the Taylor series at the point x0 converges to the value of the function\n",
      "at x0. (The Taylor series may not converge, and more remarkably, it may\n",
      "not converge to the value of the function.) We sometimes denote the class\n",
      "of analytic functions as Cω. Analytic functions are of course smooth, and\n",
      "Cω ⊆C∞.\n",
      "************** domain *** real line versus complex plane ******************\n",
      "refer to proof of Theorem 1.17) on page 49 as an example of analytic contin-\n",
      "uation.\n",
      "The property of being analytic is quite diﬀerent for real and complex\n",
      "functions. In the case of a complex function of a complex variable f(z), if the\n",
      "ﬁrst derivative of f exists at all points within a region D, then the derivatives\n",
      "of all orders exist. Furthermore, the Taylor series converges to the function\n",
      "value within the region over which the function is analytic. (These facts can\n",
      "be shown using the Cauchy integral formula; see Churchill, 1960, Chapters 5\n",
      "and 6, for example.) The deﬁnition of an analytic complex function is usually\n",
      "diﬀerent from that of a real function. An analytic complex function is deﬁned\n",
      "as one whose (ﬁrst) derivative exists over a region.\n",
      "0.1.9 Lp Real Function Spaces\n",
      "Deﬁnition 0.1.44 (Lp space)\n",
      "Given the measure space (Ω, F, ν) and the real number p ≥1. The space of\n",
      "all measurable functions f on Ωfor which R |f|pdν < ∞is called the Lp(ν)\n",
      "space, or just the Lp space.\n",
      "Although the measure ν is needed to deﬁne the integral, we often drop the\n",
      "ν in Lp(ν). If the integral is taken only over some D ∈F, we may denote the\n",
      "space as Lp(D), and a more complete notation may be Lp(ν, D).\n",
      "An Lp space is a linear space (exercise).\n",
      "An important fact about the Lp spaces is that they are Banach spaces\n",
      "(that is, among other things, they are complete). This fact is called the Riesz-\n",
      "Fischer theorem and is proved in most texts on real analysis.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "742\n",
      "0 Statistical Mathematics\n",
      "There are several types of useful operations on functions in a given function\n",
      "space F. Most binary operations require that the domains of the two func-\n",
      "tions be the same. Function composition, equation (0.1.71), is a very common\n",
      "operation that only requires that the range of one function be in the domain of\n",
      "the other. Many interesting binary operations on functions involve integration\n",
      "of the functions, and so require that the functions be in some Lp space.\n",
      "We now describe some of these operations. Each operation is a mapping.\n",
      "Some binary operations map Lp × Lp to Lq or map Lp × Lp to IR, often for\n",
      "p = 2. Some useful unary operations map Lp to IR, to [−1, 1], or to ¯IR+. The\n",
      "transforms described in Section 0.1.12 map L1 to L1.\n",
      "Convolutions and Covariances and Correlations\n",
      "The convolution of the functions f and g is\n",
      "(f ⋆g)(t) =\n",
      "Z\n",
      "D\n",
      "f(x)g(t −x) dx.\n",
      "(0.1.72)\n",
      "The range of integration is usually either [0, t] or IR. The convolution is a\n",
      "function; often we write the convolution without the dummy argument: f ⋆g.\n",
      "The convolution is a measure of the amount of overlap of one function as\n",
      "it is shifted over another function. The convolution can be thought of as a\n",
      "blending of one function with another.\n",
      "Several properties follow immediately from the deﬁnition:\n",
      "•\n",
      "commutativity:\n",
      "f ⋆g = g ⋆f\n",
      "•\n",
      "associativity:\n",
      "f ⋆(g ⋆h) = (f ⋆g) ⋆h\n",
      "•\n",
      "distribution over addition:\n",
      "f ⋆(g + h) = (f ⋆g) + (f ⋆h)\n",
      "•\n",
      "distribution of scalar multiplication over convolution:\n",
      "a(f ⋆g) = (af) ⋆g.\n",
      "Although because the convolution is commutative the two functions are\n",
      "essentially the same in a convolution, the second function (g in the deﬁnition\n",
      "above) is sometimes called the kernel.\n",
      "The convolution of the n-vectors u and v is\n",
      "(u ⋆v)t =\n",
      "X\n",
      "1≤i,t−i≤n\n",
      "uivt−i.\n",
      "(0.1.73)\n",
      "The indices of vectors in applications involving convolutions are often deﬁned\n",
      "to begin at 0 instead of 1, and in that case, the lower limit above would be 0.\n",
      "The limits for the sum are simpler for inﬁnite-dimensional vectors.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "743\n",
      "For functions f and g that integrate to zero, that is, if\n",
      "Z\n",
      "D\n",
      "f(x) dx =\n",
      "Z\n",
      "D\n",
      "g(x) dx = 0,\n",
      "the covariance of f and g at lag t is\n",
      "Cov(f, g)(t) =\n",
      "Z\n",
      "D\n",
      "f(x)g(t + x) dx.\n",
      "(0.1.74)\n",
      "The argument of the covariance, t, is called the lag. The covariance of a func-\n",
      "tion with itself is called its autocovariance. The autocovariance of a function\n",
      "at zero lag, Cov(f, f)(0), is called its variance.\n",
      "For functions f and g that integrate to zero, the correlation of f and g at\n",
      "lag t is\n",
      "Cor(f, g)(t) =\n",
      "R\n",
      "D f(x) g(t + x) dx\n",
      "p\n",
      "Cov(f, f)(0) Cov(g, g)(0)\n",
      ".\n",
      "(0.1.75)\n",
      "The argument of the correlation, t, is often called the lag, and the correlation\n",
      "of a function with itself is called its autocorrelation.\n",
      "The correlation between two functions is a measure of their similarity. If\n",
      "f near the point x has similar values to those of g near the point x + t, then\n",
      "Cor(f, g)(t) will be relatively large (close to 1). In this case, if t is positive,\n",
      "then f leads g; if t is negative, then f lags g. These terms are symmetric,\n",
      "because\n",
      "Cor(f, g)(−t) = Cor(g, f)(t).\n",
      "Inner Products of Functions\n",
      "Inner products over linear spaces are useful operators. As we saw in Sec-\n",
      "tion 0.0.4, they can be used to deﬁne norms and metrics. An inner product is\n",
      "also sometimes called a dot product.\n",
      "Deﬁnition 0.1.45 (inner product of functions)\n",
      "The inner product of the real functions f and g over the domain D, denoted\n",
      "by ⟨f, g⟩D or usually just by ⟨f, g⟩, is deﬁned as\n",
      "⟨f, g⟩D =\n",
      "Z\n",
      "D\n",
      "f(x)g(x) dx\n",
      "(0.1.76)\n",
      "if the (Lebesgue) integral exists.\n",
      "Of course, often D = IR or D = IRd, and we just drop the subscript and write\n",
      "⟨f, g⟩. (For complex functions, we deﬁne the inner product as R\n",
      "D f(x)¯g(x) dx,\n",
      "where ¯g is the complex conjugate of g. Our primary interest will be in real-\n",
      "valued functions of real arguments.)\n",
      "To avoid questions about integrability, we generally restrict attention to\n",
      "functions whose dot products with themselves exist; that is, to functions that\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "744\n",
      "0 Statistical Mathematics\n",
      "are square Lebesgue integrable over the region of interest. These functions are\n",
      "members of the space L2(D).\n",
      "The standard properties, such as linearity and the Cauchy-Schwarz in-\n",
      "equality, obviously hold for the inner products of functions.\n",
      "We sometimes introduce a weight function, w(x), in the deﬁnition of the\n",
      "inner product of two functions. For the functions f and g, we denote this either\n",
      "as ⟨f, g⟩(µ;D) or as ⟨f, g⟩(w;D), where µ is the measure given by dµ = w(x)dx.\n",
      "In any event, the inner product of with respect to f and g with respect to a\n",
      "weight function, w(x), or with respect to the measure µ, where dµ = w(x)dx\n",
      "is deﬁned as\n",
      "⟨f, g⟩(µ;D) =\n",
      "Z\n",
      "D\n",
      "f(x)g(x)w(x) dx,\n",
      "(0.1.77)\n",
      "if the integral exists. Often, both the weight and the range are assumed to be\n",
      "ﬁxed, and the simpler notation ⟨f, g⟩is used.\n",
      "Norms of Functions\n",
      "The norm of a function f, denoted generically as ∥f∥, is a mapping into the\n",
      "nonnegative reals that satisﬁes the properties of the deﬁnition of a norm given\n",
      "on page 637. A norm of a function ∥f∥is often deﬁned as some nonnegative,\n",
      "strictly increasing function of the inner product of f with itself, ⟨f, f⟩. Not\n",
      "all norms are deﬁned in terms of inner products, however.\n",
      "The property of a norm of an object x that ∥x∥= 0 ⇒x = 0 is an\n",
      "awkward property for a function to satisfy. For a function, it is much more\n",
      "convenient to say that if its norm is zero, it must be zero almost everywhere.\n",
      "Modifying the deﬁnition of a norm in this slight way yields what is often called\n",
      "a pseudonorm.\n",
      "The most common type of norm or pseudonorm for a real scalar-valued\n",
      "function is the Lp norm. It is deﬁned similarly to the Lp vector norm\n",
      "(page 641).\n",
      "Deﬁnition 0.1.46 (Lp (pseudo)norm of functions)\n",
      "For p ≥1, the Lp norm or Lp pseudonorm of the function f, denoted as ∥f∥p,\n",
      "is deﬁned as\n",
      "∥f∥p =\n",
      "\u0012Z\n",
      "D\n",
      "|f(x)|pw(x) dx\n",
      "\u00131/p\n",
      ",\n",
      "(0.1.78)\n",
      "if the integral exists.\n",
      "The triangle inequality in this case is another version of Minkowski’s inequal-\n",
      "ity (0.0.30) (which, as before, we can prove using a version of H¨older’s inequal-\n",
      "ity (0.0.31)). For this reason, Lp (pseudo)norm for functions is also called the\n",
      "Minkowski (pseudo)norm or the H¨older (pseudo)norm.\n",
      "To emphasize the measure of the weighting function, the notation ∥f∥w\n",
      "or ∥f∥µ is sometimes used. (The ambiguity of the possible subscripts on ∥· ∥\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "745\n",
      "is usually resolved by the context.) For functions over ﬁnite domains, w(x) is\n",
      "often taken as w(x) ≡1. This is a uniform weighting.\n",
      "The space of functions for which the integrals in (0.1.78) exist is Lp(w, D).\n",
      "It is clear that ∥f∥p satisﬁes the properties that deﬁne a norm except for\n",
      "the requirement that ∥f∥p = 0 ⇒f = 0. For this latter condition, we must\n",
      "either substitute f = 0 a.e. (and perhaps introduce the concept of equivalence\n",
      "classes of functions), or else settle for ∥f∥p being a pseudonorm. See the\n",
      "discussion on pages 638 and 730.\n",
      "A common Lp function pseudonorm is the L2 norm, which is often denoted\n",
      "simply by ∥f∥. This pseudonorm is related to the inner product:\n",
      "∥f∥2 = ⟨f, f⟩1/2.\n",
      "(0.1.79)\n",
      "The space consisting of the set of functions whose L2 pseudonorms over IR\n",
      "exist together with the pseudonorm, that is, L2(IR), is a Hilbert space.\n",
      "Another common pseudonorm is the limit of the Lp pseudonorm as p →∞.\n",
      "Just as with countable sets, as the L∞norm for vectors in equation (0.0.34),\n",
      "this may be the supremum of the function.\n",
      "A related pseudonorm is more useful, however, because it is the limit of\n",
      "equation (0.1.78) as p →∞(compare equation (0.0.34)). We deﬁne\n",
      "∥f∥∞= ess sup |f(x)w(x)|,\n",
      "(0.1.80)\n",
      "where ess sup denotes the essential supremum of a function, deﬁned for a\n",
      "given measure µ by\n",
      "ess sup g(x) = inf{a : µ({x : x ∈D, g(x) > a}) = 0}.\n",
      "The essential inﬁmum of a function for a given measure µ is deﬁned similarly:\n",
      "ess inf g(x) = sup{a : µ({x : x ∈D, g(x) < a}) = 0}.\n",
      "The pseudonorm deﬁned by equation (0.1.80) is called the L∞norm, the\n",
      "Chebyshev norm, or the uniform norm.\n",
      "Another type of function norm, called the total variation, is an L∞-type of\n",
      "measure of the amount of variability of the function. For a real-valued scalar\n",
      "function f on the interval [a, b], the total variation of f on [a, b] is\n",
      "Vb\n",
      "a(f) = sup\n",
      "π\n",
      "X\n",
      "i\n",
      "|f(xi+1) −f(xi)|,\n",
      "(0.1.81)\n",
      "where π is a partition of [a, b], (a = x0 < x1 < · · · < xn = b).\n",
      "If f is continuously diﬀerentiable over [a, b], then\n",
      "Vb\n",
      "a(f) =\n",
      "Z b\n",
      "a\n",
      "|f′(x)|dx.\n",
      "(0.1.82)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "746\n",
      "0 Statistical Mathematics\n",
      "A normal function is a function whose pseudonorm is 1. A normal function\n",
      "is also called a normal function a normalized function. Although this term\n",
      "can be used with respect to any pseudonorm, it is generally reserved for the\n",
      "L2 pseudonorm (that is, the pseudonorm arising from the inner product). A\n",
      "function whose integral (over a relevant range, usually IR) is 1 is also called\n",
      "a normal function. (Although this latter deﬁnition is similar to the standard\n",
      "one, the latter is broader because it may include functions that are not square-\n",
      "integrable.) Density and weight functions are often normalized (that is, scaled\n",
      "to be normal).\n",
      "Metrics in Function Spaces\n",
      "Statistical properties such as bias and consistency are deﬁned in terms of the\n",
      "diﬀerence of the estimator and what is being estimated. For an estimator of a\n",
      "function, ﬁrst we must consider some ways of measuring this diﬀerence. These\n",
      "are general measures for functions and are not dependent on the distribution\n",
      "of a random variable. How well one function approximates another function is\n",
      "usually measured by a norm of the diﬀerence in the functions over the relevant\n",
      "range.\n",
      "The most common measure of the diﬀerence between two functions, g(·)\n",
      "and f(·), is a metric, ρ(g, f). (See Section 0.0.2 on page 623.) In normed linear\n",
      "spaces, the most useful metric for two elements is the norm of the diﬀerence\n",
      "of the two elements (see pages 623, and 638):\n",
      "ρ(g, f) = ∥g −f∥,\n",
      "if that norm exists and is ﬁnite.\n",
      "The metric corresponding to the Lp norm is\n",
      "ρp(g, f) = ∥g −f∥p.\n",
      "As we mentioned above, the Lp “norm” is not a true norm; hence, the\n",
      "metric induced is only a pseudometric.\n",
      "When one function is an estimate or approximation of the other function,\n",
      "we may call this diﬀerence the “error”.\n",
      "If g is used to approximate f, then ρ∞(g, f), that is, ∥g −f∥∞, is likely\n",
      "to be the norm of interest. This is the norm most often used in numerical\n",
      "analysis when the objective is interpolation or quadrature. This norm is also\n",
      "often used in comparing CDFs. If P and Q are CDFs, ∥P −Q∥∞is called the\n",
      "Kolmogorov distance. For CDFs, this metric always exists and is ﬁnite.\n",
      "In applications with noisy data, or when g may be very diﬀerent from f,\n",
      "∥g −f∥2 may be the more appropriate (pseudo)norm. This is the norm most\n",
      "often used in estimating probability density functions.\n",
      "For comparing two functions g and f we can use a metric based on a norm\n",
      "of their diﬀerence, ∥g −f∥. We often prefer to use a pseudometric, which is\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "747\n",
      "the same as a metric except that ρ(g, f) = 0 if and only if g = f a.e. (We\n",
      "usually just use this interpretation and call it a metric, however.)\n",
      "Deﬁnition 0.1.47 (Hellinger distance)\n",
      "Let P be absolutely continuous with respect to Q and p = dP and q = dq.\n",
      "Then\n",
      "\u0012Z\n",
      "IR\n",
      "\u0010\n",
      "q 1/r(x) −p1/r(x)\n",
      "\u0011r\n",
      "dx\n",
      "\u00131/r\n",
      "(0.1.83)\n",
      "is called the Hellinger distance between p and q.\n",
      "The most common instance has r = 2, and in this case the Hellinger\n",
      "distance is also called the Matusita distance.\n",
      "Other Distances between Functions\n",
      "Sometimes the diﬀerence in two functions is deﬁned asymmetrically. A general\n",
      "class of divergence measures for comparing CDFs was introduced indepen-\n",
      "dently by Ali and Silvey (1966) and Csisz´ar (1967) (see also Pardo (2005)).\n",
      "The measure is based on a convex function φ of a term similar to the “odds”.\n",
      "Deﬁnition 0.1.48 (φ-divergence)\n",
      "Let P be absolutely continuous with respect to Q and φ is a convex function,\n",
      "d(P, Q) =\n",
      "Z\n",
      "IR\n",
      "φ\n",
      "\u0012dP\n",
      "dQ\n",
      "\u0013\n",
      "dQ,\n",
      "(0.1.84)\n",
      "if it exists, is called the φ-divergence from Q to P .\n",
      "The φ-divergence is also called the f-divergence.\n",
      "The φ-divergence is in general not a metric because it is not symmetric.\n",
      "One function is taken as the base from which the other function is measured.\n",
      "The expression often has a more familiar form if both P and Q are dominated\n",
      "by Lebesgue measure and we write p = dP and q = dQ. The Hellinger distance\n",
      "given in equation (0.1.83) is a φ-divergence that is a metric. The Matusita\n",
      "distance is the square root of a φ-divergence with φ(t) = (\n",
      "√\n",
      "t −1)2.\n",
      "Another speciﬁc instance of φ-divergence is the Kullback-Leibler measure.\n",
      "Deﬁnition 0.1.49 (Kullback-Leibler measure)\n",
      "Let P be absolutely continuous with respect to Q and p = dP and q = dq.\n",
      "Then\n",
      "Z\n",
      "IR\n",
      "p(x) log\n",
      "\u0012p(x)\n",
      "q(x)\n",
      "\u0013\n",
      "dx.\n",
      "(0.1.85)\n",
      "is called the Kullback-Leibler measure of the diﬀerence of p and q.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "748\n",
      "0 Statistical Mathematics\n",
      "The Kullback-Leibler measure is not a metric.\n",
      "Various forms of φ-divergence are used in goodness-of-ﬁt analyses. The\n",
      "Pearson chi-squared discrepancy measure, for example, has φ(t) = (t −1)2:\n",
      "Z\n",
      "IR\n",
      "(q(x) −p(x))2\n",
      "q(x)\n",
      "dx.\n",
      "(0.1.86)\n",
      "See the discussion beginning on page 598 for other applications in which two\n",
      "probability distributions are compared.\n",
      "Convergence of Functions\n",
      "We have deﬁned almost everywhere convergence of measurable functions in\n",
      "general measure spaces (see page 726). We will now deﬁne two additional types\n",
      "of convergence of measurable functions in normed linear measure spaces. Here\n",
      "we will restrict our attention to real-valued functions over real domains, but\n",
      "the ideas are more general.\n",
      "The ﬁrst is convergence in Lp.\n",
      "Deﬁnition 0.1.50 (convergence in Lp)\n",
      "Let f1, f2, . . . be a sequence of Borel functions in Lp and let f be another\n",
      "Borel function in Lp. We say that {fn} converges in Lp to f if\n",
      "∥fn −f∥p →0.\n",
      "We write\n",
      "fn\n",
      "Lp\n",
      "→f.\n",
      "Theorem 0.1.31\n",
      "Suppose f, f1, f2, . . . ∈Lp(ν, D) and ν(D) < ∞Then\n",
      "fn\n",
      "Lp\n",
      "→f ⇒fn\n",
      "Lr\n",
      "→f\n",
      "for r ≤p.\n",
      "Proof. Exercise.\n",
      "Convergence in Lp is diﬀerent from convergence a.e.; neither implies the\n",
      "other.\n",
      "give examples:\n",
      "The second is convergence in measure.\n",
      "Deﬁnition 0.1.51 (convergence in measure)\n",
      "Let f1, f2, . . . be a sequence of Borel functions on the measure space (Ω, F, ν)\n",
      "and let f be another Borel function on (Ω, F, ν). We say that {fn} converges\n",
      "in measure to f if *** We write\n",
      "fn\n",
      "ν→f.\n",
      "Convergence in measure is weaker than both Lp convergence and a.e. con-\n",
      "vergence; a.e. implies it.\n",
      "prove\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "749\n",
      "Basis Sets in Function Spaces\n",
      "If each function in a linear space can be expressed as a linear combination\n",
      "of the functions in a set G, then G is said to be a generating set, a spanning\n",
      "set, or a basis set for the linear space. (These three terms are synonymous.)\n",
      "The basis sets for ﬁnite-dimensional vector spaces are ﬁnite; for most function\n",
      "spaces of interest, the basis sets are inﬁnite.\n",
      "A set of functions {qk} is orthogonal over the domain D with respect to the\n",
      "nonnegative weight function w(x) if the inner product with respect to w(x) of\n",
      "qk and ql, ⟨qk, ql⟩, is 0 if k ̸= l; that is,\n",
      "Z\n",
      "D\n",
      "qk(x)¯ql(x)w(x)dx = 0\n",
      "k ̸= l.\n",
      "(0.1.87)\n",
      "If, in addition,\n",
      "Z\n",
      "D\n",
      "qk(x)¯qk(x)w(x)dx = 1,\n",
      "the functions are called orthonormal.\n",
      "In the following, we will be concerned with real functions of real arguments,\n",
      "so we can take ¯qk(x) = qk(x).\n",
      "The weight function can also be incorporated into the individual functions\n",
      "to form a diﬀerent set,\n",
      "˜qk(x) = qk(x)w1/2(x).\n",
      "This set of functions also spans the same function space and is orthogonal\n",
      "over D with respect to a constant weight function.\n",
      "Basis sets consisting of orthonormal functions are generally easier to work\n",
      "with and can be formed from any basis set. Given two nonnull, linearly inde-\n",
      "pendent functions, q1 and q2, two orthonormal vectors, ˜q1 and ˜q2, that span\n",
      "the same space can be formed as\n",
      "˜q1(·) =\n",
      "1\n",
      "∥q1∥q1(·),\n",
      "˜q2(·) =\n",
      "1\n",
      "∥q2 −⟨˜q1, q2⟩˜q1∥\n",
      "\u0000q2(·) −⟨˜q1, q2⟩˜q1(·)\u0001.\n",
      "(0.1.88)\n",
      "These are the Gram-Schmidt function transformations. They can easily be\n",
      "extended to more than two functions to form a set of orthonormal functions\n",
      "from any set of linearly independent functions.\n",
      "Series Expansions in Basis Functions\n",
      "Our objective is to represent a function of interest, f(x), over some domain\n",
      "D ⊆IR, as a linear combination of “simpler” functions, q0(x), q1(x), . . .:\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "750\n",
      "0 Statistical Mathematics\n",
      "f(x) =\n",
      "∞\n",
      "X\n",
      "k=0\n",
      "ckqk(x).\n",
      "(0.1.89)\n",
      "There are various ways of constructing the qk functions. If they are developed\n",
      "through a linear operator on a function space, they are called eigenfunctions,\n",
      "and the corresponding ck are called eigenvalues.\n",
      "We choose a set {qk} that spans some class of functions over the given\n",
      "domain D. A set of orthogonal basis functions is often the best choice because\n",
      "they have nice properties that facilitate computations and a large body of\n",
      "theory about their properties is available.\n",
      "If the function to be estimated, f(x), is continuous and integrable over a\n",
      "domain D, the orthonormality property allows us to determine the coeﬃcients\n",
      "ck in the expansion (0.1.89):\n",
      "ck = ⟨f, qk⟩.\n",
      "(0.1.90)\n",
      "The coeﬃcients {ck} are called the Fourier coeﬃcients of f with respect to\n",
      "the orthonormal functions {qk}.\n",
      "In applications, we approximate the function using a truncated orthogonal\n",
      "series. The error due to ﬁnite truncation at j terms of the inﬁnite series is the\n",
      "residual function f −Pj\n",
      "k=1 ckfk. The mean squared error over the domain D\n",
      "is the scaled, squared L2 norm of the residual,\n",
      "1\n",
      "d\n",
      "f −\n",
      "j\n",
      "X\n",
      "k=0\n",
      "ckqk\n",
      "\n",
      "2\n",
      ",\n",
      "(0.1.91)\n",
      "where d is some measure of the domain D. (If the domain is the interval [a, b],\n",
      "for example, one choice is d = b −a.)\n",
      "A very important property of Fourier coeﬃcients is that they yield the\n",
      "minimum mean squared error for a given set of basis functions {qi}; that is,\n",
      "for any other constants, {ai}, and any j,\n",
      "f −\n",
      "j\n",
      "X\n",
      "k=0\n",
      "ckqk\n",
      "\n",
      "2\n",
      "≤\n",
      "f −\n",
      "j\n",
      "X\n",
      "k=0\n",
      "akqk\n",
      "\n",
      "2\n",
      ".\n",
      "(0.1.92)\n",
      "In applications of statistical data analysis, after forming the approxima-\n",
      "tion, we then estimate the coeﬃcients from equation (0.1.90) by identifying an\n",
      "appropriate probability density that is a factor of the function of interest, f.\n",
      "(Note again the diﬀerence in “approximation” and “estimation”.) Expected\n",
      "values can be estimated using observed or simulated values of the random\n",
      "variable and the approximation of the probability density function.\n",
      "The basis functions are generally chosen to be easy to use in computations.\n",
      "Common examples include the Fourier trigonometric functions sin(kt) and\n",
      "cos(kt) for k = 1, 2, . . ., orthogonal polynomials such as Legendre, Hermite,\n",
      "and so on, splines, and wavelets.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "751\n",
      "Orthogonal Polynomials\n",
      "The most useful type of basis function depends on the nature of the function\n",
      "being estimated. The orthogonal polynomials are useful for a very wide range\n",
      "of functions. Orthogonal polynomials of real variables are their own complex\n",
      "conjugates. It is clear that for the kth polynomial in the orthogonal sequence,\n",
      "we can choose an ak that does not involve x, such that\n",
      "qk(x) −akxqk−1(x)\n",
      "is a polynomial of degree k −1.\n",
      "Because any polynomial of degree k −1 can be represented by a linear\n",
      "combination of the ﬁrst k members of any sequence of orthogonal polynomials,\n",
      "we can write\n",
      "qk(x) −akxqk−1(x) =\n",
      "k−1\n",
      "X\n",
      "i=0\n",
      "ciqi(x).\n",
      "Because of orthogonality, all ci for i < k −2 must be 0. Therefore, collecting\n",
      "terms, we have, for some constants ak, bk, and ck, the three-term recursion\n",
      "that applies to any sequence of orthogonal polynomials:\n",
      "qk(x) = (akx + bk)qk−1(x) −ckqk−2(x),\n",
      "for k = 2, 3, . . ..\n",
      "(0.1.93)\n",
      "This recursion formula is often used in computing orthogonal polynomials.\n",
      "The coeﬃcients in this recursion formula depend on the speciﬁc sequence of\n",
      "orthogonal polynomials, of course.\n",
      "This three-term recursion formula can also be used to develop a formula\n",
      "for the sum of products of orthogonal polynomials qi(x) and qi(y):\n",
      "k\n",
      "X\n",
      "i=0\n",
      "qi(x)qi(y) =\n",
      "1\n",
      "ak+1\n",
      "qk+1(x)qk(y) −qk(x)qk+1(y)\n",
      "x −y\n",
      ".\n",
      "(0.1.94)\n",
      "This expression, which is called the Christoﬀel-Darboux formula, is useful in\n",
      "evaluating the product of arbitrary functions that have been approximated\n",
      "by ﬁnite series of orthogonal polynomials.\n",
      "There are several widely used complete systems of univariate orthogonal\n",
      "polynomials. The diﬀerent systems are characterized by the one-dimensional\n",
      "intervals over which they are deﬁned and by their weight functions. The Leg-\n",
      "endre, Chebyshev, and Jacobi polynomials are deﬁned over [−1, 1] and hence\n",
      "can be scaled into any ﬁnite interval [a, b]. The weight function of the Ja-\n",
      "cobi polynomials is more general, so a ﬁnite sequence of them may ﬁt a given\n",
      "function better, but the Legendre and Chebyshev polynomials are simpler\n",
      "and so are often used. The Laguerre polynomials are deﬁned over the half\n",
      "line [0, ∞[ and hence can be scaled into any half-ﬁnite interval [a, ∞[. The\n",
      "Hermite polynomials are deﬁned over the reals, ] −∞, ∞[.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "752\n",
      "0 Statistical Mathematics\n",
      "Any of these systems of polynomials can be developed easily by beginning\n",
      "with the basis set 1, x, x2, . . . and orthogonalizing them by use of the Gram-\n",
      "Schmidt equations (0.1.88).\n",
      "Table 0.2 summarizes the ranges and weight functions for these standard\n",
      "orthogonal polynomials.\n",
      "Table 0.2. Orthogonal Polynomials\n",
      "Polynomial\n",
      "Weight\n",
      "Series\n",
      "Range\n",
      "Function\n",
      "Legendre\n",
      "[−1, 1]\n",
      "1 (uniform)\n",
      "Chebyshev\n",
      "[−1, 1]\n",
      "(1 −x2)1/2 (symmetric beta)\n",
      "Jacobi\n",
      "[−1, 1]\n",
      "(1 −x)α(1 + x)β (beta)\n",
      "Laguerre\n",
      "[0, ∞[\n",
      "xα−1e−x (gamma)\n",
      "Hermite\n",
      "] −∞, ∞[\n",
      "e−x2/2 (normal)\n",
      "The Legendre polynomials have a constant weight function and are de-\n",
      "ﬁned over the interval [−1, 1]. Using the Gram-Schmidt transformations on\n",
      "1, x, x2, . . ., we have\n",
      "eP0(t) = 1/\n",
      "qR 1\n",
      "−1 12dx\n",
      "= 1/\n",
      "√\n",
      "2,\n",
      "eP1(t) = (t −0)/\n",
      "qR 1\n",
      "−1 x2dx =\n",
      "p\n",
      "3/2t,\n",
      "...\n",
      "(0.1.95)\n",
      "Orthogonal polynomials are often expressed in the simpler, unnormalized\n",
      "form. The ﬁrst few unnormalized Legendre polynomials are\n",
      "P0(t) = 1\n",
      "P1(t) = t\n",
      "P2(t) = (3t2 −1)/2\n",
      "P3(t) = (5t3 −3t)/2\n",
      "P4(t) = (35t4 −30t2 + 3)/8\n",
      "P5(t) = (63t5 −70t3 + 15t)/8\n",
      "(0.1.96)\n",
      "The normalizing constant that relates the kth unnormalized Legendre poly-\n",
      "nomial to the normalized form is determined by noting\n",
      "Z 1\n",
      "−1\n",
      "(Pk(t))2dt =\n",
      "2\n",
      "2k + 1.\n",
      "The recurrence formula for the Legendre polynomials is\n",
      "Pk(t) = 2k −1\n",
      "k\n",
      "tPk−1(t) −k −1\n",
      "k\n",
      "Pk−2(t).\n",
      "(0.1.97)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "753\n",
      "The Hermite polynomials are orthogonal with respect to a Gaussian, or\n",
      "standard normal, weight function. We can form the normalized Hermite poly-\n",
      "nomials using the Gram-Schmidt transformations on 1, x, x2, . . ., with a weight\n",
      "function of ex/2 similarly to what is done in equations (0.1.95).\n",
      "The ﬁrst few unnormalized Hermite polynomials are\n",
      "He\n",
      "0(t) = 1\n",
      "He\n",
      "1(t) = t\n",
      "He\n",
      "2(t) = t2 −1\n",
      "He\n",
      "3(t) = t3 −3t\n",
      "He\n",
      "4(t) = t4 −6t2 + 3\n",
      "He\n",
      "5(t) = t5 −10t3 + 15t\n",
      "(0.1.98)\n",
      "These are not the standard Hermite polynomials, but they are the ones most\n",
      "commonly used by statisticians because the weight function is proportional\n",
      "to the normal density.\n",
      "The recurrence formula for the Hermite polynomials is\n",
      "He\n",
      "k(t) = tHe\n",
      "k−1(t) −(k −1)He\n",
      "k−2(t).\n",
      "(0.1.99)\n",
      "These Hermite polynomials are useful in probability and statistics. The\n",
      "Gram-Charlier series and the Edgeworth series for asymptotic approximations\n",
      "are based on these polynomials. See Section 1.2, beginning on page 65.\n",
      "Multivariate Orthogonal Polynomials\n",
      "Multivariate orthogonal polynomials can be formed easily as tensor products\n",
      "of univariate orthogonal polynomials. The tensor product of the functions\n",
      "f(x) over Dx and g(y) over Dy is a function of the arguments x and y over\n",
      "Dx × Dy:\n",
      "h(x, y) = f(x)g(y).\n",
      "If {q1,k(x1)} and {q2,l(x2)} are sequences of univariate orthogonal polynomi-\n",
      "als, a sequence of bivariate orthogonal polynomials can be formed as\n",
      "qkl(x1, x2) = q1,k(x1)q2,l(x2).\n",
      "(0.1.100)\n",
      "These polynomials are orthogonal in the same sense as in equation (0.1.87),\n",
      "where the integration is over the two-dimensional domain. Similarly as in\n",
      "equation (0.1.89), a bivariate function can be expressed as\n",
      "f(x1, x2) =\n",
      "∞\n",
      "X\n",
      "k=0\n",
      "∞\n",
      "X\n",
      "l=0\n",
      "cklqkl(x1, x2),\n",
      "(0.1.101)\n",
      "with the coeﬃcients being determined by integrating over both dimensions.\n",
      "Although obviously such product polynomials, or radial polynomials,\n",
      "would emphasize features along coordinate axes, they can nevertheless be\n",
      "useful for representing general multivariate functions. Often, it is useful to\n",
      "apply a rotation of the coordinate axes.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "754\n",
      "0 Statistical Mathematics\n",
      "The weight functions, such as those for the Jacobi polynomials, that have\n",
      "various shapes controlled by parameters can also often be used in a mixture\n",
      "model of the function of interest. The weight function for the Hermite poly-\n",
      "nomials can be generalized by a linear transformation (resulting in a normal\n",
      "weight with mean µ and variance σ2), and the function of interest may be\n",
      "represented as a mixture of general normals.\n",
      "0.1.10 Distribution Function Spaces\n",
      "In probability and statistics, one of the most important kinds of function is a\n",
      "cumulative distribution function, or CDF, deﬁned on page 14 both in terms\n",
      "of a probability distribution and in terms of four characterizing properties.\n",
      "A set of CDFs cannot constitute a linear space, because of the restrictions\n",
      "on the functions. Instead, we will deﬁne a distribution function space that\n",
      "has similar properties. If P is a set of CDFs such that for any w ∈[0, 1] and\n",
      "P1, P2 ∈P, (1 −w)P1 + wP2 ∈P, then P is a distribution function space.\n",
      "The CDFs of the ϵ-mixture distributions deﬁned on page 194 is a simple\n",
      "example of a distribution function space. In that space, one of the CDFs is\n",
      "degenerate.\n",
      "Important distribution function spaces are those consisting of CDFs P\n",
      "such that for given p ≥1\n",
      "Z\n",
      "∥t∥pdP < ∞.\n",
      "(0.1.102)\n",
      "Such a distribution function space is denoted by Pp. (Constrast this with the\n",
      "Lp space.) It is clear that Pp1 ⊆Pp2 if p1 ≥p2.\n",
      "Spaces of distribution functions are related to divisibility of the distribu-\n",
      "tions. They are useful in robustness studies. Most of the interesting families\n",
      "of probability distributions as discussed in Chapter 2 do not generate distri-\n",
      "bution function spaces.\n",
      "0.1.11 Transformation Groups\n",
      "On page 630 we have an example of a group on a set of bijections. Such\n",
      "transformation groups are important in statistics and are useful in establishing\n",
      "desirable properties of statistical procedures.\n",
      "Example 0.1.12 (Continuation of Example 0.0.4) Group of linear\n",
      "transformations\n",
      "A common instance of the group G of bijections is formed by functions of the\n",
      "form\n",
      "g(x) = bx −c,\n",
      "x, b, c ∈IR, b ̸= 0.\n",
      "For given g, we see that g−1(x) = (x + c)/b ∈G.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "755\n",
      "Invariant Functions\n",
      "Deﬁnition 0.1.52 (Invariant function)\n",
      "Let G be a transformation group with domain X. A function f with domain\n",
      "X is said to be invariant under the transformation group G if for all x ∈X\n",
      "and g ∈G,\n",
      "f(g(x)) = f(x).\n",
      "(0.1.103)\n",
      "We also use the phrases “invariant over ...” and “invariant with respect to\n",
      "...” to denote this kind of invariance.\n",
      "Example 0.1.13 Invariant function\n",
      "The function\n",
      "f(x) = max(d −x2)\n",
      "is invariant over the group G = {g : g(x) = bx −c,\n",
      "x, b, c ∈IR, b ̸= 0} and\n",
      "function composition.\n",
      "A transformation group G may deﬁne an equivalence relation (identity,\n",
      "symmetry, and transitivity) for elements in its domain, X. If x1, x2 ∈X and\n",
      "there exists a g in G such that g(x1) = x2, then we say x1 and x2 are equivalent\n",
      "under G, and we write\n",
      "x1 ≡x2 modG.\n",
      "(0.1.104)\n",
      "Sets of equivalent points are called orbits of G. (In other contexts such sets\n",
      "are called “residue classes”.) It is clear that a function that is invariant under\n",
      "the transformation group G must be constant over the orbits of G. A trans-\n",
      "formation group G is said to be transitive over the set X if for any x1, x2 ∈X,\n",
      "there exists a g in G such that g(x1) = x2. (This terminology is not standard.\n",
      "Also note that the equivalence relation between elements is always a transi-\n",
      "tive relation.) In this case the whole domain is a single orbit. The group in\n",
      "Example 0.1.12 is transitive over IR.\n",
      "Example 0.1.14 Orbits\n",
      "Consider the group G1 = {g1(x) = 1−x, ge(x) = x :\n",
      "x ∈[0, 1]} and function\n",
      "composition. The group G1 is not transitive, and the orbits of G1 are the pairs\n",
      "(x, 1 −x).\n",
      "Deﬁnition 0.1.53 (Maximal invariant function)\n",
      "An invariant function m over G is called maximal invariant over G if\n",
      "m(x1) = m(x2)\n",
      "⇒\n",
      "∃g ∈G ∋g(x1) = x2.\n",
      "(0.1.105)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "756\n",
      "0 Statistical Mathematics\n",
      "Maximal invariance can be used to characterize invariance. If m is maximal\n",
      "invariant under G, then the function f is invariant under G if and only if it\n",
      "depends on x only through m; that is, if and only if there exists a function h\n",
      "such that for all x, f(x) = h(m(x)).\n",
      "Any invariant function with respect to a transitive group is maximal in-\n",
      "variant.\n",
      "Equivariant Functions\n",
      "Deﬁnition 0.1.54 (Equivariant function)\n",
      "A function f is said to be equivariant under the transformation group G with\n",
      "domain X if for all x ∈X and g ∈G,\n",
      "f(g(x)) = g(f(x)).\n",
      "(0.1.106)\n",
      "We also use the phrases “equivariant over ...” and “equivariant with respect\n",
      "to ...” to denote this kind of equivariance.\n",
      "0.1.12 Transforms\n",
      "Many operations on functions can be facilitated by ﬁrst forming an inner\n",
      "product with the given functions and another speciﬁc function that has an\n",
      "additional argument. The inner product with the function having an addi-\n",
      "tional argument, being itself a function, is a transform of a given function.\n",
      "Because of the linearity of inner products, these are linear transforms. Linear\n",
      "transforms arising from inner products (that is, from integrals) include the\n",
      "familiar Fourier, Laplace, and wavelet integral transforms.\n",
      "An integral linear transform of the function f is an operator T of the\n",
      "general form\n",
      "T f(s) =\n",
      "Z\n",
      "D\n",
      "ψ(s, x)f(x) dx,\n",
      "(0.1.107)\n",
      "where the integral exists. We will denote a transform of the function f by the\n",
      "operator T as fT , that is,\n",
      "fT = T f.\n",
      "The dummy arguments of the pair of functions f and fT may range over\n",
      "diﬀerent domains, which may correspond to diﬀerent physical entities, such\n",
      "as time and frequency, for example.\n",
      "The notation for functions and their transforms requires a word of clar-\n",
      "iﬁcation. All three of the symbols f, T f, and fT represent functions. The\n",
      "corresponding notation in which the dummy arguments appear are the sym-\n",
      "bols f(x), T f(s), and fT (s). We may also write both dummy arguments, as\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "757\n",
      "in T (f(x))(s), in which x is the argument of the function f to which the trans-\n",
      "form is being applied, and s is the argument of the transform, the function\n",
      "T f.\n",
      "The linearity of the transform in equation (0.1.107) is clear:\n",
      "T (af + g) = aT f + T g,\n",
      "(0.1.108)\n",
      "where a is a constant, f and g are functions, and the transform is deﬁned over\n",
      "an appropriate domain. This relation is why it is a linear transform, and of\n",
      "course is a property of any inner product.\n",
      "There are several useful transforms that correspond to speciﬁc functions\n",
      "ψ(s, x) and domains D in equation (0.1.107). The question of the existence\n",
      "of the integral in equation (0.1.107) is of course important, and the choice of\n",
      "ψ(s, x) can determine the class of functions for which the transform is deﬁned.\n",
      "Often ψ(s, x) is chosen so that the integral exists for and f ∈L1.\n",
      "In the Fourier transform, ψ(s, x) = e2πisx, and the range of integration is\n",
      "the real line:\n",
      "Ff(s) =\n",
      "Z ∞\n",
      "−∞\n",
      "e2πisxf(x) dx.\n",
      "In this expression, i is the imaginary unit, √−1. We also write the Fourier\n",
      "transform of the function f as fF (s).\n",
      "A linear transform with ψ(s, x) ∝(esx)c for some c, such as the Fourier\n",
      "transform, the Laplace transform, and the characteristic function, satisﬁes the\n",
      "“change of scale property”:\n",
      "T (f(ax))(s) = 1\n",
      "|a|T (f(x))\n",
      "\u0010 s\n",
      "a\n",
      "\u0011\n",
      ",\n",
      "(0.1.109)\n",
      "where a is a constant. This is easily shown by making a change of variables\n",
      "in the deﬁnition (0.1.107). This change of variables is sometimes referred to\n",
      "as “time scaling”, because the argument of f often corresponds to a measure\n",
      "of time. A similar scaling applies to the argument of the transform fT , which\n",
      "is sometimes called “frequency scaling”.\n",
      "Transforms in which ψ(s, x) ∝(esx)c also have two useful translation prop-\n",
      "erties:\n",
      "•\n",
      "for a shift in the argument of f,\n",
      "T (f(x −x0))(s) = ψ(s, x0)T (f(x))(s) :\n",
      "(0.1.110)\n",
      "•\n",
      "for a shift in the argument of the transform T f,\n",
      "T (f(x))(s −s0) = T (ψ(−s0, x)f(x))(s).\n",
      "(0.1.111)\n",
      "These scaling and translation properties are major reasons for the usefulness\n",
      "of the Fourier and Laplace transforms and of the characteristic function in\n",
      "probability theory.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "758\n",
      "0 Statistical Mathematics\n",
      "Linear transforms apply to multivariate functions as well as to univari-\n",
      "ate functions. In the deﬁnition of linear transforms (0.1.107), both s and x\n",
      "may be vectors. In most cases s and x are vectors of the same order, and\n",
      "speciﬁc transforms have simple extensions. In the characteristic function of\n",
      "multivariate random variable, for example,\n",
      "ψ(s, x) = ei⟨s,x⟩.\n",
      "Fourier Transforms\n",
      "The Fourier transform of a function f(x) is the function\n",
      "Ff(s) =\n",
      "Z ∞\n",
      "−∞\n",
      "e2πisxf(x) dx,\n",
      "(0.1.112)\n",
      "if the integral exists.\n",
      "The inverse Fourier transform is\n",
      "f(x) =\n",
      "Z ∞\n",
      "−∞\n",
      "e−2πisxFf(s) ds.\n",
      "(0.1.113)\n",
      "Instead of e2πisx as in equation (0.1.112), the Fourier transform is often\n",
      "deﬁned with the function eiωx, in which ω is called the “angular frequency”.\n",
      "Fourier transforms are linear transforms, and thus enjoy the linearity prop-\n",
      "erty (0.1.108). Fourier transforms are inner products with a function of the\n",
      "form (esx)c, and thus enjoy the change of scale property (0.1.109), and the\n",
      "translation properties (0.1.110) and (0.1.111). Fourier transforms have addi-\n",
      "tional useful properties that derive from the identity\n",
      "exp(iωs) = cos(ωs) + i sin(ωs),\n",
      "in which the real component is an even function and the imaginary component\n",
      "is an odd function. Because of this, we immediately have the following:\n",
      "•\n",
      "if f(x) is even, then the Fourier transform is even\n",
      "Ff(−s) = Ff(s)\n",
      "•\n",
      "if f(x) is odd, then the Fourier transform is odd\n",
      "Ff(−s) = −Ff(s)\n",
      "•\n",
      "if f(x) is real, then\n",
      "Ff(−s) = Ff(s),\n",
      "where the overbar represents the complex conjugate.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "759\n",
      "Fourier transforms are useful in working with convolutions and correlations\n",
      "because of the following relationships, which follow immediately from the\n",
      "deﬁnition of convolutions (0.1.72) and of correlations (0.1.74):\n",
      "F(f ⋆g)(s) = Ff(s)Fg(s).\n",
      "(0.1.114)\n",
      "F(Cor(f, g))(s) = Ff(s)Fg(s).\n",
      "(0.1.115)\n",
      "F(Cor(f, f))(s) = |Ff(s)|2.\n",
      "(0.1.116)\n",
      "Equation (0.1.114) is sometimes called the “convolution theorem”. Some au-\n",
      "thors take this as the deﬁnition of the convolution of two functions. Equa-\n",
      "tion (0.1.115) is sometimes called the “correlation theorem”, and equa-\n",
      "tion (0.1.116), for the autocorrelation is sometimes called the “Wiener-\n",
      "Khinchin theorem”,\n",
      "These relationships are among the reasons that Fourier transforms are so\n",
      "useful in communications engineering. For a signal with amplitude h(t), the\n",
      "total power is the integral\n",
      "Z ∞\n",
      "−∞\n",
      "|h(t)|2dt.\n",
      "From the relations above, we have Parseval’s theorem, for the total power:\n",
      "Z ∞\n",
      "−∞\n",
      "|h(t)|2dt =\n",
      "Z ∞\n",
      "−∞\n",
      "|Fh(s)|2ds.\n",
      "(0.1.117)\n",
      "0.1.13 Functionals\n",
      "Functionals are functions whose arguments are functions. The value of a func-\n",
      "tional may be any kind of object, a real number or another function, for\n",
      "example. The domain of a functional is a set of functions.\n",
      "If F is a linear space of functions, that is, if F is such that f ∈F and\n",
      "g ∈F implies (af + g) ∈F for any real a, then the functional Υ deﬁned on\n",
      "F is said to be linear if Υ(af + g) = aΥ(f) + Υ(g).\n",
      "A similar expression deﬁnes linearity of a functional over a distribution\n",
      "function space P: Υ deﬁned on P is linear if Υ((1 −w)P1 + wP2) = (1 −\n",
      "w)Υ(P1) + wΥ(P2) for w ∈[0, 1] and P1, P2 ∈P.\n",
      "Functionals of CDFs have important uses in statistics as measures of the\n",
      "diﬀerences between two distributions or to deﬁne distributional measures of\n",
      "interest. A functional applied to a ECDF is a plug-in estimator of the distri-\n",
      "butional measure deﬁned by the same functional applied to the corresponding\n",
      "CDF.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "760\n",
      "0 Statistical Mathematics\n",
      "Derivatives of Functionals\n",
      "For the case in which the arguments are functions, the cardinality of the\n",
      "possible perturbations is greater than that of the continuum. We can be precise\n",
      "in discussions of continuity and diﬀerentiability of a functional Υ at a point\n",
      "(function) F in a domain F by deﬁning another set D consisting of diﬀerence\n",
      "functions over F; that is the set the functions D = F1 −F2 for F1, F2 ∈F.\n",
      "The concept of diﬀerentiability for functionals is necessarily more com-\n",
      "plicated than for functions over real domains. For a functional Υ over the\n",
      "domain F, we deﬁne three levels of diﬀerentiability at the function F ∈F.\n",
      "All deﬁnitions are in terms of a domain D of diﬀerence functions over F, and\n",
      "a linear functional ΛF deﬁned over D in a neighborhood of F . The ﬁrst type\n",
      "of derivative is very general. The other two types depend on a metric ρ on\n",
      "F × F induced by a norm ∥· ∥on F.\n",
      "Deﬁnition 0.1.55 (Gˆateaux diﬀerentiable)\n",
      "Υ is Gˆateaux diﬀerentiable at F iﬀthere exists a linear functional ΛF (D) over\n",
      "D such that for t ∈IR for which F + tD ∈F,\n",
      "lim\n",
      "t→0\n",
      "\u0012Υ(F + tD) −Υ(F )\n",
      "t\n",
      "−ΛF(D)\n",
      "\u0013\n",
      "= 0.\n",
      "(0.1.118)\n",
      "In this case, the linear functional ΛF is called the Gˆateaux diﬀerential of\n",
      "Υ at F in the direction of F + D.\n",
      "Deﬁnition 0.1.56 (ρ-Hadamard diﬀerentiable)\n",
      "For a metric ρ induced by a norm, Υ is ρ-Hadamard diﬀerentiable at F iﬀ\n",
      "there exists a linear functional ΛF(D) over D such that for any sequence\n",
      "tj →0 ∈IR and sequence Dj ∈D such that ρ(Dj, D) →0 and F + tjDj ∈F,\n",
      "lim\n",
      "j→∞\n",
      "\u0012Υ(F + tjDj) −Υ(F )\n",
      "tj\n",
      "−ΛF(Dj)\n",
      "\u0013\n",
      "= 0.\n",
      "(0.1.119)\n",
      "In this case, the linear functional ΛF is called the ρ-Hadamard diﬀerential\n",
      "of Υ at F .\n",
      "Deﬁnition 0.1.57 (ρ-Fr´echet diﬀerentiable)\n",
      "Υ is ρ-Fr´echet diﬀerentiable at F iﬀthere exists a linear functional Λ(D) over\n",
      "D such that for any sequence Fj ∈F for which ρ(Fj, F ) →0,\n",
      "lim\n",
      "j→∞\n",
      "\u0012Υ(Fj) −Υ(F ) −ΛF(Fj −F )\n",
      "ρ(Fj, F )\n",
      "\u0013\n",
      "= 0.\n",
      "(0.1.120)\n",
      "In this case, the linear functional ΛF is called the ρ-Fr´echet diﬀerential of\n",
      "Υ at F .\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "761\n",
      "Derivative Expansions of Functionals\n",
      "*********************\n",
      "Notes and References for Section 0.1\n",
      "After the introductory material in Section 0.0, in Section 0.1 I try to cover\n",
      "the important aspects of real analysis (which means “measure theory”) for\n",
      "statistical mathematics.\n",
      "Measure theory is the most important element of analysis for probabil-\n",
      "ity theory and mathematical statistics. In measure theory, we are concerned\n",
      "with collections of subsets, and we identify particular systems of collections of\n",
      "subsets. These systems are called “rings” (Deﬁnition 0.1.2) or “ﬁelds” (Def-\n",
      "inition 0.1.3). (The reader should also be aware that these terms are often\n",
      "used diﬀerently in algebra. The term “ring” also applies to a mathematical\n",
      "structure consisting of a set and two operations on the set satisfying certain\n",
      "properties. The prototypic ring is the set of integers with ordinary addition\n",
      "and multiplication. The term “ﬁeld” as in Deﬁnition 0.0.3 also applies to a\n",
      "mathematical structure consisting of a set and two operations on the set sat-\n",
      "isfying certain properties. The prototypic ﬁeld is the set of real numbers with\n",
      "ordinary addition and multiplication.)\n",
      "“Littlewood’s three principles of real analysis” are heuristics that state that\n",
      "if sets, functions, or series have certain properties, then stronger properties\n",
      "“almost” hold. The third principle, which is illustrated by the results of the\n",
      "Severini-Egorov theorem and the monotone convergence theorem, states that\n",
      "every convergent sequence of functions is “nearly” uniformly convergent. The\n",
      "ﬁrst principle states that a measurable set is “almost” an open set. In IR,\n",
      "this is the statement that for a measurable subset T and any ϵ > 0 there is\n",
      "a sequence of open intervals, On such that λ(T∆(∪On)) < ϵ, where λ is the\n",
      "Lebesgue measure.\n",
      "Littlewood’s second principle states that a measurable function is almost\n",
      "a continuous function. In IR, this is the statement that for a measurable real\n",
      "function f and any ϵ > 0 there is an open subset of IR, say S, such that f is\n",
      "continuous outside of S, and λ(S) < ϵ.\n",
      "The concept of an integral is one of the most important ones in mathe-\n",
      "matics. The deﬁnition of an integral based on Jordan measure by Bernhard\n",
      "Riemann in the mid-nineteenth century was rigorous and seemed to cover most\n",
      "interesting cases. By 1900, however, a number of examples had been put forth\n",
      "that indicated the inadequacy of the Riemann integral (see Hawkins (1979),\n",
      "for an interesting discussion of the mathematical developments). Lebesgue\n",
      "not only provided generalizations of basic concepts, such as what we now\n",
      "call Lebesgue measure, but took a fundamentally diﬀerent approach. (It is\n",
      "interesting to read what Lebesgue had to say about generalizations: “It is\n",
      "that a generalization made not for the vain pleasure of generalizing but in\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "762\n",
      "0 Statistical Mathematics\n",
      "order to solve previously existing problems is always a fruitful generalization”\n",
      "(Lebesgue (1926), page 194 as translated by May, 1966,).\n",
      "There are many classic and standard texts on real analysis, and it would\n",
      "be diﬃcult to select “best” ones. Many treat measure theory in the context\n",
      "of probability theory, and some of those are listed in the additional references\n",
      "for Chapter 1, beginning on page 145. Below I list a few more that I have\n",
      "found useful. I often refer to Hewitt and Stromberg (1965), from which I ﬁrst\n",
      "began learning real analysis. Royden (1988) may be more readily available,\n",
      "however. The little book by Boas Jr. (1960) is a delightful read.\n",
      "My study of complex analysis has been more superﬁcial, and I am not\n",
      "familiar with the standard texts. The text Brown and Churchill (2008) is an\n",
      "updated version of the second edition by Churchill (1960) that I used.\n",
      "There are a number of useful books on “ counterexamples in [X]”, such as\n",
      "Gelbaum and Olmsted (1990), Gelbaum and Olmsted (2003), Rajwade and Bhandari\n",
      "(2007), Steen and Seebach Jr. (1995), Stoyanov (1987), Wise and Hall (1993),\n",
      "and Romano and Siegel (1986).\n",
      "Exercises for Section 0.1\n",
      "0.1.1. Let Ωbe the universal set, and let F consist of all countable and cocount-\n",
      "able subsets of Ω. Show that F is a σ-ﬁeld on Ω(Example 0.1.3).\n",
      "0.1.2. Prove Theorem 0.1.4.\n",
      "0.1.3. Show that F1 ∪F2 in equation (0.1.2) is not a σ-ﬁeld.\n",
      "0.1.4. Show that FB in equation (0.1.3) is a σ-ﬁeld.\n",
      "0.1.5. Let f be a measurable function from the measurable space (Ω, F) to (Λ, G).\n",
      "Show that f−1[G] is a sub-σ-ﬁeld of F.\n",
      "0.1.6. Show that σ(g ◦f) ⊆σ(f) (page 704).\n",
      "0.1.7. Prove Theorem 0.1.6.\n",
      "0.1.8. Prove Theorem 0.1.8.\n",
      "0.1.9. Prove Theorem 0.1.9.\n",
      "0.1.10. Suppose that µ1, µ2, . . . are measures on the measurable space (Ω, F). Let\n",
      "{an}∞\n",
      "n=1 be a sequence of positive numbers. Prove that µ = P∞\n",
      "n=1 anµn\n",
      "is a measure on (Ω, F).\n",
      "0.1.11. Show that the function deﬁned in equation (0.1.12) is a Radon measure.\n",
      "0.1.12. Let Ωand Λ be arbitrary sets and let X : Λ 7→Ωbe an arbitrary function.\n",
      "a) Show that if F is a σ-ﬁeld on Ωthen G = {X−1(A) : A ∈F} is a\n",
      "σ-ﬁeld on Λ\n",
      "b) Show that if G is a σ-ﬁeld on Λ then F = {A ⊆Ω: X−1(A) ∈G} is\n",
      "a σ-ﬁeld on Ω.\n",
      "0.1.13. Let λ, µ, and ν be measures on (Ω, F) and a ∈IR. Show\n",
      "λ ≪ν\n",
      "and\n",
      "µ ≪ν =⇒(aλ + µ) ≪ν\n",
      "and\n",
      "λ ⊥ν\n",
      "and\n",
      "µ ⊥ν =⇒(aλ + µ) ⊥ν.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.1 Measure, Integration, and Functional Analysis\n",
      "763\n",
      "0.1.14. Prove parts (ii) and (iii) of Theorem 0.1.12.\n",
      "0.1.15. Show that the same Borel ﬁeld B(IR) is generated by the collection of all\n",
      "open sets of IR.\n",
      "0.1.16. Show that the inverse image of a Borel set under a continuous function\n",
      "f : IR 7→IR is Borel.\n",
      "0.1.17. Let (Ω, F, ν) be a measure space, and let B ∈F. Now let νB be a set\n",
      "function that is the same as ν on FB and undeﬁned elsewhere. Show that\n",
      "νB is a measure.\n",
      "0.1.18. Given the measure space (Ω, F, ν) and Fc and νc constructed as on\n",
      "page 712.\n",
      "a) Show that (Ω, Fc, νc) is a complete measure space.\n",
      "b) Show that for every A ∈Fc there is some B, C ∈F with ν(C) = 0\n",
      "such that A = B ∪C, and\n",
      "νc(A) = ν(B).\n",
      "0.1.19. Given the measure space (Ω, F, ν) and the measurable space (Λ, G). Let\n",
      "f be a function from Ωto Λ that is measurable with respect to F. Show\n",
      "that ν ◦f−1 is a measure and that its domain and range are G. This is\n",
      "the induced measure or the “pushforward” measure.\n",
      "0.1.20. Let X be a measurable function from the measurable space (Ω, F) to\n",
      "(IR, B). Prove that σ(X−1[IR]) ⊆F.\n",
      "0.1.21. Show that (IR, B) is a topological space. What are the open sets of the\n",
      "topology?\n",
      "0.1.22.\n",
      "Show that Lebesgue measure on (IR, B) is a Radon measure.\n",
      "Show that Lebesgue measure on (IR, B) is a Haar invariant measure wrt\n",
      "to the group (IR, +).\n",
      "0.1.23. Show that equation (0.1.22) holds for Lebesgue measure.\n",
      "0.1.24. Let C[0,1] be the collection of all open intervals within [0, 1]. Show that\n",
      "B[0,1] = σ(C[0,1]).\n",
      "0.1.25. Under what conditions is the indicator function IS measurable?\n",
      "0.1.26. Show that a simple function is Borel measurable.\n",
      "0.1.27. ****continuity questions\n",
      "0.1.28. Show that the Weierstrass function is H¨older continuous of order α for\n",
      "any α < 1.\n",
      "0.1.29. Let (Ω, F, ν) be a measure space and f be a nonnegative Borel function.\n",
      "For A ⊆Ω, show that λ(A) =\n",
      "R\n",
      "A f dν is a measure over (Ω, F).\n",
      "0.1.30. Show that the measure deﬁned in equation (0.1.48) is Haar invariant.\n",
      "0.1.31. Prove Theorem 0.1.19.\n",
      "0.1.32. In the text, we say that the proofs of Theorems 0.1.20 through 0.1.22\n",
      "“follow immediately from the deﬁnition” of the Lebesgue integral. “Im-\n",
      "mediately” means that there are one or two reasons that are direct results\n",
      "of the deﬁnition. For each of these theorems, state the reason(s).\n",
      "0.1.33. Prove each of the equations (0.1.66) through (0.1.70) under the conditions\n",
      "given.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "764\n",
      "0 Statistical Mathematics\n",
      "0.1.34. Assume f2 and g2 are integrable. Show that\n",
      "\u0012Z\n",
      "fgdν\n",
      "\u00132\n",
      "≤\n",
      "Z\n",
      "f2dν\n",
      "Z\n",
      "g2dν.\n",
      "This is an instance of a famous inequality. What is its name?\n",
      "0.1.35. Show that the class Ck of functions over the same domain is a linear space.\n",
      "0.1.36. Show that Lp is a linear space.\n",
      "0.1.37. Show that if f, f1, f2, . . . ∈Lp(ν, D) and ν(D) < ∞, then\n",
      "fn\n",
      "Lp\n",
      "→f ⇒fn\n",
      "Lr\n",
      "→f\n",
      "for r ≤p.\n",
      "0.1.38. Prove Theorem 0.1.27.\n",
      "0.1.39. Prove Theorem 0.1.28.\n",
      "0.1.40. Prove Theorem 0.1.29.\n",
      "Hint: First prove this for the case that g is the Heaviside function.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.2 Stochastic Processes and the Stochastic Calculus\n",
      "765\n",
      "0.2 Stochastic Processes and the Stochastic Calculus\n",
      "Most sections in this chapter generally cover prerequisite material for the rest\n",
      "of the book. This section, on the other hand, depends on some of the material\n",
      "in Chapter 1, and is closely interrelated with the material in Section 1.6.\n",
      "0.2.1 Stochastic Diﬀerential Equations\n",
      "We consider a stochastic process {Bt} in which we generally associate the\n",
      "index t with time. We often write {B(t)} in place of {Bt}, but for all prac-\n",
      "tical purposes, the notation is equivalent. If time is considered continuous,\n",
      "there are two possibilities that we will consider. One, called a jump process,\n",
      "is discontinuous in time, and the other, called a diﬀusion process, is not only\n",
      "continuous in time, but it is also diﬀerential with respect to time.\n",
      "We will ﬁrst brieﬂy discuss a particular kind of jump process and then\n",
      "turn our attention to various kinds of diﬀusion processes.\n",
      "The most obvious way of developing a diﬀusion process is to begin with\n",
      "a diﬀerential equation in which some of the terms are random variables. We\n",
      "call such a diﬀerential equation a stochastic diﬀerential equation or SDE.\n",
      "In a very important class of stochastic processes, the diﬀerences between\n",
      "the values at two time points have normal distributions and the diﬀerence\n",
      "between two points is independent of the diﬀerence between two nonoverlap-\n",
      "ping points. The simplest such process is called a Bachelier-Wiener process.\n",
      "We will discuss it ﬁrst, and then consider some other related processes.\n",
      "Poisson Jump Process\n",
      "A jump process is one that is discontinuous in time. The most important jump\n",
      "processes are Poisson processes.\n",
      "A Poisson process is a sequence of events in which the probability of k\n",
      "events (where k = 0, 1, . . .) in an interval of length ∆t, denoted by g(k, ∆t)\n",
      "satisﬁes the following conditions:\n",
      "•\n",
      "g(1, ∆t) = λ∆t + o(∆t), where λ is a positive constant and (∆t) > 0.\n",
      "•\n",
      "P∞\n",
      "k=2 g(k, ∆t) ∈o(∆t).\n",
      "•\n",
      "The numbers of changes in nonoverlapping intervals are stochastically in-\n",
      "dependent.\n",
      "This axiomatic characterization of a Poisson process leads to a diﬀerential\n",
      "equation whose solution (using mathematical induction) is\n",
      "g(k, ∆t) = (λ∆t)ke−λ∆t\n",
      "k!\n",
      ",\n",
      "for k = 1, 2, . . .\n",
      "(0.2.1)\n",
      "which, in turn leads to the familiar probability function for a Poisson distri-\n",
      "bution\n",
      "pK(k) = (θ)ke−θ\n",
      "k!\n",
      ",\n",
      "for k = 0, 1, 2, . . .\n",
      "(0.2.2)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "766\n",
      "0 Statistical Mathematics\n",
      "Bachelier-Wiener Processes\n",
      "Suppose in the sequence B0, B1, . . ., the distribution of Bt+1 −Bt is nor-\n",
      "mal with mean 0 and standard deviation 1. In this case, the distribution of\n",
      "Bt+2 −Bt is normal with mean 0 and standard deviation\n",
      "√\n",
      "2, and the distri-\n",
      "bution of Bt+0.5 −Bt is normal with mean 0 and standard deviation\n",
      "√\n",
      "0.5.\n",
      "More generally, the distribution of the change ∆B in time ∆t has a standard\n",
      "deviation of\n",
      "√\n",
      "∆t\n",
      "This kind of process with the Markovian property and with a normal\n",
      "distribution of the changes leads to a Brownian motion or a Bachelier-Wiener\n",
      "process.\n",
      "Consider a process of changes ∆B characterized by two properties:\n",
      "•\n",
      "The change ∆B during a small period of time ∆t is given by\n",
      "∆B = Z\n",
      "√\n",
      "∆t,\n",
      "(0.2.3)\n",
      "where Z is a random variable with a N(0, 1) distribution.\n",
      "•\n",
      "The values of ∆B for any two short intervals of time ∆t are independent\n",
      "(with an appropriate deﬁnition of “short”).\n",
      "Now, consider N time periods, and let T = N∆t. We have\n",
      "B(T) −B(0) =\n",
      "N\n",
      "X\n",
      "i=1\n",
      "Zi\n",
      "√\n",
      "∆t.\n",
      "(0.2.4)\n",
      "The fact that we have\n",
      "√\n",
      "∆t in this equation has important implications.\n",
      "As in ordinary calculus, we consider ∆B/∆t and take the limit as ∆t →0,\n",
      "which we call dB/dt, and we have the stochastic diﬀerential equation\n",
      "dB = Zdt.\n",
      "(0.2.5)\n",
      "A random variable formed as dB above is called a stochastic diﬀerential.\n",
      "A stochastic diﬀerential arising from a process of changes ∆B with the two\n",
      "properties above is called a Bachelier-Wiener process or a Brownian motion.\n",
      "In the following, we will generally use the phrase “Bachelier-Wiener process”.\n",
      "We can use the Bachelier-Wiener process to develop a generalized Bachelier-\n",
      "Wiener process:\n",
      "dS = µdt + σdB,\n",
      "(0.2.6)\n",
      "where µ and σ are constants.\n",
      "Properties of a Discrete Process Underlying the Bachelier-Wiener\n",
      "Process\n",
      "With ∆B = Z\n",
      "√\n",
      "∆t and Z ∼N(0, 1), we immediately have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.2 Stochastic Processes and the Stochastic Calculus\n",
      "767\n",
      "E(∆B) = 0\n",
      "E\n",
      "\u0000(∆B)2\u0001\n",
      "= V(∆B) + (E(∆B))2\n",
      "= ∆t\n",
      "E \u0000(∆B)3\u0001 = 0\n",
      "E\n",
      "\u0000(∆B)4\u0001\n",
      "= V\n",
      "\u0000(∆B)2\u0001\n",
      "+\n",
      "\u0000E\n",
      "\u0000(∆B)2\u0001\u00012\n",
      "= 3(∆t)2\n",
      "Because of independence, for ∆iB and ∆jB representing changes in two\n",
      "nonoverlapping intervals of time,\n",
      "E((∆iB)(∆jB)) = cov(∆iB, ∆jB) = 0.\n",
      "(0.2.7)\n",
      "The Bachelier-Wiener process is a random variable; that is, it is a real-\n",
      "valued mapping from a sample space Ω. We sometimes use the notation B(ω)\n",
      "to emphasize this fact.\n",
      "The Bachelier-Wiener process is a function in continuous time. We some-\n",
      "times use the notation B(t, ω) to emphasize the time dependency.\n",
      "Most of the time we drop the “ω”. Also, sometimes we write Bt instead of\n",
      "B(t).\n",
      "All of these notations are equivalent.\n",
      "There two additional properties of a Bachelier-Wiener process or Brownian\n",
      "motion that we need in order to have a useful model. We need an initial value,\n",
      "and we need it to be continuous in time.\n",
      "Because the Bachelier-Wiener process is a random variable, the values it\n",
      "takes are those of a function at some point in the underlying sample space,\n",
      "Ω. Therefore, when we speak of B(t) at some t, we must speak in terms of\n",
      "probabilities of values or ranges of values.\n",
      "When we speak of a particular value of B(t), unless we specify a speciﬁc\n",
      "point ω0 ∈Ω, the most we can say is that the value occurs almost surely.\n",
      "•\n",
      "We assume B(t) = 0 almost surely at t = 0.\n",
      "•\n",
      "We assume B(t) is almost surely continuous in t.\n",
      "These two properties together with the limiting forms of the two properties\n",
      "given at the beginning deﬁne a Bachelier-Wiener process or Brownian motion.\n",
      "(There is a theorem due to Kolmogorov that states that given the ﬁrst\n",
      "three properties, there exists a “version” that is absolutely continuous in t.)\n",
      "From the deﬁnition, we can see immediately that\n",
      "•\n",
      "the Bachelier-Wiener process is Markovian\n",
      "•\n",
      "the Bachelier-Wiener process is a martingale.\n",
      "Generalized Bachelier-Wiener Processes\n",
      "A Bachelier-Wiener process or Brownian motion is a model for changes. It\n",
      "models diﬀusion.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "768\n",
      "0 Statistical Mathematics\n",
      "If the process drifts over time (in a constant manner), we can add a term\n",
      "for the drift, adt.\n",
      "More generally, a model for the state of a process that has both a Brownian\n",
      "diﬀusion and a drift is a generalized Bachelier-Wiener process:\n",
      "dS = adt + bdB,\n",
      "(0.2.8)\n",
      "where a and b are constants. A generalized Bachelier-Wiener process is a type\n",
      "of a more general “drift-diﬀusion process”.\n",
      "While the expected value of the Bachelier-Wiener process at any time is 0,\n",
      "the expected value of the state S is not necessarily 0. Likewise, the variance\n",
      "is aﬀected by b. Both the expected value and the variance of S are functions\n",
      "of time.\n",
      "One of the most interesting properties of a Bachelier-Wiener process is\n",
      "that its ﬁrst variation is inﬁnite. It is inﬁnitely “wiggly”. We can see this\n",
      "by generating normal processes over varying length time intervals, as in Fig-\n",
      "ure 0.2.\n",
      "1\n",
      "21\n",
      "41\n",
      "61\n",
      "81\n",
      "101\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "t\n",
      "W(t)\n",
      "1\n",
      "21\n",
      "41\n",
      "61\n",
      "81\n",
      "101\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "t\n",
      "W(t)\n",
      "1\n",
      "21\n",
      "41\n",
      "61\n",
      "81\n",
      "101\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "t\n",
      "W(t)\n",
      "Figure 0.2.\n",
      "A Bachelier-Wiener Process Observed at Varying Length Intervals\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.2 Stochastic Processes and the Stochastic Calculus\n",
      "769\n",
      "Variation of Functionals\n",
      "The variation of a functional is a measure of its rate of change. It is similar\n",
      "in concept to an integral of a derivative of a function.\n",
      "For studying variation, we will be interested only in functions from the\n",
      "interval [0, T] to IR.\n",
      "To deﬁne the variation of a general function f : [0, T] 7→IR, we form N\n",
      "intervals 0 = t0 ≤t1 ≤· · · ≤tN = T. The intervals are not necessarily of\n",
      "equal length, so we deﬁne ∆as the maximum length of any interval; that is,\n",
      "∆= max(ti −ti−1).\n",
      "Now, we denote the pth variation of f as Vp(f) and deﬁne it as\n",
      "Vp(f) = lim\n",
      "∆→0\n",
      "N\n",
      "X\n",
      "i=1\n",
      "|f(ti) −f(ti−1)| p.\n",
      "(Notice that ∆→0 implies N →∞.)\n",
      "With equal intervals, ∆t, for the ﬁrst variation, we can write\n",
      "V1(f) = lim\n",
      "∆t→0\n",
      "N\n",
      "X\n",
      "i=1\n",
      "|f(ti) −f(ti−1)|\n",
      "= lim\n",
      "N→∞\n",
      "N−1\n",
      "X\n",
      "i=0\n",
      "∆t|f(ti + ∆t) −f(ti)|\n",
      "∆t\n",
      ",\n",
      "from which we can see that for a diﬀerentiable function f : [0, T] 7→IR,\n",
      "V1(f) =\n",
      "Z T\n",
      "0\n",
      "\f\f\f\f\n",
      "df\n",
      "dt\n",
      "\f\f\f\f dt.\n",
      "The notation F V (f), or more properly, FV(f), is sometimes used instead of\n",
      "V1(f).\n",
      "Again, with equal intervals, ∆t, for the second variation, we can write\n",
      "V2(f) = lim\n",
      "∆t→0\n",
      "N\n",
      "X\n",
      "i=1\n",
      "(f(ti) −f(ti−1))2\n",
      "= lim\n",
      "∆t→0 ∆t lim\n",
      "N→∞\n",
      "N−1\n",
      "X\n",
      "i=0\n",
      "∆t\n",
      "\u0012|f(ti + ∆t) −f(ti)|\n",
      "∆t\n",
      "\u00132\n",
      ".\n",
      "For a diﬀerentiable function f : [0, T] 7→IR, we have\n",
      "V2(f) = lim\n",
      "∆t→0 ∆t\n",
      "Z T\n",
      "0\n",
      "\f\f\f\f\n",
      "df\n",
      "dt\n",
      "\f\f\f\f\n",
      "2\n",
      "dt.\n",
      "The integrand is bounded, therefore this limit is 0, and we conclude that\n",
      "the second variation of a diﬀerentiable function is 0.\n",
      "If X is a stochastic functional, then Vp(X) is also stochastic. If it converges\n",
      "to a deterministic quantity, the nature of the convergence must be considered.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "770\n",
      "0 Statistical Mathematics\n",
      "First and Second Variation of a Bachelier-Wiener Process\n",
      "Two important properties of a Bachelier-Wiener process on [0, T] are\n",
      "•\n",
      "V2(B) = T a.s., which as we have seen, implies that B(t) is not diﬀeren-\n",
      "tiable.\n",
      "•\n",
      "V1(B) = ∞a.s.\n",
      "Notice that because B is a random variable we must temper our statement\n",
      "with a phrase about the probability or expected value.\n",
      "We now prove these for the quadratic mean instead of a.s. We start with\n",
      "the ﬁrst one, because it will imply the second one. Let\n",
      "XN =\n",
      "N−1\n",
      "X\n",
      "n=0\n",
      "(B(tn+1) −B(tn))2\n",
      "=\n",
      "N−1\n",
      "X\n",
      "n=0\n",
      "(∆nB)2\n",
      "note notation\n",
      "We want to show\n",
      "E \u0000(XN −T)2\u0001 →0\n",
      "as |∆t| →0.\n",
      "(0.2.9)\n",
      "Now,\n",
      "E\n",
      "\u0000(XN −T)2\u0001\n",
      "= E\n",
      "\u0000X2\n",
      "N\n",
      "\u0001\n",
      "−2TE(XN) + T 2 = E\n",
      "\u0000X2\n",
      "N\n",
      "\u0001\n",
      "−T 2.\n",
      "So now we want to show\n",
      "E \u0000X2\n",
      "N\n",
      "\u0001 = T 2.\n",
      "(0.2.10)\n",
      "E\n",
      "\u0000X2\n",
      "N\n",
      "\u0001\n",
      "= E\n",
      "\n",
      "\n",
      "N−1\n",
      "X\n",
      "i=0\n",
      "(∆iB)2\n",
      "N−1\n",
      "X\n",
      "j=0\n",
      "(∆jB)2\n",
      "\n",
      "\n",
      "= E\n",
      " N−1\n",
      "X\n",
      "i=0\n",
      "(∆iB)4\n",
      "!\n",
      "+ E\n",
      "\n",
      "X\n",
      "i̸=j\n",
      "(∆iB)2(∆jB)2\n",
      "\n",
      "\n",
      "=\n",
      "N−1\n",
      "X\n",
      "i=0\n",
      "(∆it)2 +\n",
      "X\n",
      "i̸=j\n",
      "(∆it)(∆jt).\n",
      "Because |∆t| →0 (or, if we allow diﬀerent size intervals, sup |∆it| →0),\n",
      "we have\n",
      "N−1\n",
      "X\n",
      "i=0\n",
      "(∆it)2 →0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.2 Stochastic Processes and the Stochastic Calculus\n",
      "771\n",
      "So the ﬁrst term goes to 0; now consider P\n",
      "i̸=j(∆it)(∆jt).\n",
      "X\n",
      "i̸=j\n",
      "(∆it)(∆jt) =\n",
      "N−1\n",
      "X\n",
      "i=0\n",
      "(∆it)\n",
      "\n",
      "\n",
      "i−1\n",
      "X\n",
      "j=0\n",
      "(∆jt) +\n",
      "N−1\n",
      "X\n",
      "j=i+1\n",
      "(∆jt)\n",
      "\n",
      "\n",
      "=\n",
      "N−1\n",
      "X\n",
      "i=0\n",
      "(∆it)(T −∆it)\n",
      "= T\n",
      "N−1\n",
      "X\n",
      "i=0\n",
      "(∆it) −\n",
      "N−1\n",
      "X\n",
      "i=0\n",
      "(∆it)2\n",
      "= T 2 −0.\n",
      "So now we have E \u0000(XN −T)2\u0001 →0, or XN\n",
      "L2\n",
      "→T\n",
      "as |∆t| →0; that is,\n",
      "V2(B) = T in quadratic mean, or in L2 norm.\n",
      "(I just realized that I had stated a.s. convergence, and I proved L2 conver-\n",
      "gence. One does not imply the other, but a.s. is also true in this case.)\n",
      "Now, although we have already seen that since the second variation is\n",
      "nonzero, B cannot be diﬀerentiable.\n",
      "But also because of the continuity of B in t, it is easy to see that the ﬁrst\n",
      "variation diverges if the second variation converges to a ﬁnite value. This is\n",
      "because\n",
      "N−1\n",
      "X\n",
      "n=0\n",
      "(B(tn+1) −B(tn))2 ≤sup |B(tn+1) −B(tn)|\n",
      "N−1\n",
      "X\n",
      "n=0\n",
      "|B(tn+1) −B(tn)|\n",
      "In the limit the term on the left is T > 0, and the term on the right is 0\n",
      "times V1(B); therefore V1(B) = ∞.\n",
      "Properties of Stochastic Diﬀerentials\n",
      "Although B and dB are random variables, the product dBdB is deterministic.\n",
      "We can see this by considering the stochastic process (∆B)2. We have seen\n",
      "that V\n",
      "\u0000(∆B)2\u0001\n",
      "= 2(∆t)2, so the variance of this process is 2(∆t)2; that is,\n",
      "as ∆t →0, the variance of this process goes to 0 faster, as (∆t)2.\n",
      "Also, as we have seen, E \u0000(∆B)2\u0001 = ∆t, and so (∆B)2 goes to ∆t at the\n",
      "same rate as ∆t →0. That is,\n",
      "(∆B)(∆B) a.s.\n",
      "→∆t\n",
      "as ∆t →0.\n",
      "(0.2.11)\n",
      "The convergence of (∆B)(∆B) to ∆t as ∆t →0 yields\n",
      "dBdB = dt.\n",
      "(0.2.12)\n",
      "(This equality is almost sure.) But dt is a deterministic quantity.\n",
      "This is one of the most remarkable facts about a Bachelier-Wiener process.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "772\n",
      "0 Statistical Mathematics\n",
      "Multidimensional Bachelier-Wiener Processes\n",
      "If we have two Bachelier-Wiener processes B1 and B2, with V(dB1) =\n",
      "V(dB2) = dt and cov(dB1, dB2) = ρdt (that is, Cor(dB1, dB2) = ρ), then\n",
      "by a similar argument as before, we have dB1dB2 = ρdt, almost surely.\n",
      "Again, this is deterministic.\n",
      "The results of course extend to any vector of Bachelier-Wiener processes\n",
      "(B1, . . ., Bd).\n",
      "If (B1, . . ., Bd) arise from\n",
      "∆Bi = Xi\n",
      "√\n",
      "∆t,\n",
      "where the vector of Xs has a multivariate normal distribution with mean\n",
      "0 and variance-covariance matrix Σ, then the variance-covariance matrix of\n",
      "(dB1, . . ., dBd) is Σdt, which is deterministic.\n",
      "Starting with (Z1, . . ., Zd\n",
      "iid\n",
      "∼N(0, 1) and forming the Wiener processes\n",
      "B = (B1, . . ., Bd) beginning with\n",
      "∆Bi = Zi\n",
      "√\n",
      "∆t,\n",
      "we can form a vector of Bachelier-Wiener processes B = (B1, . . ., Bd) with\n",
      "variance-covariance matrix Σdt for dB = (dB1, . . ., dBd) by the transforma-\n",
      "tion\n",
      "B = Σ1/2B,\n",
      "or equivalently by\n",
      "B = ΣCB,\n",
      "where ΣC is a Cholesky factor of Σ, that is, ΣT\n",
      "CΣC = Σ.\n",
      "Recall, for a ﬁxed matrix A,\n",
      "V(AY ) = ATV(Y )A,\n",
      "so from above, for example,\n",
      "V(dB) = ΣT\n",
      "CV(dB)ΣC = ΣT\n",
      "Cdiag(dt)ΣC = Σdt.\n",
      "(0.2.13)\n",
      "The stochastic diﬀerentials such as dB naturally lead us to consider inte-\n",
      "gration with respect to stochastic diﬀerentials, that is, stochastic integrals.\n",
      "Stochastic Integrals with Respect to Bachelier-Wiener Processes\n",
      "If B is a Bachelier-Wiener process on [0, T], we may be interested in an integral\n",
      "of the form\n",
      "Z T\n",
      "0\n",
      "g(Y (t), t)dB,\n",
      "where Y (t) is a stochastic process (that is, Y is a random variable) and g\n",
      "is some function. First, however, we must develop a deﬁnition of such an\n",
      "integral. We will return to this problem in Section 0.2.2. Before doing that,\n",
      "let us consider some generalizations of the Wiener process.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.2 Stochastic Processes and the Stochastic Calculus\n",
      "773\n",
      "Ito Processes\n",
      "An Ito process is a generalized Bachelier-Wiener process dX = adt + bdB, in\n",
      "which the parameters a and b are functions of the underlying variable X and\n",
      "of time t (of course, X is also a function of t).\n",
      "The functions a and b must be measurable with respect to the ﬁltration\n",
      "generated by B(t) (that is, to the sequence of smallest σ-ﬁelds with respect to\n",
      "which B(t) is measurable. (This is expressed more simply by saying a(X(t), t)\n",
      "and b(X(t), t) are adapted to the ﬁltration generated by B(t).)\n",
      "The Ito process is of the form\n",
      "dX(t) = a(X(t), t)dt + b(X(t), t)dB.\n",
      "(0.2.14)\n",
      "The Ito integral (or any other stochastic integral) gives us a solution to\n",
      "this stochastic diﬀerential equation:\n",
      "X(T) = X(0) +\n",
      "Z T\n",
      "0\n",
      "a(X(t), t)dt +\n",
      "Z T\n",
      "0\n",
      "b(X(t), t)dB(t).\n",
      "(0.2.15)\n",
      "(The diﬀerential in the ﬁrst integral is deterministic although the integrand\n",
      "is stochastic. The second integral, however, is a stochastic integral. Other\n",
      "deﬁnitions of this integral would require modiﬁcations in the interpretation of\n",
      "properties of the Ito process.)\n",
      "We are often interested in multidimensional Ito processes. Their second-\n",
      "order properties (variances and covariances) behave very similarly to those of\n",
      "Bachelier-Wiener processes, which we discussed earlier.\n",
      "There are many interesting forms of Ito processes.\n",
      "Geometric Brownian Motion\n",
      "The Ito process would be much easier to work with if µ(·) and σ(·) did not\n",
      "depend on the value of the state; that is, if we we use the model\n",
      "dX(t)\n",
      "X(t) = µ(t)dt + σ(t)dB.\n",
      "(0.2.16)\n",
      "The Ito process would be even easier to work with if µ(·) and σ(·) were\n",
      "constant; that is, if we we just use the model\n",
      "dX(t)\n",
      "X(t) = µdt + σdB.\n",
      "(0.2.17)\n",
      "This model is called a geometric Brownian motion, and is widely used in\n",
      "modeling prices of various ﬁnancial assets. (“Geometric” refers to series with\n",
      "multiplicative changes, as opposed to “arithmetic series” that have additive\n",
      "changes).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "774\n",
      "0 Statistical Mathematics\n",
      "The geometric Brownian motion model is similar to other common statis-\n",
      "tical models:\n",
      "dX(t)\n",
      "X(t)\n",
      "=\n",
      "µdt\n",
      "+\n",
      "σdB(t)\n",
      "or\n",
      "response = systematic component + random error.\n",
      "Without the stochastic component, the diﬀerential equation has the simple\n",
      "solution\n",
      "X(t) = ceµt,\n",
      "from which we get the formula for continuous compounding for a rate µ.\n",
      "Ornstein-Ulenbeck Process\n",
      "Also called Vasicek process\n",
      "dX(t) = (θ1 −θ2X(t))dt + θ3dB.\n",
      "(0.2.18)\n",
      "Cox-Ingersoll-Ross Process\n",
      "Also called Feller process\n",
      "dX(t) = (θ1 −θ2X(t))dt + θ3\n",
      "p\n",
      "X(t)dB.\n",
      "(0.2.19)\n",
      "***** move this *** Feller’s condition\n",
      "2θ1 > θ2\n",
      "3\n",
      "Jump-Diﬀusion Processes\n",
      "In ﬁnancial modeling, we often use a compound process that consists of some\n",
      "smooth process coupled with a jump process. The parameters controlling the\n",
      "frequency of jumps may also be modeled as a stochastic process. The amount\n",
      "of the jump is usually modeled as a random variable.\n",
      "We merely add a pure Poisson jump process djX(t) (see page 765) to the\n",
      "drift-diﬀusion process,\n",
      "dX(t) = µ(X(t), t)dt + σ(X(t), t)dB(t).\n",
      "After rearranging terms, this yields\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.2 Stochastic Processes and the Stochastic Calculus\n",
      "775\n",
      "dX(t) =\n",
      "\u0000µ(X(t), t) + (λ(X(t), t)\n",
      "R\n",
      "Z z pZ(z; X(t))dz\n",
      "\u0001\n",
      "dt\n",
      "+σ(X(t), t)dB(t)\n",
      "+djJX(t).\n",
      "(0.2.20)\n",
      "There are two stochastic terms, dB(t) and djJX(t).\n",
      "We will assume that they are independent.\n",
      "Note that I suppressed the dj on the left hand side, although, clearly, this\n",
      "is a discontinuous process, both because of the compensated process and the\n",
      "discontinuity in the drift.\n",
      "Multivariate Processes\n",
      "The multivariate Ito process has the form\n",
      "dX(t) = a(X, t)dt + B(X, t)dB(t),\n",
      "(0.2.21)\n",
      "where dX(t), a(X, t), and dB(t) are vectors and B(X, t) is a matrix.\n",
      "The elements of dB(t) can come from independent Bachelier-Wiener pro-\n",
      "cesses, or from correlated Bachelier-Wiener processes. I think it is easier to\n",
      "work with independent Bachelier-Wiener processes and incorporate any cor-\n",
      "relations into the B(X, t) matrix. Either way is just as general.\n",
      "We write the individual terms in a multivariate Ito process in the form\n",
      "dXi(t) = ai(X, t)dt + bi(X, t)dBi(t),\n",
      "(0.2.22)\n",
      "where the Bi(t) are Bachelier-Wiener processes with\n",
      "Cor(dBi(t), dBj(t)) = ρij,\n",
      "(0.2.23)\n",
      "for some constants ρij. Note that ai and bi are functions of all Xj, so the\n",
      "processes are coupled not just through the ρij.\n",
      "Recall that V(dBi(t)) = V(dBi(t)) = dt, and hence cov(dBi(t), dBj(t)) =\n",
      "ρijdt.\n",
      "Also recall that (dBi(t))2\n",
      "a.s.\n",
      "= E((dBi(t))2)\n",
      "d= t; i.e., (dBi(t))2 is non-\n",
      "stochastic. Likewise, dBi(t)dBi(t)\n",
      "a.s.\n",
      "= ρijdt.\n",
      "0.2.2 Integration with Respect to Stochastic Diﬀerentials\n",
      "The problem with developing a deﬁnition of an integral of the form\n",
      "Z T\n",
      "0\n",
      "g(Y (t), t)dB\n",
      "(0.2.24)\n",
      "following the same steps as in the deﬁnition of a Riemann integral, that is, as\n",
      "a limit of sequences of sums of areas of rectangles, is that because the sides\n",
      "of these rectangles, Y and dB, are random variables, there are diﬀerent kinds\n",
      "of convergence of a limit.\n",
      "Also, the convergence of products of Y (t) depend on where Y (t) is evalu-\n",
      "ated.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "776\n",
      "0 Statistical Mathematics\n",
      "The Ito Integral\n",
      "We begin developing a deﬁnition of\n",
      "Z T\n",
      "0\n",
      "g(Y (t), t)dB,\n",
      "by considering how the Riemann integral is deﬁned in terms of the sums\n",
      "In(t) =\n",
      "n−1\n",
      "X\n",
      "i=0\n",
      "g(Y (τi), τi)(B(ti+1) −B(ti)),\n",
      "where 0 = t0 ≤τ0 ≤t1 ≤τ1 ≤· · · ≤τn−1 ≤tn = T.\n",
      "As in the Riemann case we will deﬁne the integral in terms of a limit as\n",
      "the mesh size goes to 0.\n",
      "First, the existence depends on a ﬁnite expectation that is similar to a\n",
      "variance. We assume\n",
      "E\n",
      " Z T\n",
      "0\n",
      "g(Y (t), t)dt\n",
      "!\n",
      "< ∞.\n",
      "The convergence must be qualiﬁed because the intervals are random variables;\n",
      "furthermore, (although it is not obvious!) the convergence depends on where\n",
      "τi is in the interval [ti, ti+1].\n",
      "The ﬁrst choice in the deﬁnition of the Ito stochastic integral is to choose\n",
      "τi = ti. Other choices, such as choosing τi to be at the midpoint of the integral,\n",
      "lead to diﬀerent types of stochastic integrals.\n",
      "Next is the deﬁnition of the type of convergence. In the Ito stochastic\n",
      "integral, the convergence is in mean square, that is L2 convergence.\n",
      "With the two choices me have made, we take\n",
      "In(t) =\n",
      "n−1\n",
      "X\n",
      "i=0\n",
      "g(Y (ti), ti)(B(ti+1) −B(ti)),\n",
      "and the Ito integral is deﬁned as\n",
      "I(t) = ms-limn→∞In(t).\n",
      "(0.2.25)\n",
      "This integral based on a Bachelier-Wiener process is used throughout ﬁ-\n",
      "nancial analysis.\n",
      "Note that this integral is a random variable; in fact, it is a stochastic\n",
      "process. This is because of the fact that the diﬀerentials are from a Bachelier-\n",
      "Wiener process.\n",
      "Also, because the integral is deﬁned by a Bachelier-Wiener process, it is a\n",
      "martingale.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.2 Stochastic Processes and the Stochastic Calculus\n",
      "777\n",
      "Ito’s Lemma\n",
      "We can formalize the preceding discussion using Ito’s lemma.\n",
      "Suppose X follows an Ito process,\n",
      "dX(t) = a(X, t)dt + b(X, t)dB(t),\n",
      "where dB is a Bachelier-Wiener process. Let G be an inﬁnitely diﬀerentiable\n",
      "function of X and t. Then G follows the process\n",
      "dG(t) =\n",
      "\u0012 ∂G\n",
      "∂X a(X, t) + ∂G\n",
      "∂t + 1\n",
      "2\n",
      "∂2G\n",
      "∂X2 b2\n",
      "\u0013\n",
      "dt + ∂G\n",
      "∂X b(X, t)dB(t).\n",
      "(0.2.26)\n",
      "Thus, Ito’s lemma provides a formula that tells us that G also follows an\n",
      "Ito process.\n",
      "The drift rate is\n",
      "∂G\n",
      "∂X a(X, t) + ∂G\n",
      "∂t + 1\n",
      "2\n",
      "∂2G\n",
      "∂X2 b2\n",
      "and the volatility is\n",
      "∂G\n",
      "∂X b(X, t).\n",
      "This allows us to work out expected values and standard deviations of G\n",
      "over time.\n",
      "First, suppose that G is inﬁnitely of X and an unrelated variable y, and\n",
      "consider a Taylor series expansion for ∆G:\n",
      "∆G = ∂G\n",
      "∂X ∆X+∂G\n",
      "∂y ∆y+1\n",
      "2\n",
      "\u0012 ∂2G\n",
      "∂X2 (∆X)2 + ∂2G\n",
      "∂y2 (∆y)2 + 2 ∂2G\n",
      "∂X∂y ∆X∆y\n",
      "\u0013\n",
      "+· · ·\n",
      "(0.2.27)\n",
      "In the limit as ∆X and ∆y tend to zero, this is the usual “total derivative”\n",
      "dG = ∂G\n",
      "∂X dX + ∂G\n",
      "∂y dy,\n",
      "(0.2.28)\n",
      "in which the terms in ∆X and ∆y have dominated and eﬀectively those in\n",
      "(∆X)2 and (∆y)2 and higher powers have disappeared.\n",
      "Now consider an X that follows an Ito process,\n",
      "dX(t) = a(X, t)dt + b(X, t)dB(t),\n",
      "or\n",
      "∆X(t) = a(X, t)∆t + b(X, t)Z\n",
      "√\n",
      "∆t.\n",
      "Now let G be a function of both X and t, and consider the analogue to\n",
      "equation (0.2.27). The factor (∆X)2, which could be ignored in moving to\n",
      "equation (0.2.28), now contains a term with the factor ∆t, which cannot be\n",
      "ignored. We have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "778\n",
      "0 Statistical Mathematics\n",
      "(∆X(t))2 = b(X, t)2Z2∆t + terms of higher degree in ∆t.\n",
      "Consider the Taylor series expansion\n",
      "∆G = ∂G\n",
      "∂X ∆X + ∂G\n",
      "∂t ∆t + 1\n",
      "2\n",
      "„ ∂2G\n",
      "∂X2 (∆X)2 + ∂2G\n",
      "∂t2 (∆t)2 + 2 ∂2G\n",
      "∂X∂t ∆X∆t\n",
      "«\n",
      "+ · · ·\n",
      "(0.2.29)\n",
      "We have seen, under the assumptions of Brownian motion, (∆X(t))2 or,\n",
      "equivalently, Z2∆t, is nonstochastic; that is, we can treat Z2∆t as equal to its\n",
      "expected value as ∆t tends to zero. Therefore, when we substitute for ∆X(t),\n",
      "and take limits in equation (0.2.29) as ∆X and ∆t tend to zero, we get\n",
      "dG(t) = ∂G\n",
      "∂X dX + ∂G\n",
      "∂t dt + 1\n",
      "2\n",
      "∂2G\n",
      "∂X2 b2dt\n",
      "(0.2.30)\n",
      "or, after substituting for dX and rearranging, we have Ito’s formula\n",
      "dG(t) =\n",
      "\u0012 ∂G\n",
      "∂X a(X, t) + ∂G\n",
      "∂t + 1\n",
      "2\n",
      "∂2G\n",
      "∂X2 b2\n",
      "\u0013\n",
      "dt + ∂G\n",
      "∂X b(X, t)dB(t).\n",
      "Equation (0.2.30) is also called Ito’s formula. Compare equation (0.2.30) with\n",
      "equation (0.2.28).\n",
      "We can think of Ito’s formula as a stochastic version of the chain rule.\n",
      "There is a multivariate version of Ito’s formula for a multivariate Ito pro-\n",
      "cess. Given an inﬁnitely diﬀerential function G of the vector X = (X1, . . ., Xd)\n",
      "and the scalar t, Ito’s formula in the form of equation (0.2.30), derived in the\n",
      "same way as for the univariate case, is\n",
      "dG(t) =\n",
      "d\n",
      "X\n",
      "i=1\n",
      "∂G\n",
      "∂Xi\n",
      "dXi(t) + ∂G\n",
      "∂t dt + 1\n",
      "2\n",
      "d\n",
      "X\n",
      "i=1\n",
      "d\n",
      "X\n",
      "j=1\n",
      "∂G2\n",
      "∂Xi∂Xj\n",
      "ρijbi(X, t)bj(X, t)dt.\n",
      "(0.2.31)\n",
      "The form of equation (0.2.26), for example, is obtained by substituting for\n",
      "dXi(t).\n",
      "Solution of Stochastic Diﬀerential Equations\n",
      "*** existence Feller’s condition etc.\n",
      "The solution of a stochastic diﬀerential equation is obtained by integrating\n",
      "both sides and allowing for constant terms. Constant terms are evaluated by\n",
      "satisfying known boundary conditions, or initial values.\n",
      "In a stochastic diﬀerential equation, we must be careful in how the inte-\n",
      "gration is performed, although diﬀerent interpretations may be equally ap-\n",
      "propriate.\n",
      "For example, the SDE that deﬁnes an Ito process\n",
      "dX(t) = a(X, t)dt + b(X, t)dB(t),\n",
      "(0.2.32)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.2 Stochastic Processes and the Stochastic Calculus\n",
      "779\n",
      "when integrated from time t0 to T yields\n",
      "X(T) −X(t0) =\n",
      "Z T\n",
      "t0\n",
      "a(X, t)dt +\n",
      "Z T\n",
      "t0\n",
      "b(X, t)dB(t).\n",
      "(0.2.33)\n",
      "The second integral is a stochastic integral. We will interpret it as an Ito\n",
      "integral.\n",
      "The nature of a(X, t) and b(X, t) determine the complexity of the solution\n",
      "to the SDE.\n",
      "In the Ito process\n",
      "dS(t) = µ(t)S(t)dt + σ(t)S(t)dB(t),\n",
      "using Ito’s formula for the log as before, we get the solution\n",
      "S(T) = S(t0) exp\n",
      " Z T\n",
      "t0\n",
      "\u0012\n",
      "µ(t) −1\n",
      "2σ(t)2\n",
      "\u0013\n",
      "dt +\n",
      "Z T\n",
      "t0\n",
      "σ(t)dB(t)\n",
      "!\n",
      ".\n",
      "(0.2.34)\n",
      "In the simpler version of a geometric Brownian motion model, in which µ and\n",
      "σ are constants, we have\n",
      "S(T) = S(t0) exp\n",
      "\u0012\u0012\n",
      "µ −1\n",
      "2σ2\n",
      "\u0013\n",
      "∆t + σ∆B\n",
      "\u0013\n",
      ".\n",
      "(0.2.35)\n",
      "Given a solution of a diﬀerential equation we may determine the mean,\n",
      "variance and so on by taking expectations of the random component in the\n",
      "solution.\n",
      "Sometimes, however, it is easier just to develop an ordinary (nonstochastic)\n",
      "diﬀerential equation for the moments. We do this from an Ito process\n",
      "dX(t) = a(X, t)dt + b(X, t)dB(t),\n",
      "(0.2.36)\n",
      "by using Ito’s formula on the powers of the variable. So we have\n",
      "dXp(t) =\n",
      "\u0012\n",
      "pX(t)p−1a(X, t) + 1\n",
      "2p(p −1)X(t)p−2b(X, t)2\n",
      "\u0013\n",
      "dt +\n",
      "pX(t)p−1b(X, t)dB(t).\n",
      "** exercise\n",
      "Taking expectations of both sides, we have an ordinary diﬀerential equa-\n",
      "tion in the expected values.\n",
      "Ito’s Formula in Jump-Diﬀusion Processes\n",
      "Now suppose we are interested in a process deﬁned by a function g of S(t)\n",
      "and t. This is where Ito’s formula is used.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "780\n",
      "0 Statistical Mathematics\n",
      "The simple approach is to apply Ito’s formula directly to the drift-diﬀusion\n",
      "part and then consider djg(t) separately. (We have absorbed S(t) into t in the\n",
      "notation g(t).)\n",
      "As before,we consider the random variable of the magnitude of the change,\n",
      "∆g and write the process as a systematic component plus a random component\n",
      "djg(t) = g(t) −g(t−)\n",
      "=\n",
      " \n",
      "λ(S(t), t)\n",
      "Z\n",
      "D(∆g)\n",
      "p∆g(∆g; g(t))d∆g\n",
      "!\n",
      "dt + djJg(t)\n",
      "where the random component djJg(t) is a compensated process as before.\n",
      "Putting this all together we have\n",
      "dg(t) =\n",
      "\u0012∂g\n",
      "∂t + µ ∂g\n",
      "∂S + 1\n",
      "2σ2 ∂2g\n",
      "∂S2\n",
      "+λ(t)\n",
      "Z\n",
      "D(∆g)\n",
      "∆g p∆g(∆g; g(t))d∆g\n",
      "!\n",
      "dt\n",
      "+ ∂g\n",
      "∂S σdB(t)\n",
      "+djJg(t).\n",
      "We must remember that this is a discontinuous process.\n",
      "Notes and References for Section 0.2\n",
      "Study of continuous stochastic processes, such as Brownian motion, requires\n",
      "real analysis using random inﬁnitesimals. This area of statistical mathemat-\n",
      "ics is sometimes called stochastic calculus. Some useful texts on stochastic\n",
      "processes and the stochastic calculus are Bass (2011), Karatzas and Shreve\n",
      "(1991), Øksendal (1998), and Rogers and Williams (2000a), Rogers and Williams\n",
      "(2000b).\n",
      "Stochastic calculus is widely used in models of prices of ﬁnancial assets,\n",
      "and many of the developments in the general theory have come from that area\n",
      "of application; see Steele (2001) for example.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "781\n",
      "0.3 Some Basics of Linear Algebra\n",
      "In the following we will assume the usual axioms for the reals, IR. We will\n",
      "be concerned with two linear structures on IR. We denote one as IRn, and\n",
      "call its members vectors. We denote another as IRn×m, and call its members\n",
      "matrices. For both structures we have scalar multiplication (multiplication\n",
      "of a member of the structure by a member of IR), an addition operation, an\n",
      "additive identity, and additive inverses for all elements. The addition operation\n",
      "is denoted by “+” and the additive identity by “0”, which are the same two\n",
      "symbols used similarly in IR. We also have various types of multiplication\n",
      "operations, all with identities, and some with inverses. In addition, we deﬁne\n",
      "various real-valued functions over these structures, the most important of\n",
      "which are inner products and norms.\n",
      "Both IRn and IRn×m with addition and multiplication operations are linear\n",
      "spaces.\n",
      "In this section, we abstract some of the basic material on linear algebra\n",
      "from Gentle (2007).\n",
      "0.3.1 Inner Products, Norms, and Metrics\n",
      "Although various inner products could be deﬁned in IRn, “the” inner product\n",
      "or dot product for vectors x and y in IRn is deﬁned as Pn\n",
      "i=1 xiyi, and is often\n",
      "written as xTy. It is easy to see that this satisﬁes the deﬁnition of an inner\n",
      "product (see page 636).\n",
      "Two elements x, y ∈IRn are said to be orthogonal if ⟨x, y⟩= 0.\n",
      "An element x ∈IRn is said to be normal or normalized if ⟨x, x⟩= 1. Any\n",
      "x ̸= 0 can be normalized, that is, mapped to a normal element, x/⟨x, x⟩. A set\n",
      "of normalized elements that are pairwise orthogonal is called an orthonormal\n",
      "set. (On page 685 we discuss a method of forming a set of orthogonal vectors.)\n",
      "Various inner products could be deﬁned in IRn×m, but “the” inner product\n",
      "or dot product for matrices A and B in IRn×m is deﬁned as Pm\n",
      "j=1 aT\n",
      "j bj, where\n",
      "aj is the vector whose elements are those from the jth column of A, and\n",
      "likewise for bj. Again, it is easy to see that this satisﬁes the deﬁnition of an\n",
      "inner product.\n",
      "Norms and Metrics\n",
      "There are various norms that can be deﬁned on IRn. An important class of\n",
      "norms are the Lp norms, deﬁned for p ≥1 by\n",
      "∥x∥p =\n",
      " n\n",
      "X\n",
      "i=1\n",
      "|xi|p\n",
      "!1/p\n",
      ".\n",
      "(0.3.1)\n",
      "It is easy to see that this satisﬁes the deﬁnition of a norm (see page 641).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "782\n",
      "0 Statistical Mathematics\n",
      "The norm in IRn induced by the inner product (that is, “the” inner prod-\n",
      "uct) is the Euclidean norm or the L2 norm:\n",
      "∥x∥2 =\n",
      "p\n",
      "⟨x, x⟩=\n",
      "v\n",
      "u\n",
      "u\n",
      "t\n",
      "n\n",
      "X\n",
      "i=1\n",
      "x2\n",
      "i .\n",
      "(0.3.2)\n",
      "This is the only Lp norm induced by an inner product.\n",
      "The norm in IRn×m induced by the inner product exists only for n = m.\n",
      "In that case it is ∥A∥= Pn\n",
      "j=1 aT\n",
      "j aj = Pn\n",
      "j=1\n",
      "Pn\n",
      "i=1 a2\n",
      "ij. Note that this is not\n",
      "the L2 matrix norm; it is the Frobenius norm (see below).\n",
      "The most common and useful metrics in IRn and IRn×m are those induced\n",
      "by the norms. For IRn the L2 norm is the most common, and a metric for\n",
      "x, y ∈IRn is deﬁned as\n",
      "ρ(x, y) = ∥x −y∥2.\n",
      "(0.3.3)\n",
      "This metric is called the Euclidean distance.\n",
      "0.3.2 Matrices and Vectors\n",
      "Vectors are n-tuples and matrices are n by m rectangular arrays. We will\n",
      "be interested in vectors and matrices whose elements are real numbers. We\n",
      "denote the set of such vectors as IRn and the set of such matrics as IRn×m.\n",
      "We generally denote a member of IRn×m by an upper case letter. A member\n",
      "of IRn×m consists of nm elements, which we denote by use of two subscripts.\n",
      "We often use a lower-case letter with the two subscripts. For example, for\n",
      "a matrix A, we denote the elements as Aij or aij with i = 1, . . ., n and\n",
      "j = 1, . . ., m.\n",
      "The transpose of a matrix A in IRn×m is a matrix in IRm×n denoted by\n",
      "AT such that (AT)ij = Aji. Note that this is consistent with the use of T\n",
      "above for vectors.\n",
      "If n = m the matrix is square.\n",
      "We deﬁne (Cayley) multiplication of the matrix A ∈IRn×m and the matrix\n",
      "B ∈IRm×p as C = AB ∈IRn×p, where cij = Pm\n",
      "k=1 aikbkj.\n",
      "If x and y are n-vectors, in most cases, we can consider them to be n × 1\n",
      "matrices. Hence, xTy is a 1 × 1 matrix and xyT is an n × n matrix.\n",
      "We see from the deﬁnition that xTy is an inner product. This inner product\n",
      "is also called the dot product. The product xyT is called the outer product.\n",
      "As above, we see that\n",
      "√\n",
      "xTx is a norm (it is the induced norm). We some-\n",
      "times denote this norm as ∥x∥2, because it is\n",
      "\u0000Pn\n",
      "i=1 |xi|2\u00011/2. We call it the\n",
      "Euclidean norm and also the L2 norm. More generally, for p ≥1, we deﬁne\n",
      "the Lp norm for the n-vector x as (Pn\n",
      "i=1 |xi|p)1/p.\n",
      "We denote the Lp norm of x as ∥x∥p. We generally denote the Euclidean\n",
      "or L2 norm simply as ∥x∥.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "783\n",
      "Properties, Concepts, and Notation Associated with Matrices and\n",
      "Vectors\n",
      "Deﬁnition 0.3.1 (linear independence)\n",
      "A set of vectors x1, . . ., xn ∈IRn is said to be linearly independent if given\n",
      "ai ∈IR, Pn\n",
      "i=1 aixi = 0 implies ai = 0 for i = 1, . . ., n.\n",
      "Deﬁnition 0.3.2 (rank of a matrix)\n",
      "The rank of a matrix is the maximum number of rows or columns that are\n",
      "linearly independent. (The maximum number of rows that are linearly inde-\n",
      "pendent is the same as the maximum number of columns that are linearly\n",
      "independent.) For the matrix A, we write rank(A). We adopt the convention\n",
      "that rank(A) = 0 ⇔A = 0 (the zero matrix). A ∈IRn×m is said to be full\n",
      "rank iﬀrank(A) = min(n, m).\n",
      "An important fact is\n",
      "rank(AB) ≤min(rank(A), rank(B)),\n",
      "and a consequence of this is that the rank of an outer product is less than or\n",
      "equal to 1.\n",
      "To deﬁne the determinant of a matrix, we ﬁrst need to consider permua-\n",
      "tions of the integers from 1 to n. Let the list of integers Πj = (j1, j2, . . ., jn) be\n",
      "one of the n! permutations of the integers from 1 to n. Deﬁne a permutation to\n",
      "be even or odd according to the number of times that a smaller element follows\n",
      "a larger one in the permutation. (For example, 1, 3, 2 is an odd permutation,\n",
      "and 3, 1, 2 is an even permutation.) Let σ(Πj) = 1 if Πj = (j1, . . ., jn) is an\n",
      "even permutation, and let σ(Πj) = −1 otherwise.\n",
      "Deﬁnition 0.3.3 (determinant of a square matrix)\n",
      "The determinant of an n × n (square) A, denoted by |A|, is deﬁned by\n",
      "|A| =\n",
      "X\n",
      "all permutations\n",
      "σ(Πj)a1j1 · · ·anjn.\n",
      "The determinant is a real number. We write |A| or det(A). |A| ̸= 0 iﬀA\n",
      "is square and of full rank.\n",
      "Deﬁnition 0.3.4 (identity matrix)\n",
      "I ∈IRn×n and I[i, j] = 0 if i ̸= j and I[i, j] = 1 if i ̸= i; that is I[i, j] = δij,\n",
      "where δij is the Kronecker delta. We write the identity as In or just I.\n",
      "Deﬁnition 0.3.5 (inverse of a matrix)\n",
      "For A ∈IRn×n, if a matrix B ∈IRn×n exists, such that AB = I, then B is\n",
      "the inverse of A, and is written A−1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "784\n",
      "0 Statistical Mathematics\n",
      "A matrix has an inverse iﬀit is square and of full rank.\n",
      "Deﬁnition 0.3.6 (generalized inverse of a matrix)\n",
      "For A ∈IRn×m, a matrix B ∈IRm×n such that ABA = A is called a\n",
      "generalized inverse of A, and is written A−.\n",
      "If A is nonsingular (square and full rank), then obviously A−= A−1.\n",
      "Deﬁnition 0.3.7 (pseudoinverse or Moore-Penrose inverse of a matrix)\n",
      "For A ∈IRn×m, the matrix B ∈IRm×n such that ABA = A, BAB = B,\n",
      "(AB)T = AB, and (BA)T = BA is called the pseudoinverse of A, and is\n",
      "written A+.\n",
      "Deﬁnition 0.3.8 (orthogonal matrix)\n",
      "For A ∈IRn×m, if ATA = Im, that is, if the columns are orthonormal and\n",
      "m ≤n, or AAT = In, that is, if the rows are orthonormal and n ≤m, then A\n",
      "is said to be orthogonal.\n",
      "Deﬁnition 0.3.9 (quadratic forms)\n",
      "For A ∈IRn×n and x ∈IRn, the scalar xTAx is called a quadratic form.\n",
      "Deﬁnition 0.3.10 (nonnegative deﬁnite matrix)\n",
      "For A ∈IRn×n and any x ∈IRn, if xTAx ≥0, then is said to be nonnegative\n",
      "deﬁnite. We generally restrict the deﬁnition to symmetric matrices. This is\n",
      "essentially without loss of generality because if a matrix is nonnegative deﬁ-\n",
      "nite, then there is a similar symmetric matrix. (Two matrices are said to be\n",
      "similar if they have exactly the same eigenvalues.) We write A ⪰0 to denote\n",
      "that A is nonnegative deﬁnite.\n",
      "Deﬁnition 0.3.11 (positive deﬁnite matrix)\n",
      "For A ∈IRn×n and any x ∈IRn, if xTAx ≥0 and xTAx = 0 implies x = 0,\n",
      "then is said to be positive deﬁnite. As with nonnegative deﬁnite matrices,\n",
      "we generally restrict the deﬁnition of positive deﬁnite matrices to symmetric\n",
      "matrices. We write A ≻0 to denote that A is positive deﬁnite.\n",
      "Deﬁnition 0.3.12 (eigenvalues and eigenvectors) If A ∈IRn×n, v is an\n",
      "n-vector (complex), and c is a scalar (complex), and Av = cv, then c is an\n",
      "eigenvalue of A and v is an eigenvector of A associated with c.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "785\n",
      "The Trace and Some of Its Properties\n",
      "Deﬁnition 0.3.13 (trace of a matrix)\n",
      "The sum of the diagonal elements of a square matrix is called the trace of the\n",
      "matrix.\n",
      "We use the notation “tr(A)” to denote the trace of the matrix A:\n",
      "tr(A) =\n",
      "X\n",
      "i\n",
      "aii.\n",
      "Some properties of the trace that follow immediately from the deﬁnition:\n",
      "tr(A) = tr(AT).\n",
      "For a scalar c and an n × n matrix A,\n",
      "tr(cA) = c tr(A).\n",
      "If A and B are such that both AB and BA are deﬁned,\n",
      "tr(AB) = tr(BA).\n",
      "If x is a vector, we have\n",
      "∥x∥2 = xTx = tr(xTx) = tr(xxT).\n",
      "If x is a vector and A a matrix, we have\n",
      "xTAx = tr(xTAx) = tr(AxxT).\n",
      "Eigenanalysis of Symmetric Matrices\n",
      "The eigenvalues and eigenvectors of symmetric matrices have some interesting\n",
      "properties. First of all, for a real symmetric matrix, the eigenvalues are all\n",
      "real. Symmetric matrices are diagonalizable; therefore all of the properties of\n",
      "diagonalizable matrices carry over to symmetric matrices.\n",
      "Orthogonality of Eigenvectors\n",
      "In the case of a symmetric matrix A, any eigenvectors corresponding to dis-\n",
      "tinct eigenvalues are orthogonal. This is easily seen by assuming that c1 and\n",
      "c2 are unequal eigenvalues with corresponding eigenvectors v1 and v2. Now\n",
      "consider vT\n",
      "1 v2. Multiplying this by c2, we get\n",
      "c2vT\n",
      "1 v2 = vT\n",
      "1 Av2 = vT\n",
      "2 Av1 = c1vT\n",
      "2 v1 = c1vT\n",
      "1 v2.\n",
      "Because c1 ̸= c2, we have vT\n",
      "1 v2 = 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "786\n",
      "0 Statistical Mathematics\n",
      "Now, consider two eigenvalues ci = cj, that is, an eigenvalue of multiplicity\n",
      "greater than 1 and distinct associated eigenvectors vi and vj. By what we\n",
      "just saw, an eigenvector associated with ck ̸= ci is orthogonal to the space\n",
      "spanned by vi and vj. Assume vi is normalized and apply a Gram-Schmidt\n",
      "transformation to form\n",
      "˜vj =\n",
      "1\n",
      "∥vj −⟨vi, vj⟩vi∥(vj −⟨vi, vj⟩vi),\n",
      "yielding a vector orthogonal to vi. Now, we have\n",
      "A˜vj =\n",
      "1\n",
      "∥vj −⟨vi, vj⟩vi∥(Avj −⟨vi, vj⟩Avi)\n",
      "=\n",
      "1\n",
      "∥vj −⟨vi, vj⟩vi∥(cjvj −⟨vi, vj⟩civi)\n",
      "= cj\n",
      "1\n",
      "∥vj −⟨vi, vj⟩vi∥(vj −⟨vi, vj⟩vi)\n",
      "= cj˜vj;\n",
      "hence, ˜vj is an eigenvector of A associated with cj. We conclude therefore that\n",
      "the eigenvectors of a symmetric matrix can be chosen to be orthogonal.\n",
      "A symmetric matrix is orthogonally diagonalizable, and because the eigen-\n",
      "vectors can be chosen to be orthogonal, and can be written as\n",
      "A = VCV T,\n",
      "(0.3.4)\n",
      "where V V T = V TV = I, and so we also have\n",
      "V TAV = C.\n",
      "(0.3.5)\n",
      "Such a matrix is orthogonally similar to a diagonal matrix formed from its\n",
      "eigenvalues.\n",
      "Spectral Decomposition\n",
      "When A is symmetric and the eigenvectors vi are chosen to be orthonormal,\n",
      "I =\n",
      "X\n",
      "i\n",
      "vivT\n",
      "i ,\n",
      "(0.3.6)\n",
      "so\n",
      "A = A\n",
      "X\n",
      "i\n",
      "vivT\n",
      "i\n",
      "=\n",
      "X\n",
      "i\n",
      "AvivT\n",
      "i\n",
      "=\n",
      "X\n",
      "i\n",
      "civivT\n",
      "i .\n",
      "(0.3.7)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "787\n",
      "This representation is called the spectral decomposition of the symmetric ma-\n",
      "trix A. It is essentially the same as equation (0.3.4), so A = VCV T is also\n",
      "called the spectral decomposition.\n",
      "The representation is unique except for the ordering and the choice of\n",
      "eigenvectors for eigenvalues with multiplicities greater than 1. If the rank of\n",
      "the matrix is r, we have |c1| ≥· · · ≥|cr| > 0, and if r < n, then cr+1 = · · · =\n",
      "cn = 0.\n",
      "Note that the matrices in the spectral decomposition are projection matri-\n",
      "ces that are orthogonal to each other (but they are not orthogonal matrices)\n",
      "and they sum to the identity. Let\n",
      "Pi = vivT\n",
      "i .\n",
      "(0.3.8)\n",
      "Then we have\n",
      "PiPi = Pi,\n",
      "(0.3.9)\n",
      "PiPj = 0 for i ̸= j,\n",
      "(0.3.10)\n",
      "X\n",
      "i\n",
      "Pi = I,\n",
      "(0.3.11)\n",
      "and the spectral decomposition,\n",
      "A =\n",
      "X\n",
      "i\n",
      "ciPi.\n",
      "(0.3.12)\n",
      "The Pi are called spectral projectors.\n",
      "The spectral decomposition also applies to powers of A,\n",
      "Ak =\n",
      "X\n",
      "i\n",
      "ck\n",
      "i vivT\n",
      "i ,\n",
      "(0.3.13)\n",
      "where k is an integer. If A is nonsingular, k can be negative in the expression\n",
      "above.\n",
      "The spectral decomposition is one of the most important tools in working\n",
      "with symmetric matrices.\n",
      "Although we will not prove it here, all diagonalizable matrices have a spec-\n",
      "tral decomposition in the form of equation (0.3.12) with projection matrices\n",
      "that satisfy properties (0.3.9) through (0.3.11). These projection matrices can-\n",
      "not necessarily be expressed as outer products of eigenvectors, however. The\n",
      "eigenvalues and eigenvectors of a nonsymmetric matrix might not be real, the\n",
      "left and right eigenvectors might not be the same, and two eigenvectors might\n",
      "not be mutually orthogonal. In the spectral representation A = P\n",
      "i ciPi, how-\n",
      "ever, if cj is a simple eigenvalue with associated left and right eigenvectors yj\n",
      "and xj, respectively, then the projection matrix Pj is xjyH\n",
      "j /yH\n",
      "j xj. (Note that\n",
      "because the eigenvectors may not be real, we take the conjugate transpose.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "788\n",
      "0 Statistical Mathematics\n",
      "Quadratic Forms and the Rayleigh Quotient\n",
      "Equation (0.3.7) yields important facts about quadratic forms in A. Because\n",
      "V is of full rank, an arbitrary vector x can be written as V b for some vector\n",
      "b. Therefore, for the quadratic form xTAx we have\n",
      "xTAx = xT X\n",
      "i\n",
      "civivT\n",
      "i x\n",
      "=\n",
      "X\n",
      "i\n",
      "bTV TvivT\n",
      "i V bci\n",
      "=\n",
      "X\n",
      "i\n",
      "b2\n",
      "i ci.\n",
      "This immediately gives the inequality\n",
      "xTAx ≤max{ci}bTb.\n",
      "(Notice that max{ci} here is not necessarily c1; in the important case when\n",
      "all of the eigenvalues are nonnegative, it is, however.) Furthermore, if x ̸= 0,\n",
      "bTb = xTx, and we have the important inequality\n",
      "xTAx\n",
      "xTx ≤max{ci}.\n",
      "(0.3.14)\n",
      "Equality is achieved if x is the eigenvector corresponding to max{ci}, so we\n",
      "have\n",
      "max\n",
      "x̸=0\n",
      "xTAx\n",
      "xTx = max{ci}.\n",
      "(0.3.15)\n",
      "If c1 > 0, this is the spectral radius, ρ(A).\n",
      "*** prove the following add to matrix book\n",
      "max\n",
      "x\n",
      "xTAx\n",
      "xTCx = max(e.v.)(C−1A).\n",
      "(0.3.16)\n",
      "or if A = aaT, then\n",
      "max\n",
      "x\n",
      "xTAx\n",
      "xTCx = aTC−1a.\n",
      "(0.3.17)\n",
      "The expression on the left-hand side in (0.3.14) as a function of x is called\n",
      "the Rayleigh quotient of the symmetric matrix A and is denoted by RA(x):\n",
      "RA(x) = xTAx\n",
      "xTx\n",
      "= ⟨x, Ax⟩\n",
      "⟨x, x⟩.\n",
      "(0.3.18)\n",
      "Because if x ̸= 0, xTx > 0, it is clear that the Rayleigh quotient is nonnegative\n",
      "for all x if and only if A is nonnegative deﬁnite and is positive for all x if and\n",
      "only if A is positive deﬁnite.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "789\n",
      "The Fourier Expansion\n",
      "The vivT\n",
      "i matrices in equation (0.3.7) have the property that ⟨vivT\n",
      "i , vjvT\n",
      "j ⟩= 0\n",
      "for i ̸= j and ⟨vivT\n",
      "i , vivT\n",
      "i ⟩= 1, and so the spectral decomposition is a Fourier\n",
      "expansion and the eigenvalues are Fourier coeﬃcients. Because of orthogonal-\n",
      "ity, the eigenvalues can be represented as the dot product\n",
      "ci = ⟨A, vivT\n",
      "i ⟩.\n",
      "(0.3.19)\n",
      "The eigenvalues ci have the same properties as the Fourier coeﬃcients\n",
      "in any orthonormal expansion. In particular, the best approximating matrices\n",
      "within the subspace of n×n symmetric matrices spanned by {v1vT\n",
      "1 , . . ., vnvT\n",
      "n }\n",
      "are partial sums of the form of equation (0.3.7).\n",
      "Powers of a Symmetric Matrix\n",
      "If (c, v) is an eigenpair of the symmetric matrix A with vTv = 1, then for any\n",
      "k = 1, 2, . . .,\n",
      "\u0000A −cvvT\u0001k = Ak −ckvvT.\n",
      "(0.3.20)\n",
      "This follows from induction on k, for it clearly is true for k = 1, and if for a\n",
      "given k it is true that for k −1\n",
      "\u0000A −cvvT\u0001k−1 = Ak−1 −ck−1vvT,\n",
      "then by multiplying both sides by (A −cvvT), we see it is true for k:\n",
      "\u0000A −cvvT\u0001k = \u0000Ak−1 −ck−1vvT\u0001 (A −cvvT)\n",
      "= Ak −ck−1vvTA −cAk−1vvT + ckvvT\n",
      "= Ak −ckvvT −ckvvT + ckvvT\n",
      "= Ak −ckvvT.\n",
      "There is a similar result for nonsymmetric square matrices, where w and\n",
      "v are left and right eigenvectors, respectively, associated with the same eigen-\n",
      "value c that can be scaled so that wTv = 1. (Recall that an eigenvalue of A\n",
      "is also an eigenvalue of AT, and if w is a left eigenvector associated with the\n",
      "eigenvalue c, then ATw = cw.) The only property of symmetry used above\n",
      "was that we could scale vTv to be 1; hence, we just need wTv ̸= 0. This is\n",
      "clearly true for a diagonalizable matrix (from the deﬁnition). It is also true if\n",
      "c is simple (which is somewhat harder to prove).\n",
      "If w and v are left and right eigenvectors of A associated with the same\n",
      "eigenvalue c and wTv = 1, then for k = 1, 2, . . .,\n",
      "\u0000A −cvwT\u0001k = Ak −ckvwT.\n",
      "(0.3.21)\n",
      "We can prove this by induction as above.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "790\n",
      "0 Statistical Mathematics\n",
      "The Trace and Sums of Eigenvalues\n",
      "For a general n × n matrix A with eigenvalues c1, . . ., cn, we have tr(A) =\n",
      "Pn\n",
      "i=1 ci. This is particularly easy to see for symmetric matrices because of\n",
      "equation (0.3.4), rewritten as V TAV = C, the diagonal matrix of the eigen-\n",
      "values. For a symmetric matrix, however, we have a stronger result.\n",
      "If A is an n × n symmetric matrix with eigenvalues c1 ≥· · · ≥cn, and U\n",
      "is an n × k orthogonal matrix, with k ≤n, then\n",
      "tr(U TAU) ≤\n",
      "k\n",
      "X\n",
      "i=1\n",
      "ci.\n",
      "(0.3.22)\n",
      "To see this, we represent U in terms of the columns of V , which span IRn, as\n",
      "U = V X. Hence,\n",
      "tr(U TAU) = tr(XTV TAV X)\n",
      "= tr(XTCX)\n",
      "=\n",
      "n\n",
      "X\n",
      "i=1\n",
      "xT\n",
      "i xi ci,\n",
      "(0.3.23)\n",
      "where xT\n",
      "i is the ith row of X.\n",
      "Now XTX = XTV TV X = U TU = Ik, so either xT\n",
      "i xi = 0 or xT\n",
      "i xi = 1,\n",
      "and Pn\n",
      "i=1 xT\n",
      "i xi = k. Because c1 ≥· · · ≥cn, therefore Pn\n",
      "i=1 xT\n",
      "i xi ci ≤Pk\n",
      "i=1 ci,\n",
      "and so from equation (0.3.23) we have tr(U TAU) ≤Pk\n",
      "i=1 ci.\n",
      "0.3.2.1 Positive Deﬁnite and Nonnegative Deﬁnite\n",
      "Matrices\n",
      "The factorization of symmetric matrices in equation (0.3.4) yields some useful\n",
      "properties of positive deﬁnite and nonnegative deﬁnite matrices.\n",
      "Eigenvalues of Positive and Nonnegative Deﬁnite Matrices\n",
      "In this book, we use the terms “nonnegative deﬁnite” and “positive deﬁnite”\n",
      "only for real symmetric matrices, so the eigenvalues of nonnegative deﬁnite or\n",
      "positive deﬁnite matrices are real.\n",
      "Any real symmetric matrix is positive (nonnegative) deﬁnite if and only\n",
      "if all of its eigenvalues are positive (nonnegative). We can see this using the\n",
      "factorization (0.3.4) of a symmetric matrix. One factor is the diagonal matrix\n",
      "C of the eigenvalues, and the other factors are orthogonal. Hence, for any x,\n",
      "we have xTAx = xTVCV Tx = yTCy, where y = V Tx, and so\n",
      "xTAx > (≥) 0\n",
      "if and only if\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "791\n",
      "yTCy > (≥) 0.\n",
      "This implies that if P is a nonsingular matrix and D is a diagonal matrix,\n",
      "P TDP is positive (nonnegative) if and only if the elements of D are positive\n",
      "(nonnegative).\n",
      "A matrix (whether symmetric or not and whether real or not) all of whose\n",
      "eigenvalues have positive real parts is said to be positive stable. Positive stabil-\n",
      "ity is an important property in some applications, such as numerical solution\n",
      "of systems of nonlinear diﬀerential equations. Clearly, a positive deﬁnite ma-\n",
      "trix is positive stable.\n",
      "Inverse of Positive Deﬁnite Matrices\n",
      "If A is positive deﬁnite and A = VCV T as in equation (0.3.4), then A−1 =\n",
      "VC−1V T and A−1 is positive deﬁnite because the elements of C−1 are positive.\n",
      "Diagonalization of Positive Deﬁnite Matrices\n",
      "If A is positive deﬁnite, the elements of the diagonal matrix C in equa-\n",
      "tion (0.3.4) are positive, and so their square roots can be absorbed into V\n",
      "to form a nonsingular matrix P . The diagonalization in equation (0.3.5),\n",
      "V TAV = C, can therefore be reexpressed as\n",
      "P TAP = I.\n",
      "(0.3.24)\n",
      "Square Roots of Positive and Nonnegative Deﬁnite Matrices\n",
      "The factorization (0.3.4) together with the nonnegativity of the eigenvalues\n",
      "of positive and nonnegative deﬁnite matrices allows us to deﬁne a square root\n",
      "of such a matrix.\n",
      "Let A be a nonnegative deﬁnite matrix and let V and C be as in equa-\n",
      "tion (0.3.4): A = VCV T. Now, let S be a diagonal matrix whose elements\n",
      "are the square roots of the corresponding elements of C. Then (VSV T)2 = A;\n",
      "hence, we write\n",
      "A\n",
      "1\n",
      "2 = VSV T\n",
      "(0.3.25)\n",
      "and call this matrix the square root of A. We also can similarly deﬁne A\n",
      "1\n",
      "r for\n",
      "r > 0.\n",
      "We see immediately that A\n",
      "1\n",
      "2 is symmetric because A is symmetric.\n",
      "If A is positive deﬁnite, A−1 exists and is positive deﬁnite. It therefore has\n",
      "a square root, which we denote as A−1\n",
      "2 .\n",
      "The square roots are nonnegative, and so A\n",
      "1\n",
      "2 is nonnegative deﬁnite. Fur-\n",
      "thermore, A\n",
      "1\n",
      "2 and A−1\n",
      "2 are positive deﬁnite if A is positive deﬁnite.\n",
      "This A\n",
      "1\n",
      "2 is unique, so our reference to it as the square root is appropriate.\n",
      "(There is occasionally some ambiguity in the terms “square root” and “second\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "792\n",
      "0 Statistical Mathematics\n",
      "root” and the symbols used to denote them. If x is a nonnegative scalar, the\n",
      "usual meaning of its square root, denoted by √x, is a nonnegative number,\n",
      "while its second roots, which may be denoted by x\n",
      "1\n",
      "2 , are usually considered to\n",
      "be either of the numbers ±√x. In our notation A\n",
      "1\n",
      "2 , we mean the square root;\n",
      "that is, the nonnegative matrix, if it exists. Otherwise, we say the square root\n",
      "of the matrix does not exist. For example, I\n",
      "1\n",
      "2\n",
      "2 = I2, and while if J =\n",
      "\u00140 1\n",
      "1 0\n",
      "\u0015\n",
      ",\n",
      "J2 = I2, we do not consider J to be a square root of I2.)\n",
      "Forming a Vector from the Elements of a Matrix: vec(·), vecsy(·),\n",
      "and vech(·)\n",
      "It is sometimes useful to consider the elements of a matrix to be elements of\n",
      "a single vector. The most common way this is done is to string the columns\n",
      "of the matrix end-to-end into a vector. The vec(·) function does this:\n",
      "vec(A) = (aT\n",
      "1 , aT\n",
      "2 , . . ., aT\n",
      "m),\n",
      "(0.3.26)\n",
      "where a1, a2, . . ., am are the column vectors of the matrix A. The vec function\n",
      "is also sometimes called the “pack” function. The vec function is a mapping\n",
      "IRn×m 7→IRnm.\n",
      "For a symmetric matrix A with elements aij, the “vecsy” function stacks\n",
      "the unique elements into a vector:\n",
      "vecsy(A) = (a11, a21, a22, a31, a32, a33, . . ., an1, an2, . . ., ann).\n",
      "(0.3.27)\n",
      "The vecsy function is called the V 2 function by Kollo and von Rosen (2005). It\n",
      "is the “symmetric storage mode” used by numerical analysts since the 1950s.\n",
      "There are other ways that the unique elements could be stacked. The\n",
      "“vech” function is\n",
      "vech(A) = (a11, a21, . . ., an1, a22, . . ., an2, . . ., ann).\n",
      "(0.3.28)\n",
      "The vecsy and vech functions are mappings IRn×n 7→IRn(n+1)/2.\n",
      "The Kronecker Product\n",
      "Kronecker multiplication, denoted by ⊗, is deﬁned for any two matrices An×m\n",
      "and Bp×q as\n",
      "A ⊗B =\n",
      "\n",
      "\n",
      "a11B . . . a1mB\n",
      "...\n",
      ". . .\n",
      "...\n",
      "an1B . . . anmB\n",
      "\n",
      ".\n",
      "The Kronecker product of A and B is np × mq; that is, Kronecker matrix\n",
      "multiplication is a mapping\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "793\n",
      "IRn×m × IRp×q 7→IRnp×mq.\n",
      "The Kronecker product is also called the “right direct product” or just\n",
      "direct product. (A left direct product is a Kronecker product with the factors\n",
      "reversed.)\n",
      "Kronecker multiplication is not commutative, but it is associative and it\n",
      "is distributive over addition, as we will see below.\n",
      "The identity for Kronecker multiplication is the 1 × 1 matrix with the\n",
      "element 1; that is, it is the same as the scalar 1.\n",
      "The determinant of the Kronecker product of two square matrices An×n\n",
      "and Bm×m has a simple relationship to the determinants of the individual\n",
      "matrices:\n",
      "|A ⊗B| = |A|m|B|n.\n",
      "(0.3.29)\n",
      "The proof of this, like many facts about determinants, is straightforward but\n",
      "involves tedious manipulation of cofactors. The manipulations in this case can\n",
      "be facilitated by using the vec-permutation matrix.\n",
      "We can understand the properties of the Kronecker product by expressing\n",
      "the (i, j) element of A ⊗B in terms of the elements of A and B,\n",
      "(A ⊗B)i,j = A[i/p]+1, [j/q]+1Bi−p[i/p], j−q[i/q],\n",
      "(0.3.30)\n",
      "where [·] is the greatest integer function.\n",
      "Some additional properties of Kronecker products that are immediate re-\n",
      "sults of the deﬁnition are, assuming the matrices are conformable for the\n",
      "indicated operations,\n",
      "(aA) ⊗(bB) = ab(A ⊗B)\n",
      "= (abA) ⊗B\n",
      "= A ⊗(abB), for scalars a, b,\n",
      "(0.3.31)\n",
      "(A + B) ⊗(C) = A ⊗C + B ⊗C,\n",
      "(0.3.32)\n",
      "(A ⊗B) ⊗C = A ⊗(B ⊗C),\n",
      "(0.3.33)\n",
      "(A ⊗B)T = AT ⊗BT,\n",
      "(0.3.34)\n",
      "(A ⊗B)(C ⊗D) = AC ⊗BD.\n",
      "(0.3.35)\n",
      "These properties are all easy to see by using equation (0.3.30) to express\n",
      "the (i, j) element of the matrix on either side of the equation, taking into\n",
      "account the size of the matrices involved. For example, in the ﬁrst equation,\n",
      "if A is n × m and B is p × q, the (i, j) element on the left-hand side is\n",
      "aA[i/p]+1, [j/q]+1bBi−p[i/p], j−q[i/q]\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "794\n",
      "0 Statistical Mathematics\n",
      "and that on the right-hand side is\n",
      "abA[i/p]+1, [j/q]+1Bi−p[i/p], j−q[i/q].\n",
      "Another property of the Kronecker product of square matrices is\n",
      "tr(A ⊗B) = tr(A)tr(B).\n",
      "(0.3.36)\n",
      "This is true because the trace of the product is merely the sum of all possible\n",
      "products of the diagonal elements of the individual matrices.\n",
      "The Kronecker product and the vec function often ﬁnd uses in the same\n",
      "application. For example, an n × m normal random matrix X with parameters\n",
      "M, Σ, and Ψ can be expressed in terms of an ordinary np-variate normal\n",
      "random variable Y = vec(X) with parameters vec(M) and Σ ⊗Ψ.\n",
      "A relationship between the vec function and Kronecker multiplication is\n",
      "vec(ABC) = (CT ⊗A)vec(B)\n",
      "(0.3.37)\n",
      "for matrices A, B, and C that are conformable for the multiplication indicated.\n",
      "Matrix Factorizations\n",
      "There are a number of useful ways of factorizing a matrix.\n",
      "•\n",
      "the LU (and LR and LDU) factorization of a general matrix:\n",
      "•\n",
      "the QR factorization of a general matrix,\n",
      "•\n",
      "the similar canonical factorization or “diagonal factorization” of a diago-\n",
      "nalizable matrix (which is necessarily square):\n",
      "A = VCV −1,\n",
      "where V is a matrix whose columns correspond to the eigenvectors of\n",
      "A and is nonsingular, and C is a diagonal matrix whose entries are the\n",
      "eigenvalues corresponding to the columns of V .\n",
      "•\n",
      "the singular value factorization of a general n × m matrix A:\n",
      "A = UDV T,\n",
      "where U is an n × n orthogonal matrix, V is an m × m orthogonal matrix,\n",
      "and D is an n × m diagonal matrix with nonnegative entries. (An n × m\n",
      "diagonal matrix has min(n, m) elements on the diagonal, and all other\n",
      "entries are zero.)\n",
      "•\n",
      "the square root of a nonnegative deﬁnite matrix A (which is necessarily\n",
      "symmetric):\n",
      "A = A1/2A1/2\n",
      "•\n",
      "the Cholesky factorization of a nonnegative deﬁnite matrix:\n",
      "A = AT\n",
      "c Ac,\n",
      "where Ac is an upper triangular matrix with nonnegative diagonal ele-\n",
      "ments.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "795\n",
      "Spectral Decomposition\n",
      "For a symmetric matrix A, we can always write A = VCV T, as above. This is\n",
      "called the spectral decomposition, and is unique except for the ordering and\n",
      "the choice of eigenvectors for eigenvalues with multiplicities greater than 1.\n",
      "We can also write\n",
      "A =\n",
      "X\n",
      "i\n",
      "ciPi,\n",
      "where the Pi are the outer products of the eigenvectors,\n",
      "Pi = vivT\n",
      "i ,\n",
      "and are called spectral projectors.\n",
      "Matrix Norms\n",
      "A matrix norm is generally required to satisfy one more property in addition to\n",
      "those listed above for the deﬁnition of a norm. It is the consistency property:\n",
      "∥AB∥≤∥A∥∥B∥. The Lp matrix norm for the n × m matrix A is deﬁned as\n",
      "∥A∥p = max\n",
      "∥x∥p=1 ∥Ax∥p.\n",
      "The L2 matrix norm has the interesting relationship\n",
      "∥A∥2 =\n",
      "q\n",
      "ρ(ATA),\n",
      "where ρ(·) is the spectral radius (the modulus of the eigenvalue with the\n",
      "maximum modulus).\n",
      "The “usual” matrix norm is the Frobenius norm:\n",
      "∥A∥F =\n",
      "sX\n",
      "i,j\n",
      "a2\n",
      "ij.\n",
      "Idempotent and Projection Matrices\n",
      "A matrix A such that AA = A is called an idempotent matrix. An idempotent\n",
      "matrix is square, and it is either singular or it is the identity matrix. (It must\n",
      "be square in order to be conformable for the indicated multiplication. If it\n",
      "is not singular, we have A = (A−1A)A = A−1(AA) = A−1A = I; hence, an\n",
      "idempotent matrix is either singular or it is the identity matrix.)\n",
      "If A is idempotent and n × n, then (I −A) is also idempotent, as we see\n",
      "by multiplication.\n",
      "In this case, we also have\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "796\n",
      "0 Statistical Mathematics\n",
      "rank(I −A) = n −rank(A).\n",
      "Because the eigenvalues of A2 are the squares of the eigenvalues of A, all\n",
      "eigenvalues of an idempotent matrix must be either 0 or 1. The number of\n",
      "eigenvalues that are 1 is the rank of the matrix. We therefore have for an\n",
      "idempotent matrix A,\n",
      "tr(A) = rank(A).\n",
      "Because AA = A, any vector in the column space of A is an eigenvector of A.\n",
      "For a given vector space V, a symmetric idempotent matrix A whose\n",
      "columns span V is said to be a projection matrix onto V; in other words,\n",
      "a matrix A is a projection matrix onto span(A) if and only if A is symmetric\n",
      "and idempotent.\n",
      "It is easy to see that for any vector x, if A is a projection matrix onto\n",
      "V, the vector Ax is in V, and the vector x −Ax is in V⊥(the vectors Ax\n",
      "and x −Ax are orthogonal). For this reason, a projection matrix is sometimes\n",
      "called an “orthogonal projection matrix”. Note that an orthogonal projection\n",
      "matrix is not an orthogonal matrix, however, unless it is the identity matrix.\n",
      "Stating this in alternate notation, if A is a projection matrix and A ∈IRn×n,\n",
      "then A maps IRn onto V(A), and I −A is also a projection matrix (called the\n",
      "complementary projection matrix of A), and it maps IRn onto the orthogonal\n",
      "complement, N(A). These spaces are such that V(A) ⊕N(A) = IRn.\n",
      "Useful projection matrices often encountered in statistical linear models\n",
      "are A+A and AA+.\n",
      "If x is a general vector in IRn, that is, if x has order n and belongs to an\n",
      "n-dimensional space, and A is a projection matrix of rank r ≤n, then Ax has\n",
      "order n and belongs to span(A), which is an r-dimensional space.\n",
      "Because a projection matrix is idempotent, the matrix projects any of\n",
      "its columns onto itself, and of course it projects the full matrix onto itself:\n",
      "AA = A. More generally, if x and y are vectors in span(A) and a is a scalar,\n",
      "then\n",
      "A(ax + y) = ax + y.\n",
      "(To see this, we merely represent x and y as linear combinations of columns\n",
      "(or rows) of A and substitute in the equation.)\n",
      "The projection of a vector y onto a vector x is\n",
      "xTy\n",
      "xTxx.\n",
      "The projection matrix to accomplish this is the “outer/inner products ma-\n",
      "trix”,\n",
      "1\n",
      "xTxxxT.\n",
      "The outer/inner products matrix has rank 1. It is useful in a variety of matrix\n",
      "transformations. If x is normalized, the projection matrix for projecting a\n",
      "vector on x is just xxT. The projection matrix for projecting a vector onto a\n",
      "unit vector ei is eieT\n",
      "i , and eieT\n",
      "i y = (0, . . ., yi, . . ., 0).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "797\n",
      "Inverses of Matrices\n",
      "Often in applications we need inverses of various sums of matrices. A simple\n",
      "general result, which we can verify by multiplication, is that if A is a full-rank\n",
      "n × n matrix, B is a full-rank m × m matrix, C is any n × m matrix, and D\n",
      "is any m × n matrix such that A + CBD is full rank, then\n",
      "(A + CBD)−1 = A−1 −A−1C(B−1 + DA−1C)−1DA−1.\n",
      "From this it follows that if A is a full-rank n × n matrix and b and c are\n",
      "n-vectors such that (A + bcT) is full rank, then\n",
      "(A + bcT)−1 = A−1 −A−1bcTA−1\n",
      "1 + cTA−1b .\n",
      "If A and B are full rank matrices of the same size, the following relation-\n",
      "ships are easy to show directly.\n",
      "(I + A−1)−1 = A(A + I)−1\n",
      "(A + BBT)−1B = A−1B(I + BTA−1B)−1\n",
      "(A−1 + B−1)−1 = A(A + B)−1B\n",
      "A −A(A + B)−1A = B −B(A + B)−1B\n",
      "A−1 + B−1 = A−1(A + B)B−1\n",
      "(I + AB)−1 = I −A(I + BA)−1B\n",
      "(I + AB)−1A = A(I + BA)−1\n",
      "From the relationship det(AB) = det(A) det(B) for square matrices men-\n",
      "tioned earlier, it is easy to see that for nonsingular A,\n",
      "det(A) = 1/det(A−1).\n",
      "For a square matrix A, det(A) = 0 if and only if A is singular.\n",
      "Partitioned Matrices\n",
      "We often ﬁnd it useful to partition a matrix into submatrices, and we usually\n",
      "denote those submatrices with capital letters with subscripts indicating the\n",
      "relative positions of the submatrices. Hence, we may write\n",
      "A =\n",
      "\u0014A11 A12\n",
      "A21 A22\n",
      "\u0015\n",
      ",\n",
      "where the matrices A11 and A12 have the same number of rows, A21 and\n",
      "A22 have the same number of rows, A11 and A21 have the same number of\n",
      "columns, and A12 and A22 have the same number of columns.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "798\n",
      "0 Statistical Mathematics\n",
      "The term “submatrix” is also sometimes used to refer to a matrix formed\n",
      "from another one by deleting various rows and columns of the given matrix.\n",
      "In this terminology, B is a submatrix of A if for each element bij there is an\n",
      "akl with k ≥i and l ≥j, such that bij = akl; that is, the rows and/or columns\n",
      "of the submatrix are not contiguous in the original matrix.\n",
      "A submatrix whose principal diagonal elements are elements of the prin-\n",
      "cipal diagonal of the given matrix is called a principal submatrix; A11 is a\n",
      "principal submatrix in the example above, and if A22 is square it is also a\n",
      "principal submatrix. Sometimes the term “principal submatrix” is restricted\n",
      "to square submatrices.\n",
      "A principal submatrix that contains the (1, 1) and whose rows and columns\n",
      "are contiguous in the original matrix is called a leading principal submatrix.\n",
      "A11 is a principal submatrix in the example above.\n",
      "Multiplication and other operations with matrices, such as transposition,\n",
      "are carried out with their submatrices in the obvious way. Thus,\n",
      "\u0014A11 A12 A13\n",
      "A21 A22 A23\n",
      "\u0015T\n",
      "=\n",
      "\n",
      "\n",
      "AT\n",
      "11 AT\n",
      "21\n",
      "AT\n",
      "12 AT\n",
      "22\n",
      "AT\n",
      "13 AT\n",
      "23\n",
      "\n",
      ",\n",
      "and, assuming the submatrices are conformable for multiplication,\n",
      "\u0014A11 A12\n",
      "A21 A22\n",
      "\u0015 \u0014B11 B12\n",
      "B21 B22\n",
      "\u0015\n",
      "=\n",
      "\u0014 A11B11 + A12B21 A11B12 + A12B22\n",
      "A21B11 + A22B21 A21B12 + A22B22\n",
      "\u0015\n",
      ".\n",
      "Sometimes a matrix may be partitioned such that one partition is just a\n",
      "single column or row, that is, a vector or the transpose of a vector. In that\n",
      "case, we may use a notation such as\n",
      "[X y]\n",
      "or\n",
      "[X | y],\n",
      "where X is a matrix and y is a vector. We develop the notation in the obvious\n",
      "fashion; for example,\n",
      "[X y]T [X y] =\n",
      "\u0014XTX XTy\n",
      "yTX yTy\n",
      "\u0015\n",
      ".\n",
      "Partitioned matrices may also have useful patterns. A “block diagonal”\n",
      "matrix is one of the form\n",
      "\n",
      "\n",
      "X 0 · · · 0\n",
      "0 X · · · 0\n",
      "...\n",
      "0 0 · · · X\n",
      "\n",
      ",\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "799\n",
      "where 0 represents a submatrix with all zeros, and X represents a general\n",
      "submatrix, with at least some nonzeros. The diag(·) function previously in-\n",
      "troduced for a vector is also deﬁned for a list of matrices:\n",
      "diag(A1, A2, . . ., Ak)\n",
      "denotes the block diagonal matrix with submatrices A1, A2, . . ., Ak along the\n",
      "diagonal and zeros elsewhere.\n",
      "Inverses of Partitioned Matrices\n",
      "If A is nonsingular, and can be partitioned as\n",
      "A =\n",
      "\u0014\n",
      "A11 A12\n",
      "A21 A22\n",
      "\u0015\n",
      ",\n",
      "where both A11 and A22 are nonsingular, it is easy to see that the inverse of\n",
      "A is given by\n",
      "A−1 =\n",
      "\n",
      "\n",
      "A−1\n",
      "11 + A−1\n",
      "11 A12Z−1A21A−1\n",
      "11 −A−1\n",
      "11 A12Z−1\n",
      "−Z−1A21A−1\n",
      "11\n",
      "Z−1\n",
      "\n",
      ",\n",
      "where Z = A22 −A21A−1\n",
      "11 A12. In this partitioning Z is called the Schur com-\n",
      "plement of A11 in A.\n",
      "If\n",
      "A = [Xy]T [Xy]\n",
      "and is partitioned as into XTX and yTy on the diagonal, and X is of full\n",
      "column rank, then the Schur complement of XTX in [Xy]T [Xy] is\n",
      "yTy −yTX(XTX)−1XTy.\n",
      "This particular partitioning is useful in linear regression analysis, where this\n",
      "Schur complement is the residual sum of squares.\n",
      "Gramian Matrices and Generalized Inverses\n",
      "A matrix of the form ZTZ is called a Gramian matrix. Such matrices arise\n",
      "often in statistical applications.\n",
      "Some interesting properties of a Gramian matrix ZTZ are\n",
      "•\n",
      "ZTZ is symmetric;\n",
      "•\n",
      "ZTZ is of full rank if and only if Z is of full column rank, or, more generally,\n",
      "rank(ZTZ) = rank(Z);\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "800\n",
      "0 Statistical Mathematics\n",
      "•\n",
      "ZTZ is nonnegative deﬁnite, and positive deﬁnite if and only if Z is of full\n",
      "column rank;\n",
      "•\n",
      "ZTZ = 0\n",
      "=⇒\n",
      "Z = 0.\n",
      "The generalized inverses of ZTZ have useful properties. First, we see from\n",
      "the deﬁnition, for any generalized inverse, (ZTZ)−that ((ZTZ)−)T is also a\n",
      "generalized inverse of ZTZ. (Note that (ZTZ)−is not necessarily symmetric.)\n",
      "Another useful property of a Gramian matrix is that for any matrices B\n",
      "and C (that are conformable for the operations indicated),\n",
      "BZTZ = CZTZ\n",
      "⇐⇒\n",
      "BZT = CZT.\n",
      "The implication from right to left is obvious, and we can see the left to right\n",
      "implication by writing\n",
      "(BZTZ −CZTZ)(BT −CT) = (BZT −CZT)(BZT −CZT)T,\n",
      "and then observing that if the left side is null, then so is the right side, and if\n",
      "the right side is null, then BZT −CZT = 0. Similarly, we have\n",
      "ZTZB = ZTZC\n",
      "⇐⇒\n",
      "ZTB = ZTC.\n",
      "Also,\n",
      "Z(ZTZ)−ZTZ = Z.\n",
      "This means that (ZTZ)−ZT is a generalized inverse of Z\n",
      "An important property of Z(ZTZ)−ZT is its invariance to the choice of\n",
      "the generalized inverse of ZTZ. Suppose G is any generalized inverse of ZTZ.\n",
      "Then we have\n",
      "ZGZT = Z(ZTZ)−ZT;\n",
      "that is, Z(ZTZ)−ZT is invariant to choice of generalized inverse.\n",
      "The squared norm of the residual vector obtained from any generalized\n",
      "inverse of ZTZ has some interesting properties. First, just by direct multi-\n",
      "plication, we get the “Pythagorean property” of the norm of the predicted\n",
      "values and the residuals:\n",
      "∥X −Zβ∥2 = ∥X −Z bβ∥2 + ∥Z bβ −Zβ∥2\n",
      "where bβ = (ZTZ)−ZTX for any generalized inverse. We also have\n",
      "E(Z bβ) = Zβ,\n",
      "and\n",
      "E((Z bβ −Zβ)T(Z bβ −Zβ)) = V(Z bβ).\n",
      "Because for any vector y, we have\n",
      "∥y∥2 = yTy = tr(yTy),\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "801\n",
      "we can derive an interesting expression for E(∥X −Zβ∥2):\n",
      "E(∥X −Z bβ∥2) = tr(E(∥X −Z bβ∥2))\n",
      "= tr(E((X −Zβ)T(X −Zβ)) −E((Z bβ −Zβ)T(Z bβ −Zβ)))\n",
      "= tr(V(X) −V(Z bβ)\n",
      "= nσ2 −tr((Z(ZTZ)−ZT)σ2I(Z(ZTZ)−ZT))\n",
      "= σ2(n −tr((ZTZ)−ZTZ)).\n",
      "The trace in the latter expression is the “regression degrees of freedom”.\n",
      "The Moore-Penrose Inverse\n",
      "The Moore-Penrose inverse, or the pseudoinverse, of Z has an interesting\n",
      "relationship with a generalized inverse of ZTZ:\n",
      "ZZ+ = Z(ZTZ)−ZT.\n",
      "This can be established directly from the deﬁnition of the Moore-Penrose\n",
      "inverse.\n",
      "0.3.3 Vector/Matrix Derivatives and Integrals\n",
      "The operations of diﬀerentiation and integration of vectors and matrices are\n",
      "logical extensions of the corresponding operations on scalars. There are three\n",
      "objects involved in this operation:\n",
      "•\n",
      "the variable of the operation;\n",
      "•\n",
      "the operand (the function being diﬀerentiated or integrated); and\n",
      "•\n",
      "the result of the operation.\n",
      "In the simplest case, all three of these objects are of the same type, and\n",
      "they are scalars. If either the variable or the operand is a vector or a matrix,\n",
      "however, the structure of the result may be more complicated. This statement\n",
      "will become clearer as we proceed to consider speciﬁc cases.\n",
      "In this section, we state or show the form that the derivative takes in\n",
      "terms of simpler derivatives. We state high-level rules for the nature of the\n",
      "diﬀerentiation in terms of simple partial diﬀerentiation of a scalar with respect\n",
      "to a scalar. We do not consider whether or not the derivatives exist. In general,\n",
      "if the simpler derivatives we write that comprise the more complicated object\n",
      "exist, then the derivative of that more complicated object exists. Once a shape\n",
      "of the derivative is determined, deﬁnitions or derivations in ϵ-δ terms could\n",
      "be given, but we will refrain from that kind of formal exercise. The purpose\n",
      "of this section is not to develop a calculus for vectors and matrices but rather\n",
      "to consider some cases that ﬁnd wide applications in statistics. For a more\n",
      "careful treatment of diﬀerentiation of vectors and matrices see Gentle (2007).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "802\n",
      "0 Statistical Mathematics\n",
      "Basics of Diﬀerentiation\n",
      "It is useful to recall the heuristic interpretation of a derivative. A derivative\n",
      "of a function is the inﬁnitesimal rate of change of the function with respect\n",
      "to the variable with which the diﬀerentiation is taken. If both the function\n",
      "and the variable are scalars, this interpretation is unambiguous. If, however,\n",
      "the operand of the diﬀerentiation, Φ, is a more complicated function, say a\n",
      "vector or a matrix, and/or the variable of the diﬀerentiation, Ξ, is a more\n",
      "complicated object, the changes are more diﬃcult to measure. Change in the\n",
      "value both of the function,\n",
      "δΦ = Φnew −Φold,\n",
      "and of the variable,\n",
      "δΞ = Ξnew −Ξold,\n",
      "could be measured in various ways, by using various norms, for example. (Note\n",
      "that the subtraction is not necessarily ordinary scalar subtraction.)\n",
      "Furthermore, we cannot just divide the function values by δΞ. We do not\n",
      "have a deﬁnition for division by that kind of object. We need a mapping,\n",
      "possibly a norm, that assigns a positive real number to δΞ. We can deﬁne\n",
      "the change in the function value as just the simple diﬀerence of the function\n",
      "evaluated at the two points. This yields\n",
      "lim\n",
      "∥δΞ∥→0\n",
      "Φ(Ξ + δΞ) −Φ(Ξ)\n",
      "∥δΞ∥\n",
      ".\n",
      "(0.3.38)\n",
      "So long as we remember the complexity of δΞ, however, we can adopt a\n",
      "simpler approach. Since for both vectors and matrices, we have deﬁnitions of\n",
      "multiplication by a scalar and of addition, we can simplify the limit in the\n",
      "usual deﬁnition of a derivative, δΞ →0. Instead of using δΞ as the element\n",
      "of change, we will use tΥ, where t is a scalar and Υ is an element to be added\n",
      "to Ξ. The limit then will be taken in terms of t →0. This leads to\n",
      "lim\n",
      "t→0\n",
      "Φ(Ξ + tΥ) −Φ(Ξ)\n",
      "t\n",
      "(0.3.39)\n",
      "as a formula for the derivative of Φ with respect to Ξ.\n",
      "The expression (0.3.39) may be a useful formula for evaluating a derivative,\n",
      "but we must remember that it is not the derivative. The type of object of this\n",
      "formula is the same as the type of object of the function, Φ; it does not\n",
      "accommodate the type of object of the argument, Ξ, unless Ξ is a scalar. As\n",
      "we will see below, for example, if Ξ is a vector and Φ is a scalar, the derivative\n",
      "must be a vector, yet in that case the expression (0.3.39) is a scalar.\n",
      "The expression (0.3.38) is rarely directly useful in evaluating a derivative,\n",
      "but it serves to remind us of both the generality and the complexity of the con-\n",
      "cept. Both Φ and its arguments could be functions, for example. In functional\n",
      "analysis, various kinds of functional derivatives are deﬁned, such as a Gˆateaux\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "803\n",
      "derivative. These derivatives ﬁnd applications in developing robust statistical\n",
      "methods. Here we are just interested in the combinations of three possibilities\n",
      "for Φ, namely scalar, vector, and matrix, and the same three possibilities for\n",
      "Ξ and Υ.\n",
      "Continuity\n",
      "It is clear from the deﬁnition of continuity that for the derivative of a function\n",
      "to exist at a point, the function must be continuous at that point. A function\n",
      "of a vector or a matrix is continuous if it is continuous for each element\n",
      "of the vector or matrix. Just as scalar sums and products are continuous,\n",
      "vector/matrix sums and all of the types of vector/matrix products we have\n",
      "discussed are continuous. A continuous function of a continuous function is\n",
      "continuous.\n",
      "Many of the vector/matrix functions we have discussed are clearly contin-\n",
      "uous. For example, the Lp vector norms are continuous over the nonnegative\n",
      "reals but not over the reals unless p is an even (positive) integer. The determi-\n",
      "nant of a matrix is continuous, as we see from the deﬁnition of the determinant\n",
      "and the fact that sums and scalar products are continuous. The fact that the\n",
      "determinant is a continuous function immediately yields the result that co-\n",
      "factors and hence the adjugate are continuous. From the relationship between\n",
      "an inverse and the adjugate, we see that the inverse is a continuous function.\n",
      "Notation and Properties\n",
      "We write the diﬀerential operator with respect to the dummy variable x as\n",
      "∂/∂x or ∂/∂xT. We usually denote diﬀerentiation using the symbol for “par-\n",
      "tial” diﬀerentiation, ∂, whether the operator is written ∂xi for diﬀerentiation\n",
      "with respect to a speciﬁc scalar variable or ∂x for diﬀerentiation with respect\n",
      "to the array x that contains all of the individual elements. Sometimes, how-\n",
      "ever, if the diﬀerentiation is being taken with respect to the whole array (the\n",
      "vector or the matrix), we use the notation d/dx.\n",
      "The operand of the diﬀerential operator ∂/∂x is a function of x. (If it\n",
      "is not a function of x — that is, if it is a constant function with respect to\n",
      "x — then the operator evaluates to 0.) The result of the operation, written\n",
      "∂f/∂x, is also a function of x, with the same domain as f, and we sometimes\n",
      "write ∂f(x)/∂x to emphasize this fact. The value of this function at the ﬁxed\n",
      "point x0 is written as ∂f(x0)/∂x. (The derivative of the constant f(x0) is\n",
      "identically 0, but it is not necessary to write ∂f(x)/∂x|x0 because ∂f(x0)/∂x\n",
      "is interpreted as the value of the function ∂f(x)/∂x at the ﬁxed point x0.)\n",
      "If ∂/∂x operates on f, and f : S →T, then ∂/∂x : S →U. The nature\n",
      "of S, or more directly the nature of x, whether it is a scalar, a vector, or\n",
      "a matrix, and the nature of T determine the structure of the result U. For\n",
      "example, if x is an n-vector and f(x) = xTx, then\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "804\n",
      "0 Statistical Mathematics\n",
      "f : IRn →IR\n",
      "and\n",
      "∂f/∂x : IRn →IRn,\n",
      "as we will see. The outer product, h(x) = xxT, is a mapping to a higher rank\n",
      "array, but the derivative of the outer product is a mapping to an array of the\n",
      "same rank; that is,\n",
      "h : IRn →IRn×n\n",
      "and\n",
      "∂h/∂x : IRn →IRn.\n",
      "(Note that “rank” here means the number of dimensions. This term is often\n",
      "used in this way in numerical software. See Gentle (2007), page 5.)\n",
      "As another example, consider g(·) = det(·), so\n",
      "g : IRn×n 7→IR.\n",
      "In this case,\n",
      "∂g/∂X : IRn×n 7→IRn×n;\n",
      "that is, the derivative of the determinant of a square matrix is a square matrix,\n",
      "as we will see later.\n",
      "Higher-order diﬀerentiation is a composition of the ∂/∂x operator with\n",
      "itself or of the ∂/∂x operator and the ∂/∂xT operator. For example, consider\n",
      "the familiar function in linear least squares\n",
      "f(b) = (y −Xb)T(y −Xb).\n",
      "This is a mapping from IRm to IR. The ﬁrst derivative with respect to the m-\n",
      "vector b is a mapping from IRm to IRm, namely 2XTXb −2XTy. The second\n",
      "derivative with respect to bT is a mapping from IRm to IRm×m, namely, 2XTX.\n",
      "We see from expression (0.3.38) that diﬀerentiation is a linear operator;\n",
      "that is, if D(Φ) represents the operation deﬁned in expression (0.3.38), Ψ is an-\n",
      "other function in the class of functions over which D is deﬁned, and a is a scalar\n",
      "that does not depend on the variable Ξ, then D(aΦ + Ψ) = aD(Φ) + D(Ψ).\n",
      "This yields the familiar rules of diﬀerential calculus for derivatives of sums or\n",
      "constant scalar products. Other usual rules of diﬀerential calculus apply, such\n",
      "as for diﬀerentiation of products and composition (the chain rule). We can\n",
      "use expression (0.3.39) to work these out. For example, for the derivative of\n",
      "the product ΦΨ, after some rewriting of terms, we have the numerator\n",
      "Φ(Ξ)\n",
      "\u0000Ψ(Ξ + tΥ) −Ψ(Ξ)\n",
      "\u0001\n",
      "+ Ψ(Ξ)\u0000Φ(Ξ + tΥ) −Φ(Ξ)\u0001\n",
      "+\n",
      "\u0000Φ(Ξ + tΥ) −Φ(Ξ)\n",
      "\u0001\u0000Ψ(Ξ + tΥ) −Ψ(Ξ)\n",
      "\u0001\n",
      ".\n",
      "Now, dividing by t and taking the limit, assuming that as\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "805\n",
      "t →0,\n",
      "(Φ(Ξ + tΥ) −Φ(Ξ)) →0,\n",
      "we have\n",
      "D(ΦΨ) = D(Φ)Ψ + ΦD(Ψ),\n",
      "(0.3.40)\n",
      "where again D represents the diﬀerentiation operation.\n",
      "Diﬀerentials\n",
      "For a diﬀerentiable scalar function of a scalar variable, f(x), the diﬀerential\n",
      "of f at c with increment u is udf/dx|c. This is the linear term in a truncated\n",
      "Taylor series expansion:\n",
      "f(c + u) = f(c) + u d\n",
      "dxf(c) + r(c, u).\n",
      "(0.3.41)\n",
      "Technically, the diﬀerential is a function of both x and u, but the notation\n",
      "df is used in a generic sense to mean the diﬀerential of f. For vector/matrix\n",
      "functions of vector/matrix variables, the diﬀerential is deﬁned in a similar\n",
      "way. The structure of the diﬀerential is the same as that of the function; that\n",
      "is, for example, the diﬀerential of a matrix-valued function is a matrix.\n",
      "Types of Diﬀerentiation\n",
      "In the following sections we consider diﬀerentiation with respect to diﬀerent\n",
      "types of objects ﬁrst, and we consider diﬀerentiation of diﬀerent types of\n",
      "objects.\n",
      "Diﬀerentiation with Respect to a Scalar\n",
      "Diﬀerentiation of a structure (vector or matrix, for example) with respect to\n",
      "a scalar is quite simple; it just yields the ordinary derivative of each element\n",
      "of the structure in the same structure. Thus, the derivative of a vector or a\n",
      "matrix with respect to a scalar variable is a vector or a matrix, respectively,\n",
      "of the derivatives of the individual elements.\n",
      "Diﬀerentiation with respect to a vector or matrix, which we will consider\n",
      "below, is often best approached by considering diﬀerentiation with respect to\n",
      "the individual elements of the vector or matrix, that is, with respect to scalars.\n",
      "Derivatives of Vectors with Respect to Scalars\n",
      "The derivative of the vector y(x) = (y1, . . ., yn) with respect to the scalar x\n",
      "is the vector\n",
      "∂y/∂x = (∂y1/∂x, . . ., ∂yn/∂x).\n",
      "(0.3.42)\n",
      "The second or higher derivative of a vector with respect to a scalar is\n",
      "likewise a vector of the derivatives of the individual elements; that is, it is an\n",
      "array of higher rank.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "806\n",
      "0 Statistical Mathematics\n",
      "Derivatives of Matrices with Respect to Scalars\n",
      "The derivative of the matrix Y (x) = (yij) with respect to the scalar x is the\n",
      "matrix\n",
      "∂Y (x)/∂x = (∂yij/∂x).\n",
      "(0.3.43)\n",
      "The second or higher derivative of a matrix with respect to a scalar is\n",
      "likewise a matrix of the derivatives of the individual elements.\n",
      "Derivatives of Functions with Respect to Scalars\n",
      "Diﬀerentiation of a function of a vector or matrix that is linear in the elements\n",
      "of the vector or matrix involves just the diﬀerentiation of the elements, fol-\n",
      "lowed by application of the function. For example, the derivative of a trace of\n",
      "a matrix is just the trace of the derivative of the matrix. On the other hand,\n",
      "the derivative of the determinant of a matrix is not the determinant of the\n",
      "derivative of the matrix (see below).\n",
      "Higher-Order Derivatives with Respect to Scalars\n",
      "Because diﬀerentiation with respect to a scalar does not change the rank\n",
      "of the object (“rank” here means rank of an array or “shape”), higher-order\n",
      "derivatives ∂k/∂xk with respect to scalars are merely objects of the same rank\n",
      "whose elements are the higher-order derivatives of the individual elements.\n",
      "Diﬀerentiation with Respect to a Vector\n",
      "Diﬀerentiation of a given object with respect to an n-vector yields a vector\n",
      "for each element of the given object. The basic expression for the derivative,\n",
      "from formula (0.3.39), is\n",
      "lim\n",
      "t→0\n",
      "Φ(x + ty) −Φ(x)\n",
      "t\n",
      "(0.3.44)\n",
      "for an arbitrary conformable vector y. The arbitrary y indicates that the\n",
      "derivative is omnidirectional; it is the rate of change of a function of the\n",
      "vector in any direction.\n",
      "Derivatives of Scalars with Respect to Vectors; The Gradient\n",
      "The derivative of a scalar-valued function with respect to a vector is a vector\n",
      "of the partial derivatives of the function with respect to the elements of the\n",
      "vector. If f(x) is a scalar function of the vector x = (x1, . . ., xn),\n",
      "∂f\n",
      "∂x =\n",
      "\u0012 ∂f\n",
      "∂x1\n",
      ", . . ., ∂f\n",
      "∂xn\n",
      "\u0013\n",
      ",\n",
      "(0.3.45)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "807\n",
      "if those derivatives exist. This vector is called the gradient of the scalar-valued\n",
      "function, and is sometimes denoted by gf(x) or ∇f(x), or sometimes just gf\n",
      "or ∇f:\n",
      "gf = ∇f = ∂f\n",
      "∂x.\n",
      "(0.3.46)\n",
      "The notation gf or ∇f implies diﬀerentiation with respect to “all” arguments\n",
      "of f, hence, if f is a scalar-valued function of a vector argument, they represent\n",
      "a vector.\n",
      "This derivative is useful in ﬁnding the maximum or minimum of a function.\n",
      "Such applications arise throughout statistical and numerical analysis.\n",
      "Inner products, bilinear forms, norms, and variances are interesting scalar-\n",
      "valued functions of vectors. In these cases, the function Φ in equation (0.3.44)\n",
      "is scalar-valued and the numerator is merely Φ(x + ty) −Φ(x). Consider,\n",
      "for example, the quadratic form xTAx. Using equation (0.3.44) to evaluate\n",
      "∂xTAx/∂x, we have\n",
      "lim\n",
      "t→0\n",
      "(x + ty)TA(x + ty) −xTAx\n",
      "t\n",
      "= lim\n",
      "t→0\n",
      "xTAx + tyTAx + tyTATx + t2yTAy −xTAx\n",
      "t\n",
      "= yT(A + AT)x,\n",
      "(0.3.47)\n",
      "for an arbitrary y (that is, “in any direction”), and so ∂xTAx/∂x = (A+AT)x.\n",
      "This immediately yields the derivative of the square of the Euclidean norm\n",
      "of a vector, ∥x∥2\n",
      "2, and the derivative of the Euclidean norm itself by using\n",
      "the chain rule. Other Lp vector norms may not be diﬀerentiable everywhere\n",
      "because of the presence of the absolute value in their deﬁnitions. The fact that\n",
      "the Euclidean norm is diﬀerentiable everywhere is one of its most important\n",
      "properties.\n",
      "The derivative of the quadratic form also immediately yields the derivative\n",
      "of the variance. The derivative of the correlation, however, is slightly more\n",
      "diﬃcult because it is a ratio.\n",
      "The operator ∂/∂xT applied to the scalar function f results in gT\n",
      "f .\n",
      "The second derivative of a scalar-valued function with respect to a vector\n",
      "is a derivative of the ﬁrst derivative, which is a vector. We will now consider\n",
      "derivatives of vectors with respect to vectors.\n",
      "Derivatives of Vectors with Respect to Vectors; The Jacobian\n",
      "The derivative of an m-vector-valued function of an n-vector argument con-\n",
      "sists of nm scalar derivatives. These derivatives could be put into various\n",
      "structures. Two obvious structures are an n×m matrix and an m×n matrix.\n",
      "For a function f : S ⊆IRn →IRm, we deﬁne ∂fT/∂x to be the n × m ma-\n",
      "trix, which is the natural extension of ∂/∂x applied to a scalar function, and\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "808\n",
      "0 Statistical Mathematics\n",
      "∂f/∂xT to be its transpose, the m×n matrix. Although the notation ∂fT/∂x\n",
      "is more precise because it indicates that the elements of f correspond to the\n",
      "columns of the result, we often drop the transpose in the notation. We have\n",
      "∂f\n",
      "∂x = ∂fT\n",
      "∂x\n",
      "by convention\n",
      "=\n",
      "\u0014∂f1\n",
      "∂x . . . ∂fm\n",
      "∂x\n",
      "\u0015\n",
      "=\n",
      "\n",
      "\n",
      "∂f1\n",
      "∂x1\n",
      "∂f2\n",
      "∂x1 · · ·\n",
      "∂fm\n",
      "∂x1\n",
      "∂f1\n",
      "∂x2\n",
      "∂f2\n",
      "∂x2 · · ·\n",
      "∂fm\n",
      "∂x2\n",
      "· · ·\n",
      "∂f1\n",
      "∂xn\n",
      "∂f2\n",
      "∂xn · · ·\n",
      "∂fm\n",
      "∂xn\n",
      "\n",
      "\n",
      "(0.3.48)\n",
      "if those derivatives exist. This derivative is called the matrix gradient and\n",
      "is denoted by Gf or ∇f for the vector-valued function f. (Note that the ∇\n",
      "symbol can denote either a vector or a matrix, depending on whether the\n",
      "function being diﬀerentiated is scalar-valued or vector-valued.)\n",
      "The m × n matrix ∂f/∂xT = (∇f)T is called the Jacobian of f and is\n",
      "denoted by Jf:\n",
      "Jf = GT\n",
      "f = (∇f)T.\n",
      "(0.3.49)\n",
      "The absolute value of the determinant of the Jacobian appears in integrals\n",
      "involving a change of variables. (Occasionally, the term “Jacobian” is used\n",
      "to refer to the absolute value of the determinant rather than to the matrix\n",
      "itself.)\n",
      "To emphasize that the quantities are functions of x, we sometimes write\n",
      "∂f(x)/∂x, Jf(x), Gf(x), or ∇f(x).\n",
      "Derivatives of Matrices with Respect to Vectors\n",
      "The derivative of a matrix with respect to a vector is a three-dimensional\n",
      "object that results from applying equation (0.3.45) to each of the elements of\n",
      "the matrix. For this reason, it is simpler to consider only the partial derivatives\n",
      "of the matrix Y with respect to the individual elements of the vector x; that\n",
      "is, ∂Y/∂xi. The expressions involving the partial derivatives can be thought\n",
      "of as deﬁning one two-dimensional layer of a three-dimensional object.\n",
      "Using the rules for diﬀerentiation of powers that result directly from the\n",
      "deﬁnitions, we can write the partial derivatives of the inverse of the matrix Y\n",
      "as\n",
      "∂\n",
      "∂xY −1 = −Y −1\n",
      "\u0012 ∂\n",
      "∂xY\n",
      "\u0013\n",
      "Y −1.\n",
      "(0.3.50)\n",
      "Beyond the basics of diﬀerentiation of constant multiples or powers of a\n",
      "variable, the two most important properties of derivatives of expressions are\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "809\n",
      "the linearity of the operation and the chaining of the operation. These yield\n",
      "rules that correspond to the familiar rules of the diﬀerential calculus. A simple\n",
      "result of the linearity of the operation is the rule for diﬀerentiation of the trace:\n",
      "∂\n",
      "∂xtr(Y ) = tr\n",
      "\u0012 ∂\n",
      "∂xY\n",
      "\u0013\n",
      ".\n",
      "Higher-Order Derivatives with Respect to Vectors; The Hessian\n",
      "Higher-order derivatives are derivatives of lower-order derivatives. As we have\n",
      "seen, a derivative of a given function with respect to a vector is a more compli-\n",
      "cated object than the original function. The simplest higher-order derivative\n",
      "with respect to a vector is the second-order derivative of a scalar-valued func-\n",
      "tion. Higher-order derivatives may become uselessly complicated.\n",
      "In accordance with the meaning of derivatives of vectors with respect to\n",
      "vectors, the second derivative of a scalar-valued function with respect to a\n",
      "vector is a matrix of the partial derivatives of the function with respect to the\n",
      "elements of the vector. This matrix is called the Hessian, and is denoted by\n",
      "Hf or sometimes by ∇∇f or ∇2f:\n",
      "Hf =\n",
      "∂2f\n",
      "∂x∂xT =\n",
      "\n",
      "\n",
      "∂2f\n",
      "∂x2\n",
      "1\n",
      "∂2f\n",
      "∂x1∂x2 · · ·\n",
      "∂2f\n",
      "∂x1∂xm\n",
      "∂2f\n",
      "∂x2∂x1\n",
      "∂2f\n",
      "∂x2\n",
      "2\n",
      "· · ·\n",
      "∂2f\n",
      "∂x2∂xm\n",
      "· · ·\n",
      "∂2f\n",
      "∂xm∂x1\n",
      "∂2f\n",
      "∂xm∂x2 · · ·\n",
      "∂2f\n",
      "∂x2m\n",
      "\n",
      "\n",
      ".\n",
      "(0.3.51)\n",
      "The Hessian is a function of x. We write Hf(x) or ∇∇f(x) or ∇2f(x) for\n",
      "the value of the Hessian at x.\n",
      "Summary of Derivatives with Respect to Vectors\n",
      "As we have seen, the derivatives of functions are complicated by the problem\n",
      "of measuring the change in the function, but often the derivatives of functions\n",
      "with respect to a vector can be determined by using familiar scalar diﬀeren-\n",
      "tiation. In general, we see that\n",
      "•\n",
      "the derivative of a scalar (a quadratic form) with respect to a vector is a\n",
      "vector and\n",
      "•\n",
      "the derivative of a vector with respect to a vector is a matrix.\n",
      "Table 0.3 lists formulas for the vector derivatives of some common expres-\n",
      "sions. The derivative ∂f/∂xT is the transpose of ∂f/∂x.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "810\n",
      "0 Statistical Mathematics\n",
      "Table 0.3. Formulas for Some Vector Derivatives\n",
      "f(x)\n",
      "∂f/∂x\n",
      "ax\n",
      "a\n",
      "bTx\n",
      "b\n",
      "xTb\n",
      "bT\n",
      "xTx\n",
      "2x\n",
      "xxT\n",
      "2xT\n",
      "bTAx\n",
      "ATb\n",
      "xTAb\n",
      "bTA\n",
      "xTAx\n",
      "(A + AT)x\n",
      "2Ax, if A is symmetric\n",
      "exp(−1\n",
      "2 xTAx)\n",
      "−exp(−1\n",
      "2xTAx)Ax, if A is symmetric\n",
      "∥x∥2\n",
      "2\n",
      "2x\n",
      "V(x)\n",
      "2x/(n −1)\n",
      "In this table, x is an n-vector, a is a constant scalar, b is a\n",
      "constant conformable vector, and A is a constant conformable\n",
      "matrix.\n",
      "Diﬀerentiation with Respect to a Matrix\n",
      "The derivative of a function with respect to a matrix is a matrix with the same\n",
      "shape consisting of the partial derivatives of the function with respect to the\n",
      "elements of the matrix. This rule deﬁnes what we mean by diﬀerentiation with\n",
      "respect to a matrix.\n",
      "By the deﬁnition of diﬀerentiation with respect to a matrix X, we see that\n",
      "the derivative ∂f/∂XT is the transpose of ∂f/∂X. For scalar-valued functions,\n",
      "this rule is fairly simple. For example, consider the trace. If X is a square ma-\n",
      "trix and we apply this rule to evaluate ∂tr(X)/∂X, we get the identity ma-\n",
      "trix, where the nonzero elements arise only when j = i in ∂(P xii)/∂xij.\n",
      "If AX is a square matrix, we have for the (i, j) term in ∂tr(AX)/∂X,\n",
      "∂P\n",
      "i\n",
      "P\n",
      "k aikxki/∂xij = aji, and so ∂tr(AX)/∂X = AT, and likewise, in-\n",
      "specting ∂P\n",
      "i\n",
      "P\n",
      "k xikxki/∂xij, we get ∂tr(XTX)/∂X = 2XT. Likewise for\n",
      "the scalar-valued aTXb, where a and b are conformable constant vectors, for\n",
      "∂P\n",
      "m(P\n",
      "k akxkm)bm/∂xij = aibj, so ∂aTXb/∂X = abT.\n",
      "Now consider ∂|X|/∂X. Using an expansion in cofactors, the only term\n",
      "in |X| that involves xij is xij(−1)i+j|X−(i)(j)|, and the cofactor (x(ij)) =\n",
      "(−1)i+j|X−(i)(j)| does not involve xij. Hence, ∂|X|/∂xij = (x(ij)), and so\n",
      "∂|X|/∂X = (adj(X))T. We can write this as ∂|X|/∂X = |X|X−T.\n",
      "The chain rule can be used to evaluate ∂log |X|/∂X.\n",
      "Applying the rule stated at the beginning of this section, we see that the\n",
      "derivative of a matrix Y with respect to the matrix X is\n",
      "dY\n",
      "dX = Y ⊗\n",
      "d\n",
      "dX .\n",
      "(0.3.52)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "811\n",
      "Table 0.4 lists some formulas for the matrix derivatives of some common\n",
      "expressions. The derivatives shown in Table 0.4 can be obtained by evaluating\n",
      "expression (0.3.52), possibly also using the chain rule.\n",
      "Table 0.4. Formulas for Some Matrix Derivatives\n",
      "General X\n",
      "f(X)\n",
      "∂f/∂X\n",
      "aTXb\n",
      "abT\n",
      "tr(AX)\n",
      "AT\n",
      "tr(XTX)\n",
      "2XT\n",
      "BX\n",
      "In ⊗B\n",
      "XC\n",
      "CT ⊗Im\n",
      "BXC\n",
      "CT ⊗B\n",
      "Square and Possibly Invertible X\n",
      "f(X)\n",
      "∂f/∂X\n",
      "tr(X)\n",
      "In\n",
      "tr(Xk)\n",
      "kXk−1\n",
      "tr(BX−1C)\n",
      "−(X−1CBX−1)T\n",
      "|X|\n",
      "|X|X−T\n",
      "log |X|\n",
      "X−T\n",
      "|X|k\n",
      "k|X|kX−T\n",
      "BX−1C\n",
      "−(X−1C)T ⊗BX−1\n",
      "In this table, X is an n × m matrix, a is a\n",
      "constant n-vector, b is a constant m-vector,\n",
      "A is a constant m×n matrix, B is a constant\n",
      "p×n matrix, and C is a constant m×q matrix.\n",
      "There are some interesting applications of diﬀerentiation with respect to\n",
      "a matrix in maximum likelihood estimation. Depending on the structure of\n",
      "the parameters in the distribution, derivatives of various types of objects may\n",
      "be required. For example, the determinant of a variance-covariance matrix, in\n",
      "the sense that it is a measure of a volume, often occurs as a normalizing factor\n",
      "in a probability density function; therefore, we often encounter the need to\n",
      "diﬀerentiate a determinant with respect to a matrix.\n",
      "0.3.4 Optimization of Functions\n",
      "*** move this to Appendix on Optimization ***\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "812\n",
      "0 Statistical Mathematics\n",
      "Because a derivative measures the rate of change of a function, a point\n",
      "at which the derivative is equal to 0 is a stationary point, which may be a\n",
      "maximum or a minimum of the function. Diﬀerentiation is therefore a very\n",
      "useful tool for ﬁnding the optima of functions, and so, for a given function\n",
      "f(x), the gradient vector function, gf(x), and the Hessian matrix function,\n",
      "Hf(x), play important roles in optimization methods.\n",
      "We may seek either a maximum or a minimum of a function. Since max-\n",
      "imizing the scalar function f(x) is equivalent to minimizing −f(x), we can\n",
      "always consider optimization of a function to be minimization of a function.\n",
      "Thus, we generally use terminology for the problem of ﬁnding a minimum of\n",
      "a function. Because the function may have many ups and downs, we often use\n",
      "the phrase local minimum (or local maximum or local optimum).\n",
      "Except in the very simplest of cases, the optimization method must be\n",
      "iterative, moving through a sequence of points, x(0), x(1), x(2), . . ., that ap-\n",
      "proaches the optimum point arbitrarily closely. At the point x(k), the direc-\n",
      "tion of steepest descent is clearly −gf(x(k)), but because this direction may\n",
      "be continuously changing, the steepest descent direction may not be the best\n",
      "direction in which to seek the next point, x(k+1).\n",
      "In the following subsection we describe some speciﬁc methods of optimiza-\n",
      "tion in the context of vector/matrix diﬀerentiation. We will discuss optimiza-\n",
      "tion in somewhat more detail in Section 0.4.\n",
      "Stationary Points of Functions\n",
      "The ﬁrst derivative helps only in ﬁnding a stationary point. The matrix of\n",
      "second derivatives, the Hessian, provides information about the nature of the\n",
      "stationary point, which may be a local minimum or maximum, a saddlepoint,\n",
      "or only an inﬂection point.\n",
      "The so-called second-order optimality conditions are the following (see a\n",
      "general text on optimization for their proofs).\n",
      "•\n",
      "If (but not only if) the stationary point is a local minimum, then the\n",
      "Hessian is nonnegative deﬁnite.\n",
      "•\n",
      "If the Hessian is positive deﬁnite, then the stationary point is a local\n",
      "minimum.\n",
      "•\n",
      "Likewise, if the stationary point is a local maximum, then the Hessian\n",
      "is nonpositive deﬁnite, and if the Hessian is negative deﬁnite, then the\n",
      "stationary point is a local maximum.\n",
      "•\n",
      "If the Hessian has both positive and negative eigenvalues, then the sta-\n",
      "tionary point is a saddlepoint.\n",
      "Newton’s Method\n",
      "We consider a diﬀerentiable scalar-valued function of a vector argument, f(x).\n",
      "By a Taylor series about a stationary point x∗, truncated after the second-\n",
      "order term\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "813\n",
      "f(x) ≈f(x∗) + (x −x∗)Tgf\n",
      "\u0000x∗\n",
      "\u0001 + 1\n",
      "2(x −x∗)THf\n",
      "\u0000x∗\n",
      "\u0001(x −x∗),\n",
      "(0.3.53)\n",
      "because gf\n",
      "\u0000x∗\n",
      "\u0001 = 0, we have a general method of ﬁnding a stationary point\n",
      "for the function f(·), called Newton’s method. If x is an m-vector, gf(x) is an\n",
      "m-vector and Hf(x) is an m × m matrix.\n",
      "Newton’s method is to choose a starting point x(0), then, for k = 0, 1, . . .,\n",
      "to solve the linear systems\n",
      "Hf\n",
      "\u0000x(k)\u0001\n",
      "p(k+1) = −gf\n",
      "\u0000x(k)\u0001\n",
      "(0.3.54)\n",
      "for p(k+1), and then to update the point in the domain of f(·) by\n",
      "x(k+1) = x(k) + p(k+1).\n",
      "(0.3.55)\n",
      "The two steps are repeated until there is essentially no change from one iter-\n",
      "ation to the next. If f(·) is a quadratic function, the solution is obtained in\n",
      "one iteration because equation (0.3.53) is exact. These two steps have a very\n",
      "simple form for a function of one variable.\n",
      "Linear Least Squares\n",
      "In a least squares ﬁt of a linear model\n",
      "y = Xβ + ϵ,\n",
      "(0.3.56)\n",
      "where y is an n-vector, X is an n×m matrix, and β is an m-vector, we replace\n",
      "β by a variable b, deﬁne the residual vector\n",
      "r = y −Xb,\n",
      "(0.3.57)\n",
      "and minimize its Euclidean norm,\n",
      "f(b) = rTr,\n",
      "(0.3.58)\n",
      "with respect to the variable b. We can solve this optimization problem by\n",
      "taking the derivative of this sum of squares and equating it to zero. Doing\n",
      "this, we get\n",
      "d(y −Xb)T(y −Xb)\n",
      "db\n",
      "= d(yTy −2bTXTy + bTXTXb)\n",
      "db\n",
      "= −2XTy + 2XTXb\n",
      "= 0,\n",
      "which yields the normal equations\n",
      "XTXb = XTy.\n",
      "(0.3.59)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "814\n",
      "0 Statistical Mathematics\n",
      "The solution to the normal equations is a stationary point of the func-\n",
      "tion (0.3.58). The Hessian of (y −Xb)T(y −Xb) with respect to b is 2XTX\n",
      "and\n",
      "XTX ⪰0.\n",
      "Because the matrix of second derivatives is nonnegative deﬁnite, the value of\n",
      "b that solves the system of equations arising from the ﬁrst derivatives is a\n",
      "local minimum of equation (0.3.58).\n",
      "Quasi-Newton Methods\n",
      "All gradient-descent methods determine the path p(k) to take in the kth step\n",
      "by a system of equations of the form\n",
      "R(k)p(k) = −gf\n",
      "\u0000x(k−1)\u0001\n",
      ".\n",
      "In the steepest-descent method, R(k) is the identity, I, in these equations.\n",
      "For functions with eccentric contours, the steepest-descent method traverses\n",
      "a zigzag path to the minimum. In Newton’s method, R(k) is the Hessian\n",
      "evaluated at the previous point, Hf\n",
      "\u0000x(k−1)\u0001, which results in a more direct\n",
      "path to the minimum. Aside from the issues of consistency of the resulting\n",
      "equation and the general problems of reliability, a major disadvantage of New-\n",
      "ton’s method is the computational burden of computing the Hessian, which\n",
      "requires O(m2) function evaluations, and solving the system, which requires\n",
      "O(m3) arithmetic operations, at each iteration.\n",
      "Instead of using the Hessian at each iteration, we may use an approxima-\n",
      "tion, B(k). We may choose approximations that are simpler to update and/or\n",
      "that allow the equations for the step to be solved more easily. Methods us-\n",
      "ing such approximations are called quasi-Newton methods or variable metric\n",
      "methods.\n",
      "Because\n",
      "Hf\n",
      "\u0000x(k)\u0001\u0000x(k) −x(k−1)\u0001\n",
      "≈gf\n",
      "\u0000x(k)\u0001\n",
      "−gf\n",
      "\u0000x(k−1)\u0001\n",
      ",\n",
      "we choose B(k) so that\n",
      "B(k)\u0000x(k) −x(k−1)\u0001\n",
      "= gf\n",
      "\u0000x(k)\u0001\n",
      "−gf\n",
      "\u0000x(k−1)\u0001\n",
      ".\n",
      "(0.3.60)\n",
      "This is called the secant condition.\n",
      "We express the secant condition as\n",
      "B(k)s(k) = y(k),\n",
      "(0.3.61)\n",
      "where\n",
      "s(k) = x(k) −x(k−1)\n",
      "and\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "815\n",
      "y(k) = gf(x(k)) −gf(x(k−1)),\n",
      "as above.\n",
      "The system of equations in (0.3.61) does not fully determine B(k) of course.\n",
      "Because B(k) should approximate the Hessian, we may require that it be\n",
      "symmetric and positive deﬁnite.\n",
      "The most common approach in quasi-Newton methods is ﬁrst to choose\n",
      "a reasonable starting matrix B(0) and then to choose subsequent matrices by\n",
      "additive updates,\n",
      "B(k+1) = B(k) + B(k)\n",
      "a ,\n",
      "(0.3.62)\n",
      "subject to preservation of symmetry and positive deﬁniteness. An approximate\n",
      "Hessian B(k) may be used for several iterations before it is updated; that is,\n",
      "B(k)\n",
      "a\n",
      "may be taken as 0 for several successive iterations.\n",
      "Multiparameter Likelihood Functions\n",
      "For a sample y = (y1, . . ., yn) from a probability distribution with probability\n",
      "density function p(·; θ), the likelihood function is\n",
      "L(θ; y) =\n",
      "n\n",
      "Y\n",
      "i=1\n",
      "p(yi; θ),\n",
      "(0.3.63)\n",
      "and the log-likelihood function is l(θ; y) = log(L(θ; y)). It is often easier to\n",
      "work with the log-likelihood function.\n",
      "The log-likelihood is an important quantity in information theory and\n",
      "in unbiased estimation. If Y is a random variable with the given probability\n",
      "density function with the r-vector parameter θ, the Fisher information matrix\n",
      "that Y contains about θ is the r × r matrix\n",
      "I(θ) = Covθ\n",
      "\u0012∂l(t, Y )\n",
      "∂ti\n",
      ", ∂l(t, Y )\n",
      "∂tj\n",
      "\u0013\n",
      ",\n",
      "(0.3.64)\n",
      "where Covθ represents the variance-covariance matrix of the functions of Y\n",
      "formed by taking expectations for the given θ. (I use diﬀerent symbols here\n",
      "because the derivatives are taken with respect to a variable, but the θ in Covθ\n",
      "cannot be the variable of the diﬀerentiation. This distinction is somewhat\n",
      "pedantic, and sometimes I follow the more common practice of using the\n",
      "same symbol in an expression that involves both Covθ and ∂l(θ, Y )/∂θi.)\n",
      "For example, if the distribution is the d-variate normal distribution with\n",
      "mean d-vector µ and d ×d positive deﬁnite variance-covariance matrix Σ, the\n",
      "likelihood, equation (0.3.63), is\n",
      "L(µ, Σ; y) =\n",
      "1\n",
      "\u0000(2π)d/2|Σ|1/2\u0001n exp\n",
      " \n",
      "−1\n",
      "2\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(yi −µ)TΣ−1(yi −µ)\n",
      "!\n",
      ".\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "816\n",
      "0 Statistical Mathematics\n",
      "(Note that |Σ|1/2 = |Σ\n",
      "1\n",
      "2 |. The square root matrix Σ\n",
      "1\n",
      "2 is often useful in\n",
      "transformations of variables.)\n",
      "Anytime we have a quadratic form that we need to simplify, we should\n",
      "recall the useful fact: xTAx = tr(AxxT). Using this, and because, as is often\n",
      "the case, the log-likelihood is easier to work with, we write\n",
      "l(µ, Σ; y) = c −n\n",
      "2 log |Σ| −1\n",
      "2tr\n",
      " \n",
      "Σ−1\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(yi −µ)(yi −µ)T\n",
      "!\n",
      ",\n",
      "(0.3.65)\n",
      "where we have used c to represent the constant portion. Next, we use the\n",
      "“Pythagorean equation” on the outer product to get\n",
      "l(µ, Σ; y) = c −n\n",
      "2 log |Σ| −1\n",
      "2tr\n",
      " \n",
      "Σ−1\n",
      "n\n",
      "X\n",
      "i=1\n",
      "(yi −¯y)(yi −¯y)T\n",
      "!\n",
      "−n\n",
      "2 tr \u0000Σ−1(¯y −µ)(¯y −µ)T\u0001 . (0.3.66)\n",
      "In maximum likelihood estimation, we seek the maximum of the likelihood\n",
      "function (0.3.63) with respect to θ while we consider y to be ﬁxed. If the\n",
      "maximum occurs within an open set and if the likelihood is diﬀerentiable, we\n",
      "might be able to ﬁnd the maximum likelihood estimates by diﬀerentiation.\n",
      "In the log-likelihood for the d-variate normal distribution, we consider the\n",
      "parameters µ and Σ to be variables. To emphasize that perspective, we replace\n",
      "the parameters µ and Σ by the variables ˆµ and bΣ. Now, to determine the\n",
      "maximum, we could take derivatives with respect to ˆµ and bΣ, set them equal\n",
      "to 0, and solve for the maximum likelihood estimates. Some subtle problems\n",
      "arise that depend on the fact that for any constant vector a and scalar b,\n",
      "Pr(aTX = b) = 0, but we do not interpret the likelihood as a probability.\n",
      "Often in working out maximum likelihood estimates, students immediately\n",
      "think of diﬀerentiating, setting to 0, and solving. As noted above, this requires\n",
      "that the likelihood function be diﬀerentiable, that it be concave, and that the\n",
      "maximum occur at an interior point of the parameter space. Keeping in mind\n",
      "exactly what the problem is — one of ﬁnding a maximum— often leads to the\n",
      "correct solution more quickly.\n",
      "0.3.5 Vector Random Variables\n",
      "The simplest kind of vector random variable is one whose elements are in-\n",
      "dependent. Such random vectors are easy to work with because the elements\n",
      "can be dealt with individually, but they have limited applications. More in-\n",
      "teresting random vectors have a multivariate structure that depends on the\n",
      "relationships of the distributions of the individual elements. The simplest non-\n",
      "degenerate multivariate structure is of second degree; that is, a covariance or\n",
      "correlation structure. The probability density of a random vector with a mul-\n",
      "tivariate structure generally is best represented by using matrices. In the case\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "817\n",
      "of the multivariate normal distribution, the variances and covariances together\n",
      "with the means completely characterize the distribution. For example, the fun-\n",
      "damental integral that is associated with the d-variate normal distribution,\n",
      "sometimes called Aitken’s integral, equation (0.0.88) on page 682, provides\n",
      "that constant. The rank of the integral is the same as the rank of the inte-\n",
      "grand. (“Rank” is used here in the sense of “number of dimensions”.) In this\n",
      "case, the integrand and the integral are scalars.\n",
      "Equation (0.0.88) is a simple result that follows from the evaluation of the\n",
      "individual single integrals after making the change of variables yi = xi −µi.\n",
      "If Σ−1 is positive deﬁnite, Aitken’s integral can also be evaluated by writ-\n",
      "ing P TΣ−1P = I for some nonsingular matrix P . Now, after the transla-\n",
      "tion y = x −µ, which leaves the integral unchanged, we make the linear\n",
      "change of variables z = P −1y, with the associated Jacobian |det(P )|. From\n",
      "P TΣ−1P = I, we have |det(P )| = (det(Σ))1/2 = |Σ|1/2 because the determi-\n",
      "nant is positive. Aitken’s integral therefore is\n",
      "Z\n",
      "IRd e−yTΣ−1y/2 dy =\n",
      "Z\n",
      "IRd e−(Pz)TΣ−1Pz/2 (det(Σ))1/2dz\n",
      "=\n",
      "Z\n",
      "IRd e−zTz/2 dz (det(Σ))1/2\n",
      "= (2π)d/2(det(Σ))1/2.\n",
      "The expected value of a function f of the vector-valued random variable\n",
      "X is\n",
      "E(f(X)) =\n",
      "Z\n",
      "D(X)\n",
      "f(x)pX(x) dx,\n",
      "(0.3.67)\n",
      "where D(X) is the support of the distribution, pX(x) is the probability den-\n",
      "sity function evaluated at x, and x dx are dummy vectors whose elements\n",
      "correspond to those of X. Interpreting\n",
      "R\n",
      "D(X) dx as a nest of univariate inte-\n",
      "grals, the result of the integration of the vector f(x)pX(x) is clearly of the\n",
      "same type as f(x). For example, if f(x) = x, the expectation is the mean,\n",
      "which is a vector. For the normal distribution, we have\n",
      "E(X) = (2π)−d/2|Σ|−1/2\n",
      "Z\n",
      "IRd xe−(x−µ)TΣ−1(x−µ)/2 dx\n",
      "= µ.\n",
      "For the variance of the vector-valued random variable X,\n",
      "V(X),\n",
      "the function f in expression (0.3.67) above is the matrix (X −E(X))(X −\n",
      "E(X))T, and the result is a matrix. An example is the normal variance:\n",
      "V(X) = E\n",
      "\u0000(X −E(X))(X −E(X))T\u0001\n",
      "= (2π)−d/2|Σ|−1/2\n",
      "Z\n",
      "IRd\n",
      "\u0000(x −µ)(x −µ)T\u0001 e−(x−µ)TΣ−1(x−µ)/2 dx\n",
      "= Σ.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "818\n",
      "0 Statistical Mathematics\n",
      "0.3.6 Transition Matrices\n",
      "An important use of matrices in statistics is in models of transitions of a\n",
      "stochastic process from one state to another. In a discrete-state Markov chain,\n",
      "for example, the probability of going from state j to state i may be represented\n",
      "as elements of a transition matrix, which can any square matrix with nonneg-\n",
      "ative elements and such that the sum of the elements in any column is 1.\n",
      "Any square matrix with nonnegative elements whose columns each sum to 1\n",
      "is called a right stochastic matrix.\n",
      "(Note that many people who work with Markov chains deﬁne the transition\n",
      "matrix as the transpose of K above. This is not a good idea, because in ap-\n",
      "plications with state vectors, the state vectors would naturally have to be row\n",
      "vectors. Until about the middle of the twentieth century, many mathematicians\n",
      "thought of vectors as row vectors; that is, a system of linear equations would be\n",
      "written as xA = b. Nowadays, almost all mathematicians think of vectors as\n",
      "column vectors in matrix algebra. Even in some of my previous writings, e.g.,\n",
      "Gentle (2007), I have called the transpose of K the transition matrix, and I\n",
      "deﬁned a stochastic matrix in terms of the transpose. I think that it is time to\n",
      "adopt a notation that is more consistent with current matrix/vector notation.\n",
      "This is merely a change in notation; no concepts require any change.)\n",
      "There are various properties of transition matrices that are important for\n",
      "studying Markov chains.\n",
      "Irreducible Matrices\n",
      "Any nonnegative square matrix that can be permuted into the form\n",
      "\u0014B11 B12\n",
      "0\n",
      "B22\n",
      "\u0015\n",
      "with square diagonal submatrices is said to be reducible; a matrix that can-\n",
      "not be put into that form is irreducible. An alternate term for reducible is\n",
      "decomposable, with the associated term indecomposable.\n",
      "We see from the deﬁnition that a positive matrix is irreducible.\n",
      "We now consider irreducible square nonnegative matrices. This class in-\n",
      "cludes positive matrices.\n",
      "Irreducible matrices have several interesting properties. An n × n nonneg-\n",
      "ative matrix A is irreducible if and only if (I + A)n−1 is a positive matrix;\n",
      "that is,\n",
      "A is irreducible ⇐⇒(I + A)n−1 > 0.\n",
      "(0.3.68)\n",
      "To see this, ﬁrst assume (I +A)n−1 > 0; thus, (I +A)n−1 clearly is irreducible.\n",
      "If A is reducible, then there exists a permutation matrix Eπ such that\n",
      "ET\n",
      "π AEπ =\n",
      "\u0014B11 B12\n",
      "0\n",
      "B22\n",
      "\u0015\n",
      ",\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "819\n",
      "and so\n",
      "ET\n",
      "π (I + A)n−1Eπ =\n",
      "\u0000ET\n",
      "π (I + A)Eπ\n",
      "\u0001n−1\n",
      "=\n",
      "\u0000I + ET\n",
      "π AEπ\n",
      "\u0001n−1\n",
      "=\n",
      "\u0014\n",
      "In1 + B11\n",
      "B12\n",
      "0\n",
      "In2 + B22\n",
      "\u0015\n",
      ".\n",
      "This decomposition of (I +A)n−1 cannot exist because it is irreducible; hence\n",
      "we conclude A is irreducible if (I+A)n−1 > 0. We can see that (I+A)n−1 must\n",
      "be a positive matrix by ﬁrst observing that the (i, j)th element of (I + A)n−1\n",
      "can be expressed as\n",
      "\u0000(I + A)n−1\u0001\n",
      "ij =\n",
      " n−1\n",
      "X\n",
      "k=0\n",
      "\u0012n −1\n",
      "k\n",
      "\u0013\n",
      "Ak\n",
      "!\n",
      "ij\n",
      ".\n",
      "(0.3.69)\n",
      "Hence, for k = 1, . . ., n −1, we consider the (i, j)th entry of Ak. Let a(k)\n",
      "ij\n",
      "represent this quantity.\n",
      "Given any pair (i, j), for some l1, l2, . . ., lk−1, we have\n",
      "a(k)\n",
      "ij =\n",
      "X\n",
      "l1,l2,...,lk−1\n",
      "a1l1al1l2 · · ·alk−1j.\n",
      "Now a(k)\n",
      "ij\n",
      "> 0 if and only if a1l1, al1l2, . . ., alk−1j are all positive; that is, if\n",
      "there is a path v1, vl1, . . ., vlk−1, vj in G(A). If A is irreducible, then G(A) is\n",
      "strongly connected, and hence the path exists. So, for any pair (i, j), we have\n",
      "from equation (0.3.69)\n",
      "\u0000(I + A)n−1\u0001\n",
      "ij > 0; that is, (I + A)n−1 > 0.\n",
      "The positivity of (I + A)n−1 for an irreducible nonnegative matrix A is a\n",
      "very useful property because it allows us to extend some conclusions of the\n",
      "Perron theorem to irreducible nonnegative matrices.\n",
      "Properties of Square Irreducible Nonnegative Matrices; the\n",
      "Perron-Frobenius Theorem\n",
      "If A is a square irreducible nonnegative matrix, then we have the follow-\n",
      "ing properties. These following properties are the conclusions of the Perron-\n",
      "Frobenius theorem.\n",
      "1. ρ(A) is an eigenvalue of A. This eigenvalue is called the Perron root, as\n",
      "before.\n",
      "2. The Perron root ρ(A) is simple. (That is, the algebraic multiplicity of the\n",
      "Perron root is 1.)\n",
      "3. The dimension of the eigenspace of the Perron root is 1. (That is, the\n",
      "geometric multiplicity of ρ(A) is 1.)\n",
      "4. The eigenvector associated with ρ(A) is positive. This eigenvector is called\n",
      "the Perron vector, as before.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "820\n",
      "0 Statistical Mathematics\n",
      "The relationship (0.3.68) allows us to prove properties 1 and 4.\n",
      "The one property of square positive matrices that does not carry over to\n",
      "square irreducible nonnegative matrices is that r = ρ(A) is the only eigenvalue\n",
      "on the spectral circle of A. For example, the small irreducible nonnegative\n",
      "matrix\n",
      "A =\n",
      "\u00140 1\n",
      "1 0\n",
      "\u0015\n",
      "has eigenvalues 1 and −1, and so both are on the spectral circle.\n",
      "It turns out, however, that square irreducible nonnegative matrices that\n",
      "have only one eigenvalue on the spectral circle also have other interesting\n",
      "properties that are important, for example, in Markov chains. We therefore\n",
      "give a name to the property:\n",
      "A square irreducible nonnegative matrix is said to be primitive if it\n",
      "has only one eigenvalue on the spectral circle.\n",
      "In modeling with Markov chains and other applications, the limiting be-\n",
      "havior of Ak is an important property.\n",
      "If A is a primitive matrix, then we have the useful result\n",
      "lim\n",
      "k→∞\n",
      "\u0012 A\n",
      "ρ(A)\n",
      "\u0013k\n",
      "= vwT,\n",
      "(0.3.70)\n",
      "where v is an eigenvector of A associated with ρ(A) and w is an eigenvector\n",
      "of AT associated with ρ(A), and w and v are scaled so that wTv = 1. (Such\n",
      "eigenvectors exist because ρ(A) is a simple eigenvalue. They also exist because\n",
      "they are both positive. Note that A is not necessarily symmetric, and so its\n",
      "eigenvectors may include imaginary components; however, the eigenvectors\n",
      "associated with ρ(A) are real, and so we can write wT instead of wH.)\n",
      "To see equation (0.3.70), we consider \u0000A −ρ(A)vwT\u0001. First, if (ci, vi) is\n",
      "an eigenpair of \u0000A −ρ(A)vwT\u0001 and ci ̸= 0, then (ci, vi) is an eigenpair of A.\n",
      "We can see this by multiplying both sides of the eigen-equation by vwT:\n",
      "civwTvi = vwT \u0000A −ρ(A)vwT\u0001 vi\n",
      "= \u0000vwTA −ρ(A)vwTvwT\u0001 vi\n",
      "= \u0000ρ(A)vwT −ρ(A)vwT\u0001 vi\n",
      "= 0;\n",
      "hence,\n",
      "Avi =\n",
      "\u0000A −ρ(A)vwT\u0001\n",
      "vi\n",
      "= civi.\n",
      "Next, we show that\n",
      "ρ \u0000A −ρ(A)vwT\u0001 < ρ(A).\n",
      "(0.3.71)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.3 Some Basics of Linear Algebra\n",
      "821\n",
      "If ρ(A) were an eigenvalue of \u0000A −ρ(A)vwT\u0001, then its associated eigenvector,\n",
      "say w, would also have to be an eigenvector of A, as we saw above. But since\n",
      "as an eigenvalue of A the geometric multiplicity of ρ(A) is 1, for some scalar\n",
      "s, w = sv. But this is impossible because that would yield\n",
      "ρ(A)sv =\n",
      "\u0000A −ρ(A)vwT\u0001\n",
      "sv\n",
      "= sAv −sρ(A)v\n",
      "= 0,\n",
      "and neither ρ(A) nor sv is zero. But as we saw above, any eigenvalue of\n",
      "\u0000A −ρ(A)vwT\u0001\n",
      "is an eigenvalue of A and no eigenvalue of\n",
      "\u0000A −ρ(A)vwT\u0001\n",
      "can be as large as ρ(A) in modulus; therefore we have inequality (0.3.71).\n",
      "Finally, with w and v as deﬁned above, and with the eigenvalue ρ(A),\n",
      "\u0000A −ρ(A)vwT\u0001k = Ak −(ρ(A))kvwT,\n",
      "(0.3.72)\n",
      "for k = 1, 2, . . ..\n",
      "Dividing both sides of equation (0.3.72) by (ρ(A))k and rearranging terms,\n",
      "we have\n",
      "\u0012 A\n",
      "ρ(A)\n",
      "\u0013k\n",
      "= vwT +\n",
      "\u0000A −ρ(A)vwT\u0001\n",
      "ρ(A)\n",
      ".\n",
      "(0.3.73)\n",
      "Now\n",
      "ρ\n",
      " \u0000A −ρ(A)vwT\u0001\n",
      "ρ(A)\n",
      "!\n",
      "= ρ \u0000A −ρ(A)vwT\u0001\n",
      "ρ(A)\n",
      ",\n",
      "which is less than 1; hence, we have\n",
      "lim\n",
      "k→∞\n",
      " \u0000A −ρ(A)vwT\u0001\n",
      "ρ(A)\n",
      "!k\n",
      "= 0;\n",
      "so, taking the limit in equation (0.3.73), we have equation (0.3.70).\n",
      "Applications of the Perron-Frobenius theorem are far-ranging.\n",
      "Notes and References for Section 0.3\n",
      "Matrix algebra arises in many areas of statistics. The study and application\n",
      "of linear models is inseparable from matrix/vector operations. In other areas\n",
      "of statistics, such as stochastic processes, matrices play an important role. In\n",
      "these areas, the matrices are often of a type diﬀerent from the important ones\n",
      "in linear models.\n",
      "There are many texts on matrix algebra, some with an orientation toward\n",
      "applications in statistics. I have referred often to Gentle (2007) just because I\n",
      "am most familiar with it. Harville (1997) is a large theorem-proof compendium\n",
      "of facts about matrices that are especially useful in linear models. Almost half\n",
      "of Kollo and von Rosen (2005) is devoted to matrix theory. The rest of the\n",
      "book covers various multivariate distributions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "822\n",
      "0 Statistical Mathematics\n",
      "0.4 Optimization\n",
      "Optimization problems — maximization or minimization — arise in many\n",
      "areas of statistics. Statistical estimation and modeling both are usually special\n",
      "types of optimization problems. In a common method of statistical estimation,\n",
      "we maximize a likelihood, which is a function proportional to a probability\n",
      "density at the point of the observed data. In another method of estimation\n",
      "and in standard modeling techniques, we minimize a norm of the residuals.\n",
      "The best ﬁt of a model is often deﬁned in terms of a minimum of a norm, such\n",
      "as least squares. Other uses of optimization in statistical applications occur\n",
      "prior to collection of data, for example, when we design an experiment or a\n",
      "survey so as to minimize experimental or sampling errors.\n",
      "When a statistical method is based on the solution of an optimization\n",
      "problem, to formulate that problem unambiguously helps us both to under-\n",
      "stand the method and to decide whether the method is appropriate to the\n",
      "purposes for which it is applied.\n",
      "Some of the simpler and more common optimization problems in statistics\n",
      "can be solved easily, often by solving a system of linear equations. Many other\n",
      "problems, however, do not have closed-form solutions, and the solutions must\n",
      "be approximated by iterative methods.\n",
      "0.4.1 Overview of Optimization\n",
      "Optimization means to ﬁnd a maximum or a maximum of an objective\n",
      "function, f : D ⊆IRd 7→IR.\n",
      "Local optimization means optimization within a some subset of the do-\n",
      "main of the objective function Global optimization results in the optimum\n",
      "of all local optima.\n",
      "In unconstrained optimization, we take all points in D to be feasible.\n",
      "Important Properties of the Objective Function\n",
      "•\n",
      "domain dense or not\n",
      "•\n",
      "diﬀerentiable or not\n",
      "–\n",
      "to what order\n",
      "–\n",
      "easy or hard to compute\n",
      "•\n",
      "concave (or convex) or neither\n",
      "–\n",
      "if neither, there may be local optima\n",
      "In the following, let f(x) be the objective function, and assume we want\n",
      "to maximize it.\n",
      "(To minimize, f(x) ←−f(x) and convex ←concave.)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.4 Optimization\n",
      "823\n",
      "Methods\n",
      "•\n",
      "Analytic: yields closed form for all local maxima.\n",
      "•\n",
      "Iterative: for k = 1, 2, . . ., given x(k−1) choose x(k) so that f(x(k)) →local\n",
      "maximum of f.\n",
      "We need\n",
      "–\n",
      "a starting point: x(0);\n",
      "–\n",
      "a method to choose ˜x with good prospects of being x(k);\n",
      "–\n",
      "a method to decide whether ˜x should be x(k).\n",
      "How we choose and decide determines the diﬀerences between optimiza-\n",
      "tion algorithms.\n",
      "How to choose may be based on derivative information or on some sys-\n",
      "tematic method of exploring the domain.\n",
      "How to decide may be based on a deterministic criterion, such as requiring\n",
      "f(x(k)) > f(x(k−1)),\n",
      "or the decision may be randomized.\n",
      "Metamethods: General Tools\n",
      "•\n",
      "Transformations (for either analytic or iterative methods).\n",
      "•\n",
      "Any trick you can think of (for either analytic or iterative methods), e.g.,\n",
      "alternating conditional optimization.\n",
      "•\n",
      "Conditional bounding functions (for iterative methods).\n",
      "Convergence of Iterative Algorithms\n",
      "In an iterative algorithm, we have a sequence\n",
      "\b\u0000f\n",
      "\u0000x(k)\u0001\n",
      ", x(k)\u0001\t\n",
      ".\n",
      "The ﬁrst question is whether the sequence converges to the correct solution.\n",
      "If there is a unique maximum, and if x∗is the point at which the maximum\n",
      "occurs, the ﬁrst question can be posed more precisely as, given ϵ1 does there\n",
      "exist M1 such that for k > M1,\n",
      "\f\f\ff(x(k)) −f(x∗)\n",
      "\f\f\f < ϵ1;\n",
      "or, alternatively, for a given ϵ2 does there exist M2 such that for k > M2,\n",
      "x(k) −x∗\n",
      " < ϵ2.\n",
      "Recall that f : IRd 7→IR, so | · | above is the absolute value, while ∥· ∥is\n",
      "some kind of vector norm.\n",
      "There are complications if x∗is not unique as the point at which the\n",
      "maximum occurs.\n",
      "Similarly, there are comlications if x∗is merely a point of local maximum.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "824\n",
      "0 Statistical Mathematics\n",
      "Assessing Convergence\n",
      "In practice, we must decide when convergence has occurred; that is, whether\n",
      "the iterations have become close enough to the solution. Since we don’t know\n",
      "the solution, we cannot base this decision on the convergence criteria above.\n",
      "We put faith in our algorithm, and decide convergence has occurred if, for\n",
      "some e1, e2 > 0, either\n",
      "|f(x(k)) −f(x(k−1))| ≤e1\n",
      "or\n",
      "∥x(k) −x(k−1)∥≤e2,\n",
      "or both.\n",
      "Notice, that lacking any known value, we trust the algorithm to do the\n",
      "right thing; both x(k) and x(k−1) are just values in the algorithmic sequence.\n",
      "The fact that this particular sequence — or any sequence, even ones yielding\n",
      "nonincreasing function values — converges does not really get at the question\n",
      "of whether x(k) →x∗.\n",
      "Note also the subtle change from “<” to “≤”.\n",
      "For some special class of functions ∇f(x) may exist, and we may know that\n",
      "at the solution, ∇f(x∗) = 0. In these cases, we may have another criterion for\n",
      "deciding convergence has occurred:\n",
      "∥∇f(x∗)∥≤e3.\n",
      "Rate of Convergence of Iterative Algorithms\n",
      "If the answer to the ﬁrst question is “yes”, that is, if the algorithmic sequence\n",
      "converges, the next question is how fast the sequence converges. (We address\n",
      "this question assuming it converges to the correct solution. )\n",
      "The rate of convergence is a measure of how fast the “error” decreases. Any\n",
      "of three quantities we mentioned in discussing convergence, f(x(k)) −f(x∗),\n",
      "x(k) −x∗, or ∇f(x∗), could be taken to be the error. If we take\n",
      "ek = x(k) −x∗\n",
      "to be the error at step k, we might deﬁne the magnitude of the error as ∥ek∥\n",
      "(for some norm ∥· ∥). If the algorithm converges to the correct solution,\n",
      "lim\n",
      "k→∞∥ek∥= 0.\n",
      "Our interest is in how fast ∥ek∥decreases.\n",
      "Sometimes there is no reasonable way of quantifying the rate at which this\n",
      "quantity decreases.\n",
      "In the happy case (and a common case for simple algorithms), if there\n",
      "exist r > 0 and c > 0 such that\n",
      "lim\n",
      "k→∞\n",
      "∥ek∥\n",
      "∥ek−1∥r = c,\n",
      "we say the rate of convergence is r and the rate constant is c.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.4 Optimization\n",
      "825\n",
      "The Steps in Iterative Algorithms\n",
      "For a Special Class of Functions\n",
      "The steps in iterative algorithms are often based on some analytic relationship\n",
      "between f(x) and f(x(k−1)). For a continuously diﬀerentiable function, the\n",
      "most common relationship is the Taylor series expansion:\n",
      "f(x) = f(x(k−1)) +\n",
      "(x −x(k−1))T∇f(x(k−1)) +\n",
      "1\n",
      "2(x −x(k−1))T∇2f(x(k−1))(x −x(k−1)) +\n",
      "· · ·\n",
      "Note this limitation: “For a continuously diﬀerentiable function,\n",
      "...”.\n",
      "We cannot use this method on just any old function.\n",
      "In the following, we will consider only this restricted class of functions.\n",
      "Steepest Ascent (Descent)\n",
      "The steps are deﬁned by truncating the Taylor series. A truncation to two\n",
      "terms yields the steepest ascent direction. For a steepest ascent step, we ﬁnd\n",
      "x(k) along the path ∇f(x(k−1)) from x(k−1).\n",
      "If ∇f(x(k−1)) ≥0, then moving along the path ∇f(x(k−1)) can increase the\n",
      "function value. If f(x) is bounded above (i.e., if the maximization problem\n",
      "makes sense), then at some point along this path, the function begins to\n",
      "decrease.\n",
      "This point is not necessarily the maximum of the function, of course. Find-\n",
      "ing the maximum along the path, is a one-dimensional “line search”.\n",
      "After moving to the point x(k) in the direction of ∇f(x(k−1)), if ∇f(x(k)) =\n",
      "0, we are at a stationary point. This may not be a maximum, but it is as\n",
      "good as we can do using steepest ascent from x(k−1). (In practice, we check\n",
      "∥∇f(x(k))∥≤ϵ, for some norm ∥· ∥and some positive ϵ.)\n",
      "If ∇f(x(k)) < 0 (remember we’re maximizing the function), we change\n",
      "directions and move in the direction of ∇f(x(k)) < 0.\n",
      "Knowing that we will probably be changing direction anyway, we often\n",
      "truncate the line search before we ﬁnd the best x(k) in the direction of\n",
      "∇f(x(k−1)).\n",
      "Newton’s Method\n",
      "At the maximum x∗, ∇f(x∗) = 0.\n",
      "“Newton’s method” for optimization is based on this fact.\n",
      "Newton’s method for optimization just solves the system of equations\n",
      "∇f(x) = 0 using\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "826\n",
      "0 Statistical Mathematics\n",
      "Newton’s iterative method for solving equations:\n",
      "to solve the system of n equations in n unknowns, g(x) = 0, we move from\n",
      "point x(k−1) to point x(k) by\n",
      "x(k) = x(k−1) −\n",
      "\u0010\n",
      "∇g(x(k−1))T\u0011−1\n",
      "g(x(k−1)).\n",
      "Hence, applying this to solving ∇f(x(k−1)) = 0, we have the kth step in\n",
      "Newton’s method for optimization:\n",
      "x(k) = x(k−1) −∇2f(x(k−1))−1∇f(x(k−1)).\n",
      "The direction of the step is dk = x(k) −x(k−1).\n",
      "For numerical reasons, it is best to think of this as the problem of solving\n",
      "the equations\n",
      "∇2f(x(k−1))dk = −∇f(x(k−1)),\n",
      "and then taking x(k) = x(k−1) + dk.\n",
      "The Hessian\n",
      "The Hessian H(x) = ∇2f(x) clearly plays an important role in Newton’s\n",
      "method; if it is singular, the Newton step based on the solution to\n",
      "∇2f(x(k−1))dk = −∇f(x(k−1)),\n",
      "is undetermined.\n",
      "The relevance of the Hessian goes far beyond this, however. The Hessian\n",
      "reveals important properties of the shape of the surface f(x) at x(k−1).\n",
      "The shape is especially interesting at a stationary point; that is a point x∗\n",
      "at which ∇f(x) = 0.\n",
      "If the Hessian is negative deﬁnite at x∗, f(x∗) is a local maximum.\n",
      "If the Hessian is positive deﬁnite at x∗, f(x∗) is a local maximum.\n",
      "If the Hessian is nonsingular, but neither negative deﬁnite nor positive\n",
      "deﬁnite at x∗, it is a saddlepoint.\n",
      "If the Hessian is singular, the stationary point is none of the above.\n",
      "In minimization problems, such as least squares, we hope the Hessian\n",
      "is positive deﬁnite, in which case the function is concave. In least squares\n",
      "ﬁtting of the standard linear regression model, the Hessian is the famous\n",
      "XTX matrix.\n",
      "In maximization problems, such as MLE, it is particularly interesting to\n",
      "know whether H(x) is negative deﬁnite everywhere (or -H(x) is positive def-\n",
      "inite everywhere). In this case, the function is convex.\n",
      "When H(x) (in minimization problems or -H(x) in maximization prob-\n",
      "lems) is positive deﬁnite but nearly singular, it may be helpful to regularize\n",
      "the problem by adding a diagonal matrix with positive elements: H(x) + D.\n",
      "One kind of regularization is ridge regression, in which the Hessian is\n",
      "replaced by XTX + dI.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.4 Optimization\n",
      "827\n",
      "Modiﬁcations of Newton’s Method\n",
      "In the basic Newton step, the direction dk from x(k−1) is the best direction,\n",
      "but the point dk + x(k−1) may not be the best point. In fact, the algorithm\n",
      "can often be speeded up by not going quite that far; that is, by “damping”\n",
      "the Newton step and taking x(k) = αkdk + x(k−1). This is a line search, and\n",
      "there are several ways of doing this. In the context of least squares, a common\n",
      "way of damping is the Levenberg-Marquardt method.\n",
      "Rather than ﬁnding ∇2f(x(k−1)), we might ﬁnd an approximate Hessian\n",
      "at x(k−1), eHk, and then solve\n",
      "eHkdk = −∇f(x(k−1)).\n",
      "This is called a quasi-Newton method.\n",
      "In MLE, we may take the objective function to be the log likelihood, with\n",
      "the variable θ. In this case, the Hessian, H(θ), is ∂2 log L(θ; x)/∂θ(∂θ)T.\n",
      "Under very general regularity conditions, the expected value of H(θ) is the\n",
      "negative of the expected value of (∂log L(θ; x)/∂θ)(∂log L(θ; x)∂θ)T, which\n",
      "is the Fisher information matrix, I(θ). This quantity plays an important role\n",
      "in statistical estimation. In MLE it is often possible to compute I(θ), and take\n",
      "the Newton step as\n",
      "I(θ(k))dk = ∇log L(θ(k−1); x).\n",
      "This quasi-Newton method is called Fisher scoring.\n",
      "More Modiﬁcations of Newton’s Method\n",
      "The method of solving the Newton or quasi-Newton equations may itself be\n",
      "iterative, such as a conjugate gradient or Gauss-Seidel method. (These are\n",
      "“inner loop iterations”.) Instead of continuing the inner loop iterations to the\n",
      "solution, we may stop early. This is called a truncated Newton method.\n",
      "The best gains in iterative algorithms often occur in the ﬁrst steps. When\n",
      "the optimization is itself part of an iterative method, we may get an acceptable\n",
      "approximate solution to the optimization problem by stopping the optimiza-\n",
      "tion iterations early. Sometimes we may stop the optimization after just one\n",
      "iteration. If Newton’s method is used, this is called a one-step Newton\n",
      "method.\n",
      "0.4.2 Alternating Conditional Optimization\n",
      "The computational burden in a single iteration for solving the optimization\n",
      "problem can sometimes be reduced by more than a linear amount by sepa-\n",
      "rating x into two subvectors. The optimum is then computed by alternating\n",
      "between computations involving the two subvectors, and the iterations pro-\n",
      "ceed in a zigzag path to the solution.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "828\n",
      "0 Statistical Mathematics\n",
      "Each of the individual sequences of iterations is simpler than the sequence\n",
      "of iterations on the full x.\n",
      "For the problem\n",
      "min\n",
      "x f(x)\n",
      "if x = (x1, x2) that is, x is a vector with at least two elements, and x1 and x2\n",
      "may be vectors), an iterative alternating conditional optimization algorithm\n",
      "may start with x(0)\n",
      "2 , and then for k = 0, 1, . . .,\n",
      "1. x(k)\n",
      "1\n",
      "= arg minx1 f\n",
      "\u0010\n",
      "x1, x(k−1)\n",
      "2\n",
      "\u0011\n",
      "2. x(k)\n",
      "2\n",
      "= arg minx2 f\n",
      "\u0010\n",
      "x(k)\n",
      "1 , x2\n",
      "\u0011\n",
      "Use of Conditional Bounding Functions: MM Methods\n",
      "In an iterative method to maximize f(x), the idea, given x(k−1) at step k, is\n",
      "to try to ﬁnd a function g\n",
      "\u0000x; x(k−1)\u0001\n",
      "with these properties:\n",
      "•\n",
      "is easy to work with (that is, is easy to maximize)\n",
      "•\n",
      "g\n",
      "\u0000x; x(k−1)\u0001\n",
      "≤f(x)\n",
      "∀x\n",
      "•\n",
      "g \u0000x(k−1); x(k−1)\u0001 = f \u0000x(k−1)\u0001\n",
      "If we can ﬁnd x(k) ∋g \u0000x(k); x(k−1)\u0001 > g \u0000x(k−1); x(k−1)\u0001, we have the\n",
      "“sandwich inequality”:\n",
      "f\n",
      "\u0010\n",
      "x(k)\u0011\n",
      "≥g\n",
      "\u0010\n",
      "x(k); x(k−1)\u0011\n",
      "> g\n",
      "\u0010\n",
      "x(k−1); x(k−1)\u0011\n",
      "= f\n",
      "\u0010\n",
      "x(k−1)\u0011\n",
      ".\n",
      "An equivalent (but more complicated) method for seeing this inequality\n",
      "uses the fact that\n",
      "f\n",
      "\u0010\n",
      "x(k)\u0011\n",
      "−g\n",
      "\u0010\n",
      "x(k); x(k−1)\u0011\n",
      "≥f\n",
      "\u0010\n",
      "x(k−1)\u0011\n",
      "−g\n",
      "\u0010\n",
      "x(k); x(k−1)\u0011\n",
      ".\n",
      "(From the properties above,\n",
      "g\n",
      "\u0010\n",
      "x; x(k−1)\u0011\n",
      "−f (x)\n",
      "attains its maximum at x(k−1).)\n",
      "Hence,\n",
      "f\n",
      "\u0010\n",
      "x(k)\u0011\n",
      "= g\n",
      "\u0010\n",
      "x(k); x(k−1)\u0011\n",
      "+ f\n",
      "\u0010\n",
      "x(k)\u0011\n",
      "−g\n",
      "\u0010\n",
      "x(k); x(k−1)\u0011\n",
      "> g\n",
      "\u0010\n",
      "x(k−1); x(k−1)\u0011\n",
      "+ f\n",
      "\u0010\n",
      "x(k−1)\u0011\n",
      "−g\n",
      "\u0010\n",
      "x(k−1); x(k−1)\u0011\n",
      "= f\n",
      "\u0010\n",
      "x(k−1)\u0011\n",
      ".\n",
      "The relationship between f\n",
      "\u0000x(k)\u0001\n",
      "and f\n",
      "\u0000x(k−1)\u0001\n",
      ", that is, whether we\n",
      "have “>” or “≥” in the inequalities, depends on the relationship between\n",
      "g \u0000x(k); x(k−1)\u0001 and g \u0000x(k−1); x(k−1)\u0001.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.4 Optimization\n",
      "829\n",
      "We generally require g \u0000x(k); x(k−1)\u0001 > g \u0000x(k−1); x(k−1)\u0001.\n",
      "Clearly, the best step would be\n",
      "x(k) = arg min\n",
      "x\n",
      "g\n",
      "\u0010\n",
      "x; x(k−1)\u0011\n",
      ",\n",
      "but the overall eﬃciency of the method may be better if we don’t work\n",
      "too hard to ﬁnd the maximum, but just accept some x(k) that satisﬁes\n",
      "g\n",
      "\u0000x(k); x(k−1)\u0001\n",
      "≤g\n",
      "\u0000x(k−1); x(k−1)\u0001\n",
      ".\n",
      "After moving to x(k), we must ﬁnd a new g \u0000x; x(k)\u0001.\n",
      "Equivalent notations:\n",
      "g\n",
      "\u0010\n",
      "x; x(k−1)\u0011\n",
      "↔g(k)(x) ↔gk(x)\n",
      "Note the logical diﬀerence in k and k −1, although both determine the same\n",
      "g.\n",
      "The g that we maximize is a “minorizing” function.\n",
      "Thus, we Minorize then Maximize: MM.\n",
      "EM methods.\n",
      "Alternatively, we Majorize then Minimize: MM.\n",
      "Reference: Lange et al. (2000).\n",
      "Maximization in Alternating Algorithms\n",
      "In alternating multiple step methods such as alternating conditional maxi-\n",
      "mization methods and methods that use a conditional bounding function, at\n",
      "least one of the alternating steps involves maximization of some function.\n",
      "As we indicated in discussing conditional bounding functions, instead of\n",
      "ﬁnding a point that actually maximizes the function, which may be a diﬃcult\n",
      "task, we may just ﬁnd a point that increases the value of the function. Under\n",
      "this weaker condition, the methods still work.\n",
      "We may relax the requirement even further, so that for some steps we\n",
      "only require that the function not be decreased. So long as we maintain the\n",
      "requirement that the function actually be increased in a suﬃcient number of\n",
      "steps, the methods still work.\n",
      "The most basic requirement is that g \u0000x(k); x(k)\u0001 ≥g \u0000x(k−1); x(k−1)\u0001.\n",
      "(Even this requirement is relaxed in the class of optimization algorithms based\n",
      "on annealing. A reason for relaxing this requirement may be to avoid getting\n",
      "trapped in local optima.)\n",
      "0.4.3 Simulated Annealing\n",
      "Stochastic optimization ******* Spall (2012)\n",
      "Simulated annealing is a method that simulates the thermodynamic pro-\n",
      "cess in which a metal is heated to its melting temperature and then is allowed\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "830\n",
      "0 Statistical Mathematics\n",
      "to cool slowly so that its structure is frozen at the crystal conﬁguration of low-\n",
      "est energy. In this process the atoms go through continuous rearrangements,\n",
      "moving toward a lower energy level as they gradually lose mobility due to the\n",
      "cooling. The rearrangements do not result in a monotonic decrease in energy,\n",
      "however. The density of energy levels at a given temperature ideally is expo-\n",
      "nential, the so-called Boltzmann distribution, with a mean proportional to the\n",
      "absolute temperature. (The constant of proportionality is called “Boltzmann’s\n",
      "constant”). This is analogous to a sequence of optimization iterations that oc-\n",
      "casionally go uphill. If the function has local minima, going uphill occasionally\n",
      "is desirable.\n",
      "Metropolis et al. (1953) developed a stochastic relaxation technique that\n",
      "simulates the behavior of a system of particles approaching thermal equilib-\n",
      "rium. (This is the same paper that described the Metropolis sampling al-\n",
      "gorithm.) The energy associated with a given conﬁguration of particles is\n",
      "compared to the energy of a diﬀerent conﬁguration. If the energy of the new\n",
      "conﬁguration is lower than that of the previous one, the new conﬁguration is\n",
      "immediately accepted. If the new conﬁguration has a larger energy, it is ac-\n",
      "cepted with a nonzero probability. This probability is larger for small increases\n",
      "than for large increases in the energy level. One of the main advantages of\n",
      "simulated annealing is that the process is allowed to move away from a local\n",
      "optimum.\n",
      "Although the technique is heuristically related to the cooling of a metal,\n",
      "as in the application of Metropolis et al. (1953), it can be successfully applied\n",
      "to a wide range of optimization problems.\n",
      "The Basic Algorithm\n",
      "In simulated annealing, a “temperature” parameter controls the probability of\n",
      "moving uphill; when the temperature is high, the probability of acceptance of\n",
      "any given point is high, and the process corresponds to a pure random walk.\n",
      "When the temperature is low, however, the probability of accepting any given\n",
      "point is low; and in fact, only downhill points are accepted. The behavior at\n",
      "low temperatures corresponds to a gradient search.\n",
      "As the iterations proceed and the points move lower on the surface (it\n",
      "is hoped), the temperature is successively lowered. An “annealing schedule”\n",
      "determines how the temperature is adjusted.\n",
      "In the description of simulated annealing in Algorithm 0.1, recognizing the\n",
      "common applications in combinatorial optimization, we refer to the argument\n",
      "of the objective function as a “state”, rather than as a “point”.\n",
      "Algorithm 0.1 Simulated Annealing\n",
      "0. Set k = 1 and initialize state s.\n",
      "1. Compute T(k).\n",
      "2. Set i = 0 and j = 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "0.4 Optimization\n",
      "831\n",
      "3. Generate state r and compute δf = f(r) −f(s).\n",
      "4. Based on δf, decide whether to move from state s to state r.\n",
      "If δf ≤0,\n",
      "accept;\n",
      "otherwise,\n",
      "accept with a probability P (δf, T(k)).\n",
      "If state r is accepted, set i = i + 1.\n",
      "5. If i is equal to the limit for the number of successes at a given temperature,\n",
      "go to step 1.\n",
      "6. Set j = j + 1. If j is less than the limit for the number of iterations at\n",
      "given temperature, go to step 3.\n",
      "7. If i = 0,\n",
      "deliver s as the optimum; otherwise,\n",
      "if k < kmax,\n",
      "set k = k + 1 and go to step 1;\n",
      "otherwise,\n",
      "issue message that\n",
      "‘algorithm did not converge in kmax iterations’.\n",
      "For optimization of a continuous function over a region, the state is a point\n",
      "in that region. A new state or point may be selected by choosing a radius r and\n",
      "point on the d dimensional sphere of radius r centered at the previous point.\n",
      "For a continuous objective function, the movement in step 3 of Algorithm 0.1\n",
      "may be a random direction to step in the domain of the objective function.\n",
      "In combinatorial optimization, the selection of a new state in step 3 may be\n",
      "a random rearrangement of a given conﬁguration.\n",
      "Parameters of the Algorithm: The Probability Function\n",
      "There are a number of tuning parameters to choose in the simulated anneal-\n",
      "ing algorithm. These include such relatively simple things as the number of\n",
      "repetitions or when to adjust the temperature. The probability of acceptance\n",
      "and the type of temperature adjustments present more complicated choices.\n",
      "One approach is to assume that at a given temperature, T, the states\n",
      "have a known probability density (or set of probabilities, if the set of states\n",
      "is countable), pS(s, T), and then to deﬁne an acceptance probability to move\n",
      "from state sk to sk+1 in terms of the relative change in the probability density\n",
      "from pS(sk, T) to pS(sk+1, T). In the original application of Metropolis et\n",
      "al., the objective function was the energy of a given conﬁguration, and the\n",
      "probability of an energy change of δf at temperature T is proportional to\n",
      "exp(−δf/T).\n",
      "Even when there is no underlying probability model, the probability in\n",
      "step 4 of Algorithm 0.1 is often taken as\n",
      "P (δf, T(k)) = e−δf/T (k),\n",
      "(0.4.1)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "832\n",
      "0 Statistical Mathematics\n",
      "although a completely diﬀerent form could be used. The exponential distri-\n",
      "bution models energy changes in ensembles of molecules, but otherwise it has\n",
      "no intrinsic relationship to a given optimization problem.\n",
      "The probability can be tuned in the early stages of the computations so\n",
      "that some reasonable proportion of uphill steps are taken.\n",
      "Parameters of the Algorithm: The Cooling Schedule\n",
      "There are various ways the temperature can be updated in step 1.\n",
      "The probability of the method converging to the global optimum depends\n",
      "on a slow decrease in the temperature. In practice, the temperature is generally\n",
      "decreased by some proportion of its current value:\n",
      "T(k + 1) = b(k)T(k).\n",
      "(0.4.2)\n",
      "We would like to decrease T as rapidly as possible, yet have a high probability\n",
      "of determining the global optimum. Under the assumptions that the energy\n",
      "distribution is Gaussian and the acceptance probability is of the form (0.4.1),\n",
      "the probability of convergence goes to 1 if the temperature decreases as the\n",
      "inverse of the logarithm of the time, that is, if b(k) = (log(k))−1 in equa-\n",
      "tion (0.4.2). Under the assumption that the energy distribution is Cauchy, a\n",
      "similar argument allows b(k) = k−1, and a uniform distribution over bounded\n",
      "regions allows b(k) = exp(−ckk1/d), where ck is some constant, and d is the\n",
      "number of dimensions.\n",
      "A constant temperature is often used in simulated annealing for optimiza-\n",
      "tion of continuous functions and the additive and multiplicative adjustments,\n",
      "c(k) and b(k) are usually taken as constants, rather than varying with k.\n",
      "Notes and References for Section 0.4\n",
      "There is an extensive literature on optimization, much of it concerned with\n",
      "practical numerical algorithms. Software for optimization is widely available,\n",
      "both in special-purpose programs and in general-purpose packages such as R\n",
      "and Matlab.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Appendices\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "A\n",
      "Important Probability Distributions\n",
      "Development of stochastic models is facilitated by identifying a few probabil-\n",
      "ity distributions that seem to correspond to a variety of data-generating pro-\n",
      "cesses, and then studying the properties of these distributions. In the following\n",
      "tables, I list some of the more useful distributions, both discrete distributions\n",
      "and continuous ones. The names listed are the most common names, although\n",
      "some distributions go by diﬀerent names, especially for speciﬁc values of the\n",
      "parameters. In the ﬁrst column, following the name of the distribution, the\n",
      "parameter space is speciﬁed.\n",
      "There are two very special continuous distributions, for which I use spe-\n",
      "cial symbols: the uniform over the interval [a, b], designated U(a, b), and the\n",
      "normal (or Gaussian), denoted by N(µ, σ2). Notice that the second parame-\n",
      "ter in the notation for the normal is the variance. Sometimes, such as in the\n",
      "functions in R, the second parameter of the normal distribution is the stan-\n",
      "dard deviation instead of the variance. A normal distribution with µ = 0 and\n",
      "σ2 = 1 is called the standard normal. I also often use the notation φ(x) for\n",
      "the PDF of a standard normal and Φ(x) for the CDF of a standard normal,\n",
      "and these are generalized in the obvious way as φ(x|µ, σ2) and Φ(x|µ, σ2).\n",
      "Except for the uniform and the normal, I designate distributions by a\n",
      "name followed by symbols for the parameters, for example, binomial(n, π) or\n",
      "gamma(α, β). Some families of distributions are subfamilies of larger families.\n",
      "For example, the usual gamma family of distributions is a the two-parameter\n",
      "subfamily of the three-parameter gamma.\n",
      "There are other general families of probability distributions that are de-\n",
      "ﬁned in terms of a diﬀerential equation or of a form for the CDF. These include\n",
      "the Pearson, Johnson, Burr, and Tukey’s lambda distributions.\n",
      "Most of the common distributions fall naturally into one of two classes.\n",
      "They have either a countable support with positive probability at each point\n",
      "in the support, or a continuous (dense, uncountable) support with zero prob-\n",
      "ability for any subset with zero Lebesgue measure. The distributions listed in\n",
      "the following tables are divided into these two natural classes.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "836\n",
      "Appendix A. Important Probability Distributions\n",
      "There are situations for which these two distinct classes are not appropri-\n",
      "ate. For many such situations, however, a mixture distribution provides an\n",
      "appropriate model. We can express a PDF of a mixture distribution as\n",
      "pM(y) =\n",
      "m\n",
      "X\n",
      "j=1\n",
      "ωjpj(y | θj),\n",
      "where the m distributions with PDFs pj can be either discrete or continuous.\n",
      "A simple example is a probability model for the amount of rainfall in a given\n",
      "period, say a day. It is likely that a nonzero probability should be associated\n",
      "with zero rainfall, but with no other amount of rainfall. In the model above,\n",
      "m is 2, ω1 is the probability of no rain, p1 is a degenerate PDF with a value\n",
      "of 1 at 0, ω2 = 1 −ω1, and p2 is some continuous PDF over IR+, possibly\n",
      "similar to a distribution in the exponential family.\n",
      "A mixture family that is useful in robustness studies is the ϵ-mixture dis-\n",
      "tribution family, which is characterized by a given family with CDF P that is\n",
      "referred to as the reference distribution, together with a point xc and a weight\n",
      "ϵ. The CDF of a ϵ-mixture distribution family is\n",
      "Pxc,ϵ(x) = (1 −ϵ)P (x) + ϵI[xc,∞[(x),\n",
      "where 0 ≤ϵ ≤1.\n",
      "Another example of a mixture distribution is a binomial with constant\n",
      "parameter n, but with a nonconstant parameter π. In many applications, if\n",
      "an identical binomial distribution is assumed (that is, a constant π), it is often\n",
      "the case that “over-dispersion” will be observed; that is, the sample variance\n",
      "exceeds what would be expected given an estimate of some other parameter\n",
      "that determines the population variance. This situation can occur in a model,\n",
      "such as the binomial, in which a single parameter determines both the ﬁrst\n",
      "and second moments. The mixture model above in which each pj is a binomial\n",
      "PDF with parameters n and πj may be a better model.\n",
      "Of course, we can extend this kind of mixing even further. Instead of\n",
      "ωjpj(y | θj) with ωj ≥0 and Pm\n",
      "j=1 ωj = 1, we can take ω(θ)p(y | θ) with\n",
      "ω(θ) ≥0 and R ω(θ) dθ = 1, from which we recognize that ω(θ) is a PDF and\n",
      "θ can be considered to be the realization of a random variable.\n",
      "Extending the example of the mixture of binomial distributions, we may\n",
      "choose some reasonable PDF ω(π). An obvious choice is a beta PDF. This\n",
      "yields the beta-binomial distribution, with PDF\n",
      "pX,Π(x, π) =\n",
      "\u0012 n\n",
      "x\n",
      "\u0013 Γ(α + β)\n",
      "Γ(α)Γ(β)πx+α−1(1 −π)n−x+β−1I{0,1,...,n}×]0,1[(x, π).\n",
      "This is a standard distribution but I did not include it in the tables below.\n",
      "This distribution may be useful in situations in which a binomial model is\n",
      "appropriate, but the probability parameter is changing more-or-less continu-\n",
      "ously.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Appendix A. Important Probability Distributions\n",
      "837\n",
      "We recognize a basic property of any mixture distribution: It is a joint\n",
      "distribution factored as a marginal (prior) for a random variable, which is often\n",
      "not observable, and a conditional distribution for another random variable,\n",
      "which is usually the observable variable of interest.\n",
      "In Bayesian analyses, the ﬁrst two assumptions (a prior distribution for\n",
      "the parameters and a conditional distribution for the observable) lead immedi-\n",
      "ately to a mixture distribution. The beta-binomial above arises in a canonical\n",
      "example of Bayesian analysis.\n",
      "Some families of distributions are recognized because of their relation-\n",
      "ship to sampling distributions. These include the t, the chi-squared, and the\n",
      "Wishart. Other families are recognized because of their use as conjugate pri-\n",
      "ors. These include the inverted chi-squared and the inverted Wishart.\n",
      "General References\n",
      "Evans et al. (2000) give general descriptions of 40 probability distributions.\n",
      "Balakrishnan and Nevzorov (2003) provide an overview of the important char-\n",
      "acteristics that distinguish diﬀerent distributions and then describe the impor-\n",
      "tant characteristics of many common distributions. Leemis and McQueston\n",
      "(2008) present an interesting compact graph of the relationships among a\n",
      "large number of probability distributions. Likewise, Morris and Lock (2009)\n",
      "give a graph that illustrates various interrelationships among natural expo-\n",
      "nential families.\n",
      "The six books by Johnson, Kotz et al. (Johnson et al. (1995a), Kotz et al.\n",
      "(2000), Johnson et al. (1997), Johnson et al. (1994), Johnson et al. (1995b),\n",
      "and Johnson et al. (2005)) contain a wealth of information above many fam-\n",
      "ilies of distributions.\n",
      "Currently, the most readily accessible summary of common probability\n",
      "distributions is Wikipedia: http://wikipedia.org/ Search under the name\n",
      "of the distribution.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "838\n",
      "Appendix A. Important Probability Distributions\n",
      "Table A.1. Discrete Distributions (PDFs are wrt counting measure)\n",
      "discrete uniform\n",
      "PDF\n",
      "1\n",
      "m,\n",
      "y = a1, . . . , am\n",
      "a1, . . . , am ∈IR\n",
      "mean\n",
      "P ai/m\n",
      "variance\n",
      "P(ai −¯a)2/m, where ¯a = P ai/m\n",
      "Bernoulli\n",
      "PDF\n",
      "πy(1 −π)1−y,\n",
      "y = 0, 1\n",
      "π ∈]0,1[\n",
      "mean\n",
      "π\n",
      "variance\n",
      "π(1 −π)\n",
      "binomial (n Bernoullis)\n",
      "PDF\n",
      " \n",
      "n\n",
      "y\n",
      "!\n",
      "πy(1 −π)n−y,\n",
      "y = 0, 1, . . . , n\n",
      "n = 1, 2, . . . ;\n",
      "π ∈]0, 1[\n",
      "CF\n",
      "(1 −π + πeit)n\n",
      "mean\n",
      "nπ\n",
      "variance\n",
      "nπ(1 −π)\n",
      "geometric\n",
      "PDF\n",
      "π(1 −π)y,\n",
      "y=0,1,2,. . .\n",
      "π ∈]0,1[\n",
      "mean\n",
      "(1 −π)/π\n",
      "variance\n",
      "(1 −π)/π2\n",
      "negative binomial (n geometrics)\n",
      "PDF\n",
      " \n",
      "y + n −1\n",
      "n −1\n",
      "!\n",
      "πn(1 −π)y,\n",
      "y = 0, 1, 2, . . .\n",
      "n = 1, 2, . . . ;\n",
      "π ∈]0, 1[\n",
      "CF\n",
      "„\n",
      "π\n",
      "1 −(1 −π)eit\n",
      "«n\n",
      "mean\n",
      "n(1 −π)/π\n",
      "variance\n",
      "n(1 −π)/π2\n",
      "multinomial\n",
      "PDF\n",
      "n!\n",
      "Q yi!\n",
      "d\n",
      "Y\n",
      "i=1\n",
      "πyi\n",
      "i , yi = 0, 1, . . . , n,\n",
      "X\n",
      "yi = n\n",
      "n = 1, 2, . . .,\n",
      "CF\n",
      "“Pd\n",
      "i=1 πieiti\n",
      "”n\n",
      "for i = 1, . . . , d, πi ∈]0, 1[, P πi = 1 means\n",
      "nπi\n",
      "variances\n",
      "nπi(1 −πi)\n",
      "covariances −nπiπj\n",
      "hypergeometric\n",
      "PDF\n",
      " \n",
      "M\n",
      "y\n",
      "! \n",
      "N −M\n",
      "n −y\n",
      "!\n",
      " \n",
      "N\n",
      "n\n",
      "!\n",
      ",\n",
      "y = max(0, n −N + M), . . . , min(n, M)\n",
      "N = 2, 3, . . .;\n",
      "mean\n",
      "nM/N\n",
      "M = 1, . . . , N; n = 1, . . . , N\n",
      "variance\n",
      "(nM/N)(1 −M/N)(N −n)/(N −1)\n",
      "continued ...\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Appendix A. Important Probability Distributions\n",
      "839\n",
      "Table A.1. Discrete Distributions (continued)\n",
      "Poisson\n",
      "PDF\n",
      "θye−θ/y!,\n",
      "y = 0, 1, 2, . . .\n",
      "θ ∈IR+\n",
      "CF\n",
      "eθ(eit−1)\n",
      "mean\n",
      "θ\n",
      "variance\n",
      "θ\n",
      "power series\n",
      "PDF\n",
      "hy\n",
      "c(θ)θy,\n",
      "y = 0, 1, 2, . . .\n",
      "θ ∈IR+\n",
      "CF\n",
      "P\n",
      "y hy(θeit)y/c(θ)\n",
      "{hy} positive constants\n",
      "mean\n",
      "θ d\n",
      "dθ (log(c(θ))\n",
      "c(θ) = P\n",
      "y hyθy\n",
      "variance\n",
      "θ d\n",
      "dθ (log(c(θ)) + θ2 d2\n",
      "dθ2 (log(c(θ))\n",
      "logarithmic\n",
      "PDF\n",
      "−\n",
      "πy\n",
      "y log(1 −π),\n",
      "y = 1, 2, 3, . . .\n",
      "π ∈]0,1[\n",
      "mean\n",
      "−π/((1 −π) log(1 −π))\n",
      "variance\n",
      "−π(π + log(1 −π))/((1 −π)2(log(1 −π))2)\n",
      "Benford’s\n",
      "PDF\n",
      "logb(y + 1) −logb(y),\n",
      "y = 1, . . . , b −1\n",
      "b integer ≥3\n",
      "mean\n",
      "b −1 −logb((b −1)!)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "840\n",
      "Appendix A. Important Probability Distributions\n",
      "Table A.2. The Normal Distributions\n",
      "normal; N(µ, σ2)\n",
      "PDF\n",
      "φ(y|µ, σ2)\n",
      "def\n",
      "=\n",
      "1\n",
      "√\n",
      "2πσ\n",
      "e−(y−µ)2/2σ2\n",
      "µ ∈IR; σ ∈IR+\n",
      "CF\n",
      "eiµt−σ2t2/2\n",
      "mean\n",
      "µ\n",
      "variance\n",
      "σ2\n",
      "multivariate normal; Nd(µ, Σ) PDF\n",
      "1\n",
      "(2π)d/2|Σ|1/2 e−(y−µ)TΣ−1(y−µ)/2\n",
      "µ ∈IRd; Σ ≻0 ∈IRd×d\n",
      "CF\n",
      "eiµTt−tTΣt/2\n",
      "mean\n",
      "µ\n",
      "covariance Σ\n",
      "matrix normal\n",
      "PDF\n",
      "1\n",
      "(2π)nm/2|Ψ|n/2|Σ|m/2 e−tr(Ψ−1(Y −M)TΣ−1(Y −M))/2\n",
      "M ∈IRn×m, Ψ ≻0 ∈IRm×m,\n",
      "mean\n",
      "M\n",
      "Σ ≻0 ∈IRn×n\n",
      "covariance Ψ ⊗Σ\n",
      "complex multivariate normal\n",
      "PDF\n",
      "1\n",
      "(2π)d/2|Σ|1/2 e−(z−µ)HΣ−1(z−µ)/2\n",
      "µ ∈ICd, Σ ≻0 ∈ICd×d\n",
      "mean\n",
      "µ\n",
      "covariance Σ\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Appendix A. Important Probability Distributions\n",
      "841\n",
      "Table A.3. Sampling Distributions from the Normal Distribution\n",
      "chi-squared; χ2\n",
      "ν\n",
      "PDF\n",
      "1\n",
      "Γ(ν/2)2ν/2 yν/2−1e−y/2 I¯IR+(y)\n",
      "ν ∈IR+\n",
      "mean\n",
      "ν\n",
      "if ν ∈ZZ+,\n",
      "variance\n",
      "2ν\n",
      "t\n",
      "PDF\n",
      "Γ((ν + 1)/2)\n",
      "Γ(ν/2)√νπ (1 + y2/ν)−(ν+1)/2\n",
      "ν ∈IR+\n",
      "mean\n",
      "0\n",
      "variance\n",
      "ν/(ν −2), for ν > 2\n",
      "F\n",
      "PDF\n",
      "νν1/2\n",
      "1\n",
      "νν2/2\n",
      "2\n",
      "Γ(ν1 + ν2)\n",
      "Γ(ν1/2)Γ(ν2/2)\n",
      "yν1/2−1\n",
      "(ν2 + ν1y)(ν1+ν2)/2 I¯IR+(y)\n",
      "ν1, ν2 ∈IR+\n",
      "mean\n",
      "ν2/(ν2 −2), for ν2 > 2\n",
      "variance\n",
      "2ν2\n",
      "2(ν1 + ν2 −2)/(ν1(ν2 −2)2(ν2 −4)), for ν2 > 4\n",
      "Wishart\n",
      "PDF\n",
      "|W|(ν−d−1)/2\n",
      "2νd/2|Σ|ν/2Γd(ν/2) exp\n",
      "`\n",
      "−trace(Σ−1W)\n",
      "´\n",
      "I{M | M≻0∈IRd×d}(W)\n",
      "d = 1, 2, . . . ;\n",
      "mean\n",
      "νΣ\n",
      "ν > d −1 ∈IR;\n",
      "covariance Cov(Wij, Wkl) = ν(σikσjl + σilσjk), where Σ = (σij)\n",
      "Σ ≻0 ∈IRd×d\n",
      "noncentral chi-squared PDF\n",
      "e−λ/2\n",
      "2ν/2 yν/2−1e−y/2\n",
      "∞\n",
      "X\n",
      "k=0\n",
      "(λ/2)k\n",
      "k!\n",
      "1\n",
      "Γ(ν/2 + k)2k yk I¯IR+(y)\n",
      "ν, λ ∈IR+\n",
      "mean\n",
      "ν + λ\n",
      "variance\n",
      "2(ν + 2λ)\n",
      "noncentral t\n",
      "PDF\n",
      "νν/2e−λ2/2\n",
      "Γ(ν/2)π1/2 (ν + y2)−(ν+1)/2\n",
      "×\n",
      "ν ∈IR+, λ ∈IR\n",
      "∞\n",
      "X\n",
      "k=0\n",
      "Γ\n",
      "„ν + k + 1\n",
      "2\n",
      "« (λy)k\n",
      "k!\n",
      "„\n",
      "2\n",
      "ν + y2\n",
      "«k/2\n",
      "mean\n",
      "λ(ν/2)1/2Γ((ν −1)/2)\n",
      "Γ(ν/2)\n",
      ", for ν > 1\n",
      "variance\n",
      "ν\n",
      "ν −2(1 + λ2) −ν\n",
      "2 λ2\n",
      "„Γ((ν −1)/2)\n",
      "Γ(ν/2)\n",
      "«2\n",
      ", for ν > 2\n",
      "noncentral F\n",
      "PDF\n",
      "„ ν1\n",
      "ν2\n",
      "«ν1/2\n",
      "e−λ/2yν1/2−1\n",
      "„\n",
      "ν2\n",
      "ν2 + ν1y\n",
      "«ν1/2+ν2/2\n",
      "×\n",
      "ν1, ν2, λ ∈IR+\n",
      "∞\n",
      "X\n",
      "k=0\n",
      "(λ/2)kΓ(ν2 + ν1 + k)\n",
      "Γ(ν2)Γ(ν1 + k)k!\n",
      "„ν1\n",
      "ν2\n",
      "«k\n",
      "yk\n",
      "„\n",
      "ν2\n",
      "ν2 + ν1y\n",
      "«k\n",
      "I¯IR+(y)\n",
      "mean\n",
      "ν2(ν1 + λ)/(ν1(ν2 −2)), for ν2 > 2\n",
      "variance\n",
      "2\n",
      "„ν2\n",
      "ν1\n",
      "«2 „ (ν1 + λ)2 + (ν1 + 2λ)(ν2 −2)\n",
      "(ν2 −2)2(ν2 −4)\n",
      "«\n",
      ", for ν2 > 4\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "842\n",
      "Appendix A. Important Probability Distributions\n",
      "Table A.4. Distributions Useful as Priors for the Normal Parameters\n",
      "inverted gamma\n",
      "PDF\n",
      "1\n",
      "Γ(α)βα\n",
      "„1\n",
      "y\n",
      "«α+1\n",
      "e−1/βy I¯IR+(y)\n",
      "α, β ∈IR+\n",
      "mean\n",
      "1/β(α −1) for α > 1\n",
      "variance 1/(β2(α −1)2(α −2)) for α > 2\n",
      "inverted chi-squared PDF\n",
      "1\n",
      "Γ(ν/2)2ν/2\n",
      "„1\n",
      "y\n",
      "«ν/2+1\n",
      "e−1/2y I¯IR+(y)\n",
      "ν ∈IR+\n",
      "mean\n",
      "1/(ν −2) for ν > 2\n",
      "variance 2/((ν −2)2(ν −4)) for ν > 4\n",
      "inverted Wishart\n",
      "PDF\n",
      "*************************\n",
      "***************\n",
      "mean\n",
      "**************\n",
      "variance ***************\n",
      "Table A.5. Distributions Derived from the Univariate Normal\n",
      "lognormal\n",
      "PDF\n",
      "1\n",
      "√\n",
      "2πσ\n",
      "y−1e−(log(y)−µ)2/2σ2 I¯IR+(y)\n",
      "µ ∈IR; σ ∈IR+\n",
      "mean\n",
      "eµ+σ2/2\n",
      "variance e2µ+σ2(eσ2 −1)\n",
      "inverse Gaussian\n",
      "PDF\n",
      "s\n",
      "λ\n",
      "2πy3 e−λ(y−µ)2/2µ2y I¯IR+(y)\n",
      "µ, λ ∈IR+\n",
      "mean\n",
      "µ\n",
      "variance µ3/λ\n",
      "skew normal\n",
      "PDF\n",
      "1\n",
      "πσ e−(y−µ)2/2σ2 Z λ(y−µ)/σ\n",
      "−∞\n",
      "e−t2/2 dt\n",
      "µ, λ ∈IR; σ ∈IR+ mean\n",
      "µ + σ\n",
      "q\n",
      "2λ\n",
      "π(1+λ2)\n",
      "variance σ2(1 −2λ2/π)\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Appendix A. Important Probability Distributions\n",
      "843\n",
      "Table A.6. Other Continuous Distributions (PDFs are wrt Lebesgue measure)\n",
      "beta\n",
      "PDF\n",
      "Γ(α + β)\n",
      "Γ(α)Γ(β)yα−1(1 −y)β−1 I[0,1](y)\n",
      "α, β ∈IR+\n",
      "mean\n",
      "α/(α + β)\n",
      "variance\n",
      "αβ\n",
      "(α + β)2(α + β + 1)\n",
      "Dirichlet\n",
      "PDF\n",
      "Γ(Pd+1\n",
      "i=1 αi)\n",
      "Qd+1\n",
      "i=1 Γ(αi)\n",
      "d\n",
      "Y\n",
      "i=1\n",
      "yαi−1\n",
      "i\n",
      " \n",
      "1 −\n",
      "d\n",
      "X\n",
      "i=1\n",
      "yi\n",
      "!αd+1−1\n",
      "I[0,1]d(y)\n",
      "α ∈IRd+1\n",
      "+\n",
      "mean\n",
      "α/∥α∥1\n",
      "(αd+1/∥α∥1 is the “mean of Yd+1”.)\n",
      "variance\n",
      "α (∥α∥1 −α)\n",
      "∥α∥2\n",
      "1(∥α∥1 + 1)\n",
      "uniform; U(θ1, θ2) PDF\n",
      "1\n",
      "θ2 −θ1 I[θ1,θ2](y)\n",
      "θ1 < θ2 ∈IR\n",
      "mean\n",
      "(θ2 + θ1)/2\n",
      "variance\n",
      "(θ2\n",
      "2 −2θ1θ2 + θ2\n",
      "1)/12\n",
      "Cauchy\n",
      "PDF\n",
      "1\n",
      "πβ\n",
      "„\n",
      "1 +\n",
      "“\n",
      "y−γ\n",
      "β\n",
      "”2«\n",
      "γ ∈IR; β ∈IR+\n",
      "mean\n",
      "does not exist\n",
      "variance\n",
      "does not exist\n",
      "logistic\n",
      "PDF\n",
      "e−(y−µ)/β\n",
      "β(1 + e−(y−µ)/β)2\n",
      "µ ∈IR; β ∈IR+\n",
      "mean\n",
      "µ\n",
      "variance\n",
      "β2π2/3\n",
      "Pareto\n",
      "PDF\n",
      "αγα\n",
      "yα+1 I[γ,∞[(y)\n",
      "α, γ ∈IR+\n",
      "mean\n",
      "αγ/(α −1) for α > 1\n",
      "variance\n",
      "αγ2/((α −1)2(α −2)) for α > 2\n",
      "power function\n",
      "PDF\n",
      "(y/β)α I[0,β[(y)\n",
      "α, β ∈IR+\n",
      "mean\n",
      "αβ/(α + 1)\n",
      "variance\n",
      "αβ2/((α + 2)(α + 1)2)\n",
      "von Mises\n",
      "PDF\n",
      "1\n",
      "2πI0(κ)eκ cos(x−µ) I[µ−π,µ+π](y)\n",
      "µ ∈IR; κ ∈IR+\n",
      "mean\n",
      "µ\n",
      "variance\n",
      "1 −(I1(κ)/I0(κ))2\n",
      "continued ...\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "844\n",
      "Appendix A. Important Probability Distributions\n",
      "Table A.6. Other Continuous Distributions (continued)\n",
      "gamma\n",
      "PDF\n",
      "1\n",
      "Γ(α)βα yα−1e−y/β I¯IR+(y)\n",
      "α, β ∈IR+\n",
      "mean\n",
      "αβ\n",
      "variance\n",
      "αβ2\n",
      "three-parameter gamma PDF\n",
      "1\n",
      "Γ(α)βα (y −γ)α−1e−(y−γ)/β I]γ,∞[(y)\n",
      "α, β ∈IR+; γ ∈IR\n",
      "mean\n",
      "αβ + γ\n",
      "variance\n",
      "αβ2\n",
      "exponential\n",
      "PDF\n",
      "θ−1e−y/θ I¯IR+(y)\n",
      "θ ∈IR+\n",
      "mean\n",
      "θ\n",
      "variance\n",
      "θ2\n",
      "double exponential\n",
      "PDF\n",
      "1\n",
      "2θ e−|y−µ|/θ\n",
      "µ ∈IR; θ ∈IR+\n",
      "mean\n",
      "µ\n",
      "(folded exponential)\n",
      "variance\n",
      "2θ2\n",
      "Weibull\n",
      "PDF\n",
      "α\n",
      "β yα−1e−yα/β I¯IR+(y)\n",
      "α, β ∈IR+\n",
      "mean\n",
      "β1/αΓ(α−1 + 1)\n",
      "variance\n",
      "β2/α `\n",
      "Γ(2α−1 + 1) −(Γ(α−1 + 1))2´\n",
      "extreme value (Type I) PDF\n",
      "1\n",
      "β e−(y−α)/β exp(e−(y−α)/β)\n",
      "α ∈IR; β ∈IR+\n",
      "mean\n",
      "α −βΓ′(1)\n",
      "variance\n",
      "β2π2/6\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "B\n",
      "Useful Inequalities in Probability\n",
      "Inequalities involving functions of events and random variables are important\n",
      "throughout the ﬁeld of probability and statistics. Two important uses are for\n",
      "showing that one procedure is better than another and for showing that some\n",
      "sequence converges to a given object (a constant, a function, or a set).\n",
      "In the following, for simplicity, we will assume X ∈IR.\n",
      "B.1 Preliminaries\n",
      "A simple, but surprisingly useful inequality states that if E(X2) < ∞, then\n",
      "the variance is the minimum expectation of the form E((X −c)2) for any\n",
      "constant c. In other words, the minimum of E((X −c)2) occurs at c = E(X)).\n",
      "We see this by writing\n",
      "E((X −c)2) = E((X −E(X) + E(X) −c)2)\n",
      "= E((X −E(X))2) + E((E(X) −c)2) +\n",
      "2E((X −E(X))(E(X) −c))\n",
      "= E((X −E(X))2) + E((E(X) −c)2)\n",
      "≥E((X −E(X))2).\n",
      "(B.1)\n",
      "We will use various inequalities often, so we collect a number of them in\n",
      "this appendix, where we have categorized them into ﬁve types depending of\n",
      "the kinds of expressions in the inequalities. These ﬁve types involve relations\n",
      "between\n",
      "•\n",
      "Pr(X ∈Ai) and Pr(X ∈∪Ai) or Pr(X ∈∩Ai), e.g., Bonferroni’s\n",
      "•\n",
      "Pr(X ∈A) and E(f(X)), e.g., Chebyshev\n",
      "•\n",
      "E(f(X)) and f(E(X)), e.g., Jensen’s\n",
      "•\n",
      "E(f1(X, Y )) and E(f2(X)) and E(f3(Y )), e.g., covariance, Cauchy-Schwarz,\n",
      "information\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "846\n",
      "Appendix B. Useful Inequalities in Probability\n",
      "•\n",
      "V(Y ) and V (E(Y |X)), e.g., Rao-Blackwell\n",
      "Any special case of these involves an appropriate deﬁnition of A or f (e.g.,\n",
      "nonnegative, convex, etc.)\n",
      "A more general case of the inequalities is to replace distributions, and\n",
      "hence expected values, by conditioning on a sub-σ-ﬁeld, A.\n",
      "For each type of inequality there is an essentially straightforward method\n",
      "of proof.\n",
      "Some of these inequalities involve absolute values of the random variable.\n",
      "To work with these inequalities, it is useful to recall the triangle inequality\n",
      "for the absolute value of real numbers:\n",
      "|x + y| ≤|x| + |y|.\n",
      "(B.2)\n",
      "We can prove this merely by considering all four cases for the signs of x and\n",
      "y.\n",
      "This inequality generalizes immediately to | Pxi| ≤P|xi|.\n",
      "Expectations of absolute values of functions of random variables are func-\n",
      "tions of norms. (A norm is a function of x that (1) is positive unless x = 0\n",
      "a.e., that (2) is equivariant to scalar multiplication, and that (3) satisﬁes the\n",
      "triangle inequality.) The important form (E(|X|p))1/p for 1 ≤p is an Lp norm,\n",
      "∥X∥p. Some of the inequalities given below involving expectations of absolute\n",
      "values of random variables are essentially triangle inequalities and their truth\n",
      "establishes the expectation as a norm.\n",
      "Some of the expectations discussed below are recognizable as familiar\n",
      "norms over vector spaces. For example, the expectation in Minkowski’s in-\n",
      "equality is essentially the Lp norm of a vector, which is deﬁned for an n-vector\n",
      "x in a ﬁnite-dimensional vector space as ∥x∥p ≡(P |xi|p)1/p. Minkowski’s in-\n",
      "equality in this case is ∥x + y∥p ≤∥x∥p + ∥y∥p. For p = 1, this is the triangle\n",
      "inequality for absolute values given above.\n",
      "B.2 Pr(X ∈Ai) and Pr(X ∈∪Ai) or Pr(X ∈∩Ai)\n",
      "These inequalities are often useful in working with order statistics and in tests\n",
      "of multiple hypotheses. Instead of Pr(X ∈Ai), we may write Pr(Ai).\n",
      "Theorem B.1 (Bonferroni’s inequality)\n",
      "Given events A1, . . ., An, we have\n",
      "Pr(∩Ai) ≥\n",
      "X\n",
      "Pr(Ai) −n + 1.\n",
      "(B.3)\n",
      "Proof. We will use induction. For n = 1, we have Pr(A1) ≥Pr(A1),\n",
      "and for n = 2, we have Pr(A1 ∩A2) = Pr(A1) + Pr(A2) −Pr(A1 ∪A2) ≥\n",
      "Pr(A1) + Pr(A2) −1.\n",
      "Now assume Pr(∩k\n",
      "i=1Ai) ≥Pk\n",
      "i=1 AiPr(Ai) −k + 1, and consider ˜k =\n",
      "k + 1. We have (from the n = 2 case) Pr(∩k\n",
      "i=1Ai ∩Ak+1) ≥Pr(∩k\n",
      "i=1Ai) +\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "B.3 Pr(X ∈A) and E(f(X))\n",
      "847\n",
      "Pr(Ak+1) −1, and now simplifying and substituting the induction hypothesis\n",
      "for Pr(∩k\n",
      "i=1Ai), we have\n",
      "Pr(∩k+1\n",
      "i=1 Ai) ≥\n",
      "k+1\n",
      "X\n",
      "i=1\n",
      "Pr(Ai) −k.\n",
      "Corollary B.2.0.1\n",
      "Given a random sample X1, . . ., Xn and ﬁxed constants a1, . . ., an. For the\n",
      "order statistics, X(1), . . ., X(n), we have\n",
      "Pr\n",
      "\u0000∩{X(i) ≤ai}\n",
      "\u0001\n",
      "≥\n",
      "Y\n",
      "Pr(X(i) ≤ai).\n",
      "(B.4)\n",
      "Proof. Same.\n",
      "B.3 Pr(X ∈A) and E(f(X))\n",
      "An important class of inequalities bound tail probabilities of a random vari-\n",
      "able, that is, limit the probability that the random variable will take on a\n",
      "value beyond some distance from the expected value of the random variable.\n",
      "The important general form involving Pr(X ∈A) and E(f(X)) is Markov’s\n",
      "inequality involving absolute moments. Chebyshev’s inequality, of which there\n",
      "are two forms, is a special case of it. Useful generalizations of Markov’s inequal-\n",
      "ity involve sums of sequences of random variables. The Bernstein inequalities,\n",
      "including the special case of Hoeﬀding’s inequality, and the H´ajek-R`enyi in-\n",
      "equality, including the special case of Kolmogorov’s inequality, apply to sums\n",
      "of sequences of independent random variables. There are extensions of these\n",
      "inequalities for sums of sequences with weak dependence, such as martingales.\n",
      "Following the basic Markov’s and Chebyshev’s inequalities, we state without\n",
      "proof some inequalities for sums of independent but not necessarily identi-\n",
      "cally distributed random variables. In Section 1.6 we consider an extension of\n",
      "Hoeﬀding’s inequality to martingales (Azuma’s inequality) and an extension\n",
      "of Kolmogorov’s inequality to submartingales (Doob’s submartingale inequal-\n",
      "ity).\n",
      "Theorem B.3.1 (Markov’s inequality)\n",
      "For ϵ > 0, k > 0, and r.v. X ∋E(|X|k) exists,\n",
      "Pr(|X| ≥ϵ) ≤1\n",
      "ϵk E\n",
      "\u0000|X|k\u0001\n",
      ".\n",
      "(B.5)\n",
      "Proof. For a nonnegative random variable Y ,\n",
      "E(Y ) ≥\n",
      "Z\n",
      "y≥ϵ\n",
      "y dP (y) ≥ϵ\n",
      "Z\n",
      "y≥ϵ\n",
      "dP (y) = ϵPr(Y ≥ϵ).\n",
      "Now let Y = |X|k.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "848\n",
      "Appendix B. Useful Inequalities in Probability\n",
      "Corollary B.3.1.1 (Chebyshev’s inequality)\n",
      "For ϵ > 0,\n",
      "Pr(|X −E(X)| ≥ϵ) ≤1\n",
      "ϵ2 V(X)\n",
      "(B.6)\n",
      "Proof. In Markov’s inequality, let k = 2, and replace X by X −E(X).\n",
      "Corollary B.3.1.2 (Chebyshev’s inequality (another form))\n",
      "For f ∋f(x) ≥0 and ϵ > 0,\n",
      "Pr(f(X) ≥ϵ) ≤1\n",
      "ϵ E(f(X))\n",
      "(B.7)\n",
      "Proof. Same as Markov’s inequality; start with E(f(X)).\n",
      "Chebyshev’s inequality is often useful for ϵ =\n",
      "p\n",
      "V(X). There are also\n",
      "versions of Chebyshev’s inequality for speciﬁc families of distributions.\n",
      "•\n",
      "3σ rule for a unimodal random variable\n",
      "If X is a random variable with a unimodal absolutely continuous distribu-\n",
      "tion, and σ =\n",
      "p\n",
      "V(X), then\n",
      "Pr(|X −E(X)| ≥3σ) ≤4\n",
      "81.\n",
      "(B.8)\n",
      "See Dharmadhikari and Joag-Dev (1988).\n",
      "•\n",
      "Normal tail probability\n",
      "If X ∼N(µ, σ2), then\n",
      "Pr(|X −µ| ≥kσ) ≤\n",
      "1\n",
      "3k2 .\n",
      "(B.9)\n",
      "See DasGupta (2000).\n",
      "There are a number of inequalities that generalize Chebyshev’s inequal-\n",
      "ity to ﬁnite partial sums of a sequence of independent random variables\n",
      "X1, X2, . . . over a common probability space such that for each, E(Xi) = 0\n",
      "and E(X2\n",
      "i ) < ∞. (The common probability space means that E(·) has the\n",
      "same meaning for each i, but the Xi do not necessarily have the same dis-\n",
      "tribution.) These inequalities are often called the The Bernstein inequalities,\n",
      "but some of them have other names.\n",
      "Theorem B.3.2 (the Hoeﬀding inequality)\n",
      "Let X1, . . ., Xn be independent, E(Xi) = 0, and Pr(|Xi| ≤c) = 1. Then for\n",
      "any t > 0,\n",
      "Pr\n",
      " n\n",
      "X\n",
      "i=1\n",
      "Xi > t\n",
      "!\n",
      "≤exp\n",
      "\u0012\n",
      "−\n",
      "t2/2\n",
      "Pn\n",
      "i=1 E(X2\n",
      "i ) + ct/3\n",
      "\u0013\n",
      ".\n",
      "(B.10)\n",
      "The Hoeﬀding inequality is a special case of the Azuma inequality for mar-\n",
      "tingales (see Section 1.6).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "B.4 E(f(X)) and f(E(X))\n",
      "849\n",
      "Theorem B.3.3 (Kolmogorov’s inequality)\n",
      "For a sequence of independent random variables X1, X2, . . . over a common\n",
      "probability space such that for each, E(Xi) = 0 and E(X2\n",
      "i ) < ∞, let Sk =\n",
      "Pk\n",
      "i=1 Xi. Then for any positive integer n and any ϵ > 0,\n",
      "Pr\n",
      "\u0012\n",
      "max\n",
      "1≤k≤n|Sk| ≥ϵ\n",
      "\u0013\n",
      "≤1\n",
      "ϵ2 V(Sn).\n",
      "(B.11)\n",
      "This is a special case of Doob’s submartingale inequality (see page 133 in\n",
      "Section 1.6). It is also a special case of the H´ajek-R`enyi inequality:\n",
      "Theorem B.3.4 (the H´ajek-R`enyi inequality)\n",
      "Let X1, X2, . . . be a sequence of independent random variables over a common\n",
      "probability space such that for each E(X2\n",
      "i ) < ∞. Then for any positive integer\n",
      "n and any ϵ > 0,\n",
      "Pr\n",
      " \n",
      "max\n",
      "1≤k≤n ck\n",
      "\f\f\f\f\f\n",
      "k\n",
      "X\n",
      "i=1\n",
      "(Xi −E(Xi))\n",
      "\f\f\f\f\f ≥ϵ\n",
      "!\n",
      "≤1\n",
      "ϵ2\n",
      "n\n",
      "X\n",
      "i=1\n",
      "c2\n",
      "i V(Xi),\n",
      "(B.12)\n",
      "where c1 ≥· · · ≥cn > 0 are constants.\n",
      "B.4 E(f(X)) and f(E(X))\n",
      "Theorem B.4.1 (Jensen’s inequality)\n",
      "For f a convex function over the support of the r.v. X (and all expectations\n",
      "shown exist),\n",
      "f(E(X)) ≤E(f(X)).\n",
      "(B.13)\n",
      "Proof. By the deﬁnition of convexity, f convex over D ⇒∃c ∋∀t ∈D ∋\n",
      "c(x −t) + f(t) ≤f(x). (Notice that L(x) = c(x −t) + f(t) is a straight line\n",
      "through the point (t, f(t)). By the deﬁnition of convexity, f is convex if its\n",
      "value at the weighted average of two points does not exceed the weighted\n",
      "average of the function at those two points.) Now, given this, let t = E(X)\n",
      "and take expectations of both sides of the inequality.\n",
      "If f is strictly convex, it is clear\n",
      "f(E(X)) < E(f(X))\n",
      "(B.14)\n",
      "unless f(X) = E(f(X)) with probability 1.\n",
      "For a concave function, the inequality is reversed. (The negative of a con-\n",
      "cave function is convex.)\n",
      "Some simple examples for a nonconstant positive random variable X:\n",
      "•\n",
      "Monomials of even power: for k = 2, 4, 6, . . .,\n",
      "E(X)k ≤E(Xk).\n",
      "This inequality implies the familiar fact that E(X) ≥0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "850\n",
      "Appendix B. Useful Inequalities in Probability\n",
      "•\n",
      "Reciprocals:\n",
      "1\n",
      "E(X) ≤E\n",
      "\u0012 1\n",
      "X\n",
      "\u0013\n",
      "•\n",
      "Logs:\n",
      "E(log(X)) ≤log(E(X)).\n",
      "The canonical picture is that of a quadratic function of a uniform random\n",
      "variable:\n",
      "X\n",
      "f(X)\n",
      "E(X)\n",
      "E(f(X))\n",
      "f(E(X))\n",
      "Figure B.1.\n",
      "Jensen’s Inequality\n",
      "There are several other consequences of Jensen’s inequality. The ﬁrst one\n",
      "we state applies to Kullback-Leibler information; that is, the entropy distance.\n",
      "Corollary B.4.1.1 (Nonnegativity of the entropy distance)\n",
      "If f and g are probability densities, Ef(log(f(X)/g(X))), is the entropy dis-\n",
      "tance between f and g with respect to g. It is also called the Kullback-Leibler\n",
      "information or Kullback-Leibler distance. It is nonnegative:\n",
      "Ef(log(f(X)/g(X))) ≥0.\n",
      "(B.15)\n",
      "Proof.\n",
      "Ef(log(f(X)/g(X))) = −Ef(log(g(X)/f(X)))\n",
      "≥−log(Ef(g(X)/f(X)))\n",
      "= 0.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "B.4 E(f(X)) and f(E(X))\n",
      "851\n",
      "A related fact applies to any nonnegative integrable functions f and g on\n",
      "a measure space with a σ-ﬁnite measure ν, for which R fdν ≥R gdν > 0:\n",
      "Z\n",
      "f(log(f/g))dν ≥0.\n",
      "(B.16)\n",
      "This can be proved as above by normalizing the functions, thus forming den-\n",
      "sities.\n",
      "Applying Jensen’s inequality to the deﬁnition of the entropy distance, we\n",
      "have\n",
      "Corollary B.4.1.2\n",
      "For the PDFs f and g.\n",
      "Ef(log(f(X))) ≥Ef(log(g(X))).\n",
      "(B.17)\n",
      "This inequality, which is important for showing the convergence of the\n",
      "EM algorithm, is also sometimes called the “information inequality” (but\n",
      "see (B.25)).\n",
      "The strict form of Jensen’s inequality (B.14) also applies to the consequent\n",
      "inequalities, and hence, in the case of equality of the expectations, we can get\n",
      "equality of the functions. For example,\n",
      "Corollary B.4.1.3\n",
      "For the PDFs f and g,\n",
      "Ef(log(f(X))) = Ef(log(g(X))) ⇔f(X) = g(X) a.s.\n",
      "(B.18)\n",
      "Proof.:\n",
      "⇒\n",
      "By the equality of Ef(log(f(X))) and Ef(log(g(X))) we have\n",
      "Z\n",
      "{f>0}\n",
      "g(x)dx = 1,\n",
      "and so for any A,\n",
      "Z\n",
      "A\n",
      "g(x)dx =\n",
      "Z\n",
      "A∩{f>0}\n",
      "g(x)dx\n",
      "= Ef (g(X)/f(X)|X ∈A ∩{f > 0})\n",
      "= Pr (X ∈A ∩{f > 0})\n",
      "=\n",
      "Z\n",
      "A\n",
      "f(x)dx,\n",
      "hence f(X) = g(X) a.s.\n",
      "The proof of ⇐is similar.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "852\n",
      "Appendix B. Useful Inequalities in Probability\n",
      "B.5 E(f(X, Y )) and E(g(X)) and E(h(Y ))\n",
      "In many of the inequalities in this section, the functions f, g, and h are norms.\n",
      "The inequalities hold for general Lp norms, and although we will consider the\n",
      "inequality relationship between expected values, similar inequalities often for\n",
      "real numbers, vectors, or random variables.\n",
      "The inequalities are basically of two types:\n",
      "•\n",
      "H¨older: E(|XY |) ≤\n",
      "\u0010\n",
      "E(|X|p)\n",
      "\u00111/p\u0010\n",
      "E(|Y |q)\n",
      "\u00111/q\n",
      "•\n",
      "Minkowski: (E(|X + Y |p))1/p ≤(E(|X|p))1/p + (E(|Y |p))1/p\n",
      "H¨older inequality is somewhat more basic; it is used in the proof of Minkowski’s\n",
      "inequality. Compare inequalities (0.0.30) and (0.0.31) for vectors in IRd, and\n",
      "see the discussion on page 642.\n",
      "Note that Minkowski’s inequality has an interesting consequence: it means\n",
      "that (E(| · |p))1/p is a norm.\n",
      "Several other inequalities are special cases of these two.\n",
      "In some inequalities in this section, the functions are second-degree mono-\n",
      "mials. The basic special inequality of this form is the Cauchy-Schwartz in-\n",
      "equality, which then leads to one of the most important inequalities in appli-\n",
      "cations in statistics, the covariance inequality. The covariance inequality, in\n",
      "turn, leads to fundamental bounds on the variances of estimators.\n",
      "Theorem B.5.1 (H¨older’s inequality)\n",
      "For p, q > 1 and 1\n",
      "p + 1\n",
      "q = 1 (and if all expectations shown exist),\n",
      "E(|XY |) ≤\n",
      "\u0010\n",
      "E(|X|p)\n",
      "\u00111/p\u0010\n",
      "E(|Y |q)\n",
      "\u00111/q\n",
      ".\n",
      "(B.19)\n",
      "Note that q = p/(p −1); p and q as in this inequality are called dual indices.\n",
      "Proof. If E(|X|p) = 0 or E(|Y |q) = 0, then true because both sides = 0 wp1.\n",
      "Hence, assume both > 0.\n",
      "For p and q as in hypothesis, ∀a, b > 0, ∃s, t ∋a = es/p and b = et/q. Now\n",
      "ex is convex, so es/p+t/q ≤1\n",
      "pes + 1\n",
      "qet, or ab ≤ap/p + bq/q.\n",
      "Let\n",
      "a =\n",
      "\f\f\f\f\f\f\f\n",
      "X(ω)\n",
      "\u0010\n",
      "E(|X|p)\n",
      "\u00111/p\n",
      "\f\f\f\f\f\f\f\n",
      "and\n",
      "b =\n",
      "\f\f\f\f\f\f\f\n",
      "Y (ω)\n",
      "\u0010\n",
      "E(|Y |q)\n",
      "\u00111/q\n",
      "\f\f\f\f\f\f\f\n",
      "and so\n",
      "|X(ω)Y (ω)| ≤\n",
      "\u0010\n",
      "E(|X|p)\n",
      "\u00111/p\u0010\n",
      "E(|Y |q)\n",
      "\u00111/q \u0012|X(ω)|p\n",
      "E(|X|p)\n",
      "1\n",
      "p + |Y (ω)|q\n",
      "E(|Y |q)\n",
      "1\n",
      "q\n",
      "\u0013\n",
      ".\n",
      "Now take expectations. (The notation X(ω) and Y (ω) is meant to emphasize\n",
      "how to take expectation of XY .)\n",
      "There are several inequalities that derive from H¨older’s inequality. Some of\n",
      "these inequalities are given in the following corollaries to Theorem B.5.1.First\n",
      "is a special case of Jensen’s inequality.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "B.5 E(f(X,Y )) and E(g(X)) and E(h(Y ))\n",
      "853\n",
      "Corollary B.5.1.1 (special case of Jensen’s inequality)\n",
      "E(|X|) ≤\n",
      "\u0010\n",
      "E(|X|p)\n",
      "\u00111/p\n",
      ",\n",
      "Proof. Set Y ≡1 in H¨older’s inequality.\n",
      "Note that with p = 2, Corollary B.5.1.1 is a special case of the Cauchy-\n",
      "Schwarz inequality,\n",
      "E(|X|) ≤\n",
      "\u0010\n",
      "E(X2)\n",
      "\u00111/2\n",
      ",\n",
      "in Corollary B.5.1.3 below.\n",
      "Corollary B.5.1.2 (Lyapunov’s inequality)\n",
      "For 1 ≤r ≤s (and if all expectations shown exist),\n",
      "(E(|X|r))1/r ≤(E(|X|s))1/s.\n",
      "(B.20)\n",
      "Proof. First, we observe this is true for r = s, and for r = 1 (in which it is\n",
      "a form of Jensen’s inequality). If 1 < r < s, replace |X| in the special case\n",
      "of H¨older’s inequality above with |X|r, and let s = pr for 1 < p. This yields\n",
      "(E(|X|r))1/r ≤(E(|X|s))1/s.\n",
      "Corollary B.5.1.3 (Schwarz inequality, or Cauchy-Schwarz inequality)\n",
      "E(|XY |) ≤\n",
      "\u0010\n",
      "E(X2)E(Y 2)\n",
      "\u00111/2\n",
      ".\n",
      "(B.21)\n",
      "Proof. Let p = q = 2 in H¨older’s inequality.\n",
      "Another proof: For nonnegative r.v. X and Y and all t (real), E((tX +Y )2) =\n",
      "t2E(X2) + 2tE(XY ) + E(Y 2) ≥0. Hence the discriminant of the quadratic\n",
      "formula ≤0. Now, for any r.v., take absolute value.\n",
      "Corollary B.5.1.4 (covariance inequality) (see page 36)\n",
      "If the second moments of X and Y are ﬁnite, then\n",
      "\u0010\n",
      "E\n",
      "\u0000(X −E(X))(Y −E(Y )\n",
      "\u0001\u00112\n",
      "≤E\n",
      "\u0000(X −E(X))2\u0001\n",
      "E\n",
      "\u0000(Y −E(Y ))2\u0001\n",
      "(B.22)\n",
      "or\n",
      "\u0000Cov(X, Y )\n",
      "\u00012 ≤V(X) V(Y ).\n",
      "(B.23)\n",
      "Proof. The covariance inequality is essentially the same as the Cauchy-\n",
      "Schwarz inequality.\n",
      "The covariance inequality leads to useful lower bounds on the variances\n",
      "of estimators. These are of two types. One type includes the Hammersley-\n",
      "Chapman-Robbins inequality and its extension, the Kshirsagar inequality. The\n",
      "other type, which is based on Fisher information, requires some “regularity\n",
      "conditions”.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "854\n",
      "Appendix B. Useful Inequalities in Probability\n",
      "Corollary B.5.1.5 (Hammersley-Chapman-Robbins inequality)\n",
      "Let X be a random variable in IRd with PDF p(x; θ) and let Eθ(T(X) = g(θ).\n",
      "Let µ be a ﬁxed measure on X ⊆IRd such that p(x; θ) ≪µ. Now deﬁne S(θ)\n",
      "such that\n",
      "p(x; θ) > 0 a.e. x ∈S(θ)\n",
      "p(x; θ) = 0 a.e. x /∈S(θ).\n",
      "Then\n",
      "V(T(X)) ≥\n",
      "sup\n",
      "t∋S(θ)⊇S(θ+t)\n",
      "(g(θ + t) −g(θ))2\n",
      "Eθ\n",
      "\u0012\u0010\n",
      "p(X;θ+t)\n",
      "p(X;θ)\n",
      "\u00112\u0013.\n",
      "(B.24)\n",
      "Proof. This inequality follows from the covariance inequality, by ﬁrst consid-\n",
      "ering the case for an arbitrary t such that g(θ + t) ̸= g(θ).\n",
      "Corollary B.5.1.6 (Kshirsagar inequality)\n",
      "Corollary B.5.1.7 (information inequality)\n",
      "Subject to some “regularity conditions” (see Section 2.3), if X has PDF\n",
      "p(x; θ), then\n",
      "V(T(X)) ≥\n",
      "\u0010\n",
      "∂E(T (X))\n",
      "∂θ\n",
      "\u00112\n",
      "Eθ\n",
      "\u0012\u0010\n",
      "∂log p(X;θ)\n",
      "∂θ\n",
      "\u00112\u0013\n",
      "(B.25)\n",
      "The denominator of the quantity on the right side of the inequality is\n",
      "called the Fisher information, or just the information. Notice the similarity\n",
      "of this inequality to the Hammersley-Chapman-Robbins inequality, although\n",
      "the information inequality requires more conditions.\n",
      "Under the regularity conditions, which basically allow the interchange of\n",
      "integration and diﬀerentiation, the information inequality follows immediately\n",
      "from the covariance inequality.\n",
      "We consider the multivariate form of this inequality to Section 3.1.3. Our\n",
      "main interest will be in its application in unbiased estimation, in Section 5.1.\n",
      "If T(X) is an unbiased estimator of a diﬀerentiable function g(θ), the right\n",
      "side of the inequality together with derivatives of g(θ) forms the Cram´er-Rao\n",
      "lower bound, inequality (3.39), and the Bhattacharyya lower bound, inequal-\n",
      "ity (5.29).\n",
      "Theorem B.5.2 (Minkowski’s inequality)\n",
      "For 1 ≤p,\n",
      "(E(|X + Y |p))1/p ≤(E(|X|p))1/p + (E(|Y |p))1/p\n",
      "(B.26)\n",
      "This is a triangle inequality for Lp norms and related functions.\n",
      "Proof. First, observe the truth for p = 1 using the triangle inequality for\n",
      "the absolute value, |x + y| ≤|x| + |y|, giving E(|X + Y |) ≤E(|X|) + E(|Y |).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "B.7 Multivariate Extensions\n",
      "855\n",
      "Now assume p > 1. Now,\n",
      "E(|X + Y |p) = E(|X + Y ||X + Y |p−1)\n",
      "≤E(|X||X + Y |p−1) + E(|Y ||X + Y |p−1),\n",
      "where the inequality comes from the triangle inequality for absolute values.\n",
      "From H¨older’s inequality on the terms above with q = p/(p −1), we have\n",
      "E(|X + Y |p) ≤(E(|X|p))1/p(E(|X + Y |p))1/q + E(|Y |p))1/p(E(|X + Y |p))1/q.\n",
      "Now, if E(|X + Y |p) = 0, Minkowski’s inequality holds. On the other hand,\n",
      "if E(|X + Y |p) ̸= 0, it is positive, and so divide through by (E(|X + Y |p))1/q,\n",
      "recalling again that q = p/(p −1).\n",
      "Minkowski’s inequality is a special case of two slightly tighter inequalities;\n",
      "one for p ∈[1, 2] due to Esseen and von Bahr (1965), and one for p ≥2 due\n",
      "to Marcinkiewicz and Zygmund (1937).\n",
      "An inequality that derives from Minkowski’s inequality, but which applies\n",
      "directly to real numbers or random variables, is the following.\n",
      "•\n",
      "For 0 ≤p,\n",
      "|X + Y |p ≤2p(|X|p + |Y |p)\n",
      "(B.27)\n",
      "This is true because ∀ω ∈Ω, ∥X(ω) + Y (ω)∥≤2 max{∥X(ω)∥, ∥Y (ω)∥},\n",
      "and so\n",
      "∥X(ω) + Y (ω)∥p ≤max{2p∥X(ω)∥p, 2p∥Y (ω)∥p}\n",
      "≤2p∥X(ω)∥p + 2p∥Y (ω)∥p.\n",
      "B.6 V(Y ) and V\n",
      "\u0000E(Y |X)\n",
      "\u0001\n",
      "•\n",
      "Rao-Blackwell inequality\n",
      "V\n",
      "\u0000E(Y |X)\n",
      "\u0001\n",
      "≤V(Y )\n",
      "(B.28)\n",
      "This follows from the equality V(Y ) = V\u0000E(Y |X)\u0001 + E\u0000V(Y |X)\u0001.\n",
      "B.7 Multivariate Extensions\n",
      "There are multivariate extensions of most of these inequalities. In some cases,\n",
      "the multivariate extensions apply to the minimum or maximum element of a\n",
      "vector.\n",
      "Some inequalities involving simple inequalities are extended by conditions\n",
      "on vector norms, and the ones involving variances are usually extended by pos-\n",
      "itive (or semi-positive) deﬁniteness of the diﬀerence of two variance-covariance\n",
      "matrices.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "856\n",
      "Appendix B. Useful Inequalities in Probability\n",
      "Notes and Further Reading\n",
      "Chebyshev’s inequality, Corollary B.3.1.1, in its various forms is one of the\n",
      "most useful inequalities in probability theory. DasGupta (2000) discusses var-\n",
      "ious forms of this basic inequality.\n",
      "Another useful general-purpose relationship is the Cauchy-Schwarz in-\n",
      "equality, Corollary B.5.1.3. Steele (2004) discusses origins, various forms, and\n",
      "various proofs of this inequality, and in so doing illustrates interesting rela-\n",
      "tionships among diverse mathematical inequalities.\n",
      "Inequalities are very useful in developing asymptotic results and in prov-\n",
      "ing limit theorems. DasGupta (2008) on asymptotic theory contains a very\n",
      "extensive compendium of inequalities on pages 633 to 687. None are proved\n",
      "there, but each is accompanied by a reference to a proof. Petrov (1995) begins\n",
      "with a survey of inequalities in probability theory, including proofs, and then\n",
      "gives a number of limit theorem theorems the proofs of which often rely on\n",
      "the inequalities.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "C\n",
      "Notation and Deﬁnitions\n",
      "All notation used in this work is “standard”. I have opted for simple nota-\n",
      "tion, which, of course, results in a one-to-many map of notation to object\n",
      "classes. Within a given context, however, the overloaded notation is generally\n",
      "unambiguous. I have endeavored to use notation consistently.\n",
      "This appendix is not intended to be a comprehensive listing of deﬁnitions.\n",
      "C.1 General Notation\n",
      "There are some standard phrases widely used in mathematical statistics, and\n",
      "so for these we adopt special symbols. These phrases are sometime omitted\n",
      "because the property or condition is implicitly assumed. I try to be explicit\n",
      "about my assumptions. If we agree on simple character strings to represent\n",
      "these properties and conditions, I am more likely to state them explicitly when\n",
      "they are relevant.\n",
      "wrt\n",
      "“with respect to”.\n",
      "wlog\n",
      "“without loss of generality”.\n",
      "iid or\n",
      "iid\n",
      "∼\n",
      "“independent and identically distributed (as)”.\n",
      "Uppercase italic Latin and Greek letters, such as A, B, E, Λ, etc., are\n",
      "generally used to represent sets, random variables, and matrices. Realizations\n",
      "of random variables and placeholders in functions associated with random\n",
      "variables are usually represented by lowercase letters corresponding to the\n",
      "uppercase letters; thus, ϵ may represent a realization of the random variable\n",
      "E.\n",
      "Parameters in models (that is, unobservables in the models) are generally\n",
      "represented by Greek letters. Uppercase Latin and Greek letters are also used\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "858\n",
      "Appendix C. Notation and Deﬁnitions\n",
      "to represent cumulative distribution functions. Symbols whose meaning is\n",
      "context-independent are usually written in an upright font, whereas symbols\n",
      "representing variables are written in a slant or italic font; for example, Γ is\n",
      "used to represent the gamma function, while Γ may be used to represent a\n",
      "variable or a parameter. An upright font is also used to represent a special\n",
      "object, such as a sample space or a parameter space.\n",
      "The Greek Alphabet\n",
      "alpha\n",
      "A\n",
      "α\n",
      "nu\n",
      "N\n",
      "ν\n",
      "beta\n",
      "B\n",
      "β\n",
      "xi\n",
      "Ξ\n",
      "ξ\n",
      "gamma\n",
      "Γ\n",
      "γ\n",
      "omicron\n",
      "O\n",
      "o\n",
      "delta\n",
      "∆\n",
      "δ\n",
      "pi\n",
      "Π\n",
      "π, ϖ\n",
      "epsilon\n",
      "E\n",
      "ϵ, ε\n",
      "rho\n",
      "P\n",
      "ρ, ϱ\n",
      "zeta\n",
      "Z\n",
      "ζ\n",
      "sigma\n",
      "Σ\n",
      "σ, ς\n",
      "eta\n",
      "H\n",
      "η\n",
      "tau\n",
      "T\n",
      "τ\n",
      "theta\n",
      "Θ\n",
      "θ, ϑ\n",
      "upsilon\n",
      "Υ\n",
      "υ\n",
      "iota\n",
      "I\n",
      "ι\n",
      "phi\n",
      "Φ\n",
      "φ, ϕ\n",
      "kappa\n",
      "K\n",
      "κ, κ\n",
      "chi\n",
      "X\n",
      "χ\n",
      "lambda\n",
      "Λ\n",
      "λ\n",
      "psi\n",
      "Ψ\n",
      "ψ\n",
      "mu\n",
      "M\n",
      "µ\n",
      "omega\n",
      "Ω\n",
      "ω\n",
      "Symbols for Structures or Elements within a Structure\n",
      "Lowercase Latin and Greek letters are used to represent ordinary scalar or\n",
      "vector variables and functions. No distinction in the notation is made\n",
      "between scalars and vectors; thus, β may represent a vector and βi may\n",
      "represent the ith element of the vector β. In another context, however, β may\n",
      "represent a scalar. All vectors are considered to be column vectors, although\n",
      "we may write a vector as x = (x1, x2, . . ., xn). Transposition of a vector or a\n",
      "matrix is denoted by the superscript “T”.\n",
      "Uppercase calligraphic Latin letters, such as D, V, and W, are generally\n",
      "used to represent special collections of sets, vector spaces, or transforms (func-\n",
      "tionals).\n",
      "A single symbol in an italic font is used to represent a single variable. A\n",
      "Roman font or a special font is often used to represent a standard operator\n",
      "or a standard mathematical structure. Sometimes a string of symbols in a\n",
      "Roman font is used to represent an operator (or a standard function); for\n",
      "example, exp(·) represents the exponential function. But a string of symbols\n",
      "in an italic font on the same baseline should be interpreted as representing\n",
      "a composition (probably by multiplication) of separate objects; for example,\n",
      "exp represents the product of e, x, and p. Likewise a string of symbols in\n",
      "a Roman font (usually a single symbol) is used to represent a fundamental\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "C.1 General Notation\n",
      "859\n",
      "constant; for example, e represents the base of the natural logarithm, while e\n",
      "represents a variable.\n",
      "Subscripts generally represent indexes to a larger structure; for example,\n",
      "xij may represent the (i, j)th element of a matrix, X. A subscript in paren-\n",
      "theses represents an order statistic. A superscript in parentheses represents\n",
      "an iteration; for example, x(k)\n",
      "i\n",
      "may represent the value of xi at the kth step\n",
      "of an iterative process.\n",
      "xi\n",
      "The ith element of a structure (including a sample,\n",
      "which, if the labels are ignored, is a multiset).\n",
      "x(i) or x(i:n)\n",
      "The ith order statistic in a sample of size n.\n",
      "x(i)\n",
      "The value of x at the ith iteration.\n",
      "Symbols for Fixed Mathematical Structures\n",
      "Some important mathematical structures and other objects are:\n",
      "IR\n",
      "The ﬁeld of reals or the set over which that ﬁeld is de-\n",
      "ﬁned.\n",
      "IR+\n",
      "The set of positive reals.\n",
      "IR\n",
      "The “extended reals”; IR = IR ∪{−∞, ∞}.\n",
      "¯IR+\n",
      "The nonnegative reals; ¯IR+ = IR+ ∪{0}.\n",
      "IR+\n",
      "The extended nonnegative reals; IR+ = IR+ ∪{0, ∞}.\n",
      "IRd\n",
      "The usual d-dimensional vector space over the reals or\n",
      "the set of all d-tuples with elements in IR.\n",
      "IRn×m\n",
      "The vector space of real n × m matrices.\n",
      "ZZ\n",
      "The ring of integers or the set over which that ring is\n",
      "deﬁned.\n",
      "ZZ+\n",
      "The set of positive integers.\n",
      "IC\n",
      "The ﬁeld of complex numbers or the set over which that\n",
      "ﬁeld is deﬁned. The notation ICd and ICn×m have mean-\n",
      "ings analogous to the corresponding real spaces.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "860\n",
      "Appendix C. Notation and Deﬁnitions\n",
      "e\n",
      "The base of the natural logarithm. This is a constant;\n",
      "the symbol “e” may be used to represent a variable.\n",
      "(Note the diﬀerence in the font.)\n",
      "i\n",
      "The imaginary unit, √−1. This is a constant; the sym-\n",
      "bol “i” may be used to represent a variable. (Note the\n",
      "diﬀerence in the font.)\n",
      "C.2 General Mathematical Functions and Operators\n",
      "Functions such as sin, max, span, and so on that are commonly associated\n",
      "with strings of Latin letters are generally represented by those letters in a\n",
      "Roman font.\n",
      "Operators such as d (the diﬀerential operator) that are commonly associ-\n",
      "ated with a Latin letter are generally represented by that letter in a Roman\n",
      "font.\n",
      "Note that some symbols, such as | · |, are overloaded.\n",
      "|x|\n",
      "The modulus of the real or complex number x; if x is\n",
      "real, |x| is the absolute value of x.\n",
      "⌈x⌉\n",
      "The ceiling function evaluated at the real number x: ⌈x⌉\n",
      "is the smallest integer greater than or equal to x.\n",
      "For any x, ⌊x⌋≤x ≤⌈x⌉.\n",
      "⌊x⌋\n",
      "The ﬂoor function evaluated at the real number x: ⌊x⌋\n",
      "is the largest integer less than or equal to x.\n",
      "x!\n",
      "The factorial of x. If x = 0\n",
      "x! = 0! = 1;\n",
      "if x is a positive integer,\n",
      "x! = x(x −1) · · ·2 · 1;\n",
      "otherwise, for all x except nonpositive integers,\n",
      "x! = Γ(x + 1).\n",
      "If x = −1, −2, . . ., x! is undeﬁned.\n",
      "00\n",
      "For convenience, we deﬁne 00 as limx→0 x0 = 1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "C.2 General Mathematical Functions and Operators\n",
      "861\n",
      "x[r]\n",
      "The rth factorial of x. If x is a positive integer,\n",
      "x[r] = x(x −1) · · ·(x −(r −1)).\n",
      "Cn\n",
      "k\n",
      "or\n",
      "\u0000n\n",
      "k\n",
      "\u0001\n",
      "The binomial coeﬃcient, n!/(k!(n−k)!). If n is a positive\n",
      "integer, and k is a nonnegative integer no greater than\n",
      "n, then this is the number of ways k items can be chosen\n",
      "from a set of n items.\n",
      "Π(A)\n",
      "or Π(n)\n",
      "For the set A with ﬁnite cardinality n, Π(A) is an n-\n",
      "tuple consisting of the elements of A, each occurring\n",
      "once. For the positive integer n, Π(n) an n-tuple con-\n",
      "sisting of the elements 1, . . ., n. There are n! possible\n",
      "values of either Π(A) or Π(n).\n",
      "d\n",
      "The diﬀerential operator.\n",
      "∆\n",
      "A perturbation operator; ∆x represents a perturbation\n",
      "of x and not a multiplication of x by ∆, even if x is a\n",
      "type of object for which a multiplication is deﬁned.\n",
      "∆(·, ·)\n",
      "A real-valued diﬀerence function; ∆(x, y) is a mea-\n",
      "sure of the diﬀerence of x and y. For simple objects,\n",
      "∆(x, y) = |x −y|. For more complicated objects, a sub-\n",
      "traction operator may not be deﬁned, and ∆is a gener-\n",
      "alized diﬀerence.\n",
      "˜x\n",
      "A perturbation of the object x;\n",
      "∆(x, ˜x) = ∆x.\n",
      "˜x\n",
      "An average of a sample of objects generically denoted\n",
      "by x.\n",
      "¯x\n",
      "The mean of a sample of objects generically denoted by\n",
      "x.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "862\n",
      "Appendix C. Notation and Deﬁnitions\n",
      "O(f(n))\n",
      "The order class big O with respect to f(n).\n",
      "g(n) ∈O(f(n))\n",
      "means there exists some ﬁxed c such that ∥g(n)∥≤\n",
      "c∥f(n)∥∀n. In particular, g(n) ∈O(1) means g(n) is\n",
      "bounded.\n",
      "In one special case, we will use O(f(n)) to represent\n",
      "some unspeciﬁed scalar or vector x ∈O(f(n)). This is\n",
      "the case of a convergent series. An example is\n",
      "s = f1(n) + · · · + fk(n) + O(f(n)),\n",
      "where f1(n), . . ., fk(n) are ﬁnite constants.\n",
      "We may also express the order class deﬁned by conver-\n",
      "gence as x →a as O(f(x))x→a (where a may be inﬁnite).\n",
      "Hence, g ∈O(f(x))x→a iﬀ\n",
      "lim sup\n",
      "x→a\n",
      "∥g(n)∥/∥f(n)∥< ∞.\n",
      "o(f(n))\n",
      "Little o; g(n) ∈o(f(n)) means for all c > 0 there exists\n",
      "some ﬁxed N such that 0 ≤g(n) < cf(n) ∀n ≥N.\n",
      "(The functions f and g and the constant c could all also\n",
      "be negative, with a reversal of the inequalities.) Hence,\n",
      "g(n) = o(f(n)) means ∥g(n)∥/∥f(n)∥→0 as n →∞.\n",
      "In particular, g(n) ∈o(1) means g(n) →0.\n",
      "We also use o(f(n)) to represent some unspeciﬁed scalar\n",
      "or vector x ∈o(f(n)) in special case of a convergent\n",
      "series, as above:\n",
      "s = f1(n) + · · · + fk(n) + o(f(n)).\n",
      "We may also express this kind of convergence in the form\n",
      "g ∈o(f(x))x→a as x →a (where a may be inﬁnite).\n",
      "Spaces of Functions\n",
      "Ck\n",
      "For an integer k ≥0, the class of functions whose deriva-\n",
      "tives up to the kth derivative exist and are continuous.\n",
      "Lp\n",
      "For a real number p ≥1, the class of functions f on\n",
      "a measure space (Ω, F, ν) with a metric ∥· ∥such that\n",
      "R ∥f∥pdν < ∞.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "C.2 General Mathematical Functions and Operators\n",
      "863\n",
      "Functions of Convenience\n",
      "IS(·)\n",
      "The indicator function:\n",
      "IS(x) = 1, if x ∈S,\n",
      "= 0, otherwise.\n",
      "(C.1)\n",
      "If x is a scalar, the set S is often taken as the interval\n",
      "] −∞, y[, and in this case, the indicator function is the\n",
      "Heaviside function, H, evaluated at the diﬀerence of the\n",
      "argument and the upper bound on the interval:\n",
      "I]−∞,y[(x) = H(y −x).\n",
      "(An alternative deﬁnition of the Heaviside function is\n",
      "the same as this except that H(0) = 1\n",
      "2.) It is interesting\n",
      "to note that\n",
      "I]−∞,y[(x) = I]x,∞[(y).\n",
      "In higher dimensions, the set S is often taken as the\n",
      "product set,\n",
      "Ad = ] −∞, y1[×] −∞, y2[× · · ·×] −∞, yd[\n",
      "= A1 × A2 × · · · × Ad,\n",
      "and in this case,\n",
      "IAd(x) = IA1(x1)IA2(x2) · · ·IAd(xd),\n",
      "where x = (x1, x2, . . ., xd).\n",
      "The derivative of the indicator function is the Dirac\n",
      "delta function, δ(·).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "864\n",
      "Appendix C. Notation and Deﬁnitions\n",
      "δ(·)\n",
      "The Dirac delta “function”, deﬁned by\n",
      "δ(x) = 0,\n",
      "for x ̸= 0,\n",
      "and\n",
      "Z ∞\n",
      "−∞\n",
      "δ(t) dt = 1.\n",
      "The Dirac delta function is not a function in the usual\n",
      "sense. We do, however, refer to it as a function, and\n",
      "treat it in many ways as a function. For any continuous\n",
      "function f, we have the useful fact\n",
      "Z ∞\n",
      "−∞\n",
      "f(y) dI]−∞,y[(x) =\n",
      "Z ∞\n",
      "−∞\n",
      "f(y) δ(y −x) dy\n",
      "= f(x).\n",
      "Special Functions\n",
      "Various common mathematical functions are referred to collectively as “spe-\n",
      "cial functions”. These include the trigonometric functions, both circular and\n",
      "hyperbolic, the various orthogonal polynomial systems, and solutions to spe-\n",
      "cial diﬀerential equations, such as Bessel functions.\n",
      "I list only a few below. The functions are often written without paren-\n",
      "theses enclosing the arguments, for example log x, but I usually enclose the\n",
      "arguments in parentheses.\n",
      "Good general references on special functions in mathematics are Olver et al.\n",
      "(2010) and Thompson (1997).\n",
      "log(x)\n",
      "The natural logarithm evaluated at x.\n",
      "sin(x)\n",
      "The sine evaluated at x (in radians) and similarly for\n",
      "other trigonometric functions.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "C.2 General Mathematical Functions and Operators\n",
      "865\n",
      "Γ(α)\n",
      "The complete gamma function:\n",
      "Γ(α) =\n",
      "Z ∞\n",
      "0\n",
      "tα−1e−tdt.\n",
      "(C.2)\n",
      "(This is called Euler’s integral.) Integration by parts im-\n",
      "mediately gives the replication formula\n",
      "Γ(α + 1) = αΓ(α),\n",
      "and so if α is a positive integer, Γ(α + 1) = α!. More\n",
      "generally, Γ(α + 1) can be taken as the deﬁnition of α!.\n",
      "This does not exist for negative integers, but does for\n",
      "all other real α.\n",
      "Direct evaluation of the integral yields Γ(1/2) = √π.\n",
      "Using this and the replication formula, with some ma-\n",
      "nipulation we get for the positive integer j\n",
      "Γ(j + 1/2) = 1 · 2 · · ·(2j −1)\n",
      "2j\n",
      "√π.\n",
      "The notation Γd(α) denotes the multivariate gamma\n",
      "function, where α is a d-vector. (In other literature this\n",
      "notation denotes the incomplete univariate gamma func-\n",
      "tion, for which I use γ(α, d); see below.)\n",
      "Associated with the gamma function are some other useful functions:\n",
      "ψ(α)\n",
      "The psi function or the digamma function:\n",
      "ψ(α) = d log(Γ(α))/dα.\n",
      "(C.3)\n",
      "ψ′(α)\n",
      "The trigamma function,\n",
      "ψ′(α) = dψ(α)/dα.\n",
      "(C.4)\n",
      "More general are the polygamma functions, for n =\n",
      "1, 2, . . ., ψ(n)(α) = d(n)ψ(α)/(dα)(n), which for a ﬁxed\n",
      "n, is called the (n + 2)-gamma function.\n",
      "γ(α, x)\n",
      "The incomplete gamma function,\n",
      "γ(α, x) =\n",
      "Z x\n",
      "0\n",
      "tα−1e−tdt.\n",
      "(C.5)\n",
      "This is also often denoted as Γx(α).\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "866\n",
      "Appendix C. Notation and Deﬁnitions\n",
      "P (α, x)\n",
      "The regularized incomplete gamma function, which is\n",
      "the CDF of the standard gamma distribution,\n",
      "P (α, x) = γ(α, x)\n",
      "Γ(α) .\n",
      "(C.6)\n",
      "B(α, β)\n",
      "The beta function,\n",
      "B(α, β) = Γ(α)Γ(β)\n",
      "Γ(α + β)\n",
      "(C.7)\n",
      "=\n",
      "Z 1\n",
      "0\n",
      "tα−1(1 −t)β−1dt\n",
      "(C.8)\n",
      "=\n",
      "Z ∞\n",
      "0\n",
      "tα−1\n",
      "(1 + t)α+β .\n",
      "(C.9)\n",
      "The integral in equation (C.8) is called Euler’s beta in-\n",
      "tegral.\n",
      "Ix(α, β)\n",
      "The regularized incomplete beta function, which is the\n",
      "CDF of the beta distribution,\n",
      "Ix(α, β) =\n",
      "1\n",
      "B(α, β)\n",
      "Z x\n",
      "0\n",
      "tα−1(1 −t)β−1dt.\n",
      "(C.10)\n",
      "C.3 Sets, Measure, and Probability\n",
      "The notation listed below does not always represent the things associated\n",
      "with it here, but for these objects, I generally use either this notation or other\n",
      "symbols in the same font.\n",
      "Ω\n",
      "Sample space; the universal set in a given space or prob-\n",
      "ability distribution.\n",
      "#A\n",
      "The cardinality of the set A.\n",
      "Ac\n",
      "The complement of the set A; Ac = Ω−A.\n",
      "A1 ∪A2\n",
      "The union of the sets A1 and A2; x ∈A1 ∪A2 iﬀx ∈A1\n",
      "or x ∈A2.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "C.3 Sets, Measure, and Probability\n",
      "867\n",
      "A1 ∩A2\n",
      "The intersection of the sets A1 and A2; x ∈A1 ∩A2 iﬀ\n",
      "x ∈A1 and x ∈A2.\n",
      "A1 −A2\n",
      "The set A1 minus the set A2; x ∈A1 −A2 iﬀx ∈A1\n",
      "and x /∈A2.\n",
      "A1∆A2\n",
      "The symmetric diﬀerence of the sets A1 and A2;\n",
      "A1∆A2 = (A1 −A2) ∪(A2 −A1).\n",
      "A1 × A2\n",
      "Cartesian (or cross) product of the sets A1 and A2;\n",
      "(a1, a2) ∈A1 × A2 iﬀa1 ∈A1 and a2 ∈A2.\n",
      "The following objects require a notion of “open sets”, either as the collection\n",
      "of sets that deﬁne a topology (Section 0.0.2) or as deﬁned in a metric space\n",
      "(Sections 0.0.2 and 0.0.5).\n",
      "A◦\n",
      "The set of interior points of the set A.\n",
      "A\n",
      "The set of closure points of the set A.\n",
      "∂A\n",
      "The set of boundary points of the set A: ∂A = A −A◦.\n",
      "F\n",
      "A σ-ﬁeld.\n",
      "B(Ω)\n",
      "The Borel σ-ﬁeld generated by a collection of open sets\n",
      "deﬁning a topology in Ω. This requires deﬁnition of a col-\n",
      "lection, so we may also use the notation B(Ω, T ), where\n",
      "T is a collection of sets deﬁning a topology.\n",
      "B\n",
      "The Borel σ-ﬁeld B(IR).\n",
      "Bd\n",
      "The Borel σ-ﬁeld B(IRd).\n",
      "BI\n",
      "The Borel σ-ﬁeld restricted to the interval I; that is,\n",
      "the σ-ﬁeld generated by all open intervals contained in\n",
      "I and Ω= I.\n",
      "(Ω, F)\n",
      "A measurable space: the sample space Ωand the σ-ﬁeld\n",
      "F.\n",
      "(Ω, F, ν)\n",
      "A measure space: the sample space Ω, the σ-ﬁeld F, and\n",
      "the measure ν deﬁned over the sets in F.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "868\n",
      "Appendix C. Notation and Deﬁnitions\n",
      "λ ≪ν\n",
      "The measure ν dominates the measure λ; that is, λ is\n",
      "absolutely continuous with respect to ν:\n",
      "ν(A) = 0\n",
      "⇒\n",
      "λ(A) = 0,\n",
      "for any set A in the domain of both λ and ν.\n",
      "λ ⊥ν\n",
      "The measures ν and λ on a common measurable space\n",
      "(Ω, F) are singular with respect to each other; that is,\n",
      "there exists two disjoint sets A and B in F such that\n",
      "A∪B = Ωand for any measurable set A1 ⊆A, ν(A1) =\n",
      "0, while for any measurable set B1 ⊆B, µ(B1) = 0.\n",
      "(Ω, F, P )\n",
      "The “probability triple”: the sample space Ω, the σ-ﬁeld\n",
      "F, and the probability measure P .\n",
      "P\n",
      "A family of probability distributions.\n",
      "Θ\n",
      "Parameter space.\n",
      "X\n",
      "The range of a random variable.\n",
      "OP(f(n))\n",
      "Bounded convergence in probability; X(n) ∈OP (f(n))\n",
      "means that for any positive ϵ, there is a constant Cϵ\n",
      "such that supn Pr(∥X(n)∥≥Cϵ∥f(n)∥) < ϵ.\n",
      "oP(f(n))\n",
      "Convergent in probability; X(n) ∈oP (f(n)) means that\n",
      "for any positive ϵ, Pr(∥X(n)−f(n)∥> ϵ) →0 as n →∞.\n",
      "C.4 Linear Spaces and Matrices\n",
      "V(G)\n",
      "For the set of vectors (all of the same order) G, the\n",
      "vector space generated by that set.\n",
      "V(X)\n",
      "For the matrix X, the vector space generated by the\n",
      "columns of X.\n",
      "dim(V)\n",
      "The dimension of the vector space V; that is, the maxi-\n",
      "mum number of linearly independent vectors in the vec-\n",
      "tor space.\n",
      "span(Y )\n",
      "For Y either a set of vectors or a matrix, the vector\n",
      "space V(Y )\n",
      ".\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "C.4 Linear Spaces and Matrices\n",
      "869\n",
      "tr(A)\n",
      "The trace of the square matrix A, that is, the sum of\n",
      "the diagonal elements.\n",
      "rank(A)\n",
      "The rank of the matrix A, that is, the maximum number\n",
      "of independent rows (or columns) of A.\n",
      "ρ(A)\n",
      "The spectral radius of the matrix A (the maximum ab-\n",
      "solute value of its eigenvalues).\n",
      "A > 0\n",
      "A ≥0\n",
      "If A is a matrix, this notation means, respectively, that\n",
      "each element of A is positive or nonnegative. These may\n",
      "also be written as 0 < A or 0 ≤A.\n",
      "A ≻0\n",
      "A ⪰0\n",
      "This notation means that A is a symmetric matrix and\n",
      "that it is, respectively, positive deﬁnite or nonnegative\n",
      "deﬁnite. These may also be written as 0 ≺A or 0 ⪯A.\n",
      "A ≻B\n",
      "A ⪰B\n",
      "This notation means that A and B are symmetric ma-\n",
      "trices and that A −B is, respectively, positive deﬁnite\n",
      "or nonnegative deﬁnite. These may also be written as\n",
      "B ≺A or B ⪯A.\n",
      "AT\n",
      "For the matrix A, its transpose (also used for a vector\n",
      "to represent the corresponding row vector).\n",
      "AH\n",
      "The conjugate transpose, also called the adjoint, of the\n",
      "matrix A;\n",
      "AH = ¯AT = AT.\n",
      "A−1\n",
      "The inverse of the square, nonsingular matrix A.\n",
      "A−T\n",
      "The inverse of the transpose of the square, nonsingular\n",
      "matrix A.\n",
      "A+\n",
      "The m × n g4 inverse, the Moore-Penrose inverse, or\n",
      "the pseudoinverse of the n × m matrix A; that is, A+ a\n",
      "matrix such that AA+A = A; A+AA+ = A+; A+A is\n",
      "symmetric; and AA+ is symmetric.\n",
      "A−\n",
      "An m×n g1 inverse, or generalized inverse of the matrix\n",
      "n × m A; that is, A−a matrix such that AA−A = A.\n",
      "A\n",
      "1\n",
      "2\n",
      "The square root of a nonnegative deﬁnite or positive\n",
      "deﬁnite matrix A; (A\n",
      "1\n",
      "2 )2 = A.\n",
      "A−1\n",
      "2\n",
      "The square root of the inverse of a positive deﬁnite ma-\n",
      "trix A; (A−1\n",
      "2 )2 = A−1.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "870\n",
      "Appendix C. Notation and Deﬁnitions\n",
      "Norms and Inner Products\n",
      "Lp\n",
      "For real p ≥1, a norm formed by accumulating the pth\n",
      "powers of the moduli of individual elements in an object\n",
      "and then taking the (1/p)th power of the result.\n",
      "∥· ∥\n",
      "In general, the norm of the object ·.\n",
      "∥· ∥p\n",
      "In general, the Lp norm of the object ·.\n",
      "∥x∥p\n",
      "For the vector x, the Lp norm\n",
      "∥x∥p =\n",
      "\u0010X\n",
      "|xi|p\u0011 1\n",
      "p .\n",
      "∥X∥p\n",
      "For the matrix X, the Lp norm\n",
      "∥X∥p = max\n",
      "∥v∥p=1 ∥Xv∥p.\n",
      "∥X∥F\n",
      "For the matrix X, the Frobenius norm\n",
      "∥X∥F =\n",
      "sX\n",
      "i,j\n",
      "x2\n",
      "ij.\n",
      "⟨x, y⟩\n",
      "The inner product or dot product of x and y.\n",
      "κp(A)\n",
      "The Lp condition number of the nonsingular square ma-\n",
      "trix A with respect to inversion.\n",
      "Notation Relating to Matrix Determinants\n",
      "|A|\n",
      "The determinant of the square matrix A,\n",
      "|A| = det(A).\n",
      "det(A)\n",
      "The determinant of the square matrix A,\n",
      "det(A) = |A|.\n",
      "|A(i1,...,ik)|\n",
      "A principal minor of a square matrix A; in this case, it is\n",
      "the minor corresponding to the matrix formed from rows\n",
      "i1, . . ., ik and columns i1, . . ., ik from a given matrix A.\n",
      "|A−(i)(j)|\n",
      "The minor associated with the (i, j)th element of a\n",
      "square matrix A.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "C.4 Linear Spaces and Matrices\n",
      "871\n",
      "a(ij)\n",
      "The cofactor associated with the (i, j)th element of a\n",
      "square matrix A; that is, a(ij) = (−1)i+j|A−(i)(j)|.\n",
      "adj(A)\n",
      "The adjugate, also called the classical adjoint, of the\n",
      "square matrix A: adj(A) = (a(ji)); that is, the matrix\n",
      "of the same size as A formed from the cofactors of the\n",
      "elements of AT.\n",
      "Matrix-Vector Diﬀerentiation\n",
      "dt\n",
      "The diﬀerential operator on the scalar, vector, or matrix\n",
      "t. This is an operator; d may be used to represent a\n",
      "variable. (Note the diﬀerence in the font.)\n",
      "gf\n",
      "or ∇f\n",
      "For the scalar-valued function f of a vector variable, the\n",
      "vector whose ith element is ∂f/∂xi. This is the gradient,\n",
      "also often denoted as gf.\n",
      "∇f\n",
      "For the vector-valued function f of a vector variable, the\n",
      "matrix whose element in position (i, j) is\n",
      "∂fj(x)\n",
      "∂xi\n",
      ".\n",
      "This is also written as ∂fT/∂x or just as ∂f/∂x. This\n",
      "is the transpose of the Jacobian of f.\n",
      "Jf\n",
      "For the vector-valued function f of a vector variable, the\n",
      "Jacobian of f denoted as Jf. The element in position\n",
      "(i, j) is\n",
      "∂fi(x)\n",
      "∂xj\n",
      ".\n",
      "This is the transpose of (∇f): Jf = (∇f)T.\n",
      "Hf\n",
      "or ∇∇f\n",
      "The Hessian of the scalar-valued function f of a vector\n",
      "variable. The Hessian is the transpose of the Jacobian\n",
      "of the gradient. Except in pathological cases, it is sym-\n",
      "metric. The element in position (i, j) is\n",
      "∂2f(x)\n",
      "∂xi∂xj\n",
      ".\n",
      "The symbol ∇2f is also sometimes used to denote the\n",
      "Hessian of f, but I prefer not to use that notation.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "872\n",
      "Appendix C. Notation and Deﬁnitions\n",
      "∇2f\n",
      "For the vector-valued function f of a vector variable, the\n",
      "trace of the Hessian. This is also called the Laplacian.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "References\n",
      "The number(s) following a reference indicate the page(s) on which the refer-\n",
      "ence is cited. A few of these references are general ones that are not cited in\n",
      "the text.\n",
      "Martin Aigner and G¨unter M. Ziegler. Proofs from THE BOOK. Springer-\n",
      "Verlag, Berlin, fourth edition, 2010. 674, 688\n",
      "S. M. Ali and S. D. Silvey. A general class of coeﬃcients of divergence of one\n",
      "distribution from another. Journal of the Royal Statistical Society, Series\n",
      "B, 28:131–142, 1966. 747\n",
      "Robert B. Ash and Catherine A. Doleans-Dade. Probability & Measure The-\n",
      "ory. Academic Press, New York, second edition, 1999. 145\n",
      "K. B. Athreya and P. E. Ney. Branching Processes. Springer, Berlin, 1972.\n",
      "(Reprinted by Dover Publications, New York, 2004.). 144, 201\n",
      "Krishna B. Athreya and Soumen N. Lahiri. Measure Theory and Probability\n",
      "Theory. Springer, New York, 2006. 145\n",
      "George Bachman and Lawrence Narici. Functional Analysis. Dover Publi-\n",
      "cations, Inc., Mineola, New York, 2000. (reprint with list of errata of the\n",
      "book published by Academic Press, New York, 1966). 638\n",
      "R. R. Bahadur. A note on quantiles in large samples. Annals of Mathematical\n",
      "Statistics, 37:577–580, 1966. 97\n",
      "N. Balakrishnan and Chin-Diew Lai.\n",
      "Continuous Bivariate Distributions.\n",
      "Springer, New York, second edition, 2009. 141\n",
      "N. Balakrishnan and V. B. Nevzorov. A Primer on Statistical Distributions.\n",
      "Wiley–Interscience, New York, 2003. 837\n",
      "O. E. Barndorﬀ-Nielson. Information and Exponential Families in Statistical\n",
      "Theory. Wiley, Chichester, 1978. 200\n",
      "O. E. Barndorﬀ-Nielson and D. R. Cox. Inference and Asymptotics. Chapman\n",
      "and Hall, New York, 1994. 145, 321\n",
      "Richard F. Bass. Stochastic Processes. Cambridge University Press, Cam-\n",
      "bridge, United Kingdom, 2011. 780\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "874\n",
      "References\n",
      "Martin Baxter and Andrew Rennie. Financial Calculus: An Introduction to\n",
      "Derivative Pricing. Cambridge University Press, Cambridge, United King-\n",
      "dom, 1996. 144\n",
      "Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: A\n",
      "practical and powerful approach to multiple testing. Journal of the Royal\n",
      "Statistical Society, Series B, 57:289–300, 1995. 538\n",
      "James 0. Berger. Could Fisher, Jeﬀreys and Neyman have agreed on testing?\n",
      "Statistical Science, 18:1–31, 2003. 558\n",
      "James O. Berger.\n",
      "Statistical Decision Theory and Bayesian Analysis.\n",
      "Springer, New York, second edition, 1985. 328, 381\n",
      "James O. Berger and Thomas Sellke. Testing a point null hypothesis: The\n",
      "irreconcilability of p values and evidence (with discussion). Journal of the\n",
      "American Statistical Association, 82:112–139, 1987. 381\n",
      "James O. Berger and Robert L. Wolpert.\n",
      "The Likelihood Principle.\n",
      "The\n",
      "Institute of Mathematical Statistics, Hayward, California, second edition,\n",
      "1988. 318\n",
      "R. N. Bhattacharya and R. Ranga Rao. Normal Approximations and Asymp-\n",
      "totic Expansions. John Wiley & Sons, New York, 1976. 143\n",
      "P. J. Bickel and E. L. Lehmann.\n",
      "Unbiased estimation in convex families.\n",
      "Annals of Mathematical Statistics, 40:1523–1535, 1969. 51, 404\n",
      "Peter J. Bickel and David A. Freedman.\n",
      "Some asymptotic theory for the\n",
      "bootstrap. Annals of Statistics, 9:1196–1217, 1981. 600\n",
      "Christophe Biernacki. Testing for a global maximum of the likelihood. Journal\n",
      "of Computational and Graphical Statistics, 14:657–674, 2005. 502\n",
      "Patrick Billingsley. Probability and Measure. John Wiley & Sons, New York,\n",
      "third edition, 1995. 48, 86, 88, 104, 119, 145, 691, 710, 713, 735, 738, 739\n",
      "Allan Birnbaum.\n",
      "On the foundations of statistical inference (with discus-\n",
      "sion by L. J. Savage, George Barnard, Jerome Cornﬁeld, Irwin Bross,\n",
      "George E. P. Box, I. J. Good, D. V. Lindley, C. W. Clunies-Ross, John\n",
      "W. Pratt, Howard Levene, Thomas Goldman, A. P. Dempster, and Oscar\n",
      "Kempthorne). Journal of the American Statistical Association, 57:269–326,\n",
      "1962. 318\n",
      "David Blackwell and M. A. Girshick. Theory of Games and Statistical Deci-\n",
      "sions. John Wiley & Sons, New York, 1954. (Reprinted by Dover Publica-\n",
      "tions, New York, 1979.). 319\n",
      "Colin R. Blyth. Some probability paradoxes in choice from among random\n",
      "alternatives. Journal of the American Statistical Association, 67:366–373,\n",
      "1972. 220\n",
      "Ralph Boas Jr. A Primer on Real Functions. The Mathematical Association\n",
      "of America, Washington, DC, 1960. 723, 762\n",
      "Leo Breiman. Statistical modeling: The two cultures (with discussion). Sta-\n",
      "tistical Science, 16:199–231, 2001. 322\n",
      "Leo Breiman. Probability. Addison-Wesley, New York, 1968. (Reprinted by\n",
      "the Society for Industrial and Applied Mathematics, Philadelphia, 1992.).\n",
      "145, 200\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "References\n",
      "875\n",
      "Lyle D. Broemeling.\n",
      "Bayesian Analysis of Linear Models.\n",
      "Chapman &\n",
      "Hall/CRC, Boca Raton, 1984. 382\n",
      "James Ward Brown and Ruel V. Churchill. Complex Variables and Applica-\n",
      "tions. McGraw-Hill Book Company, New York, eighth edition, 2008. 762\n",
      "Lawrence D. Brown. Fundamentals of Statistical Exponential Families with\n",
      "Applications in Statistical Decision Theory.\n",
      "Institute of Mathematical\n",
      "Statistics, Hayward, California, 1986. 173, 200\n",
      "H. D. Brunk.\n",
      "On an extension of the concept of conditional expectation.\n",
      "Proceedings of the American Mathematical Society, 14:298–304, 1963. 141\n",
      "H. D. Brunk.\n",
      "Conditional expectation given a σ-lattice and applications.\n",
      "Annals of Mathematical Statistics, 36:1339–1350, 1965. 141\n",
      "A. Buse. The likelihood ratio, Wald, and Lagrange multiplier tests: An ex-\n",
      "pository note. The American Statistician, 36:153–157, 1982. 558\n",
      "George Casella and Roger L. Berger. Reconciling Bayesian and frequentist\n",
      "evidence in the one-sided testing problem, (with discussion). Journal of the\n",
      "American Statistical Association, 82:106–111 and 123–139, 1987. 381\n",
      "Kai Lai Chung. A Course in Probability Theory. Academic Press, New York,\n",
      "second revised edition, 2000. 48, 145\n",
      "Daren B. H. Cline and Sidney I. Resnick. Multivariate subexponential distri-\n",
      "butions. Stochastic Processes and their Applications, 42:49–72, 1992. 200\n",
      "Noel Cressie and Timothy R. C. Read.\n",
      "Multinomial goodness-of-ﬁt tests.\n",
      "Journal of the Royal Statistical Society, Series B, 46:440–464, 1984. 253\n",
      "I. Csisz´ar. Information-type measures of diﬀerence of probability distributions\n",
      "and indirect observations. Studia Scientiarum Mathematicarum Hungarica,\n",
      "2:299–318, 1967. 747\n",
      "Anirban DasGupta.\n",
      "Best constants in chebyshev inequalities with various\n",
      "applications. Metrika, 51:185–200, 2000. 848, 856\n",
      "Anirban DasGupta. Asymptotic Theory of Statistics and Probability. Springer,\n",
      "New York, 2008. 321, 856\n",
      "H. A. David. The Method of Paired Comparisons. Griﬃth/Oxford University\n",
      "Press, London, second edition, 1988. 319\n",
      "H. T. David and Shawki A. Salem. Three shrinkage constructions for Pitman-\n",
      "closeness in the one-dimensional location case. Communications in Statis-\n",
      "tics — Theory and Methods, 20:3605–3627, 1991. 319\n",
      "Herbert A. David and H. N. Nagaraja. Order Statistics. John Wiley & Sons,\n",
      "New York, third edition, 2003. 97, 109, 143, 149\n",
      "Laurie Davies and Ursula Gather. Robust statistics.\n",
      "In James E. Gentle,\n",
      "Wolfgang H¨ardle, and Yuichi Mori, editors, Handbook of Computational\n",
      "Statistics; Concepts and Methods, pages 711–750, Berlin, 2012. Springer.\n",
      "610\n",
      "Bruno de Finetti. Probability and exchangeability from a subjective point of\n",
      "view. International Statistical Review, 47:139–135, 1979. 138\n",
      "Laurens de Haan and Ana Ferreira. Extreme Value Theory. An Introduction.\n",
      "Springer, New York, 2006. 109, 143\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "876\n",
      "References\n",
      "Morris. H. DeGroot. Optimal Statistical Decisions. John Wiley & Sons, New\n",
      "York, 1970. (A paperback edition with some additional material was pub-\n",
      "lished in the Wiley Classics Series in 2004.). 140\n",
      "A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood estima-\n",
      "tion from incomplete data via the EM algorithm (with discussion). Journal\n",
      "of the Royal Statistical Society, Series B, 45:51–59, 1977. 470, 479, 502\n",
      "A. P. Dempster, N. M. Laird, and D. B. Rubin.\n",
      "Iteratively reweighted\n",
      "least squares for linear regression when errors are normal/independent dis-\n",
      "tributed. In Paruchuri R. Krishnaiah, editor, Multivariate Analysis V, pages\n",
      "35–57, Amsterdam, 1980. North-Holland. 480\n",
      "Dipak Dey, Sujit K. Ghosh, and Bani K. Mallick, editors. Generalized Linear\n",
      "Models: A Bayesian Perspective. Marcel Dekker, Inc, New York, 2000. 382\n",
      "Sudhakar Dharmadhikari and Kumar Joag-Dev. Unimodality, Convexity, and\n",
      "Applications. Academic Press, New York, 1988. 848\n",
      "Persi Diaconis. Finite forms of de Finetti’s theorem on exchangeability. Syn-\n",
      "these, 36:271–281, 1977. 114\n",
      "J. L. Doob. Stochastic Processes. John Wiley & Sons, New York, 1953. (A\n",
      "paperback edition with some additional material was published in the Wiley\n",
      "Classics Series in 1990.). 144\n",
      "J. L. Doob. Measure Theory. Springer-Verlag, New York, 1994.\n",
      "R. M. Dudley. Real Analysis and Probability. Cambridge University Press,\n",
      "Cambridge, United Kingdom, second edition, 2002. 145\n",
      "R. M. Dudley. Uniform Central Limit Theorems. Cambridge University Press,\n",
      "Cambridge, United Kingdom, 1999. 143, 144\n",
      "J. Durbin. Estimation of parameters in time series regression models. Journal\n",
      "of the Royal Statistical Society, Series B, 22:139–153, 1960. 320, 503\n",
      "A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic minimax character of\n",
      "the sample distribution function and of the classical multinomial estimator.\n",
      "Annals of Mathematical Statistics, 27:642–669, 1956. 135\n",
      "Morris L. Eaton. Group Invariance Applications in Statistics. Institute of\n",
      "Mathematical Statistics, Hayward, California, 1989. 320\n",
      "A. W. F. Edwards. Likelihood. Johns Hopkins University Press, Baltimore,\n",
      "expanded edition, 1992. 502, 558\n",
      "B. Efron. Biased versus unbiased estimation. Advances in Mathematics, 16:\n",
      "259–277, 1975. 220\n",
      "Bradley Efron and Robert J. Tibshirani. An Introduction to the Bootstrap.\n",
      "Chapman & Hall, New York, 1993. 321\n",
      "F. Eiker. Asymptotic normality and consistency of the least squares estimators\n",
      "for families of linear regressions. Annals of Mathematical Statistics, 34:447–\n",
      "456, 1963. 321\n",
      "Merran Evans, Nicholas Hastings, and Brian Peacock. Statistical Distribu-\n",
      "tions. John Wiley & Sons, New York, third edition, 2000. 837\n",
      "Eugene F. Fama and Richard Roll. Parameter estimates for symmetric stable\n",
      "distributions. Journal of the American Statistical Association, 66:331–338,\n",
      "1971. 252\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "References\n",
      "877\n",
      "William Feller. An Introduction to Probability Theory and Its Applications,\n",
      "Volume I. John Wiley & Sons, New York, 1957. 145\n",
      "William Feller. An Introduction to Probability Theory and Its Applications,\n",
      "Volume II. John Wiley & Sons, New York, 1971. 62, 145, 200\n",
      "Thomas S. Ferguson. Mathematical Statistics. A Decision Theoretic Approach.\n",
      "Academic Press, New York, 1967. 319, 361\n",
      "Thomas S. Ferguson. A Bayesian analysis of some nonparametric problems.\n",
      "Annals of Statistics, 1:209–230, 1973. 382\n",
      "R. A. Fisher and L. H. C. Tippett. Limiting forms of the frequency distribution\n",
      "of the largest or smallest member of a sample. Proceedings of the Cambridge\n",
      "Philosophical Society, 24:180–190, 1928. 109, 143\n",
      "Bernard Flury and Alice Zopp`e. Exercises in EM. The American Statistician,\n",
      "54:207–209, 2000. 471\n",
      "David A. Freedman. How can the score test be inconsistent? The American\n",
      "Statistician, 61:291–295, 2007. 558\n",
      "David A. Freedman. Bernard Friedman’s urn. Annals of Mathematical Statis-\n",
      "tics, 36:965–970, 1965. 131\n",
      "Janos Galambos. The Asymptotic Theory of Extreme Order Statistics. John\n",
      "Wiley & Sons, New York, 1978. 143\n",
      "R. C. Geary. The distribution of “Student’s” ratio for non-normal samples.\n",
      "Journal of the Royal Statistical Society, Supplement 3, pages 178–184, 1936.\n",
      "142\n",
      "Seymour Geisser. Predictive Inference: An Introduction. Chapman & Hall,\n",
      "New York, 1993. 321\n",
      "Bernard R. Gelbaum and John M. H. Olmsted. Counterexamples in Analysis.\n",
      "Dover Publications, Inc., Mineola, New York, 2003. (corrected reprint of\n",
      "the second printing published by Holden-Day, Inc., San Francisco, 1965).\n",
      "688, 762\n",
      "Bernard R. Gelbaum and John M. H. Olmsted. Theorems and Counterexam-\n",
      "ples in Mathematics. Springer, New York, 1990. 688, 762\n",
      "Alan E. Gelfand and Adrian F. M. Smith.\n",
      "Sampling-based approaches to\n",
      "calculating marginal densities. Journal of the American Statistical Associa-\n",
      "tion, 85:398–409, 1990. (Reprinted in Samuel Kotz and Norman L. Johnson\n",
      "(Editors) (1997), Breakthroughs in Statistics, Volume III, Springer-Verlag,\n",
      "New York, 526–550.). 378\n",
      "Christopher Genovese and Larry Wasserman. Operating characteristics and\n",
      "extensions of the false discovery rate procedure. Journal of the Royal Sta-\n",
      "tistical Society, Series B, 64:499–517, 2002. 538\n",
      "James E. Gentle. Matrix Algebra: Theory, Computations, and Applications\n",
      "in Statistics. Springer, New York, 2007. 127, 144, 176, 432, 433, 685, 730,\n",
      "781, 801, 804, 818, 821\n",
      "James E. Gentle. Computational Statistics. Springer, New York, 2009. 321,\n",
      "644\n",
      "J. K. Ghosh. A new proof of the Bahadur representation of quantiles and an\n",
      "application. Annals of Mathematical Statistics, 42:1957–1961, 1971. 97\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "878\n",
      "References\n",
      "Malay Ghosh. Objective priors: An introduction for frequentists (with discus-\n",
      "sion). Statistical Science, 26:187–211, 2011. 381\n",
      "Malay Ghosh and Glen Meeden.\n",
      "Bayesian Methods for Finite Population\n",
      "Sampling. Chapman & Hall/CRC, Boca Raton, 1998. 382\n",
      "Malay Ghosh and Pranab Kumar Sen. Bayesian Pitman closeness. Commu-\n",
      "nications in Statistics — Theory and Methods, 20:3423–3437, 1991. 381\n",
      "B. Gnedenko.\n",
      "Sur la distribution limite du terme maximum d’une serie\n",
      "aleatoire. Annals of Mathematics, Second Series, 44:423–453, 1943. 109,\n",
      "143\n",
      "B. V. Gnedenko and A. N. Kolmogorov.\n",
      "Limit Distributions for Sums of\n",
      "Independent Random Variables. Addison-Wesley Publishing Company, Inc.,\n",
      "Reading, Massachusetts, 1954. (Translated from the 1949 Russian edition\n",
      "by K. L. Chung, with an appendix by J. L. Doob.). 1, 142, 143, 145\n",
      "Boris V. Gnedenko. Theory of Probability. Gordon and Breach Science Pub-\n",
      "lishers, Amsterdam, sixth edition, 1997. (Translated from the 1988 Russian\n",
      "edition by Igor A. Ushakov, after the death of Gnedenko in 1995.). 145\n",
      "V. P. Godambe. An optimum property of regular maximum likelihood esti-\n",
      "mation. Annals of Mathematical Statistics, 31:1208–1211, 1960. 320, 503\n",
      "Irving J. Good. Good Thinking. The Foundations of Probability and Its Ap-\n",
      "plications. University of Minnesota Press, Minneapolis, 1983. (Reprinted\n",
      "by Dover Publications, New York, 2009.). 380\n",
      "Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. Concrete Math-\n",
      "ematics. Addison-Wesley, Upper Saddle River, NJ, second revised edition,\n",
      "1994. viii, 689\n",
      "Igor Griva, Stephen G. Nash, and Ariela Sofer. Linear and Nonlinear Op-\n",
      "timization. Society for Industrial and Applied Mathematics, Philadelphia,\n",
      "second edition, 2009. 658\n",
      "Allan Gut. Probability: A Graduate Course. Springer, New York, 2005. 48,\n",
      "145\n",
      "Ian Hacking. The Emergence of Probability: A Philosophical Study of Early\n",
      "Ideas about Probability, Induction and Statistical Inference. Cambridge Uni-\n",
      "versity Press, Cambridge, United Kingdom, 1975. 140\n",
      "P. Hall and C. C. Heyde. Martingale Limit Theory and Its Application. Aca-\n",
      "demic Press, New York, 1980. 144\n",
      "Peter Hall. The Bootstrap and Edgeworth Expansion. Springer, New York,\n",
      "1992. 143\n",
      "Paul R. Halmos. The theory of unbiased estimation. Annals of Mathematical\n",
      "Statistics, 17:34–43, 1946. 320, 442\n",
      "Hugo C. Hamaker. Probability and exchangeability from an objective point\n",
      "of view. International Statistical Review, 45:223–231, 1977. 138\n",
      "Theodore E. Harris. The Theory of Branching Processes. Dover Publications,\n",
      "Inc., Mineola, New York, 1989. (reprint with corrections and additional\n",
      "material of the book published simultaneously by Springer-Verlag, Berlin,\n",
      "and Prentice-Hall, Englewood Cliﬀs, New Jersey, 1963). 144\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "References\n",
      "879\n",
      "H. O. Hartley. In Dr. Bayes’ consulting room. The American Statistician,\n",
      "17(1):22–24, 1963. 381\n",
      "David A. Harville. Quadratic unbiased estimation of variance components for\n",
      "the one-way classiﬁcation. Biometrika, 56:313–326, 1969. 503\n",
      "David A. Harville.\n",
      "Matrix Algebra from a Statistician’s Point of View.\n",
      "Springer, New York, 1997. 821\n",
      "Thomas Hawkins. Lebesgue’s Theory of Integration: Its Origins and Devel-\n",
      "opment.\n",
      "Chelsea Publishing Company, New York, second edition, 1979.\n",
      "(Reprinted by the American Mathematical Society, 2001). 761\n",
      "Leon H. Herbach. Properties of model II — Type analysis of variance tests,\n",
      "A: Optimum nature of the F-test for model II in the balanced case. Annals\n",
      "of Mathematical Statistics, 30:939–959, 1959. 503\n",
      "Thomas P. Hettmansperger and Lawrence A. Klimko. A note on the strong\n",
      "convergence of distributions. Annals of Statistics, 2:597–598, 1974. 83\n",
      "Edwin Hewitt and Karl Stromberg. Real and Abstract Analysis. Springer-\n",
      "Verlag, Berlin, 1965. A corrected second printing was published in 1969.\n",
      "viii, 75, 633, 645, 648, 762\n",
      "C. C. Heyde. On a property of the lognormal distribution. Journal of the\n",
      "Royal Statistical Society, Series B, 29:392–393, 1963. 142\n",
      "Christopher C. Heyde. Quasi-Likelihood and Its Application: A General Ap-\n",
      "proach to Optimal Parameter Estimation. Springer, New York, 1997. 320,\n",
      "503\n",
      "Wassily Hoeﬀding. A class of statistics with asymptotically normal distribu-\n",
      "tion. Annals of Mathematical Statistics, 19:293–325, 1948. 443\n",
      "Robert V. Hogg.\n",
      "Adaptive robust procedures: A partial review and some\n",
      "suggestions for future applications and theory (with discussion). Journal of\n",
      "the American Statistical Association, 69:909–927, 1974. 610\n",
      "Robert V. Hogg and Russell V. Lenth. A review of some adaptive statistical\n",
      "techniques. Communications in Statistics — Theory and Methods, 13:1551–\n",
      "1579, 1984. 610\n",
      "Peter J. Huber. The behavior of maximum likelihood estimates under non-\n",
      "standard conditions. Proceedings of the Fifth Berkeley Symposium on Math-\n",
      "ematical Statistics and Probability, I:221–233, 1967. 321\n",
      "Peter J. Huber and Elvezio M. Ronchetti. Robust Statistics. John Wiley &\n",
      "Sons, New York, second edition, 2009. 610\n",
      "Aleksander Janicki and Aleksander Weron. Simulation and Chaotic Behavior\n",
      "of α-Stable Stochastic Processes. Marcel Dekker, Inc., New York, 1994. 200\n",
      "E. T. Jaynes. Probability Theory: The Logic of Science. Cambridge University\n",
      "Press, Cambridge, United Kingdom, 2003. 319\n",
      "E. T. Jaynes. Information theory and statistical mechanics. Physical Review\n",
      "Series II, 106:620–630, 1957a. 382\n",
      "E. T. Jaynes. Information theory and statistical mechanics ii. Physical Review\n",
      "Series II, 108:171–190, 1957b. 382\n",
      "Harold Jeﬀreys. Theory of Probability. Oxford University Press, Oxford, third\n",
      "edition, 1961. 368, 380\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "880\n",
      "References\n",
      "Jiming Jiang. Large Sample Techniques for Statistics. Springer, New York,\n",
      "2010. 311, 321\n",
      "Norman L. Johnson, Samuel Kotz, and N. Balakrishnan. Continuous Uni-\n",
      "variate Distributions. Volume 1. John Wiley & Sons, New York, 1994. 837\n",
      "Norman L. Johnson, Samuel Kotz, and N. Balakrishnan. Continuous Multi-\n",
      "variate Distributions. Volume 1: Models and Applications. John Wiley &\n",
      "Sons, New York, second edition, 1995a. 837\n",
      "Norman L. Johnson, Samuel Kotz, and N. Balakrishnan. Continuous Uni-\n",
      "variate Distributions. Volume 2. John Wiley & Sons, New York, 1995b.\n",
      "837\n",
      "Norman L. Johnson, Samuel Kotz, and N. Balakrishnan. Discrete Multivariate\n",
      "Distributions. John Wiley & Sons, New York, 1997. 837\n",
      "Norman L. Johnson, Adrienne W. Kemp, and Samuel Kotz. Univariate Dis-\n",
      "crete Distributions. John Wiley & Sons, New York, third edition, 2005.\n",
      "837\n",
      "Valen E. Johnson and David Rossell. On the use of non-local densities in\n",
      "Bayesian hypothesis tests. Journal of the Royal Statistical Society, Series\n",
      "B, 72:143–170, 2010. 372\n",
      "L. B. W. Jolley. Summation of Series. Dover Publications, Inc., New York,\n",
      "second revised edition, 1961. 689\n",
      "Joseph B. Kadane, Mark J. Schervish, and Teddy Seidenfeld, editors. Rethink-\n",
      "ing the Foundations of Statistics. Cambridge University Press, Cambridge,\n",
      "United Kingdom, 1999. 380\n",
      "John D. Kalbﬂeisch and Ross L. Prentice. The Statistical Analysis of Failure\n",
      "Time Data. John Wiley & Sons, New York, second edition, 2002. 609\n",
      "Ioannis Karatzas and Steven E. Shreve.\n",
      "Brownian Motion and Stochastic\n",
      "Calculus. Springer-Verlag, New York, second edition, 1991. 780\n",
      "Robert E. Kass and Larry Wasserman. The selection of prior distributions by\n",
      "formal rules. Journal of the American Statistical Association, 91:1343–1370,\n",
      "1996. 382\n",
      "M. G. Kendall.\n",
      "Regression, structure and functional relationship. Part I.\n",
      "Biometrika, 38:11–25, 1951. 320\n",
      "J. M. Keynes. A Treatise on Probability. MacMillan & Co., London, 1921.\n",
      "138\n",
      "Andr´e I. Khuri. Advanced Calculus with Applications in Statistics. John Wiley\n",
      "& Sons, New York, second edition, 2003. 688, 736\n",
      "J. Kiefer. On Wald’s complete class theorems. Annals of Mathematical Statis-\n",
      "tics, 24:70–75, 1953. 320\n",
      "J. Kiefer. On Bahadur’s representation of sample quantiles. Annals of Math-\n",
      "ematical Statistics, 38:1323–1342, 1967. 97\n",
      "Dong K. Kim and Jeremy M. G. Taylor. The restricted EM algorithm for\n",
      "maximum likelihood estimation under linear restrictions on the parameters.\n",
      "Journal of the American Statistical Association, 90:708–716, 1995. 480\n",
      "Stephen M. Kogan and Douglas B. Williams. Characteristic function based\n",
      "estimation of stable distribution parameters. In Robert J. Adler, Raisa E.\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "References\n",
      "881\n",
      "Feldman, and Murad S. Taqqu, editors, Statistics and Related Topics, pages\n",
      "311–335, Boston, 1998. Birkh¨auser. 252\n",
      "T˜onu Kollo and Dietrich von Rosen. Advanced Multivariate Statistics with\n",
      "Matrices. Springer, Dordrecht, The Netherlands, 2005. 141, 792, 821\n",
      "A. N. Kolmogorov. Foundations of the Theory of Probability. Chelsea Pub-\n",
      "lishing Company, New York, 1956. translated from the German. 139\n",
      "A. N. Kolmogorov and S. V. Fomin. Elements of the Theory of Functions\n",
      "and Functional Analysis, in two volumes. Gaylock Press, Rochester, NY,\n",
      "1954, 1960. (translated from the Russian) Reprinted in one volume (1999)\n",
      "by Dover Publications, Inc., Mineola, NY.\n",
      "Samuel Kotz, N. Balakrishnan, and Norman L. Johnson. Continuous Mul-\n",
      "tivariate Distributions. Volume 2. John Wiley & Sons, New York, second\n",
      "edition, 2000. 837\n",
      "Ioannis A. Koutrouvelis.\n",
      "Regression-type estimation of the parameters of\n",
      "stable laws. Journal of the American Statistical Association, 75:918–928,\n",
      "1980. 252\n",
      "Jeanne Kowalski and Xin M. Tu. Modern Applied U-Statistics. John Wiley\n",
      "& Sons, New York, 2008. 443\n",
      "Charles H. Kraft, John W. Pratt, and A. Seidenberg. Intuitive probability on\n",
      "ﬁnite sets. Annals of Mathematical Statistics, 30:408–419, 1959. 139\n",
      "H. O. Lancaster. Zero correlation and independence. Australian Journal of\n",
      "Statistics, 1:53–56, 1959. 185\n",
      "Kenneth Lange, David R. Hunter, and Ilsoon Yang. Optimization transfer\n",
      "using surrogate objective functions (with discussion. Journal of Computa-\n",
      "tional and Graphical Statistics, 9:1–59, 2000. 829\n",
      "L. Le Cam. On some asymptotic properties of maximum likelihood estimates\n",
      "and related Bayes estimates. University of California Publications in Statis-\n",
      "tics, 1:277–330, 1953. Though this is a widely-cited paper and a review,\n",
      "MR0054913, appeared in Mathematical Reviews, no such serial as Univer-\n",
      "sity of California Publications in Statistics seems to be widely available.\n",
      "421, 422\n",
      "L. Le Cam. An extension of Wald’s theory of statistical decision functions.\n",
      "Annals of Mathematical Statistics, 26:69–81, 1955. 320\n",
      "Lucien Le Cam and Grace Lo Yang. Asymptotics in Statistics: Some Basic\n",
      "Concepts. Springer, New York, second edition, 2000. 200\n",
      "Henri Lebesgue.\n",
      "Sur le d´eveloppement de la notion d’int´egrale. Revue de\n",
      "m´etaphysique et de morale, 34:149–167, 1926. Translated by Kenneth O.\n",
      "May and reprinted in Measure and the Integral, edited by Kenneth O. May\n",
      "(1966), Holden-Day, Inc., San Francisco, 178–194. 762\n",
      "Hugues Leblanc. Statistical and Inductive Probabilities. Prentice-Hall, Inc.,\n",
      "Englewood Cliﬀs, New Jersey, 1962. Reprinted by Dover Publications, New\n",
      "York, 2006. 138\n",
      "Lawrence M. Leemis and Jacquelyn T. McQueston. Univariate distribution\n",
      "relationships. The American Statistician, 62:45–53, 2008. 837\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "882\n",
      "References\n",
      "E. L. Lehmann. A general concept of unbiasedness. Annals of Mathematical\n",
      "Statistics, 22:587–592, 1951. 320\n",
      "E. L. Lehmann. Elements of Large-Sample Theory. Springer, New York, 1999.\n",
      "321\n",
      "E. L. Lehmann and George Casella. Theory of Point Estimation. Springer,\n",
      "New York, second edition, 1998. vii\n",
      "E. L. Lehmann and Joseph P. Romano.\n",
      "Testing Statistical Hypotheses.\n",
      "Springer, New York, third edition, 2005. vii\n",
      "D. V. Lindley. A statistical paradox. Biometrika, 44:187–192, 1957. 371\n",
      "D. V. Lindley and L. D. Phillips. Inference for a Bernoulli process (A Bayesian\n",
      "view). The American Statistician, 30:112–119, 1976. 1, 381, 558\n",
      "J. E. Littlewood. Lectures on the Theory of Functions. Oxford University\n",
      "Press, Oxford, UK, 1944. 726\n",
      "Thomas A. Louis. Finding the observed information matrix when using the\n",
      "EM algorithm. Journal of the the Royal Statistical Society, Series B, 44:\n",
      "226–233, 1982. 479\n",
      "Eugene Lukacs. Characteristic Functions. Hafner Publishing Company, New\n",
      "York, second edition, 1970. 142\n",
      "Charles F. Manski. Analog Estimation Methods in Econometrics. Chapman\n",
      "& Hall, New York, 1988. 319\n",
      "P. Massart. The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality.\n",
      "Annals of Probability, 18:1269–1283, 1990. 135, 144\n",
      "Sharon Bertsch McGrayne. The Theory that Would Not Die. Yale University\n",
      "Press, New Haven, 2011. 381\n",
      "X.-L. Meng and D. B. Rubin. Maximum likelihood estimation via the ECM\n",
      "algorithm: A general framework. Biometrika, 80:267–278, 1993. 479\n",
      "N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and\n",
      "E. Teller. Equations of state calculation by fast computing machines. Jour-\n",
      "nal of Chemical Physics, 21:1087–1092, 1953. (Reprinted in Samuel Kotz\n",
      "and Norman L. Johnson (Editors) (1997), Breakthroughs in Statistics, Vol-\n",
      "ume III, Springer-Verlag, New York, 127–139.). 830\n",
      "Sean Meyn and Richard L. Tweedie. Markov Chains and Stochastic Stability.\n",
      "Cambridge University Press, Cambridge, United Kingdom, second edition,\n",
      "2009. 144\n",
      "Thomas Mikosch. Copulas: Tales and facts (with discussion). Extremes, 9:\n",
      "3–66, 2006. 141\n",
      "B. J. T. Morgan, K. J. Palmer, and M. S. Ridout. Negative score test statistic.\n",
      "The American Statistician, 61:285–288, 2007. 533, 535\n",
      "Carl N. Morris. Natural exponential families with quadratic variance func-\n",
      "tions. Annals of Statistics, 10:65–80, 1982. 200\n",
      "Carl N. Morris and Kari F. Lock. Unifying the named natural exponential\n",
      "families and their relatives. The American Statistician, 63:247–253, 2009.\n",
      "173, 200, 837\n",
      "MS2. Shao (2003). 232, 271, 275, 287, 316, 397, 398, 413, 418, 419, 420, 440,\n",
      "442, 486, 525, 529, 530, 531, 546, 547, 551, 558, 609\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "References\n",
      "883\n",
      "Roger B. Nelsen. An Introduction to Copulas. Springer, New York, second\n",
      "edition, 2006. 40\n",
      "J. Neyman and E. L. Scott. Consistent estimates based on partially consistent\n",
      "observations. Econometrica, 16:1–32, 1948. 503\n",
      "Shu Kay Ng, Thriyambakam Krishnan, and Geoﬀrey J. McLachlan. The EM\n",
      "algorithm. In James E. Gentle, Wolfgang H¨ardle, and Yuichi Mori, editors,\n",
      "Handbook of Computational Statistics; Concepts and Methods, pages 139–\n",
      "172, Berlin, 2012. Springer. 502\n",
      "Bernt Øksendal. Stochastic Diﬀerential Equations. An Introduction with Ap-\n",
      "plications. Springer, Heidelberg, ﬁfth edition, 1998. 780\n",
      "Frank W. J. Olver, Daniel W. Lozier, Ronald F. Boisvert, and Charles W.\n",
      "Clark, editors.\n",
      "NIST Handbook of Mathematical Functions.\n",
      "National\n",
      "Institute of Standards and Technology and Cambridge University Press,\n",
      "Gaithersburg, Maryland and Cambridge, United Kingdom, 2010. Available\n",
      "at http://dlmf.nist.gov/. 466, 864\n",
      "Art B. Owen. Empirical Likelihood. Chapman & Hall/CRC, Boca Raton,\n",
      "2001. 503\n",
      "Leandro Pardo. Statistical Inference Based on Divergence Measures. Chapman\n",
      "& Hall/CRC, Boca Raton, 2005. 319, 747\n",
      "Valentin V. Petrov. Limit Theorems of Probability Theory. Oxford University\n",
      "Press, Oxford, United Kingdom, 1995. 102, 142, 856\n",
      "E. J. G. Pitman. Subexponential distribution functions. Journal of the Aus-\n",
      "tralian Mathematical Society (Series A), 29:337–347, 1980. 201\n",
      "E. J. G. Pitman. The “closest” estimates of statistical parameters. Commu-\n",
      "nications in Statistics — Theory and Methods, 20:3423–3437, 1991. 319\n",
      "David Pollard. A User’s Guide to Mearure Theoretic Probability. Cambridge\n",
      "University Press, Cambridge, United Kingdom, 2003. second printing with\n",
      "corrections. 145\n",
      "Raquel Prado and Mike West.\n",
      "Time Series: Modelling, Computation and\n",
      "Inference. Chapman & Hall/CRC Press, Boca Raton, 2010. 382\n",
      "S. James Press. Estimation in univariate and multivariate stable distributions.\n",
      "Journal of the American Statistical Association, 67:842–846, 1972. 252\n",
      "S. James Press and Judith M. Tanur. The Subjectivity of Scientists and the\n",
      "Bayesian Approach. Wiley-Interscience, New York, 2001. 317, 318\n",
      "A. R. Rajwade and A. K. Bhandari. Surprises and Counterexamples in Real\n",
      "Function Theory. Hindustan Book Agency, New Delhi, 2007. 762\n",
      "C. R. Rao. Some comments on the minimum mean square as a criterion of\n",
      "estimation. In M. Cs¨org¨o, D. A. Dawson, J. N. K. Rao, and A. K. Md. E.\n",
      "Saleh, editors, Statistics and Related Topics, pages 123–143, Amsterdam,\n",
      "1981. North-Holland. 218, 319\n",
      "J. N. K. Rao. Impact of frequentist and Bayesian methods on survey sampling\n",
      "practice: A selective appraisal (with discussion). Statistical Science, 26:240–\n",
      "270, 2011. 382\n",
      "J. N. K. Rao and J. T. Webster. On two methods of bias reduction in the\n",
      "estimation of ratios. Biometrika, 53:571–577, 1966. 303\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "884\n",
      "References\n",
      "Herbert Robbins. Statistical methods related to the law of the iterated loga-\n",
      "rithm. Annals of Mathematical Statistics, 41:1397–1409, 1970. 132\n",
      "Christian P. Robert.\n",
      "The Bayesian Choice.\n",
      "Springer, New York, second\n",
      "edition, 2001. 381\n",
      "L. C. G. Rogers and David Williams. Diﬀusions, Markov Processes, and Mar-\n",
      "tingales: Volume 1, Foundations. Cambridge University Press, Cambridge,\n",
      "United Kingdom, second edition, 2000a. 780\n",
      "L. C. G. Rogers and David Williams. Diﬀusions, Markov Processes, and Mar-\n",
      "tingales: Volume 2, Itˆo Calculus. Cambridge University Press, Cambridge,\n",
      "United Kingdom, second edition, 2000b. 780\n",
      "Joseph P. Romano and Andrew F. Siegel. Counterexamples in probability and\n",
      "statistics. Wadsworth & Brooks/Cole, Monterey, California, 1986. 79, 80,\n",
      "82, 391, 401, 420, 421, 461, 484, 688, 762\n",
      "Jutta Roosen and David A. Hennessy. Testing for the monotone likelihood\n",
      "ratio assumption. Journal of Business & Economic Statistics, 22:358–366,\n",
      "2004. 558\n",
      "Richard M. Royall. Statistical Evidence: A Likelihood Paradigm. Chapman &\n",
      "Hall/CRC, Boca Raton, 1997. 318, 541, 558\n",
      "H. L. Royden. Real Analysis. MacMillan, New York, third edition, 1988. 762\n",
      "Francisco J. Samaniego. A Comparison of the Bayesian and Frequentist Ap-\n",
      "proaches to Estimation. Springer-Verlag, New York, 2010. 318\n",
      "Gennady Samorodnitsky and Murad S. Taqqu. Stable Non-Gaussian Random\n",
      "Processes. Chapman & Hall/CRC, Boca Raton, 1994. 200\n",
      "Carl-Erik S¨arndal, Bengt Swensson, and Jan Wretman. Model Assisted Survey\n",
      "Sampling. Springer-Verlag, New York, 1997. 439\n",
      "Leonard J. Savage. The Foundations of Statistics. Dover Publications, New\n",
      "York, second revised edition, 1972. (First edition by John Wiley & Sons,\n",
      "New York, 1954.). 380\n",
      "Henry Scheﬀ´e. A useful convergence theorem for probability distributions.\n",
      "Annals of Mathematical Statistics, 18:434–438, 1947. 83\n",
      "Mark J. Schervish. Theory of Statistics. Springer, New York, 1995. 75, 114\n",
      "Robert Schlaifer. Probability and Statistics for Business Decisions. McGraw-\n",
      "Hill Book Company, New York, 1959. 380\n",
      "David W. Scott.\n",
      "Multivariate density estimation and visualization.\n",
      "In\n",
      "James E. Gentle, Wolfgang H¨ardle, and Yuichi Mori, editors, Handbook\n",
      "of Computational Statistics; Concepts and Methods, pages 549–570, Berlin,\n",
      "2012. Springer. 579\n",
      "David W. Scott. Multivariate Density Estimation. John Wiley & Sons, New\n",
      "York, 1992. 579\n",
      "Shayle R. Searle, George Casella, and Charles E. McCulloch. Variance Com-\n",
      "ponents. Wiley-Interscience, New York, 1992. 503\n",
      "Robert J. Serﬂing. Approximation Theorems of Mathematical Statistics. John\n",
      "Wiley & Sons, New York, 1980. 143, 321, 412, 443\n",
      "J. Sethuraman. A constructive deﬁnition of dirichlet priors. Statistica Sinica,\n",
      "4:639–650, 1994. 382\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "References\n",
      "885\n",
      "Glenn Shafer.\n",
      "A Mathematical Theory of Evidence.\n",
      "Princeton University\n",
      "Press, Princeton, New Jersey, 1976. 381\n",
      "Glenn Shafer. Lindley’s paradox (with discussion by D. V. Lindley, Morris H.\n",
      "DeGroot, I. J. Good, Bruce M. Hill, and Robert E. Kass). Journal of the\n",
      "American Statistical Association, 77:325–351, 1982. 372\n",
      "Jun Shao. Mathematical Statistics. Springer, New York, second edition, 2003.\n",
      "vii\n",
      "Jun Shao. Mathematical Statistics: Exercises and Solutions. Springer, New\n",
      "York, 2005.\n",
      "Jun Shao and Dongsheng Tu. The Jackknife and Bootstrap. Springer-Verlag,\n",
      "New York, 1995. 321\n",
      "J. A. Shohat and J. D. Tamarkin.\n",
      "The Problem of Moments.\n",
      "American\n",
      "Mathematical Society, New York, 1943. 142\n",
      "Galen R. Shorack and Jon A. Wellner. Empirical Processes With Applications\n",
      "to Statistics (Classics in Applied Mathematics). Society for Industrial and\n",
      "Applied Mathematics, Philadelphia, 2009. (Originally published in 1986 by\n",
      "John Wiley & Sons. This “Classic Edition” includes errata as well as some\n",
      "updated material.). 144\n",
      "Christopher G. Small. Expansions and Asymptotics for Statistics. Chapman\n",
      "& Hall/CRC, Boca Raton, 2010. 609\n",
      "Christopher G. Small and Jinfang Wang. Numerical Methods for Nonlinear\n",
      "Estimating Functions. Oxford University Press, Oxford, 2003. 320\n",
      "Daniel Solow. How to Read and Do Proofs. John Wiley & Sons, New York,\n",
      "third edition, 2003. 688\n",
      "Ehsan S. Sooﬁ. Capturing the intangible concept of information. Journal of\n",
      "the American Statistical Association, 89:1243–1254, 1994. 318\n",
      "James C. Spall.\n",
      "Stochastic optimization.\n",
      "In James E. Gentle, Wolfgang\n",
      "H¨ardle, and Yuichi Mori, editors, Handbook of Computational Statistics;\n",
      "Concepts and Methods, pages 173–202, Berlin, 2012. Springer. 829\n",
      "Robert G. Staudte and Simon J. Sheather. Robust Estimation and Testing.\n",
      "John Wiley & Sons, New York, 1990. 610\n",
      "J. Michael Steele. Stochastic Calculus and Financial Applications. Springer-\n",
      "Verlag, New York, 2001. 780\n",
      "J. Michael Steele. The Cauchy-Schwarz Master Class: An Introduction to the\n",
      "Art of Mathematical Inequalities. Cambridge University Press, Cambridge,\n",
      "United Kingdom, 2004. 856\n",
      "Lynn Arthur Steen and J. Arthur Seebach Jr. Counterexamples in Topology.\n",
      "Dover Publications, Inc., Mineola, New York, 1995. (reprint of the second\n",
      "edition published by Springer-Verlag, New York, 1978). 762\n",
      "Frederick F. Stephan.\n",
      "The expected value and variance of the reciprocal\n",
      "and other negative powers of a positive bernoullian variate.\n",
      "Annals of\n",
      "Mathematical Statistics, 16:50–61, 1945. 460\n",
      "Fred W. Steutel. Preservation of Inﬁnite Divisibility under Mixing and Related\n",
      "Topics. Mathematisch Centrum Amsterdam, Amsterdam, The Netherlands,\n",
      "1970. 200\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "886\n",
      "References\n",
      "Fred W. Steutel and Klaas van Harn. Inﬁnite Divisibility of Probability Dis-\n",
      "tributions on the Real Line. Marcel Dekker, Inc., New York, 2004. 200\n",
      "Stephen M. Stigler. A History of Statistics: The Measurement of Uncertainty\n",
      "before 1900. Belknap Press of Harvard University Press, Cambridge, 1986.\n",
      "380\n",
      "John D. Storey. A direct approach to false discovery rates. Journal of the\n",
      "Royal Statistical Society, Series B, 64:479–498, 2002. 559\n",
      "John D. Storey. The positive false discovery rate: A Bayesian interpretation\n",
      "and the q-value. Annals of Statistics, 31:2013–2035, 2003. 559\n",
      "Jordan M. Stoyanov. Counterexamples in Probability. John Wiley & Sons,\n",
      "Ltd., Chichester, United Kingdom, 1987. 762\n",
      "William E. Strawderman.\n",
      "Proper Bayes minimax estimators of the multi-\n",
      "variate normal mean. Annals of Mathematical Statistics, 42:385–388, 1971.\n",
      "273\n",
      "Robert Lee Taylor, Peter Z. Daﬀer, and Ronald F. Patterson. Limit Theo-\n",
      "rems for Sums of Exchangeable Random Variables. Rowman & Allanheld,\n",
      "Totowa, NJ, 1985. 143\n",
      "Jozef L. Teugels. The class of subexponential distributions. Annals of Proba-\n",
      "bility, 3:1000–1011, 1975. 200\n",
      "William J. Thompson. Atlas for Computing Mathematical Functions: An Il-\n",
      "lustrated Guide for Practitioners with Programs in Fortran 90 and Mathe-\n",
      "matica. John Wiley & Sons, New York, 1997. 864\n",
      "W. A. Thompson Jr. The problem of negative estimates of variance compo-\n",
      "nents. Annals of Mathematical Statistics, 33:273–289, 1962. 503\n",
      "TPE2. Lehmann and Casella (1998). 233, 311, 442\n",
      "TSH3. Lehmann and Romano (2005). 200, 528, 558\n",
      "Richard Valliant, Alan H. Dorfman, and Richard M. Royall. Finite Population\n",
      "Sampling and Inference: A Prediction Approach. John Wiley & Sons, New\n",
      "York, 2000. 439\n",
      "A. W. van der Vaart. Asymptotic Statistics. Cambridge University Press,\n",
      "Cambridge, United Kingdom, 1998. 321\n",
      "Various Authors. Chapter 4, Theory and Methods of Statistics. In Adrian E.\n",
      "Raftery, Martin A. Tanner, and Martin T. Wells, editors, Statistics in the\n",
      "21st Century, New York, 2002. Chapman and Hall.\n",
      "Geert Verbeke and Geert Molenberghs. What can go wrong with the score\n",
      "test? The American Statistician, 61:289–290, 2007. 558\n",
      "R. von Mises (de Mis`es). La distribution de la plus grande de n valeurs. Revue\n",
      "Math´ematique de l’Union Interbalkanique, pages 141–160, 1939. 143\n",
      "R. von Mises (v. Mises). Fundamentals¨atze der wahrscheinlichkeitsrechnung.\n",
      "Mathematische Zeitschrift, 4:1–97, 1919a. 138\n",
      "R. von Mises (v. Mises). Grundlagen der wahrscheinlichkeitsrechnung. Math-\n",
      "ematische Zeitschrift, 5:52–99, 1919b. 138\n",
      "Abraham Wald.\n",
      "Contributions to the theory of statistical estimation and\n",
      "testing hypotheses. Annals of Mathematical Statistics, 10:299–326, 1939.\n",
      "320\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "References\n",
      "887\n",
      "Abraham Wald. Sequential tests of statistical hypotheses. Annals of Mathe-\n",
      "matical Statistics, 16:117–186, 1945. 559\n",
      "Abraham Wald. An essentially complete class of admissible decision functions.\n",
      "Annals of Mathematical Statistics, 18:549–555, 1947a. 320\n",
      "Abraham Wald. Foundations of a general theory of sequential decision func-\n",
      "tions. Econometrica, 15:279–313, 1947b. 559\n",
      "Abraham Wald. Note on the consistency of the maximum likelihood estimate.\n",
      "Annals of Mathematical Statistics, 20:595–601, 1949. 242, 449\n",
      "Abraham Wald. Statistical Decision Functions.\n",
      "John Wiley & Sons, New\n",
      "York, 1950. Reprinted by Chelsea Publishing Company, New York, 1971.\n",
      "319, 320, 559\n",
      "A. M. Walker. On the asymptotic behaviour of posterior distributions. Journal\n",
      "of the Royal Statistical Society, Series B, 31:80–88, 1969. 334\n",
      "R.W.M. Wedderburn. Quasi-likelihood functions, generalized linear models,\n",
      "and the Gauss-Newton method. Biometrika, 61:439–447, 1974. 499, 503\n",
      "Mike West and JeﬀHarrison.\n",
      "Bayesian Forecasting and Dynamic Models.\n",
      "Springer-Verlag, New York, second edition, 1997. 382\n",
      "Halbert White. A heteroskedasticity-consistent covariance matrix estimator\n",
      "and a direct test for heteroskedasticity. Econometrica, 48:817–838, 1980.\n",
      "321\n",
      "Peter Whittle. Probability via Expectation. Springer, New York, fourth edition,\n",
      "2000. 140\n",
      "R. A. Wijsman. On the attainment of the Cram´er-Rao lower bound. Annals\n",
      "of Statistics, 1:538–542, 1973. 443\n",
      "Gary L. Wise and Eric B. Hall.\n",
      "Counterexamples in Probability and Real\n",
      "Analysis. The Clarendon Press, Oxford University Press, New York, 1993.\n",
      "762\n",
      "J. Wolfowitz. On ϵ-complete classes of decision functions. Annals of Mathe-\n",
      "matical Statistics, 22:461–465, 1951. 265\n",
      "C.F.J. Wu. Jackknife, bootstrap and other resampling methods in regression\n",
      "analysis (with discussion). Annals of Statistics, 14:1261–1350, 1986. 321\n",
      "Nailong Wu. The Maximum Entropy Method. Springer, New York, 1997. 319\n",
      "Ronald R. Yager and Liping Liu, editors. Classic Works of the Dempster-\n",
      "Shafer Theory of Belief Functions. Springer-Verlag, New York, 2008. 381\n",
      "Ryszard Zieli´nski. Kernel estimators and the Dvoretzky-Kiefer-Wolfowitz in-\n",
      "equality. Applicationes Mathematicae, 34:401–404, 2007. 594\n",
      "A. Zygmund. A remark on characteristic funtions. Annals of Mathematical\n",
      "Statistics, 18:272–276, 1947. 148\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "a.e. (almost everywhere), 710, 720\n",
      "a.s. (almost sure) convergence, 76\n",
      "a.s. (almost surely), 4, 9, 710\n",
      "Abelian group, 630\n",
      "absolute moment, 31, 847\n",
      "ﬁniteness, 31\n",
      "absolute-error loss, 262\n",
      "absolutely continuous function, 722\n",
      "absolutely continuous measure wrt\n",
      "another measure, 711, 868\n",
      "absolutely continuous random variable,\n",
      "19\n",
      "acceptance region, 292\n",
      "acceptance/rejection method, 664\n",
      "accumulation point, 623, 648, 650\n",
      "accuracy of conﬁdence set, 546\n",
      "accuracy of conﬁdence sets, 546\n",
      "ACF (autocorrelation function), 123\n",
      "action space, 259\n",
      "adapted to, 125\n",
      "adjugate, 871\n",
      "admissibility, 264, 270–274, 353\n",
      "almost everywhere; “λ-admissibility”,\n",
      "264\n",
      "in Bayesian analyses, 328, 353\n",
      "Pitman, 274\n",
      "restricted; “T -admissibility”, 264\n",
      "aﬃne independence, 636\n",
      "Aitken’s integral, 682, 817\n",
      "algebra of sets, 693\n",
      "almost equivariant, 285\n",
      "almost everywhere (a.e.), 710, 720\n",
      "almost surely (a.s.), 4, 9, 710\n",
      "almost uniform convergence, 726\n",
      "α-stable, 62\n",
      "α0-α1 loss (weighted 0-1 loss), 263,\n",
      "365–366\n",
      "alternative hypothesis, 291\n",
      "AMISE (asymptotic mean integrated\n",
      "squared error), 575\n",
      "analog (plug-in) estimator, 319\n",
      "analysis of deviance, 494\n",
      "analytic continuation, 48, 661, 741\n",
      "analytic function, 656, 741\n",
      "ancillarity, 223, 226\n",
      "AOV model, 433–438, 488–491\n",
      "random eﬀects, 436, 490\n",
      "approximate inference, 235\n",
      "Archimedean ordered ﬁeld, 634\n",
      "ARE (asymptotic relative eﬃciency),\n",
      "314, 419, 483\n",
      "ASH (average shifted histogram), 589\n",
      "asymptotic accuracy of conﬁdence set,\n",
      "551\n",
      "asymptotic bias, 311\n",
      "and consistency, 312\n",
      "asymptotic conﬁdence set, 550–551\n",
      "asymptotic correctness of conﬁdence\n",
      "set, 551\n",
      "asymptotic distribution, 80, 92–101\n",
      "asymptotic eﬃciency, 418–422, 481–487\n",
      "and consistency, 421\n",
      "asymptotic expectation, 100–101,\n",
      "310–316\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "890\n",
      "Index\n",
      "asymptotic inference, 295, 301, 306–317\n",
      "asymptotic mean integrated squared\n",
      "error (AMISE), 575\n",
      "asymptotic mean squared error, 313\n",
      "asymptotic relative eﬃciency, 314, 419,\n",
      "483\n",
      "asymptotic signiﬁcance, 314, 527\n",
      "asymptotic signiﬁcance level, 315\n",
      "asymptotic variance, 310, 313\n",
      "asymptotic variance-covariance matrix,\n",
      "420–421\n",
      "asymptotically Fisher eﬃcient, 419\n",
      "asymptotically pivotal function, 551\n",
      "asymptotically unbiased estimation,\n",
      "311, 414–418\n",
      "autocorrelation, 123\n",
      "autocovariance, 123\n",
      "average shifted histogram (ASH), 589\n",
      "Axiom of Choice, 674\n",
      "axpy operation, 635\n",
      "Bachelier-Wiener process, 130, 766–772\n",
      "Bahadur representation, 97\n",
      "Banach space, 639, 648, 741\n",
      "basis functions, 749\n",
      "basis set, 635, 686\n",
      "Basu’s theorem, 226\n",
      "Bayes action, 345\n",
      "Bayes credible set, 372–376\n",
      "Bayes estimator, 330, 345\n",
      "Bayes factor, 366–369\n",
      "Bayes risk, 328\n",
      "Bayes rule, 329, 352–376\n",
      "Bayes sensitivity analysis, 336, 339, 347\n",
      "Bayesian estimation, 352–361, 372–376\n",
      "Bayesian expected loss, 329\n",
      "Bayesian Inference\n",
      "choice of prior, 346–352\n",
      "Bayesian inference, 268, 325–387\n",
      "Bayesian testing, 362–372\n",
      "belief function, 381\n",
      "Benford’s law, 183, 202, 839\n",
      "Benjamini-Hochberg method, 538\n",
      "Bernoulli’s theorem, 102\n",
      "Bernstein inequalities, 848\n",
      "Bernstein’s theorem, 189\n",
      "beta function, 866\n",
      "beta integral, 681\n",
      "beta-binomial distribution, 836\n",
      "Bhattacharyya bound on variance, 402\n",
      "bias, 218\n",
      "and consistency, 312\n",
      "asymptotic, 311\n",
      "limiting, 311\n",
      "biased estimator\n",
      "minimax, 276\n",
      "Pitman-closer, 220\n",
      "smaller MSE, 243, 273, 274, 428\n",
      "big O, 83, 309, 652\n",
      "big O in probability, 83, 308\n",
      "bijection, 625, 630, 701, 703\n",
      "binomial series, 682\n",
      "birth process, 129\n",
      "Boltzmann distribution, 830\n",
      "Bolzano-Weierstrass property, 649\n",
      "Bolzano-Weierstrass theorem, 649, 650\n",
      "bona ﬁde density estimator, 579\n",
      "Bonferroni, 538\n",
      "Bonferroni’s method for simultaneous\n",
      "conﬁdence intervals, 557\n",
      "bootstrap, 248–250, 304\n",
      "conﬁdence sets, 552–557\n",
      "variance estimation, 304\n",
      "bootstrap principle, 249\n",
      "Borel function, 9, 719\n",
      "Borel measure, 717\n",
      "Borel set, 714\n",
      "Borel σ-ﬁeld, 697, 714–716\n",
      "Borel-Cantelli lemma, 73, 74\n",
      "boundary, 622, 645\n",
      "bounded completeness, 162\n",
      "bounded convergence theorem, 734\n",
      "bounded in probability, 83\n",
      "bounded variation, 657\n",
      "Bowley coeﬃcient, 53\n",
      "branching process, 129\n",
      "Breslow’s estimator (proportional\n",
      "hazards), 578\n",
      "Brownian bridge, 130\n",
      "Brownian motion, 130, 766–773\n",
      "Burr distribution, 197\n",
      "Ck class of functions, 862\n",
      "Ck class of functions, 740\n",
      "cadlag, 126, 143\n",
      "canonical exponential form, 173, 231\n",
      "Cantor function, 15, 145, 722\n",
      "Cantor set, 714, 717, 723\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "891\n",
      "measure, 717\n",
      "Carath´eodory extension theorem, 712\n",
      "cardinality, 616\n",
      "Carleman criteria, 34\n",
      "cartesian product, 617, 700\n",
      "cartesian product measurable space,\n",
      "701\n",
      "Cauchy criterion, 77, 648, 689\n",
      "Cauchy sequence, 639, 648, 689\n",
      "Cauchy-Schwarz inequality, 636, 853\n",
      "causal inference, 216\n",
      "CDF (cumulative distribution function),\n",
      "13\n",
      "inverse, 15\n",
      "notation, 137\n",
      "relation to uniform distribution, 15\n",
      "tail, 14, 166\n",
      "CDF-skewing, 195\n",
      "censored data, 452, 471\n",
      "censoring, 193\n",
      "central limit theorem\n",
      "iid sequence, 87\n",
      "independent sequence, 104–108\n",
      "martingale, 134\n",
      "multivariate, 107\n",
      "central moment, 30\n",
      "CF (characteristic function), 45–51\n",
      "empirical (ECF), 251\n",
      "change of variables, 734\n",
      "change of variables method, 56\n",
      "characteristic exponent, 62\n",
      "characteristic function (CF), 45–51\n",
      "empirical (ECF), 251\n",
      "characteristic of a ﬁeld, 633\n",
      "Chebyshev norm, 745\n",
      "Chebyshev’s inequality, 848\n",
      "Chernoﬀconsistency, 528\n",
      "chi-squared discrepancy measure, 253,\n",
      "748\n",
      "Cholesky factorization, 794\n",
      "Christoﬀel-Darboux formula, 751\n",
      "CIR (Cox-Ingersoll-Ross) process, 774\n",
      "clopen set, 622, 625, 640\n",
      "closed set, 622, 624\n",
      "closure, 622, 645\n",
      "random variable space, 35\n",
      "cluster point, 623, 648\n",
      "Cochran’s theorem, 188, 430–432\n",
      "cocountable, 697\n",
      "cofactor, 871\n",
      "coherency, 139\n",
      "collection of sets, 618, 692\n",
      "commutative group, 630\n",
      "compact set, 622, 645\n",
      "complement of a set, 617\n",
      "complete\n",
      "measure, 707\n",
      "measure space, 709\n",
      "metric space, 639, 648, 741\n",
      "probability space, 4\n",
      "complete class of decision rules, 265,\n",
      "353\n",
      "complete family of distributions, 162\n",
      "complete statistic, 225\n",
      "complete suﬃciency, 225, 226\n",
      "completing the square, 684\n",
      "completion\n",
      "of a measure space, 712\n",
      "of a metric space, 639\n",
      "complex numbers, IC, 660–663\n",
      "composite hypothesis, 291\n",
      "computational complexity, 303, 414\n",
      "computational inference, 235, 295, 301\n",
      "concave function, 658\n",
      "concentrated likelihood, 242, 499\n",
      "conditional\n",
      "entropy, 121\n",
      "expectation, 110–118, 236\n",
      "independence, 120\n",
      "probability, 119\n",
      "probability distribution, 119\n",
      "conditional likelihood, 242, 500\n",
      "conditionality principle, 318\n",
      "conﬁdence coeﬃcient, 297, 542\n",
      "limiting, 315\n",
      "conﬁdence interval, 297\n",
      "equal-tail, 543\n",
      "conﬁdence set, 296–301, 507–560\n",
      "Bayes credible set, 372\n",
      "simultaneous, 557–558\n",
      "unbiased, 547\n",
      "uniformly most accurate unbiased\n",
      "(UMAU), 547\n",
      "conjugate prior, 270, 327, 337, 346\n",
      "connected space, 623, 645\n",
      "consistency, 75, 307–310, 571–572\n",
      "an, 308\n",
      "and asymptotic eﬃciency, 421\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "892\n",
      "Index\n",
      "Chernoﬀ, 528\n",
      "in mean, 308\n",
      "in mean squared error, 308, 313\n",
      "Lr, 308\n",
      "of estimators, 307, 571\n",
      "of positive deﬁnite matrices, 310\n",
      "of tests, 315, 527\n",
      "strong, 307\n",
      "weak, 307\n",
      "consistent estimator, 307, 571\n",
      "continuity theorem, 87\n",
      "continuous function, 626, 720–724, 803\n",
      "absolutely continuous, 722\n",
      "H¨older-continuous, 723\n",
      "Lipschitz-continuous, 723\n",
      "Lipschitz-continuous PDF, 584\n",
      "continuous random variable, 19\n",
      "contradiction (method of proof), 675\n",
      "contrast, 435, 557\n",
      "convergence, 75–102\n",
      "almost sure, 76, 726\n",
      "in Lr, 76\n",
      "in absolute mean, 77\n",
      "in distribution, 78\n",
      "in law, 78\n",
      "in mean, 77\n",
      "in mean square, 77, 571\n",
      "in probability, 77\n",
      "in quadratic mean, 571\n",
      "in second moment, 77\n",
      "of function estimators, 571–572,\n",
      "574–575\n",
      "of probability density functions, 82\n",
      "pointwise, 725\n",
      "uniform, 725\n",
      "weak, 78, 80\n",
      "with probability 1, 76\n",
      "wp1, 76\n",
      "convergence of a sequence of sets, 627\n",
      "convergence of powers of a matrix, 820\n",
      "convergence-determining class, 79\n",
      "convex function, 658, 849\n",
      "convex loss, 261, 264, 267, 269\n",
      "convex set, 658\n",
      "convexity, 658\n",
      "convolution, 57, 742\n",
      "convolution theorem, 759\n",
      "copula, 39–40, 120\n",
      "Cor(·,·), 37\n",
      "correctness of conﬁdence sets, 546\n",
      "correlation, 36, 40\n",
      "correlation of functions, 742\n",
      "correlation theorem, 759\n",
      "countable, 616\n",
      "counting measure, 708\n",
      "Cov(·,·), 36\n",
      "covariance, 36, 39\n",
      "covariance inequality, 399, 853\n",
      "covariance of functions, 742\n",
      "cover (by a collection of sets), 620\n",
      "coverage probability, 297\n",
      "Cox proportional hazards model, 579\n",
      "Cox-Ingersoll-Ross (CIR) process, 774\n",
      "Cram´er von Mises test, 536\n",
      "Cram´er-Rao lower bound, 235, 399, 421\n",
      "Cram´er-Wold device, 91\n",
      "credible set, 372–376\n",
      "Cressie-Read divergence measure, 253\n",
      "critical region, 292\n",
      "CRLB (information inequality), 235,\n",
      "399, 421\n",
      "cumulant, 34\n",
      "cumulant-generating function, 50\n",
      "cumulative distribution function (CDF),\n",
      "13\n",
      "inverse, 15\n",
      "notation, 137\n",
      "relation to uniform distribution, 15\n",
      "tail, 14, 166\n",
      "curved exponential families, 175\n",
      "Darmois theorem, 189\n",
      "data-generating process, 205, 237\n",
      "de Finetti’s representation theorem, 74,\n",
      "114, 333\n",
      "de Moivre Laplace central limit\n",
      "theorem, 105\n",
      "de Moivre’s formula, 679\n",
      "de Moivre’s martingale, 152\n",
      "De Morgan’s law, 617\n",
      "decision rule, 260\n",
      "randomized, 260\n",
      "decision theory, 259–276, 278–289\n",
      "decomposable matrix, 818\n",
      "decomposition of a function, 568\n",
      "Dedekind completeness, 644\n",
      "degenerate random variable, 9\n",
      "degree of statistical function, 392, 405\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "893\n",
      "delete d jackknife, 302\n",
      "δ-ﬁeld, 694\n",
      "delta method, 94, 316, 481, 557\n",
      "second order, 94, 481\n",
      "dense set, 622, 641\n",
      "density function, 19\n",
      "derivative, 739\n",
      "derivative of a functional, 760–761\n",
      "derivative with respect to a vector or\n",
      "matrix, 801\n",
      "det(·), 783\n",
      "determinant of a square matrix, 783\n",
      "determining class, 3, 79\n",
      "deviance, 245, 494\n",
      "DF see CDF, 13\n",
      "DFT (discrete Fourier transform), 686\n",
      "diag(·), 799\n",
      "diﬀerence equation, 687\n",
      "diﬀerential, 805\n",
      "diﬀerential equation, 687\n",
      "diﬀerential scaling, 195\n",
      "diﬀerentiation of vectors and matrices,\n",
      "801\n",
      "diﬀusion process, 765\n",
      "digamma function, 466, 865\n",
      "dimensionality, 198\n",
      "problems in higher dimensions, 199,\n",
      "203\n",
      "Dirac delta function, 738, 864\n",
      "Dirac measure, 708\n",
      "direct product, 617, 793\n",
      "Dirichlet function, 721\n",
      "discrete Fourier transform (DFT), 686\n",
      "discrete random variable, 18\n",
      "disjoint sets, 618\n",
      "disjointiﬁcation, 619\n",
      "distribution family\n",
      "Benford’s, 839\n",
      "Bernoulli, 94, 237, 269, 281, 333, 340,\n",
      "390, 394, 397, 398, 447, 452, 459,\n",
      "481, 482, 518, 520, 539, 541, 838\n",
      "beta, 63, 170, 232, 337, 355, 357, 360,\n",
      "374, 383, 385, 843\n",
      "binomial, 58, 167, 170, 179, 237, 276,\n",
      "337, 340, 355, 360, 365, 374, 383,\n",
      "385, 390, 447, 459, 531, 838\n",
      "Cauchy, 26, 43, 65, 149, 171, 174, 181,\n",
      "461, 843\n",
      "chi-squared, 56, 59, 60, 187, 841\n",
      "complex multivariate normal, 187,\n",
      "840\n",
      "conditional, 111, 119\n",
      "Dirichlet, 170, 843\n",
      "discrete uniform, 838\n",
      "double exponential, 167, 170, 171,\n",
      "181, 313, 844\n",
      "doubly noncentral F, 188\n",
      "elliptical, 198\n",
      "ϵ-mixture distribution, 157, 194, 461,\n",
      "601–605, 754\n",
      "exponential, 20, 55, 63, 98, 99, 129,\n",
      "167, 170, 171, 181, 228, 382, 397,\n",
      "450, 451, 456, 471, 483, 510, 512,\n",
      "521, 529, 844\n",
      "spacings, 64, 129\n",
      "exponential class, 169–177\n",
      "attainment of CRLB, 400\n",
      "conjugate priors, 337, 382\n",
      "extreme value, 99, 844\n",
      "F, 59, 188, 841\n",
      "families, 155–199, 835–844\n",
      "gamma, 58, 99, 170, 181, 231, 384,\n",
      "387, 455, 466, 467, 844\n",
      "geometric, 838\n",
      "hypergeometric, 159, 384, 468, 838\n",
      "inﬁnitely divisible, 183\n",
      "inverse Gaussian, 58, 170, 181, 842\n",
      "inverted chi-squared, 170, 342, 842\n",
      "inverted gamma, 382, 842\n",
      "inverted Wishart, 842\n",
      "location-scale, 179\n",
      "logarithmic, 839\n",
      "logistic, 170, 181, 843\n",
      "lognormal, 43, 147, 170, 842\n",
      "multinomial, 170, 470, 838\n",
      "multivariate matrix normal, 186, 840\n",
      "multivariate normal, 186, 272, 840\n",
      "negative binomial, 58, 170, 237, 340,\n",
      "357, 384, 390, 443, 447, 459, 838\n",
      "noncentral chi-squared, 188, 841\n",
      "noncentral F, 188, 841\n",
      "noncentral t, 841\n",
      "normal, 56, 58, 60, 96, 167, 170, 181,\n",
      "185–191, 227, 230, 242, 289, 298,\n",
      "312, 341, 342, 359, 366, 371, 396,\n",
      "400, 473, 670, 840\n",
      "Pareto, 164, 170, 171, 843\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "894\n",
      "Index\n",
      "Poisson, 57, 167, 170, 378, 384, 387,\n",
      "401, 443, 839\n",
      "positive Poisson, 192, 203\n",
      "power function, 164, 843\n",
      "power law, 164\n",
      "power series, 170, 175, 839\n",
      "regular, 168–169\n",
      "skew normal, 196, 842\n",
      "skewed distributions, 195\n",
      "by CDF, 195\n",
      "spherical, 198\n",
      "stable, 183\n",
      "t, 188, 841\n",
      "two-piece distribution, 196\n",
      "uniform, 63, 64, 81, 98, 99, 167, 171,\n",
      "181, 226, 227, 397, 454, 462, 838,\n",
      "843\n",
      "von Mises, 667, 843\n",
      "Weibull, 170, 171, 844\n",
      "Wishart, 841\n",
      "zeta, 164\n",
      "Zipf, 164\n",
      "distribution function space, 194, 754\n",
      "distribution function see cumulative\n",
      "distribution function, 13\n",
      "distribution vector, 127\n",
      "divisibility, 60–61, 754\n",
      "DKW inequality, 135, 144\n",
      "domain of attraction, 109\n",
      "dominated convergence theorem, 89,\n",
      "734\n",
      "conditional, 112\n",
      "dominating measure, 21, 711, 868\n",
      "dominating statistical rule, 264\n",
      "Donsker’s theorem, 137\n",
      "Doob’s martingale inequality, 133\n",
      "dot product, 636, 743\n",
      "double integral, 735\n",
      "Dvoretzky/Kiefer/Wolfowitz inequality,\n",
      "144, 562\n",
      "Dvoretzky/Kiefer/Wolfowitz/Massart\n",
      "inequality, 135, 248\n",
      "Dynkin system, 694\n",
      "Dynkin’s π-λ theorem, 698\n",
      "E(·), 25, 28, 817\n",
      "ECDF (empirical cumulative distri-\n",
      "bution function), 25, 134–137,\n",
      "246–250, 602\n",
      "Edgeworth series, 68, 753\n",
      "eﬃciency, 256, 313, 457\n",
      "estimating function, 257\n",
      "Godambe, 257\n",
      "eﬃcient estimating function, 257\n",
      "eﬃcient estimator, 256, 399\n",
      "Egoroﬀ’s theorem, 726\n",
      "eigenfunction, 750\n",
      "eigenvalue, 750, 784\n",
      "eigenvector, 784\n",
      "eigenvector, left, 789\n",
      "element, 621\n",
      "elliptical family, 198\n",
      "EM method, 469–480\n",
      "empirical Bayes, 352, 357\n",
      "empirical characteristic function (ECF),\n",
      "251\n",
      "empirical cumulative distribution\n",
      "function (ECDF), 25, 134–137,\n",
      "246–250, 602\n",
      "empirical likelihood, 250, 499\n",
      "empirical likelihood ratio test, 536\n",
      "empirical process, 133–137\n",
      "empty set, 616\n",
      "entropy, 40–42, 121, 157\n",
      "conditional, 121\n",
      "Shannon, 41\n",
      "ϵ-mixture distribution, 157, 194, 461,\n",
      "601–605, 754\n",
      "equal-tail conﬁdence interval, 543\n",
      "equivariance, 220, 266, 267, 279–289\n",
      "equivariance principle, 279\n",
      "equivariant function, 756\n",
      "equivariant statistical procedures, 267,\n",
      "279–289\n",
      "equivariant conﬁdence sets, 549–550\n",
      "equivariant estimation, 285–289, 357,\n",
      "458\n",
      "invariant tests, 525–527\n",
      "Esseen-von-Bahr inequality, 855\n",
      "essential inﬁmum, 745\n",
      "essential supremum, 745\n",
      "essentially complete, 265\n",
      "estimability, 391, 426\n",
      "estimating equation, 243, 254\n",
      "estimating function, 254–257, 463\n",
      "martingale, 257\n",
      "estimator\n",
      "Bayes, 330, 352–361\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "895\n",
      "equivariant, 285–289, 458\n",
      "maximum likelihood, 242–244,\n",
      "449–465\n",
      "method of moments (MME), 247,\n",
      "272, 416\n",
      "order statistics, 252\n",
      "plug-in, 247, 416, 418\n",
      "randomized, 276, 420\n",
      "uniformly minimum variance\n",
      "unbiased, 392–403\n",
      "Euclidean distance, 643, 782\n",
      "Euclidean norm, 782\n",
      "Euler’s formula, 45, 661, 679\n",
      "Euler’s integral, 865\n",
      "event, 3, 709\n",
      "evidence, statistical, 318, 541\n",
      "exact inference, 235\n",
      "exchangeability, 7, 24, 74, 333\n",
      "expectation functional, 404, 416\n",
      "expected value\n",
      "conditional expectation, 110\n",
      "of a Borel function of random\n",
      "variable, 28\n",
      "of a random variable, 25\n",
      "experimental support, 245\n",
      "exponential class of families, 169–177\n",
      "attainment of CRLB, 400\n",
      "canonical exponential form, 173\n",
      "conjugate priors, 337, 382\n",
      "curved, 175\n",
      "full rank, 162, 175\n",
      "mean-value parameter, 172\n",
      "natural parameter, 173\n",
      "one-parameter, 173, 271\n",
      "exponential criterion, 226\n",
      "exponential tail, 564\n",
      "extended real numbers, IR, 640\n",
      "extension of a measure, 712\n",
      "extension theorem\n",
      "Carath´eodory, 712\n",
      "Kolmogorov, 125\n",
      "extreme value distribution, 99, 108–109\n",
      "extreme value index, 109\n",
      "f-divergence, 253, 747\n",
      "factorial moment, 34, 45\n",
      "factorial-moment-generating function,\n",
      "45\n",
      "factorization criterion, 222\n",
      "false discovery rate (FDR), 537\n",
      "false nondiscovery rate (FNR), 537\n",
      "family of probability distributions, 12,\n",
      "155–199, 835–844\n",
      "family wise error rate (FWER), 537\n",
      "Fatou’s lemma, 89, 733\n",
      "conditional, 112\n",
      "FDR (false discovery rate), 537\n",
      "Feller process, 774\n",
      "Feller’s condition, 107, 774, 778\n",
      "FI regularity conditions, 168, 229, 399,\n",
      "457\n",
      "ﬁeld, 632, 648\n",
      "characteristic of, 633\n",
      "order of, 633\n",
      "ordered, 634\n",
      "ﬁeld of sets (algebra), 693\n",
      "ﬁlter, 568\n",
      "ﬁltered probability space, 126\n",
      "ﬁltration, 125\n",
      "ﬁnite measure, 707\n",
      "ﬁnite population sampling, 305, 382,\n",
      "438–442\n",
      "ﬁrst limit theorem, 87\n",
      "ﬁrst passage time, 123\n",
      "ﬁrst-order ancillarity, 223\n",
      "Fisher eﬃciency, 420\n",
      "Fisher eﬃcient, 256, 313, 400, 419, 457\n",
      "Fisher information, 229–235, 399, 815\n",
      "regularity conditions, 168, 229, 399,\n",
      "457\n",
      "Fisher scoring, 466\n",
      "ﬁxed-eﬀects AOV model, 434, 435, 488,\n",
      "489\n",
      "FNR (false nondiscovery rate), 537\n",
      "forward martingale, 131\n",
      "Fourier coeﬃcient, 686, 750, 789\n",
      "Fourier expansion, 789\n",
      "Fourier transform, 757, 758\n",
      "Fr´echet derivative, 760\n",
      "Freeman-Tukey statistic, 253\n",
      "frequency polygon, 589\n",
      "frequency-generating function, 44\n",
      "frequentist risk, 328\n",
      "Frobenius norm, 782, 795\n",
      "Fubini’s theorem, 735\n",
      "full rank exponential families, 162, 175\n",
      "function, 625, 655\n",
      "real, 655–660\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "896\n",
      "Index\n",
      "function estimation, 565–576\n",
      "function space, 740–754\n",
      "of random variables, 35\n",
      "functional, 51–54, 247, 759–761\n",
      "expectation, 404, 416\n",
      "FWER (family wise error rate), 537\n",
      "Galois ﬁeld, 633\n",
      "Galton-Watson process, 129\n",
      "gambler’s ruin, 90\n",
      "game theory, 320\n",
      "gamma function, 865\n",
      "gamma integral, 681\n",
      "gamma-minimax Bayes action, 347\n",
      "Gˆateaux derivative, 606, 760\n",
      "Gauss-Markov theorem, 427\n",
      "Gaussian copula, 40\n",
      "GEE (generalized estimating equation),\n",
      "254, 486\n",
      "generalized Bayes action, 345\n",
      "generalized estimating equation (GEE),\n",
      "254, 486\n",
      "generalized inverse, 784, 799\n",
      "generalized lambda family of distribu-\n",
      "tions, 197\n",
      "generalized linear model, 492–498\n",
      "generating function, 42–51\n",
      "geometric Brownian motion, 773\n",
      "geometric series, 682\n",
      "Gibbs lemma, 41\n",
      "Gibbs method, 669\n",
      "Gini’s mean diﬀerence, 407\n",
      "Glivenko-Cantelli theorem, 135, 248,\n",
      "562\n",
      "Godambe eﬃciency, 257\n",
      "goodness of ﬁt test, 536\n",
      "gradient of a function, 807, 808, 871\n",
      "Gram-Charlier series, 68, 753\n",
      "Gram-Schmidt transformation, 685\n",
      "Gramian matrix, 799\n",
      "group, 630\n",
      "transformation, 630, 754\n",
      "group family, 178–183\n",
      "Gumbel distribution, 99\n",
      "Haar invariance, 729\n",
      "Haar invariant measure, 708, 729\n",
      "Hadamard derivative, 760\n",
      "H´ajek-R`enyi inequality, 133, 849\n",
      "Hamburger moment problem, 142\n",
      "Hammersley-Chapman-Robbins\n",
      "inequality, 854\n",
      "Hausdorﬀmoment problem, 142\n",
      "Hausdorﬀspace, 623, 625\n",
      "hazard function, 577\n",
      "Heaviside function, 738, 863\n",
      "heavy-tailed family, 165\n",
      "Heine-Borel theorem, 645\n",
      "Heine-Cantor theorem, 722\n",
      "Hellinger distance, 747\n",
      "Helly-Bray theorem, 90\n",
      "Helmert matrix, 433\n",
      "Helmert transformation, 187\n",
      "Hermite polynomial, 68, 753\n",
      "Hessian, 450, 658, 809, 871\n",
      "hierarchical Bayes, 351\n",
      "hierarchical Bayesian model, 357, 378\n",
      "higher dimensions, 199, 203\n",
      "highest posterior density credible set,\n",
      "373\n",
      "Hilbert space, 639, 648, 745\n",
      "histospline, 589\n",
      "Hodges’ supereﬃcient estimator, 422\n",
      "Hoeﬀding inequality, 848\n",
      "H¨older norm, 643, 744\n",
      "H¨older’s inequality, 642, 852\n",
      "H¨older-continuous function, 723\n",
      "homogeneous process, 122\n",
      "homomorphism, 631\n",
      "Horowitz’s estimator (proportional\n",
      "hazards), 578\n",
      "Horvitz-Thompson estimator, 441\n",
      "HPD (highest posterior density)\n",
      "credible set, 373\n",
      "hypergeometric series, 682\n",
      "hyperparameter, 330, 335\n",
      "hypothesis testing, 290–296, 362–372,\n",
      "507–560\n",
      "alternative hypothesis, 291\n",
      "asymptotic signiﬁcance, 527\n",
      "Bayesian testing, 362–372\n",
      "composite hypothesis, 291\n",
      "consistency, 315, 527\n",
      "invariant tests, 525–527\n",
      "Lagrange multiplier test, 530\n",
      "likelihood ratio test, 528–530\n",
      "multiple tests, 536–538\n",
      "Neyman-Pearson Lemma, 517\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "897\n",
      "nonparametric tests, 535–536\n",
      "nonrandomized test, 293, 509\n",
      "null hypothesis, 291\n",
      "observed signiﬁcance level, 292\n",
      "p-value, 292\n",
      "randomized test, 293, 509, 513\n",
      "Rao test, 530\n",
      "score test, 530, 533\n",
      "sequential tests, 538–539\n",
      "signiﬁcance level, 292\n",
      "simple hypothesis, 291\n",
      "size of test, 292, 295, 510\n",
      "SPRT, 539\n",
      "test statistic, 292\n",
      "test with random component, 513\n",
      "unbiased test, 296, 523\n",
      "uniform consistency, 315, 527\n",
      "Wald test, 530\n",
      "i.o. (inﬁnitely often), 71\n",
      "convergence, 76\n",
      "IAE (integrated absolute error), 572,\n",
      "575\n",
      "ideal bootstrap, 304\n",
      "idempotent matrix, 795\n",
      "identiﬁability, 12, 21\n",
      "identity matrix, 783\n",
      "iid (“independent and identically\n",
      "distributed”), 24, 857\n",
      "IMAE (integrated mean absolute error),\n",
      "574\n",
      "image of a function, 625, 701\n",
      "importance sampling, 684\n",
      "improper integral, 738\n",
      "improper prior, 330, 345\n",
      "IMSE (integrated mean squared error),\n",
      "573\n",
      "inclusion-exclusion formula (“disjointiﬁ-\n",
      "cation”), 619, 706\n",
      "incomplete beta function, 866\n",
      "incomplete gamma function, 866\n",
      "independence, 5, 23, 74, 110, 120\n",
      "independence of normal random\n",
      "variables, 185\n",
      "index of stability, 62\n",
      "indicator function, 719, 863\n",
      "induced likelihood, 458\n",
      "induced measure, 4, 712\n",
      "induction (method of proof), 675\n",
      "inductive probability, 138\n",
      "inequalities, 845\n",
      "inﬁmum, 644\n",
      "essential, 745\n",
      "inﬁnite divisibility, 61\n",
      "inﬁnitely divisible family, 183\n",
      "inﬁnitely often, 71\n",
      "convergence, 76\n",
      "inﬂuence function, 606–607\n",
      "information, 40, 42, 229–235, 399, 850\n",
      "information inequality, 234, 399, 421,\n",
      "851, 854\n",
      "information theory, 253\n",
      "inner product, 636, 743, 744, 781\n",
      "inner product space, 636\n",
      "integrable function, 729\n",
      "integrable random variable, 26\n",
      "integral, 727–738\n",
      "double, 735\n",
      "iterated, 735\n",
      "integrated expectation\n",
      "integrated absolute bias, 573\n",
      "integrated absolute error (IAE), 572,\n",
      "575\n",
      "integrated bias, 573\n",
      "integrated mean absolute error\n",
      "(IMAE), 574\n",
      "integrated mean squared error\n",
      "(IMSE), 573\n",
      "integrated squared bias, 573\n",
      "integrated squared error (ISE), 572\n",
      "integrated variance, 573\n",
      "integration, 726–738\n",
      "integration by parts, 735\n",
      "interior, 622, 645\n",
      "interior point, 645\n",
      "interquantile range, 53\n",
      "interquartile range, 53\n",
      "intersection of sets, 616\n",
      "invariance, 220, 266, 279–289\n",
      "Haar, 729\n",
      "invariant family, 182\n",
      "invariant function, 281, 755\n",
      "invariant tests, 525–527\n",
      "inverse CDF, 15\n",
      "inverse CDF method, 663\n",
      "inverse cumulative distribution function,\n",
      "15\n",
      "inverse function, 625\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "898\n",
      "Index\n",
      "inverse image, 626, 702\n",
      "inverse of a matrix, 784, 797\n",
      "inverse of a partitioned matrix, 799\n",
      "inverse probability, 211, 317, 380\n",
      "inversion theorem, 48\n",
      "IRLS (iteratively reweighted least\n",
      "squares), 496\n",
      "irreducible Markov chain, 129\n",
      "irreducible matrix, 818\n",
      "ISE (integrated squared error), 572\n",
      "isomorphism, 631\n",
      "iterated integral, 735\n",
      "iterated logarithm, law of, 104\n",
      "iteratively reweighted least squares\n",
      "(IRLS), 496\n",
      "Ito’s formula, 778\n",
      "jackknife, 301\n",
      "bias reduction, 414\n",
      "delete d, 302\n",
      "higher order, 415\n",
      "variance estimation, 301–303\n",
      "Jacobian, 56, 808\n",
      "James-Stein estimator, 272\n",
      "Jeﬀreys’s noninformative prior, 350, 357\n",
      "Jensen’s inequality, 849, 853\n",
      "joint entropy, 42\n",
      "jump process, 765\n",
      "jump-diﬀusion process, 774\n",
      "kernel (function), 591\n",
      "kernel (in a convolution), 742\n",
      "kernel density estimation, 590\n",
      "kernel in a PDF, 19, 164\n",
      "kernel method, 568\n",
      "kernel of U-statistic, 406\n",
      "Kolmogorov distance, 536, 572, 574,\n",
      "598, 746\n",
      "Kolmogorov’s extension theorem, 125\n",
      "Kolmogorov’s inequality, 133, 849\n",
      "Kolmogorov’s zero-one law, 72\n",
      "Kolmogorov-Smirnov test, 536\n",
      "Kronecker multiplication, 792\n",
      "Kronecker’s lemma, 654\n",
      "KS test (Kolmogorov-Smirnov), 536\n",
      "Kshirsagar inequality, 854\n",
      "Kullback-Leibler information, 850\n",
      "Kullback-Leibler measure, 253, 747\n",
      "Kumaraswamy distribution, 235\n",
      "L1 consistency, 575\n",
      "L2 consistency, 574\n",
      "L2 norm, 744\n",
      "L2 space, 744\n",
      "Lp metric, 643, 746\n",
      "Lp norm, 641, 744, 781\n",
      "of a vector, 641, 781, 803\n",
      "Lp space, 862\n",
      "Lp space, 35, 741, 745\n",
      "LJ functional, 53\n",
      "L-invariance, 266, 281\n",
      "L-unbiasedness, 265, 523, 524\n",
      "Lagrange multiplier test, 530\n",
      "lambda family of distributions, 197\n",
      "λ-system, 693\n",
      "Landau distribution, 185\n",
      "Laplacian, 659\n",
      "Laplacian operator, 872\n",
      "LAV (least absolute values) estimation,\n",
      "259\n",
      "law of large numbers, 102\n",
      "law of the iterated logarithm, 104\n",
      "Le Cam regularity conditions, 169, 481\n",
      "least absolute values (LAV) estimation,\n",
      "259\n",
      "least favorable prior distribution, 372\n",
      "least squares, 115, 424–438\n",
      "least squares (LS) estimation, 259, 424\n",
      "Lebesgue integral, 727–735\n",
      "Lebesgue measure, 717\n",
      "Lebesgue monotone convergence\n",
      "theorem, 733\n",
      "Lebesgue σ-ﬁeld, 717\n",
      "Lebesgue’s dominated convergence\n",
      "theorem, 734\n",
      "left eigenvector, 789\n",
      "Legendre polynomial, 752\n",
      "Lehmann-Scheﬀ´e theorem, 394\n",
      "level of signiﬁcance, 295\n",
      "L´evy-Cram´er theorem, 87\n",
      "L´evy distance, 599\n",
      "L´evy process, 129–130\n",
      "lexicographic ordering of combinations,\n",
      "408\n",
      "likelihood, 241–245, 445–505\n",
      "induced, 458\n",
      "likelihood equation, 243, 450, 463\n",
      "roots, 450, 481–482\n",
      "likelihood function, 158, 241, 445, 815\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "899\n",
      "equivalence class, 241, 448\n",
      "likelihood principle, 238, 245, 318, 341,\n",
      "445, 447, 448, 459, 539\n",
      "likelihood ratio, 158, 167, 244, 517, 528\n",
      "likelihood ratio martingale, 132\n",
      "likelihood ratio test, 528–530\n",
      "lim inf, 70, 72, 626–629, 650–651, 725\n",
      "sequence of functions, 725\n",
      "sequence of points, 650–651\n",
      "sequence of probabilities, 70\n",
      "sequence of random variables, 72\n",
      "sequence of sets, 626–629\n",
      "lim sup, 70, 72, 626–629, 650–651, 725\n",
      "sequence of functions, 725\n",
      "sequence of points, 650–651\n",
      "sequence of probabilities, 70\n",
      "sequence of random variables, 72\n",
      "sequence of sets, 626–629\n",
      "limit point, 623, 648\n",
      "limiting Bayes action, 346\n",
      "limiting bias, 311\n",
      "and consistency, 312\n",
      "limiting conﬁdence coeﬃcient, 315\n",
      "limiting expectation, 100, 310\n",
      "limiting mean squared error, 313\n",
      "limiting size of test, 314, 527\n",
      "limiting variance, 313\n",
      "Lindeberg’s central limit theorem, 107\n",
      "Lindeberg’s condition, 106, 107\n",
      "Lindley-Jeﬀrey paradox, 371\n",
      "linear algebra, 781–821\n",
      "linear combination, 635\n",
      "linear independence, 635, 783\n",
      "aﬃne independence, 636\n",
      "linear manifold, 635, 636\n",
      "linear model, 213, 423–438, 531\n",
      "linear ordering, 620\n",
      "linear space, 634–640, 740–754, 781\n",
      "linear transform, 686, 756\n",
      "linex loss function, 262\n",
      "link function, 492\n",
      "Lipschitz constant, 723\n",
      "Lipschitz-continuous function, 584, 723\n",
      "little o, 83, 652\n",
      "little o in probability, 83\n",
      "Littlewood’s principles, 761\n",
      "LMVUE (locally minimum variance\n",
      "unbiased estimator), 393\n",
      "local absolute continuity, 722\n",
      "local uniform convergence, 726\n",
      "locally minimum variance unbiased\n",
      "estimator (LMVUE), 393\n",
      "location equivariance, 285\n",
      "location-scale equivariance, 289\n",
      "location-scale family, 179, 280, 289\n",
      "log-likelihood function, 241, 815\n",
      "logconcave family, 165\n",
      "loss function, 260–263\n",
      "absolute-error, 262\n",
      "α0-α1 (weighted 0-1), 263, 365\n",
      "convex, 261, 264, 267, 269\n",
      "linex, 262\n",
      "randomized decision rule, 260\n",
      "squared-error, 262, 269, 270, 287, 357,\n",
      "393, 523\n",
      "Stein’s loss, 288\n",
      "0-1, 262, 365\n",
      "0-1-γ loss, 262, 364\n",
      "lower conﬁdence bound, 298\n",
      "lower conﬁdence interval, 298\n",
      "LS (least squares) estimation, 259, 424\n",
      "LSE, 424\n",
      "Lyapunov’s condition, 106\n",
      "Lyapunov’s inequality, 853\n",
      "Mρ functional, 53\n",
      "MAE (mean absolute error), 571\n",
      "Mallows distance, 599\n",
      "manifold\n",
      "linear, 635, 636\n",
      "Mann-Whitney statistic, 411\n",
      "MAP estimator, 345\n",
      "Marcinkiewicz-Zygmund inequality, 855\n",
      "Markov chain, 126–129, 666\n",
      "Markov chain Monte Carlo (MCMC),\n",
      "377–380\n",
      "Markov property, 122\n",
      "Markov’s inequality, 847\n",
      "martingale, 130–134\n",
      "de Moivre, 152\n",
      "martingale estimating function, 257\n",
      "martingale transform, 152\n",
      "mathematical induction, 675\n",
      "matrix, 782–811\n",
      "matrix derivative, 801\n",
      "matrix gradient, 808\n",
      "matrix norm, 795\n",
      "Matusita distance, 747\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "900\n",
      "Index\n",
      "maximal invariant, 755\n",
      "maximum a posterior probability\n",
      "(MAP) estimator, 345\n",
      "maximum absolute error (SAE), 572\n",
      "maximum entropy, 254\n",
      "maximum entropy principle, 351\n",
      "maximum likelihood estimation,\n",
      "242–244, 448–502\n",
      "maximum likelihood method, 445–505,\n",
      "580\n",
      "MCMC (Markov chain Monte Carlo),\n",
      "377–380\n",
      "mean, 31\n",
      "sample, 25, 85, 187, 190\n",
      "mean absolute error (MAE), 571\n",
      "mean functional, 51\n",
      "mean integrated absolute error (MIAE),\n",
      "574, 575\n",
      "mean integrated squared error (MISE),\n",
      "574\n",
      "mean square consistent, 574\n",
      "mean squared error (MSE), 218, 570,\n",
      "573\n",
      "mean squared error, of series expansion,\n",
      "750\n",
      "mean squared prediction error, 236\n",
      "mean sup absolute error (MSAE), 574\n",
      "mean-value parameter, in exponential\n",
      "class, 172\n",
      "mean-value theorem, 680\n",
      "measurable function, 703\n",
      "measurable set, 709\n",
      "measurable space, 700\n",
      "measure, 704\n",
      "Borel, 717\n",
      "complete, 707\n",
      "counting, 708\n",
      "Dirac, 708\n",
      "dominating, 21, 711\n",
      "Haar invariant, 708, 729\n",
      "induced, 712\n",
      "Lebesgue, 717\n",
      "probability, 3, 707\n",
      "pushforward, 712\n",
      "Radon, 708\n",
      "singular, 711, 868\n",
      "measure space, 709\n",
      "complete, 709\n",
      "measure theory, 692–762\n",
      "median-unbiasedness, 218, 259\n",
      "method of moments, 247, 272, 416\n",
      "metric, 623, 641, 781\n",
      "in IRd, 641\n",
      "in a function space, 746\n",
      "metric space, 624\n",
      "Metropolis algorithm, 666\n",
      "Metropolis-Hastings algorithm, 668\n",
      "MGF (moment-generating function), 43\n",
      "MIAE (mean integrated absolute error),\n",
      "574, 575\n",
      "minimal complete, 265\n",
      "minimal suﬃciency, 224, 226\n",
      "minimaxity, 268, 274–276, 353, 398\n",
      "Bayes rule, 353\n",
      "minimum risk equivariance (MRE), 267\n",
      "minimum risk equivariant estimation\n",
      "(MREE), 285–289\n",
      "Minkowski distance, 643\n",
      "Minkowski norm, 642, 744\n",
      "Minkowski’s inequality, 642, 854\n",
      "MISE (mean integrated squared error),\n",
      "574\n",
      "mixture distribution, 22, 194, 601–605,\n",
      "754, 836\n",
      "MLE (maximum likelihood estimator),\n",
      "242–244, 448–502\n",
      "MME, 247, 272, 416\n",
      "mode, 18, 165\n",
      "model\n",
      "algorithm, 214\n",
      "equation, 213\n",
      "moment, 26, 30, 52\n",
      "uniqueness of, 32\n",
      "moment problem, 142\n",
      "moment-equivalent distribution, 33\n",
      "moment-generating function (MGF), 43\n",
      "moment-indeterminant distribution, 33\n",
      "moments, method of, 247, 272, 416\n",
      "monotone convergence theorem, 89,\n",
      "649, 733\n",
      "conditional, 112\n",
      "monotone likelihood ratio, 165, 167,\n",
      "245, 520\n",
      "exponential class, 177\n",
      "Monte Carlo, 377–380\n",
      "Moore-Penrose inverse, 425, 429, 784,\n",
      "801\n",
      "morphism, 631\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "901\n",
      "MRE (minimum risk equivariance), 267\n",
      "MREE (minimum risk equivariant\n",
      "estimation), 285–289\n",
      "MRIE (minimum risk invariant\n",
      "estimation), 285\n",
      "MSAE (mean sup absolute error), 574\n",
      "MSE (mean squared error), 218, 570,\n",
      "573\n",
      "MSPE (mean squared prediction error),\n",
      "236\n",
      "multiple tests, 536–538\n",
      "multivariate central limit theorem, 107\n",
      "n-divisibility, 60\n",
      "natural exponential family, 173, 175,\n",
      "200\n",
      "natural parameter space, 173\n",
      "negligible set, 710, 711\n",
      "neighborhood, 623, 624\n",
      "Newton’s method, 812, 825\n",
      "Neyman structure, 520, 525\n",
      "Neyman-Pearson Lemma, 517\n",
      "Neyman-Scott problem, 490\n",
      "no-data problem, 205, 330\n",
      "nondegenerate random variable, 9\n",
      "nonexistence of optimal statistical\n",
      "methods, 277\n",
      "noninformative prior, 350\n",
      "nonnegative deﬁnite matrix, 784, 790\n",
      "nonparametric family, 12, 159\n",
      "nonparametric inference, 215, 246,\n",
      "499–502, 561–563\n",
      "function estimation, 565–597\n",
      "likelihood methods, 499\n",
      "test, 535–536\n",
      "nonparametric probability density\n",
      "estimation, 579–597\n",
      "nonparametric test, 535–536\n",
      "nonrandomized test, 293, 509\n",
      "norm, 637, 641, 781, 846\n",
      "Euclidean, 643, 782\n",
      "Frobenius, 782, 795\n",
      "in IRd, 641\n",
      "Lp, 781\n",
      "of a function, 744\n",
      "of a matrix, 782, 795\n",
      "of a vector, 641\n",
      "normal distribution, characterizations\n",
      "of, 189\n",
      "normal equations, 251, 256, 438\n",
      "normal function, 746\n",
      "normal integral, 681\n",
      "normal vector, 685, 781\n",
      "nuisance parameter, 223\n",
      "null hypothesis, 291\n",
      "O(·), 83, 652\n",
      "o(·), 83, 652\n",
      "OP(·), 83\n",
      "oP(·), 83\n",
      "objective prior, 350–351\n",
      "observed signiﬁcance level, 292\n",
      "octile skewness, 53\n",
      "one-parameter exponential family, 173,\n",
      "271\n",
      "one-sided conﬁdence interval, 298\n",
      "one-step MLE, 467\n",
      "one-to-one function, 625\n",
      "one-way AOV model, 434–436, 488–490\n",
      "open cover, 622\n",
      "open set, 622, 624, 713\n",
      "optimization, 687, 822–832\n",
      "optimization of vector/matrix functions,\n",
      "811\n",
      "orbit, 755\n",
      "order of a ﬁeld, 633\n",
      "order of kernel or U statistic, 404\n",
      "order statistic, 62–65, 96–99, 108–109,\n",
      "222, 252, 409, 563–564\n",
      "asymptotic distribution, 96–99\n",
      "ordered ﬁeld, 634\n",
      "ordered set, 620, 644\n",
      "ordering, 620\n",
      "linear, 620\n",
      "total, 620\n",
      "well, 620\n",
      "Ornstein-Ulenbeck process, 774\n",
      "orthogonal matrix, 784\n",
      "orthogonal polynomials, 568, 751–754\n",
      "orthogonality, 637\n",
      "orthogonalizing vectors, 685\n",
      "orthogonally diagonalizable, 786\n",
      "orthogonally similar, 786\n",
      "orthonormal vectors, 685, 781\n",
      "outer measure, 705\n",
      "outlier-generating distribution, 166\n",
      "over-dispersion, 498\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "902\n",
      "Index\n",
      "Pp distribution function space, 754\n",
      "p-value, 292, 512\n",
      "parameter space, 12, 159, 168\n",
      "natural, 173\n",
      "parametric family, 12, 159, 235\n",
      "parametric inference, 215\n",
      "parametric-support family, 177, 228,\n",
      "499\n",
      "Pareto tail, 564\n",
      "Pareto-type distribution, 184\n",
      "Parseval’s theorem, 759\n",
      "partial correlation, 118\n",
      "partial likelihood, 501, 578\n",
      "partition function in a PDF, 19, 172,\n",
      "337\n",
      "partition of a set, 618\n",
      "PCER (per comparison error rate), 537\n",
      "PDF (probability density function), 17\n",
      "estimation of, 579–597\n",
      "PDF decomposition, 568, 684\n",
      "Pearson chi-squared discrepancy\n",
      "measure, 253, 748\n",
      "Pearson family of distributions, 197\n",
      "penalized maximum likelihood method,\n",
      "581\n",
      "per comparison error rate (PCER), 537\n",
      "permutation test, 535\n",
      "Perron root, 819\n",
      "Perron vector, 128, 819\n",
      "Perron-Frobenius theorem, 819\n",
      "φ-divergence, 253, 747\n",
      "π-λ theorem, 697\n",
      "π-system, 693\n",
      "Pitman admissible, 274\n",
      "Pitman closeness, 219, 221, 274, 381\n",
      "Pitman estimator, 287, 288\n",
      "pivotal function, 297, 544\n",
      "asymptotic, 551\n",
      "PlanetMath, 613\n",
      "plug-in estimator, 246, 416, 418, 602\n",
      "point estimation, 217, 285–289, 389–444,\n",
      "448–487\n",
      "pointwise convergence, 571–572, 725\n",
      "pointwise properties, 569\n",
      "Poisson process, 129, 765\n",
      "Poisson series, 682\n",
      "polar coordinates, 679\n",
      "Polya’s theorem, 85\n",
      "Polya’s urn process, 24, 131\n",
      "polygamma function, 865\n",
      "polynomial tail, 564\n",
      "“portmanteau” theorem, 86\n",
      "poset, 620\n",
      "positive deﬁnite matrix, 784, 790–792\n",
      "positive stable, 791\n",
      "posterior distribution, 330, 336\n",
      "posterior Pitman closeness, 381\n",
      "posterior predictive distribution, 336\n",
      "power divergence measure, 253\n",
      "power function, 294, 314, 513\n",
      "power law, 164\n",
      "power of test, 294, 513\n",
      "power series expansion, 67\n",
      "power set, 618, 696, 715\n",
      "prediction, 115–118, 216, 236\n",
      "prediction set, 300, 543\n",
      "predictive distribution\n",
      "posterior, 336\n",
      "prior, 336\n",
      "preimage, 626, 702\n",
      "primitive matrix, 820\n",
      "principal minor, 870\n",
      "principle, 239, 318\n",
      "bootstrap, 249\n",
      "conditionality, 318\n",
      "equivariance, 279\n",
      "likelihood, 238, 245, 318, 341, 447\n",
      "maximum entropy, 351\n",
      "substitution, 247\n",
      "suﬃciency, 223, 318\n",
      "prior distribution, 330, 335\n",
      "conjugate prior, 327, 337, 346\n",
      "elicitation, 347\n",
      "empirical, 351\n",
      "hierarchical, 351\n",
      "improper prior, 330, 345\n",
      "Jeﬀreys’s noninformative prior, 350\n",
      "least favorable, 372\n",
      "noninformative prior, 350\n",
      "objective prior, 350–351\n",
      "reference prior, 351\n",
      "prior predictive distribution, 336\n",
      "probability, 1–153, 155–204\n",
      "alternative ways of developing the\n",
      "measure, 138\n",
      "inductive, 138\n",
      "statistical, 138\n",
      "subjective, 138\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "903\n",
      "probability density function (PDF), 17\n",
      "estimation of, 579–597\n",
      "probability function, 18\n",
      "probability mass function, 18\n",
      "probability measure, 3, 707\n",
      "probability of an event, 4, 729\n",
      "probability space, 3, 709\n",
      "probability-generating function, 44\n",
      "probit model, 492\n",
      "product measure, 713\n",
      "product set, 617\n",
      "proﬁle likelihood, 242, 499\n",
      "projection matrix, 438, 795\n",
      "projection of a random variable,\n",
      "115–118, 438\n",
      "U-statistic, 413\n",
      "projection of a vector onto a linear\n",
      "space, 637\n",
      "proper diﬀerence, 617\n",
      "proper subset, 621\n",
      "proportional hazards, 578\n",
      "pseudoinverse, 429, 784, 801\n",
      "pseudometric, 639, 746\n",
      "pseudonorm, 638\n",
      "pseudovalue, 302\n",
      "psi function, 865\n",
      "pushforward measure, 712\n",
      "“Pythagorean Theorem” of statistics,\n",
      "685\n",
      "quadratic form, 430–432, 784\n",
      "quadratic mean diﬀerentiable family,\n",
      "169\n",
      "quantile, 11, 52, 64, 96\n",
      "conﬁdence interval, 544\n",
      "estimation, 418\n",
      "functional, 404, 605\n",
      "in forming conﬁdence sets, 298\n",
      "quantile function, 15, 28, 52\n",
      "quartile skewness, 53\n",
      "quasi-likelihood, 498, 499\n",
      "quasi-Newton method, 814, 827\n",
      "Radon measure, 708\n",
      "Radon-Nikodym derivative, 739\n",
      "Radon-Nikodym theorem, 739\n",
      "random sample, 24, 333\n",
      "simple, 24\n",
      "random variable, 8–25\n",
      "space, 35\n",
      "random-eﬀects AOV model, 436, 490\n",
      "randomized decision rule, 260\n",
      "conﬁdence set, 544, 545\n",
      "loss function, 260\n",
      "point estimator, 276, 420\n",
      "test, 293, 509, 513\n",
      "rank of a matrix, 783\n",
      "rank statistic, 536, 609\n",
      "rank test, 536\n",
      "Rao test, 530\n",
      "Rao-Blackwell inequality, 855\n",
      "Rao-Blackwell theorem, 264, 267\n",
      "Rao-Blackwellization, 267\n",
      "rational numbers, 632, 634, 641, 648\n",
      "Dirichlet function, 721\n",
      "measure, 717\n",
      "Thomae function, 721\n",
      "raw moment, 30\n",
      "Rayleigh quotient, 788\n",
      "real numbers, IR, 640–660\n",
      "extended reals, IR, 640\n",
      "recursion formula for orthogonal\n",
      "polynomials, 751\n",
      "reducibility, 818\n",
      "reference noninformative prior, 351\n",
      "regression, 538\n",
      "regression model, 213\n",
      "regular family, 168\n",
      "regularity conditions, 168\n",
      "Fisher information, 168, 229, 399, 457\n",
      "Le Cam, 169, 481\n",
      "Walker, 334\n",
      "regularization of ﬁts, 252, 428\n",
      "regularized incomplete beta function,\n",
      "866\n",
      "regularized incomplete gamma function,\n",
      "866\n",
      "rejection region, 292\n",
      "relation, 625\n",
      "relative eﬃciency, 313, 419\n",
      "REML, 489\n",
      "resampling, 248, 249\n",
      "resampling vector, 249\n",
      "residual, 258\n",
      "restricted Bayes, 268\n",
      "restricted maximum likelihood method,\n",
      "580\n",
      "restricted measure, 709\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "904\n",
      "Index\n",
      "ρ-Fr´echet derivative, 760\n",
      "ρ-Hadamard derivative, 760\n",
      "ridge regression, 428\n",
      "Riemann integral, 735\n",
      "Riemann-Stieltjes integral, 736\n",
      "Riesz-Fischer theorem, 741\n",
      "right direct product, 793\n",
      "right stochastic matrix, 818\n",
      "ring, 632\n",
      "ring of sets, 693\n",
      "risk\n",
      "Bayes, 328\n",
      "frequentist, 328\n",
      "risk function, 263\n",
      "RLE (root of likelihood equation, which\n",
      "also see), 450\n",
      "robust statistics, 602–609\n",
      "Bayesian robustness, 348\n",
      "Rolle’s theorem, 680\n",
      "root of likelihood equation, 450, 481–482\n",
      "roughness of a function, 576, 586\n",
      "SAE (sup absolute error), 572\n",
      "sample continuous, 126\n",
      "sample covariance\n",
      "as U-statistic, 407\n",
      "sample mean, 25, 85, 187, 190\n",
      "sample quantile, 64, 97, 258\n",
      "sample space, 3, 692\n",
      "sample variance, 25, 187, 190, 243, 248,\n",
      "460\n",
      "as U-statistic, 407\n",
      "relation to V-statistic, 417\n",
      "sampling design, 441\n",
      "sampling from ﬁnite populations, 305,\n",
      "382\n",
      "sandwich estimator, 304, 316\n",
      "scale equivariance, 287\n",
      "Scheﬀ´e’s method for simultaneous\n",
      "conﬁdence intervals, 558\n",
      "Schur complement, 799\n",
      "Schwarz inequality, 853\n",
      "score function, 244, 255, 463–464, 481,\n",
      "530\n",
      "score test, 530, 533\n",
      "scoring, 244\n",
      "SDE (stochastic diﬀerential equation),\n",
      "765\n",
      "second characteristic function, 50\n",
      "second order delta method, 94, 481\n",
      "self-information, 40\n",
      "seminorm, 638\n",
      "semiparametric inference, 499–502\n",
      "separable set, 622, 641\n",
      "sequences of real numbers, 648–655\n",
      "sequential probability ratio test (SPRT),\n",
      "539\n",
      "sequential test, 538–539\n",
      "series, 654\n",
      "convergence, 677\n",
      "series estimator, 568\n",
      "series expansion, 65–69, 679, 749, 805\n",
      "set, 621\n",
      "Severini-Egorov theorem, 726\n",
      "Shannon entropy, 41\n",
      "Shannon information, 229\n",
      "shrinkage of estimators, 220, 273, 319,\n",
      "597\n",
      "Sierpinski system, 694\n",
      "Sierpinski’s π-λ theorem, 698\n",
      "σ-algebra, 694\n",
      "σ-ﬁeld, 694\n",
      "ﬁltration, 125\n",
      "generated by a collection of sets, 695\n",
      "generated by a measurable function,\n",
      "704\n",
      "generated by a random variable, 10\n",
      "σ-ﬁnite measure, 707\n",
      "σ-lattice, 695\n",
      "σ-ring, 694\n",
      "sign test, 536\n",
      "signal to noise ratio, 214\n",
      "signed measure, 704\n",
      "signiﬁcance level, 292\n",
      "asymptotic, 314, 315, 527\n",
      "signiﬁcance test, 292\n",
      "similar region, 519\n",
      "similar test, 524, 525\n",
      "simple function, 719, 720\n",
      "simple hypothesis, 291\n",
      "simple random sample, 24\n",
      "simple random variable, 10\n",
      "simulated annealing, 378, 829\n",
      "simultaneous conﬁdence sets, 557–558\n",
      "singular distribution, 176\n",
      "singular measure, 711, 718, 868\n",
      "singular value factorization, 794\n",
      "size of test, 292, 295, 510\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "905\n",
      "limiting, 314, 527\n",
      "skewed distribution\n",
      "CDF-skewing, 195\n",
      "diﬀerential scaling, 195\n",
      "skewness coeﬃcient, 31\n",
      "Sklar’s theorem, 39\n",
      "Skorokhod space and metric, 144\n",
      "Skorokhod’s theorem, 86, 87, 89\n",
      "SLLN (strong law of large numbers),\n",
      "103\n",
      "slowly varing function, 166\n",
      "Slutsky’s theorem, 91\n",
      "smallest subset, 617\n",
      "Smith-Volterra-Cantor set, 715, 718\n",
      "measure, 718\n",
      "smoothing matrix, 591\n",
      "space, 621\n",
      "spectral decomposition, 787, 795\n",
      "spectral projector, 787\n",
      "spectrum of a measure, 710\n",
      "spherical family, 198\n",
      "SPRT (sequential probability ratio\n",
      "test), 539\n",
      "square root matrix, 791\n",
      "squared-error loss, 262, 269, 270, 287,\n",
      "357, 393, 523\n",
      "stable family, 183\n",
      "stable random variable, 61\n",
      "standard deviation, 31\n",
      "standard normal distribution, 835\n",
      "state space, 122\n",
      "stationary point of vector/matrix\n",
      "functions, 812\n",
      "stationary process, 124\n",
      "statistic, 212\n",
      "statistical function, 51–54, 217, 246,\n",
      "389, 602\n",
      "degree, 392, 405\n",
      "estimable, 391\n",
      "statistical probability, 138\n",
      "steepest descent, 812, 814\n",
      "Stein shrinkage, 273\n",
      "Stein’s loss function, 288\n",
      "Stieltjes moment problem, 142\n",
      "stochastic diﬀerential, 766, 771, 775\n",
      "stochastic diﬀerential equation (SDE),\n",
      "765\n",
      "stochastic integration, 765–780\n",
      "stochastic matrix, 818\n",
      "stochastic process, 121–137, 765–772\n",
      "stochastic vector, 127\n",
      "stopping time, 122, 539\n",
      "strictly stationary process, 124\n",
      "strong law of large numbers, 103, 104\n",
      "strongly unimodal family, 165\n",
      "sub measurable space, 700, 709\n",
      "sub measure space, 709\n",
      "sub-σ-ﬁeld, 699\n",
      "subexponential family, 166\n",
      "subharmonic function, 659\n",
      "subjective inference, 207, 210, 317, 327\n",
      "subjective probability, 138, 207, 317\n",
      "submartingale, 130\n",
      "subset, 621\n",
      "substitution principle, 246, 247\n",
      "suﬃciency, 222\n",
      "in Bayesian inference, 343\n",
      "suﬃciency principle, 223, 318\n",
      "sup absolute error (SAE), 572\n",
      "supereﬃciency, 421\n",
      "supermartingale, 130\n",
      "superpopulation model, 306\n",
      "support of a distribution, 12, 168\n",
      "support of a measure, 710\n",
      "support of an hypothesis, 245\n",
      "supremum, 644\n",
      "essential, 745\n",
      "surjective function, 701\n",
      "survey sampling, 305, 438–442\n",
      "symmetric diﬀerence, 617\n",
      "symmetric family, 164\n",
      "symmetric matrix, 785–792\n",
      "symmetric statistic, 212\n",
      "symmetric storage mode, 792\n",
      "tail σ-ﬁeld, 72\n",
      "tail CDF, 14, 166\n",
      "tail event, 72\n",
      "tail index, 564\n",
      "Taylor series, 656, 679, 741, 805\n",
      "Taylor’s theorem, 656\n",
      "tensor product, 753\n",
      "tessellation, 589\n",
      "test statistic, 292, 510\n",
      "testing hypotheses, 290–296, 362–372,\n",
      "507–560\n",
      "alternative hypothesis, 291\n",
      "asymptotic signiﬁcance, 527\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "906\n",
      "Index\n",
      "Bayesian testing, 362–372\n",
      "composite hypothesis, 291\n",
      "consistency, 315, 527\n",
      "invariant tests, 525–527\n",
      "Lagrange multiplier test, 530\n",
      "likelihood ratio test, 528–530\n",
      "multiple tests, 536–538\n",
      "Neyman-Pearson Lemma, 517\n",
      "nonparametric tests, 535–536\n",
      "nonrandomized test, 509\n",
      "null hypothesis, 291\n",
      "observed signiﬁcance level, 292\n",
      "p-value, 292\n",
      "randomized test, 509, 513\n",
      "Rao test, 530\n",
      "score test, 530, 533\n",
      "sequential tests, 538–539\n",
      "signiﬁcance level, 292\n",
      "simple hypothesis, 291\n",
      "size of test, 292, 295, 510\n",
      "SPRT, 539\n",
      "test statistic, 292\n",
      "unbiased test, 296, 523\n",
      "uniform consistency, 315, 527\n",
      "Wald test, 530\n",
      "Thomae function, 721\n",
      "tightness, 88\n",
      "tolerance set, 300, 543\n",
      "topological space, 621\n",
      "topological support of a measure, 710\n",
      "topology, 621\n",
      "total ordering, 620\n",
      "total variation, 745\n",
      "totally positive family, 168\n",
      "tr(·), 785\n",
      "trace of a matrix, 785\n",
      "trajectory, 125\n",
      "transform, 686, 756–759\n",
      "discrete, 686\n",
      "transformation group, 630, 754\n",
      "transition matrix, 127, 818–821\n",
      "transitive transformation group, 755\n",
      "triangle inequality, 637, 854\n",
      "triangular array, 61, 107\n",
      "trigamma function, 466, 865\n",
      "truncated distribution, 192, 203\n",
      "Tukey’s method for simultaneous\n",
      "conﬁdence intervals, 558\n",
      "two-sample Wilcoxon statistic, 411\n",
      "type I error, 294\n",
      "U-estimability, 391, 426\n",
      "U-statistic, 404–414\n",
      "UMAU (uniformly most accurate\n",
      "unbiased) conﬁdence set, 547\n",
      "UMVUE (uniformly minimum variance\n",
      "unbiased estimation), 392–403\n",
      "unbiased conﬁdence set, 300\n",
      "unbiased estimating function, 255\n",
      "unbiased estimator, 218\n",
      "unbiased point estimation, 389–444\n",
      "unbiased test, 296, 523\n",
      "unbiasedness, 218, 255, 265, 267, 296,\n",
      "390, 523\n",
      "and squared-error loss; UMVU, 393\n",
      "estimability, 426\n",
      "estimating function, 255\n",
      "L-unbiasedness, 265, 523, 524\n",
      "median-unbiasedness, 218, 259\n",
      "test, 296, 523\n",
      "unbiasedness of conﬁdence set, 547\n",
      "uniform consistency\n",
      "of tests, 315, 527\n",
      "uniform convergence, 725\n",
      "uniform norm, 745\n",
      "uniform property, 221, 266, 296, 300\n",
      "uniformly continuous function, 721\n",
      "uniformly minimum variance unbiased\n",
      "estimation (UMVUE), 392–403\n",
      "uniformly most accurate unbiased\n",
      "(UMAU) conﬁdence set, 547\n",
      "uniformly most powerful test, 520\n",
      "unimodal family, 165\n",
      "union of sets, 616\n",
      "universal set, 616\n",
      "upper conﬁdence bound, 298\n",
      "upper conﬁdence interval, 298\n",
      "urn process, 1, 7, 24, 131\n",
      "utility, 259\n",
      "V(·), 31, 817\n",
      "V-statistic, 417–418\n",
      "variable metric method, 814\n",
      "variable selection, 538\n",
      "variance, 31, 39\n",
      "asymptotic, 313\n",
      "bound, 235, 399, 421\n",
      "estimation, 301–304, 310, 317, 460\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n",
      "Index\n",
      "907\n",
      "bootstrap, 304\n",
      "jackknife, 301–303\n",
      "limiting, 313\n",
      "sample, 25, 187, 190, 460\n",
      "variance bound, 235, 399, 421\n",
      "variance stabilizing transformation, 95\n",
      "variance-covariance matrix\n",
      "of a random matrix, 38\n",
      "of a random vector, 37\n",
      "Vasicek process, 774\n",
      "vec(·), 792\n",
      "vech(·), 792\n",
      "vecsy(·), 792\n",
      "vector, 635, 782–811\n",
      "vector derivative, 801\n",
      "vector measure, 704\n",
      "vector space, 634–635\n",
      "Vitali set, 718\n",
      "Wald test, 530\n",
      "Walker regularity conditions, 334\n",
      "Wasserstein-Mallows distance, 599\n",
      "weak convergence, 78\n",
      "weak convergence in mean square, 571\n",
      "weak convergence in quadratic mean,\n",
      "571\n",
      "weak law of large numbers, 103\n",
      "weakly stationary process, 124\n",
      "Weierstrass function, 724\n",
      "well-ordered set, 620, 644\n",
      "well-ordering, 620\n",
      "white noise, 123\n",
      "Wiener-Khinchin theorem, 759\n",
      "Wilcoxon statistic, 411, 563\n",
      "window size, 591\n",
      "WLLN (weak law of large numbers),\n",
      "103\n",
      "wlog (“without loss of generality”), 857\n",
      "Woodruﬀ’s interval, 551\n",
      "wrt (“with respect to”), 857\n",
      "00, 860\n",
      "0-1 loss, 262, 365\n",
      "0-1-γ loss, 262, 364\n",
      "zero-one law, Kolmogorov’s, 72\n",
      "Theory of Statistics c⃝2000–2020 James E. Gentle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path1 = \"../../data/mcelreath_2020_statistical-rethinking.pdf\"\n",
    "pdf_path2 = \"../../data/Theory of Statistic.pdf\"\n",
    "pdf_path3 = \"../../data/Deep Learning with Python.pdf\"\n",
    "pdf_path4 = \"../../data/Natural_Image_Statistics.pdf\"\n",
    "pdf_path5 = \"../../data/mml-book.pdf\"\n",
    "\n",
    "pdf_path = pdf_path2\n",
    "\n",
    "pages_data = extract_page_data_fitz(pdf_path)\n",
    "start_chapter = correct_page_numbers(pages_data, sequence_length=10)\n",
    "text = extract_text(pdf_path, start_chapter)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1e12a",
   "metadata": {},
   "source": [
    "### Set-up ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea0f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/davide/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "def text_chunking(text, max_words=750, overlap_sentences=5, min_words=400):\n",
    "    \"\"\"\n",
    "    Creates text chunks up to max_words using sentences as undivisible units.\n",
    "    Each chunk can overlap with the next one by overlap_sentences.\n",
    "    Chunks smaller than min_words are merged with the next chunk.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(sentences):\n",
    "        chunk_sentences = []\n",
    "        word_count = 0\n",
    "        chunk_start = i\n",
    "        \n",
    "        # Build chunk\n",
    "        while i < len(sentences):\n",
    "            if word_count + word_counts[i] > max_words and chunk_sentences:\n",
    "                break\n",
    "            chunk_sentences.append(sentences[i])\n",
    "            word_count += word_counts[i]\n",
    "            i += 1\n",
    "        \n",
    "        if chunk_sentences:\n",
    "            chunks.append(\" \".join(chunk_sentences))\n",
    "            \n",
    "            # Add overlap for next chunk\n",
    "            if i < len(sentences):\n",
    "                chunk_size = len(chunk_sentences)\n",
    "                overlap = min(overlap_sentences, chunk_size - 1)\n",
    "                i = max(i - overlap, chunk_start + 1)\n",
    "    \n",
    "    # Merge small chunks with next chunk\n",
    "    merged_chunks = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        current_chunk = chunks[i]\n",
    "        current_words = len(current_chunk.split())\n",
    "        \n",
    "        # If current chunk is too small and there's a next chunk, merge them\n",
    "        if current_words < min_words and i + 1 < len(chunks):\n",
    "            next_chunk = chunks[i + 1]\n",
    "            next_words = len(next_chunk.split())\n",
    "            \n",
    "            # Only merge if combined size won't be too large\n",
    "            if current_words + next_words <= max_words:\n",
    "                merged_chunk = current_chunk + \" \" + next_chunk\n",
    "                merged_chunks.append(merged_chunk)\n",
    "                i += 2  # Skip next chunk since we merged it\n",
    "            else:\n",
    "                # Keep small chunk as-is if merging would be too large\n",
    "                merged_chunks.append(current_chunk)\n",
    "                i += 1\n",
    "        else:\n",
    "            merged_chunks.append(current_chunk)\n",
    "            i += 1\n",
    "    \n",
    "    # Remove chunks that are too long (likely data blocks or malformed content)\n",
    "    final_chunks = []\n",
    "    for chunk in merged_chunks:\n",
    "        if len(chunk.split()) <= 1000:\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b929671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chunks = text_chunking(text, max_words=750, overlap_sentences=5, min_words=400)\n",
    "\n",
    "n_words = []\n",
    "for c in chunks:\n",
    "    words = c.split()\n",
    "    n_words.append(len(words))\n",
    "\n",
    "plt.hist(n_words, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fecc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "\n",
    "def initialize_chromadb(EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Initialize ChromaDB client and embedding function.\n",
    "    \"\"\"\n",
    "    # Create a ephemeral directory for storing the database\n",
    "    client = chromadb.Client()\n",
    "\n",
    "    # Initialize an embedding function (using a Sentence Transformer model)\n",
    "    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=EMBEDDING_MODEL\n",
    "    )\n",
    "\n",
    "    return client, embedding_func\n",
    "\n",
    "\n",
    "def initialize_collection(client, embedding_func, collection_name):\n",
    "    \"\"\"\n",
    "    Initialize a collection in ChromaDB.\n",
    "    \"\"\"\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_func,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "\n",
    "def update_collection(\n",
    "    collection,\n",
    "    text,\n",
    "    max_words=200,\n",
    "    min_words=100,\n",
    "    overlap_sentences=3,\n",
    "):\n",
    "    chunks = text_chunking(text, max_words=max_words, min_words=min_words, overlap_sentences=overlap_sentences)\n",
    "\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        ids=[f\"chunk_{j:04d}\" for j in range(len(chunks))],\n",
    "        metadatas=[{\n",
    "            \"chunk_index\": j,\n",
    "        } for j in range(len(chunks))]\n",
    "    )\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a40458",
   "metadata": {},
   "source": [
    "### Test chapters splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57a8fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../../src\")  \n",
    "from runpod_client import format_messages_as_prompt, run_prompt, clean_and_parse_json\n",
    "import messages_templates\n",
    "import toc_parser \n",
    "\n",
    "importlib.reload(toc_parser)\n",
    "importlib.reload(messages_templates)\n",
    "\n",
    "from toc_parser import extract_chapters_from_toc\n",
    "from messages_templates import get_toc_extraction_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc6e0e1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m toc \u001b[38;5;241m=\u001b[39m get_toc_extraction_messages(text[:\u001b[38;5;241m1000\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m toc_formatted \u001b[38;5;241m=\u001b[39m \u001b[43mformat_messages_as_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/Text2Test/notebooks/dev/../../src/runpod_client.py:25\u001b[0m, in \u001b[0;36mformat_messages_as_prompt\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m     23\u001b[0m parts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[0;32m---> 25\u001b[0m     parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(parts)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "toc = get_toc_extraction_messages(text[:1000])\n",
    "toc_formatted = format_messages_as_prompt(toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6191a608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user\\nYou are a precise document parser that extracts structured information from table of contents. You NEVER hallucinate, invent, or make up information. You ONLY extract what is explicitly present in the provided text. If you cannot find clear chapter information, you return an empty array.\\n\\nI need to extract main chapter information from this table of contents. Only extract numbered chapters, ignore subsections. Do not make up any information.\\n\\nHere is the table of contents:\\n\\n1\\nProbability Theory\\nProbability theory provides the basis for mathematical statistics.\\nProbability theory has two distinct elements. One is just a special case\\nof measure theory and can be approached in that way. For this aspect, the\\npresentation in this chapter assumes familiarity with the material in\\nSection 0.1 beginning on page 692. This aspect is “pure” mathematics. The\\nother aspect of probability theory is essentially built on a gedanken experiment\\ninvolving drawing balls from an urn that contains balls of diﬀerent colors, and\\nnoting the colors of the balls drawn. In this aspect of probability theory, we\\nmay treat “probability” as a primitive (that is, undeﬁned) concept. In this line\\nof development, we relate “probability” informally to some notion of long-term\\nfrequency or to expectations or beliefs relating to the types of balls that will\\nbe drawn. Following some additional axiomatic developments, however, this\\naspect of probability theory is also essentially “pure” mathematic\\n\\nWARNING: DO NOT HALLUCINATE OR INVENT INFORMATION\\n- Do NOT make up chapter titles like \"Probability\", \"Statistical Inference\", \"Linear Regression\"\\n- Do NOT guess page numbers\\n- Do NOT create generic textbook chapters\\n- ONLY extract what you can clearly see in the provided text\\n\\nCRITICAL RULES:\\n1. Extract ONLY main chapters that start with a number (1, 2, 3, etc.)\\n2. Do NOT extract subsections (like 1.1, 1.2, 2.1, etc.)\\n3. Use the EXACT chapter titles shown in the document\\n4. Use the EXACT page numbers shown in the document\\n5. Handle both roman numerals (i, ii, iii, v, x) and arabic numerals (1, 25, 100)\\n6. Calculate end pages as: next chapter\\'s start page minus 1\\n7. Return ONLY valid JSON - no explanations, no markdown formatting\\n8. If you cannot clearly identify chapters, return empty array []\\n\\nLook for patterns like:\\n- \"1 Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\"\\n- \"2 Distribution Theory and Statistical Models . . . . . . . . . . . . . . . . 155\"\\n- \"3 Basic Statistical Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\"\\n\\nDO NOT extract lines like:\\n- \"1.1 Some Important Music Concepts . . . . . . . . . . . 3\"\\n- \"Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v\"\\n\\nUse ONLY the exact titles from the document. Do not shorten or modify them.\\n\\nReturn JSON array: [{\"chapter_number\": \"X\", \"chapter_title\": \"...\", \"start_page\": X, \"end_page\": X}]\\n\\nExtract only what is explicitly visible in the text. Do not hallucinate. Be complete and extract all chapters that are clearly numbered. If no clear main chapters, return an empty array [].<end_of_turn>\\n<start_of_turn>model\\nI will carefully examine the table of contents and extract only the main chapters that are explicitly shown, using their exact titles and page numbers. I will not invent or hallucinate any information.\\n\\nLooking at the provided table of contents, I will now extract the main chapters:<end_of_turn>\\n<start_of_turn>user\\nPerfect. Now provide the JSON array with the extracted chapters.<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d996c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use prompt optimized for gemma3\n",
      "[RunPod] Job started: 742a4b2c-734b-4e75-a0cc-a780652ce443-e1\n",
      "[RunPod] Status: IN_QUEUE\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: IN_PROGRESS\n",
      "[RunPod] Status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "chapters_json = extract_chapters_from_toc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebad8519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davide/miniconda3/envs/llm_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  \n",
    "client, embedding_func = initialize_chromadb(EMBEDDING_MODEL)\n",
    "\n",
    "# Create two collections with different purposes\n",
    "whole_text_collection = initialize_collection(\n",
    "    client, embedding_func, \"whole_text_chunks\"\n",
    ")\n",
    "\n",
    "chapter_collection = initialize_collection(\n",
    "    client, embedding_func, \"chapter_chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_text(collection, query='', nresults=2, sim_th=None):\n",
    "    \"\"\"Get relevant text from a collection for a given query\"\"\"\n",
    "\n",
    "    query_result = collection.query(query_texts=query, n_results=nresults)\n",
    "    docs = query_result.get('documents')[0]\n",
    "\n",
    "    if sim_th is not None:\n",
    "        similarities = [1 - d for d in query_result.get(\"distances\")[0]]\n",
    "        relevant_docs = [d for d, s in zip(docs, similarities) if s >= sim_th]\n",
    "        return ''.join(relevant_docs)\n",
    "    return ''.join([doc for doc in docs if doc is not None])\n",
    "\n",
    "\n",
    "def generate_answer(base_url, model, prompt, context=[], top_k=5, top_p=0.9, temp=0.5):\n",
    "    url = base_url + \"/generate\"\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"stream\": False,\n",
    "        \"context\": context,\n",
    "        \"options\": {\"temperature\": temp, \"top_p\": top_p, \"top_k\": top_k},\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=data)\n",
    "        response.raise_for_status()\n",
    "        response_dict = response.json()\n",
    "        return response_dict.get('response', ''), response_dict.get('context', [])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "        return \"\", []\n",
    "\n",
    "\n",
    "def get_contextual_prompt(question, context):\n",
    "    contextual_prompt = (\n",
    "        \"You are a helpful assistant. Use the information provided in the context below to answer the question. \"\n",
    "        \"Ensure your answer is accurate, concise, and directly addresses the question. \"\n",
    "        \"If the context does not provide enough information to answer the question, state that explicitly.\\n\\n\"\n",
    "        \"### Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"### Question:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Answer:\"\n",
    "    )\n",
    "    return contextual_prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
