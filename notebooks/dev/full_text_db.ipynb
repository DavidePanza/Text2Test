{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9f12e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### This extract pages number (starting from actual page 1) and bundles it with page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49993dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import warnings\n",
    "\n",
    "def extract_page_data_fitz(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts page numbers and text from a PDF file using PyMuPDF.\n",
    "    The function looks for page numbers in the top and bottom 15% of each page.\n",
    "    It returns a list of dictionaries, each containing the page index, page number,\n",
    "    and the full text of the page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_data = []\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        height = page.rect.height\n",
    "        width = page.rect.width\n",
    "\n",
    "        top_rect = fitz.Rect(0, 0, width, height * 0.15)\n",
    "        bottom_rect = fitz.Rect(0, height * 0.85, width, height)\n",
    "\n",
    "        top_text = page.get_text(\"text\", clip=top_rect).split()\n",
    "        bottom_text = page.get_text(\"text\", clip=bottom_rect).split()\n",
    "\n",
    "        found_number = None\n",
    "        for text in top_text + bottom_text:\n",
    "            if text.isdigit():\n",
    "                found_number = int(text)\n",
    "                break\n",
    "\n",
    "        full_text = page.get_text(\"text\")\n",
    "\n",
    "        pages_data.append({\n",
    "            \"index\": i,\n",
    "            \"number\": found_number,\n",
    "            \"content\": full_text\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "    return pages_data\n",
    "\n",
    "\n",
    "def correct_page_numbers(pages_data, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Corrects the page numbers in the extracted data.\n",
    "    It looks for a sequence of consecutive page numbers and fills in the gaps.\n",
    "    The function also handles the case where page numbers are not in a sequential order\n",
    "    by correcting them based on the first found sequence of consecutive page numbers.\n",
    "    The function also sets page numbers less than 1 to None.\n",
    "    If no sequence is found, it returns None.\n",
    "    The function returns the index of the first page with number 1.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find first sequence of 'sequence_length' consecutive page numbers\n",
    "        seen = [(i, d[\"number\"]) for i, d in enumerate(pages_data) if isinstance(d[\"number\"], int)]\n",
    "\n",
    "        for start in range(len(seen) - sequence_length + 1):\n",
    "            valid = True\n",
    "            for j in range(sequence_length):\n",
    "                if seen[start + j][1] != seen[start][1] + j:\n",
    "                    valid = False\n",
    "                    break\n",
    "            if valid:\n",
    "                base_index, base_number = seen[start]\n",
    "                break\n",
    "        else:\n",
    "            # No sequence found\n",
    "            return None\n",
    "\n",
    "        # Forward fill from base_index\n",
    "        for offset, page in enumerate(pages_data[base_index:], start=0):\n",
    "            page[\"number\"] = base_number + offset\n",
    "\n",
    "        # Backward fill before base_index\n",
    "        for offset in range(1, base_index + 1):\n",
    "            page = pages_data[base_index - offset]\n",
    "            page[\"number\"] = base_number - offset\n",
    "\n",
    "        # Set pages < 1 == None\n",
    "        for page in pages_data:\n",
    "            if page[\"number\"] < 1:\n",
    "                page[\"number\"] = None\n",
    "\n",
    "        # Find index of first page with number 1\n",
    "        start_chapter = next((page['index'] for page in pages_data if page[\"number\"] == 1), None)\n",
    "\n",
    "        return start_chapter\n",
    "\n",
    "    except Exception:\n",
    "        # Catch any unexpected errors and return None\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_text(pdf_path, start_chapter=None):\n",
    "    \"\"\"\n",
    "    Extracts the text from a PDF file using PyMuPDF.\n",
    "    It returns the text of the book starting from the specified page index.\n",
    "    If no start_chapter is provided, it extracts the text from the entire PDF.\n",
    "    \"\"\"\n",
    "    if start_chapter:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        all_pages_text = []\n",
    "        for page_range in range(start_chapter, len(doc)):\n",
    "            page_text = doc[page_range].get_text(\"text\")\n",
    "            all_pages_text.append(page_text)\n",
    "        doc.close()\n",
    "        whole_text = \"\\n\".join(all_pages_text)\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            \"start_chapter is None: extracting text from the entire PDF.\",\n",
    "            UserWarning\n",
    "        )\n",
    "        doc = fitz.open(pdf_path)\n",
    "        whole_text = \"\"\n",
    "        for page in doc:\n",
    "            page_text = page.get_text(\"text\")\n",
    "            whole_text += page_text\n",
    "        doc.close()\n",
    "    \n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72d8e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "Introduction\n",
      "1.1 What this book is all about\n",
      "The purpose of this book is to present a general theory of early vision and image pro-\n",
      "cessing. The theory is normative, i.e. it says what is the optimal way of doing these\n",
      "things. It is based on construction of statistical models of images combined with\n",
      "Bayesian inference. Bayesian inference shows how we can use prior information\n",
      "on the structure of typical images to greatly improve image analysis, and statistical\n",
      "models are used for learning and storing that prior information.\n",
      "The theory predicts what kind of features should be computed from the incoming\n",
      "visual stimuli in the visual cortex. The predictions on the primary visual cortex\n",
      "have been largely conﬁrmed by experiments in visual neuroscience. The theory also\n",
      "predicts something about what should happen in higher areas such as V2, which\n",
      "gives new hints for people doing neuroscientiﬁc experiments.\n",
      "Also, the theory can be applied on engineering problems to develop more efﬁ-\n",
      "cient methods for denoising, synthesis, reconstruction, compression, and other tasks\n",
      "of image analysis, although we do not go into the details of such applications in this\n",
      "book.\n",
      "The statistical models presented in this book are quite different from classic sta-\n",
      "tistical models. In fact, they are so sophisticated that many of them have been devel-\n",
      "oped only during the last 10 years, so they are interesting in their own right. The key\n",
      "point in these models is the non-gaussianity (non-normality) inherent in image data.\n",
      "The basic model presented is independent component analysis, but that is merely a\n",
      "starting point for more sophisticated models.\n",
      "A preview of what kind of properties these models learn is in Figure 1.1. The\n",
      "ﬁgure shows a number of linear features learned from natural images by a statistical\n",
      "model. Chapters 5–7 will already consider models which learn such linear features.\n",
      "In addition to the features themselves, the results in Figure 1.1 show another visu-\n",
      "ally striking phenomenon, which is their spatial arrangement, or topography. The\n",
      "results in the ﬁgure actually come from a model called Topographic ICA, which\n",
      "is explained in Chapter 11. The spatial arrangement is also related to computation\n",
      "1\n",
      "\n",
      "2\n",
      "1 Introduction\n",
      "of nonlinear, invariant features, which is the topic of Chapter 10. Thus, the result\n",
      "in this ﬁgure combines several of the results we develop in this book. All of these\n",
      "properties are similar to those observed in the visual system of the brain.\n",
      "Fig. 1.1: An example of the results we will obtain in this book. Each small square in the image\n",
      "is one image feature, grey-scale coded to that middle grey means zero, white positive, and black\n",
      "negative values. The model has learned local, oriented features which are similar to those computed\n",
      "by cells in the brain. Furthermore, the model uses the statistical dependencies of the features to\n",
      "arrange them on a 2D surface. Such a spatial arrangement can also be observed in the visual cortex.\n",
      "The arrangement is also related to computation of nonlinear, invariant features.\n",
      "In the rest of this introduction, we present the basic problem of image analysis,\n",
      "and an overview of the various ideas discussed in more detail in this book.\n",
      "\n",
      "1.3 The magic of your visual system\n",
      "3\n",
      "1.2 What is vision?\n",
      "We can deﬁne vision as the process of acquiring knowledge about environmental ob-\n",
      "jects and events by extracting information from the light the objects emit or reﬂect.\n",
      "The ﬁrst thing we will need to consider is in what form this information initially is\n",
      "available.\n",
      "The light emitted and reﬂected by objects has to be collected and then measured\n",
      "before any information can be extracted from it. Both biological and artiﬁcial sys-\n",
      "tems typically perform the ﬁrst step by projecting light to form a two-dimensional\n",
      "image. Although there are, of course, countless differences between the eye and any\n",
      "camera, the image formation process is essentially the same. From the image, the\n",
      "intensity of the light is then measured in a large number of spatial locations, or sam-\n",
      "pled. In the human eye this is performed by the photoreceptors, whereas artiﬁcial\n",
      "systems employ a variety of technologies. However, all systems share the funda-\n",
      "mental idea of converting the light ﬁrst into a two-dimensional image and then into\n",
      "some kind of signal that represents the intensity of the light at each point in the\n",
      "image.\n",
      "Although in general the projected images have both temporal and chromatic di-\n",
      "mensions, we will be mostly concerned with static, monochrome (grey-scale) im-\n",
      "ages. Such an image can be deﬁned as a scalar function over two dimensions, I(x,y),\n",
      "giving the intensity (luminance) value at every location (x,y) in the image. Although\n",
      "in the general case both quantities (the position (x,y) and the intensity I(x,y)) take\n",
      "continuous values, we will focus on the typical case where the image has been sam-\n",
      "pled at discrete points in space. This means that in our discussion x and y take only\n",
      "integer values, and the image can be fully described by an array containing the inten-\n",
      "sity values at each sample point.1 In digital systems, the sampling is typically rect-\n",
      "angular, i.e. the points where the intensities are sampled form a rectangular array.\n",
      "Although the spatial sampling performed by biological systems is not rectangular\n",
      "or even regular, the effects of the sampling process are not very different.\n",
      "It is from this kind of image data that vision extracts information. Information\n",
      "about the physical environment is contained in such images, but only implicitly.\n",
      "The visual system must somehow transform this implicit information into an explicit\n",
      "form, for example by recognizing the identities of objects in the environment.This is\n",
      "not a simple problem, as the demonstration of the next section attempts to illustrate.\n",
      "1.3 The magic of your visual system\n",
      "Vision is an exceptionally difﬁcult computational task. Although this is clear to\n",
      "vision scientists, it might come as a surprise to others. The reason for this is that we\n",
      "1 When images are stored on computers, the entries in the arrays also have to be discretized; this is,\n",
      "however, of less importance in the discussion that follows, and we will assume that this has been\n",
      "done at a high enough resolution so that this step can be ignored.\n",
      "\n",
      "4\n",
      "1 Introduction\n",
      "are equipped with a truly amazing visual system that performs the task effortlessly\n",
      "and quite reliably in our daily environment. We are simply not aware of the whole\n",
      "computational process going on in our brains, rather we experience only the result\n",
      "of that computation.\n",
      "To illustrate the difﬁculties in vision, Figure 1.2 displays an image in its numer-\n",
      "ical format (as described in the previous section), where light intensities have been\n",
      "measured and are shown as a function of spatial location. In other words, if you\n",
      "were to colour each square with the shade of grey corresponding to the contained\n",
      "number you would see the image in the form we are used to, and it would be eas-\n",
      "ily interpretable. Without looking at the solution just yet, take a minute and try to\n",
      "decipher what the image portrays. You will probably ﬁnd this extremely difﬁcult.\n",
      "Now, have a look at the solution in Figure 1.4. It is immediately clear what the\n",
      "image represents! Our visual system performs the task of recognizing the image\n",
      "completely effortlessly. Even though the image at the level of our photoreceptors\n",
      "is represented essentially in the format of Figure 1.2, our visual system somehow\n",
      "manages to make sense of all this data and ﬁgure out the real-world object that\n",
      "caused the image.\n",
      "In the discussion thus far, we have made a number of drastic simpliﬁcations.\n",
      "Among other things, the human retina contains photoreceptors with varying sensi-\n",
      "tivity to the different wavelengths of light, and we typically view the world through\n",
      "two eyes, not one. Finally, perhaps the most important difference is that we normally\n",
      "perceive dynamic images rather than static ones. Nonetheless, these differences do\n",
      "not change the fact that the optical information is, at the level of photoreceptors,\n",
      "represented in a format analogous to that we showed in Figure 1.2, and that the task\n",
      "of the visual system is to understand all this data.\n",
      "Most people would agree that this task initially seems amazingly hard. But after a\n",
      "moment of thought it might seem reasonable to think that perhaps the problem is not\n",
      "so difﬁcult after all? Image intensity edges can be detected by ﬁnding oriented seg-\n",
      "ments where small numbers border with large numbers. The detection of such fea-\n",
      "tures can be computationally formalized and straightforwardly implemented. Per-\n",
      "haps such oriented segments can be grouped together and subsequently object form\n",
      "be analyzed? Indeed, such computations can be done, and they form the basis of\n",
      "many computer vision algorithms. However, although current computer vision sys-\n",
      "tems work fairly well on synthetic images or on images from highly restricted en-\n",
      "vironments, they still perform quite poorly on images from an unrestricted, natural\n",
      "environment. In fact, perhaps one of the main ﬁndings of computer vision research\n",
      "to date has been that the analysis of real-world images is extremely difﬁcult! Even\n",
      "such a basic task as identifying the contours of an object is complicated because\n",
      "often there is no clear image contour along some part of its physical contour, as\n",
      "illustrated in Figure 1.3.\n",
      "In light of the difﬁculties computer vision research has run into, the computa-\n",
      "tional accomplishment of our own visual system seems all the more amazing. We\n",
      "perceive our environment quite accurately almost all the time, and only relatively\n",
      "rarely make perceptual mistakes. Quite clearly, biology has solved the task of ev-\n",
      "\n",
      "1.3 The magic of your visual system\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 1 1 3 3 3 3 2 2 1 1 1 1 1 0 2 4 5 3 3 3 3 3 4 4 2 2 3 4 2\n",
      "2 4 5 5 5 5 4 3 3 3 4 3 3 2 3 3 3 2 3 4 6 5 5 3 4 5 5 5 6 5 3 4 3 3 4 4 4 4 4 5 6 5 4 4 5 3\n",
      "2 3 4 4 4 4 3 3 4 4 4 4 2 2 3 3 2 3 4 4 5 4 5 6 5 4 4 5 4 3 2 2 2 3 5 5 5 4 4 5 6 6 4 4 4 3\n",
      "3 4 4 3 3 3 4 4 4 3 2 3 4 3 3 2 2 3 4 5 3 3 4 3 4 4 3 4 3 2 2 2 3 3 4 3 4 5 4 3 4 5 5 4 5 3\n",
      "3 5 5 3 2 3 3 2 2 2 2 2 2 3 3 3 3 4 5 5 4 3 3 3 3 2 2 3 4 6 5 4 4 5 5 4 4 4 4 5 5 5 6 5 5 3\n",
      "3 5 4 2 2 2 2 1 1 3 1 2 3 3 4 4 4 3 5 4 3 3 4 3 2 2 3 3 2 3 4 3 3 3 4 5 5 4 3 3 4 5 5 5 5 3\n",
      "3 5 4 2 1 1 1 1 2 2 2 3 3 3 4 3 3 4 4 3 3 4 4 3 3 3 4 5 5 4 4 4 3 2 3 3 3 5 3 3 3 5 6 5 4 3\n",
      "4 5 4 3 2 1 0 1 2 1 2 3 2 3 3 2 4 4 3 3 5 4 3 3 2 1 2 4 5 4 3 4 3 2 3 4 4 5 5 3 3 3 4 4 3 1\n",
      "4 5 3 3 2 1 1 2 3 3 3 2 3 2 2 3 4 4 3 4 5 4 3 2 2 2 3 3 4 5 5 4 4 4 4 4 4 5 5 4 3 4 4 4 3 1\n",
      "3 5 4 3 2 2 2 3 4 5 4 3 3 2 3 3 3 4 3 4 5 4 3 4 4 4 3 3 3 4 5 5 4 3 3 3 3 4 5 4 4 4 4 5 4 1\n",
      "3 6 4 4 3 4 5 6 6 6 5 5 4 3 3 4 4 3 4 5 5 4 5 5 5 4 3 3 3 3 4 4 4 3 2 2 3 4 5 4 4 4 3 4 4 1\n",
      "3 6 6 5 4 5 6 6 7 7 6 5 5 5 4 4 4 5 5 5 5 5 5 5 5 5 4 3 4 4 3 3 3 2 2 2 3 3 3 4 4 4 4 3 4 2\n",
      "3 5 6 5 4 6 6 7 7 7 7 6 6 6 5 6 6 6 6 6 6 6 6 5 5 5 5 4 3 3 3 3 4 2 3 2 3 3 3 5 5 5 4 4 3 2\n",
      "2 5 6 5 4 6 7 7 7 7 7 7 7 8 8 7 7 7 7 7 7 7 7 6 6 6 5 4 4 3 3 3 4 3 3 3 3 3 3 3 3 3 4 5 4 2\n",
      "3 5 6 4 5 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 7 7 7 6 6 6 5 5 4 4 4 4 3 3 3 3 3 3 2 2 3 3 4 4 5 3\n",
      "2 5 5 5 6 7 7 7 7 7 8 8 8 9 8 8 8 8 8 8 8 8 7 7 6 6 6 5 4 4 4 3 2 3 2 3 3 3 3 4 4 4 4 5 5 3\n",
      "3 5 5 5 7 7 7 7 8 8 8 9 8 9 9 9 8 8 8 8 8 8 7 7 6 6 6 5 5 5 4 3 3 2 1 2 2 2 3 4 5 4 4 4 5 3\n",
      "3 4 4 6 8 7 7 7 8 8 9 8 8 8 9 8 8 8 8 8 8 8 7 7 7 6 6 5 5 5 5 4 4 4 3 2 2 3 3 3 5 5 5 4 5 4\n",
      "3 5 5 6 8 7 7 7 8 8 9 9 9 9 8 8 8 8 8 8 8 7 7 6 6 6 6 5 5 5 5 4 4 4 4 3 3 3 4 4 5 6 4 4 5 3\n",
      "3 6 5 6 7 7 7 7 8 8 8 9 9 8 8 8 8 8 8 7 7 7 6 6 6 6 6 6 6 6 5 5 5 4 3 3 4 4 4 4 4 3 4 3 3 2\n",
      "2 5 5 7 7 7 8 7 8 8 8 8 8 7 7 8 7 7 7 7 7 7 7 6 6 6 6 6 6 6 5 5 5 4 3 3 3 4 5 4 5 5 5 4 4 3\n",
      "2 3 5 7 7 7 7 7 7 7 8 7 7 7 7 8 7 7 7 7 7 7 6 6 6 6 6 6 5 5 5 5 5 4 4 3 3 3 4 3 4 5 5 6 6 4\n",
      "3 4 6 7 7 7 7 7 7 8 8 8 8 8 7 8 7 7 8 7 7 7 6 6 6 6 6 6 5 5 5 4 5 4 4 3 3 3 5 5 4 4 5 6 6 4\n",
      "4 4 6 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 7 7 6 6 6 6 5 5 5 5 4 4 4 4 3 3 3 5 4 4 4 6 6 3\n",
      "3 4 6 7 7 6 7 7 8 8 8 8 9 9 8 8 8 8 8 8 8 7 6 5 5 5 5 5 5 4 5 5 5 4 4 4 4 3 3 4 4 3 4 4 2 2\n",
      "2 4 6 6 6 6 6 7 7 6 6 7 7 8 8 8 8 8 8 8 7 6 5 5 5 5 5 5 5 5 4 4 4 4 5 5 4 4 3 4 4 4 3 3 1 1\n",
      "2 4 6 6 5 6 6 6 6 6 7 7 7 7 8 7 7 7 8 7 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 4 4 5 5 5 4 3 2 1\n",
      "1 4 6 6 5 6 7 7 8 8 8 8 7 7 7 8 7 7 7 6 5 6 8 7 7 6 6 5 5 5 4 4 5 5 5 5 5 5 4 5 5 4 2 2 2 1\n",
      "2 5 6 6 6 7 7 7 8 7 6 7 7 7 7 8 8 7 6 4 3 7 6 4 5 4 2 2 3 4 4 4 4 5 5 5 4 4 4 4 5 2 1 2 2 1\n",
      "3 5 6 6 7 6 6 5 4 3 2 2 4 6 7 7 8 7 5 1 2 5 5 5 8 6 2 2 2 1 2 4 4 5 5 4 4 4 4 3 4 3 0 1 2 2\n",
      "3 5 6 7 7 6 4 2 5 5 3 3 4 4 6 7 7 7 4 1 3 5 5 5 7 6 5 4 4 3 2 3 4 5 5 4 4 4 3 4 4 4 1 0 2 2\n",
      "1 4 6 7 7 6 5 5 5 7 6 5 4 6 7 6 7 6 4 2 2 5 6 6 6 7 7 6 5 5 4 4 5 5 6 5 4 4 3 4 4 4 2 0 3 2\n",
      "0 2 6 7 7 6 6 7 7 8 6 5 5 6 6 6 6 6 4 2 2 2 6 7 7 7 7 6 5 4 4 5 6 6 6 5 4 4 3 3 4 3 2 3 4 3\n",
      "0 2 6 7 7 7 7 7 7 7 7 7 7 7 6 6 6 6 4 3 3 2 4 7 7 7 6 5 5 5 5 6 6 6 6 5 4 3 3 3 4 3 2 5 5 3\n",
      "1 3 6 6 7 7 7 7 7 7 8 7 6 6 6 6 7 6 5 3 3 3 3 5 6 6 6 6 6 6 6 6 6 6 5 4 4 4 3 4 4 3 4 5 5 2\n",
      "1 4 5 6 7 7 7 7 7 7 7 6 7 6 6 7 6 6 5 3 3 3 3 4 5 5 5 6 6 6 6 6 6 6 5 4 4 4 3 3 4 2 3 4 4 1\n",
      "0 2 5 6 7 7 7 7 7 6 6 6 7 7 7 6 6 6 5 4 3 3 3 4 5 6 6 6 6 6 6 6 6 5 5 4 4 4 3 3 4 3 3 4 2 2\n",
      "0 1 3 6 6 7 7 7 7 7 7 7 7 7 6 6 6 6 5 4 3 2 3 5 6 7 7 6 6 5 5 6 6 5 5 4 3 4 4 4 4 3 3 1 1 2\n",
      "0 1 2 6 6 6 6 7 7 8 8 8 8 7 6 6 7 6 5 5 4 3 2 4 6 7 7 7 6 6 5 5 5 5 4 4 3 3 4 4 3 2 2 0 1 2\n",
      "0 0 1 6 6 6 7 7 8 8 8 7 7 7 6 8 8 7 5 4 4 3 3 2 5 7 7 7 6 6 5 5 5 4 4 4 3 3 3 4 3 0 2 1 1 2\n",
      "0 1 1 5 6 6 6 7 8 8 8 7 7 6 6 8 8 7 5 4 3 3 3 2 5 7 7 7 6 6 6 5 4 4 4 4 3 3 3 3 4 2 3 1 2 2\n",
      "0 1 1 5 6 6 6 7 7 8 7 7 8 6 5 6 6 6 5 3 2 1 2 2 4 6 7 7 6 6 5 5 4 4 4 4 3 3 3 3 4 4 3 0 2 2\n",
      "0 1 0 4 6 6 7 7 7 7 7 8 7 6 5 5 5 5 4 1 1 2 3 3 4 5 7 7 6 6 5 5 5 4 4 3 3 3 3 3 4 4 3 1 2 3\n",
      "0 1 1 3 6 6 7 7 7 7 8 7 7 6 6 6 6 6 5 4 3 3 4 4 4 5 6 7 6 6 6 6 5 5 4 3 3 3 3 3 4 4 3 1 2 3\n",
      "0 2 4 5 6 7 7 7 7 7 7 7 6 6 6 6 8 7 7 7 6 4 5 4 4 5 7 6 6 6 6 6 5 5 4 3 3 3 3 3 4 3 3 1 2 3\n",
      "1 5 8 7 6 7 7 7 7 7 7 7 7 6 6 6 7 7 8 7 5 5 4 4 4 4 7 6 5 6 6 6 5 5 4 3 3 3 3 3 4 1 1 1 2 3\n",
      "2 6 9 8 6 6 7 7 7 7 7 7 5 4 4 5 5 5 5 5 5 3 4 3 2 1 6 7 5 6 6 6 5 4 4 3 3 3 3 3 4 1 0 0 2 2\n",
      "1 5 8 7 4 6 7 7 7 7 7 6 4 2 2 7 9 9 9 9 9 6 4 2 1 4 6 7 6 7 6 6 5 4 3 3 3 2 3 3 4 2 0 0 2 3\n",
      "0 2 4 4 2 5 6 7 7 7 7 6 6 6 3 4 5 6 6 5 6 2 1 2 4 5 6 6 7 7 6 5 5 4 3 3 2 2 2 3 4 5 0 0 0 2\n",
      "0 1 2 1 1 3 6 7 7 7 7 7 7 7 6 5 4 3 3 3 3 4 4 4 4 5 6 6 7 6 5 5 5 4 3 2 2 2 2 3 4 8 1 0 0 0\n",
      "0 1 1 0 1 2 4 6 7 7 7 7 7 7 6 6 6 7 6 6 6 5 5 4 5 6 6 6 6 5 5 5 4 3 2 2 2 2 2 2 5 9 2 0 0 0\n",
      "0 1 0 0 1 2 2 5 6 7 6 7 7 7 7 6 7 7 7 7 6 5 5 5 5 6 6 6 6 5 5 5 4 3 2 1 1 1 2 2 6 9 2 0 0 0\n",
      "0 1 1 0 1 2 2 3 5 6 6 7 7 7 7 7 7 7 7 7 6 6 5 5 6 6 5 6 5 4 4 4 3 2 1 1 1 1 2 2 7 9 2 0 0 0\n",
      "0 1 0 0 1 2 2 1 3 6 6 7 7 7 7 7 8 8 7 7 7 6 6 6 6 6 5 5 4 3 4 4 2 1 1 1 1 1 2 3 8 7 1 0 0 0\n",
      "0 1 0 0 1 2 2 2 2 4 6 7 7 7 8 8 8 7 7 7 6 6 6 6 6 6 5 5 4 3 3 2 1 1 1 1 1 1 1 6 9 6 0 0 0 0\n",
      "0 0 0 0 1 2 2 1 2 3 4 6 7 7 7 7 7 7 7 7 6 5 6 6 6 5 5 4 3 2 2 1 1 1 1 1 1 1 2 8 8 3 0 0 0 0\n",
      "0 1 0 1 2 3 2 1 1 1 1 4 7 7 6 6 6 7 7 6 6 5 5 5 6 5 4 3 2 2 2 1 1 2 2 1 1 1 6 8 7 1 0 0 0 0\n",
      "0 1 1 2 5 4 2 1 0 0 0 0 4 6 6 6 7 7 7 7 6 5 5 5 5 4 3 3 2 1 1 1 2 2 2 2 1 4 8 8 5 0 0 0 0 0\n",
      "0 1 3 5 6 3 1 0 0 0 0 0 1 5 6 7 7 7 7 7 7 6 5 5 4 3 2 2 1 2 2 2 2 2 2 1 3 7 8 7 2 0 0 0 0 0\n",
      "1 3 4 2 1 0 0 0 0 0 0 0 0 3 6 6 7 7 7 7 6 6 5 5 4 2 2 2 2 3 3 3 2 2 1 3 8 8 8 5 0 0 0 0 0 0\n",
      "1 1 0 0 0 0 0 0 0 0 0 0 0 2 4 3 4 4 4 4 4 3 3 2 2 1 2 2 2 2 2 2 2 1 1 5 5 5 5 1 0 0 0 0 0 0\n",
      "Fig. 1.2: An image displayed in numerical format. The shade of grey of each square has been\n",
      "replaced by the corresponding numerical intensity value. What does this mystery image depict?\n",
      "\n",
      "6\n",
      "1 Introduction\n",
      "Fig. 1.3: This image of a cup demonstrates that physical contours and image contours are often very\n",
      "different. The physical edge of the cup near the lower-left corner of the image yields practically no\n",
      "image contour (as shown by the magniﬁcation). On the other hand, the shadow casts a clear image\n",
      "contour where there in fact is no physical edge.\n",
      "eryday vision in a way that is completely superior to any present-day machine vision\n",
      "system.\n",
      "This being the case, it is natural that computer vision scientists have tried to draw\n",
      "inspiration from biology. Many systems contain image processing steps that mimic\n",
      "the processing that is known to occur in the early parts of the biological visual\n",
      "system. However, beyond the very early stages, little is actually known about the\n",
      "representations used in the brain. Thus, there is actually not much to guide computer\n",
      "vision research at the present.\n",
      "On the other hand, it is quite clear that good computational theories of vision\n",
      "would be useful in guiding research on biological vision, by allowing hypothesis-\n",
      "driven experiments. So it seems that there is a dilemma: computational theory is\n",
      "needed to guide experimental research, and the results of experiments are needed\n",
      "to guide theoretical investigations. The solution, as we see it, is to seek synergy by\n",
      "multidisciplinary research into the computational basis of vision.\n",
      "1.4 Importance of prior information\n",
      "1.4.1 Ecological adaptation provides prior information\n",
      "A very promising approach for solving the difﬁcult problems in vision is based on\n",
      "adaptation to the statistics of the input. An adaptive representation is one that does\n",
      "not attempt to represent all possible kinds of data; instead, the representation is\n",
      "adapted to a particular kind of data. The advantage is that then the representation\n",
      "can concentrate on those aspects of the data that are useful for further analysis. This\n",
      "is in stark contrast to classic representations (e.g. Fourier analysis) that are ﬁxed\n",
      "based on some general theoretical criteria, and completely ignore what kind of data\n",
      "is being analyzed.\n",
      "\n",
      "1.4 Importance of prior information\n",
      "7\n",
      "Fig. 1.4: The image of Figure 1.2. It is immediately clear that the image shows a male face. Many\n",
      "observers will probably even recognize the speciﬁc individual (note that it might help to view the\n",
      "image from relatively far away).\n",
      "\n",
      "8\n",
      "1 Introduction\n",
      "Thus, the visual system is not viewed as a general signal processing machine or\n",
      "a general problem-solving system. Instead, it is acknowledged that it has evolved to\n",
      "solve some very particular problems that form a small subset of all possible prob-\n",
      "lems. For example, the biological visual system needs to recognize faces under dif-\n",
      "ferent lighting environments, while the people are speaking, possibly with differ-\n",
      "ent emotional expressions superimposed; this is deﬁnitely an extremely demanding\n",
      "problem. But on the other hand, the visual system does not need to recognize a face\n",
      "when it is given in an unconventional format, as in Figure 1.2.\n",
      "What distinguished these two representations (numbers vs. a photographic im-\n",
      "age) from each other is that the latter is ecologically valid, i.e. during the evolution\n",
      "of the human species, our ancestors have encountered this problem many times,\n",
      "and it has been important for their survival. The case of an array of numbers does\n",
      "deﬁnitely not have any of these two characteristics. Most people would label it as\n",
      "“artiﬁcial”.\n",
      "In vision research, more and more emphasis is being laid on the importance of\n",
      "the enormous amount of prior information that the brain has about the structure of\n",
      "the world. A formalization of these concepts has recently been pursued under the\n",
      "heading “Bayesian perception”, although the principle goes back to the “maximum\n",
      "likelihood principle” by Helmholtz in the 19th century. Bayesian inference is the\n",
      "natural theory to use when inexact and incomplete information is combined with\n",
      "prior information. Such prior information should presumably be reﬂected in the\n",
      "whole visual system.\n",
      "Similar ideas are becoming dominant in computer vision as well. Computer vi-\n",
      "sion systems have been used on many different kinds of images: “ordinary” (i.e.\n",
      "optical) images, satellite images, magnetic resonance images, to name a few. Is it\n",
      "realistic to assume that the same kind of processing would adequately represent all\n",
      "these different kinds of data? Could better results be obtained if one uses methods\n",
      "(e.g. features) that are speciﬁc to a given application?\n",
      "1.4.2 Generative models and latent quantities\n",
      "The traditional computational approach to vision focuses on how, from the image\n",
      "data I, one can compute quantities of interest called si, which we group together\n",
      "in a vector s. These quantities might be, for instance, scalar variables such as the\n",
      "distances to objects, or binary parameters such as signifying if an object belongs to\n",
      "some given categories. In other words, the emphasis is on a function f that trans-\n",
      "forms images into world or object information, as in s = f(I). This operation might\n",
      "be called image analysis.\n",
      "Several researchers have pointed out that the opposite operation, image synthesis,\n",
      "often is simpler. That is, the mapping g that generates the image given the state of\n",
      "the world\n",
      "I = g(s),\n",
      "(1.1)\n",
      "\n",
      "1.4 Importance of prior information\n",
      "9\n",
      "is considerably easier to work with, and more intuitive, than the mapping f. This\n",
      "operation is often called synthesis. Moreover, the framework based on a ﬁxed ana-\n",
      "lyzing function f does not give much room for using prior information. Perhaps, by\n",
      "intelligently choosing the function f, some prior information on the data could be\n",
      "incorporated.\n",
      "Generative models use Eq. (1.1) as a starting point. They attempt to explain ob-\n",
      "served data by some underlying hidden (latent) causes or factors si about which we\n",
      "have only indirect information.\n",
      "The key point is that the models incorporate a set of prior probabilities for the\n",
      "latent variables si. That is, it is speciﬁed how often different combinations of latent\n",
      "variables occur together. For example, this probability distribution could describe,\n",
      "in the case of a cup, the typical shape of a cup. Thus, this probability distribution\n",
      "for the latent variables is what formalizes the prior information on the structure of\n",
      "the world.\n",
      "This framework is sufﬁciently ﬂexible to be able to accommodate many different\n",
      "kinds of prior information. It all depends on how we deﬁned the latent variables,\n",
      "and the synthesis function g.\n",
      "But how does knowing g help us, one may ask. The answer is that one may\n",
      "then search for the parameters ˆs that produce an image ˆI = g(ˆs) which, as well as\n",
      "possible, matches the observed image I. In other words, a combination of latent\n",
      "variables that is the “most likely”. Under reasonable assumptions, this might lead to\n",
      "a good approximation of the correct parameters s.\n",
      "To make all this concrete, consider again the image of the cup in Figure 1.3.\n",
      "The traditional approach of vision would propose that an early stage extracts local\n",
      "edge information in the image, after which some sort of grouping of these edge\n",
      "pieces would be done. Finally, the evoked edge pattern would be compared with\n",
      "patterns in memory, and recognized as a cup. Meanwhile, analysis of other scene\n",
      "variables, such as lighting direction or scene depth, would proceed in parallel. The\n",
      "analysis-by-synthesis framework, on the other hand, would suggest that our visual\n",
      "system has an unconscious internal model for image generation. Estimates of object\n",
      "identity, lighting direction, and scene depth are all adjusted until a satisfactory match\n",
      "between the observed image and the internally generated image is achieved.\n",
      "1.4.3 Projection onto the retina loses information\n",
      "One very important reason why it is natural to formulate vision as inference of\n",
      "latent quantities is that the world is three dimensional whereas the retina is only\n",
      "two-dimensional. Thus, the whole 3D structure of the world is seemingly lost in the\n",
      "eye! Our visual system is so good in reconstructing a three-dimensional perception\n",
      "of the world that we hardly realize that a complicated reconstruction procedure is\n",
      "necessary. Information about the depth of objects and the space between is only\n",
      "implicit in the retinal image.\n",
      "\n",
      "10\n",
      "1 Introduction\n",
      "We do beneﬁt from having two eyes which give slightly different views of the\n",
      "outside world. This helps a bit in solving the problem of depth perception, but it\n",
      "is only part of the story. Even if you close one eye, you can still understand which\n",
      "object is in front of another. Television is also based on the principle that we can\n",
      "quite well reconstruct the 3D structure of the world from a 2D image, especially if\n",
      "the camera (or the observer) is moving.\n",
      "1.4.4 Bayesian inference and priors\n",
      "The fundamental formalism for modelling how prior information can be used in\n",
      "the visual system is based on what is called Bayesian inference. Bayesian inference\n",
      "refers to statistically estimating the hidden variables s given an observed image I.\n",
      "In most models it is impossible (even in theory) to know the precise values of s,\n",
      "so one must be content with a probability density p(s|I). This is the probability of\n",
      "the latent variables given the observed image. By Bayes’ rule, which is explained in\n",
      "Section 4.7, this can be calculated as\n",
      "p(s|I) = p(I|s)p(s)\n",
      "p(I)\n",
      ".\n",
      "(1.2)\n",
      "To obtain an estimate of the hidden variables, many models simply ﬁnd the particu-\n",
      "lar s which maximize this density,\n",
      "ˆs = argmax\n",
      "s\n",
      "p(s|I).\n",
      "(1.3)\n",
      "Ecological adaptation is now possible by learning the prior probability distri-\n",
      "bution from a large number of natural images. Learning refers, in general, to the\n",
      "process of constructing a representation of the regularities of data. The dominant\n",
      "theoretical approach to learning in neuroscience and computer science is the prob-\n",
      "abilistic approach, in which learning is accomplished by statistical estimation: the\n",
      "data is described by a statistical model that contains a number of parameters, and\n",
      "learning consists of ﬁnding “good” values for those parameters, based on the input\n",
      "data. In statistical terminology, the input data is a sample that contains observations.\n",
      "The advantage of formulating adaptation in terms of statistical estimation is very\n",
      "much due to the existence of an extensive theory of statistical theory and inference.\n",
      "Once the statistical model is formulated, the theory of statistical estimation imme-\n",
      "diately offers a number of tools to estimate the parameters. And after estimation of\n",
      "the parameters, the model can be used in inference according to the Bayesian theory,\n",
      "which again offers a number of well-studied tools that can be readily used.\n",
      "\n",
      "1.5 Natural images\n",
      "11\n",
      "1.5 Natural images\n",
      "1.5.1 The image space\n",
      "How can we apply the concept of prior information about the environment in early\n",
      "vision? “Early” vision refers to the initial parts of visual processing, which are usu-\n",
      "ally formalized as the computation of features, i.e. some relatively simple functions\n",
      "of the image (features will be deﬁned in Section 1.8 below). Early vision does not\n",
      "yet accomplish such tasks as object recognition. In this book, we consider early\n",
      "vision only.\n",
      "The central concept we need here is the image space. Earlier we described an\n",
      "image representation in which each image is represented as a numerical array con-\n",
      "taining the intensity values of its picture elements, or pixels. To make the following\n",
      "discussion concrete, say that we are dealing with images of a ﬁxed size of 256-by-\n",
      "256 pixels. This gives a total of 65536 = 2562 pixels in an image. Each image can\n",
      "then be considered as a point in a 65536-dimensionalspace, each axis of which spec-\n",
      "iﬁes the intensity value of one pixel. Conversely, each point in the space speciﬁes\n",
      "one particular image. This space is illustrated in Figure 1.5.\n",
      "Image space\n",
      "I(2,1)\n",
      "I(1,1)\n",
      "I(1,2)\n",
      "1\n",
      "I(x,y)\n",
      "Image pixels\n",
      "2\n",
      "2\n",
      "4\n",
      "1 4\n",
      "Fig. 1.5: The space representation of images. Images are mapped to points in the space in a one-to-\n",
      "one fashion. Each axis of the image space corresponds to the brightness value of one speciﬁc pixel\n",
      "in the image.\n",
      "Next, consider taking an enormous set of images, and plotting each as the corre-\n",
      "sponding point in our image space. (Of course, plotting a 65536-dimensional space\n",
      "is not very easy to do on a two-dimensional page, so we will have to be content with\n",
      "making a thought experiment.) An important question is: how would the points be\n",
      "distributed in this space? In other words, what is the probability density function of\n",
      "\n",
      "12\n",
      "1 Introduction\n",
      "our images like? The answer, of course, depends on the set of images chosen. Astro-\n",
      "nomical images have very different properties from holiday snapshots, for example,\n",
      "and the two sets would yield very different clouds of points in our space.\n",
      "It is this probability density function of the image set in question that we will\n",
      "model in this book.\n",
      "1.5.2 Deﬁnition of natural images\n",
      "In this book we will be speciﬁcally concerned with a particular set of images called\n",
      "natural images or images of natural scenes. Some images from our data set are\n",
      "shown in Figure 1.6. This set is supposed to resemble the natural input of the visual\n",
      "system we are investigating. So what is meant by “natural input”? This is actually\n",
      "not a trivial question at all. The underlying assumption in this line of research is\n",
      "that biological visual systems are, through a complex combination of the effects\n",
      "of evolution and development, adapted to process the kind of sensory input that\n",
      "they receive. Natural images is thus some set that we believe has similar statistical\n",
      "structure to that which the visual system is adapted to.\n",
      "Fig. 1.6: Three representative examples from our set of natural images.\n",
      "This poses an obvious problem, at least in the case of human vision. The human\n",
      "visual system has evolved in an environment that is in many ways different from\n",
      "the one most of us experience daily today. It is probably quite safe to say that im-\n",
      "ages of skyscrapers, cars, and other modern entities have not affected our genetic\n",
      "makeup to any signiﬁcant degree. On the other hand, few people today experience\n",
      "nature as omnipresent as it was tens of thousands of years ago. Thus, the input on\n",
      "the time-scale of evolution has been somewhat different from that on the time-scale\n",
      "of the individual. Should we then choose images of nature or images from a modern,\n",
      "urban environment to model the “natural input” of our visual system? Most work\n",
      "to date has focused on the former, and this is also our choice in this book. Fortu-\n",
      "nately, this choice of image set does not have a drastic inﬂuence on the results of\n",
      "the analysis: Most image sets collected for the purpose of analysing natural images\n",
      "give quite similar results in statistical modelling, and these results are usually com-\n",
      "\n",
      "1.6 Redundancy and information\n",
      "13\n",
      "pletely different from what you would get using most artiﬁcial, randomly generated\n",
      "data sets.\n",
      "Returning to our original question, how would natural images be distributed in\n",
      "the image space? The important thing to note is that they would not be anything like\n",
      "uniformly distributed in this space. It is easy for us to draw images from a uniform\n",
      "distribution, and they do not look anything like our natural images! Figure 1.7 shows\n",
      "three images randomly drawn from a uniform distribution over the image space. As\n",
      "there is no question that we can easily distinguish these images from natural images\n",
      "(Figure 1.6) it follows that these are drawn from separate, very different, distribu-\n",
      "tions. In fact, the distribution of natural images is highly non-uniform. This is the\n",
      "same as saying that natural images contain a lot of redundancy, an information-\n",
      "theoretic term that we turn to now.\n",
      "Fig. 1.7: Three images drawn randomly from a uniform distribution in the image space. Each pixel\n",
      "is drawn independently from a uniform distribution from black to white.\n",
      "1.6 Redundancy and information\n",
      "1.6.1 Information theory and image coding\n",
      "At this point, we make a short excursion to a subject that may seem, at ﬁrst sight, to\n",
      "be outside of the scope of statistical modelling: information theory.\n",
      "The development of the theory of information by Claude Shannon and others is\n",
      "one of the milestones of science. Shannon considered the transmission of a message\n",
      "across a communication channel and developed a mathematical theory that quan-\n",
      "tiﬁed the variables involved (these will be presented in Chapter 8). Because of its\n",
      "generality the theory has found, and continues to ﬁnd, a growing number of appli-\n",
      "cations in a variety of disciplines.\n",
      "One of the key ideas in information theory is that the amount of memory needed\n",
      "to store an image is often less than what is needed in a trivial representation (code),\n",
      "where each pixel is stored using a ﬁxed number of bits, such as 8 or 24. This is\n",
      "because some of the memory capacity is essentially consumed by redundant struc-\n",
      "ture in the image. The more rigid the structure, the less bits is really needed to code\n",
      "\n",
      "14\n",
      "1 Introduction\n",
      "the image. Thus, the contents of any image, indeed any signal, can essentially be\n",
      "divided into information and redundancy. This is depicted in Figure 1.8.\n",
      "Memory bits in trivial code\n",
      "Bits needed\n",
      "Information\n",
      "for an efficient code\n",
      "Redundancy\n",
      "Fig. 1.8: Redundancy in a signal. Some of the memory consumed by the storage of a typical image\n",
      "is normally unnecessary because of redundancy (structure) in the image. If the signal is optimally\n",
      "compressed, stripping it of all redundancy, it can be stored using much less bits.\n",
      "To make this more concrete, consider the binary image of Figure 1.9. The image\n",
      "contains a total of 32 ×22 = 704 pixels. Thus, the trivial representation (where the\n",
      "colour of each pixel is indicated by a ‘1’ or a ‘0’) for this image requires 704 bits.\n",
      "But it is not difﬁcult to imagine that one could compress it into a much smaller\n",
      "number of bits. For example, one could invent a representation that assumes a white\n",
      "background on which black squares (with given positions and sizes) are printed.\n",
      "In such a representation, our image could be coded by simply specifying the top-\n",
      "left corners of the squares ((5,5) and (19,11)) and their sizes (8 and 6). This could\n",
      "certainly be coded in less than 704 bits.2\n",
      "The important thing to understand is that this kind of representation is good for\n",
      "certain kinds of images (those with a small number of black squares) but not oth-\n",
      "ers (that do not have this structure and thus require a huge amount of squares to be\n",
      "completely represented). Hence, if we are dealing mostly with images of the former\n",
      "kind, and we are using the standard binary coding format, then our representation is\n",
      "highly redundant. By compressing it using our black-squares-on-white representa-\n",
      "tion we achieve an efﬁcient representation. Although natural images are much more\n",
      "variable than this hypothetical class of images, it is nonetheless true that they also\n",
      "show structure and can be compressed.\n",
      "Attneave was the ﬁrst to explicitly point out the redundancy in images in 1954.\n",
      "The above argument is essentially the same as originally given by Attneave, al-\n",
      "though he considered a ‘guessing game’ in which subjects guessed the colour of\n",
      "pixels in the image. The fact that subjects perform much better than chance proves\n",
      "that the image is predictable, and information theory ensures that predictability is\n",
      "essentially the same thing as redundancy.\n",
      "Making use of this redundancy of images is essential for vision. But the same\n",
      "statistical structure is in fact also crucial for many other tasks involving images.\n",
      "2 The speciﬁcation of each square requires three numbers which each could be coded in 5 bits,\n",
      "giving a total of 30 bits for two squares. Additionally, a few bits might be needed to indicate how\n",
      "many squares are coded, assuming that we do not know a priori that there are exactly two squares.\n",
      "\n",
      "1.6 Redundancy and information\n",
      "15\n",
      "Fig. 1.9: A binary image containing a lot of structure. Images like this can be coded efﬁciently; see\n",
      "main text for discussion.\n",
      "Engineers who seek to ﬁnd compact digital image formats for storing or transmitting\n",
      "images also need to understand this structure. Image synthesis and noise reduction\n",
      "are other tasks that optimally would make use of this structure. Thus, the analysis\n",
      "of the statistical properties of images has widespread applications indeed, although\n",
      "perhaps understanding vision is the most profound.\n",
      "1.6.2 Redundancy reduction and neural coding\n",
      "Following its conception, it did not take long before psychologists and biologists\n",
      "understood that information theory was directly relevant to the tasks of biological\n",
      "systems. Indeed, the sensory input is a signal that carries information about the\n",
      "outside world. This information is communicated by sensory neurons by means of\n",
      "action potentials.\n",
      "In Attneave’s original article describing the redundancy inherent in images, At-\n",
      "tneave suggested that the visual system recodes the inputs to reduce redundancy,\n",
      "providing an ‘economical description’ of the sensory signals. He likened the task\n",
      "of the visual system to that of an engineer who seeks to represent pictures with the\n",
      "smallest possible number of bits. It is easy to see the intuitive appeal of this idea.\n",
      "Consider again the image of Figure 1.9. Recoding images of this kind using our\n",
      "black-squares-on-white representation, we reduce redundancy and obtain an efﬁ-\n",
      "cient representation. However, at the same time we have discovered the structure in\n",
      "the signal: we now have the concept of ‘squares’ which did not exist in the origi-\n",
      "nal representation. More generally: to reduce redundancy one must ﬁrst identify it.\n",
      "Thus, redundancy reduction requires discovering structure.\n",
      "\n",
      "16\n",
      "1 Introduction\n",
      "Although he was arguably the ﬁrst to spell it out explicitly, Attneave was certainly\n",
      "not the only one to have this idea. Around the same time, Barlow, in 1961, provided\n",
      "similar arguments from a more biological/physiological viewpoint. Barlow has also\n",
      "pointed out that the idea, in the form of ‘economy of thought’, is clearly expressed\n",
      "already in the writings of Mach and Pearson in the 19th century. Nevertheless, with\n",
      "the writings of Attneave and Barlow, the redundancy reduction (or efﬁcient coding)\n",
      "hypothesis was born.\n",
      "1.7 Statistical modelling of the visual system\n",
      "1.7.1 Connecting information theory and Bayesian inference\n",
      "Earlier we emphasized the importance of prior information and Bayesian modelling,\n",
      "but in the preceding section we talked about information theory and coding. This\n",
      "may seem a bit confusing at ﬁrst sight, but the reason is that the two approaches are\n",
      "very closely related.\n",
      "Information theory wants to ﬁnd an economical representation of the data for\n",
      "efﬁcient compression, while Bayesian modelling uses prior information on the data\n",
      "for such purposes as denoising and recovery of the 3D structure. To accomplish their\n",
      "goals, both of these methods fundamentally need the same thing: a good model of\n",
      "the statistical distribution of the data. That is the basic motivation for this book. It\n",
      "leads to a new approach to normative visual modelling as will be discussed next.\n",
      "1.7.2 Normative vs. descriptive modelling of visual system\n",
      "In visual neuroscience, the classic theories of receptive ﬁeld structure3 can be called\n",
      "descriptive in the sense that they give us mathematical tools (such as Fourier and\n",
      "Gabor analysis, see Chapter 2) that allow us to describe parts of the visual system\n",
      "in terms of a small number of parameters.\n",
      "However, the question we really want to answer is: Why is the visual system\n",
      "built the way neuroscientiﬁc measurements show? The basic approach to answer\n",
      "such a question in neuroscience is to assume that the system in question has been\n",
      "optimized by evolution to perform a certain function. (This does not mean that the\n",
      "system would be completely determined genetically, because evolution can just have\n",
      "designed mechanisms for self-organization and learning that enable the system to\n",
      "ﬁnd the optimal form.)\n",
      "Models based on the assumption of optimality are often called normative because\n",
      "they tell how the system should behave. Of course, there is no justiﬁcation to assume\n",
      "3 i.e., the way visual neurons respond to stimulation, see Section 3.3\n",
      "\n",
      "1.7 Statistical modelling of the visual system\n",
      "17\n",
      "that evolution has optimized all parts of the organism; most of them may be far from\n",
      "the optimum, and such an optimum may not even be a well-deﬁned concept.\n",
      "However, in certain cases it can be demonstrated that the system is not far from\n",
      "optimal in certain respects. This happens to be the case with the early cortical visual\n",
      "processing system (in particular, the primary visual cortex, see Chapter 3 for a brief\n",
      "description of the visual system). That brain area seems to function largely based\n",
      "on principles of statistical modelling, as will be seen in this book. Thus, there is\n",
      "convincing proof that parts of the system are optimal for statistical inference, and it\n",
      "is this proof that justiﬁes these normative models.\n",
      "Previous models of the early visual system did not provide satisfactory norma-\n",
      "tive models, they only provided practical descriptive models. Although there were\n",
      "some attempts to develop a normative theory, the predictions were too vague.4 The\n",
      "statistical approach is the ﬁrst one to give exact quantitative models of visual pro-\n",
      "cessing, and these have been found to provide a good match with neuroscientiﬁc\n",
      "measurements.\n",
      "1.7.3 Towards predictive theoretical neuroscience\n",
      "Let us mention one more important application of this framework: a mode of mod-\n",
      "elling where we are able to predict the properties of visual processing beyond the\n",
      "primary visual cortex. Then, we obtain quantitative predictions on what kinds of\n",
      "visual processing should take place in areas whose function is not well understood\n",
      "at this point.\n",
      "Almost all the experimental results in early visual processing have concerned\n",
      "the primary visual cortex, or even earlier areas such as the retina. Likewise, most\n",
      "research in this new framework of modelling natural image statistics has been on\n",
      "very low-level features. However, the methodology of statistical modelling can most\n",
      "probably be extended to many other areas.\n",
      "Formulating statistical generative models holds great promise as a framework\n",
      "that will give new testable theories for visual neuroscience, for the following rea-\n",
      "sons:\n",
      "• This framework is highly constructive. From just a couple of simple theoretical\n",
      "speciﬁcations, natural images lead to the emergence of complex phenomena, e.g.\n",
      "the forms of the receptive ﬁelds of simple cells and their spatial organization in\n",
      "Fig. 1.1.\n",
      "• This framework is, therefore, less subjective than many other modelling ap-\n",
      "proaches. The rigorous theory of statistical estimation makes it rather difﬁcult\n",
      "to insert the theorist’s subjective expectations in the model, and therefore the\n",
      "4 The main theory attempting to do this is the joint space-frequency localization theory leading to\n",
      "Gabor models, see Section 2.4.2. However, this does not provide predictions on how the parameters\n",
      "in Gabor models should be chosen, and what’s more serious, it is not really clear why the features\n",
      "should be jointly localized in space and frequency in the ﬁrst place.\n",
      "\n",
      "18\n",
      "1 Introduction\n",
      "results are strongly determined by the data, i.e. the objective reality. Thus, the\n",
      "framework can be called data-driven.\n",
      "• In fact, in statistical generative models we often see emergence of new kinds of\n",
      "feature detectors — sometimes very different from what was expected when the\n",
      "model was formulated.\n",
      "So far, experiments in vision research have been based on rather vague, qualita-\n",
      "tive predictions. (This is even more true for other domains of neuroscience.) How-\n",
      "ever, using the methodology described here, visual neuroscience has the potential of\n",
      "starting a new mode of operation where theoretical developments directly give new\n",
      "quantitative hypotheses to be falsiﬁed or conﬁrmed in experimental research. Be-\n",
      "coming theory-driven would be a real revolution in the way neuroscience is done. In\n",
      "fact, this same development is what gave much of the driving force to exact natural\n",
      "sciences in the 19th and 20th centuries.\n",
      "1.8 Features and statistical models of natural images\n",
      "1.8.1 Image representations and features\n",
      "Most statistical models of natural images are based on computing features. The word\n",
      "“feature” is used rather loosely for any function of the image which is to be used in\n",
      "further visual processing. The same word can be used for the output (value) of the\n",
      "function, or the computational operation of which computes that value.\n",
      "A classic approach to represent an image is a linear weighted sum of features.\n",
      "Let us denote each feature by Ai(x,y),i = 1,...,n. These features are assumed to\n",
      "be ﬁxed. For each incoming image, the coefﬁcients of each feature in an image are\n",
      "denoted by si. Algebraically, we can write:\n",
      "I(x,y) =\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si\n",
      "(1.4)\n",
      "If we assume for simplicity that the number of features n equals the number of\n",
      "pixels, the system in Eq. (1.4) can be inverted. This means that for a given image I,\n",
      "we can ﬁnd the coefﬁcients si that fulﬁll this equation. In fact, they can be computed\n",
      "linearly as\n",
      "si = ∑\n",
      "x,y\n",
      "Wi(x,y)I(x,y)\n",
      "(1.5)\n",
      "for certain inverse weights W. The terminology is not very ﬁxed here, so either Ai,\n",
      "Wi, or si can be called a “feature”. The Wi can also be called a feature detector.\n",
      "There are many different sets of features that can be used. Classic choices include\n",
      "Fourier functions (gratings), wavelets, Gabor functions, features of discrete cosine\n",
      "transform, and many more. What all these sets have in common is that they attempt\n",
      "to represent all possible images, not just natural images, in a way which is “optimal”.\n",
      "\n",
      "1.8 Features and statistical models of natural images\n",
      "19\n",
      "What we want to do is to learn these features so that they are adapted to the\n",
      "properties of natural images. We do not believe that there could be a single set of\n",
      "features which would be optimal for all kinds of images. Also, we want to use the\n",
      "features to build a statistical model of natural images. The basis for both of these is\n",
      "to consider the statistics of the features si.\n",
      "1.8.2 Statistics of features\n",
      "The most fundamental statistical properties of images are captured by the his-\n",
      "tograms of the outputs si of linear feature detectors. Let us denote the output of\n",
      "a single linear feature detector with weights W(x,y) by s:\n",
      "s = ∑\n",
      "x,y\n",
      "W(x,y)I(x,y)\n",
      "(1.6)\n",
      "Now, the point is to look at the statistics of the output when the input of the detector\n",
      "consists of a large number of natural image patches. Natural image patches means\n",
      "small subimages (windows) taken in random locations in randomly selected natural\n",
      "images. Thus, the feature s is a random variable, and for each input patch we get\n",
      "a realization (observation) of that random variable. (This procedure is explained in\n",
      "more detail in Section 4.1).\n",
      "Now we shall illustrate this with real natural image data. Let us consider a couple\n",
      "of simple feature detectors and the histograms of their output when the input consists\n",
      "of natural images. In Fig. 1.10 we show three simple feature detectors. The ﬁrst is a\n",
      "Dirac detector, which means that all the weights W(x,y) are zero except for one. The\n",
      "second is a simple one-dimensional grating. The third is a basic Gabor edge detector.\n",
      "All three feature detectors have been normalized to unit norm, i.e. ∑x,yW(x,y)2 = 1.\n",
      "The statistics of the output are contained in the histogram of the outputs. In\n",
      "Fig. 1.11, we show the output histograms for the three different kinds of linear detec-\n",
      "tors. We can see that the histograms are rather different. In addition to the different\n",
      "shapes, note that their variances are also quite different from each other.\n",
      "Thus, we see that different feature detectors are characterized by different statis-\n",
      "tics of their outputs for natural image input. This basic observation is the basis for\n",
      "the theory in this book. We can learn features from image data by optimizing some\n",
      "statistical properties of the features si.\n",
      "1.8.3 From features to statistical models\n",
      "The Bayesian goal of building a statistical (prior) model of the data, and learning\n",
      "features based on their output statistics are intimately related. This is because the\n",
      "most practical way of building a statistical model proceeds by using features and\n",
      "building a statistical model for them. The point is that the statistical model for fea-\n",
      "\n",
      "20\n",
      "1 Introduction\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "Fig. 1.10: Three basic ﬁlters. a) a Dirac feature, i.e. only one pixel is non-zero. b) a sinusoidal\n",
      "grating. c) Gabor edge detector.\n",
      "a)\n",
      "−5\n",
      "0\n",
      "5\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "b)\n",
      "−40\n",
      "−20\n",
      "0\n",
      "20\n",
      "40\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "c)\n",
      "−10\n",
      "−5\n",
      "0\n",
      "5\n",
      "10\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "Fig. 1.11: The histograms of the outputs of the ﬁlters in Fig. 1.10 when the input is natural images\n",
      "with mean pixel value subtracted. a) output of Dirac ﬁlter, which is the same as the histogram of\n",
      "the original pixels themselves. b) output of grating feature detector. c) output of edge detector.\n",
      "Note that the scales of both axes are different in the three plots.\n",
      "tures can be much simpler than the corresponding model for the pixels, so it makes\n",
      "sense to ﬁrst transform the data into a feature space.\n",
      "In fact, a large class of model builds independent models for each of the features\n",
      "si in Equation (1.5). Independence is here to be taken both in an intuitive sense,\n",
      "and in the technical sense of statistical independence. Most models in Part II of this\n",
      "book are based on this idea. Even if the features are not modelled independently, the\n",
      "interactions (dependencies) of the features are usually much simpler than those of\n",
      "the original pixels; such models are considered in Part III of this book.\n",
      "Thus, we will describe most of the models in this book based on the principle\n",
      "of learning features. Another reason for using this approach is that the most inter-\n",
      "esting neurophysiological results concern usually the form of the features obtained.\n",
      "In fact, it is very difﬁcult to interpret or visualize a probability distribution given\n",
      "by the model; comparing the distribution with neurophysiological measurements is\n",
      "next to impossible. It is the features which give a simple and intuitive idea of what\n",
      "kind of visual processing these normative models dictate, and they allow a direct\n",
      "comparison with measured properties (receptive ﬁelds) of the visual cortex.\n",
      "\n",
      "1.10 References\n",
      "21\n",
      "1.9 The statistical-ecological approach recapitulated\n",
      "This chapter started with describing the difﬁculty of vision, and ended up proposing\n",
      "one particular solution, which can be called the statistical-ecological approach. The\n",
      "two basic ingredients in this approach are\n",
      "• Ecology: The visual system is only interested in properties that are important\n",
      "in a real environment. This is related to the concept of situatedness in cognitive\n",
      "sciences.\n",
      "• Statistics: Natural images have regularities. The regularities in the ecologically\n",
      "valid environment could be modelled by different formal frameworks, but statis-\n",
      "tical modelling seems to be the one that is most relevant.\n",
      "Thus, we take the following approach to visual modelling:\n",
      "1. Different sets of features are good for different kinds of data.\n",
      "2. The images that our eyes receive have certain statistical properties (regularities).\n",
      "3. The visual system has learned a model of these statistical properties.\n",
      "4. The model of the statistical properties enables (close to) optimal statistical infer-\n",
      "ence.\n",
      "5. The model of the statistical properties is reﬂected in the measurable properties of\n",
      "the visual system (e.g. receptive ﬁelds of the neurons)\n",
      "Most of this book will be concerned on developing different kinds of statistical\n",
      "models for natural images. These statistical models are based on very few theoretical\n",
      "assumptions, while they give rise to detailed quantitative predictions. We will show\n",
      "how these normative models are useful in two respects:\n",
      "1. They provide predictions that are largely validated by classic neuroscientiﬁc\n",
      "measurements. Thus, they provide concise and theoretically well-justiﬁed ex-\n",
      "planations of well-known measurements. This is the evidence that justiﬁes our\n",
      "normative modelling.\n",
      "2. Moreover, the models lead to new predictions of phenomena which have not yet\n",
      "been observed, thus enabling theory-driven neuroscience. This is the big promise\n",
      "of natural image statistics modelling.\n",
      "Another application of these models is in computer science and engineering.\n",
      "Such applications will not be considered in detail in this book: We hope we will\n",
      "have convinced the reader of the wide applicability of such methods. See below for\n",
      "references on this topic.\n",
      "1.10 References\n",
      "For textbook accounts of computer vision methods, such as edge detection algo-\n",
      "rithms, see, e.g. (Marr, 1982; Sonka et al, 1998; Gonzales and Woods, 2002); for\n",
      "information theory, see (Cover and Thomas, 2006). The approach of generative\n",
      "\n",
      "22\n",
      "1 Introduction\n",
      "models is presented in, e.g. (Grenander, 1976–1981; Kersten and Schrater, 2002;\n",
      "Mumford, 1994; Hinton and Ghahramani, 1997).\n",
      "For short reviews on using natural image statistics for visual modelling, see\n",
      "(Field, 1999; Simoncelli and Olshausen, 2001; Olshausen, 2003; Olshausen and\n",
      "Field, 2004; Hyv¨arinen et al, 2005b). For reviews on engineering applications of\n",
      "statistical models, see (Simoncelli, 2005).\n",
      "Historical references include (Mach, 1886; Pearson, 1892; Helmholtz, 1867;\n",
      "Shannon, 1948; Attneave, 1954; Barlow, 1961). See also (Barlow, 2001a,b) on a\n",
      "discussion on the history of redundancy reduction.\n",
      "\n",
      "Part I\n",
      "Background\n",
      "\n",
      "\n",
      "Chapter 2\n",
      "Linear ﬁlters and frequency analysis\n",
      "This chapter reviews some classical image analysis tools: linear ﬁltering, linear\n",
      "bases, frequency analysis, and space-frequency analysis. Some of the basic ideas\n",
      "are illustrated in Figure 2.1. These basic methods need to be understood before the\n",
      "results of statistical image models can be fully appreciated. The idea of processing\n",
      "of different frequencies is central in the reviewed tools. Therefore, a great deal of the\n",
      "following material is devoted to explaining what a frequency-based representation\n",
      "of images is, and why it is relevant in image analysis.\n",
      "2.1 Linear ﬁltering\n",
      "2.1.1 Deﬁnition\n",
      "Linear ﬁltering is a fundamental image-processing method in which a ﬁlter is ap-\n",
      "plied to an input image to produce an output image.\n",
      "Figure 2.2 illustrates the way in which the ﬁlter and the input image interact to\n",
      "form an output image: the ﬁlter is centered at each image location (x,y), and the\n",
      "pixel value of the output image O(x,y) is given by the linear correlation of the ﬁlter\n",
      "and the ﬁlter-size subarea of the image at coordinate (x,y). (Note that the word\n",
      "“correlation” is used here in a slightly different way than in the statistical context.)\n",
      "Letting W(x,y) denote a ﬁlter with size (2K +1)×(2K +1), I(x,y) the input image,\n",
      "and O(x,y) the output image, linear ﬁltering is given by\n",
      "O(x,y) =\n",
      "K\n",
      "∑\n",
      "x∗=−K\n",
      "K\n",
      "∑\n",
      "y∗=−K\n",
      "W(x∗,y∗)I(x+x∗,y+y∗).\n",
      "(2.1)\n",
      "An example of linear ﬁltering is shown in Figure 2.3.\n",
      "What Equation (2.1) means is that we “slide” the ﬁlter over the whole image and\n",
      "compute a weighted sum of the image pixel values, separately at each pixel location.\n",
      "25\n",
      "\n",
      "26\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "d)\n",
      "e)\n",
      "Fig. 2.1: The two classical image analysis tools reviewed in this chapter are linear ﬁltering b)-c) and\n",
      "space-frequency analysis d)-e). a) An input image. b) An example output of linear ﬁltering of a); in\n",
      "this case, the ﬁlter has retained medium-scaled vertical structures in the image. A more complete\n",
      "description of what a linear ﬁltering operation does is provided by the frequency representation\n",
      "(Section 2.2). c) An example of how the outputs of several linear ﬁlters can be combined in image\n",
      "analysis. In this case, the outputs of four ﬁlters have been processed nonlinearly and added together\n",
      "to form an edge image: in the image, lighter areas correspond to image locations with a luminance\n",
      "edge. This kind of a result could be used, for example, as a starting point to locate objects of a\n",
      "certain shape. d)–e) An example of space-frequency analysis, where the main idea is to analyze\n",
      "the magnitude of a frequency d) at different locations. The end result e) reﬂects the magnitude of\n",
      "this frequency at different points in the input image a). From the point of view of image analysis,\n",
      "this result suggests that the upper part of the image is of different texture than the lower part.\n",
      "\n",
      "2.1 Linear ﬁltering\n",
      "27\n",
      "a)\n",
      "W(−1,−1) W(−1,0) W(−1,1)\n",
      "W(0,−1)\n",
      "W(0,0)\n",
      "W(0,1)\n",
      "W(1,−1)\n",
      "W(1,0)\n",
      "W(1,1)\n",
      "b)\n",
      "x\n",
      "y\n",
      "input image I(x,y)\n",
      "coordinate (x,y)\n",
      "I(x−1,y−1) I(x−1,y) I(x−1,y+1)\n",
      "I(x,y−1)\n",
      "I(x,y)\n",
      "I(x,y+1)\n",
      "I(x+1,y−1) I(x+1,y) I(x+1,y+1)\n",
      "c)\n",
      "x\n",
      "y\n",
      "output image O(x,y)\n",
      "O(x,y) = W(−1,−1)I(x−1,y−1)+··· +W(1,1)I(x+1,y+1)\n",
      "= ∑1\n",
      "x∗=−1∑1\n",
      "y∗=−1W(x∗,y∗)I(x+x∗,y+y∗)\n",
      "Fig. 2.2: Linear ﬁltering is an operation that involves a ﬁlter (denoted here by W(x,y)) an input\n",
      "image (here I(x,y)) and yields an output image (here O(x,y)). The pixel value of the output im-\n",
      "age at location (x,y), that is, O(x,y), is given by the linear correlation of the ﬁlter W(x,y) and a\n",
      "ﬁlter-size subarea of the input image I(x,y) centered at coordinate (x,y). a) A 3 × 3 linear ﬁlter\n",
      "(template) W(x,y). b) An image I(x,y) and a 3×3 subarea of the image centered at location (x,y).\n",
      "c) The output pixel value O(x,y) is obtained by taking the pixel-wise multiplication of the ﬁlter a)\n",
      "and image subarea b), and summing this product over both x- and y-dimensions. Mathematically,\n",
      "O(x,y) = ∑x∗∑y∗W(x∗,y∗)I(x+x∗,y+y∗).\n",
      "\n",
      "28\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "Fig. 2.3: An example of linear ﬁltering. a) An input image. b) A ﬁlter. c) An output image.\n",
      "Visual inspection of a ﬁlter alone is usually not sufﬁcient to interpret a ﬁltering\n",
      "operation. This is also the case in the example in Figure 2.3: what does this ﬁltering\n",
      "operation actually accomplish? For a complete interpretation of a ﬁltering opera-\n",
      "tion a different type of mathematical language is needed. This language utilizes a\n",
      "frequency-based representation of images, as explained in Section 2.2 below.\n",
      "2.1.2 Impulse response and convolution\n",
      "The impulse response H(x,y) is the response of a ﬁlter to an impulse\n",
      "δ(x,y) =\n",
      "(\n",
      "1,\n",
      "if x = 0 and y = 0\n",
      "0,\n",
      "otherwise,\n",
      "(2.2)\n",
      "that is, to an image in which a single pixel is “on” (equal to 1) and the others are\n",
      "“off” (equal to 0). The impulse response characterizes the system just as well as\n",
      "the original ﬁlter coefﬁcients W(x,y). In fact, in frequency-based analysis of lin-\n",
      "ear ﬁltering, rather than ﬁltering with a ﬁlter, it is customary to work with im-\n",
      "pulse responses and an operation called convolution. This is because the frequency-\n",
      "modifying properties of the linear ﬁlter can be read out directly from the frequency-\n",
      "based representation of the impulse response, as will be seen below. (In general, this\n",
      "holds for any linear shift-invariant system, which are deﬁned in Section 20.1.)\n",
      "Based on the deﬁnition of ﬁltering in Equation (2.1), it is not difﬁcult to see that\n",
      "H(x,y) = W(−x,−y)\n",
      "(2.3)\n",
      "Thus, the impulse response H(x,y) is a “mirror image” of the ﬁlter weights W(x,y):\n",
      "the relationship is simply that of a 180◦rotation around the center of the ﬁlter. This\n",
      "is due to the change of signs of x∗and y∗; the impulse response is equal to one only if\n",
      "x+x∗= 0, which implies that only at points x∗= −x we have one and elsewhere the\n",
      "\n",
      "2.2 Frequency-based representation\n",
      "29\n",
      "impulse response is zero. For a ﬁlter that is symmetric with respect to this rotation,\n",
      "the impulse response is identical to the ﬁlter.\n",
      "The convolution of two images I1 and I2 is deﬁned as\n",
      "I1(x,y)∗I2(x,y) =\n",
      "∞\n",
      "∑\n",
      "x∗=−∞\n",
      "∞\n",
      "∑\n",
      "y∗=−∞\n",
      "I1(x−x∗,y−y∗)I2(x∗,y∗)\n",
      "(2.4)\n",
      "The only real difference to the deﬁnition of a ﬁltering operation in Eq. (2.1) is that\n",
      "we have minus signs instead of plus signs. Note that convolution is symmetric in\n",
      "the sense that we can change the order of I1 and I2, since by making the change in\n",
      "summation index x′\n",
      "∗= x−x∗and y′\n",
      "∗= y−y∗we get the same formula with the roles\n",
      "of I1 and I2 interchanged (this is left as an exercise).\n",
      "Therefore, we can express the ﬁltering operation using the impulse response\n",
      "(which is considered just another image here) and the convolution simply as\n",
      "O(x,y) = I(x,y)∗H(x,y)\n",
      "(2.5)\n",
      "which is a slight modiﬁcation of the original deﬁnition in Eq. (2.1). Introduction\n",
      "of this formula may seem like splitting hairs, but the point is that convolution is\n",
      "a well-known mathematical operation with interesting properties, and the impulse\n",
      "response is an important concept as well, so this formula shows how ﬁltering can be\n",
      "interpreted using these concepts.\n",
      "2.2 Frequency-based representation\n",
      "2.2.1 Motivation\n",
      "Frequency-based representation is a very useful tool in the analysis of image-\n",
      "processing systems. In particular, a frequency-based representation can be used to\n",
      "interpret what happens during linear ﬁltering: it describes linear ﬁltering as modiﬁ-\n",
      "cation of strengths (amplitudes) and spatial locations (phases) of frequencies (sinu-\n",
      "soidal components) that form the input image. As an example and sneak preview,\n",
      "Figures 2.8a)–d) on page 36 show how the ﬁltering operation of Figure 2.3 can be\n",
      "interpreted as attenuation of low and high frequencies, which can be seen in the\n",
      "output image as disappearance of large- and ﬁne-scale structures or, alternatively,\n",
      "preservation of medium-scale structures. This interpretation can be read out from\n",
      "Figure 2.8d), which shows the frequency ampliﬁcation map for this ﬁlter: this map,\n",
      "which is called the amplitude response of the ﬁlter, shows that both low frequen-\n",
      "cies (in the middle of the ﬁgure) and high frequencies (far from the middle) are\n",
      "attenuated; in the map, higher grey-scale value indicates larger amplitude response.\n",
      "In what follows we will ﬁrst describe the frequency-based representation, and\n",
      "then demonstrate its special role in the analysis and design of linear ﬁlters.\n",
      "\n",
      "30\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "2.2.2 Representation in one and two dimensions\n",
      "Figure 2.4 illustrates the main idea of the frequency-based representation in the case\n",
      "of one-dimensional data. In the usual (spatial) representation (Figure 2.4a), a signal\n",
      "is represented by a set of numbers at each point x = 0,...,N −1; in this example,\n",
      "N = 7. Therefore, to reconstruct the signal in Figure 2.4a), we need 7 numbers. In the\n",
      "frequency-based representation of this signal we also use 7 numbers to describe the\n",
      "contents of the signal, but in this case the numbers have a totally different meaning:\n",
      "they are the amplitudes and phases of sinusoidal components, that is, parameters A\n",
      "and ψ of signals of the form Acos(ωx + ψ), where ω is the frequency parameter,\n",
      "see Figure 2.4c.\n",
      "The theory of the discrete Fourier transform (treated in detail in Chapter 20)\n",
      "states that any signal of length 7 can be represented by the four amplitudes and the\n",
      "three phases of the four frequencies; the phase of the constant signal correspond-\n",
      "ing to ω = 0 is irrelevant because the constant signal does not change when it is\n",
      "shifted spatially. For a family of signals of given length N, the set of frequencies\n",
      "ωu, u = 0,...,U −1, employed in the representation is ﬁxed; in our example, these\n",
      "frequencies are listed in the second column of the table in Figure 2.4b). Overall, the\n",
      "frequency-based representation is given by the sum\n",
      "I(x) =\n",
      "U−1\n",
      "∑\n",
      "u=0\n",
      "Au cos(ωux+ψu),\n",
      "(2.6)\n",
      "where ωu are the frequencies and Au their amplitudes and ψu their phases.\n",
      "In the case of images – that is, two-dimensional data – the sinusoidal components\n",
      "are of the form\n",
      "Acos(ωxx+ωyy+ψ),\n",
      "(2.7)\n",
      "where ωx is the frequency in the x-direction and ωy in the y-direction. In order to\n",
      "grasp the properties of such a component, let us deﬁne vector ω = (ωx,ωy), and\n",
      "denote the dot-product by ⟨.⟩. Then, the component (2.7) can be written as\n",
      "Acos(ωxx+ωyy+ψ) = Acos(⟨(x,y),ω⟩+ψ)\n",
      "= Acos(\n",
      "∥ω∥\n",
      "|{z}\n",
      "“frequency”\n",
      "⟨(x,y), ω\n",
      "∥ω∥⟩\n",
      "|\n",
      "{z\n",
      "}\n",
      "projection\n",
      "+ψ),\n",
      "(2.8)\n",
      "which shows that computation of the argument of the cosine function can be inter-\n",
      "preted as a projection of coordinate vector (x,y) onto the direction of the vector ω,\n",
      "followed by a scaling with frequency ∥ω∥. Figure 2.5 illustrates this dependency of\n",
      "the frequency and the direction of the sinusoidal component on ωx and ωy.\n",
      "Figure 2.5 also illustrates why it is necessary to consider both positive and nega-\n",
      "tive values of either ωx or ωy: otherwise it is not possible to represent all directions\n",
      "in the (x,y)-plane. However, there is a certain redundancy in this representation. For\n",
      "example, the frequency pairs ω = (ωx,ωy) and −ω = (−ωx,−ωy) represent sinu-\n",
      "\n",
      "2.2 Frequency-based representation\n",
      "31\n",
      "a)\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−2\n",
      "−1012\n",
      "x\n",
      "c)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0.51\n",
      "1.52\n",
      "u\n",
      "Au\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "−3\n",
      "0\n",
      "3\n",
      "u\n",
      "ψu\n",
      "b)\n",
      "u ωx,u Au\n",
      "ψu\n",
      "Au cos(ωx,ux+ψu)\n",
      "∑u\n",
      "ℓ=1 Aℓcos(ωx,ℓx+ψℓ)\n",
      "0\n",
      "0\n",
      "0.42\n",
      "0\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−2\n",
      "−1012\n",
      "x\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−2\n",
      "−1012\n",
      "x\n",
      "1\n",
      "2π\n",
      "7\n",
      "0.91 2.36\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−2\n",
      "−1012\n",
      "x\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−2\n",
      "−1012\n",
      "x\n",
      "2\n",
      "4π\n",
      "7\n",
      "0.45 3.04\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−2\n",
      "−1012\n",
      "x\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−2\n",
      "−1012\n",
      "x\n",
      "3\n",
      "6π\n",
      "7\n",
      "1.24 -3.01\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−2\n",
      "−1012\n",
      "x\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−2\n",
      "−1012\n",
      "x\n",
      "Fig. 2.4: In frequency-based representation, a signal is represented as amplitudes and phases of\n",
      "sinusoidal components. a) A signal. b) A table showing how the signal in a) can be constructed\n",
      "from sinusoidal components. In the table, the number of sinusoidal components runs from 1 to\n",
      "4 (frequency index u runs from 0 to 3), and the rightmost column shows the cumulative sum\n",
      "of the sinusoidal components with frequencies ωx,u having amplitudes Au and phases φu. In the\n",
      "ﬁfth column, the grey continuous lines show the continuous frequency components from which\n",
      "the discrete versions have been sampled. Here the frequency components are added in increasing\n",
      "frequency, that is, ωx,u1 > ωx,u2 if u1 > u2. c) A frequency-based representation for the signal in a):\n",
      "the signal is represented by the set of frequency amplitudes Au, which is also called the amplitude\n",
      "spectrum (on the left), and the set of frequency phases ψu (on the right) of the corresponding\n",
      "frequencies ωx,u, u = 0,...,3. Note that the phase of the constant component u = 0 corresponding\n",
      "to frequency ωx,0 = 0 is irrelevant; thus 7 numbers are needed in the frequency-based representation\n",
      "of the signal, just as in the usual representation a).\n",
      "\n",
      "32\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "soidal components that have the same direction and frequency, because ω and −ω\n",
      "have the same direction and length. So, it can be seen that any half of the (ωx,ωy)-\n",
      "plane sufﬁces to represent all directions. In practice it has become customary to use\n",
      "a redundant frequency representation which employs the whole (ωx,ωy)-plane, that\n",
      "is, negative and positive parts of both the ωx- and ωy-axis.1\n",
      "ω = (ωx,ωy)\n",
      "∥ω∥\n",
      "ω\n",
      "∥ω∥\n",
      "I(x,y)\n",
      "( 1\n",
      "10, 1\n",
      "10)\n",
      "√\n",
      "2\n",
      "10 ≈0.14\n",
      "ωx\n",
      "ωy\n",
      "ω\n",
      "∥ω∥\n",
      "( 1\n",
      "10,−1\n",
      "10)\n",
      "√\n",
      "2\n",
      "10 ≈0.14\n",
      "ωx\n",
      "ωy\n",
      "ω\n",
      "∥ω∥\n",
      "( 6\n",
      "10, 3\n",
      "10)\n",
      "3\n",
      "√\n",
      "5\n",
      "10 ≈0.67\n",
      "ωx\n",
      "ωy\n",
      "ω\n",
      "∥ω∥\n",
      "Fig. 2.5: In an equation I(x,y) = cos\n",
      "\u0000ωxx + ωyy\n",
      "\u0001\n",
      "of a two-dimensional sinusoidal image, the fre-\n",
      "quencies ωx and ωy determine the direction and the frequency of the component. More speciﬁcally,\n",
      "if ω = (ωx,ωy), then the length of ω determines the frequency of the component, and the direction\n",
      "of ω determines the direction of the component. This is illustrated here for three different (ωx,ωy)\n",
      "pairs; the sinusoidal components are of size 128 × 128 pixels. Notice that in the plots in the third\n",
      "column, ωy runs from top to bottom because of the convention that in images the y-axis runs in this\n",
      "direction. See equation (2.8) on page 30 for details.\n",
      "1 In fact, because cosine is an even function — that is, cos(−α) = cos(α) — the frequency com-\n",
      "ponents corresponding to these two frequency pairs are identical when Acos[⟨(x,y),ω⟩+ψ] =\n",
      "Acos[−(⟨(x,y),ω⟩+ψ)] = Acos[⟨(x,y),(−ω)⟩+(−ψ)], that is, when their amplitudes are the\n",
      "same and their phases are negatives or each other. Therefore, when employing the whole (ωx,ωy)-\n",
      "plane, the amplitude of a frequency component are customarily split evenly among the frequency\n",
      "pairs ω and −ω with phases ψ and −ψ.\n",
      "\n",
      "2.2 Frequency-based representation\n",
      "33\n",
      "Figure 2.6 shows an example of the resulting frequency representation. Again,\n",
      "the theory of discrete Fourier transform (see Chapter 20) states that any image of\n",
      "size 3 × 3 pixels can be represented by ﬁve amplitudes and four phases, a total of\n",
      "9 numbers as in the usual spatial representation; the phase of the constant signal\n",
      "corresponding to ω = (0,0) is irrelevant as before.\n",
      "Note on terminology\n",
      "The square of the amplitude A2 is also called the Fourier\n",
      "energy or power, and when it is computed for many different frequencies, we get\n",
      "what is called the power spectrum. The power spectrum is a classic way of charac-\n",
      "terizing which frequencies are present and with which “strengths” in a given signal\n",
      "or image. If the original amplitudes are used, we talk about the amplitude spectrum\n",
      "(Fig. 2.4 c). It should be noted that the terminology of frequency-based analysis is\n",
      "not very well standardized, so other sources may use different terminology.\n",
      "2.2.3 Frequency-based representation and linear ﬁltering\n",
      "Sinusoidals also play a special role in the analysis and design of linear ﬁlters. What\n",
      "makes sinusoidals special is the fact that when a sinusoidal is input into a linear\n",
      "ﬁlter, the response is a sinusoidal with the same frequency, but possibly different\n",
      "amplitude and phase. Figure 2.7 illustrates this phenomenon. Furthermore, both the\n",
      "ampliﬁcation factor of the amplitude and the shift in the phase depend only on the\n",
      "frequency of the input, and not its amplitude or phase (see Chapter 20 for a detailed\n",
      "discussion).\n",
      "For a linear ﬁlter with impulse response H(x,y), let\n",
      "\f\f ˜H(ωx,ωy)\n",
      "\f\f denote the am-\n",
      "plitude magniﬁcation factor of the system for horizontal frequency ωx and vertical\n",
      "frequency ωy, and ∠˜H(ωx,ωy) denote the phase shift of the ﬁlter. Then if the input\n",
      "signal has frequency-based representation\n",
      "I(x,y) = ∑\n",
      "ωx ∑\n",
      "ωy\n",
      "Aωx,ωy cos\n",
      "\u0000ωxx+ωyy+ψωx,ωy\n",
      "\u0001\n",
      ",\n",
      "(2.9)\n",
      "where the sum over ωx and ωy is here and below taken over both positive and nega-\n",
      "tive frequencies, the response of the linear ﬁlter has the following frequency-based\n",
      "representation\n",
      "O(x,y) = H(x,y)∗I(x,y)\n",
      "= ∑\n",
      "ωx ∑\n",
      "ωy\n",
      "\f\f ˜H(ωx,ωy)\n",
      "\f\fAωx,ωy\n",
      "|\n",
      "{z\n",
      "}\n",
      "amplitude\n",
      "cos\n",
      "\u0000ωxx+ωyy+ψωx,ωy +∠˜H(ωx,ωy)\n",
      "|\n",
      "{z\n",
      "}\n",
      "phase\n",
      "\u0001\n",
      ".\n",
      "(2.10)\n",
      "The amplitude magniﬁcation factor\n",
      "\f\f ˜H(ωx,ωy)\n",
      "\f\f is called the amplitude response\n",
      "of the linear system, while the phase shift ∠˜H(ωx,ωy) is called the phase response.\n",
      "\n",
      "34\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "a)\n",
      "x\n",
      "y\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "\n",
      "\n",
      "−0.70\n",
      "0.59\n",
      "0.67\n",
      "0.01 −0.25 −0.08\n",
      "−0.78\n",
      "0.48\n",
      "0.89\n",
      "\n",
      "\n",
      "b)\n",
      "ωx\n",
      "ωy\n",
      "−2.1 0 2.1\n",
      "2.1\n",
      "0\n",
      "−2.1\n",
      "\n",
      "\n",
      "1.81 0.89 1.34\n",
      "2.68 0.82 2.68\n",
      "1.34 0.89 1.81\n",
      "\n",
      "\n",
      "c)\n",
      "\n",
      "\n",
      "2.06 −1.07\n",
      "1.95\n",
      "−2.93\n",
      "0.00\n",
      "2.93\n",
      "−1.95\n",
      "1.07 −2.06\n",
      "\n",
      "\n",
      "d)\n",
      "location of the component\n",
      "in the (ωx,ωy)-plane\n",
      "ωx\n",
      "ωy\n",
      "−2.1\n",
      "0\n",
      "2.1\n",
      "2.1\n",
      "0\n",
      "−2.1\n",
      "ωx\n",
      "ωy\n",
      "−2.1\n",
      "0\n",
      "2.1\n",
      "2.1\n",
      "0\n",
      "−2.1\n",
      "ωx\n",
      "ωy\n",
      "−2.1\n",
      "0\n",
      "2.1\n",
      "2.1\n",
      "0\n",
      "−2.1\n",
      "ωx\n",
      "ωy\n",
      "−2.1\n",
      "0\n",
      "2.1\n",
      "2.1\n",
      "0\n",
      "−2.1\n",
      "the component\n",
      "x\n",
      "y\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "x\n",
      "y\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "x\n",
      "y\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "x\n",
      "y\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "Fig. 2.6: An example of a two-dimensional frequency representation. a) The grey-scale (left) and\n",
      "numerical (right) representation of an image of size 3 × 3 pixels. b) Amplitude information of\n",
      "the frequency representation of the image in a): the grey-scale (left) and numerical (right) repre-\n",
      "sentation of the amplitudes of the different frequencies. Notice the symmetries/redundancies: the\n",
      "amplitude of frequency ω is the same as the amplitude of frequency −ω. c) Phase information of\n",
      "the frequency representation of the image in a); the axis of this representation are the same as in b).\n",
      "Notice the symmetries/redundancies: the phase of ω is the negative of the phase of −ω. d) Four\n",
      "examples of the actual sinusoidal components that make up the image in a) in the frequency repre-\n",
      "sentation. In each column, the ﬁrst row shows the location of the component in the (ωx,ωy)-plane,\n",
      "while the second row shows the actual component. The leftmost component is the constant com-\n",
      "ponent corresponding to ω = (0,0). The second component is a horizontal frequency component.\n",
      "Because of the symmetry in the frequency representation, the third and the fourth components are\n",
      "identical. Notice that the second component (the horizontal frequency component) is stronger than\n",
      "the other components, which can also be seen in the amplitude representation in b).\n",
      "\n",
      "2.2 Frequency-based representation\n",
      "35\n",
      "a)\n",
      "I(x)\n",
      "H(x)\n",
      "H(x)∗I(x)\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "x\n",
      "b)\n",
      "I(x,y)\n",
      "H(x,y)\n",
      "H(x,y)∗I(x,y)\n",
      "Fig. 2.7: Sinusoidal components play a special role in the analysis and design of linear systems,\n",
      "because if a sinusoidal is input into a linear ﬁlter, the output is a sinusoidal with the same frequency\n",
      "but possibly different amplitude and phase. a) Two examples of this phenomenon are shown here\n",
      "in the one-dimensional case, one on each row. The left column shows the input signal, which is\n",
      "here assumed to be a sinusoidal of inﬁnite duration. The middle column shows a randomly selected\n",
      "impulse response, and the column on the right the response of this linear system to the sinusoidal.\n",
      "Notice the different scale in the output signals, which is related to the amplitude change taking\n",
      "place in the ﬁltering. b) An illustration of the two-dimensional case, with a 64×64 -pixel input on\n",
      "the left, a random 11×11 -pixel impulse response in the middle, and the output on the right.\n",
      "The way these quantities are determined for a linear ﬁlter is described shortly below;\n",
      "for now, let us just assume that they are available.\n",
      "Figure 2.8 shows an example of the insight offered by the frequency representa-\n",
      "tion of linear ﬁltering (equation (2.10)). The example shows how a linear ﬁlter can\n",
      "be analyzed or designed by its amplitude response (the phase response is zero for\n",
      "all frequencies in this example). Notice that while relating the forms of the ﬁlters\n",
      "themselves (Figures 2.8b and f) to the end result of the ﬁltering is very difﬁcult,\n",
      "describing what the ﬁlter does is straightforward once the frequency-based repre-\n",
      "sentation (Figures 2.8d and e) is available.\n",
      "How can the amplitude and phase responses of a linear system be determined?\n",
      "Consider a situation where we feed into the system a signal which contains a mixture\n",
      "of all frequencies with unit amplitudes and zero phases:\n",
      "\n",
      "36\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "d)\n",
      "ωx\n",
      "ωy\n",
      "−2.2\n",
      "−1.1\n",
      "0\n",
      "1.1\n",
      "2.2\n",
      "2.2\n",
      "1.1\n",
      "0\n",
      "−1.1\n",
      "−2.2\n",
      "e)\n",
      "ωx\n",
      "ωy\n",
      "−2.2\n",
      "−1.1\n",
      "0\n",
      "1.1\n",
      "2.2\n",
      "2.2\n",
      "1.1\n",
      "0\n",
      "−1.1\n",
      "−2.2\n",
      "f)\n",
      "g)\n",
      "Fig. 2.8: An example of the usefulness of frequency-based representation in the analysis and design\n",
      "of linear ﬁlters. a) An input image. b) A ﬁlter of size 17×17 pixels. c) The output of linear ﬁltering\n",
      "of image a) with ﬁlter b). d) The amplitude response of the ﬁlter in b); in this representation dark\n",
      "pixels indicate amplitude response values close to zero and bright pixels values close to one. The\n",
      "amplitude response shows that the ﬁlter attenuates low and high frequencies, that is, frequencies\n",
      "which are either close to the origin (ωu,ωv) = (0,0) or far away from it. This can be veriﬁed in\n",
      "c), where medium-scaled structures have been preserved in the image, while details and large-\n",
      "scale grey-scale variations are no longer visible. The phase response of the ﬁlter is zero for all\n",
      "frequencies. e) Assume that we want to design a ﬁlter that has a reverse effect than the ﬁlter shown\n",
      "in b): our new ﬁlter attenuates medium frequencies. The ﬁlter can be designed by specifying its\n",
      "amplitude and phase response. The amplitude response is shown here, the phase response is zero\n",
      "for all frequencies. f) The ﬁlter corresponding to the frequency-based representation e). g) The\n",
      "result obtained when the ﬁlter f) is applied to the image in a). The results is as expected: the ﬁlter\n",
      "preserves details and large-scale grey-scale variations, while medium-scale variations are no longer\n",
      "visible. Notice that just by examining the ﬁlters themselves (b and f) it is difﬁcult to say what the\n",
      "ﬁlters do, while this becomes straightforward once the frequency-based representations (d and e)\n",
      "are available.\n",
      "I(x,y) = ∑\n",
      "ωx ∑\n",
      "ωy\n",
      "cos\n",
      "\u0000ωxx+ωyy\n",
      "\u0001\n",
      ".\n",
      "(2.11)\n",
      "Then, applying equation (2.10), the frequency-based representation of the output is\n",
      "O(x,y) = H(x,y)∗I(x,y) =∑\n",
      "ωx ∑\n",
      "ωy\n",
      "\f\f ˜H(ωx,ωy)\n",
      "\f\f\n",
      "|\n",
      "{z\n",
      "}\n",
      "amplitude\n",
      "cos\n",
      "\u0000ωxx+ωyy+∠˜H(ωx,ωy)\n",
      "|\n",
      "{z\n",
      "}\n",
      "phase\n",
      "\u0001\n",
      ".\n",
      "(2.12)\n",
      "\n",
      "2.2 Frequency-based representation\n",
      "37\n",
      "k 0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "ωx,k−2π\n",
      "3 −2π\n",
      "3 −2π\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2π\n",
      "3\n",
      "2π\n",
      "3\n",
      "2π\n",
      "3\n",
      "ωy,k−2π\n",
      "3\n",
      "0\n",
      "2π\n",
      "3 −2π\n",
      "3\n",
      "0\n",
      "2π\n",
      "3 −2π\n",
      "3\n",
      "0\n",
      "2π\n",
      "3\n",
      "cos\n",
      "\u0000ωx,kx+ωy,ky\n",
      "\u0001\n",
      "∑k\n",
      "ℓ=0 cos\n",
      "\u0000ωx,ℓx+ωy,ℓy\n",
      "\u0001\n",
      "Fig. 2.9: An image containing all frequencies with unit amplitudes and zero phases is an impulse.\n",
      "Here, the different frequency components are being added from left to right; the right-most image\n",
      "is an impulse response.\n",
      "In other words, the amplitude and phase responses of the linear system can be read\n",
      "from the frequency-based representation of the output O(x,y). What remains to be\n",
      "determined is what kind of a signal the signal I(x,y) in Equation (2.11) is. The theory\n",
      "of the Fourier transform states that the image obtained when all frequencies with\n",
      "identical amplitudes and zero phases are added together is an impulse. Figure 2.9\n",
      "illustrates this when image size is 3 × 3 pixels. To summarize, when we feed an\n",
      "impulse into a linear ﬁlter,\n",
      "• from the point of view of frequency-based description, we are giving the system\n",
      "an input with equal amplitudes of all frequencies at phase zero\n",
      "• the linear system modiﬁes the amplitudes and phases of these frequencies ac-\n",
      "cording to the amplitude and phase response of the system\n",
      "• the amplitude and phase response properties can be easily read out from the im-\n",
      "pulse response, since the amplitudes of the input were equal and phases were all\n",
      "zero.\n",
      "In other words, the amplitude and phase responses of a linear ﬁlter are obtained\n",
      "from the frequency-based representation of the impulse response: the amplitude\n",
      "responses are the amplitudes of the frequencies, and the phase responses are the\n",
      "phases of the frequencies. An example of this principle is shown in Figure 2.8: the\n",
      "amplitude response images d) and e) are in fact the amplitudes of the frequency-\n",
      "based representations of the impulse responses b) and f).\n",
      "2.2.4 Computation and mathematical details\n",
      "Above we have outlined the nature of the frequency-based representation in the one-\n",
      "and two-dimensional case, and the usefulness of this representation in the design\n",
      "and analysis of linear systems. The material presented so far should therefore pro-\n",
      "vide the reader with the knowledge needed to understand what the frequency-based\n",
      "representation is, and why it is used.\n",
      "\n",
      "38\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "However, a number of questions have been left unanswered in the text above,\n",
      "including the following:\n",
      "• What exactly are the values of the frequencies ω used in the frequency-based\n",
      "representation?\n",
      "• How is the frequency-based representation computed?\n",
      "• What guarantees that a frequency-based representation exists?\n",
      "The set of mathematical tools used to deﬁne and analyze frequency-based rep-\n",
      "resentations are part of mathematics called Fourier analysis. In particular, Fourier\n",
      "transforms are used to convert data and impulse responses to and from frequency-\n",
      "based representation. There are different types of Fourier transforms for different\n",
      "purposes: continuous/discrete and ﬁnite/inﬁnite data. When working with digital im-\n",
      "ages, the most important Fourier transform is the discrete Fourier transform (DFT),\n",
      "which is particularly suited for representation of discrete and ﬁnite data in comput-\n",
      "ers. The basics of DFT are described in Chapter 20. The computational implementa-\n",
      "tion of DFT is usually through a particular algorithm called Fast Fourier Transform,\n",
      "or FFT.\n",
      "The DFT has fairly abstract mathematical properties, because complex numbers\n",
      "are employed in the transform. The results of the DFT are, however, quite easily un-\n",
      "derstood in terms of the frequency-based representation: for example, Figure 2.8d)\n",
      "was computed by taking the DFT of the ﬁlter in Figure 2.8b), and then showing the\n",
      "magnitudes of the complex numbers of the result of the DFT.\n",
      "A working knowledge of the frequency-based representation is not needed in\n",
      "reading this book: it is sufﬁcient to understand what the frequency-based represen-\n",
      "tation is and why it is used. If you are interested in working with frequency-based\n",
      "representations, then studying the DFT is critical, because the DFT has some coun-\n",
      "terintuitive properties that must be known when working with results of transforms;\n",
      "for example, the DFT assumes that the data (signal or image) is periodic, which\n",
      "causes periodicity effects when ﬁltering is done in the frequency-based representa-\n",
      "tion.\n",
      "2.3 Representation using linear basis\n",
      "Now we consider a general and widely-used framework for image representation: a\n",
      "linear basis. We will also see how frequency-based representation can be seen as a\n",
      "special case in this framework.\n",
      "2.3.1 Basic idea\n",
      "A traditional way to represent grey-scale images is the pixel-based representation,\n",
      "where the value of each pixel denotes the grey-scale value at that particular loca-\n",
      "\n",
      "2.3 Representation using linear basis\n",
      "39\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "Fig. 2.10: Three different 3 × 2 images consisting of vertical lines: a) f1 b) f2 c) I3. Here black\n",
      "denotes a grey-scale value of zero, light grey a grey-scale value of 4, and the darker grey a grey-\n",
      "scale value of 2.\n",
      "tion in the image. For many different purposes, more convenient representations of\n",
      "images can be devised.\n",
      "Consider the three 3 × 2 images f1, f2 and I3 shown in Figure 2.10. The tradi-\n",
      "tional pixel-based representations of the images are\n",
      "f1 =\n",
      "\u0014 4 0 0\n",
      "4 0 0\n",
      "\u0015\n",
      "f2 =\n",
      "\u0014\n",
      "0 0 4\n",
      "0 0 4\n",
      "\u0015\n",
      "I3 =\n",
      "\u0014 2 0 2\n",
      "2 0 2\n",
      "\u0015\n",
      "These images consist of vertical lines with different grey-scale values. A com-\n",
      "pact way to describe a set of images containing only vertical lines is to deﬁne the\n",
      "following basis images\n",
      "B1 =\n",
      "\u00141 0 0\n",
      "1 0 0\n",
      "\u0015\n",
      "B2 =\n",
      "\u0014\n",
      "0 1 0\n",
      "0 1 0\n",
      "\u0015\n",
      "B3 =\n",
      "\u00140 0 1\n",
      "0 0 1\n",
      "\u0015\n",
      ",\n",
      "and then represent each image as a weighted sum of these basis images. For example,\n",
      "f1 = 4B1, f2 = 4B3 and I3 = 2B1 + 2B3. Such a representation could convey more\n",
      "information about the inherent structure in these images. Also, if we had a very\n",
      "large set of such images, consisting of only vertical lines, and were interested in\n",
      "compressing our data, we could store the basis images, and then for each image just\n",
      "save the three coefﬁcients of the basis images.\n",
      "This simple example utilized a special property of the images f1, f2 and I3, that\n",
      "is, the fact that in this case each image contains only vertical lines. Note that not\n",
      "every possible 3 × 2 image can be represented as a weighted sum of the basis im-\n",
      "ages B1, B2 and B3 (one example is an image with a single non-zero pixel at any\n",
      "\n",
      "40\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "image location). This kind of a basis is called an undercomplete basis. Usually the\n",
      "word basis is used to refer to a complete basis, a basis which – when used in the\n",
      "form of a weighted sum – can be used to represent any image. For the set of 3 × 2\n",
      "images, one example of a complete basis is the set of 6 images with a single one at\n",
      "exactly one image location (x,y). This basis is typically associated with the tradi-\n",
      "tional pixel-based image representation: each pixel value denotes the coefﬁcient of\n",
      "the corresponding basis image.\n",
      "A particularly important case is an orthogonal basis. Then, the coefﬁcients in the\n",
      "basis simply equal the dot-products with the basis vectors. For more on bases and\n",
      "orthogonality, see Section 19.6.\n",
      "The use of different bases in the representation of images has several important\n",
      "areas of application in image processing. Two of these were mentioned already: the\n",
      "description and analysis of the structure of images, and image compression. Other\n",
      "important applications are in the domain of image processing systems: different\n",
      "image representations are central in the analysis of these systems (including the\n",
      "analysis of the visual system), design of such systems, and in their efﬁcient imple-\n",
      "mentation.\n",
      "2.3.2 Frequency-based representation as a basis\n",
      "Now we consider how frequency-based representation can be rephrased as ﬁnding\n",
      "the coefﬁcients in a basis. Consider the situation where we want to analyze, in a\n",
      "given signal, the amplitude A and phase ψ of a sinusoidal component\n",
      "Acos(ωx+ψ).\n",
      "(2.13)\n",
      "The key point here is that instead of determining A and ψ directly, we can determine\n",
      "the coefﬁcients C and S of a cosine and sine signal with (centered) phase zero:\n",
      "Ccos(ωx)+Ssin(ωx).\n",
      "(2.14)\n",
      "To show this we will demonstrate that there is a one-to-one correspondence between\n",
      "signals of the form (2.13) and of the form (2.14). First, a given sinusoidal of the\n",
      "form (2.13) can be represented in form (2.14) as follows:\n",
      "Acos(ωx+ψ) = A(cos(ωx)cosψ −sin(ωx)sinψ)\n",
      "= Acosψ\n",
      "| {z }\n",
      "=C\n",
      "cos(ωx)+A(−sinψ)\n",
      "|\n",
      "{z\n",
      "}\n",
      "=S\n",
      "sin(ωx)\n",
      "= Ccos(ωx)+Ssin(ωx).\n",
      "(2.15)\n",
      "Conversely, if we are given a signal in form (2.14), the derivation (2.15) can be\n",
      "reversed (this is left as an exercise), so that we get\n",
      "\n",
      "2.4 Space-frequency analysis\n",
      "41\n",
      "A =\n",
      "p\n",
      "C2 +S2,\n",
      "(2.16)\n",
      "ψ = −atan S\n",
      "C.\n",
      "(2.17)\n",
      "Thus, to analyze the amplitude and phase of frequency ω in a given signal, it sufﬁces\n",
      "to ﬁnd coefﬁcients C and S in equation Ccos(ωx)+Ssin(ωx); after the coefﬁcients\n",
      "have been computed, equations (2.16) and (2.17) can be used to compute the am-\n",
      "plitude and phase. In particular, the square of the amplitude (“Fourier power” or\n",
      "“Fourier energy”) is obtained as the sum of squares of the coefﬁcients.\n",
      "The formula in Equation (2.16) is also very interesting from a neural modelling\n",
      "viewpoint because it shows how to compute the amplitude using quite simple op-\n",
      "erations, since computation of the coefﬁcients in a linear basis is a linear operation\n",
      "(at least if the basis is complete). Computation of Fourier energy in a given fre-\n",
      "quency thus requires two linear operations, followed by squaring, and summing of\n",
      "the squares. As we will see in Chapter 3, something similar to linear Fourier op-\n",
      "erators seems to be computed in the early parts of visual processing, which makes\n",
      "computation of Fourier energy rather straightforward.\n",
      "How are the coefﬁcients C and S then computed? The key is orthogonality. The\n",
      "signals cos(ωx) and sin(ωx) are orthogonal, at least approximately. So, the coefﬁ-\n",
      "cients C and S are simply obtained as the dot-products of the signal with the basis\n",
      "vectors given by the cos and sin functions.\n",
      "In fact, Discrete Fourier Transform can be viewed as deﬁning a basis with such\n",
      "cos and sin functions with many different frequencies, and those frequencies are\n",
      "carefully chosen so that the sinusoidals are exactly orthogonal.Then, the coefﬁcients\n",
      "for all the sin and cos functions, in the different frequencies, can be computed as the\n",
      "dot-products\n",
      "∑\n",
      "x\n",
      "I(x)cos(ωx) and ∑\n",
      "x\n",
      "I(x)sin(ωx)\n",
      "(2.18)\n",
      "The idea generalizes to two dimensions (images) in the same way as frequency-\n",
      "based analysis was shown to generalize above. More details on the DFT can be\n",
      "found in Chapter 20.\n",
      "2.4 Space-frequency analysis\n",
      "2.4.1 Introduction\n",
      "The frequency representation utilizes global sinusoidal gratings, that is, components\n",
      "which span the whole image (see, e.g., Figure 2.5). This is particularly useful for the\n",
      "analysis and design of linear ﬁlters. However, because of the global nature of sinu-\n",
      "soidals, the frequency representation tells us nothing about the spatial relationship\n",
      "of different frequencies in an image. This is illustrated in Figures 2.11a) and b),\n",
      "which show an image and the amplitudes of its frequency representation. The upper\n",
      "part of the image contains grass, which tends to have a more vertical orientation\n",
      "\n",
      "42\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "a)\n",
      "b)\n",
      "ωx\n",
      "ωy\n",
      "−2.5\n",
      "−1.3\n",
      "0\n",
      "1.3\n",
      "2.5\n",
      "2.5\n",
      "1.3\n",
      "0\n",
      "−1.3\n",
      "−2.5\n",
      "c)\n",
      "d)\n",
      "e)\n",
      "f)\n",
      "Fig. 2.11: The main idea in space-frequency analysis is to consider the amplitudes/phases of differ-\n",
      "ent frequencies at different locations in an image. a) An image. Notice how different areas of the\n",
      "image differ in their frequency contents. b) The standard frequency representation: the amplitudes\n",
      "of the different frequencies that make up the frequency representation of the image in a). Note that\n",
      "while this representation suggests that fairly high horizontal frequencies are present in the image,\n",
      "it does not convey information about the spatial location of different frequencies. For purposes of\n",
      "visibility, the amplitudes of different frequencies have been transformed with logarithmic mapping\n",
      "before display. c) A spatially localized non-negative windowing function. d) A localized area of\n",
      "the image can be obtained by multiplying the image a) with the windowing function c). e) In this\n",
      "example the amplitude (strength) of this horizontal frequency is analyzed at each point in the im-\n",
      "age. f) Applying the weighting scheme c)-d) at each point in the image a), and then analyzing the\n",
      "amplitude of the frequency e) at each of these points results in this spatial map of the amplitude of\n",
      "the frequency. As can be seen, the frequency tends to have larger amplitudes in the upper part of\n",
      "the image, as can be expected.\n",
      "and sharper structure than the lower part of the image. The amplitudes of the fre-\n",
      "quency representation (Figures 2.11b) show that many horizontal or near-horizontal\n",
      "frequencies – that is, frequencies in the vicinity of axis ωy = 0 – have a relatively\n",
      "large amplitude, even at fairly high frequencies. (Notice that structures with vertical\n",
      "lines correspond to horizontal frequencies.) From the amplitude spectrum there is,\n",
      "however, no way to tell the spatial location of these frequencies.\n",
      "The spatial locations of different frequencies can contain important information\n",
      "about the image. In this example, if we are able to locate those areas which tend to\n",
      "have more horizontal frequencies, we can use that information, for example, to facil-\n",
      "itate the identiﬁcation of the grass area in the image. How can the spatial locations\n",
      "of these frequencies be found? A straightforward way to do this is to analyze the fre-\n",
      "\n",
      "2.4 Space-frequency analysis\n",
      "43\n",
      "quency contents of limited spatial areas. Figures 2.11c)–f) illustrate this idea. The\n",
      "original image (Figure 2.11a) is multiplied with a non-negative windowing function\n",
      "(Figure 2.11c) to examine the frequency contents of a spatially localized image area\n",
      "(Figure 2.11d). For example, for the horizontal frequency shown in Figure 2.11e), a\n",
      "spatial map of the amplitude of this frequency is shown in Figure 2.11f); the map has\n",
      "been obtained by applying the weighting scheme (Figure 2.11c and d) at every point\n",
      "of the image, and analyzing the amplitude of the frequency in the localized image\n",
      "area. Now, we see that in Figure 2.11f) that the different areas (grass vs. water) are\n",
      "clearly separated by this space-frequency analysis.\n",
      "In the case of space-frequency analysis, the computational operations underlying\n",
      "the analysis are of great interest to us. This is because some important results ob-\n",
      "tained with statistical modelling of natural images can be interpreted as performing\n",
      "space-frequency analysis, so the way the analysis is computed needs to be under-\n",
      "stood to appreciate these results. Because of this connection, we need to delve a\n",
      "little deeper into the mathematics.\n",
      "Before going into the details, we ﬁrst state the main result: space-frequency anal-\n",
      "ysis can be done by the method illustrated in Figure 2.12: by ﬁltering the image\n",
      "with two different localized sinusoidal ﬁlters, and computing the amplitudes and\n",
      "phases (in this example only the amplitudes) from the outputs of these two ﬁlters.\n",
      "The following section explains the mathematics behind this method.\n",
      "2.4.2 Space-frequency analysis and Gabor ﬁlters\n",
      "Consider a situation where we want to analyze the local frequency contents of an\n",
      "image: we want to compute the amplitude and phase for each location (x0,y0). Al-\n",
      "ternatively, we could compute a set of coefﬁcients C(x0,y0) and S(x0,y0) as in Sec-\n",
      "tion 2.3.2, which now are functions of the location. The analysis is made local by\n",
      "applying a weighting function, say W(x,y), centered at (x0,y0) before the analysis.\n",
      "We can simply modify the analysis in Section 2.3.2 by centering the signal f\n",
      "around the point (x0,y0) and weighting it by w before the analysis. Thus, we get a\n",
      "formula for the coefﬁcient C at a given point:\n",
      "C(x0,y0) ≈∑\n",
      "x ∑\n",
      "y\n",
      "I(x,y)W(x−x0,y−y0)cos(ωx(x−x0)+ωy(y−y0)).\n",
      "(2.19)\n",
      "and similarly for S(x0,y0). Note that the order in which we multiply the three images\n",
      "(image f, weighting image w, sinusoidal cos) inside the sum is irrelevant. Therefore\n",
      "it does not make any difference whether we apply the weighting to the image I(x,y)\n",
      "or to the sinusoidal cos(ωx(x −x0) + ωy(y −y0)). If we deﬁne a new weighting\n",
      "function\n",
      "W2(x,y) = W(x,y)cos(ωxx+ωyy),\n",
      "(2.20)\n",
      "equation (2.19) becomes\n",
      "\n",
      "44\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "C(x0,y0) ≈∑\n",
      "x ∑\n",
      "y\n",
      "I(x,y)W2(x−x0,y−y0).\n",
      "(2.21)\n",
      "Equations (2.20) and (2.21) show that computation of coefﬁcients C(x0,y0) can be\n",
      "approximated by ﬁltering the image with a ﬁlter that is the product of a sinusoidal\n",
      "cos(ωxx + ωyy) and the weighting window. Similar analysis applies to coefﬁcients\n",
      "S(x0,y0), except that in that case the sinusoidal is sin(ωxx+ωyy). Because the mag-\n",
      "nitude of the weighting function W(x,y) typically falls off quite fast from the origin\n",
      "(x,y) = (0,0), computational savings can be obtained by truncating the ﬁlter to zero\n",
      "for (x,y) far away from the origin.\n",
      "To summarize the result in an example, Figure 2.12 shows how the ampli-\n",
      "tude map of Figure 2.11f on page 42 was computed: C(x0,y0) and S(x0,y0) were\n",
      "computed by ﬁltering the image with weighted versions of cos(ωxx + ωyy) and\n",
      "sin(ωxx + ωyy), respectively, and the amplitude map A(x0,y0) was obtained by\n",
      "A(x0,y0) =\n",
      "p\n",
      "C(x0,y0)2 +S(x0,y0)2.\n",
      "The two ﬁlters used in the computation of C(x0,y0) and S(x0,y0) are often called\n",
      "a quadrature-phase pair. This is because sin(x+ π\n",
      "2 ) = cos(x), so the two ﬁlters are\n",
      "W(x,y)cos(ωxx + ωyy) and W(x,y)cos(ωxx + ωyy + π\n",
      "2 ), that is, they are otherwise\n",
      "identical expect for a a phase difference of one quarter of a whole cycle: 2π\n",
      "4 = π\n",
      "2 .\n",
      "When the weighting function is a gaussian window, which in the one-dimensional\n",
      "case is of the form\n",
      "Wσ(x) = 1\n",
      "d e−x2\n",
      "σ2 ,\n",
      "(2.22)\n",
      "the resulting ﬁlter is called a Gabor ﬁlter; parameter σ determines the width of\n",
      "the window, and the scaling constant d is typically chosen so that ∑xWσ(x) = 1.\n",
      "Overall, a one-dimensional Gabor function\n",
      "Wσ,ω,ψ(x) = Wσ(x)cos(ωx+ψ)\n",
      "(2.23)\n",
      "has three parameters, width σ, frequency ω and phase ψ. One-dimensional Gabor\n",
      "functions are illustrated in Fig. 2.13.\n",
      "In the two-dimensional case, a Gabor ﬁlter has a few additional parameters that\n",
      "control the two-dimensional shape and orientation of the ﬁlter. When the sinusoidal\n",
      "in the ﬁlter has vertical orientation the ﬁlter is given by the following equation\n",
      "Wσx,σy,ω,ψ(x,y) = 1\n",
      "d e\n",
      "−\n",
      "\u0012\n",
      "x2\n",
      "σ2x\n",
      "+ y2\n",
      "σ2y\n",
      "\u0013\n",
      "cos(ωx+ψ);\n",
      "(2.24)\n",
      "here σx and σy control the width of the weighting window in the x- and y-directions,\n",
      "respectively. A Gabor-ﬁlter with orientation α can be obtained by rotating the orig-\n",
      "inal (x,y) coordinate system by −α to yield a new coordinate system (x∗,y∗) (this\n",
      "rotation is equivalent to the rotation of the ﬁlter itself by α). The equations that\n",
      "relate the two coordinate systems are\n",
      "x = x∗cosα +y∗sinα\n",
      "(2.25)\n",
      "y = −x∗sinα +y∗cosα.\n",
      "(2.26)\n",
      "\n",
      "2.4 Space-frequency analysis\n",
      "45\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "d)\n",
      "e)\n",
      "f)\n",
      "Fig. 2.12: An example showing how the spatial map of amplitudes of Figure 2.11 f) on page 42\n",
      "was computed. a) The analyzed image. b) The spatial ﬁlter (called a Gabor ﬁlter) obtained by\n",
      "multiplying cos(ωxx + ωyy) with the weighting window W(x,y); the ﬁlter has been truncated to\n",
      "a size of 33 × 33 pixels. c) The spatial ﬁlter obtained by multiplying sin(ωxx + ωyy) with the\n",
      "weighting window W(x,y). d) Coefﬁcients C(x0,y0), obtained by ﬁltering image a) with ﬁlter b).\n",
      "e) Coefﬁcients S(x0,y0), obtained by ﬁltering image a) with ﬁlter c). f) The spatial amplitude map\n",
      "A(x0,y0) =\n",
      "p\n",
      "C(x0,y0)2 +S(x0,y0)2.\n",
      "\n",
      "46\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "a)\n",
      "×\n",
      "=\n",
      "b)\n",
      "Fig. 2.13: Illustration of one-dimensional Gabor functions. a) Construction of the function by\n",
      "multiplication of the envelope with a sinusoidal function. b) Two Gabor functions in quadrature\n",
      "phase.\n",
      "Substituting equations (2.25) and (2.26) into equation (2.24) gives the ﬁnal form\n",
      "(not shown here).\n",
      "Examples of two-dimensional Gabor functions were already given in Fig. 2.12b)-\n",
      "c); two more will be given in the next section, Fig. 2.15.\n",
      "2.4.3 Spatial localization vs. spectral accuracy\n",
      "Above we have outlined a scheme where the frequency contents of an image at a\n",
      "certain point is analyzed by ﬁrst multiplying the image with a localized weighting\n",
      "window, and then analyzing the frequency contents of the weighted image. How\n",
      "accurate is this procedure, that is, how well can it capture the localized frequency\n",
      "structure?\n",
      "The answer is that there is a trade-off between spatial localization and spectral\n",
      "(frequency) accuracy, because the use of a weighting window changes the spectral\n",
      "contents. Figure 2.14 illustrates this phenomenon by showing how the results of\n",
      "space-frequency analysis of a pure sinusoidal I(x) = cos\n",
      "\u0000 1\n",
      "2x\n",
      "\u0001\n",
      "depend on the degree\n",
      "of spatial localization. The mathematical theory behind this phenomenon goes un-\n",
      "\n",
      "2.4 Space-frequency analysis\n",
      "47\n",
      "σ\n",
      "W(x)\n",
      "W(x)cos\n",
      "\u0000 1\n",
      "2x\n",
      "\u0001\n",
      "A(ω)\n",
      "∞0\n",
      "1\n",
      "0\n",
      "150\n",
      "x\n",
      "−1\n",
      "1\n",
      "0\n",
      "150\n",
      "x\n",
      "0\n",
      "70\n",
      "−5\n",
      "0\n",
      "5\n",
      "ω\n",
      "16 0\n",
      "1\n",
      "0\n",
      "150\n",
      "x\n",
      "−1\n",
      "1\n",
      "0\n",
      "150\n",
      "x\n",
      "0\n",
      "15\n",
      "−5\n",
      "0\n",
      "5\n",
      "ω\n",
      "4 0\n",
      "1\n",
      "0\n",
      "150\n",
      "x\n",
      "−1\n",
      "1\n",
      "0\n",
      "150\n",
      "x\n",
      "0\n",
      "4\n",
      "−5\n",
      "0\n",
      "5\n",
      "ω\n",
      "1 0\n",
      "1\n",
      "0\n",
      "150\n",
      "x\n",
      "−1\n",
      "1\n",
      "0\n",
      "150\n",
      "x\n",
      "0\n",
      "1.5\n",
      "−5\n",
      "0\n",
      "5\n",
      "ω\n",
      "Fig. 2.14: In space-frequency analysis, there is a trade-off between spatial localization and spectral\n",
      "accuracy. This example shows how the use of a weighting window W(x) (second column) changes\n",
      "the spectral (frequency) contents of a pure sinusoidal I(x) = cos\n",
      "\u0000 1\n",
      "2x\n",
      "\u0001\n",
      ", x = 0,...,127. The rightmost\n",
      "column shows the amplitude spectrum of the localized signal W(x)I(x), which in turn is plotted\n",
      "in the third column. With no spatial localization (window width σ = ∞; top row), the amplitude\n",
      "spectrum A(ω) shows a clear peak at the location ω = 1\n",
      "2. As window width σ decreases, spatial\n",
      "localization increases, but accuracy in the spectral domain decreases: the two peaks in A(ω) spread\n",
      "out and eventually fuse so that A(ω) is a unimodal function when σ = 1.\n",
      "der the name time-bandwidth product theorem in signal processing , or uncertainty\n",
      "principle in physics. These theories state that there is a lower bound on the product\n",
      "of the spread of the energy in the spatial domain and the frequency domain. See\n",
      "References at the end of this chapter for more details.\n",
      "With images, the extra dimensions introduce another factor: uncertainty about\n",
      "orientation. This parameter behaves just like frequency and location in the sense\n",
      "that if we want to have a ﬁlter which is very localized in orientation, we have to give\n",
      "up localization in the other parameters. This is illustrated in Fig. 2.15, in which we\n",
      "see that a basic Gabor which is very localized in space a) responds to a wider range\n",
      "of different orientations than the Gabor in b). The Gabor in b) has has been designed\n",
      "to respond only to a small range of orientations, which was only possible by making\n",
      "it more extended in space.\n",
      "\n",
      "48\n",
      "2 Linear ﬁlters and frequency analysis\n",
      "a)\n",
      "b)\n",
      "Fig. 2.15: Uncertainty in two dimensions. Compared with a “basic” Gabor function in a), the\n",
      "Gabor in b) is very localized in orientation, that is, it responds to only a small range of different\n",
      "orientations. The thin black lines show the orientations of thin lines which still fall on the white\n",
      "(positive) area of the function: a line which is more oblique will partly fall on the black (negative)\n",
      "areas and thus the response of the ﬁlter will the reduced. We can see that in b), the range of\n",
      "orientations producing very large responses (falling on the white area only) is much smaller than in\n",
      "a). This illustrates that in order to make the basic Gabor function in a) more localized in orientation,\n",
      "it is necessary to make it longer, and thus to reduce its spatial localization.\n",
      "2.5 References\n",
      "Most of the material covered in this chapter can be found in most image-processing\n",
      "textbooks. A classic choice is (Gonzales and Woods, 2002), which does not, how-\n",
      "ever, consider space-frequency analysis. A large number of textbooks explain time-\n",
      "frequency analysis, which is the one-dimensional counterpart of space-frequency\n",
      "analysis, for example (Cohen, 1995). Related material can also be found in text-\n",
      "books on wavelets, which are a closely related method (see Section 17.3.2 for a very\n",
      "short introduction), for example (Vetterli and Kovaˇcevi´c, 1995; Strang and Nguyen,\n",
      "1996).\n",
      "2.6 Exercices\n",
      "Mathematical exercises\n",
      "1. Show that convolution is a symmetric operation.\n",
      "2. Show Eq. (2.3).\n",
      "3. Prove Equation (2.17). Hint: Find two different values for x so that you get the\n",
      "two equations\n",
      "Acosψ = C\n",
      "(2.27)\n",
      "−Asinψ = S\n",
      "(2.28)\n",
      "Now, solve for A and ψ as follows. First, take the squares of both sides of both\n",
      "equations (2.27) and (2.28) and sum the two resulting equations. Recall the sum\n",
      "\n",
      "2.6 Exercices\n",
      "49\n",
      "of squares of a sine and a cosine function. Second, divide both sides of the equa-\n",
      "tions (2.27) and (2.28) with each other.\n",
      "Computer assignments\n",
      "The computer assignments in this book are designed to be made with MatlabTM.\n",
      "Most of them will work on Matlab clones such as Octave. We will assume that you\n",
      "know the basics of Matlab.\n",
      "1. The command meshgrid is very useful for image processing. It creates two-\n",
      "dimensional coordinates, just like a command such as [5:.01:5] creates a one-\n",
      "dimensional coordinate. Give the command [X,Y]=meshgrid([-5:0.1:5]); and\n",
      "plot the matrices X and Y using imagesc.\n",
      "2. Create Fourier gratings by sin(X), sin(Y), sin(X+Y). Plot the gratings.\n",
      "3. Create a gabor function using these X and Y, simply by plugging in those matri-\n",
      "ces in the formula in Eq. 2.24. Try out different values for the parameters until\n",
      "you get a function which looks like the one in Fig. 2.12b).\n",
      "4. Change the roles of X and Y to get a Gabor ﬁlter in a different orientation.\n",
      "5. Try out a Gabor function of a different orientation by plugging in X+Y instead of\n",
      "X and X-Y instead of Y.\n",
      "6. Linear ﬁltering is easily done with the function conv2 (the “2” means two-\n",
      "dimensional convolution, i.e. images). Take any image, import it to Matlab, and\n",
      "convolve it with the three Gabor functions obtained above.\n",
      "\n",
      "\n",
      "Chapter 3\n",
      "Outline of the visual system\n",
      "In this chapter, we review very brieﬂy the structure of the human visual system. This\n",
      "exposition contains a large number of terms which are likely to be new for readers\n",
      "who are not familiar with neuroscience. Only a few of them are needed later in this\n",
      "book; they are given in italics for emphasis.\n",
      "3.1 Neurons and ﬁring rates\n",
      "Neurons\n",
      "The main information processing workload of the brain is carried by\n",
      "nerve cells, or neurons. Estimates of the number of neurons in the brain typically\n",
      "vary between 1010 and 1011. What distinguishes neurons from other cells are their\n",
      "special information-processing capabilities. A neuron receives signals from other\n",
      "neurons, processes them, and sends the result of that processing to other neurons. A\n",
      "schematic diagram of a neuron is shown in Fig. 3.1, while a more realistic picture is\n",
      "given in Fig. 3.2.\n",
      "Axons How can such tiny cells send signals to other cells which may be far away?\n",
      "Each neuron has one very long formation called an axon which connects it to other\n",
      "cells. Axons can be many centimeters or even a couple of metres long, so they can\n",
      "reach from one place in the brain to almost any other. Axons have a sophisticated\n",
      "biochemical machinery to transmit signals over such relatively long distances. The\n",
      "machinery is based on a phenomenon called action potential.\n",
      "Action potentials\n",
      "An action potential is a very short (1 ms) electrical impulse\n",
      "travelling via the axon of the neuron. Action potentials are illustrated in Fig. 3.3.\n",
      "Due to their typical shape, action potential are also called spikes. Action potentials\n",
      "are fundamental to the information processing in neurons; they constitute the signals\n",
      "by which the brain receives, analyzes, and conveys information.\n",
      "51\n",
      "\n",
      "52\n",
      "3 Outline of the visual system\n",
      "Axon\n",
      "Computation\n",
      "reception\n",
      "Signal\n",
      "transmission\n",
      "Signal\n",
      "Other neurons\n",
      "Synapses\n",
      "body\n",
      "Cell\n",
      "Dendrites\n",
      "Fig. 3.1: A schematic diagram of information-processing in a neuron. Flow of information is from\n",
      "right to left.\n",
      "Fig. 3.2: Neurons (thick bodies, some lettered), each with one axon (thicker line going up) for send-\n",
      "ing out the signal, and many dendrites (thinner lines) for receiving signals. Drawing by Santiago\n",
      "Ram´on y Cajal in 1900.\n",
      "\n",
      "3.1 Neurons and ﬁring rates\n",
      "53\n",
      "Action potentials are all-or-none, in the sense that they always have the same\n",
      "strength (a potential of about 100mV) and shape. Thus, a key principle in brain\n",
      "function is that the meaning of a spike is not determined by what the spike is like\n",
      "(because they are all the same), but rather, where it is, i.e. which axon is it travelling\n",
      "along, or equivalently, which neuron sent it. (Of course, the meaning also depends\n",
      "on when the spike was ﬁred.)\n",
      "Time\n",
      "Electric potential\n",
      "Fig. 3.3: An action potential is a wave of electrical potential which travels along the axon. It travels\n",
      "quite fast, and is very short both in time and its spatial length (along the axon). The ﬁgure shows\n",
      "the potentials in different parts of the axon soon after the neuron has emitted two action potentials.\n",
      "Signal reception and processing At the receiving end, action potentials are input\n",
      "to neurons via shorter formations called dendrites. Typically, an axon has many\n",
      "branches, and each of them connects to a dendrite of another neuron. Thus, the axon\n",
      "could be thought of as output wires along which the output signal of the neuron is\n",
      "sent to other neurons; dendrites are input wires which receive the signal from other\n",
      "neurons. The site where an axon meets a dendrite is called a synapse. The main cell\n",
      "body, or soma, is often thought of as the main “processor” which does the actual\n",
      "computation. However, an important part of the computation is already done in the\n",
      "dendrites.\n",
      "Firing rate The output of the neuron consists of a sequence of spikes emitted\n",
      "(a “spike train”). To fully describe such a sequence, one should record the time\n",
      "intervals between each successive spike. To simplify the situation, most research in\n",
      "visual neuroscience has concentrated on the neurons’ ﬁring rates, i.e. the number of\n",
      "spikes “ﬁred” (emitted) by a neuron per second. This gives a single scalar quantity\n",
      "which characterizes the activity of the cell. Since it is these action potentials which\n",
      "are transmitted to other cells, the ﬁring rate can also be viewed as the “result” of the\n",
      "computation performed by the neuron, in other words, its output.\n",
      "Actually, most visual neurons are emitting spikes all the time, but with a rela-\n",
      "tively low frequency (of the order of 1 Hz). The “normal” ﬁring rate of the neuron\n",
      "when there is no speciﬁc stimulation is called the spontaneous ﬁring rate. When the\n",
      "ﬁring rate is increased from the spontaneous one, the neuron is said to be active or\n",
      "activated.\n",
      "\n",
      "54\n",
      "3 Outline of the visual system\n",
      "Computation by the neuron\n",
      "How is information processed, i.e. how are the in-\n",
      "coming signals integrated in the soma to form the outcoming signal? This question\n",
      "is extremely complex and we can only give an extremely simpliﬁed exposition here.\n",
      "A fundamental principle of neural computation is that the reception of a spike at a\n",
      "dendrite can either excite (increase the ﬁring rate) of the receiving neuron, or inhibit\n",
      "it (decrease the ﬁring rate), depending on the neuron from which the signal came.\n",
      "Furthermore, depending on the dendrite and the synapse, some incoming signals\n",
      "have a stronger tendency to excite or inhibit the neuron. Thus, a neuron can be\n",
      "thought of as an elementary pattern-matching device: its ﬁring rates is large when\n",
      "it receives input from those neurons which excite it (strongly), and no input from\n",
      "those neurons which inhibit it. A basic mathematical model for such an action is to\n",
      "consider the ﬁring rate as a linear combination of incoming signals; we will consider\n",
      "linear models below.\n",
      "Thinking in terms of the original visual stimuli, it is often thought that a neuron\n",
      "is active when the input contains a feature for which the neuron is specialized — but\n",
      "this is a very gross simpliﬁcation. Thus, for example, a hypothetical “grandmother\n",
      "cell” is one that only ﬁres when the brain perceives, or perhaps thinks of, the grand-\n",
      "mother. Next we will consider what are the actual response properties of neurons in\n",
      "the visual system.\n",
      "3.2 From the eye to the cortex\n",
      "Figure 3.4 illustrates the earliest stages of the main visual pathway. Light enters\n",
      "the eye, reaching the retina. The retina is a curved, thin sheet of brain tissue that\n",
      "grows out into the eye to provide the starting point for neural processing of visual\n",
      "signals. The retina is covered by a more than a hundred million photoreceptors,\n",
      "which convert the light into an electric signal, i.e. neural activity.\n",
      "From the photoreceptors, the signal is transmitted through a couple of neural\n",
      "layers. The last of the retinal processing layer consists of ganglion cells, which\n",
      "send the output of the retina (in form of action potentials) away from the eye using\n",
      "their very long axons. The axons of the ganglion cells form the optic nerve. The\n",
      "optic nerve transmits the visual signals to the lateral geniculate nucleus (LGN) of\n",
      "the thalamus. The thalamus is a structure in the middle of the brain through which\n",
      "most sensory signals pass on their way from the sensory organs to the main sensory\n",
      "processing areas in the brain.\n",
      "From the LGN the signal goes to various other destinations, the most important\n",
      "being the visual cortex at the back of the head, where most of the visual processing\n",
      "is performed. Cortex, or cerebral cortex to be more precise, means here the surface\n",
      "of the two cerebral hemispheres, also called the “grey matter”. Most of the neurons\n",
      "associated with sensory or cognitive processing are located in the cortex. The rest of\n",
      "the cerebral cortex consists mainly of axons connecting cortical neurons with each\n",
      "other, or the “white matter”.\n",
      "\n",
      "3.3 Linear models of visual neurons\n",
      "55\n",
      "The visual cortex contains some 1/5 of the total cortical area in humans, which\n",
      "reﬂects the importance of visual processing to us. It consists of a number of distinct\n",
      "areas. The primary visual cortex, or V1 for short, is the area to which most of the\n",
      "retinal output ﬁrst arrives. It is the most widely-studied visual area, and also the\n",
      "main focus in this book.\n",
      "Fig. 3.4: The main visual pathway in the human brain.\n",
      "3.3 Linear models of visual neurons\n",
      "3.3.1 Responses to visual stimulation\n",
      "How to make sense of the bewildering network of neurons processing visual infor-\n",
      "mation in the brain? Much of visual neuroscience has been concerned with measur-\n",
      "ing the ﬁring rates of cells as a function of some properties of a visual input. For\n",
      "example, an experiment might run as follows: An image is suddenly projected onto a\n",
      "(previously blank) screen that an animal is watching, and the number of spikes ﬁred\n",
      "by some recorded cell in the next second are counted. By systematically changing\n",
      "some properties of the stimulus and monitoring the elicited response, one can make\n",
      "a quantitative model of the response of the neuron. An example is shown in Fig. 3.5.\n",
      "Such a model mathematically describes the response (ﬁring rate) rj of a neuron as a\n",
      "function of the stimulus I(x,y).\n",
      "In the early visual system, the response of a typical neuron depends only on\n",
      "the intensity pattern of a very small part of the visual ﬁeld. This area, where light\n",
      "increments or decrements can elicit increased ﬁring rates, is called the (classical)\n",
      "receptive ﬁeld (RF) of the neuron. More generally, the concept also refers to the\n",
      "particular light pattern that yields the maximum response.\n",
      "\n",
      "56\n",
      "3 Outline of the visual system\n",
      "time\n",
      "stimulus\n",
      "spiking activity of cell\n",
      "orientation of bar stimulus\n",
      "response (spikes/s)\n",
      "Fig. 3.5: A caricature of a typical experiment. A dark bar on a white background is ﬂashed onto\n",
      "the screen, and action potentials are recorded from a neuron. Varying the orientation of the bar\n",
      "yields varying responses. Counting the number of spikes elicited within a ﬁxed time window fol-\n",
      "lowing the stimulus, and plotting these counts as a function of bar orientation, one can construct a\n",
      "mathematical model of the response of the neuron.\n",
      "So, what kind of light patterns actually elicit the strongest responses? This of\n",
      "course varies from neuron to neuron. One thing that most cells have in common is\n",
      "that they don’t respond to a static image which consists of a uniform surface. They\n",
      "respond to stimuli in which there is some change, either temporally or spatially;\n",
      "such change is called contrast in vision science.\n",
      "The retinal ganglion cells as well as cells in the lateral geniculate nucleus typi-\n",
      "cally have circular center-surround receptive ﬁeld structure: Some neurons are ex-\n",
      "cited by light in a small circular area of the visual ﬁeld, but inhibited by light in a\n",
      "surrounding annulus. Other cells show the opposite effect, responding maximally to\n",
      "light that ﬁlls the surround but not the center. This is depicted in Figure 3.6a).\n",
      "3.3.2 Simple cells and linear models\n",
      "The cells that we are modelling are mainly in the primary visual cortex (V1). Cells\n",
      "in V1 have more interesting receptive ﬁelds than those in the retina or LGN. The so-\n",
      "called simple cells typically have adjacent elongated (instead of concentric circular)\n",
      "regions of excitation and inhibition. This means that these cells respond maximally\n",
      "to oriented image structure. This is illustrated in Figure 3.6b).\n",
      "Linear models are the ubiquitous workhorses of science and engineering. They\n",
      "are also the simplest successful neuron models of the visual system. A linear model\n",
      "for a visual neuron1 means that the response of a neuron is modelled by a weighted\n",
      "1 Note that there are two different kinds of models one could develop for a visual neuron. First,\n",
      "one can model the output (ﬁring rate) as a function of the input stimulus, which is what we do here.\n",
      "\n",
      "3.3 Linear models of visual neurons\n",
      "57\n",
      "−\n",
      "−\n",
      "−\n",
      "−\n",
      "−\n",
      "−\n",
      "−\n",
      "−−\n",
      "−\n",
      "+\n",
      "simple cells\n",
      "b\n",
      "off\n",
      "+\n",
      "−\n",
      "−\n",
      "+\n",
      "+\n",
      "−\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "−−\n",
      "−\n",
      "+\n",
      "−\n",
      "+\n",
      "+\n",
      "+\n",
      "−\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "on\n",
      "+\n",
      "+\n",
      "a\n",
      "+ −\n",
      "−\n",
      "++\n",
      "−\n",
      "−\n",
      "−\n",
      "+\n",
      "+\n",
      "−\n",
      "−\n",
      "−−−\n",
      "+\n",
      "−\n",
      "−\n",
      "−\n",
      "−\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "+\n",
      "Fig. 3.6: Typical classical receptive ﬁelds of neurons early in the visual pathway. Plus signs de-\n",
      "note regions of the visual ﬁeld where light causes excitation, minuses regions where light inhibits\n",
      "responses. a) Retinal ganglion and LGN neurons typically exhibit center-surround receptive ﬁeld\n",
      "organization, in one of two arrangements. b) The majority of simple cells in V1, on the other hand,\n",
      "have oriented receptive ﬁelds.\n",
      "sum of the image intensities, as in\n",
      "rj = ∑\n",
      "x,y\n",
      "Wj(x,y)I(x,y)+r0,\n",
      "(3.1)\n",
      "where Wj(x,y) contains the pattern of excitation and inhibition for light for the neu-\n",
      "ron j in question. The constant r0 is the spontaneous ﬁring rate. We can deﬁne the\n",
      "spontaneous ﬁring rate to be the baseline (zero) by subtracting it from the ﬁring rate:\n",
      "˜rj = rj −r0,\n",
      "(3.2)\n",
      "which will be done in all that follows.\n",
      "Linear receptive-ﬁeld models can be estimated from visual neurons by employ-\n",
      "ing a method called reverse correlation. In this method, a linear receptive ﬁeld is\n",
      "estimated so that the mean square error between the estimated rj in equation (3.1),\n",
      "and the actual ﬁring rate is minimized, where the mean is taken over a large set of\n",
      "visual stimuli. The name “reverse correlation” comes from the fact that the general\n",
      "solution to this problem involves the computation of the time-correlation of stimu-\n",
      "lus and ﬁring rate. However, the solution is simpliﬁed when temporally and spatially\n",
      "uncorrelated (“white noise”, see Section 4.6.4) sequences are used as visual stim-\n",
      "uli – in this case, the optimal Wj is obtained by computing an average stimulus\n",
      "over those stimuli which elicited a spike. Examples of estimated receptive ﬁelds are\n",
      "shown in Fig. 3.7.\n",
      "Alternatively, one could model the output as a function of the direct inputs to the cell, i.e. the rates\n",
      "of action potentials received in its dendrites. This latter approach is more general, because it can\n",
      "be applied to any neuron in the brain. However, it is not usually used in vision research because it\n",
      "does not tell us much about the function of the visual system unless we already know the response\n",
      "properties of those neurons whose ﬁring rates are input to the neuron via dendrites, and just ﬁnding\n",
      "those cells whose axons connect to a given neuron is technically very difﬁcult.\n",
      "\n",
      "58\n",
      "3 Outline of the visual system\n",
      "Fig. 3.7: Receptive ﬁelds of simple cells estimated by reverse correlation based on single-cell\n",
      "recordings in a macaque monkey. Courtesy of Dario Ringach, UCLA.\n",
      "3.3.3 Gabor models and selectivities of simple cells\n",
      "How can we describe the receptive ﬁeld of simple cells in mathematical terms? Typ-\n",
      "ically, this is based on modelling the receptive ﬁelds by Gabor functions, reviewed in\n",
      "Section 2.4.2. A Gabor function consists of an oscillatory sinusoidal function which\n",
      "generates the alternation between the excitatory and inhibitory (“white/black”) ar-\n",
      "eas, and a gaussian “envelope” function which determines the spatial size of the\n",
      "receptive ﬁeld. In fact, when comparing the receptive ﬁeld in Fig. 3.7 with the Ga-\n",
      "bor functions in Fig. 2.12b)-c), it seems obvious that Gabor functions provide a\n",
      "reasonable model for the receptive ﬁelds.\n",
      "Using a Gabor function, the receptive ﬁeld is reduced to a small number of pa-\n",
      "rameters:\n",
      "• Orientation of the oscillation\n",
      "• Frequency of oscillation\n",
      "• Phase of the oscillation\n",
      "• Width of the envelope (in the direction of the oscillation)\n",
      "• Length of the envelope (in the direction orthogonal to the oscillation). The ratio\n",
      "of the length to the width is called the aspect ratio.\n",
      "• The location in the image (on the retina)\n",
      "\n",
      "3.4 Nonlinear models of visual neurons\n",
      "59\n",
      "These parameters are enough to describe the basic selectivity properties of simple\n",
      "cells: a simple cell typically gives a strong response when the input consists of a\n",
      "Gabor function with approximately the right (“preferred”) values for all, or at least\n",
      "most, of these parameters (the width and the length of the envelope are not criti-\n",
      "cal for all simple cells). Thus, we say that simple cells are selective for frequency,\n",
      "orientation, phase, and location.\n",
      "In principle, one could simply try to ﬁnd a Gabor function which gives the best\n",
      "ﬁt to the receptive ﬁeld estimated by reverse correlation. In practice, however, more\n",
      "direct methods are often used since reverse correlation is rather laborious. Typi-\n",
      "cally, what is computed are tuning curves for some of these parameters. This was\n",
      "illustrated in Fig. 3.5. Typical stimuli include two-dimensional Fourier gratings (see\n",
      "Fig. 2.5) and simple, possibly short, lines or bars. Examples of such analyses will\n",
      "be seen in Chapters 6 and 10.\n",
      "3.3.4 Frequency channels\n",
      "The selectivity of simple cells (as well as many other cells) to frequency is related\n",
      "to the concept of “frequency channels” which is widely used in vision science. The\n",
      "idea is that in the early visual processing (something like V1), information of differ-\n",
      "ent frequencies is processed independently. Justiﬁcation for talking about different\n",
      "channels is abundant in research on V1. In fact, the very point in using Gabor models\n",
      "is to model the selectivity of simple cells to a particular frequency range.\n",
      "Furthermore, a number of psychological experiments point to such a division\n",
      "of early processing. For example, in Figure 3.8, the information in the high- and\n",
      "low-frequency parts are quite different, yet observes have no difﬁculty in process-\n",
      "ing (reading) them separately. This ﬁgure also illustrates the practical meaning of\n",
      "frequency selectivity: some of the cells in V1 respond to the “yes” letters but do\n",
      "not respond to the “no” letters, while for other cells, the responses are the other\n",
      "way round. (The responses depend, however, on viewing distance: stimuli which\n",
      "are low-frequency when viewed from a close distance will be high-frequency when\n",
      "viewed from far away.)\n",
      "3.4 Nonlinear models of visual neurons\n",
      "3.4.1 Nonlinearities in simple-cell responses\n",
      "Linear models are widely used in modelling visual neurons, but they are deﬁnitely a\n",
      "rough approximation of the reality. Real neurons exhibit different kinds of nonlinear\n",
      "behaviour. The most basic nonlinearities can be handled by adding a simple scalar\n",
      "\n",
      "60\n",
      "3 Outline of the visual system\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "Fig. 3.8: A ﬁgure with independent (contradictory?) information in different frequency channels. a)\n",
      "the original ﬁgure, b) low-frequency part of ﬁgure in a), obtained by taking the Fourier transform\n",
      "and setting to zero all high-frequency components (whose distance from zero is larger than a certain\n",
      "threshold) , c) high-frequency part of ﬁgure in a). The sum of the ﬁgures in b) and c) equals a).\n",
      "\n",
      "3.4 Nonlinear models of visual neurons\n",
      "61\n",
      "nonlinearity to the model, which leads to what is simply called a linear-nonlinear\n",
      "model.\n",
      "In the linear-nonlinear model, a linear stage is followed by a static nonlinearity\n",
      "f:\n",
      "˜rj = f\n",
      " \n",
      "∑\n",
      "x,y\n",
      "Wj(x,y)I(x,y)\n",
      "!\n",
      ".\n",
      "(3.3)\n",
      "A special case of the linear-nonlinear model is half-wave rectiﬁcation, deﬁned by\n",
      "f(α) = max{0,α}.\n",
      "(3.4)\n",
      "One reason for using this model is that if a neuron has a relatively low spontaneous\n",
      "ﬁring rate, the ﬁring rates predicted by the linear model may then tend to be negative.\n",
      "The ﬁring rate, by deﬁnition, cannot be negative.\n",
      "We must distinguish here between two cases. Negative ﬁring rates are, of course,\n",
      "impossible by deﬁnition. In contrast, it is possible to have positive ﬁring rates that\n",
      "are smaller than the spontaneous ﬁring rate; they give a negative ˜rj in Equation (3.2).\n",
      "Such ﬁring rates correspond to the sum term in Eq. (3.1) being negative, but not so\n",
      "large that the rj becomes negative. However, in V1, the spontaneous ﬁring rate tends\n",
      "to be rather low, and the models easily predict negative ﬁring rates for cortical cells.\n",
      "(This is less of a problem for ganglion and LGN cells, since their spontaneous ﬁring\n",
      "rates are relatively high.)\n",
      "Thus, half-wave rectiﬁcation offers one way to interpret the purely linear model\n",
      "in Eq. (3.1) in a more physiologically plausible way: the linear model combines the\n",
      "outputs of two half-wave rectiﬁed (non-negative) cells with reversed polarities into\n",
      "a single output rj – one cell corresponds to linear RF Wj and the other to RF −Wj.\n",
      "The linear-nonlinear model is ﬂexible and can accommodate a number of other\n",
      "properties of simple cell responses as well. First, when the linear model predicts\n",
      "small outputs, i.e., the stimulus is weak, no output (increase in ﬁring rate) is actually\n",
      "observed in simple cells. In other words, it seems there is a threshold which the\n",
      "stimulus must attain to elicit any response. This phenomenon, combined with half-\n",
      "wave rectiﬁcation, could be modelled by using a nonlinearity such as\n",
      "f(α) = max(0,α −c)\n",
      "(3.5)\n",
      "where c is a constant that gives the threshold.\n",
      "Second, due to biological properties, neurons have a maximum ﬁring rate. When\n",
      "the stimulus intensity is increased above a certain limit, no change in the cell re-\n",
      "sponse is observed, a phenomenon called saturation. This is in contradiction with\n",
      "the linear model, which has no maximum response: if you multiply the input stimu-\n",
      "lus by, say, 1,000,000, the output of the neuron increases by the same factor. To take\n",
      "this property into account, we need to use a nonlinearity that saturates as well, i.e.\n",
      "has a maximum value. Combining the three nonlinear properties listed here leads us\n",
      "to a linear-nonlinear model with the nonlinearity\n",
      "f(α) = min(d,max(0,α −c))\n",
      "(3.6)\n",
      "\n",
      "62\n",
      "3 Outline of the visual system\n",
      "where d is the maximum response. Figure 3.9 shows the form of this function.\n",
      "Alternatively, we could use a smooth function with the same kind of behaviour,\n",
      "such as\n",
      "f(α) = d\n",
      "α2\n",
      "c′ +α2 .\n",
      "(3.7)\n",
      "where c′ is another constant that is related to the threshold c.\n",
      "Fig. 3.9: The nonlinear function in (3.6).\n",
      "3.4.2 Complex cells and energy models\n",
      "Although linear-nonlinear models are useful in modelling many cells, there are also\n",
      "neurons in V1 called complex cells for which these models are completely inade-\n",
      "quate. These cells do not show any clear spatial zones of excitation or inhibition.\n",
      "Complex cells respond, just like simple cells, selectively to bars and edges at a par-\n",
      "ticular location and of a particular orientation; they are, however, relatively invariant\n",
      "to the spatial phase of the stimulus. An example of this is that reversing the contrast\n",
      "polarity (e.g. from white bar to black bar) of the stimulus does not markedly alter\n",
      "the response of a typical complex cell.\n",
      "The responses of complex cells have often been modelled by the classical ‘energy\n",
      "model’. (The term ‘energy’ simply denotes the squaring operation.) In such a model\n",
      "(see Figure 3.10) we have\n",
      "rj =\n",
      " \n",
      "∑\n",
      "x,y\n",
      "Wj1(x,y)I(x,y)\n",
      "!2\n",
      "+\n",
      " \n",
      "∑\n",
      "x,y\n",
      "Wj2(x,y)I(x,y)\n",
      "!2\n",
      "where Wj1(x,y) and Wj2(x,y) are quadrature-phase Gabor functions, i.e., they have\n",
      "a phase-shift of 90 degrees, one being odd-symmetric and the other being even-\n",
      "symmetric. It is often assumed that V1 complex cells pool the responses of simple\n",
      "\n",
      "3.5 Interactions between visual neurons\n",
      "63\n",
      "cells, in which case the linear responses in the above equation are outputs of simple\n",
      "cells.\n",
      "The justiﬁcation for this model is that since the two linear ﬁlters are Gabors in\n",
      "quadrature-phase, the model is computing the local Fourier “energy” in a particular\n",
      "range of frequencies and orientations, see Equation (2.16). This provides a model of\n",
      "a cell which is selective for frequency and orientation, and is also spatially localized,\n",
      "but does not care about the phase of the input. In other words it is phase-invariant\n",
      "(This will be discussed in more detail in Chapter 10.)\n",
      "The problem of negative responses considered earlier suggests a simple modiﬁ-\n",
      "cation of the model, where each linear RF again corresponds to two simple cells.\n",
      "The output of a linear RF is divided to the positive and negative parts and half-wave\n",
      "rectiﬁed. In this case, the half-wave rectiﬁed outputs are further squared so that they\n",
      "compute the squaring operation of the energy model. In addition, complex cells sat-\n",
      "urate just as simple cells, so it makes sense to add a saturating nonlinearity to the\n",
      "model as well.\n",
      "++\n",
      "++\n",
      "−\n",
      "+\n",
      "++\n",
      "+\n",
      "+\n",
      "+++\n",
      "+\n",
      "−\n",
      "−−−\n",
      "−−−\n",
      "−\n",
      "−−\n",
      "−−−\n",
      "−\n",
      "+\n",
      "++\n",
      "+++\n",
      "+\n",
      "STIMULUS\n",
      "Fig. 3.10: The classic energy model for complex cells. The response of a complex cell is modelled\n",
      "by linearly ﬁltering with quadrature-phase Gabor ﬁlters (Gabor functions whose sinusoidal com-\n",
      "ponents have a 90 degrees phase difference), taking squares, and summing. Note that this is purely\n",
      "a mathematical description of the response and should not be directly interpreted as a hierarchical\n",
      "model summing simple cell responses.\n",
      "3.5 Interactions between visual neurons\n",
      "In the preceding models, V1 cells are considered completely independent units:\n",
      "each of them just takes its input and computes its output. However, different kinds\n",
      "interactions between the cells have been observed.\n",
      "The principal kind of interaction seems to be inhibition: when a cell j is active,\n",
      "the responses of another cell i is reduced from what they would be without that cell\n",
      "j being active. To be more precise, let us consider two simple cells whose receptive\n",
      "ﬁelds Wi and Wj are orthogonal (for more on orthogonality see Chapter 19). Take,\n",
      "for example, two cells in the same location, one with vertical and the other with\n",
      "horizontal orientation). Take any stimulus I0 which excites the cell Wj. For example,\n",
      "we could take a stimulus which is equal to the receptive ﬁeld Wj itself. Now, we add\n",
      "\n",
      "64\n",
      "3 Outline of the visual system\n",
      "another stimulus pattern, say I1, to I0. This simply means that we add the intensities\n",
      "pixel-by-pixel, showing the following stimulus to the retina:\n",
      "I(x,y) = I0(x,y)+I1(x,y)\n",
      "(3.8)\n",
      "The added stimulus I1 is often called a mask or a pedestal.\n",
      "The point is that by choosing I1 suitably, we can demonstrate a phenomenon\n",
      "which is probably due to interaction between the two cells. Speciﬁcally, let us\n",
      "choose a stimulus which is equal to the receptive ﬁeld of cell i: I1 = Wi. This is\n",
      "maximally excitatory for the cell i, but it is orthogonal to the receptive ﬁeld of cell\n",
      "j. With this kind of stimuli, the typical empirical observation is that the cell j has a\n",
      "lower ﬁring rate for the compound stimulus I = I0 +I1 than for I0 alone. This inhibi-\n",
      "tion cannot be explained by the linear models (or the linear-nonlinear models). The\n",
      "mask I1 should have no effect on the linear ﬁlter stage, because the mask is orthog-\n",
      "onal to the receptive ﬁeld Wj. So, to incorporate this phenomenon in our models,\n",
      "we must include some interaction between the linear ﬁlters: The outputs of some\n",
      "model cells must reduce the outputs of others. (It is not completely clear whether\n",
      "this empirical phenomenon is really due to interaction between the cells, but that is\n",
      "a widely-held view, so it makes sense to adopt it in our models.)\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "Fig. 3.11: Interaction between different simple cells. a) Original stimulus I0 of a simple cell, cho-\n",
      "sen here as equal the receptive ﬁeld of Wj. b) Masking pattern I1 which is orthogonal to I0. c)\n",
      "Compound stimulus I. The response to I is smaller than the response to I0 although the linear\n",
      "models predicts that the responses should be equal.\n",
      "This phenomenon is typically called “contrast gain control”. The idea is that\n",
      "when there is more contrast in the image (due to the addition of the mask), the\n",
      "system adjusts its responses to be generally weaker. It is thought to be necessary\n",
      "because of the saturating nonlinearity in the cells and the drastic changes in illumi-\n",
      "nation conditions observed in the real world. For example, the cells would be re-\n",
      "sponding with the maximum value most of the time in bright daylight (or a brightly\n",
      "lit part of the visual scene), and they would be responding hardly at all in a dim\n",
      "environment (or a dimly lit part of the scene). Gain control mechanisms alleviate\n",
      "\n",
      "3.6 Topographic organization\n",
      "65\n",
      "this problem by normalizing the variation of luminance over different scenes, or\n",
      "different parts of the same scene. For more on this point, see Section 9.5.2\n",
      "This leads us to one of the most accurate currently known simple-cell models, in\n",
      "terms of predictive power, the divisive normalization model. Let W1,...,WK denote\n",
      "the receptive ﬁelds of those cells whose receptive ﬁelds are approximately in the\n",
      "same location, and σ a scalar parameter. In the divisive normalization model, the\n",
      "output of the cell corresponding to RF Wj is given by\n",
      "rj =\n",
      "f\n",
      "\u0000∑x,yWj(x,y)I(x,y)\n",
      "\u0001\n",
      "∑K\n",
      "i=1 f\n",
      "\u0000∑x,yWi(x,y)I(x,y)\n",
      "\u0001\n",
      "+σ2 ,\n",
      "(3.9)\n",
      "where f is again a static nonlinearity, such as the half-wave rectiﬁcation followed\n",
      "by squaring. This divisive normalization model provides a simple account of con-\n",
      "trast gain control mechanisms. In addition, it also automatically accounts for such\n",
      "simple-cell nonlinearities as response saturation and threshold. In fact, if the input\n",
      "stimulus is such that it only excites cell j, and the linear responses in the denomi-\n",
      "nator are all zero expect for the one corresponding to cell j, the model reduces to\n",
      "the linear-nonlinear model in Section 3.4.1. If we further deﬁne f to be the square\n",
      "function, we get the nonlinearity in Equation 3.7.\n",
      "3.6 Topographic organization\n",
      "A further interesting point is how the receptive ﬁelds of neighbouring cells are re-\n",
      "lated. In the retina, the receptive ﬁelds of retinal ganglion cells are necessarily linked\n",
      "to the physical position of the cells. This is due to the fact that the visual ﬁeld\n",
      "is mapped in an orderly fashion to the retina. Thus, neighbouring retinal ganglion\n",
      "cells respond to neighbouring areas of the visual ﬁeld. However, there is nothing to\n",
      "guarantee the existence of a similar organization further up the visual pathway.\n",
      "But the fact of the matter is that, just like in the retina, neighbouring neurons in\n",
      "the LGN and in V1 tend to have receptive ﬁelds covering neighbouring areas of the\n",
      "visual ﬁeld. This phenomenon is called retinotopy. Yet this is only one of several\n",
      "types of organization. In V1, the orientation of receptive ﬁelds also tends to shift\n",
      "gradually along the surface of the cortex. In fact, neurons are often approximately\n",
      "organized according to several functional parameters (such as location, frequency,\n",
      "orientation) simultaneously. This kind of topographic organization also exists in\n",
      "many other visual areas.\n",
      "Topographical representations are not restricted to cortical areas devoted to vi-\n",
      "sion, but are present in various forms throughout the brain. Examples include the\n",
      "2 In fact, different kinds of gain control mechanisms seem to be operating in different parts of\n",
      "the visual system. In the retina, such mechanisms normalize the general luminance level of the\n",
      "inputs, hence the name “luminance gain control”. Contrast gain control seems to be done after that\n",
      "initial gain control. The removal of the mean grey-scale value (DC component) that we do in later\n",
      "chapters can be thought to represent a simple luminance gain control mechanism.\n",
      "\n",
      "66\n",
      "3 Outline of the visual system\n",
      "tonotopic map (frequency-based organization) in the primary auditory cortex and\n",
      "the complete body map for the sense of touch. In fact, one might be pressed to ﬁnd\n",
      "a brain area that would not exhibit any sort of topography.\n",
      "3.7 Processing after the primary visual cortex\n",
      "From V1, the visual signals are sent to other areas, such as V2, V4, and V5, called\n",
      "extrastriate as another name for V1 is the “striate cortex”. The function of some\n",
      "of these areas (mainly V5, which analyzes motion) is relatively well understood,\n",
      "but the function of most of them is not really understood at all. For example, it\n",
      "is assumed that V2 is the next stage in the visual processing, but the differences\n",
      "in the features computed in V1 and V2 are not really known. V4 has been vari-\n",
      "ously described as being selective to long contours, corners, crosses, circles, “non-\n",
      "Cartesian” gratings, colour, or temporal changes (see the references section below).\n",
      "Another problem is that the extrastriate cortex may be quite different in humans\n",
      "and monkeys (not to mention other experimental animals), so results from animal\n",
      "experiments may not generalize to humans.\n",
      "3.8 References\n",
      "Among general introductions to the visual system, see, e.g., (Palmer, 1999). A most\n",
      "interesting review of the state of modelling of the visual cortex, with extensive ref-\n",
      "erences to experiments, is in (Carandini et al, 2005).\n",
      "For a textbook account of reverse correlation, see e.g. (Dayan and Abbott, 2001);\n",
      "reviews are (Ringach and Shapley, 2004; Simoncelli et al, 2004). Classic application\n",
      "of reverse correlation for estimating simple cell receptive ﬁelds is (Jones and Palmer,\n",
      "1987b; Jones et al, 1987; Jones and Palmer, 1987a). For spatiotemporal extensions\n",
      "see (DeAngelis et al, 1993a,b). LGN responses are estimated, e.g., in (Cai et al,\n",
      "1997), and retinal ones, e.g. in (Davis and Naka, 1980).\n",
      "The nonlinearities in neuron responses are measured in (Anzai et al, 1999b;\n",
      "Ringach and Malone, 2007); theoretical studies include (Hansel and van Vreeswijk,\n",
      "2002; Miller and Troyer, 2002). These studies concentrate on the “thresholding”\n",
      "part of the nonlinearity, ignoring saturation. Reverse correlation in the presence of\n",
      "nonlinearities is considered in (Nykamp and Ringach, 2002).\n",
      "A review on contrast gain control can be found in (Carandini, 2004). The divisive\n",
      "normalization model is considered in (Heeger, 1992; Carandini et al, 1997, 1999).\n",
      "More on the interactions can be found in (Albright and Stoner, 2002). For review\n",
      "of the topographic organization in different parts of the cortex, see (Mountcastle,\n",
      "1997).\n",
      "A discussion on our ignorance of V2 function can be found in (Boynton and\n",
      "Hedg´e, 2004). Proposed selectivities in V4 include long contours (Pollen et al,\n",
      "\n",
      "3.9 Exercices\n",
      "67\n",
      "2002), corners and related features (Pasupathy and Connor, 1999, 2001), crosses,\n",
      "circles, and other non-Cartesian gratings (Gallant et al, 1993; Wilkinson et al, 2000),\n",
      "as well as temporal changes (Gardner et al, 2005). An alternative viewpoint is that\n",
      "the processing might be quite similar in most extrastriate areas, the main difference\n",
      "being the spatial scale (Hegd´e and Essen, 2007). A model of V5 is proposed in\n",
      "(Simoncelli and Heeger, 1998).\n",
      "Basic historical references on the visual cortex include (Hubel and Wiesel, 1962,\n",
      "1963, 1968, 1977).\n",
      "3.9 Exercices\n",
      "Mathematical exercises\n",
      "1. Show that the addition of a mask which is orthogonal to the receptive ﬁeld, as in\n",
      "Section 3.5 should not change the output of the cell in the linear model\n",
      "2. What is the justiﬁcation for using the same letter d for the constants in Equa-\n",
      "tions (3.6) and (3.7)?\n",
      "Computer assignments\n",
      "1. Plot function in Equation (3.7) and compare with the function in (3.6).\n",
      "2. Receptive ﬁelds in the ganglion cells and the LGN are often modelled as a\n",
      "“difference-of-gaussians” model in which W(x,y) is deﬁned as\n",
      "exp(−1\n",
      "2σ2\n",
      "1\n",
      "[(x−x0)2 +(y−y0)2])−aexp(−1\n",
      "2σ2\n",
      "2\n",
      "[(x−x0)2 +(y−y0)2]) (3.10)\n",
      "Plot the receptive ﬁelds for some choices of the parameters. Find some parameter\n",
      "values that reproduce a center-surround receptive ﬁeld.\n",
      "\n",
      "\n",
      "Chapter 4\n",
      "Multivariate probability and statistics\n",
      "This chapter provides the theoretical background in probability theory and statis-\n",
      "tical estimation needed in this book. This is not meant as a ﬁrst introduction to\n",
      "probability, however, and the reader is supposed to be familiar with the basics of\n",
      "probability theory. The main emphasis here is on the extension of the basic notions\n",
      "to multidimensional data.\n",
      "4.1 Natural images patches as random vectors\n",
      "To put the theory on a concrete basis, we shall ﬁrst discuss the fundamental idea on\n",
      "how natural image patches can be considered as a random vector.\n",
      "A random vector is a vector whose elements are random variables. Randomness\n",
      "can be deﬁned in different ways. In probability theory, it is usually formalized by\n",
      "assuming that the value of the random variable or vector depends on some other\n",
      "variable (“state of the world”) whose value we do not know. So, the randomness is\n",
      "due to our ignorance.\n",
      "In this book, an image patch I is typically modelled as a random vector, whose\n",
      "obtained values (called “observations”) are the numerical grey-scale values of pixels\n",
      "in a patch (window) of a natural image. A patch simply means a small sub-image,\n",
      "such as the two depicted in Fig. 1.3. We use small patches because whole images\n",
      "have too large dimensions for existing computers (we must be able to perform com-\n",
      "plicated computations on a large number of such images or patches). A typical patch\n",
      "size we use in this book is 32×32 pixels.\n",
      "To get one observation of the random vector in question, we randomly select one\n",
      "of the images in the image set we have, and then randomly select the location of the\n",
      "image patch. The randomness of the values in the vector stems from the fact that\n",
      "the patch is taken in some “random” position in a “randomly” selected image from\n",
      "our database. The “random” position and image selection are based on a random\n",
      "number generator implemented in a computer.\n",
      "69\n",
      "\n",
      "70\n",
      "4 Multivariate probability and statistics\n",
      "It may be weird to call an image patch a “vector” as it is two-dimensional and\n",
      "could also be called a matrix. However, for the purposes of most of this book, a\n",
      "two-dimensional image patch has to be treated like a one-dimensional vector, or a\n",
      "point in the image space, as was illustrated in Fig. 1.5. (A point and a vector are\n",
      "basically the same thing in this context.) This is because observed data are typically\n",
      "considered to be such vectors in statistics, and matrices are used for something quite\n",
      "different (that is, to represent linear transformations, see Section 19.3 for details).\n",
      "In practical calculations, one often has to transform the image patches into one-\n",
      "dimensional vectors, i.e. “vectorize” them. Such a transformation can be done in\n",
      "many different ways, for example by scanning the numerical values in the matrix\n",
      "row-by-row; the statistical analysis of the vector is not at all inﬂuenced by the choice\n",
      "of that transformation. In most of the chapters in this book, it is assumed that such\n",
      "a transformation has been made.\n",
      "On a more theoretical level, the random vector can also represent the whole set of\n",
      "natural images, i.e. each observation is one natural image, in which case the database\n",
      "is inﬁnitely large and does not exist in reality.\n",
      "4.2 Multivariate probability distributions\n",
      "4.2.1 Notation and motivation\n",
      "In this chapter, we will denote random variables by z1,z2,...,zn and s1,s2,...,sn\n",
      "for some number n. Taken together, the random variables z1,z2,...,zn form an n-\n",
      "dimensional random vector which we denote by z:\n",
      "z =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "z1\n",
      "z2\n",
      "...\n",
      "zn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(4.1)\n",
      "Likewise, the variables s1,s2,...,sn can be collected to a random vector, denoted by\n",
      "s.\n",
      "Although we will be considering general random vectors, in order to make things\n",
      "concrete, you can think of each zi as the grey-scale value of a pixel in the image\n",
      "patch. In the simple case of two variables, z1 and z2, this means that you take samples\n",
      "of two adjacent pixels (say, one just to the right of the other). A scatter plot of such\n",
      "a pixel pair is given in Fig. 4.1. However, this is by no means the only thing the\n",
      "variables can represent; in most chapters of this book, we will also consider various\n",
      "kinds of features which are random variables as well.\n",
      "The fundamental goal of the models in this book is to describe the probability\n",
      "distribution of the random vector of natural image patches. So, we need to next\n",
      "consider the concept of a probability density function.\n",
      "\n",
      "4.2 Multivariate probability distributions\n",
      "71\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Fig. 4.1: Scatter plot of the grey-scale values of two neighbouring pixels. The horizontal axis gives\n",
      "the value of the pixel on the left, and the vertical axis gives the value of the pixel on the right. Each\n",
      "dot corresponds to one observed pair of pixels.\n",
      "4.2.2 Probability density function\n",
      "A probability distribution of a random vector such as z is usually represented using\n",
      "a probability density function (pdf). The pdf at a point in the n-dimensional space is\n",
      "denoted by pz.\n",
      "The deﬁnition of the pdf of a multidimensional random vector is a simple gen-\n",
      "eralization of the deﬁnition of the pdf of a random variable in one dimension. Let\n",
      "us ﬁrst recall that deﬁnition. Denote by z a random variable. The idea is that we\n",
      "take a small number v, and look at the probability that z takes a value in the interval\n",
      "[a,a +v] for any given a. Then we divide that probability by v, and that is the value\n",
      "of the probability density function at the point a. That is\n",
      "pz(a) = P(z is in [a,a +v])\n",
      "v\n",
      "(4.2)\n",
      "This principle is illustrated in Fig. 4.2. Rigorously speaking, we should take the\n",
      "limit of an inﬁnitely small v in this deﬁnition.\n",
      "This principle is simple to generalize to the case of an n-dimensional random\n",
      "vector. The value of the pdf function at a point, say a = (a1,a2,...,an), gives the\n",
      "probability that an observation of z belongs to a small neighbourhood of the point\n",
      "a, divided by the volume of the neighbourhood. Computing the probability that the\n",
      "values of each zi are between the values of ai and ai +v, we obtain\n",
      "pz(a) = P(zi is in [ai,ai +v] for all i)\n",
      "vn\n",
      "(4.3)\n",
      "where vn is the volume of the n-dimensional cube whose edges all have length v.\n",
      "Again, rigorously speaking, this equation is true only in the limit of inﬁnitely small\n",
      "v.\n",
      "A most important property of a pdf is that it is normalized: its integral is equal to\n",
      "one\n",
      "Z\n",
      "pz(a)da = 1\n",
      "(4.4)\n",
      "\n",
      "72\n",
      "4 Multivariate probability and statistics\n",
      "a\n",
      "a+v\n",
      "Fig. 4.2: The pdf of a random variable at a point a gives the probability that the random variable\n",
      "takes a value in a small interval [a,a + v], divided by the length of that interval, i.e. v. In other\n",
      "words, the shaded area, equal to p(a)v, gives the probability that that the variable takes a value in\n",
      "that interval.\n",
      "This constraint means that you cannot just take any non-negative function and say\n",
      "that it is a pdf: you have to normalize the function by dividing it by its integral.\n",
      "(Calculating such an integral can actually be quite difﬁcult and sometimes leads to\n",
      "serious problems, as discussed in Chapter 21).\n",
      "For notational simplicity, we often omit the subscript z. We often also write p(z)\n",
      "which means the value of pz at the point z. This simpliﬁed notation is rather ambigu-\n",
      "ous because now z is used as an ordinary vector (like a above) instead of a random\n",
      "vector. However, often it can be used without any confusion.\n",
      "Example 1\n",
      "The most classic probability density function for two variables is the\n",
      "gaussian, or normal, distribution. Let us ﬁrst recall the one-dimensional gaussian\n",
      "distribution, which in the basic case is given by\n",
      "p(z) =\n",
      "1\n",
      "√\n",
      "2π exp(−1\n",
      "2z2)\n",
      "(4.5)\n",
      "It is plotted in Fig. 4.3 b). This is the “standardized” version (mean is zero and\n",
      "variance is one), as explained below. The most basic case of a two-dimensional\n",
      "gaussian distribution is obtained by taking this one-dimensional pdf separately for\n",
      "each variables, and multiplying them together. (The meaning of such multiplication\n",
      "is that the variables are independent, as will be explained below.) Thus, the pdf is\n",
      "given by\n",
      "p(z1,z2) = 1\n",
      "2π exp(−1\n",
      "2(z2\n",
      "1 +z2\n",
      "2))\n",
      "(4.6)\n",
      "A scatter plot of the distribution is shown in Fig. 4.3 a). The two-dimensional pdf\n",
      "itself is plotted in Fig. 4.3 c).\n",
      "Example 2 Let us next consider the following two-dimensional pdf:\n",
      "\n",
      "4.3 Marginal and joint probabilities\n",
      "73\n",
      "p(z1,z2) =\n",
      "(\n",
      "1, if |z1|+|z2| < 1\n",
      "0, otherwise\n",
      "(4.7)\n",
      "This means that the data is uniformly distributed inside a square which has been\n",
      "rotated 45 degrees. A scatter plot of data from this distribution is shown in Figure 4.4\n",
      "a).\n",
      "a)\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "b)\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "c)\n",
      "−5\n",
      "0\n",
      "5\n",
      "−5\n",
      "0\n",
      "5\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "Fig. 4.3: a) scatter plot of the two-dimensional gaussian distribution in Equation (4.6). b) The\n",
      "one-dimensional standardized gaussian pdf. As explained in Section 4.3, it is also the marginal\n",
      "distribution of one of the variables in a), and furthermore, turns out to be equal to the conditional\n",
      "distribution of one variable given the other variable. c) The probability density function of the\n",
      "two-dimensional gaussian distribution.\n",
      "4.3 Marginal and joint probabilities\n",
      "Consider the random vector z whose pdf is denoted by pz. It is important to make\n",
      "a clear distinction between the joint pdf and the marginal pdf’s. The joint pdf is\n",
      "just what we called pdf above. The marginal pdf’s are what you might call the\n",
      "\n",
      "74\n",
      "4 Multivariate probability and statistics\n",
      "a)\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "b)\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "c)\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "2.5\n",
      "d)\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "2.5\n",
      "Fig. 4.4: a) scatter plot of data obtained from the pdf in Eq. (4.7). b) marginal pdf of one of the\n",
      "variables in a). c) conditional pdf of z2 given z1 = 0. d) conditional pdf of z2 given z1 = .75\n",
      "“individual” pdf’s of zi, i.e. the pdf’s of those variables, pz1(z1), pz2(z2),... when\n",
      "we just consider one of the variables and ignore the existence of the other variables.\n",
      "There is actually a simple connection between marginal and joint pdf’s. We can\n",
      "obtain a marginal pdf by integrating the joint pdf over one of the variables. This is\n",
      "sometimes called “integrating out”. Consider for simplicity the case where we only\n",
      "have two variables, z1 and z2. Then, the marginal pdf of z1 is obtained by\n",
      "pz1(z1) =\n",
      "Z\n",
      "pz(z1,z2)dz2\n",
      "(4.8)\n",
      "This is a continuous-space version of the intuitive idea that for a given value of z1,\n",
      "we “count” how many observations we have with that value, going through all the\n",
      "possible values of z2.1 (In this continuous-valued case, no observed values of z1 are\n",
      "1 Note again that the notation in Eq. (4.8) is sloppy, because now z1 in the parentheses, both on\n",
      "the left and the right-hand side, stands for any value z1 might obtain, although the same notation is\n",
      "used for the random quantity itself. A more rigorous notation would be something like:\n",
      "pz1(v1) =\n",
      "Z\n",
      "pz(v1,v2)dv2\n",
      "(4.9)\n",
      "\n",
      "4.4 Conditional probabilities\n",
      "75\n",
      "likely to be exactly equal to the speciﬁed value, but we can use the idea of a small\n",
      "interval centred around that value as in the deﬁnition of the pdf above.)\n",
      "Example 3 In the case of the gaussian distribution in Equation (4.6), we have\n",
      "p(z1) =\n",
      "Z\n",
      "p(z1,z2)dz2 =\n",
      "Z\n",
      "1\n",
      "2π exp(−1\n",
      "2(z2\n",
      "1 +z2\n",
      "2))dz2\n",
      "=\n",
      "1\n",
      "√\n",
      "2π\n",
      "exp(−1\n",
      "2z2\n",
      "1)\n",
      "Z\n",
      "1\n",
      "√\n",
      "2π\n",
      "exp(−1\n",
      "2z2\n",
      "2)dz2\n",
      "(4.10)\n",
      "Here, we used the fact that the pdf is factorizable since exp(a+b) = exp(a)exp(b).\n",
      "In the last integral, we recognize the pdf of the one-dimensional gaussian distribu-\n",
      "tion of zero mean and unit variance given in Equation (4.5). Thus, that integral is\n",
      "one, because the integral of any pdf is equal to one. This means that the marginal\n",
      "distribution p(z1) is just the the classic one-dimensional standardized gaussian pdf.\n",
      "Example 4 Going back to our example in Eq. (4.7), we can calculate the marginal\n",
      "pdf of z1 to equal\n",
      "pz1(z1) =\n",
      "(\n",
      "1 −|z1|, if |z1| < 1\n",
      "0, otherwise\n",
      "(4.11)\n",
      "which is plotted in Fig. 4.4 b), and shows the fact that there is more “stuff” near\n",
      "the origin, and no observation can have an absolute value larger than one. Due to\n",
      "symmetry, the marginal pdf of z2 has exactly the same form.\n",
      "4.4 Conditional probabilities\n",
      "Another important concept is the conditional pdf of z2 given z1. This means the pdf\n",
      "of z2 when we have observed the value of z1. Let us denote the observed value of z1\n",
      "by a. Then conditional pdf is basically obtained by just ﬁxing the value of z1 to a in\n",
      "the pdf, which gives pz(a,z2). However, this is not enough because a pdf must have\n",
      "an integral equal to one. Therefore, we must normalize pz(a,z2) by dividing it by\n",
      "its integral. Thus, we obtain the conditional pdf, denoted by p(z2 |z1 = a) as\n",
      "p(z2 |z1 = a) =\n",
      "pz(a,z2)\n",
      "R pz(a,z2)dz2\n",
      "(4.12)\n",
      "Note that the integral in the denominator equals the marginal pdf of z1 at point a, so\n",
      "we can also write\n",
      "where we have used two new variables, v1 to denote the point where we want to evaluate the\n",
      "marginal density, and v2 which is the integration variable. However, in practice we often do not\n",
      "want to introduce new variable names in order to keep things simple, so we use the notation in\n",
      "Eq. (4.8).\n",
      "\n",
      "76\n",
      "4 Multivariate probability and statistics\n",
      "p(z2 |z1 = a) = pz(a,z2)\n",
      "pz1(a)\n",
      "(4.13)\n",
      "Again, for notational simplicity, we can omit the subscripts and just write\n",
      "p(z2 |z1 = a) = p(a,z2)\n",
      "p(a)\n",
      "(4.14)\n",
      "or, we can even avoid introducing the new quantity a and write\n",
      "p(z2 |z1) = p(z1,z2)\n",
      "p(z1)\n",
      "(4.15)\n",
      "Example 5 For the gaussian density in Equation (4.6), the computation of the con-\n",
      "ditional pdf is quite simple, if we use the same factorization as in Equation (4.10):\n",
      "p(z2|z1) = p(z1,z2)\n",
      "p(z1)\n",
      "=\n",
      "1\n",
      "√\n",
      "2π exp(−1\n",
      "2z2\n",
      "1)\n",
      "1\n",
      "√\n",
      "2π exp(−1\n",
      "2z2\n",
      "2)\n",
      "1\n",
      "√\n",
      "2π exp(−1\n",
      "2z2\n",
      "1)\n",
      "=\n",
      "1\n",
      "√\n",
      "2π exp(−1\n",
      "2z2\n",
      "2) (4.16)\n",
      "which turns out to be the same as the marginal distribution of z2. (This kind of situ-\n",
      "ation where p(z2|z1) = p(z2) is related to independence as discussed in Section 4.5\n",
      "below.)\n",
      "Example 6 In our example pdf in Eq. (4.7), the conditional pdf changes quite a lot\n",
      "as a function of the value a of z1. If z1 is zero (i.e. a = 0), the conditional pdf of z2\n",
      "is the uniform density in the interval [−1,1]. In contrast, if z1 is close to 1 (or -1),\n",
      "the values that can be taken by z2 are quite small. Simply ﬁxing z1 = a in the pdf,\n",
      "we have\n",
      "p(a,z2) =\n",
      "(\n",
      "1, if |z2| < 1 −|a|\n",
      "0 otherwise\n",
      "(4.17)\n",
      "which can be easily integrated:\n",
      "Z\n",
      "p(a,z2)dz2 = 2(1 −|a|)\n",
      "(4.18)\n",
      "(This is just the length of the segment in which z2 is allowed to take values.) So, we\n",
      "get\n",
      "p(z2|z1) =\n",
      "(\n",
      "1\n",
      "2−2|z1|, if |z2| < 1 −|z1|\n",
      "0 otherwise\n",
      "(4.19)\n",
      "where we have replaced a by z1. This pdf is plotted for z1 = 0 and z1 = 0.75 in\n",
      "Fig. 4.4 a) and b), respectively.\n",
      "Generalization to many dimensions\n",
      "The concepts of marginal and conditional\n",
      "pdf’s extend naturally to the case where we have n random variables instead of just\n",
      "two. The point is that instead of two random variables, z1 and z2, we can have two\n",
      "\n",
      "4.5 Independence\n",
      "77\n",
      "random vectors, say z1 and z2, and use exactly the same formulas as for the two\n",
      "random variables. So, starting with a random vector z, we take some of its variables\n",
      "and put them in the vector z1, and leave the rest in the vector z2\n",
      "z =\n",
      "\u0012\n",
      "z1\n",
      "z2\n",
      "\u0013\n",
      "(4.20)\n",
      "Now, the marginal pdf of z1 is obtained by the same integral formula as above:\n",
      "pz1(z1) =\n",
      "Z\n",
      "pz(z1,z2)dz2\n",
      "(4.21)\n",
      "and, likewise, the conditional pdf of z2 given z1 is given by:\n",
      "p(z2 |z1) = p(z1,z2)\n",
      "p(z1)\n",
      "(4.22)\n",
      "Both of these are, naturally, multidimensional pdf’s.\n",
      "Discrete-valued variables\n",
      "For the sake of completeness, let us note that these\n",
      "formulas are also valid for random variables with discrete values; then the integrals\n",
      "are simply replaced by sums. For example, for the conditional probabilities, we\n",
      "simply have\n",
      "P(z2 |z1) = Pz(z1,z2)\n",
      "Pz1(z1)\n",
      "(4.23)\n",
      "where marginal probability of z1 can be computed as\n",
      "Pz1(z1) = ∑\n",
      "z2\n",
      "Pz(z1,z2)\n",
      "(4.24)\n",
      "4.5 Independence\n",
      "Let us consider two random variables, z1 and z2. Basically, the variables z1 and z2\n",
      "are said to be statistically independent if information on the value taken by z1 does\n",
      "not give any information on the value of z2, and vice versa.\n",
      "As an example, let us consider again the grey-scale values of two neighbouring\n",
      "pixels. As in Fig. 4.1, we go through many different locations in an image in random\n",
      "order, and take the grey-scale values of the pixels as the observed values of the two\n",
      "random variables. These random variables will not be independent. One of the basic\n",
      "statistical properties of natural images is that two neighbouring pixels are depen-\n",
      "dent. Intuitively, it is clear that two neighbouring pixels tend to have very similar\n",
      "grey-scale values: If one of them is black, then the other one is black with a high\n",
      "probability, so they do give information on each other. This is seen in the oblique\n",
      "shape (having an angle of 45 degrees) of the data “cloud” in Fig. 4.1. Actually, the\n",
      "\n",
      "78\n",
      "4 Multivariate probability and statistics\n",
      "grey-scale values are correlated, which is a special form of dependence as we will\n",
      "see below.\n",
      "The idea that z1 gives no information on z2 can be intuitively expressed using con-\n",
      "ditional probabilities: the conditional probability p(z2 |z1) should be just the same\n",
      "as p(z2):\n",
      "p(z2 |z1) = p(z2)\n",
      "(4.25)\n",
      "for any observed value a of z1. This implies\n",
      "p(z1,z2)\n",
      "p(z1)\n",
      "= p(z2)\n",
      "(4.26)\n",
      "or\n",
      "p(z1,z2) = p(z1)p(z2)\n",
      "(4.27)\n",
      "for any values of z1 and z2. Equation (4.27) is usually taken as the deﬁnition of\n",
      "independence because it is mathematically so simple. It simply says that the joint\n",
      "pdf must be a product of the marginal pdf’s. The joint pdf is then called factorizable.\n",
      "The deﬁnition is easily generalized to n variables z1,z2,...,zn, in which case it is\n",
      "p(z1,z2,...,zn) = p(z1)p(z2)... p(zn)\n",
      "(4.28)\n",
      "Example 7 For the gaussian distribution in Equation (4.6) and Fig. 4.3, we have\n",
      "p(z1,z2) =\n",
      "1\n",
      "√\n",
      "2π\n",
      "exp(−1\n",
      "2z2\n",
      "1)×\n",
      "1\n",
      "√\n",
      "2π\n",
      "exp(−1\n",
      "2z2\n",
      "2)\n",
      "(4.29)\n",
      "So, we have factorized the joint pdf as the product of two pdf’s, each of which\n",
      "depends on only one of the variables. Thus, z1 and z2 are independent. This can also\n",
      "be seen in the form of the conditional pdf in Equation (4.16), which does not depend\n",
      "on the conditioning variable at all.\n",
      "Example 8\n",
      "For our second pdf in Eq. (4.7), we computed the conditional pdf\n",
      "p(z2|z1) in Eq. (4.19). This is clearly not the same as the marginal pdf in Eq. (4.11);\n",
      "it depends on z1. So the variables are not independent. (See the discussion just before\n",
      "Eq. (4.17) for an intuitive explanation of the dependencies.)\n",
      "Example 9 Consider the uniform distribution on a square:\n",
      "p(z1,z2) =\n",
      "(\n",
      "1\n",
      "12,\n",
      "if |z1| ≤\n",
      "√\n",
      "3 and |z2| ≤\n",
      "√\n",
      "3\n",
      "0,\n",
      "otherwise\n",
      "(4.30)\n",
      "A scatter plot from this distribution is shown in Fig. 4.5. Now, z1 and z2 are indepen-\n",
      "dent because the pdf can be expressed as the product of the marginal distributions,\n",
      "which are\n",
      "\n",
      "4.6 Expectation and covariance\n",
      "79\n",
      "p(z1) =\n",
      "(\n",
      "1\n",
      "2\n",
      "√\n",
      "3,\n",
      "if |z1| ≤\n",
      "√\n",
      "3\n",
      "0,\n",
      "otherwise\n",
      "(4.31)\n",
      "and the same for z2.\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "Fig. 4.5: A scatter plot of the two-dimensional uniform distribution in Equation (4.30)\n",
      "4.6 Expectation and covariance\n",
      "4.6.1 Expectation\n",
      "The expectation of a random vector, or its “mean” value, is, in theory, obtained by\n",
      "the same kind of integral as for a single random variable\n",
      "E{z} =\n",
      "Z\n",
      "pz(z)zdz\n",
      "(4.32)\n",
      "In practice, the expectation can be computed by taking the expectation of each vari-\n",
      "able separately, completely ignoring the existence of the other variables\n",
      "E{z} =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "E{z1}\n",
      "E{z2}\n",
      "...\n",
      "E{zn}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "R pz1(z1)z1 dz1\n",
      "R pz2(z2)z2 dz2\n",
      "...\n",
      "R pzn(zn)zn dzn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(4.33)\n",
      "The expectation of any transformation g, whether one- or multidimensional, can be\n",
      "computed as:\n",
      "E{g(z)} =\n",
      "Z\n",
      "pz(z)g(z)dz\n",
      "(4.34)\n",
      "\n",
      "80\n",
      "4 Multivariate probability and statistics\n",
      "The expectation is a linear operation, which means\n",
      "E{az+bs} = aE{z} +bE{s}\n",
      "(4.35)\n",
      "for any constants a and b. In fact, this generalizes to any multiplication by a matrix\n",
      "M:\n",
      "E{Mz} = ME{z}\n",
      "(4.36)\n",
      "4.6.2 Variance and covariance in one dimension\n",
      "The variance of a random variable is deﬁned as\n",
      "var(z1) = E{z2\n",
      "1} −(E{z1})2\n",
      "(4.37)\n",
      "This can also be written var(z1) = E{(z1 −E{z1})2}, which more clearly shows\n",
      "how variance measures average deviation from the mean value.\n",
      "When we have more than one random variable, it is useful to analyze the covari-\n",
      "ance:\n",
      "cov(z1,z2) = E{z1z2} −E{z1}E{z2}\n",
      "(4.38)\n",
      "which measures how well we can predict the value of one of the variables using a\n",
      "simple linear predictor, as will be seen below.\n",
      "The covariance is often normalized to yield the correlation coefﬁcient\n",
      "corr(z1,z2) =\n",
      "cov(z1,z2)\n",
      "p\n",
      "var(z1)var(z2)\n",
      "(4.39)\n",
      "which is invariant to the scaling of the variables, i.e. it is not changed if one or both\n",
      "of the variables is multiplied by a constant.\n",
      "If the covariance is zero, which is equivalent to saying that the correlation coef-\n",
      "ﬁcient is zero, the variables are said to be uncorrelated.\n",
      "4.6.3 Covariance matrix\n",
      "The variances and covariances of the elements of a random vector z are often col-\n",
      "lected to a covariance matrix whose i, j-th element is simply the covariance of zi\n",
      "and zj:\n",
      "C(z) =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cov(z1,z1) cov(z1,z2) ... cov(z1,zn)\n",
      "cov(z2,z1) cov(z2,z2) ... cov(z2,zn)\n",
      "...\n",
      "...\n",
      "...\n",
      "cov(zn,z1) cov(zn,z2) ... cov(zn,zn)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(4.40)\n",
      "\n",
      "4.6 Expectation and covariance\n",
      "81\n",
      "Note that the covariance of zi with itself is the same as the variance of zi. So, the\n",
      "diagonal of the covariance matrix gives the variances. The covariance matrix is ba-\n",
      "sically a generalization of variance to random vectors: in many cases, when moving\n",
      "from a single random variable to random vectors, the covariance matrix takes the\n",
      "place of variance.\n",
      "In matrix notation, the covariance matrix is simply obtained as a generalization\n",
      "of the one-dimensional deﬁnitions in Equations (4.38) and (4.37) as\n",
      "C(z) = E{zzT} −E{z}E{z}T\n",
      "(4.41)\n",
      "where taking the transposes in the correct places is essential. In most of this book,\n",
      "we will be dealing with random variables whose means are zero, in which case the\n",
      "second term in Equation (4.41) is zero.\n",
      "If the variables are uncorrelated, the covariance matrix is diagonal. If they are\n",
      "all further standardized to unit variance, the covariance matrix equals the identity\n",
      "matrix.\n",
      "The covariance matrix is the basis for the analysis of natural images in the next\n",
      "chapter. However, in many further chapters, the covariance matrix is not enough,\n",
      "and we need further concepts, such as independence, so we need to understand the\n",
      "connection between these concepts.\n",
      "4.6.4 Independence and covariances\n",
      "A most important property of independent random variables z1 and z2 is that the\n",
      "expectation of any product of a function of z1 and a function of z2 is equal to the\n",
      "product of the expectations:\n",
      "E{g1(z1)g2(z2)} = E{g1(z1)}E{g2(z2)}\n",
      "(4.42)\n",
      "for any functions g1 and g2. This implies that independent variables are uncorre-\n",
      "lated, since we can take g1(z) = g2(z) = z, in which case Eq. (4.42) simply says that\n",
      "the covariance is zero.\n",
      "Example 10 In the standardized gaussian distribution in Equation (4.6), the means\n",
      "of both z1 and z2 are zero, and their variances are equal to one (we will not try to\n",
      "prove this here). Actually, the word “standardized” means exactly that the means\n",
      "and variances have been standardized in this way. The covariance cov(z1,z2) equals\n",
      "zero, because the variables are independent, and thus uncorrelated.\n",
      "Example 11\n",
      "What would be the covariance of z1 and z2 in our example pdf in\n",
      "Eq. (4.7)? First, we have to compute the means. Without computing any integrals,\n",
      "we can actually see that E{z1} = E{z2} = 0 because of symmetry: both variables are\n",
      "symmetric with respect to the origin, so their means are zero. This can be justiﬁed as\n",
      "follows: take a new variable y = −z1. Because of symmetry of the pdf with respect\n",
      "\n",
      "82\n",
      "4 Multivariate probability and statistics\n",
      "to zero, the change of sign has no effect and the pdf of y is just the same as the pdf\n",
      "of z1. Thus, we have\n",
      "E{y} = E{−z1} = −E{z1} = E{z1}\n",
      "(4.43)\n",
      "which implies that E{z1} = 0. Actually, the covariance is zero because of the same\n",
      "kind of symmetry with respect to zero. Namely, we have cov(y,z2) = cov(z1,z2)\n",
      "because again, the change of sign has no effect and the joint pdf of y,z2 is just\n",
      "the same as the pdf of z1,z2. This means cov(y,z2) = E{(−z1)z2} = −E{z1z2} =\n",
      "E{z1z2}. This obviously implies that the covariance is zero. The covariance matrix\n",
      "of the vector z is thus diagonal (we don’t bother to compute the diagonal elements,\n",
      "which are the variances).\n",
      "Example 12 Let’s have a look at a more classical example of covariances. Assume\n",
      "that z1 has mean equal to zero and variance equal to one. Assume that n (a “noise”\n",
      "variable) is independent from z1. Let us consider a variable z2 which is a linear\n",
      "function of x, with noise added:\n",
      "z2 = az1 +n\n",
      "(4.44)\n",
      "What is the covariance of the two variables? We can calculate\n",
      "cov(z1,z2) = E{z1(az1 +n)} +0 ×E{z2} = aE{z2\n",
      "1} +E{z1n}\n",
      "= a +E{z1}E{n} = a +0 ×E{z1} = a\n",
      "(4.45)\n",
      "Here, we have the equality E{z1n} = E{z1}E{n} because of the uncorrelatedness\n",
      "of z1 and n, which is implied by their independence. A scatter plot of such data,\n",
      "created for parameter a set at 0.5 and with noise variance var(n) = 1, is shown in\n",
      "Fig. 4.6. The covariance matrix of the vector z = (z1,z2) is equal to\n",
      "C(z) =\n",
      "\u0012\n",
      "1 a\n",
      "a 1\n",
      "\u0013\n",
      "(4.46)\n",
      "Example 13\n",
      "White noise refers to a collection of random variables which are in-\n",
      "dependent and have the same distribution. (In some sources, only uncorrelatedness\n",
      "is required, not independence, but in this book the deﬁnition of white noise includes\n",
      "independence.) Depending on the context, the variables could be the value of noise\n",
      "at different time points n(t), or at different pixels N(x,y). In the ﬁrst case, white\n",
      "noise in the system is independent at different time points; in the latter, noise at dif-\n",
      "ferent pixels is independent. When modelling physical noise, which can be found in\n",
      "most measurement devices, it is often realistic and mathematically simple to assume\n",
      "that the noise is white.\n",
      "\n",
      "4.7 Bayesian inference\n",
      "83\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "Fig. 4.6: A scatter plot of the distribution created by the dependence relation in Equation (4.44)\n",
      "4.7 Bayesian inference\n",
      "Bayesian inference is a framework that has recently been increasingly applied to\n",
      "model such phenomena as perception and intelligence. There are two viewpoints on\n",
      "what Bayesian inference is.\n",
      "1. Bayesian inference attempts to infer underlying causes when we observe their\n",
      "effects.\n",
      "2. Bayesian inference uses prior information on parameters in order to estimate\n",
      "them better.\n",
      "Both of these goals can be accomplished by using the celebrated Bayes’ formula,\n",
      "which we will now explain.\n",
      "4.7.1 Motivating example\n",
      "Let us start with a classic example. Assume that we have a test for a rare genetic\n",
      "disorder. The test is relatively reliable but not perfect. For a patient with the disorder,\n",
      "the probability of a positive test result is 99%, whereas for a patient without the\n",
      "disorder, the probability of a positive test is only 2%. Let us denote the test result by\n",
      "t and the disorder by d. A positive test result is expressed as t = 1 and a negative one\n",
      "as t = 0. Likewise, d = 1 means that the patient really has the disorder, whereas d = 0\n",
      "means the patients doesn’t. Then, the speciﬁcations we just gave can be expressed\n",
      "as the following conditional probabilities:\n",
      "P(t = 1|d = 1) = .99\n",
      "(4.47)\n",
      "P(t = 1|d = 0) = .02\n",
      "(4.48)\n",
      "\n",
      "84\n",
      "4 Multivariate probability and statistics\n",
      "Because probabilities sum to one, we immediately ﬁnd the following probabilities\n",
      "as well:\n",
      "P(t = 0|d = 1) = .01\n",
      "(4.49)\n",
      "P(t = 0|d = 0) = .98\n",
      "(4.50)\n",
      "Now, the question we want to answer is: Given a positive test result, what is the\n",
      "probability that the patient has the disorder? Knowing this probability is, of course,\n",
      "quite important when applying this medical test. Basically, we then want to compute\n",
      "the conditional probability of the form p(d = 1|t = 1). The order of the variables in\n",
      "this conditional probability is reversed from the formulas above. This is because the\n",
      "formulas above gave us the observable effects given the causes, but now we want to\n",
      "know the causes, given observations of their effects.\n",
      "To ﬁnd that probability, let’s try to use the deﬁnition in Eq. (4.23)\n",
      "P(d = 1|t = 1) = P(d = 1,t = 1)\n",
      "P(t = 1)\n",
      "(4.51)\n",
      "which presents us with two problems: We know neither the denominator nor the nu-\n",
      "merator. To get further, let’s assume we know the marginal distribution P(d). Then,\n",
      "we can easily ﬁnd the numerator by using the deﬁnition of conditional probability\n",
      "P(d = 1,t = 1) = P(t = 1|d = 1)P(d = 1)\n",
      "(4.52)\n",
      "and after some heavy thinking, we see that we can also compute the denominator in\n",
      "Eq. (4.51) by using the formula for marginal probability:\n",
      "P(t = 1) = P(d = 1,t = 1)+P(d = 0,t = 1)\n",
      "(4.53)\n",
      "which can be computed once we know the joint probabilities by (4.52) and its cor-\n",
      "responding version with d = 0. Thus, in the end we have\n",
      "P(d = 1|t = 1) =\n",
      "P(t = 1|d = 1)P(d = 1)\n",
      "P(t = 1|d = 1)P(d = 1)+P(t = 1|d = 0)P(d = 0)\n",
      "(4.54)\n",
      "So, we see that the key to inferring the causes from observed effects is to know\n",
      "the marginal distribution of the causes, in this case P(d). This distribution is also\n",
      "called the prior distribution of d, because it incorporates our knowledge of the cause\n",
      "d prior to any observations. For example, let’s assume 0.1% of the patients given\n",
      "this test have the genetic disorder. Then, before the test our best guess is that a\n",
      "given patient has the disorder with the probability of 0.001. However, after making\n",
      "the test, we have more information on the patient, and that information is given\n",
      "by the conditional distribution P(d |t = 1) which we are trying to compute. This\n",
      "distribution, which incorporates both our prior knowledge on d and the observation\n",
      "of t, is called the posterior probability.\n",
      "To\n",
      "see\n",
      "a\n",
      "rather\n",
      "surprising\n",
      "phenomenon,\n",
      "let\n",
      "us\n",
      "plug\n",
      "in\n",
      "the\n",
      "value\n",
      "P(d = 1) = 0.001 as the prior probability of disorder in Equation (4.54). Then,\n",
      "\n",
      "4.7 Bayesian inference\n",
      "85\n",
      "we can calculate\n",
      "P(d = 1|t = 1) =\n",
      "0.99 ×0.001\n",
      "0.99 ×0.001 +0.02×(1 −0.001) ≈0.05\n",
      "(4.55)\n",
      "Thus, even after a positive test result, the probability that the patient has the disorder\n",
      "is approximately 5%. Many people ﬁnd this quite surprising, because they would\n",
      "have guessed that the probability is something like 99%, as the test gives the right\n",
      "result in 99% of the cases.\n",
      "This posterior probability depends very much on the prior probability. Assume\n",
      "that half the tested patients actually have the disorder, P(d = 1) = 0.5. Then the\n",
      "posterior probability is 99%. This prior actually gives us no information because the\n",
      "chances are 50-50, and the 99% accuracy of the test is directly seen in the posterior\n",
      "probability.\n",
      "Thus, in cases where the prior assigns very different probabilities to different\n",
      "causes, Bayesian inference shows that the posterior probabilities of the causes can\n",
      "be very different from what one might expect by just looking at the effects.\n",
      "4.7.2 Bayes’ Rule\n",
      "The logic of the previous section was actually the proof of the celebrated Bayes’\n",
      "rule. In the general case, we consider a continuous-valued random vector s that\n",
      "gives the causes and z that gives the observed effects. The Bayes’ rule then takes the\n",
      "form\n",
      "p(s|z) =\n",
      "p(z|s) ps(s)\n",
      "R p(z|s) ps(s)ds\n",
      "(4.56)\n",
      "which is completely analogous to Eq. (4.54) and can be derived in the same way.\n",
      "This is the Bayes’ rule, in one of its formulations. It gives the posterior distribution\n",
      "of s based on its prior distribution p(s) and the conditional probabilities p(z|s). Note\n",
      "that instead of random variables, we can directly use vectors in the formula without\n",
      "changing anything.\n",
      "To explicitly show what is random and what is observed in Bayes rule, we should\n",
      "rewrite it as\n",
      "p(s = b|z = a) =\n",
      "pz|s(a|b) ps(b)\n",
      "R pz|s(a|u) ps(u)du\n",
      "(4.57)\n",
      "where pz|s(a|b) is the conditional probability p(z = a|s = b), a is the observed value\n",
      "of z, and b is a possible value of s. This form is, of course, much more difﬁcult to\n",
      "read than Eq. (4.56).\n",
      "In theoretical treatment, Bayes rule can sometimes be simpliﬁed because the de-\n",
      "nominator is actually equal to p(z), which gives\n",
      "p(s|z) = p(z|s) ps(s)\n",
      "pz(z)\n",
      "(4.58)\n",
      "\n",
      "86\n",
      "4 Multivariate probability and statistics\n",
      "However, in practice we usually have to use the form in Eq. (4.56) because we do\n",
      "not know how to directly compute pz.\n",
      "The prior ps contains the prior information on the random variable s. The con-\n",
      "ditional probabilities p(z|s) show the connection between the observed quantity z\n",
      "(the “effect”) and the underlying variable s (the “cause”).\n",
      "Where do we get the prior distribution p(s)? In some cases, p(s) can be esti-\n",
      "mated, because we might be able to observe the original s. In the medical example\n",
      "above, the prior distribution p(d) can be estimated if some of the patients are sub-\n",
      "jected to additional tests which are much more accurate (thus usually much more\n",
      "expensive) so that we really know for sure how many of the patients have the dis-\n",
      "order. In other cases, the prior might be formulated more subjectively, based on the\n",
      "opinion of an expert.\n",
      "4.7.3 Non-informative priors\n",
      "Sometimes, we have no information on the prior probabilities ps. Then, we should\n",
      "use a non-informative prior that expresses this fact. In the case of discrete variables,\n",
      "a non-informative prior is one that assigns the same probability to all the possible\n",
      "values of s (e.g. 50% probability of a patient to have the disorder or not).\n",
      "In the case of continuous-valued priors deﬁned in the whole real line [−∞,∞], the\n",
      "situation is a bit more complicated. If we take a “ﬂat” pdf that is constant, p(s) = c,\n",
      "it cannot be a real pdf because the integral of such a pdf is inﬁnite (or zero if c = 0).\n",
      "Such a prior is called improper. Still, they can often be used in Bayesian inference\n",
      "even though the non-integrability may pose some theoretical problems.\n",
      "What happens in the Bayes rule if we take such a ﬂat, non-informative prior? We\n",
      "get\n",
      "p(s|z) =\n",
      "p(z|s)c\n",
      "R p(z|s)cds =\n",
      "p(z|s)\n",
      "R p(z|s)ds\n",
      "(4.59)\n",
      "The denominator does not depend on s (this is always true in Bayes’ rule), so we\n",
      "see that p(s|z) is basically the same of p(z|s); it is just rescaled so that the integral\n",
      "is equal to one. What this shows is that if we have no information on the prior\n",
      "probabilities, the probabilities of effects given the causes are simply proportional to\n",
      "the probabilities of causes given the effects. However, if the prior p(s) is far from\n",
      "ﬂat, these two probabilities can be very different from each other, as the example\n",
      "above showed in the case where the disorder is rare.\n",
      "4.7.4 Bayesian inference as an incremental learning process\n",
      "The transformation from the prior probability p(s) to p(s|z) can be compared to an\n",
      "incremental (on-line) learning process where a biological organism receives more\n",
      "and more information in an uncertain environment.\n",
      "\n",
      "4.7 Bayesian inference\n",
      "87\n",
      "In the beginning, the organism’s belief about the value of a quantity s is the\n",
      "prior probability p(s). Here we assume that the organism performs probabilistic\n",
      "inference: the organism does not “think” that it knows the value of s with certainty;\n",
      "rather, it just assigns probabilities to different values s might take. This does not\n",
      "mean that we assume the organism is highly intelligent and knows Bayes’ rule.\n",
      "Rather, we assume that the neural networks in the nervous system of the organism\n",
      "have evolved to perform something similar.\n",
      "Then, the organism receives information via sensory organs or similar means. A\n",
      "statistical formulation of “incoming information” is that the organism observes the\n",
      "value of a random variable z1. Now, the belief of the organism is expressed by the\n",
      "posterior pdf p(s|z1). This pdf gives the probabilities that the organism assigns to\n",
      "different values of s.\n",
      "Next, assume that the organism observes another piece of information, say z2.\n",
      "Then the organism’s belief is changed to p(s|z1,z2)\n",
      "p(s|z1,z2) = p(z1,z2 |s)p(s)\n",
      "p(z1,z2)\n",
      "(4.60)\n",
      "Assume further that z2 is independent from z1 given s, which means p(z1,z2 |s) =\n",
      "p(z1 |s)p(z2 |s) (see Sec. 4.5 for more on independence). Then, the posterior be-\n",
      "comes\n",
      "p(s|z1,z2) = p(z1 |s)p(z2 |s)p(s)\n",
      "p(z1)p(z2)\n",
      "= p(z2 |s)\n",
      "p(z2)\n",
      "p(z1 |s)p(s)\n",
      "p(z1)\n",
      "(4.61)\n",
      "Now, the expression p(z1 |s)p(s)/p(z1) is nothing but the posterior p(s|z1) that the\n",
      "organism computed previously. So, we have\n",
      "p(s|z1,z2) = p(z2 |s)p(s|z1)\n",
      "p(z2)\n",
      "(4.62)\n",
      "The right-hand side is just like the Bayes’ Rule applied on s and z2 but instead of the\n",
      "prior p(s) it has p(s|z1). Thus, the new posterior (after observing z2) is computed\n",
      "as if the previous posterior were a prior.\n",
      "This points out an incremental learning interpretation of Bayes rule. When the\n",
      "organism observes new information (new random variables), it updates its belief\n",
      "about the world by the Bayes rule, where the current belief is taken as the prior, and\n",
      "the new belief is computed as the posterior. This is illustrated in Fig. 4.7\n",
      "Such learning can happen on different time scales. It could be that s is a very\n",
      "slowly changing parameter, say, the length of the arms (or tentacles) of the organism.\n",
      "In that case, the organism can collect a large number of observations over time, and\n",
      "the belief would change very slowly. The ﬁrst “prior” belief that the organism may\n",
      "have had before collection of any data, eventually loses its signiﬁcance (see next\n",
      "section).\n",
      "On the other hand, s could be a quantity that has to be computed instantaneously,\n",
      "say, the probability that the animal in front of you is trying to eat you. Then, only\n",
      "a few observed quantities (given by the current visual input) are available. Such\n",
      "\n",
      "88\n",
      "4 Multivariate probability and statistics\n",
      "Prior p(s)\n",
      "Posterior p(s|z)\n",
      "Replace prior by posterior\n",
      "Observe new z\n",
      "Fig. 4.7: Computation of posterior as an incremental learning process. Given the current prior, the\n",
      "organism observes the input z, and computes the posterior p(s|z). The prior is then replaced by this\n",
      "new posterior, which is used as the prior in the future.\n",
      "inference can then be heavily inﬂuenced by the prior information that the organism\n",
      "has at the moment of encountering the animal. For example, if the animal is small\n",
      "and cute, the prior probability is small, and even if the animal seems to behaves in\n",
      "an aggressive way, you will probably infer that it is not going to try to eat you.\n",
      "4.8 Parameter estimation and likelihood\n",
      "4.8.1 Models, estimation, and samples\n",
      "A statistical model describes the pdf of the observed random vector using a number\n",
      "of parameters. The parameters typically have an intuitive interpretation; for exam-\n",
      "ple, in this book, they often deﬁne image features. A model is basically a conditional\n",
      "density of the observed data variable, p(z|α), where α is the parameter. The param-\n",
      "eter could be a multidimensional vector as well. Different values of the parameter\n",
      "imply different distributions for the data, which is why this can be thought of as a\n",
      "conditional density.\n",
      "For example, consider the one-dimensional gaussian pdf\n",
      "p(z|α) =\n",
      "1\n",
      "√\n",
      "2π\n",
      "exp(−1\n",
      "2(z−α)2)\n",
      "(4.63)\n",
      "Here, the parameter α has an intuitive interpretation as the mean of the distribution.\n",
      "Given α, the observed data variable z then takes values around α, with variance\n",
      "equal to one.\n",
      "Typically, we have a large number of observations of the random variable z,\n",
      "which might come from measuring some phenomenon n times, and these obser-\n",
      "vations are independent. The set of observations is called a sample in statistics.2 So,\n",
      "we want to use all the observations to better estimate the parameters. For example,\n",
      "2 In signal processing, sampling refers the process of reducing a continuous signal to a discrete\n",
      "signal. For example, an image I(x,y) with continuous-valued coordinates x and y is reduced to a\n",
      "\n",
      "4.8 Parameter estimation and likelihood\n",
      "89\n",
      "in the model in (4.63), it is obviously not a very good idea to estimate the mean of\n",
      "the distribution based on just a single observation.\n",
      "Estimation has a very boring mathematical deﬁnition, but basically it means that\n",
      "we want to ﬁnd a reasonable approximation of the value of the parameter based on\n",
      "the observations in the sample. A method (a formula or an algorithm) that estimates\n",
      "α is called an estimator. The value given by the estimator for a particular sample is\n",
      "called an estimate. Both are usually denoted by a hat: ˆα.\n",
      "Assume we now have a sample of n observations. Let us denote the observed\n",
      "values by z(1),z(2),...,z(n). Because the observations are independent, the joint\n",
      "probability is simply obtained by multiplying the probabilities of the observations,\n",
      "so we have\n",
      "p(z(1),z(2),...,z(n)|α) = p(z(1)|α)× p(z(2)|α)×...× p(z(n)|α)\n",
      "(4.64)\n",
      "This conditional density is called the likelihood. It is often simpler to consider the\n",
      "logarithm, which transforms products into sums. If we take the logarithm, we have\n",
      "the log-likelihood as\n",
      "log p(z(1),z(2),...,z(n)|α) = log p(z(1)|α)+log p(z(2)|α)+...\n",
      "+log p(z(n)|α)\n",
      "(4.65)\n",
      "4.8.2 Maximum likelihood and maximum a posteriori\n",
      "The question is then, How can we estimate α? In a Bayesian interpretation, we can\n",
      "consider the parameters as “causes” in Bayes’ rule, and the observed data are the\n",
      "effects. Then, the estimation of the parameters means that we compute the posterior\n",
      "pdf of the parameters using Bayes rule:\n",
      "p(α |z(1)...,z(n)) = p(z(1)...,z(n)|α)p(α)\n",
      "p(z(1)...,z(n))\n",
      "(4.66)\n",
      "In estimating parameters of the model, one usually takes a ﬂat prior, i.e. p(α) = c.\n",
      "Moreover, the term p(z(1)...,z(n)) does not depend on α, it is just for normaliza-\n",
      "tion, so we don’t need to care about its value. Thus, we see that\n",
      "p(α |z(1)...,z(n)) = p(z(1),z(2),...,z(n)|α)×const.\n",
      "(4.67)\n",
      "the posterior of the parameters is proportional to the likelihood in the case of a ﬂat\n",
      "prior.\n",
      "Usually, we want a single value as an estimate. Thus, we have to somehow sum-\n",
      "marize the posterior distribution p(α |z(1)...,z(n)), which is a function of α. The\n",
      "ﬁnite-dimensional vector in which the coordinates x and y take only a limited number of values\n",
      "(e.g. as on a rectangular grid). These two meanings of the word “sample” need to be distinguished.\n",
      "\n",
      "90\n",
      "4 Multivariate probability and statistics\n",
      "most widespread solution is to use the value of α that gives the highest value of the\n",
      "posterior pdf. Such estimation is called maximum a posteriori (MAP) estimation.\n",
      "In the case of a ﬂat prior, the maximum of the posterior distribution is obtained at\n",
      "the same point as the maximum of the likelihood, because likelihood is then propor-\n",
      "tional to the posterior. Thus, the estimation is then called the maximum likelihood\n",
      "estimator. If the prior is not ﬂat, the maximum a posteriori estimator may be quite\n",
      "different from the maximum likelihood estimator.\n",
      "The maximum likelihood estimator has another intuitive interpretation: it gives\n",
      "the parameter value that gives the highest probability for the observed data. This in-\n",
      "terpretation is slightly different from the Bayesian interpretation that we used above.\n",
      "Sometimes the maximum likelihood estimator can be computed by a simple al-\n",
      "gebraic formula, but in most cases, the maximization has to be done numerically.\n",
      "For a brief introduction to optimization methods, see Chapter 18.\n",
      "Example 13 In the case of the model in Eq. (4.63), we have\n",
      "log p(z|α) = −1\n",
      "2(z−α)2 +const.\n",
      "(4.68)\n",
      "where the constant is not important because it does not depend on α. So, we have\n",
      "for a sample\n",
      "log p(z(1),z(2),...,z(n)|α) = −1\n",
      "2\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "(z(i)−α)2 +const.\n",
      "(4.69)\n",
      "It can be shown (this is left as an exercise) that this is maximized by\n",
      "ˆα = 1\n",
      "n\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "z(i)\n",
      "(4.70)\n",
      "Thus, the maximum likelihood estimator is given by the average of the observed\n",
      "values. This is not a trivial result: in some other models, the maximum likelihood\n",
      "estimator of such a location parameter is given by the median.\n",
      "Example 14\n",
      "Here’s an example of maximum likelihood estimation with a less\n",
      "obvious result. Consider the exponential distribution\n",
      "p(z|α) = α exp(−αz)\n",
      "(4.71)\n",
      "where z is constrained to be positive. The parameter α determines how likely large\n",
      "values are and what the mean is. Some examples of this pdf are shown in Fig. 4.8.\n",
      "The log-pdf is given by\n",
      "log p(z|α) = logα −αz\n",
      "(4.72)\n",
      "so the log-likelihood for a sample equals\n",
      "\n",
      "4.8 Parameter estimation and likelihood\n",
      "91\n",
      "log p(z(1),z(2),...,z(n)|α) = nlogα −α\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "z(i)\n",
      "(4.73)\n",
      "To solve for the α which maximizes the likelihood, we take the derivative of this\n",
      "with respect to α and ﬁnd the point where it is zero. This gives\n",
      "n\n",
      "α −\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "z(i) = 0\n",
      "(4.74)\n",
      "from which we obtain\n",
      "ˆα =\n",
      "1\n",
      "1\n",
      "n ∑n\n",
      "i=1 z(i)\n",
      "(4.75)\n",
      "So, the estimate is the reciprocal of the mean of the z in the sample.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "2.5\n",
      "3\n",
      "Fig. 4.8: The exponential pdf in Equation (4.71) plotted for three different values of α, which is\n",
      "equal 1,2, or 3. The value of α is equal to the value of the pdf at zero.\n",
      "4.8.3 Prior and large samples\n",
      "If the prior is not ﬂat, we have the log-posterior\n",
      "log p(α |z(1),z(2),...,z(n))\n",
      "= log p(α)+log p(z(1)|α)+log p(z(2)|α)+...+log p(z(n)|α)+const.\n",
      "(4.76)\n",
      "which usually needs to be maximized numerically.\n",
      "Looking at Equation (4.76), we see an interesting phenomenon: when n grows\n",
      "large, the prior loses its signiﬁcance. There are more and more terms in the like-\n",
      "lihood part, and, eventually, they will completely determine the posterior because\n",
      "\n",
      "92\n",
      "4 Multivariate probability and statistics\n",
      "the single prior term will not have any inﬂuence anymore. In other words, when\n",
      "we have a very large sample, the data outweighs the prior information. This phe-\n",
      "nomenon is related to the learning interpretation we discussed above: the organism\n",
      "eventually learns so much from the incoming data that the prior belief it had in the\n",
      "very beginning is simply forgotten.\n",
      "4.9 References\n",
      "Most of the material in this chapter is very classic. Most of it can be found in ba-\n",
      "sic textbooks to probability theory, while Section 4.8 can be found in introductory\n",
      "textbooks to the theory of statistics. A textbook covering both areas is (Papoulis and\n",
      "Pillai, 2001). Some textbooks on probabilistic machine learning also cover all this\n",
      "material, in particular (Mackay, 2003; Bishop, 2006).\n",
      "4.10 Exercices\n",
      "Mathematical exercises\n",
      "1. Show that a conditional pdf as deﬁned in Eq. (4.15) is properly normalized, i.e.\n",
      "its integral is always equal to one.\n",
      "2. Compute the mean and variance of a random variable distributed uniformly in\n",
      "the interval [a,b] (b > a).\n",
      "3. Consider n scalar random variables xi, i = 1,2,...,n, having, respectively, the\n",
      "variances σ2\n",
      "xi. Show that if the random variables xi are all uncorrelated, the vari-\n",
      "ance of their sum equals the sum of their variances\n",
      "σ2\n",
      "y =\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "σ2\n",
      "xi\n",
      "(4.77)\n",
      "4. Assume the random vector x has uncorrelated variables, all with unit variance.\n",
      "Show that the covariance matrix equals the identity matrix.\n",
      "5. Take a linear transformation of x in the preceding exercise: y = Mx for some\n",
      "matrix M. Show that the covariance matrix of y equals MMT .\n",
      "6. Show that the maximum likelihood estimator of the mean of a gaussian distribu-\n",
      "tion equals the sample average, i.e. Eq. (4.70).\n",
      "7. Next we consider estimation of the variance parameter in a gaussian distribution.\n",
      "We have the pdf\n",
      "p(z|σ) =\n",
      "1\n",
      "√\n",
      "2πσ\n",
      "exp(−z2\n",
      "2σ2 )\n",
      "(4.78)\n",
      "\n",
      "4.10 Exercices\n",
      "93\n",
      "Formulate the likelihood and the log-likelihood, given a sample z(1),...,z(n).\n",
      "Then, ﬁnd the maximum likelihood estimator for σ.\n",
      "Computer assignments\n",
      "1. Generate 1,000 samples of 100 independent observations of a gaussian variable\n",
      "of zero mean and unit variance (e.g. with Matlab’s randn function). That is,\n",
      "you generate a matrix of size 1000 ×100 whose all elements are all independent\n",
      "gaussian observations.\n",
      "a. Compute the average of each sample. This is the maximum likelihood estima-\n",
      "tor of the mean for that sample.\n",
      "b. Plot a histogram of the 1,000 sample averages.\n",
      "c. Repeat all the above, increasing the sample size to 1,000 and to 10,000.\n",
      "d. Compare the three histograms. What is changing?\n",
      "2. Generate a sample of 10,000 observations of a two-dimensional random vector\n",
      "x with independent standardized gaussian variables. Put each observation in a\n",
      "column and each random variable in a row, i.e. you have a 2 × 10000 matrix,\n",
      "denote it by X.\n",
      "a. Compute the covariance matrix of this sample of x, e.g. by using the cov\n",
      "function in Matlab. Note that the transpose convention in Matlab is different\n",
      "from what we use here, so you have to apply the cov function of the transpose\n",
      "of X. Compare the result with the theoretical covariance matrix (what is its\n",
      "value?)\n",
      "b. Multiply x (or in practice, X) from the left with the matrix\n",
      "A =\n",
      "\u00122 3\n",
      "0 1\n",
      "\u0013\n",
      "(4.79)\n",
      "Compute the covariance matrix of Ax. Compare with AAT.\n",
      "\n",
      "\n",
      "Part II\n",
      "Statistics of linear features\n",
      "\n",
      "\n",
      "Chapter 5\n",
      "Principal components and whitening\n",
      "The most classical method of analyzing the statistical structure of multidimen-\n",
      "sional random data is principal component analysis (PCA), which is also called the\n",
      "Karhunen-Lo`eve transformation, or the Hotelling transformation. In this chapter we\n",
      "will consider the application of PCA to natural images. It will be found that it is not a\n",
      "successful model in terms of modelling the visual system. However, PCA provides\n",
      "the basis for all subsequent models. In fact, before applying the more successful\n",
      "models described in the following chapters, PCA is often applied as a preprocessing\n",
      "of the data. So, the investigation of the statistical structure of natural images must\n",
      "start with PCA.\n",
      "Before introducing PCA, however, we will consider a very simple and funda-\n",
      "mental concept: the DC component.\n",
      "5.1 DC component or mean grey-scale value\n",
      "To begin with, we consider a simple property of an image patch: its DC component.\n",
      "The DC component refers to the mean grey-scale value of the pixels in an image or\n",
      "an image patch.1 It is often assumed that the DC component does not contain inter-\n",
      "esting information. Therefore, it is often removed from the image before any further\n",
      "processing to simplify the analysis. Removing the DC component thus means that\n",
      "we preprocess each image (in practice, image patch) as follows\n",
      "˜I(x,y) = I(x,y)−1\n",
      "m ∑\n",
      "x′,y′\n",
      "I(x′,y′)\n",
      "(5.1)\n",
      "where m is the number of pixels. All subsequent computations would then use ˜I.\n",
      "1 The name ”DC” comes from a rather unrelated context in electrical engineering, in which it\n",
      "originally meant “direct current” as opposed to “alternating current”. The expression has become\n",
      "rather synonymous with “constant” in electrical engineering.\n",
      "97\n",
      "\n",
      "98\n",
      "5 Principal components and whitening\n",
      "In section 1.8, we looked at the outputs of some simple feature detectors when the\n",
      "input is natural images. Let us see what the effect of DC removal is on the statistics\n",
      "of these features; the features are depicted in Fig. 1.10 on page 20. Let us denote the\n",
      "output of a linear feature detector with weights Wi(x,y) by s:\n",
      "si = ∑\n",
      "x,y\n",
      "Wi(x,y)I(x,y)\n",
      "(5.2)\n",
      "The ensuing histograms of the si, for the three detectors when input with natural\n",
      "images, and after DC removal, are shown in Fig. 5.1. Comparing with Fig. 1.11 on\n",
      "page 20, we can see that the ﬁrst histogram changes radically, where as the latter two\n",
      "do not. This is because the latter two ﬁlters were not affected by the DC component\n",
      "in the ﬁrst place, which is because the sum of their weight was approximately zero:\n",
      "∑x,yW(x,y) = 0. Actually, the three histograms are now more similar to each other:\n",
      "the main difference is in the scale. However, they are by no means identical, as will\n",
      "be seen in the analyses of this book.\n",
      "The effect of DC component removal depends on the size of the image patch.\n",
      "Here, the patches were relatively small, so the removal had a large effect on the\n",
      "statistics. In contrast, removing the DC component from whole images has little\n",
      "effect on the statistics.\n",
      "In the rest of this book, we will assume that the DC component has been removed\n",
      "unless otherwise mentioned. Removing the DC component also means that the mean\n",
      "of any s is zero; this is intuitively rather obvious but needs some assumptions to be\n",
      "shown rigorously (see Exercises). Thus, in what follows we shall assume that the\n",
      "mean of any feature s is zero.\n",
      "a)\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "b)\n",
      "−40\n",
      "−20\n",
      "0\n",
      "20\n",
      "40\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "c)\n",
      "−10\n",
      "−5\n",
      "0\n",
      "5\n",
      "10\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "Fig. 5.1: Effect of DC removal. These are histograms of the outputs of the ﬁlters in Fig. 1.10 when\n",
      "the output is natural images whose DC component has been removed. Left: output of Dirac ﬁlter,\n",
      "which is the same as the histogram of the original pixels themselves. Center: output of grating\n",
      "feature detector. Right: output of edge detector. The scales of the axes are different from those in\n",
      "Fig. 1.10.\n",
      "Some examples of natural image patches with DC component removed are shown\n",
      "in Figure 5.2. This is the kind of data analyzed in almost all of the rest of this book.\n",
      "\n",
      "5.2 Principal component analysis\n",
      "99\n",
      "Fig. 5.2: Some natural image patches, with DC component removed.\n",
      "5.2 Principal component analysis\n",
      "5.2.1 A basic dependency of pixels in natural images\n",
      "The point in PCA is to analyze the dependencies of the pixel grey-scale values\n",
      "I(x,y) and I(x′,y′) for two different pixel coordinate pairs (x,y) and (x′,y′). More\n",
      "speciﬁcally, PCA considers the second-order structure of natural images, i.e. the\n",
      "variances and and covariances of pixel values I(x,y).\n",
      "If the pixel values were all uncorrelated, PCA would have nothing to analyze.\n",
      "Even a rudimentary analysis of natural images shows, however, that the pixel val-\n",
      "ues are far from independent. It is intuitively rather clear that natural images are\n",
      "typically smooth in the sense that quite often, the pixel values are very similar in\n",
      "two near-by pixels. This can be easily demonstrated by a scatter plot of the pixel\n",
      "values for two neighbouring pixels sampled from natural images. This is shown in\n",
      "Figure 5.3. The scatter plot shows that the pixels are correlated. In fact, we can com-\n",
      "pute the correlation coefﬁcient (Equation 4.39), and it turns out to be approximately\n",
      "equal to 0.9.\n",
      "Actually, we can easily compute the correlation coefﬁcients of a single pixel with\n",
      "all near-by pixels. Such a plot is shown in grey-scale in Fig. 5.4, both without re-\n",
      "moval of DC component (in a) and with DC removal (in b). We see that the correla-\n",
      "tion coefﬁcients (and, thus, the covariances) fall off with increasing distance. These\n",
      "two plots, with or without DC removal, look rather similar because the plots use\n",
      "different scales; the actual values are quite different. We can take one-dimensional\n",
      "cross-sections to see the actual values. They are shown in Fig. 5.4 c) and d). We see\n",
      "\n",
      "100\n",
      "5 Principal components and whitening\n",
      "that without DC removal, all the correlation coefﬁcients are strongly positive. Re-\n",
      "moving the DC components reduces the correlations to some extent, and introduces\n",
      "negative correlations.\n",
      "a)\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "b)\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "Fig. 5.3: Scatter plot of the grey-scale values of two neighbouring pixels. a) Original pixel values.\n",
      "The values have been scaled so that the mean is zero and the variance one. b) Pixel values after\n",
      "removal of DC component in a 32×32 patch.\n",
      "5.2.2 Learning one feature by maximization of variance\n",
      "5.2.2.1 Principal component as variance-maximizing feature\n",
      "The covariances found in natural images pixel values can be analyzed by PCA. In\n",
      "PCA, the point is to ﬁnd linear features that explain most of the variance of the data.\n",
      "It is natural to start the deﬁnition of PCA by looking at the deﬁnition of the ﬁrst\n",
      "principal component. We consider the variance of the output:\n",
      "var(s) = E{s2} −(E{s})2 = E{s2}\n",
      "(5.3)\n",
      "where the latter equality is true because s has zero mean.\n",
      "Principal components are features s that contain (or “explain”) as much of the\n",
      "variance of the input data as possible. It turns out that the amount of variance ex-\n",
      "plained is directly related to the variance of the feature, as will be discussed in\n",
      "Section 5.3.1 below. Thus, the ﬁrst principal component is deﬁned as the feature, or\n",
      "linear combination of the pixel values, which has the maximum variance. Finding a\n",
      "feature with maximum variance can also be considered interesting in its own right.\n",
      "The idea is to ﬁnd the “main axis” of the data cloud, which is illustrated in Fig. 5.5.\n",
      "Some constraint on the weights W, which we call the principal component\n",
      "weights, must be imposed as well. If no constraint were imposed, the maximum\n",
      "of the variance would be attained when W becomes inﬁnitely large (and the mini-\n",
      "\n",
      "5.2 Principal component analysis\n",
      "101\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "−15\n",
      "−10\n",
      "−5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "d)\n",
      "−15\n",
      "−10\n",
      "−5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Fig. 5.4: The correlation coefﬁcients of a pixel (in the middle) with all other pixels. a) For original\n",
      "pixels. Black is small positive and and white is one. b) After removing DC component. The scale\n",
      "is different from a): black is now negative and white is plus one. c) A cross-section of a) in 1D to\n",
      "show the actual values. d) A cross-section of b) in 1D.\n",
      "Fig. 5.5: Illustration of PCA. The principal component of this (artiﬁcial) two-dimensional data is\n",
      "the oblique axis plotted. Projection on the principal axis explains more of the variance of the data\n",
      "than projection on any other axis.\n",
      "\n",
      "102\n",
      "5 Principal components and whitening\n",
      "mum would be attained when all the W(x,y) are zero). In fact, just multiplying all\n",
      "the weights in W by a factor of two, we would get a variance that is four times\n",
      "as large, and by dividing all the coefﬁcients by two the variance decreases to one\n",
      "quarter.\n",
      "A natural thing to do is to constrain the norm of W:\n",
      "∥W∥=\n",
      "r\n",
      "∑\n",
      "x,y\n",
      "W(x,y)2\n",
      "(5.4)\n",
      "For simplicity, we constrain the norm to be equal to one, but any other value would\n",
      "give the same results.\n",
      "5.2.2.2 Learning one feature from natural images\n",
      "What is then the feature detector that maximizes the variance of the output, given\n",
      "natural image input, and under the constraint that the norm of the detector weights\n",
      "equals one? We can ﬁnd the solution by taking a random sample of image patches.\n",
      "Let us denote by T the total number of patches used, and by It each patch, where t\n",
      "is an index that goes from 1 to T. Then, the expectation of s2 can be approximated\n",
      "by the average over this sample. Thus, we maximize\n",
      "1\n",
      "T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      " \n",
      "∑\n",
      "x,y\n",
      "W(x,y)It(x,y)\n",
      "!2\n",
      "(5.5)\n",
      "with respect to the weights in W(x,y), while constraining the values of W(x,y) so\n",
      "that the norm in Equation (5.4) is equal to one. The computation of the solution is\n",
      "discussed in Section 5.2.4.\n",
      "Typical solutions for natural images are shown in Fig. 5.6. The feature detector\n",
      "is an object of the same size and shape as an image patch, so it can be plotted as an\n",
      "image patch itself. To test whether the principal component weights are stable, we\n",
      "computed it ten times for different image samples. It can be seen that the component\n",
      "is quite stable.\n",
      "Fig. 5.6: The feature detectors giving the ﬁrst principal component of image windows of size\n",
      "32 × 32, computed for ten different randomly sampled datasets taken from natural images. The\n",
      "feature detector is grey-scale-coded so that the grey-scale value of a pixel gives the value of the\n",
      "coefﬁcient at that pixel. Grey pixels mean zero coefﬁcients, light-grey or white pixels are positive,\n",
      "and dark-grey or black are negative.\n",
      "\n",
      "5.2 Principal component analysis\n",
      "103\n",
      "5.2.3 Learning many features by PCA\n",
      "5.2.3.1 Deﬁning many principal components\n",
      "One of the central problems with PCA is that it basically gives only one well-deﬁned\n",
      "feature. It cannot be extended to the learning of many features in a very meaningful\n",
      "way. However, if we want to model visual processing by PCA, it would be absurd\n",
      "to compute just a single feature which would then be supposed to analyze the whole\n",
      "visual scene.\n",
      "Deﬁnition\n",
      "Typically, the way to obtain many principal components is by a “de-\n",
      "ﬂation” approach: After estimating the ﬁrst principal component, we want to ﬁnd\n",
      "the feature of maximum variance under the constraint that the new feature must be\n",
      "orthogonal to the ﬁrst one (i.e. the dot-product is zero, as in Equation 5.8). This\n",
      "will then be called the second principal component. This procedure can be repeated\n",
      "to obtain as many components as there are dimensions in the data space. To put\n",
      "this formally, assume that we have estimated k principal components, given by the\n",
      "weight vectors W1,W2 ...,Wk. Then the k +1-th principal component weight vector\n",
      "is deﬁned by\n",
      "max\n",
      "W var\n",
      " \n",
      "∑\n",
      "x,y\n",
      "I(x,y)W(x,y)\n",
      "!\n",
      "(5.6)\n",
      "under the constraints\n",
      "∥W∥=\n",
      "r\n",
      "∑\n",
      "x,y\n",
      "W(x,y)2 = 1\n",
      "(5.7)\n",
      "∑\n",
      "x,y\n",
      "Wj(x,y)W(x,y) = 0 for all j = 1,...,k\n",
      "(5.8)\n",
      "An interesting property is that any two principal components are uncorrelated, and\n",
      "not only orthogonal. In fact, we could change the constraint in the deﬁnition to\n",
      "uncorrelatedness, and the principal components would be the same.\n",
      "Critique of the deﬁnition\n",
      "This classic deﬁnition of many principal components\n",
      "is rather unsatisfactory, however. There is no really good justiﬁcation for thinking\n",
      "that the second principal component corresponds to something interesting: it is not\n",
      "a feature that maximizes any property in itself. It only maximizes the variance not\n",
      "explained by the ﬁrst principal component.\n",
      "Moreover, the solution is not quite well deﬁned, since for natural images, there\n",
      "are many principal components that give practically the same variance. After the ﬁrst\n",
      "few principal components, the differences of the variances of different directions get\n",
      "smaller and smaller. This is a serious problem for the following reason. If two prin-\n",
      "cipal components, say si and sj, have the same variance, then any linear combination\n",
      "q1si + q2sj has the same variance as well,2 and the weights q1Wi(x,y) + q2Wj(x,y)\n",
      "2 This is due to the fact that the principal components are uncorrelated, see Section 5.8.1.\n",
      "\n",
      "104\n",
      "5 Principal components and whitening\n",
      "fulﬁll the constraints of unit norm and orthogonality,if we normalize the coefﬁcients\n",
      "q1 and q2 so that q2\n",
      "1 + q2\n",
      "2 = 1. So, not only we cannot say what is the order of the\n",
      "components, but actually there is an inﬁnite number of different components from\n",
      "which we cannot choose the “right” one.\n",
      "In practice, the variances of the principal components are not exactly equal due\n",
      "to random ﬂuctuations, but this theoretical result means that the principal compo-\n",
      "nents are highly dependent on those random ﬂuctuations. In the particular sample\n",
      "of natural images that we are using, the maximum variance (orthogonal to previous\n",
      "components) can be obtained by any of these linear combinations. Thus, we cannot\n",
      "really say what the 100th principal component is, for example, because the result\n",
      "we get from computing it depends so much on random sampling effects. This will\n",
      "be demonstrated in the experiments that follow.\n",
      "5.2.3.2 All principal components of natural images\n",
      "The ﬁrst 320 principal components of natural images patches are shown in Fig-\n",
      "ure 5.7, while Figure 5.9 shows the variances of the principal components. For lack\n",
      "of space, we don’t show all the components, but it is obvious from the ﬁgure what\n",
      "they look like. As can be seen, the ﬁrst couple of features seem quite meaningful:\n",
      "they are oriented, something like very low-frequency edge detectors. However, most\n",
      "of the features given by PCA do not seem to be very interesting. In fact, after the\n",
      "ﬁrst, say, 50 features, the rest seem to be just garbage. They are localized in fre-\n",
      "quency as they clearly are very high-frequency features. However, they do not seem\n",
      "to have any meaningful spatial structure. For example, they are not oriented.\n",
      "In fact, most of the features do not seem to be really well deﬁned for the reason\n",
      "explained in the previous section: the variances are too similar for different features.\n",
      "For example, some of the possible 100th principal components, for different random\n",
      "sets of natural image patches, are shown in Figure 5.8. The random changes in the\n",
      "component are obvious.\n",
      "5.2.4 Computational implementation of PCA\n",
      "In practice, numerical solution of the optimization problem which deﬁnes the prin-\n",
      "cipal components is rather simple and based on what is called the eigenvalue de-\n",
      "composition. We will not go into the mathematical details here; they can be found\n",
      "in Section 5.8.1. Brieﬂy, the computation is based on the following principles\n",
      "1. The variance of any linear feature as in Equation (5.2) can be computed if we\n",
      "just know the variances and covariances of the image pixels.\n",
      "2. We can collect the variances and covariances of image pixels in a single matrix,\n",
      "called the covariance matrix, as explained in Section 4.6.3. Each entry in the ma-\n",
      "trix then gives the covariance between two pixels—variance is simply covariance\n",
      "of a pixel with itself.\n",
      "\n",
      "5.2 Principal component analysis\n",
      "105\n",
      "Fig. 5.7: The 320 ﬁrst principal components weights Wi of image patches of size 32×32. The order\n",
      "of decreasing variances is left to right on each row, and top to bottom.\n",
      "Fig. 5.8: Ten different estimations of the 100th principal component of image windows of size\n",
      "32×32. The random image sample was different in each run.\n",
      "\n",
      "106\n",
      "5 Principal components and whitening\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "Fig. 5.9: The logarithms of the variances of the principal components for natural image patches,\n",
      "the ﬁrst of which were shown in Fig. 5.7.\n",
      "3. Any sufﬁciently sophisticated software for scientiﬁc computation is able to com-\n",
      "pute the eigenvalue decomposition of that matrix. (However, the amount of com-\n",
      "putation needed grows fast with the size of the image patch, so the patch size\n",
      "cannot be too large.)\n",
      "4. As a result of the eigenvalue decomposition we get two things. First, the eigen-\n",
      "vectors, which give the Wi which are the principal component weights. Second,\n",
      "the eigenvalues, which give the variances of the principal components si. So, we\n",
      "only need to order the eigenvectors in the order of descending eigenvalues, and\n",
      "we have computed the whole PCA decomposition.\n",
      "5.2.5 The implications of translation-invariance\n",
      "Many of the properties of the PCA of natural images are due a particular property of\n",
      "the covariance matrix of natural images. Namely, the covariance for natural images\n",
      "is translation-invariant, i.e. it depends only on the distance\n",
      "cov(I(x,y),I(x′,y′)) = f((x−x′)2 +(y−y′)2)\n",
      "(5.9)\n",
      "for some function f. After all, the covariance of two neighbouring pixels is not\n",
      "likely to be any different depending on whether they are on the left or the right side\n",
      "of the image. (This form of translation-invariance should not be confused with the\n",
      "invariances of complex cells, discussed in Section 3.4.2 and Chapter 10).\n",
      "\n",
      "5.3 PCA as a preprocessing tool\n",
      "107\n",
      "The principal component weights Wi(x,y) for a covariance matrix of this form\n",
      "can be shown to always have a very particular form: they are sinusoids:\n",
      "Wi(x,y) = sin(ax+by+c)\n",
      "(5.10)\n",
      "for some constants a,b,c (the scaling is arbitrary so you could also multiply Wi with\n",
      "a constant d). See Section 5.8.2 below for a detailed mathematical analysis which\n",
      "proves this.\n",
      "The constants a and b determine the frequency of the oscillation. For example,\n",
      "the ﬁrst principal components have lower frequencies than the later ones. They also\n",
      "determine the orientation of the oscillation. It can be seen in Fig. 5.7 that some of\n",
      "the oscillations are oblique while others are vertical and horizontal. Some have os-\n",
      "cillations in one orientation only, while others form a kind of checkerboard pattern.\n",
      "The constant c determines the phase.\n",
      "The variances associated with the principal components thus tell how strongly the\n",
      "frequency of the sinusoid is present in the data, which is closely related to computing\n",
      "the power spectrum of the data.\n",
      "Because of random effects in the sampling of image patches and computation\n",
      "of the covariance matrix, the estimated feature weights are not exactly sinusoids.\n",
      "Of course, just the ﬁnite resolution of the images makes them different from real\n",
      "sinusoidal functions.\n",
      "Since the principal component weights are sinusoids, they actually perform some\n",
      "kind of Fourier analysis. If you apply the obtained Wi as feature detectors on an\n",
      "image patch, that will be related to a discrete Fourier transform of the image patch.\n",
      "In particular, this can be interpreted as computing the coefﬁcients of the patch in the\n",
      "basis given by sinusoidal functions, as discussed in Section 2.3.2.\n",
      "An alternative viewpoint is that you could also consider the computation of the\n",
      "principal components as doing a Fourier analysis of the covariance matrix of the\n",
      "image patch; this interesting connection will be considered in Section 5.6.\n",
      "5.3 PCA as a preprocessing tool\n",
      "So far, we have presented PCA as a method for learning features, which is the classic\n",
      "approach to PCA. However, we saw that the results were rather disappointing in the\n",
      "sense that the features were not interesting as neurophysiological models, and they\n",
      "were not even well-deﬁned.\n",
      "However, PCA is not a useless model. It accomplishes several useful preprocess-\n",
      "ing tasks, which will be discussed in this section.\n",
      "\n",
      "108\n",
      "5 Principal components and whitening\n",
      "5.3.1 Dimension reduction by PCA\n",
      "One task where PCA is very useful is in reducing the dimension of the data so that\n",
      "the maximum amount of the variance is preserved.\n",
      "Consider the following general problem that also occurs in many other areas\n",
      "than image processing. We have a very large number, say m, of random variables\n",
      "x1,...,xm. Computations that use all the variables would be too burdensome. We\n",
      "want to reduce the dimension of the data by linearly transforming the variables into\n",
      "a smaller number, say n, of variables that we denote by z1,...,zn:\n",
      "zi =\n",
      "m\n",
      "∑\n",
      "j=1\n",
      "wijxj, for all i = 1,...,n\n",
      "(5.11)\n",
      "The number of new variables n might be only 10% or 1% of the original number\n",
      "m. We want to ﬁnd the new variables so that they preserve as much information on\n",
      "the original data as possible. This “preservation of information” has to be exactly\n",
      "deﬁned. The most wide-spread deﬁnition is to look at the squared error that we get\n",
      "when we try to reconstruct the original data using the zi. That is, we reconstruct xj\n",
      "as a linear transformation ∑i a jizi, minimizing the average error\n",
      "E\n",
      "\n",
      "\n",
      "∑\n",
      "j\n",
      " \n",
      "xj −∑\n",
      "i\n",
      "a jizi\n",
      "!2\n",
      "\n",
      "= E\n",
      "(\n",
      "∥x −∑\n",
      "i\n",
      "aizi∥2\n",
      ")\n",
      "(5.12)\n",
      "where the a ji are also determined so that they minimize this error. For simplicity, let\n",
      "us consider only transformations for which the transforming weights are orthogonal\n",
      "and have unit norm:\n",
      "∑\n",
      "j\n",
      "w2\n",
      "ij = 1, for all i\n",
      "(5.13)\n",
      "∑\n",
      "j\n",
      "wijwk j = 0, for all i ̸= k\n",
      "(5.14)\n",
      "What is the best way of doing this dimension reduction? The solution is to take\n",
      "as the zi the n ﬁrst principal components! (A basic version of this result is shown in\n",
      "the exercises.) Furthermore, the optimal reconstruction weight vectors ai in Equa-\n",
      "tion (5.12) are given by the very same principal components weights which compute\n",
      "the zi.\n",
      "The solution is not uniquely deﬁned, though, because any orthogonal transforma-\n",
      "tion of the zi is just as good. This is understandable because any such transformation\n",
      "of the zi contains just the same information: we can make the inverse transformation\n",
      "to get the zi from the transformed ones.\n",
      "As discussed above, the features given by PCA suffer from the problem of not\n",
      "being uniquely deﬁned. This problem is much less serious in the case of dimension\n",
      "reduction. What matters in the dimension reduction context is not so much the actual\n",
      "components themselves, but the subspace which they span. The principal subspace\n",
      "\n",
      "5.3 PCA as a preprocessing tool\n",
      "109\n",
      "means the set of all possible linear combinations of the n ﬁrst principal components.\n",
      "It corresponds to taking all possible linear combinations of the principal component\n",
      "weight vectors Wi associated with the n principal components. As pointed out above,\n",
      "if two principal components si and sj have the same variance, any linear combina-\n",
      "tion q1si + q2sj has the same variance for q2\n",
      "1 + q2\n",
      "2 = 1. This is not a problem here,\n",
      "however, since such a linear combination still belongs to the same subspace as the\n",
      "two principal components si and sj. Thus, it does not matter if we consider the com-\n",
      "ponents si and sj, or two components of the form q1si +q2sj and r1si +r2sj where\n",
      "the coefﬁcients r1 and r2 give a different linear combination than the q1 and q2.\n",
      "So, the n-dimensional principal subspace is usually uniquely deﬁned even if some\n",
      "principal components have equal variances. Of course, it may happen that the n-th\n",
      "and the (n + 1)-th principal components have equal variances, and that we cannot\n",
      "decide which one to include in the subspace. But the effect on the whole subspace\n",
      "is usually quite small and can be ignored in practice.\n",
      "Returning to the case of image data, we can rephrase this result by saying that\n",
      "it is the set of features deﬁned by the n ﬁrst principal components and their linear\n",
      "combinations that is (relatively) well deﬁned, and not the features themselves.\n",
      "5.3.2 Whitening by PCA\n",
      "5.3.2.1 Whitening as normalized decorrelation\n",
      "Another task for which PCA is quite useful is whitening. Whitening is an impor-\n",
      "tant preprocessing method where the image pixels are transformed to a set of new\n",
      "variables s1,...,sn so that the si are uncorrelated and have unit variance:\n",
      "E{sisj} =\n",
      "(\n",
      "0 if i ̸= j\n",
      "1 if i = j\n",
      "(5.15)\n",
      "(It is assumed that all the variables have zero mean.) It is also said that the resulting\n",
      "vector (s1,...,sn) is white.\n",
      "In addition to the principal components weights being orthogonal, the principal\n",
      "components themselves are uncorrelated, as will be shown in more detail in Sec-\n",
      "tion 5.8.1. So, after PCA, the only thing we need to do to get whitened data is to\n",
      "normalize the variances of the principal components by dividing them by their stan-\n",
      "dard deviations. Denoting the principal components by yi, this means we compute\n",
      "si =\n",
      "yi\n",
      "p\n",
      "var(yi)\n",
      "(5.16)\n",
      "to get whitened components si. Whitening is a useful preprocessing method that\n",
      "will be used later in this book. The intuitive idea is that it completely removes the\n",
      "second-order information of the data. “Second-order” means here correlations and\n",
      "\n",
      "110\n",
      "5 Principal components and whitening\n",
      "variances. So, it allows us to concentrate on properties that are not dependent on\n",
      "covariances, such as sparseness in the next chapter.\n",
      "Whitening by PCA is illustrated in Figure 5.10.\n",
      "a)\n",
      "−1.5\n",
      "0\n",
      "1.5\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "u2\n",
      "u1\n",
      "b)\n",
      "−1.5\n",
      "0\n",
      "1.5\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "c)\n",
      "−1.5\n",
      "0\n",
      "1.5\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "Fig. 5.10: Illustration of PCA and whitening. a) The original data “cloud”. The arrows show the\n",
      "principal components. The ﬁrst one points in the direction of the largest variance in the data, and\n",
      "the second in the remaining orthogonal direction. b) When the data is transformed to the principal\n",
      "components, i.e. the principal components are taken as the new coordinates, the variation in the\n",
      "data is aligned with those new axes, which is because the principal components are uncorrelated.\n",
      "c) When the principal components are further normalized to unit variance, the data cloud has\n",
      "equal variance in all directions, which means it has been whitened. The change in the lengths of\n",
      "the arrows reﬂects this normalization; the larger the variance, the shorter the arrow.\n",
      "5.3.2.2 Whitening transformations and orthogonality\n",
      "It must be noted that there are many whitening transformations. In fact, if the ran-\n",
      "dom variables si,i = 1...,n are white, then any orthogonal transformation of those\n",
      "variables is also white (the proof is left as an exercise). Often, whitening is based\n",
      "on PCA because PCA is a well-known method that can be computed very fast, but\n",
      "\n",
      "5.3 PCA as a preprocessing tool\n",
      "111\n",
      "it must be kept in mind that PCA is just one among the many whitening transforma-\n",
      "tions. Yet, PCA is a unique method because it allows us to combine three different\n",
      "preprocessing methods into one: dimension reduction, whitening, and anti-aliasing\n",
      "(which will be discussed in the next section).\n",
      "In later chapters, we will often use the fact that the connection between orthog-\n",
      "onality and uncorrelatedness is even stronger for whitened data. In fact, if we com-\n",
      "pute two linear components ∑i visi and ∑i wisi from white data, they are uncorrelated\n",
      "only if the two vectors v and w (which contain the entries vi and wi, respectively)\n",
      "are orthogonal.\n",
      "In general, we have the following theoretical result. For white data, multiplication\n",
      "by a square matrix gives white components if and only if the matrix is orthogonal.\n",
      "Thus, when we have computed one particular whitening transformation, we also\n",
      "know that only orthogonal transformations of the transformed data can be white.\n",
      "Note here the tricky point in terminology: a matrix if called orthogonal if its\n",
      "columns, or equivalently its rows, are orthogonal, and the norms of its columns\n",
      "are all equal to one. To emphasize this, some authors call an orthogonal matrix\n",
      "orthonormal. We stick to the word “orthogonal” in this book.\n",
      "5.3.3 Anti-aliasing by PCA\n",
      "PCA also helps combat the problem of aliasing, which refers to a class of problems\n",
      "due to the sampling of the data at a limited resolution — in our case the limited\n",
      "number of pixels used to represent an image. Sampling of the image loses infor-\n",
      "mation; this is obvious since we only know the image through its values at a ﬁnite\n",
      "number of pixels. However, sampling can also introduce less obvious distortions in\n",
      "the data. Here we consider two important ones, and show how PCA can help.\n",
      "5.3.3.1 Oblique gratings can have higher frequencies\n",
      "One problem is that in the case of the rectangular sampling grid, oblique higher\n",
      "frequencies are overrepresented in the data, because the grid is able to represent\n",
      "oblique oscillations which have a higher frequency than either the vertical or hori-\n",
      "zontal oscillations of the highest possible frequency.\n",
      "This is because we can have an image which takes the form of a checkerboard\n",
      "as illustrated in Figure 5.11 a). If you draw long oblique lines along the white and\n",
      "black squares, the distance between such lines is equal to\n",
      "p\n",
      "1/2 as can be calculated\n",
      "by basic trigonometry. This is smaller than one, which is the shortest half-cycle (half\n",
      "the length of an oscillation) we can have in the vertical and horizontal orientation.\n",
      "(It corresponds to the Nyquist frequency as discussed in the next subsection, and\n",
      "illustrated in Figure 5.11 b).\n",
      "In the Fourier transform, this lack of symmetry is seen in the fact that the area\n",
      "of possible 2-D frequencies is of the form of a square, instead of a circle as would\n",
      "\n",
      "112\n",
      "5 Principal components and whitening\n",
      "be natural for data which is the same in all orientations (“rotation-invariant”, as\n",
      "will be discussed in Section 5.7). Filtering out the highest oblique frequencies is\n",
      "thus a meaningful preprocessing step to avoid any artefacts due to this aliasing phe-\n",
      "nomenon. (Note that we are here talking about the rectangular form of the sampling\n",
      "grid, i.e. the relation of the pixels centerpoints to each other. This is not at all related\n",
      "to the shape of the sampling window, i.e. the shape of the patch.)\n",
      "It turns out that we can simply ﬁlter out the oblique frequencies by PCA. With\n",
      "natural images, the last principal components are those that correspond to the high-\n",
      "est oblique frequencies. Thus, simple dimension reduction by PCA alleviates this\n",
      "problem.\n",
      "a)\n",
      "b)\n",
      "Fig. 5.11: Effects of sampling (limited number of pixels) on very high-frequency gratings. a) A\n",
      "sinusoidal grating which has a very high frequency in the oblique orientation. The cycle of the\n",
      "oscillation has a length of 2\n",
      "p\n",
      "1/2 =\n",
      "√\n",
      "2 which is shorter than the smallest possible cycle length\n",
      "(equal to two) in the vertical and horizontal orientations. b) A sinusoidal grating which has the\n",
      "Nyquist frequency. Although it is supposed to be sinusoidal, due to the limited sampling (i.e.,\n",
      "limited number of pixels), it is really a block grating.\n",
      "5.3.3.2 Highest frequencies cannot can have only two different phases\n",
      "Another problem is that at the highest frequencies, we cannot have sinusoidal grat-\n",
      "ings with different phases. Let us consider the highest possible frequency, called in\n",
      "Fourier theory the Nyquist frequency, which means that there is one cycle for every\n",
      "two pixels, see Figure 5.11 b). What happens when you change the phase of the\n",
      "grating a little bit, i.e. shift the “sinusoidal” grating a bit? Actually, almost noth-\n",
      "ing happens: the grating does not shift at all because due to the limited resolution\n",
      "given by the pixel size, it is impossible to represent a small shift in the grating. (The\n",
      "grey-scale values will be changed; they depend on the match between the sampling\n",
      "lattice and the underlying sinusoidal they try to represent.) The sampled image re-\n",
      "ally changes only when the phase is changed so much that the best approximation\n",
      "is to ﬂip all the pixels from white to black and vice versa.\n",
      "\n",
      "5.4 Canonical preprocessing used in this book\n",
      "113\n",
      "Thus, a grating sampled at the Nyquist frequency can really have only two differ-\n",
      "ent phases which can be distinguished. This means that many concepts depending\n",
      "on the phase of the grating, such as the phase tuning curve (Section 6.4) or phase-\n",
      "invariance (Chapter 10) are rather meaningless on the Nyquist frequency. So, we\n",
      "would like to low-pass ﬁlter the image to be able to analyze such phenomena with-\n",
      "out the distorting effect of a limited resolution.\n",
      "Again, we can alleviate this problem by PCA dimension reduction, because it\n",
      "amounts to discarding the highest frequencies.\n",
      "5.3.3.3 Dimension selection to avoid aliasing\n",
      "So, we would like to do PCA so that we get rid of the checkerboard patterns as well\n",
      "as everything in the Nyquist frequency. On the other hand, we don’t want to get rid\n",
      "of any lower frequencies.\n",
      "To investigate the dimension reduction needed, we computed what amount of\n",
      "checkerboard and Nyquist gratings is present in the data as a function of dimension\n",
      "after PCA. We also computed this for gratings that had half the Nyquist frequency\n",
      "(i.e. a cycle was four pixels), which is a reasonable candidate for the highest fre-\n",
      "quency patterns that we want to retain.\n",
      "The results are shown in Figure 5.12. We can see in the ﬁgure that to get rid\n",
      "of checkerboard patterns, not much dimension reduction is necessary: 10% or so\n",
      "seems to be enough.3 To get rid of the Nyquist frequency, at least 30% seems to be\n",
      "necessary. And if we look at how much we can reduce the dimension without losing\n",
      "any information on the highest frequencies that we are really interested in, it seems\n",
      "we can easily reduce even 60%-70% of the dimensions.\n",
      "Thus, the exact number of dimensions is not easy to determine because we don’t\n",
      "have a very clear criterion. Nevertheless, a reduction of at least 30% seems to be\n",
      "necessary to avoid the artifacts, and even 60%-70% can be recommended. In the\n",
      "experiments in this book, we usually reduce dimension by 75%.\n",
      "5.4 Canonical preprocessing used in this book\n",
      "Now, we have arrived at a preprocessing method that we call “canonical preprocess-\n",
      "ing” because it is used almost everywhere in this book. Canonical preprocessing\n",
      "means:\n",
      "1. Remove the DC component as in Eq. (5.1).\n",
      "2. Compute the principal components of the image patches.\n",
      "3 Note that this may be an underestimate: van Hateren proposed that 30% may be needed (van\n",
      "Hateren and van der Schaaf, 1998). This is not important in the following because we will anyway\n",
      "reduce at least 30% for other reasons that will be explained next.\n",
      "\n",
      "114\n",
      "5 Principal components and whitening\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Fig. 5.12: The percentage of different frequencies present in the data as a function of PCA di-\n",
      "mension reduction. Horizontal axis: Percentage of dimensions retained by PCA. Vertical axis:\n",
      "Percentage of energy of a given grating retained. Solid lines: Gratings of half Nyquist frequency\n",
      "(vertical and oblique) (wanted). Dotted line (see lower right-hand corner): checkerboard pattern\n",
      "(unwanted). Dashed lines: Gratings of Nyquist frequency (vertical and oblique) (unwanted).\n",
      "3. Retain only the n ﬁrst principal components and discard the rest. The number n\n",
      "is typically chosen as 25% of the original dimension.\n",
      "4. Divide the principal components by their standard deviations as in Equation (5.16)\n",
      "to get whitened data.\n",
      "Here we see two important (and interrelated) reasons for doing whitening by\n",
      "PCA instead of some other whitening method. We can reduce dimension to com-\n",
      "bat aliasing and to reduce computational load with hardly any extra computational\n",
      "effort.\n",
      "Notation\n",
      "The end-product of this preprocessing is an n-dimensional vector for\n",
      "each image patch. The preprocessed vector will be denoted by z, and its elements by\n",
      "z1,...,zn, when considered as a random vector and random variables. Observations\n",
      "of the random vector will be denoted by z1,z2,..., or more often with the subscript\n",
      "t as in zt. In the rest of this book, we will often use such canonically preprocessed\n",
      "data. Likewise, observed image patches will be denoted by I, and their individual\n",
      "pixels by I(x,y), when these are considered a random vector and random variables,\n",
      "respectively, and their observations will be denoted by It and It(x,y).\n",
      "\n",
      "5.5 Gaussianity as the basis for PCA\n",
      "115\n",
      "5.5 Gaussianity as the basis for PCA\n",
      "5.5.1 The probability model related to PCA\n",
      "In PCA and whitening, it is assumed that the only interesting aspect of the data\n",
      "variables x1,...,xn is variances and covariances. This is the case with gaussian data,\n",
      "where the probability density function equals\n",
      "p(x1,...,xn) =\n",
      "1\n",
      "(2π)n/2|detC|−1/2 exp(−1\n",
      "2 ∑\n",
      "i,j\n",
      "xixj[C−1]ij)\n",
      "(5.17)\n",
      "where C is the covariance matrix of the data, C−1 is its inverse, and [C−1]ij is\n",
      "the i, j-th element of the inverse. Thus, the probability distribution is completely\n",
      "characterized by the covariances (as always, the means are assumed zero here).\n",
      "These covariance-based methods are thus perfectly sufﬁcient if the distribution\n",
      "of the data is gaussian. However, the distribution of image data is typically very far\n",
      "from gaussian. Methods based on the gaussian distribution thus neglect some of the\n",
      "most important aspects of image data, as will be seen in the next chapter.\n",
      "Using the gaussian distribution, we can also interpret PCA as a statistical model.\n",
      "After all, one of the motivations behind estimation of statistical models for natural\n",
      "images was that we would like to use them in Bayesian inference. For that, it is\n",
      "not enough to just have a set of features. We also need to understand how we can\n",
      "compute the prior probability density function p(x1,...,xn) for any given image\n",
      "patch.\n",
      "The solution is actually quite trivial: we just plug in the covariance of the data in\n",
      "Eq. (5.17). There is actually no need to go through the trouble of computing PCA in\n",
      "order to get a probabilistic model! The assumption of gaussianity is what gives us\n",
      "this simple solution.\n",
      "In later chapters, we will see the importance of the assumption of gaussianity, as\n",
      "we will consider models which do not make this assumption.\n",
      "5.5.2 PCA as a generative model\n",
      "A more challenging question is how to interpret PCA as a generative model, i.e. a\n",
      "model which describes a process which “generated” the data. There is a large litera-\n",
      "ture on such modelling, which is typically called factor analysis. PCA is considered\n",
      "a special case, perhaps the simplest one, of factor analytic models. The point is that\n",
      "we can express data as a linear transformation of the principal components\n",
      "I(x,y) = ∑\n",
      "i\n",
      "Wi(x,y)si\n",
      "(5.18)\n",
      "\n",
      "116\n",
      "5 Principal components and whitening\n",
      "What we have done here is simply to invert the transformation from the data to the\n",
      "principal components, so that the data is a function of the principal components\n",
      "and not vice versa. This is very simple because the vectors Wi are orthogonal: then\n",
      "the inverse of the system (matrix) they form is just the same matrix transposed, as\n",
      "discussed in Section 19.7. Therefore, the feature vectors in this generative model\n",
      "are just the same as the feature detector weights that we computed with PCA.\n",
      "Now, we deﬁne the distribution of the si as follows:\n",
      "1. the distribution of each si is gaussian with variance equal to the variance of the\n",
      "i-th principal component\n",
      "2. the si are statistically independent from each other.\n",
      "This gives us, using Equation (5.18), a proper generative model of the data. That is,\n",
      "the data can be seen as a function of the “hidden” variables that are now given by\n",
      "the principal components si.\n",
      "5.5.3 Image synthesis results\n",
      "Once we have a generative model, we can do an interesting experiment to test our\n",
      "model: We can generate image patches from our model, and see what they look\n",
      "like. Such results are shown in Figure 5.13. What we see is that the PCA model\n",
      "captures the general smoothness of the images. The smoothness comes from the\n",
      "fact that the ﬁrst principal components correspond to feature vectors which change\n",
      "very smoothly. Other structure is not easy to see in these results. The results can be\n",
      "compared with real natural images patches shown in Figure 5.2 on page 99; they\n",
      "clearly have a more sophisticated structure, visible even in these small patches.\n",
      "5.6 Power spectrum of natural images\n",
      "An alternative way of analyzing the covariance structure of images is through\n",
      "Fourier analysis. The covariances and the frequency-based properties are related\n",
      "via the Wiener-Khinchin theorem. We begin by considering the power spectra of\n",
      "natural images and then show the connection.\n",
      "5.6.1 The 1/ f Fourier amplitude or 1/ f 2 power spectrum\n",
      "The fundamental result on frequency-based representation of natural images is that\n",
      "the power spectrum of natural images typically falls off inversely proportional to\n",
      "the square of the frequency. Since the power spectrum is the square of the Fourier\n",
      "amplitude (spectrum), this means that the Fourier amplitude falls off as a function\n",
      "\n",
      "5.6 Power spectrum of natural images\n",
      "117\n",
      "Fig. 5.13: Image synthesis using PCA. 20 patches were randomly generated using the PCA model\n",
      "whose parameters were estimated from natural images. Compare with real natural image patches\n",
      "in Figure 5.2.\n",
      "c/ f where c is some constant and f is the frequency. It is usually more convenient\n",
      "to plot the logarithms. For the logarithm this means\n",
      "Log Fourier amplitude = −log f +const.\n",
      "(5.19)\n",
      "or\n",
      "Log power spectrum = −2log f +const.\n",
      "(5.20)\n",
      "for some constant which is the logarithm of the constant c.\n",
      "Figure 5.15 a) shows the logarithm of the power spectrum of the natural im-\n",
      "age in Fig. 5.14 a). What we can see in this 2D plot is just that the spectrum is\n",
      "smaller for higher frequencies. To actually see how it falls off, we have to look at\n",
      "one-dimensional cross-sections of the power spectrum, so that we average over all\n",
      "orientations. This is how we get Fig. 5.15 b), in which we have also taken the log-\n",
      "arithm of the frequency as in Equation (5.20). This plot partly veriﬁes our result: it\n",
      "is largely linear with a slope close to minus two, as expected. (Actually, more thor-\n",
      "ough investigations have found that the log-power spectrum may, in fact, change a\n",
      "bit slower than 1/ f 2, with a exponent closer to 1.8 or 1.9. ) In addition, the power\n",
      "spectra are very similar for the two images in Fig. 5.15.\n",
      "A large literature in physics and other ﬁelds has considered the signiﬁcance of\n",
      "such a behaviour of the power spectrum. Many other kinds of data have the same\n",
      "kind of spectra. An important reason for this is that if the data is scale-invariant,\n",
      "or self-similar, i.e. it is similar whether you zoom in or out, the power spectrum is\n",
      "\n",
      "118\n",
      "5 Principal components and whitening\n",
      "necessarily something like proportional to 1/ f 2; see References section below for\n",
      "some relevant work.\n",
      "a)\n",
      "b)\n",
      "Fig. 5.14: Two natural images used in the experiments.\n",
      "a)\n",
      "−100\n",
      "−50\n",
      "0\n",
      "50\n",
      "100\n",
      "−100\n",
      "−50\n",
      "0\n",
      "50\n",
      "100\n",
      "b)\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "2.5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "Fig. 5.15: Power spectrum or Fourier amplitude of natural images. a) The logarithm of two-\n",
      "dimensional power spectrum of natural image in Fig. 5.14 a). b) The average over orientations\n",
      "of one-dimensional cross-sections of the power spectrum of the two images in Fig. 5.14. Only the\n",
      "positive part is shown since this is symmetric with respect to the origin. This is a log-log plot where\n",
      "a logarithm of base 10 has been taken of both the frequency (horizontal axis) and the power (ver-\n",
      "tical axis) in order to better show the 1/ f 2 behaviour, which corresponds to a linear dependency\n",
      "with slope of −2.\n",
      "\n",
      "5.6 Power spectrum of natural images\n",
      "119\n",
      "5.6.2 Connection between power spectrum and covariances\n",
      "What is then the connection between the power spectrum of an image, and the co-\n",
      "variances between pixels we have been computing in this chapter? To this end, we\n",
      "need a theorem from the theory of stochastic processes (we will not rigorously de-\n",
      "ﬁne what stochastic processes are because that is not necessary for the purposes of\n",
      "this book). The celebrated Wiener-Khinchin theorem states that for a stochastic pro-\n",
      "cess, the average power spectrum is the Fourier Transform of the autocorrelation\n",
      "function.\n",
      "The theorem talks about the “autocorrelation function”. This is the terminology\n",
      "of stochastic processes, which we have not used in this chapter: we simply consid-\n",
      "ered different pixels as different random variables. The “autocorrelation function”\n",
      "means simply the correlations of variables (i.e. pixel values) as a function of the hor-\n",
      "izontal and vertical distances between them. Thus, the autocorrelation function is a\n",
      "matrix constructed as follows. First, take one row of the covariance matrix, say the\n",
      "one corresponding to the pixel at (x0,y0). To avoid border effects, let’s take (x0,y0)\n",
      "which is in the middle of the image patch. Then, convert this vector back to the\n",
      "shape of the image patch. Thus, we have a matrix C(x0,y0)\n",
      "\n",
      "\n",
      "\n",
      "cov(I(x0,y0),I(1,1)) ... cov(I(x0,y0),I(1,n))\n",
      "...\n",
      "cov(I(x0,y0),I(n,1)) ... cov(I(x0,y0),I(n,n))\n",
      "\n",
      "\n",
      "\n",
      "(5.21)\n",
      "which has the same size m×m as the image patch. This matrix is nothing else than\n",
      "what was already estimated from natural images and plotted in Fig. 5.4.\n",
      "Actually, it is obvious that this matrix essentially contains all the information in\n",
      "the covariance matrix. As discussed in Section 5.2.5, it is commonly assumed that\n",
      "image patches are translation-invariant in the sense that the covariances actually\n",
      "only depend on the distance between two pixels, and not on where in the patch the\n",
      "pixels happen to be. (This may not hold for whole images, where the upper half\n",
      "may depict sky more often and lower parts, but it certainly holds for small image\n",
      "patches.) Thus, to analyze the covariance structure of images, all we really need is\n",
      "a matrix like in Equation (5.21).\n",
      "What the Wiener-Khinchin theorem now says is that when we take the Fourier\n",
      "transformation of C(x0,y0), just as if this matrix were an image patch, the Fourier\n",
      "amplitudes equal the average power spectrum of the original image patches. (Due to\n",
      "the special symmetry properties of covariances, the phases in the Fourier transform\n",
      "of C(x0,y0) are all zero, so the amplitudes are also equal to the coefﬁcients of the\n",
      "cos functions.)\n",
      "Thus, we can see the connection between the 1/ f Fourier amplitude of natural\n",
      "images and the covariances of the pixels structure. The average 1/ f Fourier ampli-\n",
      "tude or the 1/ f 2 power spectrum of single images implies that the Fourier transform\n",
      "of C(x0,y0) also falls of as 1/ f 2. Now, since the features obtained from PCA are\n",
      "not very different from those used in a discrete Fourier transform (sine and cosine\n",
      "\n",
      "120\n",
      "5 Principal components and whitening\n",
      "functions), and the squares of the coefﬁcients in that basis are the variances of the\n",
      "principal components, we see that the variances of the principal components fall\n",
      "off as 1/ f 2 as a function of frequency. (This cannot be seen in the variance plot in\n",
      "Fig. 5.9 because that plot does not give the variances as a function of frequency.)\n",
      "Another implication of the Wiener-Khinchin theorem is that it shows how consid-\n",
      "ering the power spectrum of images alone is related to using gaussian model. Since\n",
      "the average power spectrum contains essentially the same information as the covari-\n",
      "ance matrix, and using covariances only is equivalent to using a gaussian model, we\n",
      "see that considering the average power spectrum alone is essentially equivalent to\n",
      "modelling the data with a gaussian pdf as in Section 5.5. Since the power spectrum\n",
      "does not contain information about phase, using the phase structure is thus related\n",
      "to using the non-gaussian aspects of the data, which will be considered in the next\n",
      "chapters.\n",
      "5.6.3 Relative importance of amplitude and phase\n",
      "When considering frequency-based representations of natural images, the following\n",
      "question naturally arises: Which is more important, phase or amplitude (power) —\n",
      "or are they equally important? Most researchers agree that the phase information is\n",
      "more important for the perceptual system than the amplitude structure. This view is\n",
      "justiﬁed by experiments in which we take the phase structure from one image and\n",
      "the power structure from another, and determine whether the image is more similar\n",
      "to one of the natural images. What this means is that we take the Fourier transform\n",
      "(using the Discrete Fourier Transform) of the two images, and isolate the phase and\n",
      "amplitude from the two transforms. Then we compute the inverse of the Fourier\n",
      "transform from the combination of the phase from the ﬁrst image and the amplitude\n",
      "from the second; this gives us a new image. We also create another image with the\n",
      "inverse Fourier transform using the phase from the second image and the amplitude\n",
      "from the ﬁrst.\n",
      "Results of such an experiment are shown in Fig. 5.16. In both cases, the image\n",
      "“looks” more like the image from which the phase structure was taken, although\n",
      "in a) this is not very strongly so. This may be natural if one looks at the Fourier\n",
      "amplitudes of the images: since they are both rather similar (showing the typical\n",
      "1/ f fall-off), they cannot provide much information about what the image really\n",
      "depicts. If all natural images really have amplitude spectra which approximately\n",
      "show the 1/ f shape, the power spectrum cannot provide much information on any\n",
      "natural image, and thus the phase information has to be the key to identifying the\n",
      "contents in the images.\n",
      "Thus, one can conclude that since PCA concentrates only on information in the\n",
      "power spectrum, and the power spectrum does not contain a lot of perceptually im-\n",
      "portant information, one cannot expect PCA and related methods to yield too much\n",
      "useful information about the visual system. Indeed, this provides an explanation for\n",
      "the rather disappointing performance of PCA in learning features from natural im-\n",
      "\n",
      "5.7 Anisotropy in natural images\n",
      "121\n",
      "ages as seen in Fig. 5.7 — the performace is disappointing, at least, if we want to\n",
      "model receptive ﬁelds in V1. In the next chapter we will see that using information\n",
      "not contained in the covariances gives an interesting model of simple cell receptive\n",
      "ﬁelds.\n",
      "a)\n",
      "b)\n",
      "Fig. 5.16: Relative importance of phase and power/amplitude information in natural images. a)\n",
      "Image which has the Fourier phases of the image in Fig. 5.14 a), and the Fourier amplitudes of the\n",
      "image in Fig. 5.14 b). b) Image which has the phases of the image in Fig. 5.14 b), and the amplitude\n",
      "structure of the image in Fig. 5.14 a). In both cases the images are perceptually more similar to the\n",
      "image from which the phase structure was taken, which indicates that the visual system is more\n",
      "sensitive to the phase structure of natural images.\n",
      "5.7 Anisotropy in natural images\n",
      "The concept of anisotropy refers to the fact that natural images are not completely\n",
      "rotationally invariant (which would be called isotropy). In other words, the statistical\n",
      "structure is not the same in all orientations: if you rotate an image, the statistics\n",
      "change.\n",
      "This may come as a surprise after looking at the correlation coefﬁcients in Fig-\n",
      "ure 5.4 d), in which it seems that the correlation is simply a function of the distance:\n",
      "the closer to each other the two pixels are, the stronger their correlation; and the\n",
      "orientation does not seem to have any effect. In fact, isotropy is not a bad ﬁrst ap-\n",
      "proximation, but a closer analysis reveals some dependencies on orientation.\n",
      "Figure 5.17 show the results of such an analysis. We have taken the correlation\n",
      "coefﬁcients computed in Figure 5.4, and analyzed how they depend on the orienta-\n",
      "tion of the line segment connecting the two pixels. An orientation of 0 (or π) means\n",
      "that the two pixels have the same y coordinate; orientation of π/2 means that they\n",
      "have the same x coordinate. Other values mean that the pixels have an oblique rela-\n",
      "tionship to each other. Figure 5.17 shows that the correlations are the very strongest\n",
      "\n",
      "122\n",
      "5 Principal components and whitening\n",
      "if the pixels have the same y coordinate, that is, they are on the same horizontal line.\n",
      "The correlations are also elevated if the pixels have the same x coordinate.\n",
      "In fact, we already saw in Figure 5.6 that the ﬁrst principal component is, con-\n",
      "sistently, a low-frequency horizontal edge. This is in line with the dominance of\n",
      "horizontal correlations. If the images exactly isotropic, horizontal edges and verti-\n",
      "cal edges would have exactly the same variance, and the ﬁrst principal component\n",
      "would not be well-deﬁned at all; this would be reﬂected in Figure 5.6 so that we\n",
      "would get edges with different random orientations.\n",
      "Thus, we have discovered a form of anisotropy in natural image statistics. It will\n",
      "be seen in different forms in all the later models and analyses as well.\n",
      "0\n",
      "1.57\n",
      "3.14\n",
      "−0.2\n",
      "−0.1\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "Fig. 5.17: Anisotropy, i.e. lack of rotational invariance, in natural image statistics. We took the\n",
      "correlation coefﬁcients in Figure 5.4 and plotted them on three circles with different radii (the\n",
      "maximum radius allowed by the patch size, and that multiplied by one half and one quarter). For\n",
      "each of the radii, the plot shows that the correlations are maximized for the orientations of 0 or\n",
      "π, which mean horizontal orientation: the pixels are on the same horizontal line. The vertical\n",
      "orientation π/2 shows another maximum which is less pronounced.\n",
      "5.8 Mathematics of principal component analysis*\n",
      "This section is dedicated to a more sophisticated mathematical analysis of PCA and\n",
      "whitening. It can be skipped by a reader not interested in mathematical details.\n",
      "\n",
      "5.8 Mathematics of principal component analysis*\n",
      "123\n",
      "5.8.1 Eigenvalue decomposition of the covariance matrix\n",
      "The “second-order” structure of data is completely described by the covariance ma-\n",
      "trix, deﬁned in Section 4.6.3. In our case with the x and y coordinates of the patches,\n",
      "we can write:\n",
      "C(x,y;x′,y′) = E{I(x,y)I(x′,y′)}\n",
      "(5.22)\n",
      "The point is that the covariance of any two linear features can be computed by\n",
      "E\n",
      "(\n",
      "[∑\n",
      "x,y\n",
      "W1(x,y)I(x,y)][∑\n",
      "x,y\n",
      "W2(x,y)I(x,y))]\n",
      ")\n",
      "= E\n",
      "(\n",
      "[ ∑\n",
      "xyx′y′\n",
      "W1(x,y)I(x,y)W2(x′,y′)I(x′,y′))]\n",
      ")\n",
      "= ∑\n",
      "xyx′y′\n",
      "W1(x,y)W2(x′,y′)E{I(x,y)I(x′,y′)}\n",
      "= ∑\n",
      "xyx′y′\n",
      "W1(x,y)W2(x′,y′)C(x,y;x′,y′)\n",
      "(5.23)\n",
      "which reduces to a something which can be computed using the covariance ma-\n",
      "trix. The second-order structure is thus conveniently represented by a single matrix,\n",
      "which enables us to use classic methods of linear algebra to analyze the second-\n",
      "order structure.\n",
      "To go into details, we change the notation so that the whole image is in one\n",
      "vector, x, so that each pixel is one element in the vector. This can be accomplished,\n",
      "for example, by scanning the image row by row, as was explained in Section 4.1.\n",
      "This simpliﬁes the notation enormously.\n",
      "Now, considering any linear combination wTx = ∑i wixi we can compute its vari-\n",
      "ance simply by:\n",
      "E{(wTx)2} = E{(wTx)(xTw)} = E{wT(xxT)w} = wTE{xxT}w\n",
      "= wTCw\n",
      "(5.24)\n",
      "where we denote the covariance matrix by C = E{xxT}. So, the basic PCA problem\n",
      "can be formulated as\n",
      "max\n",
      "w:∥w∥=1wTCw\n",
      "(5.25)\n",
      "A basic concept in linear algebra is the eigenvalue decomposition. The starting\n",
      "point is that C is a symmetric matrix, because cov(xi,xj) = cov(xj,xi). In linear\n",
      "algebra, it is shown that any symmetric matrix can be expressed as a product of the\n",
      "form:\n",
      "C = UDUT\n",
      "(5.26)\n",
      "\n",
      "124\n",
      "5 Principal components and whitening\n",
      "where U is an orthogonal matrix, and D = diag(λ1,...,λm) is diagonal. The columns\n",
      "of U are called the eigenvectors, and the λi are called the eigenvalues. Many efﬁcient\n",
      "algorithms exist for computing the eigenvalue decomposition of a matrix.\n",
      "Now, we can solve PCA easily. Lets us make the change of variables v = UTw.\n",
      "Then we have\n",
      "wTCw = wTUDUTw = vTDv = ∑\n",
      "i\n",
      "v2\n",
      "i λi\n",
      "(5.27)\n",
      "Because U is orthogonal, ∥v∥= ∥w∥, so the constraint is the same for v as it was\n",
      "for w. Let us make the further change of variables to mi = v2\n",
      "i . The constraint of unit\n",
      "norm of v is now equivalent to the constraints that the sum of the mi must equal\n",
      "one (they must also be positive because they are squares). Then, the problem is\n",
      "transformed to\n",
      "max\n",
      "mi≥0,∑mi=1∑\n",
      "i\n",
      "miλi\n",
      "(5.28)\n",
      "It is rather obvious that the maximum is found when the mi corresponding to the\n",
      "largest λi is one and the others are zero. Let us denote by i∗the index of the max-\n",
      "imum eigenvalue. Going back to the w, this corresponds to w begin equal to the\n",
      "i∗-th eigenvector, that is, the i∗-th column of U. Thus, we see how the ﬁrst principal\n",
      "component is easily computed by the eigenvalue decomposition.\n",
      "Since the eigenvectors of a symmetric matrix are orthogonal, ﬁnding the second\n",
      "principal component means maximizing the variance so that vi∗is kept zero. This is\n",
      "actually equivalent to making the new w orthogonal to the ﬁrst eigenvector. Thus,\n",
      "in terms of mi, we have exactly the same optimization problem, but with the extra\n",
      "constraint that mi∗= 0. Obviously, the optimum is obtained when w is equal to the\n",
      "eigenvector corresponding to the second largest eigenvalue. This logic applies to the\n",
      "k-th principal component.\n",
      "Thus, all the principal components can be found by ordering the eigenvectors\n",
      "ui,i = 1,...,m in U so that the corresponding eigenvalues are in decreasing order.\n",
      "Let us assume that U is ordered so. Then the i-th principal component si is equal to\n",
      "si = uT\n",
      "i x\n",
      "(5.29)\n",
      "Note that it can be proven that the λi are all non-negative for a covariance matrix.\n",
      "Using the eigenvalue decomposition, we can prove some interesting properties\n",
      "of PCA. First, the principal components are uncorrelated, because for the vector of\n",
      "the principal components\n",
      "s = UTx\n",
      "(5.30)\n",
      "we have\n",
      "E{ssT} = E{UTxxTU} = UTE{xxT}U = UT(UDUT)U\n",
      "= (UTU)D(UT U) = D\n",
      "(5.31)\n",
      "because of the orthogonality of U. Thus, the covariance matrix is diagonal, which\n",
      "shows that the principal components are uncorrelated.\n",
      "\n",
      "5.8 Mathematics of principal component analysis*\n",
      "125\n",
      "Moreover, we see that the variances of the principal components are equal to the\n",
      "λi. Thus, to obtain variables that are white, that is, uncorrelated and have unit vari-\n",
      "ance, it is enough to divide each principal component by the square root of the cor-\n",
      "responding eigenvalue. This proves that diag(1/√λ1,...,1/√λm)UT is a whitening\n",
      "matrix for x.\n",
      "This relation also has an important implication for the uniqueness of PCA. If\n",
      "two of the eigenvalues are equal, then the variance of those principal components\n",
      "are equal. Then, the principal components are not well-deﬁned anymore, because we\n",
      "can make a rotation of those principal components without affecting their variances.\n",
      "This is because if zi and zi+1 have the same variance, then linear combinations such\n",
      "as\n",
      "p\n",
      "1/2zi +\n",
      "p\n",
      "1/2zi+1 and\n",
      "p\n",
      "1/2zi −\n",
      "p\n",
      "1/2zi+1 have the same variance as well;\n",
      "all the constraints (unit variance and orthogonality) are still fulﬁlled, so these are\n",
      "equally valid principal components. In fact, in linear algebra, it is well-known that\n",
      "the eigenvalue decomposition is uniquely deﬁned only when the eigenvalues are all\n",
      "distinct.\n",
      "5.8.2 Eigenvectors and translation-invariance\n",
      "Using the eigenvalue decomposition, we can show why the principal components\n",
      "of a typical image covariance matrix are sinusoids as stated in Section 5.2.5. This is\n",
      "because of their property of being translation-invariant, i.e. the covariance depends\n",
      "only on the distance as in (5.9). For simplicity, let us consider a one-dimensional\n",
      "covariance matrix c(x−x′). The function c is even-symmetric with respect to zero,\n",
      "i.e. c(−u) = c(u). By a simple change of variable z = x−x′, we have\n",
      "∑\n",
      "x\n",
      "cov(x,x′)sin(x+α) = ∑\n",
      "x\n",
      "c(x−x′)sin(x+α) = ∑\n",
      "z\n",
      "c(z)sin(z+x′ +α)\n",
      "(5.32)\n",
      "Using the property that sin(a +b) = sinacosb +cosasinb, we have\n",
      "∑\n",
      "z\n",
      "c(z)sin(z+x′ +α) = ∑\n",
      "z\n",
      "c(z)(sin(z)cos(x′ +α)+cos(z)sin(x′ +α))\n",
      "= [∑\n",
      "z\n",
      "c(z)sin(z)]cos(x′ +α)+[∑\n",
      "z\n",
      "c(z)cos(z)]sin(x′ +α)\n",
      "(5.33)\n",
      "Finally, because c(z) is even-symmetric and sin is odd-symmetric, the ﬁrst sum in\n",
      "brackets is zero. So, we have\n",
      "∑\n",
      "x\n",
      "cov(x,x′)sin(x+α) = [∑\n",
      "z\n",
      "c(z)cos(z)]sin(x′ +α)\n",
      "(5.34)\n",
      "which shows that the sinusoid is an eigenvectorof the covariance matrix, with eigen-\n",
      "value ∑z c(z)cos(z). The parameter α gives the phase of the sinusoid; this formula\n",
      "shows that α can have any value, so sinusoids of any phase are eigenvectors.\n",
      "\n",
      "126\n",
      "5 Principal components and whitening\n",
      "This proof can be extended to sinusoids of different frequencies β: they all are\n",
      "eigenvalues with eigenvalues that depend on how strongly the frequency is present\n",
      "in the data: ∑z c(z)cos(βz).\n",
      "In the two-dimensional case, we have cov(I(x,y),I(x′,y′)) = c((x −x′)2 + (y −\n",
      "y′)2) and with ξ = x−x′ and η = y−y′ we have\n",
      "∑\n",
      "x,y\n",
      "c((x−x′)2 +(y−y′)2)sin(ax+by+c)\n",
      "= ∑\n",
      "ξ,η\n",
      "c(ξ,η)sin(aξ +bη +ax′ +by′ +c)\n",
      "= ∑\n",
      "ξ,η\n",
      "c(ξ,η)[sin(aξ +bη)cos(ax′ +by′ +c)+cos(aξ +bη)sin(ax′ +by′ +c)]\n",
      "= 0 +[∑\n",
      "ξ,η\n",
      "c(ξ,η)cos(aξ +bη)]sin(ax′ +by′ +c)\n",
      "(5.35)\n",
      "which shows, likewise, that sinusoids of the form sin(ax′ +by′+c) are eigenvectors.\n",
      "5.9 Decorrelation models of retina and LGN *\n",
      "In this section, we consider some further methods for whitening and decorrelation\n",
      "of natural images, and the application of such methods as models of processing in\n",
      "the retina and the LGN. This material can be skipped without interrupting the ﬂow\n",
      "of ideas.\n",
      "5.9.1 Whitening and redundancy reduction\n",
      "The starting point here is the redundancy reduction hypothesis, discussed in Chap-\n",
      "ter 1. In its original form, this theory states that the early visual system tries to\n",
      "reduce the redundancy in its input. As we have seen in in this chapter, image pixel\n",
      "data is highly correlated, so a ﬁrst approach to reduce the redundancy would be to\n",
      "decorrelate image data, i.e. to transform it into uncorrelated components.\n",
      "One way to decorrelate image data is to whiten it with a spatial ﬁlter. In a visual\n",
      "system, this ﬁltering would correspond to a set of neurons, with identical spatial\n",
      "receptive ﬁelds, spaced suitably in a lattice. The outputs of the neurons would then\n",
      "be uncorrelated, and if we were to look at the outputs of the whole set of neurons as\n",
      "an image (or a set of images for multiple input images), these output images would\n",
      "on the average have a ﬂat power spectrum.\n",
      "The whitening theory has led to well-known if rather controversial models of the\n",
      "computational underpinnings of the retina and the lateral geniculate nucleus (LGN).\n",
      "In this section we will discuss spatial whitening and spatial receptive ﬁelds accord-\n",
      "\n",
      "5.9 Decorrelation models of retina and LGN *\n",
      "127\n",
      "ing to this line of thought; the case of temporal whitening and temporal receptive\n",
      "ﬁelds will be discussed in detail in Section 16.3.2 (page 345).\n",
      "The basic idea is that whitening alone could explain the center-surround structure\n",
      "of the receptive ﬁelds of ganglion cells in the retina, as well as those in the LGN.\n",
      "Indeed, certain spatial whitening ﬁlters are very similar to ganglion RF’s, as we\n",
      "will see below. However, such a proposal is problematic because there are many\n",
      "completely different ways of whitening the image input, and it is not clear why this\n",
      "particular method should be used. Nevertheless, this theory is interesting because\n",
      "of its simplicity and because it sheds light on certain fundamental properties of the\n",
      "covariance structure of images.\n",
      "There are at least two ways to derive the whitening operation in question. The\n",
      "ﬁrst is to compute it directly from the covariance matrix of image patches sampled\n",
      "from the data; this will lead to a set of receptive ﬁelds, but with a suitable constraint\n",
      "the RF’s will be identical except for different center locations, as we will see below.\n",
      "We will call this patch-based whitening. The second way is to specify a whitening\n",
      "ﬁlter in the frequency domain, which will give us additional insight and control over\n",
      "the process. This we will call ﬁlter-based whitening.\n",
      "5.9.2 Patch-based decorrelation\n",
      "Our ﬁrst approach to spatial whitening is based on the PCA whitening introduced\n",
      "above in Section 5.3.2 (page 109). The data transformation is illustrated in the two-\n",
      "dimensional case in Figure 5.18.\n",
      "Here, we will use the matrix notation because it directly shows some important\n",
      "properties of the representation we construct. Here, we denote by U the matrix with\n",
      "the vectors deﬁning the principal components as its columns\n",
      "U = (u1,u2,··· ,uk).\n",
      "(5.36)\n",
      "Let x denote the data vector. Because the vectors u are orthogonal, each of the\n",
      "principal components yk, k = 1,...,K, of the data vector x can be computed simply\n",
      "by taking the dot product between the data vector and the kth PCA vector:\n",
      "yk = uT\n",
      "k x,\n",
      "k = 1,...,K.\n",
      "(5.37)\n",
      "Deﬁning the vector y = (y1,y2,··· ,yK)T , the Equations (5.37) for all k = 1,...,K\n",
      "can be expressed in a single matrix equation\n",
      "y = UTx.\n",
      "(5.38)\n",
      "In our two-dimensional illustration, the result of this transformation is shown in\n",
      "Figure 5.18b).\n",
      "As in Section 5.3.2, we next whiten the data by dividing the principal components\n",
      "with their standard deviations. Thus we obtain whitened components sk\n",
      "\n",
      "128\n",
      "5 Principal components and whitening\n",
      "a)\n",
      "−1.5\n",
      "0\n",
      "1.5\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "u2\n",
      "u1\n",
      "(1, 0)\n",
      "(0, 1)\n",
      "b)\n",
      "−1.5\n",
      "0\n",
      "1.5\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "c)\n",
      "−1.5\n",
      "0\n",
      "1.5\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "d)\n",
      "−1.5\n",
      "0\n",
      "1.5\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "w2\n",
      "w1\n",
      "Fig. 5.18: An illustration of the whitening procedure that is used to derive a set of whitening\n",
      "ﬁlters wk, k = 1,...,K (here, we take K = 2). The procedure utilizes the PCA basis vectors uk,\n",
      "k = 1,...,K. a) The original generated data points and the PCA basis vectors u1 and u2 (grey)\n",
      "and the unit vectors (1,0) and (0,1) (black). b) The data points are ﬁrst rotated so that the new\n",
      "axes match the PCA basis vectors. c) The data points are then scaled along the axes so that the\n",
      "data have the same variance along both axes. This also makes the two dimensions of the data\n",
      "uncorrelated, so the end result is a whitened data set. (For purposes of visualization of the data\n",
      "points and the vectors, in this illustration this variance is smaller than 1, while in whitening it is 1;\n",
      "this difference corresponds to an overall scaling of the data.) d) Finally, the data points are rotated\n",
      "back to the original orientation. Note that the data are already white after the second transformation\n",
      "in c), and the last transformation is one of inﬁnitely many possible rotations that keep the data\n",
      "white; in this method, it is the one that inverts the rotation done by PCA. Mathematically, the\n",
      "three transformations in b)-d) can in fact be combined into a single linear transformation because\n",
      "each transformation is linear; the combined operation can be done by computing the dot products\n",
      "between the original data points and the vectors w1 and w2 which are the result of applying the\n",
      "three transformations to the original unit vectors. See text for details.\n",
      "\n",
      "5.9 Decorrelation models of retina and LGN *\n",
      "129\n",
      "sk =\n",
      "yk\n",
      "p\n",
      "var(yk)\n",
      ",\n",
      "k = 1,...,K,\n",
      "(5.39)\n",
      "This is shown for our illustrative example in Figure 5.18c).\n",
      "Again, we can express these K equations by a single matrix equation. Deﬁne a\n",
      "vector s = (s1,s2,··· ,sK)T, and let Λ denote a diagonal matrix with the inverses of\n",
      "the square roots of the variances on its diagonal:\n",
      "Λ =\n",
      "\n",
      "\n",
      "1\n",
      "√\n",
      "var(y1)\n",
      "0\n",
      "···\n",
      "0\n",
      "0\n",
      "1\n",
      "√\n",
      "var(y2) ···\n",
      "0\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "0\n",
      "0\n",
      "···\n",
      "1\n",
      "√\n",
      "var(yK)\n",
      "\n",
      "\n",
      ";\n",
      "(5.40)\n",
      "then\n",
      "s = Λy = ΛUTx.\n",
      "(5.41)\n",
      "So far we have just expressed PCA whitening as a matrix formulation. Now,\n",
      "we will make a new operation. Among the inﬁnitely many whitening matrices, we\n",
      "choose the one which is given by inverting the PCA computation given by U. In\n",
      "a sense, we go “back from the principal components to the original coordinates”\n",
      "(Figure 5.18d). Denoting by z the ﬁnal component computed, this is deﬁned by the\n",
      "following matrix equation:\n",
      "z = Us = UΛUTx.\n",
      "(5.42)\n",
      "The computation presented in Equation (5.43) consist of three linear (matrix)\n",
      "transformations in a cascade. The theory of linear transformation states that a cas-\n",
      "cade of consecutive linear transformations is simply another linear transformation,\n",
      "and that this combined transformation – which we will here denote by W – can be\n",
      "obtained as the matrix product of the individual transformations:\n",
      "z = UΛUT\n",
      "| {z }\n",
      "=W\n",
      "x = Wx.\n",
      "(5.43)\n",
      "When written as k scalar equations, Equation (5.43) shows that the components of\n",
      "vector z = [z1 z2 ··· zK]T can be obtained as a dot product between the data vector\n",
      "x and the kth row of matrix W = [w1 w2 ···wK]T :\n",
      "zk = wT\n",
      "k x,\n",
      "k = 1,...,K.\n",
      "(5.44)\n",
      "The vectors wk, k = 1,...,K, are of great interest to us, since they are ﬁlters which\n",
      "map the input x to the whitened data. In other words, the vectors wk, k = 1,...,K,\n",
      "can be interpreted as receptive ﬁelds. The receptive ﬁelds wk, k = 1,...,K, can be\n",
      "obtained simply by computing the matrix product in Equation (5.43).\n",
      "Matrix square root\n",
      "As an aside, we mention an interesting mathematical inter-\n",
      "pretation of the matrix W. The matrix W is called the inverse matrix square root of\n",
      "\n",
      "130\n",
      "5 Principal components and whitening\n",
      "the covariance matrix C, and denoted by C−1/2. In other words, the inverse W−1\n",
      "is called the square root of C, and denoted by C1/2. The reason is that if we multi-\n",
      "ply W−1 with itself, we get C. This is because, ﬁrst, (UΛUT)−1 = UΛ−1UT, and\n",
      "second, we can calculate\n",
      "W−1W−1 = (UΛ−1UT)(UΛ−1UT) = UΛ−1(UTU)Λ−1UT = UΛ −2UT\n",
      "(5.45)\n",
      "The matrix Λ −2 is simply a diagonal matrix with the variances in its diagonal, so the\n",
      "result is nothing else than the eigenvalue decomposition of the covariance matrix as\n",
      "in Equation (5.26).\n",
      "Symmetric whitening matrix\n",
      "Another interesting mathematical property of the\n",
      "whitening matrix W in Equation (5.43) is that it is symmetric, which can be shown\n",
      "as\n",
      "WT =\n",
      "\u0000UΛUT\u0001T =\n",
      "\u0000UT\u0001T ΛTUT = UΛUT = W.\n",
      "(5.46)\n",
      "In fact, it is the only symmetric whitening matrix.\n",
      "Application to natural images When the previous procedure is applied to natural\n",
      "image data, interesting receptive ﬁelds emerge. Figure 5.19a) shows the resulting\n",
      "whitening ﬁlters (rows / columns of W); a closeup of one of the ﬁlters is shown\n",
      "in Figure 5.19b). As can be seen, the whitening principle results in the emergence\n",
      "of ﬁlters which have center-surround structure. All of the ﬁlters are identical, so\n",
      "processing image patches with such ﬁlters is analogous to ﬁltering them with the\n",
      "spatial ﬁlter shown in Figure 5.19b).\n",
      "As pointed out several times above, whitening can be done in inﬁnitely many\n",
      "different ways: if W is a whitening transformation, so is any orthogonal transfor-\n",
      "mation of W. Here the whitening solution in Equation (5.43) has been selected so\n",
      "that it results in center-surround-type ﬁlters. This is a general property that we will\n",
      "bump into time and again below: the whitening principle does constrain the form\n",
      "of the emerging ﬁlters, but additional assumptions are needed before the results can\n",
      "have a meaningful interpretation.\n",
      "Note that the theory results in a single receptive ﬁeld structure, while in the retina,\n",
      "there are receptive ﬁelds with differing spatial properties – in particular scale (fre-\n",
      "quency) – in the retina and the LGN. This is another limitation of the whitening\n",
      "principle, and additional assumptions are needed to produce a range of differing\n",
      "ﬁlters.\n",
      "5.9.3 Filter-based decorrelation\n",
      "Now we reformulate this theory in a ﬁlter-based framework. Then the theory pos-\n",
      "tulates that the amplitude response properties (see Section 2.2.3, page 33) of retinal\n",
      "and LGN receptive ﬁelds follow from the following two assumptions:\n",
      "\n",
      "5.9 Decorrelation models of retina and LGN *\n",
      "131\n",
      "a)\n",
      "b)\n",
      "Fig. 5.19: The application of the whitening principle results in the emergence of a set of center-\n",
      "surround ﬁlters from natural image data. a) The set of ﬁlters (rows / columns of whitening matrix\n",
      "W) obtained from image data. b) A closeup of one of the ﬁlters; the other ﬁlters are identical except\n",
      "for spatial location.\n",
      "\n",
      "132\n",
      "5 Principal components and whitening\n",
      "1. the linear ﬁlters are whitening natural image data\n",
      "2. with the constraint that noise is not ampliﬁed unduly.\n",
      "Additional assumptions are needed to derive the phase response in order to specify\n",
      "the ﬁlter completely. This is equivalent to the observation made above in the general\n",
      "case of whitening: there are inﬁnitely many whitening transformations. Here, the\n",
      "phases are deﬁned by specifying that the energy of the ﬁlter should be concentrated\n",
      "in either time or space, which in the spatial case can be loosely interpreted to mean\n",
      "that the spatial RF’s should be as localized as possible.\n",
      "The amplitude response of the ﬁlter will be derived in two parts: the ﬁrst part is\n",
      "the whitening ﬁlter, and the second part suppresses noise. The ﬁlter will be derived\n",
      "in the frequency domain, and thereafter converted to the spatial domain by inverse\n",
      "Fourier transform. It is often assumed that the statistics of image data do not depend\n",
      "on spatial orientation; we make the same assumption here and study the orientation-\n",
      "independent spatial frequency ωs. Conversion from the usual two-dimensional fre-\n",
      "quencies ωx and ωy to spatial frequency ωs is given by ωs =\n",
      "q\n",
      "ω2x +ω2y . Let Ri(ωs)\n",
      "denote the average power spectrum in natural images. We know that for uncorrelated\n",
      "/ whitened data, the average power spectrum should be a constant (ﬂat). Because the\n",
      "average power spectrum of the ﬁltered data is the product of the average power spec-\n",
      "trum of the original data and the squared amplitude response of the whitening ﬁlter,\n",
      "which we denote by |V(ωs)|2 , this means that the amplitude response of a whitening\n",
      "ﬁlter can be speciﬁed by\n",
      "|V(ωs)| =\n",
      "1\n",
      "p\n",
      "Ri(ωs)\n",
      ",\n",
      "(5.47)\n",
      "since then |V(ωs)|2 Ri(ωs) = 1.\n",
      "Real measurement data contains noise. Assume that the noise, whose average\n",
      "power spectrum is Rn(ωs), is additive and uncorrelated with the original image data,\n",
      "whose average power spectrum is Ro(ωs); then Ri(ωs) = Ro(ωs) + Rn(ωs). To de-\n",
      "rive the amplitude response of the ﬁlter that suppresses noise, one can use a Wiener\n",
      "ﬁltering approach. Wiener ﬁltering yields a linear ﬁlter that can be used to com-\n",
      "pensate for the presence of additive noise: the resulting ﬁlter optimally restores the\n",
      "original signal in the least mean square sense. The derivation of the Wiener ﬁlter in\n",
      "the frequency space is somewhat involved, and we will skip it here; see (Dong and\n",
      "Atick, 1995b). The resulting response properties of the ﬁlter are fairly intuitive: the\n",
      "amplitude response |F(ωs)| of the Wiener ﬁlter is given by\n",
      "|F(ωs)| = Ri(ωs)−Rn(ωs)\n",
      "Ri(ωs)\n",
      ".\n",
      "(5.48)\n",
      "Notice that if there are frequencies that contain no noise – that is, Rn(ωs) = 0 –\n",
      "the amplitude response is simply 1, and that higher noise power leads to decreased\n",
      "amplitude response.\n",
      "The overall amplitude response of the ﬁlter |W(ωs)| is obtained by cascading the\n",
      "whitening and the noise-suppressive ﬁlters (equations (5.47) and (5.48)). Because\n",
      "this cascading corresponds to multiplication in the frequency domain, the amplitude\n",
      "\n",
      "5.9 Decorrelation models of retina and LGN *\n",
      "133\n",
      "response of the resulting ﬁlter is\n",
      "|W(ωs)| = |V(ωs)||F(ωs)| =\n",
      "1\n",
      "p\n",
      "Ri(ωs)\n",
      "Ri(ωs)−Rn(ωs)\n",
      "Ri(ωs)\n",
      ".\n",
      "(5.49)\n",
      "In practice, Ri(ωs) can be estimated directly from image data for each ωs, or one can\n",
      "use the parametric form derived in Section 5.6 (page 116). (Negative values given\n",
      "by this formula (5.49) have to be truncated to zero.) Noise is assumed to be spatially\n",
      "uncorrelated, implying a constant (ﬂat) power spectrum, and to have power equal to\n",
      "data power at a certain characteristic frequency, denoted by ωs,c, so that\n",
      "Rn(ωs) = Ri(ωs,c)\n",
      "2\n",
      "for all ωs.\n",
      "(5.50)\n",
      "In order to fully specify the resulting ﬁlter, we have to deﬁne its phase response.\n",
      "Here we simply set the phase response to zero for all frequencies:\n",
      "∠W(ωs) = 0\n",
      "for all ωs.\n",
      "(5.51)\n",
      "With the phases of all frequencies at zero, the energy of the ﬁlter is highly concen-\n",
      "trated around the spatial origin, yielding a highly spatially localized ﬁlter. After the\n",
      "amplitude and the phase responses have been deﬁned, the spatial ﬁlter itself can be\n",
      "obtained by taking the inverse two-dimensional Fourier transform.\n",
      "The ﬁlter properties that result from the application of equations (5.49), (5.50)\n",
      "and (5.51) are illustrated in Figure 5.20 for characteristic frequency value ωs,c =\n",
      "0.3cycles per pixel. For this experiment, 100,000 image windows of size 16 × 16\n",
      "pixels were sampled from natural images.4 The average power spectrum of these\n",
      "images was then computed; the average of this spectrum over all spatial orientations\n",
      "is shown in Figure 5.20a). The squared amplitude response of the whitening ﬁlter,\n",
      "obtained from equation (5.49), is shown in Figure 5.20b). The power spectrum of the\n",
      "ﬁltered data is shown in Figure 5.20c); it is approximately ﬂat at lower frequencies\n",
      "and drops off sharply at high frequencies because of the higher relative noise power\n",
      "at high frequencies. The resulting ﬁlter is shown in Figure 5.20d); for comparison,\n",
      "a measured spatial receptive ﬁeld of an LGN neuron is shown in Figure 5.20e).\n",
      "Thus, the center-surround receptive-ﬁeld structure, found in the retina and the\n",
      "LGN, emerges from this computational model and natural image data. However, we\n",
      "made several assumptions above – such as the spacing of the receptive ﬁelds – and\n",
      "obtained as a result only a single ﬁlter instead of a range of ﬁlters in different scales\n",
      "and locations. In Section 16.3.2 (page 345) we will see that in the temporal domain,\n",
      "similar principles lead to the emergence of temporal RF properties of these neurons.\n",
      "4 Here, we did not use our ordinary data set but that of van Hateren and van der Schaaf (1998).\n",
      "\n",
      "134\n",
      "5 Principal components and whitening\n",
      "a)\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "1010\n",
      "Ri(ωs)\n",
      "10−2 10−1 100\n",
      "ωs (c/p)\n",
      "b)\n",
      "10−2\n",
      "10−1\n",
      "100\n",
      "101\n",
      "|W (ωs)|2\n",
      "10−2\n",
      "10−1\n",
      "100\n",
      "ωs (c/p)\n",
      "c)\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "1010\n",
      "|W (ωs)|2 Ri(ωs)\n",
      "10−2 10−1 100\n",
      "ωs (c/p)\n",
      "d)\n",
      "e)\n",
      "Fig. 5.20: The application of the whitening principle, combined with noise reduction and zero\n",
      "phase response, leads to the emergence of center-surround ﬁlters from natural image data. a) The\n",
      "power spectrum Ri(ωs) of natural image data. b) The squared amplitude response of a whitening\n",
      "ﬁlter which suppresses noise: this curve follows the inverse of the data power spectrum at low\n",
      "frequencies, but then drops off quickly at high frequencies, because the proportion of noise is larger\n",
      "at high frequencies. c) The power spectrum of the resulting (ﬁltered) data, showing approximately\n",
      "ﬂat (white) power at low frequencies, and dropping off sharply at high frequencies. d) The resulting\n",
      "ﬁlter which has been obtained from the amplitude response in b) and by specifying a zero phase\n",
      "response for all frequencies; see text for details. e) For comparison, the spatial receptive ﬁeld of\n",
      "an LGN neuron.\n",
      "5.10 Concluding remarks and References\n",
      "This chapter considered models of natural images which were based on analyzing\n",
      "the covariances of the image pixels. The classic model is principal component anal-\n",
      "ysis, in which variance of a linear feature detector is maximized. PCA fails to yield\n",
      "interesting feature detectors if the goal is to model visual cells in brain. However,\n",
      "it is an important model historically and conceptually, and also provides the basis\n",
      "for the preprocessing we use later in this book: dimension reduction combined with\n",
      "whitening. In the next chapter, we will consider a different kind of learning criterion\n",
      "which does yield features which are interesting for visual modelling.\n",
      "Most of the work on second-order statistics of images is based on the (approxi-\n",
      "mate) 1/ f 2 property of the power spectrum. This was investigated early in (Field,\n",
      "1987; Burton and Moorehead, 1987; Tolhurst et al, 1992; Ruderman and Bialek,\n",
      "\n",
      "5.11 Exercices\n",
      "135\n",
      "1994a; van der Schaaf and van Hateren, 1996). It has been proposed to explain\n",
      "certain scaling phenomena in the visual cortex, such as the orientation bandwidth\n",
      "(van der Schaaf and van Hateren, 1996) and the relative sensitivity of cells tuned to\n",
      "different frequencies (Field, 1987). Early work on PCA of images include (Sanger,\n",
      "1989; Hancock et al, 1992). The 1/ f property is closely related to the study of self-\n",
      "similar stochastic processes (Embrechts and Maejima, 2000) which has a very long\n",
      "history (Mandelbrot and van Ness, 1968). The study of self-critical systems (Bak\n",
      "et al, 1987) may also have some connection. A model of how such self-similarities\n",
      "come about as a result of composing an image of different “objects” is proposed in\n",
      "(Ruderman, 1997).\n",
      "A recent paper with a very useful discussion and review of the psychophysical\n",
      "importance of the Fourier powers vs. phases is (Wichmann et al, 2006); see also\n",
      "(Hansen and Hess, 2007).\n",
      "Another line of research proposes that whitening explains retinal ganglion recep-\n",
      "tive ﬁelds (Atick and Redlich, 1992). (An extension of this theory explains LGN\n",
      "receptive ﬁeld by considering temporal correlations as well (Dan et al, 1996a), see\n",
      "also Chapter 16.) For uniformity of presentation, we follow the mathematical the-\n",
      "ory of (Dong and Atick, 1995b) both here in the spatial case and in the temporal\n",
      "case in Section 16.3.2. As argued above, the proposal is problematic because there\n",
      "are many ways of whitening data. A possible solution to the problem is to consider\n",
      "energy consumption or wiring length, see Chapter 11 for this concept, as was done\n",
      "in (Vincent and Baddeley, 2003; Vincent et al, 2005).\n",
      "The anisotropy of pixel correlations has been used to explain some anisotropic\n",
      "properties in visual psychophysics in (Baddeley and Hancock, 1991).\n",
      "An attempt to characterize the proportion of information explained by the covari-\n",
      "ance structure in natural images can be found in (Chandler and Field, 2007).\n",
      "5.11 Exercices\n",
      "Mathematical exercises\n",
      "1. Show that if the expectations of the grey-scale values of the pixels are the same\n",
      "for all x,y:\n",
      "E{I(x,y)} = E{I(x′,y′)} for any x,y,x′,y′\n",
      "(5.52)\n",
      "then removing the DC component implies than the expectation of ˜I(x,y) is zero\n",
      "for any x,y.\n",
      "2. Show that if ∑x,yWx,y = 0, the removal of the DC component has no effect on the\n",
      "output of the features detector.\n",
      "3. Show that if the vector (y1,...,yn)T is white, any orthogonal transformation of\n",
      "that vector is white as well.\n",
      "4. To get used to matrix notation:\n",
      "\n",
      "136\n",
      "5 Principal components and whitening\n",
      "a. The covariance matrix of the vector x = (x1,...,xn)T is deﬁned as the matrix\n",
      "C with elements cij = cov(xi,xj). Under what condition do we have C =\n",
      "E{xxT}?\n",
      "b. Show that the covariance matrix of y = Mx equals MCMT\n",
      "5. Denote by w a vector which reduces the dimension of x to one as z = ∑i wixi.\n",
      "Now, we will show that taking the ﬁrst principal component is the optimal way of\n",
      "reducing dimension if the optimality criterion is least-squares error. This means\n",
      "that we reconstruct the original data as a linear transformation of z as:\n",
      "J(W) = E{∑\n",
      "j\n",
      "(xj −wjz)2}\n",
      "(5.53)\n",
      "a. Show that J is equal to\n",
      "∑\n",
      "j\n",
      "w2\n",
      "j ∑\n",
      "i,i′\n",
      "wiwi′cov(xi,xi′)−2∑\n",
      "j\n",
      "wj∑\n",
      "i\n",
      "wicov(xj,xi)+∑\n",
      "j\n",
      "var(xj)\n",
      "(5.54)\n",
      "b. Using this expression for J, show that the w which minimizes J under the\n",
      "constraint ∥w∥= 1 is the ﬁrst principal component of x.\n",
      "Computer assignments\n",
      "1. Take some images from the Web. Take a large sample of extremely small patches\n",
      "of the images, so that the patch contains just two neighbouring pixels. Convert\n",
      "the pixels to grey-scale if necessary. Make a scatter plot of the pixels. What can\n",
      "you see? Compute the correlation coefﬁcient of the pixel values.\n",
      "2. Using the same patches, convert them into two new variables: the sum of the\n",
      "grey-scale values and and their difference. Do the scatter plot and computer the\n",
      "correlation coefﬁcient.\n",
      "3. Using the same images, take a sample of 1,000 patches of the form of 1 × 10\n",
      "pixels. Compute the covariance matrix. Plot the covariance matrix (because the\n",
      "patches are one-dimensional, you can easily plot this two-dimensional matrix).\n",
      "4. The same as above, but remove the DC component of the patch. How does this\n",
      "change the covariance matrix?\n",
      "5. The same as above, but with only 50 patches sampled from the images. How are\n",
      "the results changed, and why?\n",
      "6. *Take the sample of 1,000 one-dimensional patches computed above. Compute\n",
      "the eigenvalue decomposition of the covariance matrix. Plot the principal com-\n",
      "ponent weights Wi(x).\n",
      "\n",
      "Chapter 6\n",
      "Sparse coding and simple cells\n",
      "In the preceding chapter, we saw how features can be learned by PCA of natural\n",
      "images. This is a classic method of utilizing the second-order information of sta-\n",
      "tistical data. However, the features it gave were not very interesting from a neural\n",
      "modelling viewpoint, which motivates us to ﬁnd better models. In fact, it is clear\n",
      "that the second-order structure of natural images is scratching the surface of the sta-\n",
      "tistical structure of natural images. Look at the outputs of the feature detectors of\n",
      "Figure 1.10, for example. We can see that the outputs of different kinds of ﬁlters\n",
      "differ from each other in other ways than just variance: the output of the Gabor ﬁlter\n",
      "has a histogram that has a strong peak at zero, whereas this is not the case for the\n",
      "histogram of pixel values. This difference is captured in a property called sparse-\n",
      "ness. It turns out that a more interesting model is indeed obtained if we look at the\n",
      "sparseness of the output s instead of the variance as in PCA.\n",
      "6.1 Deﬁnition of sparseness\n",
      "Sparseness means that the random variable is most of the time very close to zero,\n",
      "and only occasionally gets clearly nonzero values. One often says that the random\n",
      "variable is “active” only rarely.\n",
      "It is very important to distinguish sparseness from small variance. When we say\n",
      "“very close to zero”, this is relative to the general deviation of the random variable\n",
      "from zero, i.e. relative to its variance and standard deviation. Thus, “very close to\n",
      "zero” would mean something like “an absolute value that is smaller than 0.1 times\n",
      "the standard deviation”.\n",
      "To say that a random variable is sparse needs a baseline of comparison. Here\n",
      "it is the gaussian (normal) distribution; a random variable is sparse if it is active\n",
      "more rarely compared to a gaussian random variable of the same variance (and\n",
      "zero mean). Figure 6.1 shows a sample of a sparse random variable, compared\n",
      "to the gaussian random variable of the same variance. Another way of looking at\n",
      "sparseness is to consider the probability density function (pdf). The property of\n",
      "137\n",
      "\n",
      "138\n",
      "6 Sparse coding and simple cells\n",
      "being most of the time very close to zero is closely related to the property that the pdf\n",
      "has a peak at zero. Since the variable must have some deviation from zero (variance\n",
      "was normalized to unity), the peak at zero must be compensated by a relatively\n",
      "large probability mass at large values; a phenomenon often called “heavy tails”. In\n",
      "between these two extremes, the pdf takes relatively small values, compared to the\n",
      "gaussian pdf. This is illustrated in Fig. 6.2.1\n",
      "−5\n",
      "0\n",
      "5\n",
      "gaussian\n",
      "−5\n",
      "0\n",
      "5\n",
      "sparse\n",
      "Fig. 6.1: Illustration of sparseness. Random samples of a gaussian variable (top) and a sparse\n",
      "variable (bottom). The sparse variable is practically zero most of the time, occasionally taking\n",
      "very large values. Note that the variables have the same variance, and that these are not time series\n",
      "but just observations of random variables.\n",
      "6.2 Learning one feature by maximization of sparseness\n",
      "To begin with, we consider the problem of learning a single feature based on max-\n",
      "imization of sparseness. As explained in Section 1.8, learning features is a simple\n",
      "approach to building statistical models. Similar to the case of PCA, we consider one\n",
      "linear feature s computed using weights W(x,y) as\n",
      "s = ∑\n",
      "x,y\n",
      "W(x,y)I(x,y)\n",
      "(6.1)\n",
      "1 Here we consider the case of symmetric distributions only. It is possible to talk about the sparse-\n",
      "ness of non-symmetric distributions as well. For example, if the random variable only obtains\n",
      "non-negative values, the same idea of being very close to zero most of the time is still valid and\n",
      "is reﬂected in a peak on the right side of the origin. See Section 13.2.3 for more information.\n",
      "However, most distributions found in this book are symmetric.\n",
      "\n",
      "6.2 Learning one feature by maximization of sparseness\n",
      "139\n",
      "a)\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "a)\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "−5\n",
      "−4\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "Fig. 6.2: Illustration of a typical sparse probability density. The sparse density function, called\n",
      "Laplacian, is given by the solid curve (see Eq. (7.18) in the next chapter for an exact formula) For\n",
      "comparison, the density of the absolute value of a gaussian random variable of the same variance\n",
      "is given by the dash-dotted curve. a) the probability density functions, b) their logarithms.\n",
      "While a single feature is not very useful for vision, this approach shows the basic\n",
      "principles in a simpliﬁed setting. Another way in which we simplify the problem\n",
      "is by postponing the formulation of a proper statistical model. Thus, we do not\n",
      "really estimate the feature in this section, but rather learn it by some intuitively\n",
      "justiﬁed statistical criteria. In Section 6.3 we show how to learn many features, and\n",
      "in Chapter 7 we show how to formulate a proper statistical model and learn the\n",
      "features by estimating it.\n",
      "6.2.1 Measuring sparseness: General framework\n",
      "To be able to ﬁnd features that maximize sparseness, we have to develop statistical\n",
      "criteria for measuring sparseness. When measuring sparseness, we can ﬁrst normal-\n",
      "ize s to unit variance, which is simple to do by dividing s by its standard deviation.\n",
      "This simpliﬁes the formulation of the measures.\n",
      "A simple way to approach the problem is to look at the expectation of some\n",
      "function of s, a linear feature of the data. If the function is the square function, we\n",
      "are just measuring variance (which we just normalized to be equal to one), so we\n",
      "have to use something else. Since we know that the variance is equal to unity, we\n",
      "can consider the square function as a baseline and look at the expectations of the\n",
      "form\n",
      "E{h(s2)}\n",
      "(6.2)\n",
      "where h is some nonlinear function.\n",
      "How should the function h be chosen so that the formula in Equation (6.2) mea-\n",
      "sures sparseness? The starting point is the observation that sparse variables have a\n",
      "\n",
      "140\n",
      "6 Sparse coding and simple cells\n",
      "lot of data (probability mass) around zero because of the peak at zero, as well as a\n",
      "lot of data very far from zero because of heavy tails. Thus, we have two different\n",
      "approaches to measuring sparseness. We can choose h so that it emphasizes values\n",
      "that are close to zero, or values that are much larger than one. However, it may not\n",
      "be necessary to explicitly measure both of them, because the constraint of unit vari-\n",
      "ance means that if there is a peak at zero, there has to be something like heavy tails\n",
      "to make the variance equal to unity, and vice versa.\n",
      "6.2.2 Measuring sparseness using kurtosis\n",
      "A simple function that measures sparseness with emphasis on large values (heavy\n",
      "tails) is the quadratic function\n",
      "h1(u) = (u −1)2\n",
      "(6.3)\n",
      "(We denote by u the argument of h, to emphasize that it is not a function of s directly.\n",
      "Typically, u = s2.) Algebraic simpliﬁcations show that then the sparseness measure\n",
      "is equal to\n",
      "E{h1(s2)} = E{(s2 −1)2} = E{s4 −2s2 +1} = E{s4} −1\n",
      "(6.4)\n",
      "where the last equality holds because of the unit variance constraint. Thus, this mea-\n",
      "sure of sparseness is basically the same as the fourth moment; subtraction of the\n",
      "constant (one) is largely irrelevant since it just shifts the measurement scale.\n",
      "Using the fourth moment is closely related to the classic statistic called kurtosis\n",
      "kurt(s) = E{s4} −3(E{s2})2.\n",
      "(6.5)\n",
      "If the variance is normalized to 1, kurtosis is in fact the same as the fourth moment\n",
      "minus a constant (three). This constant is chosen so that kurtosis is zero for a gaus-\n",
      "sian random variable (this is left as an exercise). If kurtosis is positive, the variable\n",
      "is called leptokurtic (or super-gaussian); this is a simple operational deﬁnition of\n",
      "sparseness.\n",
      "However, kurtosis is not a very good measure of sparseness for our purposes.\n",
      "The basic problem with kurtosis is its sensitivity to outliers. An “outlier” is a data\n",
      "point that is very far from the mean, possibly due to an error in the data collection\n",
      "process. Consider, for example, a data set that has 1,000 scalar values and has been\n",
      "normalized to unit variance. Assume that one of the values is equal to 10. Then,\n",
      "kurtosis is necessarily equal to at least 104/1000 −3 = 7. A kurtosis of 7 is usually\n",
      "considered a sign of strong sparseness. But here it was due to a single value, and not\n",
      "representative of the whole data set at all!\n",
      "Thus, kurtosis is a very unreliable measure of sparseness. This is due to the fact\n",
      "that h1 puts much more weight on heavy tails than on values close to zero (it grows\n",
      "\n",
      "6.2 Learning one feature by maximization of sparseness\n",
      "141\n",
      "inﬁnitely when going far from zero). It is, therefore, useful to consider other mea-\n",
      "sures of sparseness, i.e. other nonlinear functions h.\n",
      "6.2.3 Measuring sparseness using convex functions of square\n",
      "Convexity and sparseness\n",
      "Many valid measures can be found by considering\n",
      "functions h that are convex. 2 Convexity means that a line segment that connects\n",
      "two points on the graph is always above the graph of the function, as illustrated in\n",
      "Figure 6.3. Algebraically, this can be expressed as follows:\n",
      "h(αx1 +(1 −α)x2) < αh(x1)+(1 −α)h(x2)\n",
      "(6.6)\n",
      "for any 0 < α < 1. It can be shown that this is true if the second derivative of h is\n",
      "positive for all x (except perhaps in single points).\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "Fig. 6.3: Illustration of convexity. The plotted function is y = −√x +x+ 1\n",
      "2, which from the view-\n",
      "point of measurement of sparseness is equivalent to just the negative square root, as explained in\n",
      "the text. The segment (dashed line) connecting two points on its graph is above the graph; actually,\n",
      "this is always the case.\n",
      "Why is convexity enough to yield a valid measure of sparseness? The reason is\n",
      "that the expectation of a convex function has a large value if the data is concentrated\n",
      "in the extremes, in this case near zero and very far from zero. Any points between the\n",
      "extremes decrease the expectation of the convex h due to the fundamental equation\n",
      "(6.6), where x1 and x2 correspond to the extremes, and αx1 +(1−α)x2 is a point in\n",
      "between.\n",
      "The function h1 in Equation (6.3) is one example of a convex function, but below\n",
      "we will propose better ones.\n",
      "2 The convexity we consider here is usually called “strict” convexity in mathematical literature.\n",
      "\n",
      "142\n",
      "6 Sparse coding and simple cells\n",
      "An example distribution\n",
      "To illustrate this phenomenon, consider a simple case\n",
      "where s takes only three values:\n",
      "P(s = −\n",
      "√\n",
      "5) = 0.1,P(s =\n",
      "√\n",
      "5) = 0.1,P(s = 0) = 0.8\n",
      "(6.7)\n",
      "This distribution has zero mean, unit variance, and is quite sparse. The square s2\n",
      "takes the values 0 and 5, which can be considered very large in the sense that it is\n",
      "rare for a random variable to take values that are\n",
      "√\n",
      "5 times the standard deviation,\n",
      "and 0 is, of course an extremely small absolute value. Now, let us move some of the\n",
      "probability mass from 0 to 1, and to preserve unit variance, make the largest value\n",
      "smaller. We deﬁne\n",
      "P(s = −2) = 0.1, P(s = 2) = 0.1,\n",
      "P(s = 0) = 0.6,\n",
      "(6.8)\n",
      "P(s = −1) = 0.1, P(s = 1) = 0.1\n",
      "(6.9)\n",
      "We can now compute the value of the measure E{h(s2)} for the new distribution\n",
      "and compare it with the value obtained for the original distribution, based on the\n",
      "deﬁnition of convexity:\n",
      "0.2h(4)+0.2h(1)+0.6h(0)\n",
      "= 0.2h(0.8 ×5 +0.2×0)+0.2h(0.2×5+0.8×0)+0.6h(0)\n",
      "< 0.2 ×(0.8h(5)+0.2h(0))+0.2×(0.2h(5)+0.8h(0))+0.6h(0)\n",
      "= 0.2h(5)+0.8h(0)\n",
      "(6.10)\n",
      "where the inequality is due to the deﬁnition of convexity in Eq. (6.6). Now, the last\n",
      "expression is the value of the sparseness measure in the original distribution. Thus,\n",
      "we see that the convexity of h makes the sparseness measure smaller when proba-\n",
      "bility mass is taken away from the extremes. This is true for any convex function.\n",
      "Suitable convex functions A simple convex function — which will be found to be\n",
      "very suitable for our purposes — is given by the negative square root:\n",
      "h2(u) = −√u\n",
      "(6.11)\n",
      "This function is actually equivalent to the one in Figure 6.3, because the addition of\n",
      "u = s2 just adds a constant 1 to the measure: Adding a linear term to the sparseness\n",
      "measure h has no effect because it only adds a constant due to the constraint of unit\n",
      "variance. Adding a linear term has no effect on convexity either (which is left as\n",
      "an exercise). This linear term and the constant term were just added to the function\n",
      "Figure 6.3 to illustrate the fact that it puts more weight on values near zero and far\n",
      "from zero, but the weight for values far from zero do not grow too fast.\n",
      "The validity of h2 as a sparseness measure is easy to see from Figure 6.3, which\n",
      "shows how the measure gives large values if the data is either around zero, or takes\n",
      "very large values. In contrast to h1, or kurtosis, it does not suffer from sensitivity to\n",
      "\n",
      "6.2 Learning one feature by maximization of sparseness\n",
      "143\n",
      "outliers because it is equivalent to using the square root which grows very slowly\n",
      "when going away from zero. Moreover, h2 also emphasizes the concentration around\n",
      "zero because it has a peak at zero itself.3\n",
      "Another point to consider is that the function h2(s2) is actually equal to the neg-\n",
      "ative of the absolute value function −|s|. It is not differentiable at zero, because\n",
      "its slope abruptly changes from −1 to +1. This may cause practical problems, for\n",
      "example in the optimization algorithms that will be used to maximize sparseness.\n",
      "Thus, it is often useful to take a smoother function, such as\n",
      "h3(u) = −logcosh√u\n",
      "(6.12)\n",
      "which is as a function of s\n",
      "h3(s2) = −logcoshs\n",
      "(6.13)\n",
      "The relevant functions and their derivatives are plotted in Figure 6.4. Note that the\n",
      "point is to have a function h that is a convex function as a function of the square\n",
      "u = s2 as in Eq. (6.12). When expressed as a function of s as in Eq. (6.13), the\n",
      "function need not be convex anymore.\n",
      "Alternatively, one could modify h2 as\n",
      "h2b = −\n",
      "√\n",
      "u +ε\n",
      "(6.14)\n",
      "where ε is a small constant. This is another smoother version of the square root\n",
      "function. It has the beneﬁt of being simpler than h3 when we consider h as a function\n",
      "of u; in contrast, h3 tends to be simpler when considered a function of s.\n",
      "There are many different convex function that one might choose, so the question\n",
      "arises whether there is an optimal one that we should use. In fact, estimation theory\n",
      "as described in detail in Chapter 7 shows that the optimal measure of sparseness is\n",
      "basically given by choosing\n",
      "hopt(s2) = log ps(s)\n",
      "(6.15)\n",
      "where ps is the probability density function of s. The function h2 is typically not a\n",
      "bad approximation of this optimal function for natural images. Often, the logarithm\n",
      "of a pdf has an even stronger singularity (peak) at zero than what h2 has. Thus, to\n",
      "avoid the singularity, it may be better to use something more similar to h2 or h3.\n",
      "This will be considered in more detail in Section 7.7.2.\n",
      "Summary To recapitulate, ﬁnding linear feature detectors of maximum sparseness\n",
      "can be done by ﬁnding a maximum of\n",
      "E{h(∑\n",
      "x,y\n",
      "W(x,y)I(x,y)]2)}\n",
      "(6.16)\n",
      "3 One could also argue that h2 does not give a large value for large values of s at all, but only for s\n",
      "very close to zero, because the function h2 has a peak at zero. This is a complicated point because\n",
      "we can add a linear function to h2 as pointed out above. In any case, it is certain that h2 puts much\n",
      "more weight on values of u very close to zero.\n",
      "\n",
      "144\n",
      "6 Sparse coding and simple cells\n",
      "a)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−2.5\n",
      "−2\n",
      "−1.5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "b)\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "−5\n",
      "−4\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "c)\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "Fig. 6.4: Illustration of the log cosh function and its comparison with the absolute value function.\n",
      "a) The function h2 is Eq. (6.11) is given in solid curve. The function h3 in (6.12) is given as a\n",
      "dash-dotted curve. b) The same functions h2 and h3 are given as function of s (and not its square).\n",
      "c) the derivatives of the functions in b).\n",
      "with respect to W, constraining W so that\n",
      "E{[∑\n",
      "x,y\n",
      "W(x,y)I(x,y)]2} = 1,\n",
      "(6.17)\n",
      "where the function h is typically chosen as in Eq. (6.12).\n",
      "Usually, there are many local maxima of the objective function (see Section 18.3\n",
      "for the concept of global and local maxima). Each of the local maxima gives a\n",
      "different feature.\n",
      "6.2.4 The case of canonically preprocessed data\n",
      "In practice, we use data that has been preprocessed by the canonical way described\n",
      "in Section 5.4. That is, the dimension of the data has been reduced by PCA to re-\n",
      "\n",
      "6.3 Learning many features by maximization of sparseness\n",
      "145\n",
      "duce computational load and to get rid of the aliasing artifacts, and the data has\n",
      "been whitened to simplify the correlation structure. Denoting the canonically pre-\n",
      "processed data by zi,i = 1,...,n the maximization then takes the form\n",
      "E\n",
      "(\n",
      "h([\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "vizi]2)\n",
      ")\n",
      "(6.18)\n",
      "with respect to the weights vi which are constrained so that\n",
      "∥v∥2 = ∑\n",
      "i\n",
      "v2\n",
      "i = 1\n",
      "(6.19)\n",
      "6.2.5 One feature learned from natural images\n",
      "Consider again the three distributions in Figure 5.1. All of them look quite sparse\n",
      "in the sense that the histograms (which are just estimates of the pdf’s) have a peak\n",
      "at zero. It is not obvious what kind of features are maximally sparse. However,\n",
      "optimizing a sparseness measure we can ﬁnd well-deﬁned features.\n",
      "Figure 6.5 shows the weights Wi obtained by ﬁnding a local maximum of sparse-\n",
      "ness, using the sparseness measure h3 and canonically preprocessed data. It turns out\n",
      "that features similar to Gabor functions and simple-cell receptive ﬁelds are charac-\n",
      "terized by maximum sparseness. The features that are local maxima of sparseness,\n",
      "they turn out to have the three basic localization properties: they are localized in\n",
      "the (x,y)-space, localized in frequency (i.e. they are band-pass), and localized in\n",
      "orientation space (i.e. they are oriented).\n",
      "Note that in contrast to variance, sparseness has many local maxima. Most local\n",
      "maxima (almost all, in fact) are localized in space, frequency, and orientation. The\n",
      "sparsenesses of different local maxima are often not very different from each other.\n",
      "In fact, if you consider a feature detectors whose weights are given by the Gabor\n",
      "functions which are but otherwise similar but are in two different locations, it is\n",
      "natural to assume that the sparsenesses of the two features must be equal, since\n",
      "the properties of natural images should be the same in all locations. The fact that\n",
      "sparseness has many local maxima forms the basis for learning many features.\n",
      "6.3 Learning many features by maximization of sparseness\n",
      "A single feature is certainly not enough: Any vision system needs many features\n",
      "to represent different aspects of an image. Since sparseness is locally maximized\n",
      "by many different features, we could, in principle, just ﬁnd many different local\n",
      "maxima — for example, by running an optimization algorithm starting from many\n",
      "\n",
      "146\n",
      "6 Sparse coding and simple cells\n",
      "Fig. 6.5: Three weight vectors found by maximization of sparseness in natural images. The max-\n",
      "imization was started in three different points which each gave one vector corresponding to one\n",
      "local maximum of sparseness.\n",
      "different random initial conditions. Such a method would not be very reliable, how-\n",
      "ever, because the algorithm could ﬁnd the same maxima many times.\n",
      "A better method of learning many features is to ﬁnd many local maxima that\n",
      "fulﬁll some given constraint. Typically, one of two options is used. First, we could\n",
      "constrain the detector weights Wi to be orthogonal to each other, just as in PCA.\n",
      "Second, we could constraint the different si to be uncorrelated. We choose here the\n",
      "latter because it is a natural consequence of the generative-model approach that will\n",
      "be explained in Chapter 7.\n",
      "Actually, these two methods are not that different after all, because if the data is\n",
      "whitened as part of canonical preprocessing (see Section 5.4), orthogonality and un-\n",
      "correlatedness are, in fact, the same thing, as was discussed in Section 5.3.2.2. This\n",
      "is one of the utilities in canonical preprocessing. Thus, decorrelation is equivalent\n",
      "to orthogonalization, which is a classic operation in matrix computations.\n",
      "Note that there is no order that would be intrinsically deﬁned between the fea-\n",
      "tures. This is in contrast to PCA, where the deﬁnition automatically leads to the\n",
      "order of the ﬁrst, second, etc. principal component. One can order the obtained\n",
      "components according to their sparseness, but such an ordering is not as important\n",
      "as in the case of PCA.\n",
      "6.3.1 Deﬂationary decorrelation\n",
      "There are basically two approaches that one can use in constraining the different fea-\n",
      "ture detectors to have uncorrelated outputs. The ﬁrst one is called deﬂation, and pro-\n",
      "ceeds by learning the features one-by-one. First, one learns the ﬁrst feature. Then,\n",
      "one learns a second feature under the constraint that its output must be uncorrelated\n",
      "from the output of the ﬁrst one, then a third feature whose output must be uncorre-\n",
      "lated from the two ﬁrst ones, and so on, always constraining the new feature to be\n",
      "uncorrelated from the previously found ones. In algorithmic form, this deﬂationary\n",
      "approach can be described as follows:\n",
      "1. Set k = 1.\n",
      "\n",
      "6.3 Learning many features by maximization of sparseness\n",
      "147\n",
      "2. Find a vector W that maximizes the sparseness:\n",
      "E\n",
      "(\n",
      "h\n",
      " \n",
      "[∑\n",
      "x,y\n",
      "W(x,y)I(x,y)]2\n",
      "!)\n",
      "(6.20)\n",
      "under the constraints of unit variance of deﬂationary decorrelation:\n",
      "E\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "∑\n",
      "x,y\n",
      "W(x,y)I(x,y)\n",
      "!2\n",
      "\n",
      "= 1\n",
      "(6.21)\n",
      "E\n",
      "(\n",
      "∑\n",
      "x,y\n",
      "W(x,y)I(x,y)∑\n",
      "x,y\n",
      "Wi(x,y)I(x,y)\n",
      ")\n",
      "= 0 for all 1 ≤i < k\n",
      "(6.22)\n",
      "3. Store this vector in Wk and increment k by one.\n",
      "4. If k does not equal the dimension of the space, go back to step 2.\n",
      "The deﬂationary approach is easy to understand. However, it is not recommended\n",
      "because of some drawbacks. Basically, in the deﬂationary approach those features\n",
      "that are found in the beginning are privileged over others. They can do the optimiza-\n",
      "tion in the whole space whereas the last vectors (k close to the dimension of the\n",
      "space) have very little space where to optimize. This leads to the gradual deterio-\n",
      "ration of the features: the latter ones are often rather poor because their form is so\n",
      "severely restricted. In other words, the random errors (due to limited sample size),\n",
      "as well as numerical errors (due to inexact optimization) in the ﬁrst feature weights\n",
      "propagate to latter weights, and produce new errors in them. A further problem\n",
      "is that the method is not very principled; in fact, the more principled approach to\n",
      "sparse coding discussed in the next chapter leads to the following method, symmet-\n",
      "ric decorrelation.\n",
      "6.3.2 Symmetric decorrelation\n",
      "It would be more natural and efﬁcient to use a method in which all the features\n",
      "are learned on an equal footing. This is achieved in what is called the symmetric\n",
      "approach. In the symmetric approach, we maximize the sum of the sparsenesses of\n",
      "the outputs. In this maximization, the outputs of all units are constrained to be un-\n",
      "correlated. Thus, no ﬁlters are privileged. Using the measures of sparseness deﬁned\n",
      "above, this leads to an opimization problem of the following form:\n",
      "Maximize\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "E\n",
      "(\n",
      "h\n",
      " \n",
      "[∑\n",
      "x,y\n",
      "Wi(x,y)I(x,y)]2\n",
      "!)\n",
      "(6.23)\n",
      "under the constraints of unit variance and symmetric decorrelation:\n",
      "\n",
      "148\n",
      "6 Sparse coding and simple cells\n",
      "E\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "∑\n",
      "x,y\n",
      "Wi(x,y)I(x,y)\n",
      "!2\n",
      "\n",
      "= 1 for all i\n",
      "(6.24)\n",
      "E\n",
      "(\n",
      "∑\n",
      "x,y\n",
      "Wi(x,y)I(x,y)∑\n",
      "x,y\n",
      "Wj(x,y)I(x,y)\n",
      ")\n",
      "= 0 for all i ̸= j\n",
      "(6.25)\n",
      "This approach can also be motivated by considering that we are actually maximiz-\n",
      "ing the sparseness of a representation instead of sparsenesses of the features; these\n",
      "concepts will be discussed next.\n",
      "Whichever method of decorrelation is used, this approach limits the number of\n",
      "features that we can learn to the dimensionality of the data. For canonically prepro-\n",
      "cessed data, this is the dimensionality chosen in the PCA stage. This is because the\n",
      "features are constrained orthogonal in the whitened space, and there can be at most n\n",
      "orthogonal vectors in an n-dimensional space. Some methods are able to learn more\n",
      "features than this, they will be treated later in Section 13.1.\n",
      "6.3.3 Sparseness of feature vs. sparseness of representation\n",
      "When considering a group of features, sparseness has two distinct aspects. First, we\n",
      "can look at the distribution of a single feature s when the input consists of many\n",
      "natural images It,t = 1,...T, as we did above — this is what we call the sparseness\n",
      "of features (or “lifetime sparseness”). The second aspect is to look at the distribution\n",
      "of the features si over the index i = 1,...,n, for a single input image I — this is what\n",
      "we call the sparseness of the representation (or “population sparseness”).\n",
      "Sparseness of a representation means that a given image is represented by only\n",
      "a small number of active (clearly non-zero) features. This was, in fact, one of the\n",
      "main motivations of looking for sparse features in the ﬁrst place, and it has been\n",
      "considered the deﬁning feature of sparse coding, i.e. a sparse representation.\n",
      "A sparse representation can be compared to a vocabulary in a spoken language.\n",
      "A vocabulary typically consists of tens of thousands of words. Yet, to describe a\n",
      "single event or a single object, we only need to choose a few words. Thus, most of\n",
      "the words are not active in the representation of a single event. In the same way, a\n",
      "sparse representation consists of a large number of potential features; yet, to describe\n",
      "a single input image, only a small subset of them are activated.\n",
      "This kind of reduction of active elements must be clearly distinguished from\n",
      "dimension reduction techniques such as principal component analysis (PCA). In\n",
      "PCA, we choose once and for all a small set of features that are used for representing\n",
      "all the input patches. The number of these principal features is smaller than the\n",
      "dimension of the original data, which is why this is called dimension reduction. In\n",
      "a sparse representation, the active features are different from patch to patch, and the\n",
      "total number of features in the representation need not be smaller than the number\n",
      "of dimensions in the original data — in fact, it can even be larger.\n",
      "\n",
      "6.3 Learning many features by maximization of sparseness\n",
      "149\n",
      "What is then the connection between these two concepts of sparseness? Basically,\n",
      "we could measure the sparseness of the representation of a given image using the\n",
      "same measures as we used for the sparseness of the features. Thus, for a single image\n",
      "It, the sparseness of the representation given by the image ﬁlters Wi,i = 1...,n can\n",
      "be measured as:\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "h\n",
      " \n",
      "[∑\n",
      "x,y\n",
      "Wi(x,y)It(x,y)]2\n",
      "!\n",
      "(6.26)\n",
      "For this measure to be justiﬁed in the same way as we justiﬁed it above, it must be\n",
      "assumed that for the single image, the following two normalization conditions hold:\n",
      "1. the mean of the features is zero, and\n",
      "2. the mean of the square of the features equals one (or any other constant).\n",
      "While these conditions do not often hold exactly for a single image, they typically\n",
      "are approximately true for large sets of features. In particular, if the features are\n",
      "statistically independent and identically distributed (see Section 4.5), the conditions\n",
      "will be approximately fulﬁlled by the law of large numbers — the basic statistical\n",
      "law that says that the average of independent observations tends to the expectation.\n",
      "Now, let us assume that we have observed T image patches It(x,y),t = 1,...,T,\n",
      "and let us simply take the sum of the sparsenesses of each image computed as in\n",
      "Equation (6.26) above. This gives\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "h\n",
      " \n",
      "[∑\n",
      "x,y\n",
      "Wi(x,y)It(x,y)]2\n",
      "!\n",
      "(6.27)\n",
      "Rearranging the summations, we see that this is equal to\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "h\n",
      " \n",
      "[∑\n",
      "x,y\n",
      "Wi(x,y)It(x,y)]2\n",
      "!\n",
      "(6.28)\n",
      "The expression in Equation (6.28) is the sum of the sparsenesses of the features.\n",
      "The expression in Equation (6.27) is the sum of the sparsenesses of representations.\n",
      "Thus we see that these two measures are equal. However, for this equality to be\n",
      "meaningful, it must be assumed that the normalization conditions given above hold\n",
      "as well. Above, we argued that they are approximately fulﬁlled if the features are\n",
      "approximately independent.\n",
      "So, we can conclude that sparseness of features and sparseness of representation\n",
      "give approximately the same function to maximize, hence the same feature set. The\n",
      "functions are closer to equal when the feature sets are large and the features are\n",
      "statistically independent and have identical distributions. However, the measures\n",
      "might be different if the normalization conditions above are far from true.4\n",
      "4 Here’s a counterexample in which the sparseness of features is zero but the sparseness of repre-\n",
      "sentation is high. Consider ten independent gaussian features with zero mean. Assume nine have\n",
      "a very small variance, and one of them has a very large variance. Each of the features, considered\n",
      "separately, is gaussian, and thus not sparse. However, for each image, the feature distribution has\n",
      "\n",
      "150\n",
      "6 Sparse coding and simple cells\n",
      "6.4 Sparse coding features for natural images\n",
      "6.4.1 Full set of features\n",
      "Now we are ready to learn a whole set of features from natural images. We sampled\n",
      "randomly 50,000 image patches of 32 × 32 pixels, and applied canonical prepro-\n",
      "cessing to them, reducing the dimension to 256, which meant retaining 25% of the\n",
      "dimensions. We used the logcosh function, i.e. h3 in Eq. (6.12), and symmetric\n",
      "decorrelation. The actual optimization was done using a special algorithm called\n",
      "FastICA, described in Section 18.7.\n",
      "The obtained results are shown in Figure 6.6. Again, the feature detector weights\n",
      "are coded so that the grey-scale value of a pixel means the value of the coefﬁcient\n",
      "at that pixel. Grey pixels mean zero coefﬁcients.\n",
      "Visually, one can see that these feature detectors have interesting properties. First,\n",
      "they are localized in space: most of the coefﬁcients are practically zero outside of a\n",
      "small receptive ﬁeld. The feature detectors are also oriented. Furthermore, they are\n",
      "multiscale in the sense that most of them seem to be coding for small things whereas\n",
      "a few are coding for large things (in fact, so large that they do not ﬁt in the window,\n",
      "so that the detectors are not completely localized).\n",
      "6.4.2 Analysis of tuning properties\n",
      "We can analyze the feature detectors Wi further by looking at the responses when\n",
      "gratings, i.e. sinusoidal functions, are input to them. In other words, we create arti-\n",
      "ﬁcial images which are two-dimensional sinusoids, and compute the outputs si. We\n",
      "consider sinusoidal functions of the form\n",
      "fo(x,y) = sin(2πα(sin(θ)x+cos(θ)y))\n",
      "(6.29)\n",
      "fe(x,y) = cos(2πα(sin(θ)x+cos(θ)y))\n",
      "(6.30)\n",
      "These are sinusoidal gratings where θ gives the orientation (angle) of the oscilla-\n",
      "tion, the x axis corresponding to θ = 0. The parameter α gives the frequency. The\n",
      "two functions give two oscillations in different phases; more precisely, they are in\n",
      "quadrature-phase, i.e. a 90 degrees phase difference.\n",
      "Now, we compute these functions for a large number of orientations and fre-\n",
      "quencies. We normalize the obtained functions to unit norm. Then we compute the\n",
      "dot-products of the Wi with each of the gratings. We can then compute the opti-\n",
      "mal orientation and frequency by ﬁnding the α and θ that maximize the sum of the\n",
      "squares of the two dot-products corresponding to the sin and cos functions. (We take\n",
      "nine values close to zero and one which is typically very large, and therefore the distribution is\n",
      "sparse. The key here is that the features have different variances, which violates the normalization\n",
      "conditions.\n",
      "\n",
      "6.4 Sparse coding features for natural images\n",
      "151\n",
      "Fig. 6.6: The whole set of symmetrically orthogonalized feature vectors Wi maximizing sparsity,\n",
      "learned from natural images.\n",
      "the sum of squares because we do not want the phase of the Wi to have inﬂuence on\n",
      "this computation.)\n",
      "This is actually almost the same as computing the 2-D power spectrum for all\n",
      "orientations and frequencies. We could do similar computations using the Discrete\n",
      "(or Fast) Fourier Transform as well, but we prefer here this direct computation for\n",
      "two reasons. First, we see the concrete meaning of the power spectrum in these\n",
      "computations. Second, we can compute the gratings for many more combinations\n",
      "of orientations and frequencies than is possible by the DFT.\n",
      "In neurophysiology, this kind of analysis is usually done using drifting gratings.\n",
      "In other words, the gratings move on the screen in the direction of their oscilla-\n",
      "tion. The maximum response of the cell for a drifting grating of a given (spatial)\n",
      "frequency and orientation is measured. This is more or less the same thing as the\n",
      "\n",
      "152\n",
      "6 Sparse coding and simple cells\n",
      "analysis that we are conducting here on our model simple cells. The fact that the\n",
      "gratings move in time may be necessary in neurophysiology because movement\n",
      "greatly enhances the cell responses, and so this method allows faster and and more\n",
      "accurate measurement of the optimal orientation and frequency. However, it compli-\n",
      "cates the analysis because we have an additional parameter, the temporal frequency\n",
      "of the grating, in the system. Fortunately, we do not need to use drifting gratings in\n",
      "our analysis.\n",
      "When we have found the optimal frequency and orientation parameters, we can\n",
      "analyze the selectivities by changing one of the parameters in the grating, and com-\n",
      "puting again the total response to two gratings that have the new parameters and\n",
      "are in quadrature phase. Such analysis of selectivity (tuning curves) is routinely\n",
      "performed in visual neuroscience.\n",
      "In the same way, we can analyze the selectivity to phase. Here, we must obvi-\n",
      "ously take a slightly different approach since we cannot take two ﬁlters in quadra-\n",
      "ture phase since then the total response would not depend on the phase at all. In\n",
      "neurophysiology, this is analyzed by simply plotting the response as a function of\n",
      "time when the input is a drifting grating with the optimal frequency and orientation.\n",
      "We can simulate the response by simply taking the dot-product of Wi with gratings\n",
      "whose phase goes through all possible values, and still keeping the orientation and\n",
      "frequency at optimal values. (The real utility of the analysis of phase selectivity will\n",
      "be seen when the responses of linear features are compared with nonlinear ones in\n",
      "Chapter 10.)\n",
      "In Figure 6.7 we show the results of the analysis for the ﬁrst ten features in\n",
      "Fig. 6.6, i.e., the ﬁrst ten receptive ﬁelds on the ﬁrst row. What we see is that all\n",
      "the cells are tuned to a speciﬁc values of frequency, orientation, and phase: any\n",
      "deviation from the optimal value decreases the response.\n",
      "It is also interesting to look at how the (optimal) orientations and frequencies are\n",
      "related to each other. This is shown in Fig. 6.8. One can see that the model tries\n",
      "to cover all possible combinations of these variables. However, there is a strong\n",
      "emphasis on the highest frequencies that are present in the image. Note that prepro-\n",
      "cessing by PCA removed the very highest frequencies, so the highest frequencies\n",
      "present are much lower (approx. 9 cycles per patch) than the Nyquist frequency\n",
      "(32/2=16 cycles per patch).\n",
      "Another way of looking at the distributions is to plot the histograms of the two\n",
      "parameters separately, as shown in Fig. 6.9. Here we see again that most of the fea-\n",
      "tures have very high frequencies. The orientations are covered rather uniformly, but\n",
      "there are more features with horizontal orientation (0 or, equivalently, π). This is an-\n",
      "other expression of the anisotropy of natural images, already seen in the correlations\n",
      "in Section 5.7.\n",
      "\n",
      "6.4 Sparse coding features for natural images\n",
      "153\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.05\n",
      "0.1\n",
      "0.15\n",
      "0.2\n",
      "0.25\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.05\n",
      "0.1\n",
      "0.15\n",
      "0.2\n",
      "0.25\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "Fig. 6.7: Tuning curves of the ten ﬁrst sparse coding features Wi in Figure 6.6. Left: change in\n",
      "frequency (the unit is cycles in the window of 32 × 32 pixels, so that 16 means wavelength of 2\n",
      "pixels). Middle: change in orientation. Right: change in phase.\n",
      "\n",
      "154\n",
      "6 Sparse coding and simple cells\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "Fig. 6.8: Scatter plot of the frequencies and orientations of the sparse coding features. Horizontal\n",
      "axis: orientation, vertical axis: frequency\n",
      "a)\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "b)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "Fig. 6.9: Histograms of the optimal a) frequencies and b) orientations of the linear features ob-\n",
      "tained by sparse coding.\n",
      "\n",
      "6.5 How is sparseness useful?\n",
      "155\n",
      "6.5 How is sparseness useful?\n",
      "6.5.1 Bayesian modelling\n",
      "The central idea in this book is that it is useful to ﬁnd good statistical models for nat-\n",
      "ural images because such models provide the prior probabilities needed in Bayesian\n",
      "inference, or, in general, the prior information that the visual system needs on the\n",
      "environment. These tasks include denoising and completion of missing data.\n",
      "So, sparse coding models are useful for the visual system simply because they\n",
      "provide a better statistical model of the input data. The outputs of ﬁlter detectors are\n",
      "sparse, so this sparseness should be accounted for by the model. We did not really\n",
      "show that we get a better statistical model this way, but this point will be considered\n",
      "in the next chapter.\n",
      "A related viewpoint is that of information theory: sparseness is assumed to lead\n",
      "to a more efﬁcient code of the input. This viewpoint will be considered in Chapter 8.\n",
      "6.5.2 Neural modelling\n",
      "Another viewpoint is to just consider the power of the statistical models to account\n",
      "for the properties of the visual system. From the viewpoint of computational neuro-\n",
      "science, sparse coding leads to the emergence of receptive ﬁelds similar to simple\n",
      "cells, so sparse coding is clearly a better model of the visual cortex in this respect\n",
      "than, say, PCA. Results in Chapters 15 and 16 give even more support to this claim.\n",
      "This viewpoint does not consider why the visual system should use sparseness.\n",
      "6.5.3 Metabolic economy\n",
      "However, there are other, additional, reasons as well why it would be advantageous\n",
      "for the visual system to use sparse coding, and these reasons have nothing to do with\n",
      "the statistics of the input stimuli. The point is that ﬁring of cells consumes energy,\n",
      "and energy is one of the major constraints on the biological “design” of the brain. A\n",
      "sparse code means that most cells do not ﬁre more than their spontaneous ﬁring rate\n",
      "most of the time. Thus, sparse coding is energy-efﬁcient.\n",
      "So, we have a fortunate coincidence where those linear features that are opti-\n",
      "mal statistically are also optimal from the viewpoint of energy consumption. Possi-\n",
      "bly, future research will show some deep connections between these two optimality\n",
      "properties.\n",
      "\n",
      "156\n",
      "6 Sparse coding and simple cells\n",
      "6.6 Concluding remarks and References\n",
      "In this chapter, we learned feature detectors which maximize the sparseness of their\n",
      "outputs when the input is natural images. Sparseness is a statistical property which\n",
      "is completely unrelated to variance, which was the criterion in PCA in the preceding\n",
      "chapter. Maximization of sparseness yield receptive ﬁelds which are quite similar to\n",
      "those of simple cells. This fundamental result is the basis of all the developments in\n",
      "the rest of this book.\n",
      "Early work on ﬁnding maximally sparse projection can be found in (Field, 1987,\n",
      "1994). Estimating a whole basis for image patches was ﬁrst accomplished in the\n",
      "seminal paper (Olshausen and Field, 1996) using a method considered in Sec-\n",
      "tion 13.1. A detailed comparison with simple cell receptive ﬁelds is in (van Hateren\n",
      "and van der Schaaf, 1998); see also (van Hateren and Ruderman, 1998). A discus-\n",
      "sion on sparseness of features vs. sparseness of representation is in (Willmore and\n",
      "Tolhurst, 2001).\n",
      "The idea of increasing metabolic efﬁciency by sparse coding dates back to (Bar-\n",
      "low, 1972); for more recent analysis, see e.g. (Levy and Baxter, 1996; Balasubra-\n",
      "maniam et al, 2001; Attwell and Laughlin, 2001; Lennie, 2003).\n",
      "Some researchers have actually measured the sparseness of real neuron outputs,\n",
      "typically concluding that they are sparse, see (Baddeley et al, 1997; Gallant et al,\n",
      "1998; Vinje and Gallant, 2000, 2002; Weliky et al, 2003).\n",
      "An approach that is popular in engineering is to take a ﬁxed linear basis and then\n",
      "analyze the statistics of the coefﬁcients in that basis. Typically, one takes a wavelet\n",
      "basis (see Section 17.3.2) which is not very much unlike the sparse coding basis.\n",
      "See (Simoncelli, 2005) for reviews based on such an approach.\n",
      "Approaches for sparse coding using concepts related to spike trains instead of\n",
      "mean ﬁring rates include (Olshausen, 2002; Smith and Lewicki, 2005, 2006).\n",
      "6.7 Exercices\n",
      "Mathematical exercises\n",
      "1. Show that if f(x) is a (strictly) convex function, i.e. fulﬁls Eq. (6.6), f(x)+ax+b\n",
      "has the same property, for any constants a,b.\n",
      "2. Show that the kurtosis of a gaussian random variable is zero. (For simplicity,\n",
      "assume the variable is standardized to zero mean and unit variance. Hint: try\n",
      "partial integration to calculate the fourth moment.)\n",
      "3. The Gram-Schmidt orthogonalization algorithm is deﬁned as follows. Given n\n",
      "feature detector vectors Wi(x,y) which have been normalized to unit norm, do\n",
      "a. Set i →1.\n",
      "b. Compute the new value of the vector wi as\n",
      "\n",
      "6.7 Exercices\n",
      "157\n",
      "Wi(x,y) ←Wi(x,y)−\n",
      "i−1\n",
      "∑\n",
      "j=1 ∑\n",
      "x′,y′\n",
      "Wj(x′,y′)Wi(x′,y′)Wj(x,y)\n",
      "(6.31)\n",
      "c. Renormalize Wi: Wi(x,y) ←Wi(x,y)/\n",
      "q\n",
      "∑x′,y′ Wi(x′,y′)2.\n",
      "d. Increment i by one and go back to step 1, if i is not yet larger than n.\n",
      "Show that the set of vectors is orthogonal after application of this algorithm.\n",
      "Computer assignments\n",
      "1. Take some images. Take samples of 10 × 10 pixels. Construct a simple edge\n",
      "detector. Compute its output. Plot the histogram of the output, and compute its\n",
      "kurtosis.\n",
      "\n",
      "\n",
      "Chapter 7\n",
      "Independent component analysis\n",
      "In this chapter, we discuss a statistical generative model called independent compo-\n",
      "nent analysis. It is basically a proper probabilistic formulation of the ideas under-\n",
      "pinning sparse coding. It shows how sparse coding can be interpreted as providing\n",
      "a Bayesian prior, and answers some questions which were not properly answered in\n",
      "the sparse coding framework.\n",
      "7.1 Limitations of the sparse coding approach\n",
      "In the preceding chapter, we showed that by ﬁnding linear feature detectors that\n",
      "maximize the sparseness of the outputs, we ﬁnd features that are localized in space,\n",
      "frequency, and orientation, thus being similar to Gabor functions and simple cell\n",
      "receptive ﬁelds. While that approach had intuitive appeal, it was not completely\n",
      "satisfactory in the following respects:\n",
      "1. The choice of the sparseness measure was rather ad hoc. It would be interesting\n",
      "to ﬁnd a principled way of determining the optimal nonlinear function h used in\n",
      "the measures.\n",
      "2. The learning of many features was done by simply constraining the outputs of\n",
      "feature detectors to be uncorrelated. This is also quite ad hoc, and some justiﬁ-\n",
      "cation for the decorrelation is needed.\n",
      "3. The main motivation for this kind of statistical modelling of natural images is\n",
      "that the statistical model can be used as a prior distribution in Bayesian infer-\n",
      "ence. However, just ﬁnding maximally sparse features does not give us a prior\n",
      "distribution.\n",
      "A principled approach that also solves these problems is using generative models. A\n",
      "generative model describes how the observed data (natural images) is generated as\n",
      "transformations of some simple original variables. The original variables are called\n",
      "latent since they cannot usually be observed directly.\n",
      "159\n",
      "\n",
      "160\n",
      "7 Independent component analysis\n",
      "The generative model we propose here for modelling natural image patches is\n",
      "called independent component analysis. This model was originally developed to\n",
      "solve rather different kinds of problems, in particular, the so-called blind source\n",
      "separation problem, see the References section below for more information. How-\n",
      "ever, it turns out that the same model can be interpreted as a form of sparse coding,\n",
      "and is more or less equivalent to ﬁnding linear features that are maximally sparse,\n",
      "as we will see in this chapter.\n",
      "7.2 Deﬁnition of ICA\n",
      "7.2.1 Independence\n",
      "The latent variables in independent component analysis (ICA) are called indepen-\n",
      "dent components. While the term “component” is mainly used for historical reasons\n",
      "(inspired by the expression “principal components”), the word “independence” tells\n",
      "what the basic starting point of ICA is: the latent variables are assumed to be statis-\n",
      "tically independent.\n",
      "Let us consider two random variables, say s1 and s2. Basically, the variables s1\n",
      "and s2 are statistically independent if information on the value of s1 does not give\n",
      "any information on the value of s2, and vice versa. In this book, whenever the word\n",
      "“independent” is used, it always refers to statistical independence, unless otherwise\n",
      "mentioned.\n",
      "Section 4.5 gave a more extensive treatment of independence. Here we recall the\n",
      "basic deﬁnition. Let us denote by p(s1,s2) the joint probability density function of\n",
      "s1 and s2. Let us further denote by p1(s1) the marginal pdf of s1, i.e. the pdf of s1\n",
      "when it is considered alone. Then we deﬁne that s1 and s2 are independent if and\n",
      "only if the joint pdf is factorizable, i.e. the pdf can be expressed as a product of the\n",
      "individual marginal pdf’s\n",
      "p(s1,s2) = p1(s1)p2(s2).\n",
      "(7.1)\n",
      "This deﬁnition extends naturally for any number n of random variables, in which\n",
      "case the joint density must be a product of n terms. (Note that we use here a sim-\n",
      "pliﬁed notation in which si appears in two roles: it is the random variable, and the\n",
      "value taken by the random variable — often these are denoted by slightly different\n",
      "symbols.)\n",
      "It is important to understand the difference between independence and uncor-\n",
      "relatedness. If the two random variables are independent, they are necessarily un-\n",
      "correlated as well. However, it is quite possible to have random variables that are\n",
      "uncorrelated, yet strongly dependent. Thus, correlatedness is a special kind of de-\n",
      "pendence. In fact, if the two variables s1 and s2 were independent, any nonlinear\n",
      "transformation of the outputs would be uncorrelated as well:\n",
      "\n",
      "7.2 Deﬁnition of ICA\n",
      "161\n",
      "cov(g1(s1),g2(s2)) = E{g1(s1)g2(s2)} −E{g1(s1)}E{g2(s2)} = 0\n",
      "(7.2)\n",
      "for any two functions g1 and g2. When probing the dependence of si and sj, a sim-\n",
      "ple approach would thus be to consider the correlations of some nonlinear functions.\n",
      "However, for statistical and computational reasons, we will develop a different ap-\n",
      "proach below.\n",
      "7.2.2 Generative model\n",
      "The generative model in ICA is deﬁned by a linear transformation of the latent\n",
      "independent components. Let us again denote by I(x,y) the pixel grey-scale values\n",
      "(point luminances) in an image, or in practice, a small image patch. In ICA, an\n",
      "image patch is generated as a linear superposition of some features Ai, as discussed\n",
      "in Section 2.3:\n",
      "I(x,y) =\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si\n",
      "(7.3)\n",
      "for all x and y. The si are coefﬁcients that are different from patch to patch. They\n",
      "can thus be considered as random variables, since their values change randomly\n",
      "from patch to patch. In contrast, the features Ai are the same for all patches.\n",
      "The deﬁnition of ICA is now based on three assumptions made regarding this\n",
      "linear generative model:\n",
      "1. The fundamental assumption is that the si are statistically independent when con-\n",
      "sidered as random variables.\n",
      "2. In the next sections we will also see that in order to be able to estimate the model,\n",
      "we will also have to assume that the distributions of the si are non-gaussian. This\n",
      "assumption shows the connection to sparse coding since sparseness is a form of\n",
      "non-gaussianity.\n",
      "3. We will also usually assume that the linear system deﬁned by the Ai is invertible\n",
      "but this is a technical assumption that is not always completely necessary. In\n",
      "fact, we will see below that we might prefer to assume that the linear system is\n",
      "invertible after canonical preprocessing, which is not quite the same thing.\n",
      "These assumptions are enough to enable estimation of the model. Estimation\n",
      "means that given a large enough sample of image patches, It,t = 1,...,T, we can\n",
      "recover some reasonable approximations of the values of Ai, without knowing the\n",
      "values of the latent components si in advance.\n",
      "One thing that we cannot recover is the scaling and signs of the components. In\n",
      "fact, you could multiply a component si by any constant, say -2, and if you divide the\n",
      "corresponding Ai by the same constant, this does not show up in the data in any way.\n",
      "So, we can only recover the components up to a multiplicative constant. Usually, we\n",
      "simplify the situation by deﬁning that the components have unit variance. This only\n",
      "leaves the signs of the components undetermined. So, for any component si, we\n",
      "could just as well consider the component −si.\n",
      "\n",
      "162\n",
      "7 Independent component analysis\n",
      "As typical in linear models, estimation of the Ai is equivalent to determining the\n",
      "values of the Wi which give the si as outputs of linear feature detectors with some\n",
      "weights Wi:\n",
      "si = ∑\n",
      "x,y\n",
      "Wi(x,y)I(x,y)\n",
      "(7.4)\n",
      "for each image patch. The coefﬁcients Wi are obtained by inverting the matrix of the\n",
      "Ai.\n",
      "7.2.3 Model for preprocessed data\n",
      "In practice, we will usually prefer to formulate statistical models for canonically pre-\n",
      "processed data (see Section 5.4). The data variables in that reduced representation\n",
      "are denoted by zi. For a single patch, they can be collected to a vector z = (z1,...,zn).\n",
      "Since a linear transformation of a linear transformation is still a linear transforma-\n",
      "tion, the zi are also linear transformations of the independent components si, al-\n",
      "though the coefﬁcients are different from those in the original space. Thus, we have\n",
      "zi =\n",
      "m\n",
      "∑\n",
      "j=1\n",
      "bijsj\n",
      "(7.5)\n",
      "for some coefﬁcients bij which can be obtained by transforming the features Ai using\n",
      "the same PCA transformation which is applied on the images.\n",
      "We want to choose the number n of independent components so that the lin-\n",
      "ear system can be inverted. Since we are working with preprocessed data, we will\n",
      "choose n so that it equals the number of variables after canonical preprocessing (in-\n",
      "stead of the number of original pixels). Then, the system in Eq. (7.5) can be inverted\n",
      "in a unique way and we can compute the si as a linear function of the zi:\n",
      "si =\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "vijzj = vT\n",
      "i z\n",
      "(7.6)\n",
      "Here, the vector vi = (v1i,...,vni) allows a simple expression using vector prod-\n",
      "ucts. The coefﬁcients vij are obtained by inverting the matrix of the coefﬁcients bij.\n",
      "The coefﬁcients Wi in Equation (7.4) are then obtained by concatenating the linear\n",
      "transformations given by vij and canonical preprocessing (i.e. multiplying the two\n",
      "matrices).\n",
      "7.3 Insufﬁciency of second-order information\n",
      "When comparing the feature learning results by PCA and sparse coding, it is nat-\n",
      "ural to conclude that the second-order information (i.e. covariances) used in PCA\n",
      "\n",
      "7.3 Insufﬁciency of second-order information\n",
      "163\n",
      "and other whitening methods is insufﬁcient. In this section, we justify the same\n",
      "conclusion from another viewpoint: we show that second-order information is not\n",
      "sufﬁcient for estimation of the ICA model, which also implies that the components\n",
      "should not be gaussian.\n",
      "7.3.1 Why whitening does not ﬁnd independent components\n",
      "It is tempting to think that if we just whiten the data, maybe the whitened com-\n",
      "ponents are equal to the independent components. The justiﬁcation would be that\n",
      "ICA is a whitening transformation, because it gives components which are inde-\n",
      "pendent, and thus uncorrelated, and we deﬁned their variances to be equal to one.\n",
      "The fundamental error with this logic is that there is an inﬁnite number of whiten-\n",
      "ing transformations, because any orthogonal transformation of whitened data is still\n",
      "white, as pointed out in Section 5.3.2. So, if you whiten the data by, say, PCA, you\n",
      "get just one of those many whitening transformations, and there is absolutely no\n",
      "reason to assume that you would get the ICA transformation.\n",
      "There is a reason why it is, in fact, not possible to estimate the ICA model using\n",
      "any method which is only based on covariances. This is due to the symmetry of the\n",
      "covariance matrix: cov(z1,z2) = cov(z2,z1). Thus, the number of different covari-\n",
      "ances you can estimate from data is equal to n(n+1)\n",
      "2\n",
      ", i.e. roughly one half of n2. In\n",
      "contrast, the number of parameters bij we want to estimate (this refers to the model\n",
      "with preprocessed data in Equation (7.5)) is equal to n2. So, if we try to solve the bij\n",
      "by forcing the model to give just the right covariance structure, we have less equa-\n",
      "tions (by one half!) than we have variables, so the solution is not uniquely deﬁned!\n",
      "The same logic applies equally well to the original data before preprocessing.\n",
      "This is illustrated in Figure 7.1. We take two independent components s1 and\n",
      "s2 with very sparse distributions. Their joint distribution, in a), has a “star-shape”\n",
      "because the data is rather much concentrated on the coordinate axes. Then, we mix\n",
      "these variables linearly using randomly selected coefﬁcients b11 = 0.5, b12 = 1.5,\n",
      "b21 = 1 and b22 = 0.2. The resulting distribution is shown in Figure 7.1b). The star\n",
      "has now been “twisted”. When we whiten the data with PCA, we get the distribution\n",
      "in c). Clearly, the distribution is not the same as the original distribution in a). So,\n",
      "whitening failed to recover the original components.\n",
      "On the positive side, we see that the whitened distribution in Figure 7.1c) has\n",
      "the right “shape”, because what remains to be determined is the right orthogonal\n",
      "transformation, since all whitening transformations are orthogonal transformations\n",
      "of each other. In two dimension, an orthogonal transformation is basically a rotation.\n",
      "So, we have solved part of the problem. After whitening, we know that we only\n",
      "need to look for the remaining orthogonal transformation, which reduces the space\n",
      "in which we need to search for the right solution.\n",
      "Thus, we see why it was justiﬁed to constrain the different feature detectors to\n",
      "give uncorrelated outputs in the sparse coding framework in Section 6.3. Constrain-\n",
      "ing the transformation to be orthogonal for whitened data is equivalent to constrain-\n",
      "\n",
      "164\n",
      "7 Independent component analysis\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "Fig. 7.1: a) The joint distribution of the independent components s1 and s2 with sparse distribu-\n",
      "tions. Horizontal axis: s1, vertical axis: s2. b) The joint distribution of the observed data which are\n",
      "linear transformations of the s1 and s2. c) The joint distribution of observed data after whitening\n",
      "by PCA.\n",
      "ing the features si to be uncorrelated (and to have unit variance). Even in the case of\n",
      "ICA estimation, the features are often constrained to be uncorrelated, because this\n",
      "simpliﬁes the objective function, as discussed later in this chapter, and allows the\n",
      "development of very efﬁcient algorithms (see Section 18.7). In contrast, in the ICA\n",
      "framework, it is not justiﬁed, for example, to constrain the original features Ai or the\n",
      "detector weights Wi to be orthogonal, since the mixing matrix (or rather, its inverse)\n",
      "is not necessarily orthogonal in the ICA model.\n",
      "7.3.2 Why components have to be non-gaussian\n",
      "The insufﬁciency of second-order information also implies that the independent\n",
      "components must not be gaussian, because gaussian data contains nothing else than\n",
      "second-order information. In this section, we explain a couple of different view-\n",
      "points which further elaborate this point.\n",
      "7.3.2.1 Whitened gaussian pdf is spherically symmetric\n",
      "We saw above that after whitening, we have to ﬁnd the right rotation (orthogonal\n",
      "transformation) which gives ICA. If the data is gaussian, this is, in fact, not possible\n",
      "due to a symmetry property of gaussian data.\n",
      "To see why, let us consider the deﬁnition of the gaussian pdf in Equation (5.17)\n",
      "on page 115. Consider whitened variables, whose covariance matrix is the identity\n",
      "matrix by deﬁnition. The inverse of the identity matrix is the identity matrix, so\n",
      "C−1 is the identity matrix. Thus, we have ∑ij xixj[C−1]ij = ∑i x2\n",
      "i . Furthermore, the\n",
      "determinant of the identity matrix is equal to one. So, the pdf in Equation (5.17)\n",
      "becomes\n",
      "p(x1,...,xn) =\n",
      "1\n",
      "(2π)n/2 exp(−1\n",
      "2 ∑\n",
      "i\n",
      "x2\n",
      "i ) =\n",
      "1\n",
      "(2π)n/2 exp(−1\n",
      "2∥x∥2)\n",
      "(7.7)\n",
      "\n",
      "7.3 Insufﬁciency of second-order information\n",
      "165\n",
      "This pdf depends only on the norm ∥x∥. Such a pdf is called spherically symmetric:\n",
      "It is the same in all directions. So, there is no information left in the data to determine\n",
      "the rotation corresponding to the independent components.\n",
      "An illustration of this special property of the gaussian distribution is in Fig-\n",
      "ure 7.2, which shows a scatter plot of two uncorrelated gaussian variables of unit\n",
      "variance. The distribution is the same in all directions, except for random sampling\n",
      "effects. The circles show contours on which the pdf is constant. It is clear that if you\n",
      "rotate the data in any way, the distribution does not change, so there is no way to\n",
      "distinguish the right rotation from the wrong ones.\n",
      "Fig. 7.2: A scatter plot of two uncorrelated gaussian variables of unit variance. This is what any\n",
      "whitening method would give when applied on gaussian data. The distribution is spherically sym-\n",
      "metric, i.e. the same in all directions. This is also seen by looking at contours on which the pdf is\n",
      "constant: they are circles, as further plotted here.\n",
      "7.3.2.2 Uncorrelated gaussian variables are independent\n",
      "A further justiﬁcation why ICA is not possible for gaussian variables is provided by\n",
      "a fundamental result in probability theory. It says that if random variables s1,...,sn\n",
      "have a gaussian distribution and they are uncorrelated, then they are also indepen-\n",
      "dent. Thus, for gaussian variables, uncorrelatedness and independence are the same\n",
      "thing, although in general uncorrelatedness does not imply independence. This fur-\n",
      "ther shows why ICA brings nothing new for gaussian variables: The main interest-\n",
      "ing thing you can do to gaussian variables is to decorrelate them, which is already\n",
      "accomplished by PCA and other whitening methods in Chapter 5.\n",
      "\n",
      "166\n",
      "7 Independent component analysis\n",
      "It is easy to see from Equation (7.7) why uncorrelated gaussian variables are\n",
      "independent. Here, the variables are actually white, i.e. they have also been stan-\n",
      "dardized to unit variance, but this makes no difference since such standardization\n",
      "obviously cannot change the dependencies between the variables. The point is that\n",
      "the pdf in Equation (7.7) is something which can be factorized:\n",
      "p(x1,...,xn) = ∏\n",
      "i\n",
      "1\n",
      "√\n",
      "2π exp(−1\n",
      "2x2\n",
      "i )\n",
      "(7.8)\n",
      "where we have used the classic identity exp(a + b) = exp(a)exp(b). This form is\n",
      "factorized, i.e. it is a product of the one-dimensional standardized gaussian pdf’s.\n",
      "Such factorization is the essence of the deﬁnition of independence, as in Equa-\n",
      "tion (7.1). So, we have shown that gaussian variables xi are independent if they\n",
      "are uncorrelated.\n",
      "Thus, the components in ICA have to be non-gaussian in order for ICA to be\n",
      "meaningful. This also explains why models based on non-gaussianity (such as ICA)\n",
      "are very new in the ﬁeld of statistics: classic statistics is largely based on the assump-\n",
      "tion that continuous-valued variables have a gaussian distribution—that is why it is\n",
      "called the “normal” distribution!\n",
      "7.4 The probability density deﬁned by ICA\n",
      "Now that we have a statistical generative model of the data, we can compute the\n",
      "probability of each observed image patch using basic results in probability theory.\n",
      "Then we can estimate the optimal features using classic estimation theory. This\n",
      "solves some of the problems we mentioned in the introduction to this chapter: we\n",
      "will ﬁnd the optimal measure of sparseness, and we will see why the constraint of\n",
      "uncorrelatedness of the features makes sense. And obviously we can then use the\n",
      "model as a prior probability in Bayesian inference.\n",
      "Let us assume for the moment that we know the probability density functions\n",
      "(pdf’s) of the latent independent components si. These are denoted by pi. Then, by\n",
      "deﬁnition of independence, the multidimensional pdf of all the si is given by the\n",
      "product:\n",
      "p(s1,...,sn) =\n",
      "n\n",
      "∏\n",
      "i=1\n",
      "pi(si)\n",
      "(7.9)\n",
      "What we really want to ﬁnd is the pdf of the observed preprocessed variables zi,\n",
      "which is almost the same thing as having a pdf of the image patches I(x,y). It is\n",
      "tempting to think that we could just plug the formula for the si given by equation\n",
      "(7.6) into equation (7.9). However, this is not possible. The next digression (which\n",
      "can be skipped by readers not interested in mathematical details) will show why not.\n",
      "Short digression to probability theory To see why we cannot just combine (7.9) and (7.6),\n",
      "let us consider what the pdf means in the one-dimensional case, where we have just one variable\n",
      "s with probability density ps. By deﬁnition, the pdf at some point s0 gives the probability that s\n",
      "\n",
      "7.5 Maximum likelihood estimation in ICA\n",
      "167\n",
      "belongs to a very small interval of length d as follows:\n",
      "P(s ∈[s0,s0 +d]) = ps(s0)d\n",
      "(7.10)\n",
      "Now, let us consider a linearly transformed variable x = as for a > 0. Here, s can be solved as\n",
      "s = wx where w = 1/a (note that we use a notation that is as close to the ICA case as possible). Let\n",
      "us just plug this in the equation (7.10) and consider the probability at point s0 = wx0:\n",
      "P(wx ∈[wx0,wx0 +d]) = ps(wx0)d\n",
      "(7.11)\n",
      "Obviously, P(wx ∈[x1,x2]) = P(x ∈[x1/w,x2/w]). So, we can express (7.11) as\n",
      "ps(wx0)d = P(x ∈[x0,x0 + d\n",
      "w]) = px(x0) d\n",
      "w\n",
      "(7.12)\n",
      "Note that the length of the interval d changed to d/w, and so we changed the right-hand side\n",
      "of the equation to get the same term. Multiplying both sides of this equation by w/d we get\n",
      "ps(wx0)w = px(x0). Thus, the actual pdf of x is ps(wx)w, instead of simply ps(wx) ! This shows that\n",
      "in computing the pdf of a transformation, the change in scale caused by the transformation must\n",
      "be taken into account, by multiplying the probability density by a suitable constant that depends\n",
      "on the transformation.\n",
      "In general, an important theorem in probability theory says that for any linear\n",
      "transformation, the probability density function should be multiplied by the absolute\n",
      "value of the determinant detV of the matrix that gives the linear transformation.The\n",
      "determinant of a matrix is a measure of the associated change in scale (volume).\n",
      "The absolute value of the determinant is equal to the volume of the parallelepiped\n",
      "that is determined by its column vectors. (For more information on the determinant,\n",
      "see Section 19.4.)\n",
      "Thus, the pdf of the preprocessed data z deﬁned by ICA is actually given by\n",
      "p(z) = |det(V)|\n",
      "n\n",
      "∏\n",
      "i=1\n",
      "pi(vT\n",
      "i z) = |det(V)|\n",
      "n\n",
      "∏\n",
      "i=1\n",
      "pi(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "vijzj)\n",
      "(7.13)\n",
      "where V is a matrix whose elements are given by the coefﬁcients vij; in other words,\n",
      "the rows of V are given by the vectors vi.\n",
      "The pdf depends not only on the patch via z, but also on the parameters of the\n",
      "model, i.e. the weights vij. Equivalently, we could consider the probability as a\n",
      "function of the features bij, but this does not make any difference, since the vij are\n",
      "uniquely determined by the bij and vice versa. The formula for the probability in\n",
      "equation (7.13) is more easily formulated as a function of the vectors vi.\n",
      "7.5 Maximum likelihood estimation in ICA\n",
      "Maximum likelihood estimation is a classic, indeed, the classic method for estimat-\n",
      "ing parameters in a statistical model. It is based on a simple principle: Find those\n",
      "parameter values that would give the highest probability for the observed data. A\n",
      "brief description was provided in Section 4.8.\n",
      "\n",
      "168\n",
      "7 Independent component analysis\n",
      "The likelihood is the probability of observing the data for given model parame-\n",
      "ters. For a given data set, it is thus a function of the parameters. Let us assume that\n",
      "we have observed T image patches It(x,y),t = 1,...,T that are collected at random\n",
      "locations in some natural images. We consider here canonically preprocessed data,\n",
      "let us denote by zt the vector obtained by canonically preprocessing the image patch\n",
      "It.\n",
      "Because the patches are collected in random locations, we can assume that the\n",
      "patches are independent from each other. Thus, the probability of observing all these\n",
      "patches is the product of the probabilities of each patch. This gives the likelihood L\n",
      "of the observed data:\n",
      "L(v1,...,vn) =\n",
      "T\n",
      "∏\n",
      "t=1\n",
      "p(zt) =\n",
      "T\n",
      "∏\n",
      "t=1\n",
      "\"\n",
      "|det(V)|\n",
      "n\n",
      "∏\n",
      "i=1\n",
      "pi(vT\n",
      "i zt)\n",
      "#\n",
      "(7.14)\n",
      "It is much simpler to look at the logarithm of the likelihood, which is, after some\n",
      "simple rearrangements:\n",
      "logL(v1,...,vn) = T log|det(V)|+\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "log pi(vT\n",
      "i zt)\n",
      "(7.15)\n",
      "Since the logarithm is a increasing function, maximization of the likelihood is the\n",
      "same as maximization of this log-likelihood. Estimation by the maximum likelihood\n",
      "method now means that we maximize the log-likelihood in Equation (7.15) with\n",
      "respect to the parameters, that is, the weights vi. (Choosing the functions log pi will\n",
      "be discussed in Section 7.7.)\n",
      "Maximization of the log-likelihood can be accomplished by numerical optimiza-\n",
      "tion methods. In addition to general-purpose methods, special tailor-made methods\n",
      "have been developed for this particular ICA maximization task. A thorough discus-\n",
      "sion of such optimization methods can be found in Chapter 18, and we will not go\n",
      "into details here. Let us just note that the ﬁrst term in Equation 7.15 can be consid-\n",
      "ered to be constant and omitted, as we will see in Section 7.7.1.\n",
      "7.6 Results on natural images\n",
      "7.6.1 Estimation of features\n",
      "Using maximum likelihood estimation on 50,000 image patches of size 32×32 pix-\n",
      "els as in the preceding chapters, we obtain the results in Figure 7.3. These features\n",
      "have the same qualitative properties as the feature detectors estimated by maxi-\n",
      "mization of sparseness in Figure 6.6 on page 151. That is, the features are spatially\n",
      "localized, oriented, and code for different scales (frequencies).\n",
      "This is actually not surprising because, as will be shown next, maximum like-\n",
      "lihood estimation of ICA is mathematically almost equivalent to the sparse coding\n",
      "\n",
      "7.6 Results on natural images\n",
      "169\n",
      "analysis we did in Section 6.4. The only difference is that we are here showing\n",
      "the (generating) features Ai instead of the feature detectors Wi. This difference is\n",
      "explained in detail in Section 7.10.\n",
      "Fig. 7.3: The whole set of features Ai obtained by ICA. In this estimation, the functions log pi were\n",
      "chosen as in Equation (7.19) in Section 7.7.\n",
      "7.6.2 Image synthesis using ICA\n",
      "Now that we have deﬁned a generative model, we can generate image data from it.\n",
      "We generate the values of the si independently from each other, and multiply the\n",
      "estimated features Ai with them to get one generated image patch. One choice we\n",
      "\n",
      "170\n",
      "7 Independent component analysis\n",
      "have to make is what our model of the marginal (i.e. individual) distributions of the\n",
      "independent component is. We use here two distributions. In the ﬁrst case, we simply\n",
      "take the histogram of the actual component in the natural images, i.e. the histogram\n",
      "of each ∑x,yWi(x,y)I(x,y) when computed over the whole set of images. In the\n",
      "second case, we use a well-known sparse distribution, the Laplacian distribution\n",
      "(discussed in the next section), as the distribution of the independent components.\n",
      "Figures 7.4 and 7.5 show the results in the two cases. The synthesis results are\n",
      "clearly different than those obtained by PCA on page 117: Here we can see more\n",
      "oriented, edge-like structures. However, we are obviously far from reproducing all\n",
      "the properties of natural images.\n",
      "Fig. 7.4: Image synthesis using ICA. 20 patches were randomly generated using the ICA model\n",
      "whose parameters were estimated from natural images. In this ﬁgure, the marginal distributions of\n",
      "the components were those of the real independent components. Compare with real natural image\n",
      "patches in Figure 5.2 on page 99, and the PCA synthesis results in Figure 5.13 on page 117.\n",
      "7.7 Connection to maximization of sparseness\n",
      "In this section, we show how ICA estimation is related to sparseness, how we should\n",
      "model the log pi in the log-likelihood in Equation 7.15 and how this connection tells\n",
      "us how we should design the sparseness measure.\n",
      "\n",
      "7.7 Connection to maximization of sparseness\n",
      "171\n",
      "Fig. 7.5: Image synthesis using ICA, and a Laplacian approximation of the pdf of the independent\n",
      "components. Compare with Figure 7.4, in which the real distributions were used for the indepen-\n",
      "dent components. The results are perhaps less realistic because the Laplacian distribution is less\n",
      "sparse than the real distributions.\n",
      "7.7.1 Likelihood as a measure of sparseness\n",
      "Let us assume, as we typically do, that the linear features considered are constrained\n",
      "to be uncorrelated and to have unit variance. This is equivalent to assuming that the\n",
      "transformation given by V is orthogonal in the canonically preprocessed (whitened)\n",
      "space. Thus, the matrix V is constrained orthogonal. It can be proven that the deter-\n",
      "minant of an orthogonal matrix is always equal to ±1. This is because an orthogonal\n",
      "transformation does not change distances and thus not volumes either; so the abso-\n",
      "lute value of the determinant which measures the change in volume must be equal\n",
      "to 1. Thus, the ﬁrst term on the right-hand-side of Eq. (7.15) is zero and can be\n",
      "omitted.\n",
      "The second term on the right-hand-side of Eq. (7.15) is the expectation (mul-\n",
      "tiplied by T) of a nonlinear function log pi of the output si of the feature detector\n",
      "(more precisely, an estimate of that expectation, since this is computed over the sam-\n",
      "ple). Thus, what the likelihood really boils down to is measuring the expectations of\n",
      "the form E{ f(si)} for some function f.\n",
      "The connection to maximization of sparseness is now evident. If the feature out-\n",
      "puts are constrained to have unit variance, maximization of the likelihood is equiv-\n",
      "alent to maximization of the sparsenesses of the outputs, if the functions log pi are\n",
      "of the form required for sparseness measurements, i.e. if we we can express them as\n",
      "log pi(s) = hi(s2)\n",
      "(7.16)\n",
      "\n",
      "172\n",
      "7 Independent component analysis\n",
      "where the functions hi are convex. In other words, the functions\n",
      "hi(u) = log pi(√u)\n",
      "(7.17)\n",
      "should be convex for u ≥0. It turns out that this is usually the case in natural images,\n",
      "as will be seen in the next section.\n",
      "Earlier, we considered using the negative of square root as hi. In the probabilistic\n",
      "interpretation given by ICA, using the square root means that the pdf of a component\n",
      "si is of the form\n",
      "p(si) = 1\n",
      "√\n",
      "2\n",
      "exp(−\n",
      "√\n",
      "2|si|)\n",
      "(7.18)\n",
      "where the constants have been computed so that si has unit variance, and the integral\n",
      "of the pdf is equal to one, as is always required. This distribution is called Laplacian.\n",
      "It is also sometimes called the double exponential distribution, since the absolute\n",
      "value of si has the classic exponential distribution (see Equation (4.71) on page 90).\n",
      "The Laplacian pdf was already illustrated in Figure 6.2 on page 139.\n",
      "As already pointed out in Chapter 6, using the Laplacian pdf maybe numeri-\n",
      "cally problematic because of the discontinuity of its derivative. Thus, one might use\n",
      "a smoother version, where the absolute value function is replaced by the logcosh\n",
      "function. This also corresponds to assuming a particular pdf for the independent\n",
      "components, usually called the “logistic” pdf. When properly normalized and stan-\n",
      "dardized to unit variance, the pdf has the form\n",
      "log pi(s) = −2logcosh( π\n",
      "2\n",
      "√\n",
      "3\n",
      "s)−4\n",
      "√\n",
      "3\n",
      "π\n",
      "(7.19)\n",
      "In practice, the constants here are often ignored, and simply the plain logcosh func-\n",
      "tion often is used to keep things simple.\n",
      "7.7.2 Optimal sparseness measures\n",
      "The maximum likelihood framework tells us what the nonlinearities used in the\n",
      "sparseness measure really should be. They should be chosen according to Equation\n",
      "(7.17). These nonlinearities can be estimated from natural images. To really ﬁnd\n",
      "the best nonlinearities, we could ﬁrst maximize the likelihood using some initial\n",
      "guesses of the hi, then estimate the pdf’s of the obtained independent components\n",
      "and recompute the hi according to Equation (7.17). In principle, we should then\n",
      "re-estimate the Wi using these new hi, re-estimate the hi using the latest Wi and so\n",
      "on until the process converges. This is because we are basically maximizing the\n",
      "likelihood with respect to two different groups of parameters (the Wi and the hi) and\n",
      "the real maximum can only be found if we go on maximizing one of the parameters\n",
      "groups with the other ﬁxed until no increase in likelihood can be obtained. However,\n",
      "\n",
      "7.7 Connection to maximization of sparseness\n",
      "173\n",
      "in practice we do not need to bother to re-iterate this process because the hi do not\n",
      "change that much after their initial estimation.\n",
      "Figure 7.6 shows two hi’s estimated from natural images with the corresponding\n",
      "log-pdf’s; most of them tend to be very similar. These are obtained by computing\n",
      "a histogram of distribution of two independent components estimated by a ﬁxed hi:\n",
      "the histogram gives an estimate of pi from which hi can be derived.\n",
      "We see that the estimated hi are convex, if we ignore the behaviour in the tails,\n",
      "which are impossible to estimate exactly because they contain so few data points.\n",
      "The hi estimated here are not very different from the square root function, although\n",
      "sometimes they tend to be more peaked.\n",
      "If we want to use such optimal nonlinearities in practice, we need to use para-\n",
      "metric probability models for the (log-)pdf’s. Using histogram estimates that we\n",
      "show here is not used in practice because such estimates can be very inexact and\n",
      "non-smooth. One well-known option for parameterized densities is the generalized\n",
      "gaussian density (sometimes also called the generalized Laplacian density):\n",
      "p(s) = 1\n",
      "c exp(−|si|α\n",
      "bα )\n",
      "(7.20)\n",
      "The parameters b and c are determined so that this is a pdf (i.e. its integral equals\n",
      "one) which has unit variance. The correct values are\n",
      "b =\n",
      "s\n",
      "Γ ( 1\n",
      "α )\n",
      "Γ ( 3\n",
      "α )\n",
      "and\n",
      "c = 2b√πΓ ( 1\n",
      "α )\n",
      "αΓ (1/2)\n",
      "(7.21)\n",
      "where Γ is the so-called “gamma” function which can be computed very fast in most\n",
      "software for scientiﬁc computation. The parameter α > 0 controls the sparseness\n",
      "of the density. If α = 2, we actually have the gaussian density, and for α = 1, the\n",
      "Laplacian density. What is most interesting when modelling images is that for α < 1,\n",
      "we have densities that are sparser than the Laplacian density, and closer to the highly\n",
      "sparse densities sometimes found in image features.\n",
      "Another choice is the following density:\n",
      "p(s) = 1\n",
      "2\n",
      "(α +2)[α (α +1)/2](α/2+1)\n",
      "[\n",
      "p\n",
      "α (α +1)/2+|s|](α+3)\n",
      "(7.22)\n",
      "with a sparseness parameter α. When α →∞, the Laplacian density is obtained as\n",
      "the limit. The strong sparsity of the densities given by this model can be seen e.g.,\n",
      "from the fact that the kurtosis of these densities is always larger than the kurtosis of\n",
      "the Laplacian density, and reaches inﬁnity for α ≤2. Similarly, p(0) reaches inﬁnity\n",
      "as α goes to zero.\n",
      "A problem with these highly peaked distributions is that they are not smooth, in\n",
      "particular their derivatives are discontinuous at zero. For the generalized gaussian\n",
      "distribution, the derivative is actually inﬁnite at zero for α < 1. Thus, to avoid prob-\n",
      "lems in the computational maximization of sparseness measures, it may not be a bad\n",
      "idea to use something more similar to a square root function in practical maximiza-\n",
      "\n",
      "174\n",
      "7 Independent component analysis\n",
      "−5\n",
      "0\n",
      "5\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "−5\n",
      "0\n",
      "5\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "Fig. 7.6: Estimated optimal hi from natural images. After doing ICA, the histograms of the compo-\n",
      "nent with the highest kurtosis and a component with kurtosis in the middle range were computed,\n",
      "and their logarithms taken. The feature corresponding to the highest kurtosis is on the left, and\n",
      "the one corresponding to the mid-range kurtosis is on the right. Top row: feature. Second row:\n",
      "logarithm of pdf. Third row: optimal hi. Bottom row: the derivative of log-pdf for future reference.\n",
      "tion of the sparseness measures. Actually, usually we use a smoothed version of the\n",
      "square root function as discussed in Section 7.7.1.\n",
      "The two density families in Equations (7.20) and (7.22) are illustrated in Fig-\n",
      "ure 7.7. While it does not seem necessary to use such more accurate density models\n",
      "in the estimation of the basis, they are likely to be quite useful in Bayesian inference\n",
      "where we really do need a good probabilistic model.\n",
      "\n",
      "7.8 Why are independent components sparse?\n",
      "175\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "2.5\n",
      "3\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "−8\n",
      "−6\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "2.5\n",
      "3\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "−8\n",
      "−6\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "Fig. 7.7: Top row: Some plots of the density function (left) and its logarithm (right) given in Eqs\n",
      "(7.20), α is given values 0.75, 1 and 1.5. More peaked ones correspond to smaller α. Bottom row:\n",
      "Plots of the density function (7.22), α is given values 0.5,2,10. More peaked values correspond to\n",
      "smaller α.\n",
      "7.8 Why are independent components sparse?\n",
      "There are many different ways in which random variables can be non-gaussian.\n",
      "What forms do there exist, and why is it that independent components in images are\n",
      "always sparse — or are they? These are the questions that we address in this section.\n",
      "7.8.1 Different forms of non-gaussianity\n",
      "While the forms of non-gaussianity are inﬁnite, most of the non-gaussian as-\n",
      "pects that are encountered in real data can be described as sub-gaussianity, super-\n",
      "gaussianity, or skewness.\n",
      "Super-gaussianity is basically the same as sparseness. Often, super-gaussianity is\n",
      "deﬁned as positive kurtosis (see Equation (6.5) for a deﬁnition of kurtosis), but other\n",
      "deﬁnitions exist as well. The intuitive idea is that the probability density function\n",
      "has heavy tails and a peak at zero.\n",
      "\n",
      "176\n",
      "7 Independent component analysis\n",
      "The opposite of super-gaussianity is sub-gaussianity, which is typically charac-\n",
      "terized by negative kurtosis. The density function is “ﬂat” around zero. A good\n",
      "example is the uniform distribution (here standardized to unit variance and zero\n",
      "mean)\n",
      "p(s) =\n",
      "(\n",
      "1\n",
      "2\n",
      "√\n",
      "3,\n",
      "if |s| ≤\n",
      "√\n",
      "3\n",
      "0,\n",
      "otherwise\n",
      "(7.23)\n",
      "The kurtosis of this distribution equals −6/5, which is left as an exercise.\n",
      "An unrelated form of non-gaussianity is skewness which basically means the lack\n",
      "of symmetry of the probability density function. A typical example is the exponen-\n",
      "tial distribution:\n",
      "p(s) =\n",
      "(\n",
      "exp(−s),\n",
      "if s ≥0\n",
      "0,\n",
      "otherwise\n",
      "(7.24)\n",
      "which is not symmetric with respect to any point on the horizontal (s) axis. Skewness\n",
      "is often measured by the third moment (assuming the mean is zero)\n",
      "skew(s) = E{s3}\n",
      "(7.25)\n",
      "This is zero for a symmetrically-distributed random variable that has zero mean\n",
      "(this is left as an exercise). In fact, skewness is usually deﬁned as exactly the third\n",
      "moment. However, any other nonlinear odd function could be used instead of the\n",
      "third power, for example the function that gives the sign (±1) of s.\n",
      "7.8.2 Non-gaussianity in natural images\n",
      "Is it true that all the independent components in natural images are sparse, and no\n",
      "other forms of non-gaussianity are encountered? This is almost true, but not quite.\n",
      "The skewness of the components is usually very small. After all, natural images\n",
      "tend to be rather symmetric in the sense that black and white are equally probable.\n",
      "This may not be exactly so, since such symmetry depends on the measurement scale\n",
      "of the grey-scale values: a non-linear change of measurement scale will make sym-\n",
      "metric data skewed. However, in practice, skewness seems to be so small that it can\n",
      "be ignored.\n",
      "There are some sub-gaussian components, though. In particular, the DC compo-\n",
      "nent, i.e. the mean luminance of the image patch, is typically sub-gaussian. In fact,\n",
      "its distribution is often not far from a uniform distribution. If we do not remove the\n",
      "DC component from the images (in contrast to what we usually do), and we use\n",
      "an ICA algorithm that is able to estimate sub-gaussian components as well (not all\n",
      "of them are), the DC component actually tends to be estimated as one independent\n",
      "component. Depending on the window size and the preprocessing used, a couple of\n",
      "further very low-frequency components can also be sub-gaussian.\n",
      "\n",
      "7.9 General ICA as maximization of non-gaussianity\n",
      "177\n",
      "7.8.3 Why is sparseness dominant?\n",
      "One reason why the independent components in images are mostly sparse is the\n",
      "variation of the local variance in different parts of an image. Some parts of the\n",
      "image have high variation whereas others have low variation. In fact, ﬂat surfaces\n",
      "have no variation, which is sometimes called the blue-sky effect.\n",
      "To model this change in local variance, let us model an independent component\n",
      "si as a product of an “original” independent component gi of unit variance, and an\n",
      "independent, non-negative “variance” variable di:\n",
      "si = gi di\n",
      "(7.26)\n",
      "We call di a variance variable because it changes the scale of each observation of gi.\n",
      "Such a variance variables will be the central topic in Chapter 9.\n",
      "Let us assume that the original component gi is gaussian with zero mean and unit\n",
      "variance. Then the distributions of the si is necessarily super-gaussian, i.e. it has\n",
      "positive kurtosis. This can be shown using fact that for a gaussian variable, kurtosis\n",
      "is zero and thus E{g4\n",
      "i } = 3, so we have\n",
      "kurt si = E{s4\n",
      "i } −3(E{s2\n",
      "i })2 = E{d4\n",
      "i g4\n",
      "i } −3(E{d2\n",
      "i g2\n",
      "i })2\n",
      "= E{d4\n",
      "i }E{g4\n",
      "i } −3(E{d2\n",
      "i })2(E{g2\n",
      "i })2 = 3[E{d4\n",
      "i } −(E{d2\n",
      "i })2]\n",
      "(7.27)\n",
      "which is always non-negative because it is the variance of d2\n",
      "i multiplied by 3. It can\n",
      "be zero only if di is constant.\n",
      "Thus, the changes in local variance are enough to transform the gaussian distri-\n",
      "bution of gi into a sparse distribution for si. The resulting distribution is called a\n",
      "gaussian scale mixture.\n",
      "7.9 General ICA as maximization of non-gaussianity\n",
      "Now we can consider the problem of ICA estimation in more generality, in the\n",
      "case where the components are not necessarily sparse. In particular, we consider the\n",
      "following two questions: Does estimation of ICA for non-sparse components have\n",
      "a simple intuitive interpretation, and: Is there a deeper reason why maximization\n",
      "of sparseness is related to the estimation of the ICA model. These questions can\n",
      "be answered based on the Central Limit Theorem, a most fundamental theorem in\n",
      "probability theory. Here we explain this connection and show how it leads to a more\n",
      "general connection between independence and non-gaussianity.\n",
      "\n",
      "178\n",
      "7 Independent component analysis\n",
      "7.9.1 Central Limit Theorem\n",
      "The Central Limit Theorem (CLT) basically says that when you take an average or\n",
      "sum of many independent random variables, it will have a distribution that is close\n",
      "to gaussian. In the limit of an inﬁnite number of random variables, the distribution\n",
      "actually tends to the gaussian distribution, if properly normalized:\n",
      "lim\n",
      "N→∞\n",
      "1\n",
      "√\n",
      "N\n",
      "N\n",
      "∑\n",
      "n=1\n",
      "sn = gaussian\n",
      "(7.28)\n",
      "where we assume that sn have zero mean. We have to normalize the sum here be-\n",
      "cause otherwise the variance of the sum would go to inﬁnity. Note that if we nor-\n",
      "malized by 1/N, the variance would go to zero.\n",
      "Some technical restrictions are necessary for this results to hold exactly. The\n",
      "simplest choice is to assume that the sn all have the same distribution, and that\n",
      "distribution has ﬁnite moments. The CLT is illustrated in Figure 7.8.\n",
      "7.9.2 “Non-gaussian is independent”\n",
      "What does the CLT mean in the context of ICA? Let us consider a linear combi-\n",
      "nation of the observed variables, ∑i wizi. This is also a linear combination of the\n",
      "original independent components:\n",
      "∑\n",
      "i\n",
      "wizi = ∑\n",
      "i\n",
      "wi∑\n",
      "j\n",
      "aijsj = ∑\n",
      "j\n",
      "(∑\n",
      "i\n",
      "wiaij)sj = ∑\n",
      "j\n",
      "q jsj\n",
      "(7.29)\n",
      "where we have denoted q j = ∑i wiaij. We do not know the coefﬁcients q j because\n",
      "they depend on the aij.\n",
      "The CLT would suggest that this linear combination ∑j q jsj is closer to gaussian\n",
      "than the original independent components sj. This is not exactly true because the\n",
      "CLT is exactly true only in the limit of an inﬁnite number of independent compo-\n",
      "nents, and there are restrictions on the distributions (for example, the variables q jsj\n",
      "do not have identical distributions if the q j are not equal). However, the basic idea\n",
      "is correct. This is illustrated in Figure 7.9 which shows that the original indepen-\n",
      "dent components are more gaussian than the observed data after whitening, shown\n",
      "in Figure 7.1.\n",
      "Thus, based on the central limit theorem, we can intuitively motivate a general\n",
      "principle for ICA estimation: ﬁnd linear combinations ∑i wizi of the observed vari-\n",
      "ables that are maximally non-gaussian.\n",
      "Why would this work? The linear combination ∑i wizi equals a linear combina-\n",
      "tion of the independent components with some coefﬁcients q j. Now, if more than\n",
      "one of the q j is non-zero, we have a sum of two independent random variables. Be-\n",
      "cause of the CLT, we can expect that such a sum is closer to gaussian that any of the\n",
      "\n",
      "7.9 General ICA as maximization of non-gaussianity\n",
      "179\n",
      "a)\n",
      "b)\n",
      "c)\n",
      "Fig. 7.8: a) Histogram of a very sparse distribution. b) Histogram of a sum of two independent\n",
      "random variables distributed as in a), normalized by dividing by\n",
      "√\n",
      "2. c) Histogram of a normalized\n",
      "sum of ten variables with the same distribution as in a). The scale of the axes are the same in all\n",
      "plots. We see that the distribution goes towards gaussianity.\n",
      "a)\n",
      "b)\n",
      "Fig. 7.9: a) Histogram of one of the original components in Figure 7.1. b) Histogram of one of\n",
      "the whitened components in Figure 7.1. The whitened component has a distribution which is less\n",
      "sparse, thus closer to gaussian.\n",
      "original variables. (This is really only an intuitive justiﬁcation and not exactly true.)\n",
      "Thus, the non-gaussianity of such a linear combination is maximal when it equals\n",
      "one of the original independent components, and the maximally non-gaussian linear\n",
      "combinations are the independent components.\n",
      "Here, we have to emphasize that this connection between non-gaussianity and\n",
      "independence only holds for linear transformations. In Chapter 9 we will see that\n",
      "for nonlinear transformations, such a connection need not exist at all, and may in\n",
      "fact be reversed.\n",
      "7.9.3 Sparse coding as a special case of ICA\n",
      "Estimation of ICA by maximization of sparseness can now be seen as a special case\n",
      "of maximization of non-gaussianity. Sparseness is one form of non-gaussianity, the\n",
      "\n",
      "180\n",
      "7 Independent component analysis\n",
      "one that is dominant in natural images. Thus, in natural images, maximization of\n",
      "non-gaussianity is basically the same as maximization of sparseness. For other types\n",
      "of data, maximization of non-gaussianity may be quite different from maximization\n",
      "of sparseness.\n",
      "For example, in the theory of ICA, it has been proposed that the non-gaussianity\n",
      "of the components could be measured by the sum of the squares of the kurtoses:\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "[kurt(vT\n",
      "i z)]2\n",
      "(7.30)\n",
      "where, as usual, the data vector z is whitened and the feature vectors vi are con-\n",
      "strained to be orthogonal and to have unit norm. It can be shown that ICA estimation\n",
      "can really be accomplished by maximizing this objective function. This works for\n",
      "both sub-gaussian and super-gaussian independent components.\n",
      "Now, if the components all have positive kurtoses, maximizing this sum is closely\n",
      "related to ﬁnding vectors vi such that the vT\n",
      "i z are maximally non-gaussian. The\n",
      "square of kurtosis is, however, a more general measure of non-gaussianity because\n",
      "there are cases where the kurtosis is negative as we saw above in Section 7.8.1. For\n",
      "such components, maximization of non-gaussianity means minimizing kurtosis (and\n",
      "sparseness), because for negative values of kurtosis, maximization of the square\n",
      "means to minimize kurtosis.\n",
      "In fact, maximization of sparseness may not always be the correct method for\n",
      "estimation ICA even on images. If we do not remove the DC component from the\n",
      "images, the DC component turns out to be one independent component, and it some-\n",
      "times has negative kurtosis. For such data, simply maximizing sparseness of all the\n",
      "components will produce misleading results.\n",
      "Thus, we see that there is a difference between basic linear sparse coding and\n",
      "ICA in the sense that ICA works for different kinds of non-gaussianity and not just\n",
      "sparseness.\n",
      "7.10 Receptive ﬁelds vs. feature vectors\n",
      "An important point to note is the relation between the feature vectors Ai and the\n",
      "feature detector weights Wi. The feature vectors are shown Fig. 7.3. However, it is\n",
      "often the Wi that are more interesting, since they are the weights that are applied to\n",
      "the image to actually compute the si, and in neurophysiological modelling, they are\n",
      "more closely connected to the receptive ﬁelds of neurons.\n",
      "There is, in fact, a simple connection between the two: the Ai are basically low-\n",
      "pass ﬁltered versions of the Wi. In fact, simple calculations show that the covariance\n",
      "cov(I(x,y),I(x′,y′)) in images generated according to the ICA model equals\n",
      "∑\n",
      "i\n",
      "Ai(x,y)Ai(x′,y′)\n",
      "(7.31)\n",
      "\n",
      "7.11 Problem of inversion of preprocessing\n",
      "181\n",
      "because the si are uncorrelated and have unit variance. Thus we have\n",
      "∑\n",
      "x′,y′\n",
      "cov(I(x,y),I(x′,y′))Wi(x′,y′) = ∑\n",
      "x′,y′∑\n",
      "i\n",
      "Ai(x,y)Ai(x′,y′)Wi(x′,y′)\n",
      "= ∑\n",
      "i\n",
      "Ai(x,y) ∑\n",
      "x′,y′\n",
      "Ai(x′,y′)Wi(x′,y′) = Ai(x,y)\n",
      "(7.32)\n",
      "by deﬁnition of theWi as the inverse of the Ai. This means that the Ai can be obtained\n",
      "by multiplying the Wi by the covariance matrix of the data.\n",
      "Such multiplication by the covariance matrix has a simple interpretation as a low-\n",
      "pass ﬁltering operation. This is because the covariances are basically a decreasing\n",
      "function of the distance between (x,y) and (x′,y′), as shown in Figure 5.4. Thus, Ai\n",
      "and Wi have essentially the same orientation, location and frequency tuning proper-\n",
      "ties. On the other hand, the Ai are better to visualize because because they actually\n",
      "correspond to parts of the image data; especially with data that is not purely spatial,\n",
      "as in Chapter 15, visualization of the detector weights would not be straightforward.\n",
      "7.11 Problem of inversion of preprocessing\n",
      "A technical point that we have to consider is computation of the Ai for original\n",
      "images based on ICA of canonically preprocessed data. There is actually a problem\n",
      "here: in order to get the Ai we have to invert the canonical preprocessing, because\n",
      "estimation of the model gives the vectors vi in the reduced (preprocessed) space\n",
      "only. But canonical preprocessing is not invertible in the strict sense of the word,\n",
      "because it reduces the dimension and therefore loses information!\n",
      "Typically, a solution based on the idea of computing the best possible approxima-\n",
      "tion of the inverse of the PCA/whitening transformation. Such best approximation\n",
      "can be obtained by the theory of multivariate regression, or, alternatively, by the the-\n",
      "ory of pseudo-inverses (see Section 19.8). Without going into details, the description\n",
      "of the solution is simple.\n",
      "Denote by U the orthogonal matrix which contains the n vectors giving the direc-\n",
      "tions of the principal components as its rows, i.e. the n dominant eigenvectors of the\n",
      "covariance matrix. Denote by λi the corresponding eigenvalues. Then, steps 3 and\n",
      "4 of canonical preprocessing in Section 5.4 consist of multiplying the vectorized\n",
      "image patches by diag(1/√λi)U.\n",
      "We now deﬁne the inverse preprocessing as follows: After computing the fea-\n",
      "ture vectors in the preprocessed space (the vi), the basis vectors are multiplied by\n",
      "UTdiag(\n",
      "√\n",
      "λi). These are the optimal approximations of the feature vectors in the\n",
      "original space. They can also be computed by taking the pseudoinverse of the ma-\n",
      "trix of the features Wi, which is what we did in our simulations.\n",
      "Note that we have no such problem with computation of the Wi for the original\n",
      "data because we just multiply the vectors vi with the PCA/whitening matrix, and no\n",
      "inversion is needed.\n",
      "\n",
      "182\n",
      "7 Independent component analysis\n",
      "7.12 Frequency channels and ICA\n",
      "A long-standing tradition in vision science is to talk about “frequency channels”,\n",
      "and more rarely, about “orientation” channels. The idea is that in the early visual\n",
      "processing (something like V1), information of different frequencies is processed\n",
      "independently. The word “independence” as used here has nothing to do with sta-\n",
      "tistical independence: it means that processing happens in different physiological\n",
      "systems that are more or less anatomically separate, and do not exchange informa-\n",
      "tion with each other.\n",
      "Justiﬁcation for talking about different channels is abundant in research on V1. In\n",
      "recordings from single cells, simple and complex cell receptive ﬁelds are band-pass,\n",
      "i.e. respond only to stimuli in a certain frequency range, and various optimal fre-\n",
      "quencies are found in the cells (see Chapter 3 and its references). In psychophysics,\n",
      "a number of experiments also point to such a division of early processing. For exam-\n",
      "ple, in Figure 3.8 on page 60, the information on the high- and low-frequency parts\n",
      "are quite different, yet observes have no difﬁculty in processing (reading) them sep-\n",
      "arately.\n",
      "In the results of ICA on natural images, we see an interesting new interpretation\n",
      "of why the frequency channels might be independent in terms of physiological and\n",
      "anatomical separation in the brain. The reason is that the information in different\n",
      "frequency channels seems to be statistically independent, as measured by ICA. The\n",
      "feature vectors Ai given by ICA are band-pass, thus showing that a decomposition\n",
      "into statistically independent features automatically leads to frequency channels.\n",
      "7.13 Concluding remarks and References\n",
      "Independent component analysis is a statistical generative model whose estimation\n",
      "boils down to sparse coding. It gives a proper probabilistic formulation of sparse\n",
      "coding and thereby solves a number of theoretical problems in sparse coding (in\n",
      "particular: optimal ways of measuring sparseness, optimality of decorrelation), and\n",
      "gives a proper pdf to be used in Bayesian inference. The expression “independent\n",
      "component analysis” also points out another important property of this model: the\n",
      "components are considered statistically independent. This independence assumption\n",
      "is challenged in many more recent models which are the topics of Chapters 9–11.\n",
      "The ﬁrst application of ICA, as opposed to sparse coding, on image patches was\n",
      "in (Bell and Sejnowski, 1997) based on the earlier sparse coding framework in (Ol-\n",
      "shausen and Field, 1996) considered in Section 13.1.\n",
      "For more information on the ICA model, see (Hyv¨arinen and Oja, 2000) or\n",
      "(Hyv¨arinen et al, 2001b). Some of the earliest historical references on ICA include\n",
      "(H´erault and Ans, 1984; Mooijaart, 1985; Cardoso, 1989; Jutten and H´erault, 1991).\n",
      "Classic references include (Delfosse and Loubaton, 1995), which showed explicitly\n",
      "how maximization of non-gaussianity is related to ICA estimation; (Comon, 1994),\n",
      "which showed the uniqueness of the decomposition, and the validity of the sum-\n",
      "\n",
      "7.14 Exercices\n",
      "183\n",
      "of-squares-of-kurtosis in Equation (7.30); the maximum likelihood framework was\n",
      "introduced in (Pham et al, 1992; Pham and Garrat, 1997).\n",
      "For more information on the central limit theorem, see any standard textbook on\n",
      "probability theory, for example (Papoulis and Pillai, 2001).\n",
      "A related way of analyzing the statistics of natural images is to look at the cu-\n",
      "mulant tensors (Thomson, 1999, 2001). Cumulants (strictly speaking, higher-order\n",
      "cumulants) are statistics which can be used as measures of non-gaussianity; for ex-\n",
      "ample, kurtosis and skewness are one of the simplest cumulants. Cumulant tensors\n",
      "are generalizations of the covariance matrix to higher-order cumulants. Analysis of\n",
      "cumulant tensors is closely related to ICA, as discussed in detail in Chapter 11 of\n",
      "(Hyv¨arinen et al, 2001b). Equivalently, one can analyze the polyspectra (usually the\n",
      "trispectrum) which are obtained by Fourier transformations of the cumulant spec-\n",
      "tra, in a similar way as the ordinary Fourier spectrum function can be obtained as\n",
      "the Fourier transform of the autocovariance function; see e.g. (Nikias and Mendel,\n",
      "1993; Nikias and Petropulu, 1993) for further information.\n",
      "7.14 Exercices\n",
      "Mathematical exercises\n",
      "1. Prove (7.2).\n",
      "2. Based on (7.2), prove that two independent random variables are uncorrelated.\n",
      "3. Calculate the kurtosis of the uniform distribution in (7.23).\n",
      "4. Calculate the kurtosis of the Laplacian distribution in (7.18).\n",
      "5. Show that the skewness of a random variable with a pdf which is even-symmetric\n",
      "(i.e. p(−x) = p(x)) is zero.\n",
      "6. In this exercise we consider a very simple case of gaussian mixtures (see Sec-\n",
      "tion 7.8.3). Assume that a component follows s = vz where z is gaussian with\n",
      "zero mean and unit variance. Let us assume that in 50% of the natural images the\n",
      "variance coefﬁcient v has value α. In the remaining 50% of natural images, v has\n",
      "value β.\n",
      "a. What is the distribution of the random variable s in the set of all natural im-\n",
      "ages? (Give the density function p(s))\n",
      "b. Show that E{s2} = 1\n",
      "2(α2 +β 2)\n",
      "c. Show that E{s4} = 3\n",
      "2(α4 +β 4)\n",
      "d. What is the kurtosis of this distribution?\n",
      "e. Show that the kurtosis is positive for almost any parameter values.\n",
      "7. Prove (7.31).\n",
      "\n",
      "184\n",
      "7 Independent component analysis\n",
      "Computer assignments\n",
      "1. Using numerical integration, compute the kurtoses of the Laplacian and uniform\n",
      "distributions.\n",
      "2. Load the FastICA program from the web. This will be used in the following\n",
      "assignments. You will also need some images; try to ﬁnd some that have not\n",
      "been compressed, since compression can induce very nasty effects.\n",
      "3. Take patches of 16 × 16 of the image. 10,000 is a sufﬁcient amount of patches.\n",
      "Input these to FastICA.\n",
      "4. Plot histograms of the independent components. Compute their kurtoses. Plot\n",
      "(some of the) RF’s. Look at the RF’s and comment on whether they look like V1\n",
      "RF’s or not. If not, why not?\n",
      "\n",
      "Chapter 8\n",
      "Information-theoretic interpretations\n",
      "So far, we have been operating within the theoretical framework of Bayesian in-\n",
      "ference: the goal of our models is to provide priors for Bayesian inference. An al-\n",
      "ternative framework is provided by information theory. In information theory, the\n",
      "goal is to ﬁnd ways of coding the information as efﬁciently as possible. This turns\n",
      "out to be surprisingly closely connected to Bayesian inference. In many cases, both\n",
      "approaches start by estimation of parameters in a parameterized statistical model.\n",
      "This chapter is different from the previous ones because we do not provide any\n",
      "new models for natural image statistics. Rather, we describe a new framework for\n",
      "interpreting some of the previous models.\n",
      "8.1 Basic motivation for information theory\n",
      "In this section, we introduce the basic ideas of information theory using intuitive ex-\n",
      "amples illustrating the two principal motivations: data compression and data trans-\n",
      "mission.\n",
      "8.1.1 Compression\n",
      "One of the principal motivations of information theory is data compression. Let us\n",
      "begin with a simple example. Consider the following string of characters:\n",
      "BABABABADABACAABAACABDAAAAABAAAAAAAADBCA\n",
      "We need to code this using a binary string, consisting of zeros and ones, because\n",
      "that’s the form used in computer memory. Since we have four different characters,\n",
      "a basic approach would be to assign the four possible two-digit codewords for each\n",
      "of them:\n",
      "185\n",
      "\n",
      "186\n",
      "8 Information-theoretic interpretations\n",
      "A →00\n",
      "(8.1)\n",
      "B →01\n",
      "(8.2)\n",
      "C →10\n",
      "(8.3)\n",
      "D →11\n",
      "(8.4)\n",
      "(8.5)\n",
      "Replacing each of the characters by its two-digit codeword gives the code\n",
      "0100010001000100110001001000000100001000\n",
      "0111000000000001000000000000000011011000\n",
      "However, a shorter code can be obtained by using the fundamental insight of\n",
      "information theory: frequent characters should be given shorter codewords. This\n",
      "makes intuitive sense: if frequent characters have short codewords, even at the ex-\n",
      "pense of giving less frequent characters longer codewords, the average length could\n",
      "be shortened.\n",
      "In this example, the character A is the most frequent one: approximately one half\n",
      "of the letters are A’s. The letter B is the next, with a proportion of approximately\n",
      "one quarter. So, let us consider the following kind of codeword assignment:\n",
      "A →0\n",
      "(8.6)\n",
      "B →10\n",
      "(8.7)\n",
      "C →110\n",
      "(8.8)\n",
      "D →111\n",
      "(8.9)\n",
      "(8.10)\n",
      "Now, the code becomes\n",
      "1001001001001110100110001000110010111000\n",
      "001000000000111101100\n",
      "This is more than 10% shorter than the basic code given above. (In real-world appli-\n",
      "cations, the saving is often much larger, sometimes reaching more than 90% in im-\n",
      "age compression.) Note that the codeword assignment has been cleverly constructed\n",
      "so that the original string can be recovered from this code without any ambiguity.\n",
      "Compression was possible because some of the characters were more common\n",
      "than others. In other words, it was due to the statistical regularities of the data (or\n",
      "redundancy, which will be deﬁned later). Thus, it is not surprising that the methods\n",
      "developed in information theory have also been applied in the ﬁeld of natural image\n",
      "statistics.\n",
      "\n",
      "8.2 Entropy as a measure of uncertainty\n",
      "187\n",
      "8.1.2 Transmission\n",
      "A rather different application of information theory is in data transmission. In trans-\n",
      "mission, the central problem is noise. That is, the transmission will introduce ran-\n",
      "dom errors in the data. The goal here is to code the data so that the receiving end of\n",
      "the system can correct as many of the random errors as possible.\n",
      "As a simple example, consider the following binary string which we want to\n",
      "transmit:\n",
      "1111101100100011100110100101110110100101\n",
      "To code this string we use a very simple method: we simply repeat all the digits\n",
      "three times. Thus we get the code\n",
      "1111111111111110001111110000001110000000\n",
      "0011111111100000011111100011100000011100\n",
      "0111111111000111111000111000000111000111\n",
      "This is transmitted through a channel which has relatively strong noise: 25% of the\n",
      "digits are randomly changed to the opposite. So, the receiving end in the channel\n",
      "receives the following string:\n",
      "1111100111111110000111110000001110100001\n",
      "1000110100100100111010101011100001010100\n",
      "1111111111000110101000111010010111100111\n",
      "Now, we can use the repetitions in the string in the following way: we look at all\n",
      "consecutive groups of three digits, and guess that the original string probably had\n",
      "the digit which has the most occurrences among each group. Thus, we obtain the\n",
      "following string\n",
      "1111101100100101000110100101110110100101\n",
      "This string has less than 10% wrong digits. Thus, our encoding scheme reduced the\n",
      "number of errors introduced by the channel from 25% to less than 10%. (Actually,\n",
      "we were a bit lucky with this string: on the average the errors would be of the order\n",
      "of 16%.)\n",
      "So, the central idea in transmission is very different from compression. In order\n",
      "to combat noise, we want to code the data so that we introduce redundancy (to be\n",
      "deﬁned later), which often means making the code longer than the original data.\n",
      "8.2 Entropy as a measure of uncertainty\n",
      "Now, we introduce the concept of entropy, which is the foundation of information\n",
      "theory. First, we give its deﬁnition and a couple of examples, and then show how it\n",
      "is related to data compression.\n",
      "\n",
      "188\n",
      "8 Information-theoretic interpretations\n",
      "8.2.1 Deﬁnition of entropy\n",
      "Consider a random variable z which takes values in a discrete set a1,...,aN with\n",
      "probabilities P(z = ai),i = 1,...,N. The most fundamental concept of information\n",
      "theory is entropy, denoted by H(z), which is deﬁned as\n",
      "H(z) = −\n",
      "N\n",
      "∑\n",
      "i=1\n",
      "P(z = ai)log2 P(z = ai)\n",
      "(8.11)\n",
      "If the logarithms to base 2 are used, the unit of entropy is called a bit. Entropy is a\n",
      "measure of the average uncertainty in a random variable. It is always non-negative.\n",
      "We will next present a couple of examples to illustrate the basic idea.\n",
      "Example 1\n",
      "Consider a random variable which takes only two values, A and B.\n",
      "Denote the probability P(z = A) by p0. Then, entropy of z equals\n",
      "H(z) = −p0 log2 p0 −(1 −p0)log2(1 −p0)\n",
      "(8.12)\n",
      "We can plot this as a function of p0, which is shown in Fig. 8.1 The plot shows that\n",
      "the maximum entropy is obtained when p0 = 0.5, which means that both outcomes\n",
      "are equally probable. Then the entropy equals one bit. In contrast, if p0 equals 0 or\n",
      "1, there is no uncertainty at all in the random variable, and its entropy equals zero.\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "Fig. 8.1: Entropy of a random variable which can only take two values, plotted as a function of the\n",
      "probability p0 of taking one of those values.\n",
      "Example 2 Consider a random variable z which takes, for some given n, any of 2n\n",
      "values with equal probability, which is obviously 1/2n. The entropy equals\n",
      "\n",
      "8.2 Entropy as a measure of uncertainty\n",
      "189\n",
      "H(z) = −\n",
      "2n\n",
      "∑\n",
      "i=1\n",
      "1\n",
      "2n log2\n",
      "1\n",
      "2n = −2n 1\n",
      "2n (−n) = n\n",
      "(8.13)\n",
      "Thus, the entropy equals n bits. This is also the number of binary digits (also called\n",
      "bits) that you would need to represent the random variable in a basic binary repre-\n",
      "sentation.\n",
      "Example 3\n",
      "Consider a random variable z which always takes the same value, i.e.\n",
      "it is not really random at all. Its entropy equals\n",
      "H(z) = −1 ×log2 1 = 0\n",
      "(8.14)\n",
      "Again, we see that the entropy is zero since there is no uncertainty at all.\n",
      "8.2.2 Entropy as minimum coding length\n",
      "An important result in information theory shows that the numerical value of entropy\n",
      "directly gives the average code length for the shortest possible code (i.e. the one\n",
      "giving maximum compression). It is no coincidence that the unit of entropy is called\n",
      "a bit, since the length of the code is also given in bits, in the sense of the number of\n",
      "zero-one digits used in the code.\n",
      "The proof of this property is very deep, so we will only try to illustrate it with an\n",
      "example.\n",
      "Example 4 Now we will return back to the compression example in the preceding\n",
      "section to show how entropy is connected to compression. Consider the characters\n",
      "in the string as independent realizations of a random variable which takes values in\n",
      "the set {A,B,C,D}. The probabilities used in generating this data are\n",
      "P(A) = 1/2\n",
      "(8.15)\n",
      "P(B) = 1/4\n",
      "(8.16)\n",
      "P(C) = 1/8\n",
      "(8.17)\n",
      "P(D) = 1/8\n",
      "(8.18)\n",
      "We can compute the entropy, which equals 1.75 bits. Thus, the entropy is smaller\n",
      "than 2 bits, which would be (according to Example 2), the maximum entropy for\n",
      "a variable with four possible values. Using the deﬁnition of entropy as minimum\n",
      "coding length, we see that the saving in code length can be at most (2-1.75)/2 =\n",
      "12.5 % for data with these characteristics. (This holds in the case of inﬁnite strings\n",
      "in which random effects are averaged out. Of course, for a ﬁnite-length string, the\n",
      "code would be a bit shorter or longer due to random effects.)\n",
      "Note also it is essential here that the characters in the string are generated inde-\n",
      "pendently at each location; otherwise, the code length might be shorter. For example,\n",
      "\n",
      "190\n",
      "8 Information-theoretic interpretations\n",
      "if the characters would be generated in pairs, as in AACCBBAA..., this dependency\n",
      "could obviously be used to reduce the code length, possibly beyond the bound that\n",
      "entropy gives.\n",
      "8.2.3 Redundancy\n",
      "Redundancy is a word which is widely used in information theory, as well as in nat-\n",
      "ural image statistics. It is generally used to refer to the statistical regularities, which\n",
      "make part of the information “redundant”, or unnecessary. Unfortunately,when talk-\n",
      "ing about natural images, different authors use the word in slightly different ways.\n",
      "An information-theoretic deﬁnition of redundancy is based on entropy. Given a\n",
      "random variable which has 2n different values, say 1,...,2n, we compare the length\n",
      "of the basic n-bit code and the length of the shortest code, given by entropy:\n",
      "redundancy = n −H(z)\n",
      "(8.19)\n",
      "This is zero only if all the values 1,...,2n are equally likely. Using this terminology,\n",
      "we can say that compression of the string in Section 8.1.1 was possible because the\n",
      "string had some redundancy (n was larger than H(z)). Compression is possible by\n",
      "removing, or at least reducing, redundancy in the data.\n",
      "This deﬁnition is actually more general than it seems, because it can also consider\n",
      "dependencies between different variables (say, pixels). If we have two variables z1\n",
      "and z2, we can simply deﬁne a new variable z whose possible values correspond to\n",
      "all the possible combinations1 of the values of z1 and z2. Then, we can deﬁne entropy\n",
      "and redundancy for this new variable. If the variables are highly dependent from\n",
      "each other, some combinations will have very small probabilities, so the entropy\n",
      "will be small and redundancy large.\n",
      "It is important to understand that you may want to either reduce or increase re-\n",
      "dundancy, depending on your purpose. In compression you want to reduce it, but\n",
      "in information transmission, you actually want to increase it. The situation is even\n",
      "more complicated than this because usually before transmission, you want to com-\n",
      "press the data in order to reduce the time needed for data transmission. So, you ﬁrst\n",
      "try to remove the redundancy to decrease the length of the data to be transmitted,\n",
      "and then introduce new redundancy to combat noise. This need not be contradictory\n",
      "because in introducing new redundancy, you do it is a controlled way using care-\n",
      "fully designed codes which increase code length as little as possible for a required\n",
      "level of noise-resistance. A result called “source-channel coding theorem” lays out\n",
      "the conditions under which such a two-stage procedure is, in fact, optimal.\n",
      "1 More precisely: if z1 can take values in the set {1,2} and z2 can take values in the\n",
      "set {1,2,3}, we deﬁne z so that it takes values in the Cartesian product of those sets, i.e.\n",
      "{(1,1),(1,2),(1,3),(2,1),(2,2),(2,3)}, so that the probability z = (a1,a2) simply equals the prob-\n",
      "ability that z1 = a1 and z2 = a2\n",
      "\n",
      "8.2 Entropy as a measure of uncertainty\n",
      "191\n",
      "8.2.4 Differential entropy\n",
      "The extension of entropy to continuous-valued random variables or random vec-\n",
      "tors is algebraically straightforward. For a random variable with probability density\n",
      "function pz we deﬁne the differential entropy, denoted by H just like entropy, as\n",
      "follows:\n",
      "H(z) = −\n",
      "Z\n",
      "pz(z)log pz(z)dz\n",
      "(8.20)\n",
      "So, basically we have just replaced the summation in the original deﬁnition in Equa-\n",
      "tion (8.11) by an integral. The same deﬁnition also applies in the case of a random\n",
      "vector.\n",
      "It is not difﬁcult to see what kind of random variables have small differential\n",
      "entropies. They are the ones whose probability densities take large values, since\n",
      "these give strong negative contributions to the integral in Eq. (8.20). This means\n",
      "that certain small intervals are quite probable. Thus we again see that entropy is\n",
      "small when the variable is not very random, that is, it is typically contained in some\n",
      "limited intervals with high probabilities.\n",
      "Differential entropy is related to the shortest code length of a discretized version\n",
      "of the random variable z. Suppose we discretize z so that we divide the real line\n",
      "to bins (intervals) of length d, and deﬁne a new discrete-valued random variable ˜z\n",
      "which tells which of these bins the value of z belongs to. This is similar to using\n",
      "a limited number of decimals in the representation, for example using only one\n",
      "decimal place as in “1.4” to represent the values of the random variable. Then, the\n",
      "entropy of ˜z is approximately equal to the differential entropy of z plus a constant\n",
      "which only depends on the size of the bins, d.\n",
      "Example 5\n",
      "Consider a random variable z which follows a uniform distribution in\n",
      "the interval [0,a]: Its density is given by\n",
      "pz(z) =\n",
      "(\n",
      "1/a,\n",
      "for 0 ≤z ≤a\n",
      "0,\n",
      "otherwise\n",
      "(8.21)\n",
      "Differential entropy can be evaluated as\n",
      "H(z) = −\n",
      "Z a\n",
      "0\n",
      "1\n",
      "a log 1\n",
      "adz = loga\n",
      "(8.22)\n",
      "We see that the entropy is large if a is large, and small if a is small. This is natural\n",
      "because the smaller a is, the less randomness there is in z. Actually, in the limit\n",
      "where a goes to 0, differential entropy goes to −∞, because in the limit, z is no longer\n",
      "random at all: it is always 0. This example also shows that differential entropy, in\n",
      "contrast to basic entropy, need not be non-negative.\n",
      "\n",
      "192\n",
      "8 Information-theoretic interpretations\n",
      "8.2.5 Maximum entropy\n",
      "An interesting question to ask is: What kind of distributions have maximum en-\n",
      "tropy?\n",
      "In the binary case in Example 1, we already saw that it was the distribution\n",
      "with 50%-50% probabilities which is clearly consistent with the intuitive idea of\n",
      "maximum uncertainty. In the general discrete-valued case, it can be shown that the\n",
      "uniform distribution (probabilities of all possible values are equal) has maximum\n",
      "entropy.\n",
      "With continuous-valued variables, the situation is more complicated. Differential\n",
      "entropy can become inﬁnite; consider, for example when a →∞in Example 5 above.\n",
      "So, some kind of constraints are needed. The simplest constraint would perhaps be\n",
      "to constrain the random variable to take values only inside a ﬁnite interval [a,b]. In\n",
      "this case, the distribution of maximum entropy is, again, the uniform distribution,\n",
      "i.e. a pdf which equals\n",
      "1\n",
      "b−a in the whole interval and is zero outside of it. However,\n",
      "such a constraint may not be very relevant in most applications.\n",
      "If we consider random variables whose variance is constrained to a given value\n",
      "(e.g. to 1), the distribution of maximum entropy is, interestingly, the gaussian distri-\n",
      "bution. This is why the gaussian distribution can be considered the least “informa-\n",
      "tive”, or the least “structured”, of continuous-valued distributions.\n",
      "This also means that differential entropy can be considered a measure of non-\n",
      "gaussianity. The smaller the differential entropy, the further away the distribution\n",
      "is from the gaussian distribution. However, it must be noted that differential en-\n",
      "tropy depends on the scale as well. Thus, if entropy is used as a measure of non-\n",
      "gaussianity, the variables have to normalized to unit variance ﬁrst, just like in the\n",
      "case of kurtosis. We will see in Section 8.4 how differential entropy is, in fact,\n",
      "closely related to the sparseness measures we used in Chapter 6.\n",
      "8.3 Mutual information\n",
      "In information transmission, we need a measure of how much information the output\n",
      "of the channel contains about the input. This the purpose of the concept of mutual\n",
      "information.\n",
      "Let’s start with the concept of conditional entropy. It is simply the average en-\n",
      "tropy calculated for the conditional distribution of the variable z, where the condi-\n",
      "tioning is by the observation of another variable y:\n",
      "H(z|y) = −∑\n",
      "y\n",
      "P(y)∑\n",
      "z\n",
      "P(z|y)logP(z|y)\n",
      "(8.23)\n",
      "That is, it measures how much entropy there is left in z when we know (observe) the\n",
      "value of y; this is averaged over all values of y.\n",
      "\n",
      "8.3 Mutual information\n",
      "193\n",
      "If z and y are independent, the conditional distribution of z given y is the same\n",
      "as the distribution of z alone, so conditional entropy is the same as the entropy of\n",
      "z. If z and y are not independent, conditional entropy is smaller than the entropy\n",
      "of z, because then knowledge of the value of y reduces the uncertainty on z. In\n",
      "the extreme case where z = y, the conditional distribution z given y is such that all\n",
      "the probability is concentrated on the observed value of y. The entropy of such a\n",
      "distribution is zero (see Example 3 above), so the conditional entropy is zero.\n",
      "Let us assume that z is the message input to a transmission channel and y is the\n",
      "output, i.e. the received signal. Basically, if transmission is very good, knowledge\n",
      "of y will tell us very much about what z was. In other words, the conditional distri-\n",
      "bution of z given y is highly concentrated on some values. So, we could measure the\n",
      "transmitted information by the change in entropy which is due to measurement of y.\n",
      "It is called the mutual information, which we denote2 by J:\n",
      "J(z,y) = H(z)−H(z|y)\n",
      "(8.24)\n",
      "Just as entropy gives the code length of the optimal code, mutual information is\n",
      "related to the amount of information which can be obtained about z, based on obser-\n",
      "vation of the channel output y.\n",
      "Note that, in practice, mutual information depends not only on the noise in the\n",
      "channel, but also on how we code the data as the variable z in the ﬁrst place. There-\n",
      "fore, to characterize the properties of the channel itself, we need to consider the\n",
      "maximum of mutual information over all possible ways of coding z. This is called\n",
      "channel capacity and gives the maximum amount of information which can be trans-\n",
      "mitted through the channel.\n",
      "A generalization of mutual information to many variables is often used in the\n",
      "theory of ICA. In that case, the interpretation as information transmitted over a\n",
      "channel is no longer directly applicable. The generalization is based on the fact that\n",
      "mutual information can be expressed in different forms:\n",
      "J(z,y) = H(z)−H(z|y) = H(y)−H(y|z) = H(z)+H(y)−H(z,y)\n",
      "(8.25)\n",
      "where H(z,y) is the joint entropy, which is simply obtained by deﬁning a new ran-\n",
      "dom variable so that it can take all the possible combinations of values of z and y.\n",
      "Based on this formula, we deﬁne mutual information of n random variables as\n",
      "J(z1,z2,...,zn) =\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "H(zi)−H(z1,z2,...,zn)\n",
      "(8.26)\n",
      "The utility in this quantity is that it can be used a measure of independence: it is\n",
      "always non-negative and zero only if the variables zi are independent.\n",
      "2 We use a non-conventional notation J for mutual information because the conventional one, I,\n",
      "could be confused with the notation for an image.\n",
      "\n",
      "194\n",
      "8 Information-theoretic interpretations\n",
      "Conditional entropy, joint entropy, and mutual information can all be deﬁned for\n",
      "continuous-valued variables by using differential entropy in the deﬁnitions instead\n",
      "of ordinary entropy.\n",
      "8.4 Minimum entropy coding of natural images\n",
      "Now, we discuss the application of the information-theoretic concepts in the context\n",
      "of natural image statistics. This section deals with data compression models.\n",
      "8.4.1 Image compression and sparse coding\n",
      "Consider ﬁrst the engineering application of image compression. Such compression\n",
      "is routinely performed when images are transmitted over the Internet or stored on a\n",
      "disk. Most successful image compression methods begin with a linear transforma-\n",
      "tion of image patches. The purpose of such a transformation is to reduce (differen-\n",
      "tial) entropy. Grey-scale values of single pixels have a much larger entropy than, for\n",
      "example, the coefﬁcients in a Fourier or discrete cosine transform (DCT) basis (at\n",
      "least when these are applied on small patches). Since the coefﬁcients in those bases\n",
      "have smaller differential entropy, discretized (quantized) versions of the coefﬁcients\n",
      "are easier to code: the quantization error, i.e. the error in quantizing the coefﬁcients\n",
      "for a ﬁxed code length, in reduced. This is why such a transformation is done as the\n",
      "ﬁrst step.\n",
      "This means that the optimal linear transformation of images as the ﬁrst step of\n",
      "a compression method would be a transformation which minimizes the differential\n",
      "entropy of the obtained components. This turns out to be related to sparse coding,\n",
      "as we will now show.\n",
      "Let us consider the differential entropy of a linear component s. How can we\n",
      "compute the value H(s) using the deﬁnition in Equation (8.20) in practice? The key\n",
      "is to understand that entropy is actually the expectation of a nonlinear function of s.\n",
      "We have\n",
      "H(s) = E{G(s)}\n",
      "(8.27)\n",
      "where the function G is the negative of the log-pdf: G(s) = −log ps(s). In practice,\n",
      "we have a sample of s, denote it by s(t),t = 1,...,T. Assume we also have reason-\n",
      "able approximation of G, that is, we know rather well what the log-pdf is like. Then,\n",
      "differential entropy can be estimated as the sample average for a ﬁxed G as\n",
      "H(s) = 1\n",
      "T ∑\n",
      "t\n",
      "G(s(t))\n",
      "(8.28)\n",
      "Comparing this with Equation (6.2) on page 139, we see that differential entropy is\n",
      "similar the sparseness measures we used in sparse coding. In fact, in Section 7.7.2 it\n",
      "\n",
      "8.4 Minimum entropy coding of natural images\n",
      "195\n",
      "was shown that the optimal sparseness measures are obtained when we use exactly\n",
      "the log-pdf of s as the nonlinearity. Thus, differential entropy is the optimal sparse-\n",
      "ness measure in the sense of Section 7.7.2: it provides the maximum likelihood\n",
      "estimator of the ICA model.\n",
      "However, there is one important point which needs to be taken into account. The\n",
      "sparseness measures assumed that the variance of s is constrained to be equal to\n",
      "one. This is consistent with the theory of ICA which tells that the transformation\n",
      "should be orthogonal in the whitened space. In contrast, in image compression, the\n",
      "transformation giving the components is usually constrained to be orthogonal (in the\n",
      "original image space). One reason is that then the quantization error in the image\n",
      "space is the same as the quantization error in the component space. This makes\n",
      "sense because then minimizing quantization error for the components is directly\n",
      "related to the quantization error of the original image. In contrast, any method which\n",
      "whitens the data ampliﬁes high-frequency components which have low variance,\n",
      "thus emphasizing errors in their coding. So, the quantization error in the components\n",
      "is not the same as the error in the original image—which is what we usually want to\n",
      "minimize in engineering applications.\n",
      "If we consider transformations which are orthogonal in the original space, the\n",
      "constraint of unit variance of a component s is not at all fulﬁlled. For example, PCA\n",
      "is an orthogonal transformation which ﬁnds components with maximally different\n",
      "variances. From the viewpoint of information theory, log p changes quite a lot as\n",
      "a function of variance, so using a ﬁxed G may give a very bad approximation of\n",
      "entropy. So, while sparse coding, as presented in Chapter 6, is related to ﬁnding\n",
      "an optimal basis for image compression, it uses a rather unconventional constraint\n",
      "which means it does not optimize the compression in the usual sense.\n",
      "8.4.2 Mutual information and sparse coding\n",
      "Information-theoretic concepts allow us to see the connection between sparse cod-\n",
      "ing and ICA from yet another viewpoint. Consider a linear invertible transformation\n",
      "y = Vz of a random vector z which is white. The mutual information between the\n",
      "components yi is equal to\n",
      "J(y1,...,yn) =\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "H(vT\n",
      "i z)−H(Vz)\n",
      "(8.29)\n",
      "Recall that mutual information can be interpreted as a measure of dependence. Now,\n",
      "let us constrain V to be orthogonal. Then, we have H(Vz) = H(z) because the shape\n",
      "(in an intuitive sense) of the distribution is not changed at all: an orthogonal trans-\n",
      "formation simply rotates the pdf in the n-dimensional space, leaving its shape intact.\n",
      "This means that the values taken by pz and log pz in the deﬁnition in Equation (8.20)\n",
      "\n",
      "196\n",
      "8 Information-theoretic interpretations\n",
      "are not changed; they are just taken at new values of z. That is why differential en-\n",
      "tropy is not changed by an orthogonal transformation of the data.3\n",
      "So, to minimize the mutual information, we simply need to ﬁnd an orthogonal\n",
      "transformation which minimizes the differential entropies of the components; this\n",
      "is the same as maximizing the nongaussianities of the components. And for sparse\n",
      "data, maximizing non-gaussianity is usually the same as maximizing sparseness.\n",
      "Thus, we see that under the constraint of orthogonality, sparse coding is equiva-\n",
      "lent to minimization of the dependence of the components, if the data is white. This\n",
      "provides another deep link between information theory, sparse coding, and indepen-\n",
      "dence.\n",
      "8.4.3 Minimum entropy coding in the cortex\n",
      "A very straightforward application of the data compression principle is then to as-\n",
      "sume that V1 “wants” to obtain a minimum entropy code. This is very well in line\n",
      "with the results on sparse coding and ICA in Chapters 6 and 7, because we have just\n",
      "shown that the objective functions optimized there can be interpreted as differential\n",
      "entropies and code lengths. Basically, what information theory provides is a new\n",
      "interpretation of the objective functions used in learning simple cell receptive ﬁelds.\n",
      "Yet, it is not quite clear whether such an entropy-based arguments are relevant to\n",
      "the computational tasks facing the visual cortex. A critical discussion on the analogy\n",
      "between compression and cortical coding is postponed to Section 8.6.\n",
      "8.5 Information transmission in the nervous system\n",
      "Following the fundamental division of information theory into compression and\n",
      "transmission, the second inﬂuential application of information theory to visual cod-\n",
      "ing considers maximization of data transmission, usually called simply infomax.\n",
      "8.5.1 Deﬁnition of information ﬂow and infomax\n",
      "Assume that x is a continuous-valued random vector. It is the input to a neural\n",
      "system, which is modelled using a linear-nonlinear model (see Section 3.4.1), with\n",
      "3 A rigorous proof is as follows: denoting y = Vz, we simply have py(y) = pz(VT y) for an or-\n",
      "thogonal V. The basic point is that the absolute value of the determinant of the transformation\n",
      "matrix needed in transforming pdf’s, or variables in an integral formula, (see Section 7.4) is equal\n",
      "to one for an orthogonal transformation, so it can be omitted. Thus, we have\n",
      "R py(y)log py(y)dy =\n",
      "R pz(VT y)log pz(VT y)dy. In this integral we make a change of variables ˜z = VT y and we get\n",
      "R pz(˜z)log pz(˜z)d˜z.\n",
      "\n",
      "8.5 Information transmission in the nervous system\n",
      "197\n",
      "additive noise. Thus, outputs are of the form\n",
      "yi = φi(bT\n",
      "i x)+n\n",
      "(8.30)\n",
      "where the φi are some scalar functions, the bi are the connection weight vectors of\n",
      "the neurons, and n is a vector of white gaussian noise. That is, the neural network\n",
      "ﬁrst computes a linear transformation of the input data, with the coefﬁcients given by\n",
      "network connection weights bi; then it transforms the outputs using scalar functions\n",
      "φi, and there is noise in the system.\n",
      "Let us consider information ﬂow in such a neural network. Efﬁcient information\n",
      "transmission requires that we maximize the mutual information between the inputs\n",
      "x and the outputs y, hence the name “infomax”. This problem is meaningful only\n",
      "if there is some information loss in the transmission. Therefore, we assume that\n",
      "there is some noise in the network; in practice, we have to assume that the noise is\n",
      "inﬁnitely small to be able to derive clear results. We can then ask how the network\n",
      "parameters should be adapted (learned) so as to maximize information transmission.\n",
      "8.5.2 Basic infomax with linear neurons\n",
      "To begin with, we shall consider the very basic case where there are actually no non-\n",
      "linearities: we deﬁne φ(u) = u, and the noise has constant variance. (It may seem\n",
      "odd to say that white noise has constant variance, because that seems obvious. How-\n",
      "ever, in Section 8.5.4 we will consider a model where the variance is not constant\n",
      "because that is the case in neural systems.)\n",
      "By deﬁnition of mutual information, we have\n",
      "J(x,y) = H(y)−H(y|x)\n",
      "(8.31)\n",
      "In the present case, the conditional distribution of y given x is simply the distribution\n",
      "of the gaussian white noise. So, the entropy H(y|x) does not depend on the weights\n",
      "bi at all: it is just a function of the noise variance. This means that for the purpose\n",
      "of ﬁnding the bi which maximize information ﬂow, we only need to consider the\n",
      "output entropy H(y). This is true as long as the noise variance is constant.\n",
      "To simplify the situation, let us assume, just for the purposes of this section, that\n",
      "the transformation matrix B, with the bi as its rows, is orthogonal. Then, y is just an\n",
      "orthogonal transformation of x, with some noise added.\n",
      "Furthermore, in all infomax analysis, we consider the limit where the noise vari-\n",
      "ance goes to zero. This is because simple analytical results can only be obtained in\n",
      "that limit.\n",
      "So, combining these assumptions and results, infomax for linear neurons with\n",
      "constant noise variance boils down to the following: we maximize the entropy H(y),\n",
      "where y is an orthogonal transformation of x. Noise does not need to be taken into\n",
      "account because we consider the limit of zero noise. But, as shown in Section 8.4.2,\n",
      "\n",
      "198\n",
      "8 Information-theoretic interpretations\n",
      "an orthogonal transformation does not change differential entropy, so the informa-\n",
      "tion ﬂow does not depend on B at all!\n",
      "Thus, we reach the conclusion that for linear neurons with constant noise vari-\n",
      "ance, the infomax principle does not really give anything interesting. Fortunately,\n",
      "more sophisticated variants of infomax are more interesting. In the next subsections,\n",
      "we will consider the two principal cases: noise of constant variance with nonlinear\n",
      "neurons, and noise of non-constant variance with linear neurons.\n",
      "8.5.3 Infomax with nonlinear neurons\n",
      "8.5.3.1 Deﬁnition of model\n",
      "First, we consider the case where\n",
      "1. the functions φi are nonlinear. One can build a more realistic neuron model by\n",
      "taking a nonlinearity which is saturating, and has no negative outputs.\n",
      "2. The vector n is additive gaussian white noise.This is the simplest noise model to\n",
      "begin with.\n",
      "Maximization of this mutual information J(x,y) is still equivalent to maximiza-\n",
      "tion of the output entropy, as in the previous subsection. Again we take the limit\n",
      "where the noise has zero variance. We will not go into details here, but it can be\n",
      "shown that the output entropy in this nonlinear infomax model then equals\n",
      "H(y) = ∑\n",
      "i\n",
      "E{logφ′\n",
      "i (bT\n",
      "i x)} +log|detB|\n",
      "(8.32)\n",
      "It turns out that this has a simple interpretation in terms of the ICA model. Now\n",
      "we see that the output entropy is of the same form as the expectation of the likelihood\n",
      "as in Equation (7.15). The pdf’s of the independent components are here replaced\n",
      "by the functions φ′\n",
      "i . Thus, if the nonlinearities φi used in the neural network are\n",
      "chosen as the cumulative distribution functions corresponding to the densities pi of\n",
      "the independent components, i.e., φ′\n",
      "i (·) = pi(·), the output entropy is actually equal\n",
      "to the likelihood. This means that infomax is equivalent to maximum likelihood\n",
      "estimation of the ICA model.\n",
      "Usually, the logistic function\n",
      "φi(u) =\n",
      "1\n",
      "1 +exp(−u)\n",
      "(8.33)\n",
      "is used in the nonlinear infomax model (see Fig. 8.2 a). This estimates the ICA\n",
      "model in the case of sparse independent components, because if we interpret φ′\n",
      "i as a\n",
      "pdf, it is sparse. In fact, the log-pdf given by logφ′\n",
      "i is nothing else than the familiar\n",
      "logcosh function (with negative sign and some unimportant constants), which we\n",
      "have used as a measure of sparseness in Chapter 6, and as a model of a smooth\n",
      "sparse log-pdf in Equation (7.19).\n",
      "\n",
      "8.5 Information transmission in the nervous system\n",
      "199\n",
      "8.5.4 Infomax with non-constant noise variance\n",
      "Here, we present some critique of the nonlinear infomax model, and propose an\n",
      "alternative formulation.\n",
      "8.5.4.1 Problems with nonlinear neuron model\n",
      "Using a logistic function as in Equation (8.33) is a correct way of estimating the ICA\n",
      "model for natural image data in which the components really are super-gaussian.\n",
      "However, if the transfer function φi is changed to the gaussian cumulative distri-\n",
      "bution function, the method does not estimate the ICA model anymore, since this\n",
      "would amount to assuming gaussian independent components, which makes the es-\n",
      "timation impossible. An even worse situation arises if we change the function φi so\n",
      "that φ′\n",
      "i is the pdf of a sub-gaussian (anti-sparse) distribution. This amounts to esti-\n",
      "mating the ICA model assuming sub-gaussian independent components. Then, the\n",
      "estimation fails completely because we have made a completely wrong assumption\n",
      "on the distribution of the components.\n",
      "Unfortunately, the three nonlinear functions φi corresponding to Equation (8.33),\n",
      "the gaussian case, and one particular sub-gaussian case look all very similar, how-\n",
      "ever. This is illustrated in Fig. 8.2 a). All the three functions have the same kind of\n",
      "qualitative behaviour. In fact, all cumulative distribution functions look very similar\n",
      "after appropriate scaling along the x-axis.\n",
      "It is not very likely that the neural transfer functions (which are only crude ap-\n",
      "proximations anyway) would consistently be of the type in Eq. (8.33), and not closer\n",
      "to the two other transfer functions. Thus, the model can be considered to be non-\n",
      "robust, that is, too sensitive to small ﬂuctuations in its parameters.4\n",
      "8.5.4.2 Using neurons with non-constant variance\n",
      "A possible solution to the problems with nonlinear infomax is to consider a more\n",
      "realistic noise model. Let us thus take the linear function as φi, and change the\n",
      "deﬁnition of the noise term instead.\n",
      "What would be a sensible model for the noise? Many classic models of neurons\n",
      "assume that the output is coded as the mean ﬁring rate in a spike train, which follows\n",
      "a Poisson process. Without going into details, we just note that in that case, the\n",
      "variance of mean ﬁring rate has a variance that is equal to its mean. Thus, we have\n",
      "var(ni|x) ∝r +|bT\n",
      "i x|\n",
      "(8.34)\n",
      "4 It could be argued that the nonlinear transfer function can be estimated from the data and it need\n",
      "not be carefully chosen beforehand, but this only modiﬁes this robustness problem because then\n",
      "that estimation must be very precise.\n",
      "\n",
      "200\n",
      "8 Information-theoretic interpretations\n",
      "a)\n",
      "−6\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1\n",
      "b)\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "1.2\n",
      "1.4\n",
      "1.6\n",
      "1.8\n",
      "2\n",
      "1\n",
      "1.2\n",
      "1.4\n",
      "1.6\n",
      "1.8\n",
      "2\n",
      "2.2\n",
      "2.4\n",
      "2.6\n",
      "2.8\n",
      "3\n",
      "Fig. 8.2: a) Three sigmoidal nonlinearities corresponding to logistic, gaussian, and sub-gaussian\n",
      "(with log-pdf proportional to −x4) prior cumulative distributions for the independent components.\n",
      "The nonlinearities are practically indistinguishable. (Note that we can freely scale the functions\n",
      "along the x-axis since this has no inﬂuence on the behaviour in ICA estimation. Here we have\n",
      "chosen the scaling parameters so as to emphasize the similarity.) b) Three functions h that give\n",
      "the dependencies of noise variance (functions h) which are equivalent to different distributions.\n",
      "Solid line: basic Poisson like variance as in Eq. (8.36), corresponding to a sparse distribution.\n",
      "Dashed line: the case of gaussian distribution as in Eq. (8.39). Dotted line: the sub-gaussian distri-\n",
      "bution used in a). Here, the function in the basic Poisson-like variance case is very different from\n",
      "the others, which indicates better robustness for the model with changing noise variance. From\n",
      "(Hyv¨arinen, 2002), Copyright c⃝2002 Elsevier, used with permission\n",
      "where r is a constant that embodies the spontaneous ﬁring rate which is not zero\n",
      "(and hence does not have zero noise). We take the absolute value of bT\n",
      "i x because\n",
      "we consider the output of a signed neuron to be actually coded by two different\n",
      "neurons, one for the negative part and one for the positive part. The distribution of\n",
      "noise in mean ﬁring rate is non-gaussian in the Poisson process case. However, in\n",
      "the following we approximate it as gaussian noise: The fundamental property of this\n",
      "new type of noise is considered to be the variance behaviour given in Eq. (8.34), and\n",
      "not its non-gaussianity.Therefore, we call noise with this kind of variance behaviour\n",
      "“noise with Poisson-like variance” instead of Poisson noise.\n",
      "A more general form of the model can be obtained by deﬁning the variance to\n",
      "be a nonlinear function of the quantity in Eq. (8.34). To investigate the robustness\n",
      "of this model, we do in the following all the computations in the more general case\n",
      "where\n",
      "var(ni|x) = h(bT\n",
      "i x)\n",
      "(8.35)\n",
      "where h is some arbitrary function with non-negative values, for example\n",
      "h(u) = r +|u|\n",
      "(8.36)\n",
      "in the case of Eq. (8.34).\n",
      "It then can be shown that the mutual information in the limit of zero noise is\n",
      "equal to\n",
      "\n",
      "8.5 Information transmission in the nervous system\n",
      "201\n",
      "J(x,y) = log|detB|−∑\n",
      "i\n",
      "E{log\n",
      "q\n",
      "h(bT\n",
      "i x)} +const.\n",
      "(8.37)\n",
      "where terms that do not depend on B are grouped in the constant. A comparison\n",
      "of Eq. (8.37) with Eq. (8.32) reveals that in fact, mutual information is of the same\n",
      "algebraic form in the two cases. By taking h(u) = 1/φ′\n",
      "i (u)2, we obtain an expression\n",
      "of the same form. Thus, we see that considering noise with non-constant variance,\n",
      "we are able to reproduce the same results as with a nonlinear transfer function.\n",
      "If we consider the basic case of Poisson-like variance, which means deﬁning the\n",
      "function h so that we have Eq. (8.34), this is equivalent to the nonlinear infomax\n",
      "with\n",
      "φ′\n",
      "i (u) =\n",
      "1\n",
      "p\n",
      "r +|u|\n",
      "(8.38)\n",
      "In the nonlinear infomax, φ′\n",
      "i corresponds to the probability density function assumed\n",
      "for the independent component. The function in (8.38) is an improper probability\n",
      "density function, since it is not integrable. However, its qualitative behaviour is typ-\n",
      "ically super-gaussian: very heavy tails and a peak at zero.\n",
      "Thus, in the basic case of Poisson-like variance, the infomax principle is equiva-\n",
      "lent to estimation of the ICA model with this improper prior density for the compo-\n",
      "nents. Since the choice of nonlinearity is usually critical only along the sub-gaussian\n",
      "vs. super-gaussian axis, this improper prior distribution can still be expected to prop-\n",
      "erly estimate the ICA model for most super-gaussian components.5\n",
      "To investigate the robustness of this model, we can consider what the noise vari-\n",
      "ance structure should be like to make the estimation of super-gaussian components\n",
      "fail. As with the nonlinear infomax, we can ﬁnd a noise structure that corresponds\n",
      "to the estimation of gaussian independent components. In the limit of r = 0, we have\n",
      "the relation h(u) = 1/φ′(u)2, and we see that the gaussian case corresponds to\n",
      "h(u) ∝exp(u2)\n",
      "(8.39)\n",
      "This is a fast-growing (“exploding”) function which is clearly very different from\n",
      "the Poisson-like variance structure given by the essentially linear function in Equa-\n",
      "tion (8.36). In the space of possible functions h that deﬁne the noise structure in this\n",
      "model, the function in Eq. (8.39) can be considered as a borderline between those\n",
      "variance structures that enable the estimation of super-gaussian independent com-\n",
      "ponents, and those that do not. These two different choices for h, together with one\n",
      "corresponding to sub-gaussian independent components (see caption) are plotted in\n",
      "Fig. 8.2 b). The Poisson-like variance is clearly very different from the other two\n",
      "cases.\n",
      "Thus, we may conclude that the model with Poisson-like variance is quite robust\n",
      "against changes of parameters in the model, since the main parameter is the function\n",
      "5 There is, however, the problem of scaling the components. Since the improper density has inﬁnite\n",
      "variance, the estimates of the components (and the weight vectors) grow inﬁnitely large. Such be-\n",
      "haviour can be prevented by adding a penalty term of the form α ∑i ∥wi∥2 in the objective function.\n",
      "An alternative approach would be to use a saturating nonlinearity as φi, thus combining the two\n",
      "infomax models.\n",
      "\n",
      "202\n",
      "8 Information-theoretic interpretations\n",
      "h, and this can change qualitatively quite a lot before the behaviour of the model\n",
      "with respect to ICA estimation changes. This is in contrast to the nonlinear infomax\n",
      "principle where the nonlinearity has to be very carefully chosen according to the\n",
      "distribution of the data.\n",
      "8.6 Caveats in application of information theory\n",
      "We conclude this chapter with a discussion on some open problems encountered in\n",
      "the application of information theory to model cortical visual coding in the context\n",
      "of natural image statistics.\n",
      "Classic information theory is fundamentally a theory of compression and trans-\n",
      "mission of binary strings. It is important to ask Is this theory really useful in the study\n",
      "of cortical visual representations? Often, the concepts of information theory are di-\n",
      "rectly applied in neuroscience simply because it is assumed that the brain processes\n",
      "“information”. However, the concept of information, or the way it is processed, may\n",
      "be rather different in the two cases.\n",
      "In the data compression scheme, we start with a binary string, i.e. a sequence\n",
      "of zeros and ones. We want to transform the vectors into a another string, so that\n",
      "the string is as short as possible. This is basically accomplished by coding the most\n",
      "frequent realizations by short substrings or codewords, and using longer codewords\n",
      "for rarely occurring realizations. Such an approach has been found immensely useful\n",
      "in storage of information in serial digital computers.\n",
      "However, if the processing of information is massively parallel, as in the brain,\n",
      "it is not clear what would be the interpretation of such reduction in code length.\n",
      "Consider an image that is coded in the millions of neurons in V1. A straightforward\n",
      "application of information theory would suggest that for some images, we only use\n",
      "k1 neurons, where each neuron codes for one digit in the string, whereas others\n",
      "need k2 neurons where k2 > k1. Furthermore, an optimal image code would be one\n",
      "where the average number of neurons is minimized. Yet, the number of neurons that\n",
      "are located in, say, the primary visual cortex is just the same for different stimuli. It\n",
      "would be rather absurd to think that some region of V1 is not needed to represent the\n",
      "most probable images. Even if some cells are not activated above the spontaneous\n",
      "ﬁring rate, this lack of activation is an important part of the code, and does not mean\n",
      "that the neuron is not “part of the code”.\n",
      "In fact, in sparse coding the active neurons are assumed to be different for differ-\n",
      "ent stimuli, and each neuron is more or less equally important for representing some\n",
      "of the stimuli. While sparseness, when interpreted in terms of entropy, has some\n",
      "superﬁcial similarity to information theoretic arguments, reducing the length of a\n",
      "string is very different from sparse coding because sparse coding is fundamentally\n",
      "a parallel scheme where no sequential order is given to the neurons, and the outputs\n",
      "of all neurons are needed to reconstruct the stimulus. That is, there is no reduc-\n",
      "tion of code “length” because the number of coding units needed for reconstructing\n",
      "the stimulus is always the same, i.e. the total number of neurons. The whole con-\n",
      "\n",
      "8.7 Concluding remarks and References\n",
      "203\n",
      "cept of “length” is not well-deﬁned in the case of massively parallel and distributed\n",
      "processing. Reducing the length of a string is fundamentally an objective in serial\n",
      "information processing.\n",
      "Another motivation for application of information theory in learning optimal rep-\n",
      "resentations comes from transmission of data. Optimal transmission methods are\n",
      "important in systems where data have to be sent through a noisy channel of limited\n",
      "capacity. Again, the basic idea is to code different binary sequences using other bi-\n",
      "nary strings, based on their probabilities of occurrence. This allows faster and more\n",
      "reliable transmission of a serial binary signal.\n",
      "Such “limited-capacity channel” considerations may be quite relevant in the case\n",
      "of the retina and optic nerve as well as nerves coming from other peripheral sensory\n",
      "organs. Another important application for this theory is in understanding coding\n",
      "of signals using spike trains. However, in V1, a limited capacity channel may be\n",
      "difﬁcult to ﬁnd. A well-known observation is that the visual input coming from the\n",
      "lateral geniculate nucleus (LGN) is expanded in the V1 by using a representation\n",
      "consisting of many more neurons than there are in the LGN. So, the transmission of\n",
      "information from LGN to V1 may not be seriously affected by the limited capacity\n",
      "of the “channel”. Yet, the limited capacity of the channel coming to V1 is the basic\n",
      "assumption in infomax models.6\n",
      "Thus, we think application of information-theoretical arguments in the study of\n",
      "cortical visual coding has to be done with some caution. Borrowing of concepts\n",
      "originally developed in electrical engineering should be carefully justiﬁed. This is\n",
      "an important topic for future research.\n",
      "8.7 Concluding remarks and References\n",
      "Information theory provides another viewpoint to the utility of statistical modelling\n",
      "of images. The success in tasks such as compression and transmission depends on\n",
      "ﬁnding a useful representation of the data, and information theory points out that\n",
      "the optimal representation is the one which provides the best probabilistic model.\n",
      "Some studies therefore apply information-theoretic concepts to the study of natural\n",
      "image statistics and vision modelling. The idea of minimum-entropy coding gives\n",
      "some justiﬁcation for sparse coding, and information transmission leads to objec-\n",
      "tive functions which are sometimes equivalent to those of ICA. Nevertheless, we\n",
      "take here a more cautious approach because we think it is not clear if information\n",
      "theoretical concepts can be directly applied in the context of neuroscience, which\n",
      "may be far removed from the original digital communications setting in which the\n",
      "theory was originally developed.\n",
      "A basic introduction to information theory is (Mackay, 2003). A classic reference\n",
      "which can be read as an introduction as well is (Cover and Thomas, 2006).\n",
      "6 Possibly, the channel from V1 to V2 and other other extrastriate areas could have a very limited\n",
      "capacity, but that is not the usual assumption in current infomax models.\n",
      "\n",
      "204\n",
      "8 Information-theoretic interpretations\n",
      "Basic and historical references on the infomax principle are (Laughlin, 1981;\n",
      "van Hateren, 1992; Linsker, 1988; Fairhall et al, 2001). The nonlinear infomax\n",
      "principle was introduced in (Nadal and Parga, 1994; Bell and Sejnowski, 1995).\n",
      "Infomax based on noise models with non-constant variance were introduced by\n",
      "(van Vreeswijk, 2001; Hyv¨arinen, 2002), using rather different motivations. Pois-\n",
      "son models for spike trains are discussed in (Dayan and Abbott, 2001). Information\n",
      "content in spike trains in considered in, e.g. (Rieke et al, 1997). Another critique of\n",
      "the application of infomax principles to cortical coding can be found in (Ringach\n",
      "and Malone, 2007).\n",
      "8.8 Exercices\n",
      "Mathematical exercises\n",
      "1. Consider the set of all possible probability distributions for a random variable\n",
      "which takes values in the set {1,2,...,100}. Which distribution has minimum\n",
      "entropy?\n",
      "2. Prove Equation (8.25).\n",
      "3. Consider a general (not standardized) one-dimensional gaussian distribution,\n",
      "with pdf given by\n",
      "p(z) =\n",
      "1\n",
      "√\n",
      "2πσ\n",
      "exp(−1\n",
      "2σ2 (z−µ)2)\n",
      "(8.40)\n",
      "Compute its differential entropy. When it is maximized? When minimized?\n",
      "4. Consider a random variable z with pdf\n",
      "1\n",
      "σ p0( z\n",
      "σ )\n",
      "(8.41)\n",
      "where z takes values on the whole real line, and the function p0 is ﬁxed. Compute\n",
      "the differential entropy as a function of σ and p0.\n",
      "5. * Assume we have a random vector z with pdf pz, and differential entropy H(z).\n",
      "Consider a linear transformation y = Mz. What is the differential entropy of y?\n",
      "Hint: don’t forget to use the probability transformation formula involving the\n",
      "determinant of M, as in Equation (7.13); and note that the preceding exercise is\n",
      "a special case of this one.\n",
      "Computer assignments\n",
      "1. Let’s consider discrete probability distributions which take values in the set\n",
      "{1,2,...,100}. Create random probabilities for each of those values taken (re-\n",
      "\n",
      "8.8 Exercices\n",
      "205\n",
      "member to normalize). Compute the entropy of the distribution. Repeat this 1,000\n",
      "times. Find the distribution which had the largest and smallest entropies. What\n",
      "do they look like? Compare with results in Examples 2 and 3.\n",
      "\n",
      "\n",
      "Part III\n",
      "Nonlinear features & dependency of linear\n",
      "features\n",
      "\n",
      "\n",
      "Chapter 9\n",
      "Energy correlation of linear features &\n",
      "normalization\n",
      "It turns out that when we estimate ICA from natural images, the obtained compo-\n",
      "nents are not really independent. This may be surprising since after all, in the ICA\n",
      "model, the components are assumed to be independent. But it is important to under-\n",
      "stand that while the components in the theoretical model are independent, the esti-\n",
      "mates of the components of real image data are often not independent. What ICA\n",
      "does is that it ﬁnds the most independent components that are possible by a linear\n",
      "transformation, but a linear transformation has so few parameters that the estimated\n",
      "components are often quite far from being independent. In this chapter and the fol-\n",
      "lowing ones, we shall consider some dependencies that can be observed between\n",
      "the estimated independent components. They turn out to be extremely interesting\n",
      "both from the viewpoint of computational neuroscience and image processing. Like\n",
      "in the case of ICA, the models proposed here are still very far from providing a\n",
      "complete description of natural image statistics, but each model does exhibit some\n",
      "very interesting new phenomena just like ICA.\n",
      "9.1 Why estimated independent components are not independent\n",
      "9.1.1 Estimates vs. theoretical components\n",
      "A paradox with ICA is that in spite of the name of the method, the estimated com-\n",
      "ponents need not be independent. That is, when we have a sample of real image\n",
      "patches, and estimate the independent components by an ICA algorithm, we get\n",
      "components which are usually not independent. The key to this paradox is the dis-\n",
      "tinction between the estimated components and theoretical components. The theo-\n",
      "retical components, which do not really exist because they are just a mathematical\n",
      "abstraction, are assumed to be independent. However, what an ICA algorithm gives,\n",
      "for any real data, is estimates of those theoretical components, and the estimates do\n",
      "209\n",
      "\n",
      "210\n",
      "9 Energy correlation of linear features & normalization\n",
      "not have all the properties of the theoretical components. In particular, the estimates\n",
      "need not be independent.\n",
      "Actually, it is not surprising that the components estimated by ICA are not inde-\n",
      "pendent. If they were, the statistical structure of natural images would be completely\n",
      "described by the simple ICA model. If we knew the linear features Ai and the dis-\n",
      "tributions of the independent components, we would know everything there is to\n",
      "know about the statistical structure of natural images. This would be rather absurd,\n",
      "because natural images are obviously an extremely complex data set; one could say\n",
      "it is as complex as our world.\n",
      "There are two different reasons why the estimates need not have the properties\n",
      "assumed for the theoretical components. First, the real data may not fulﬁll the as-\n",
      "sumptions of the model. This is very often the case, since models are basically ab-\n",
      "stractions or approximations of reality. (We will see below how the ICA model does\n",
      "not hold for natural image data.) The second reason is random ﬂuctuation, called\n",
      "sampling effect in statistics: When we estimate the model for a ﬁnite number of\n",
      "image patches, we have only a limited amount of information about the underlying\n",
      "distribution, and some errors in the estimates are bound to occur because of this.\n",
      "Consider, for example, the two-dimensional data in Fig. 9.1. This data is white\n",
      "(uncorrelated and unit variance), so we can constrain the ICA matrix to be orthogo-\n",
      "nal. If we input this data into an ICA algorithm, the algorithm says that the horizon-\n",
      "tal and vertical axis (say, s1 and s2) are the “independent components”. However, it\n",
      "is easy to see that these components are not independent: If we know that s1 is zero,\n",
      "we know that s2 cannot be zero. Thus, information on one of the components gives\n",
      "information on the other component, so the components cannot be independent.\n",
      "This data does not follow the ICA model for any parameter values.\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "Fig. 9.1: Scatter plot of a distribution which cannot be linearly decomposed to independent compo-\n",
      "nents. Thus, the estimated components (given by the horizontal and vertical axes) are dependent.\n",
      "\n",
      "9.2 Correlations of squares of components in natural images\n",
      "211\n",
      "9.1.2 Counting the number of free parameters\n",
      "Another way of looking at this paradox is to think of the number of free parameters.\n",
      "A linear transformation of n variables to n new variables has n2 free parameters. We\n",
      "can think of the problem of ﬁnding really independent components as a large system\n",
      "of equations which express the independence of the obtained components. How\n",
      "many equations are there? In Section 4.6 we saw that two independent components\n",
      "have the following nonlinear uncorrelatedness property:\n",
      "cov(f1(si), f2(sj)) = 0\n",
      "(9.1)\n",
      "for any nonlinear functions f1 and f2. Now, there are an inﬁnite number of different\n",
      "nonlinearities we could use. So, based on Equation (4.42) we can form an inﬁnite\n",
      "number of different equations (constraints) that need to be fulﬁlled, but we only\n",
      "have a ﬁnite number of free parameters, namely n2. Thus, it is clear1 that usually,\n",
      "no solution can be found!\n",
      "Note that the situation with respect to independence is in stark contrast to the\n",
      "situation with whitening. As we saw in Chapter 5, we can always ﬁnd a linear\n",
      "transformation which gives uncorrelated, and further whitened, components. This\n",
      "is because whitening only needs to consider the covariance matrix, which has a rel-\n",
      "atively small number of free parameters. In fact, the number of equations we get is\n",
      "n(n +1)/2 (because the covariance is a symmetric operation, this is the number of\n",
      "free parameters), which is smaller than n2, so we can ﬁnd a transformation which\n",
      "whitens the data.\n",
      "9.2 Correlations of squares of components in natural images\n",
      "Now, let us consider the dependencies of the components estimated in the Chapter 7.\n",
      "The components are forced to be exactly uncorrelated by the ICA method we used.\n",
      "So, any dependencies left between the components must take the form of some kind\n",
      "of nonlinear correlations. Let us compute correlations of the type in Equation (9.1)\n",
      "for different nonlinear functions f (we use the same function as both f1 and f2).\n",
      "Because the variances of the nonlinear transformations are not necessarily equal to\n",
      "one, it is a good idea to normalize the covariance to yield the correlation coefﬁcient:\n",
      "corr(f(si), f(sj)) = E{ f(si)f(sj)} −E{ f(si)}E{ f(sj)}\n",
      "p\n",
      "var(f(si))var(f(sj))\n",
      "(9.2)\n",
      "1 Strictly speaking, we should show that we can form an inﬁnite number of equations which cannot\n",
      "be reduced to each other. This is too difﬁcult to show but it is likely to be true when we look at\n",
      "some arbitrary data distribution, such as the distribution of natural images. Of course, the situation\n",
      "is different when the data actually follows the ICA model: in that case we know that there is a\n",
      "solution. A solution is then possible because in this very special case, the equations can be reduced,\n",
      "just as if we needed to solve a system of linear equations where the matrix is not full rank.\n",
      "\n",
      "212\n",
      "9 Energy correlation of linear features & normalization\n",
      "In Figure 9.2 we show the correlation coefﬁcients for several different functions\n",
      "f. The ﬁgure shows the histograms of all the correlation coefﬁcients between dif-\n",
      "ferent pairs of independent components estimated in Chapter 7.\n",
      "It turns out that we have a strong correlation for even-symmetric functions, i.e.\n",
      "functions for which\n",
      "f(−s) = f(s)\n",
      "(9.3)\n",
      "Typical examples are the square function or the absolute value (b and a in the ﬁgure).\n",
      "a)\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "b)\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "c)\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "d)\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "e)\n",
      "−0.4\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "Fig. 9.2: Histograms of correlation coefﬁcients of nonlinear functions of independent components\n",
      "estimated from natural images. a) f (s) = |s|, b) f (s) = s2, c) f (s) is a thresholding function that\n",
      "gives 0 between -1 and 1 and gives 1 elsewhere, d) f (s) = sign(s), e) f (s) = s3. Note that linear\n",
      "correlations, i.e. the case f (s) = s are zero up to machine precision by deﬁnition.\n",
      "\n",
      "9.3 Modelling using a variance variable\n",
      "213\n",
      "9.3 Modelling using a variance variable\n",
      "Intuitively, the dependency of two components that was described above is such\n",
      "that the components tend to be “active”, i.e. have non-zero outputs, at the same\n",
      "time. However, the actual values of the components are not easily predictable from\n",
      "each other. To understand this kind of dependency consider a case where the compo-\n",
      "nents si are deﬁned as products of “original” independent variables ˜si and a common\n",
      "“variance” variable d, which is independent of the ˜si. For simplicity, let us deﬁne\n",
      "that the means of the ˜si are zeros and the variances are equal to one. Thus, we deﬁne\n",
      "the distribution of the components as follows:\n",
      "s1 = ˜s1d\n",
      "(9.4)\n",
      "s2 = ˜s2d\n",
      "...\n",
      "sn = ˜snd\n",
      "Now, si and sj are uncorrelated for i ̸= j, but they are not independent. The idea is\n",
      "that d controls the overall activity level of the two components: if d is very small, s1\n",
      "and s2 are probably both very small, and if d is very large, both components tend to\n",
      "have large absolute values.\n",
      "Such dependency can be measured by the correlation of their squares s2\n",
      "i , some-\n",
      "times called the “energies”. This means that\n",
      "cov(s2\n",
      "i ,s2\n",
      "j) = E{s2\n",
      "i s2\n",
      "j} −E{s2\n",
      "i }E{s2\n",
      "j} > 0.\n",
      "(9.5)\n",
      "In fact, assuming that ˜si and ˜sj have zero mean and unit variance, the covariance of\n",
      "the squares equals\n",
      "E{˜s2\n",
      "i d2 ˜s2\n",
      "jd2} −E{˜s2\n",
      "i d2}E{˜s2\n",
      "jd2}\n",
      "= E{˜s2\n",
      "i }E{˜s2\n",
      "j}E{d2d2} −E{˜s2\n",
      "i }E{d2}E{˜s2\n",
      "j}E{d2} = E{d4} −E{d2}2\n",
      "(9.6)\n",
      "This covariance is positive because it equals the variance of d2, and the variance of\n",
      "a random variable is always positive (unless the variable is constant).\n",
      "Moreover, if the ˜si are gaussian, the resulting components s1 and s2 can be shown\n",
      "to be sparse (leptokurtic). This is because the situation for each of the components\n",
      "is just the same as in Section 7.8.3: changing the variance of a gaussian variable\n",
      "creates a sparse variable. However, we do not assume here that original components\n",
      "˜si are gaussian, so the effect of the variance variables is to increase their sparseness.\n",
      "What is not changed from basic ICA is that the components si are uncorrelated\n",
      "in the ordinary sense. This is because we have\n",
      "E{sisj} = E{˜si}E{˜sj}E{d2} = 0\n",
      "(9.7)\n",
      "\n",
      "214\n",
      "9 Energy correlation of linear features & normalization\n",
      "due to the independence of the d from ˜sj. One can also deﬁne that the si have vari-\n",
      "ance equal to one, which is just a scaling convention as in ICA. Thus the vector\n",
      "(s1,s2,...,sn) can be considered to be white.\n",
      "9.4 Normalization of variance and contrast gain control\n",
      "To reduce the effect of the variance dependencies, it is useful to normalize the local\n",
      "variance. Let us assume that the image patch is generated as a linear combination\n",
      "of independent features, as in ICA. However, now the variances of the components\n",
      "change from patch to patch as above. This can be expressed as\n",
      "I(x,y) =\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)(d ˜si) = d\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)˜si\n",
      "(9.8)\n",
      "where d is the variance variables that gives the standard deviation at each patch. It\n",
      "is a random variable because its value changes from patch to patch.\n",
      "Here we made the strong assumption that the variances of all the components are\n",
      "determined by a single variance variable d. This may be not a bad approximation\n",
      "when considering small image patches. It simpliﬁes the situation considerably, since\n",
      "now we can simply estimate d and divide the image patch by d:\n",
      "¯I(x,y) ←I(x,y)\n",
      "ˆd\n",
      "(9.9)\n",
      "Assuming that we have a perfect estimator ˆd = d, the normalized images ¯I then\n",
      "follow the basic ICA model\n",
      "¯I(x,y) = d\n",
      "d\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)˜si =\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)˜si\n",
      "(9.10)\n",
      "with the original components ˜si. In practice we don’t have a perfect estimator, so the\n",
      "data will follow the ICA model only approximatively. Also, it is preferable to do\n",
      "¯I(x,y) ←I(x,y)\n",
      "ˆd +ε\n",
      "(9.11)\n",
      "where ε is a relatively small constant that prevents division by zero or a very small\n",
      "number.2\n",
      "This kind of normalization of variances is called contrast gain control. It can\n",
      "be compared with the subtraction of the DC component: an unrelated (irrelevant)\n",
      "variable that has a strong effect on the statistics of the features we are modelling is\n",
      "removed so that we have a more direct access to the statistics.\n",
      "2 A reasonable method for determining ε might be to take the 10% quantile of the values of d,\n",
      "which we did in our simulations.\n",
      "\n",
      "9.4 Normalization of variance and contrast gain control\n",
      "215\n",
      "For small image patches, one rather heuristic approach is to simply estimate d\n",
      "using the norm of the image patch\n",
      "ˆd = c\n",
      "r\n",
      "∑\n",
      "x,y\n",
      "I(x,y)2\n",
      "(9.12)\n",
      "where c is a constant that is needed to make the si have unit variance after normal-\n",
      "ization; it depends of the covariance matrix of the data. Usually, however, we do not\n",
      "need to compute c because it only changes the overall scaling of the data.\n",
      "When we normalize the contrast as described here, and compute the output of\n",
      "linear feature detector, the result is closely related to the neurophysiological model\n",
      "of divisive normalization, see Equation (3.9) on page 65. The output of the linear\n",
      "feature detector is then computed as\n",
      "˜s = ∑x,yW(x,y)I(x,y)\n",
      "q\n",
      "∑x,y I(x,y)2 +ε\n",
      "(9.13)\n",
      "In the divisive normalization model, the denominator was essentially the sum of the\n",
      "squares of the outputs of linear feature detectors. Here, we have the norm of the\n",
      "image patch instead. However, these two can be closely related to each other if the\n",
      "set of linear feature detectors form an orthogonal basis for a small image patch; then\n",
      "the sum of squares of pixel values and feature detectors are exactly equal.\n",
      "The approach to estimating d in this section was rather ad hoc. A more principled\n",
      "method for contrast gain control could be obtained by properly deﬁning a probability\n",
      "distribution for d together with the ˜s, and estimating all those latent variables using\n",
      "maximum likelihood estimation or some other principled methods. Furthermore, if\n",
      "the model is to be used on whole images instead of small patches, a single variance\n",
      "variable is certainly insufﬁcient. This is still an area of ongoing research, see the\n",
      "References section for more information.\n",
      "Let us just mention here a slight modiﬁcation of the divisive normalization in\n",
      "Equation (9.11) and (9.12) which has been found useful in some contexts. The idea\n",
      "is that one could compute a weighted sum of the pixel values I(x,y) to estimate the\n",
      "variance variable. In particular, low frequencies dominate Equation (9.12) because\n",
      "they have the largest variances. This effect could be eliminated by computing a\n",
      "whitening matrix and using the norms of the patches in the whitened space as ˆd.\n",
      "Note that the divisive normalization destroys the whiteness of the data, so after such\n",
      "normalization, the whitening matrix has to be recomputed, and the data has to be\n",
      "whitened with this new whitening matrix.\n",
      "\n",
      "216\n",
      "9 Energy correlation of linear features & normalization\n",
      "9.5 Physical and neurophysiological interpretations\n",
      "Why are the variances, or general activity levels, so strongly correlated, and what\n",
      "is the point in contrast gain control? A number of intuitive explanations can be put\n",
      "forward.\n",
      "9.5.1 Cancelling the effect of changing lighting conditions\n",
      "The illumination (lighting) conditions can drastically change from one image to\n",
      "another. The same scene can be observed under very different lighting conditions,\n",
      "think for example of daylight, dusk, and indoor lighting. The light coming onto the\n",
      "retina is a function of the reﬂectances of the surfaces in the scene (R), and the light\n",
      "(illuminance) level (L). In fact, the reﬂectances are multiplied by the illuminance to\n",
      "give the luminance I arriving at the retina:\n",
      "I(x,y) = L(x,y)R(x,y)\n",
      "(9.14)\n",
      "In bright daylight, the luminance levels are uniformly larger than in an indoor room,\n",
      "and so are the contrasts. The average luminance level is not visible in our images\n",
      "because we have removed the DC component, which is nothing else than the mean\n",
      "luminance in an image patch. But the general illuminance level still has a clear effect\n",
      "on the magnitude of the contrasts in the image, and these are seen in the values of the\n",
      "independent components. In a whole scene the illuminance may be quite different in\n",
      "different parts of the image due to shadows, but in a small image patch illuminance is\n",
      "likely to be approximately the same for all pixels. Thus, the single variance variable\n",
      "d, which does not depend on x or y, could be interpreted as the general illuminance\n",
      "level in an image patch.\n",
      "In this interpretation, the utility of divisive normalization is that it tries to estimate\n",
      "the reﬂectances R of the surfaces (objects). These are what we are usually interested\n",
      "in, because they are needed for object recognition. Illuminance L is usually not of\n",
      "much interest.\n",
      "9.5.2 Uniform surfaces\n",
      "A second reason for the correlated changes in the variances of features outputs is\n",
      "what is called the “blue sky effect”. Natural images contain large areas of almost\n",
      "zero contrast, such as the sky. In such areas, the variances of all the independent\n",
      "components should be set to almost zero. Thus, the variance variable d is related to\n",
      "whether the image patch is in a uniform surface or not. This would partly explain\n",
      "the observed changes in the variances of the components, but this does not seem to\n",
      "explain utility of contrast gain control.\n",
      "\n",
      "9.6 Effect of normalization on ICA\n",
      "217\n",
      "9.5.3 Saturation of cell responses\n",
      "Mechanisms related to gain control have been observed in many parts of the visual\n",
      "system, from the retina to the visual cortex (see the References section below). Be-\n",
      "fore the advent of statistical modelling, their existence was usually justiﬁed by the\n",
      "limited response range of neurons. As discussed in section 3.4.1, the neurons cannot\n",
      "ﬁre above a certain ﬁring rate. The range of contrasts that are present in the stim-\n",
      "uli coming to the retina is huge because of the changes in illuminance condition:\n",
      "the incoming signal can differ in several orders of magnitude. Contrast gain con-\n",
      "trol is assumed to solve this problem by dividing the contrasts (to be coded by cell\n",
      "responses) by a measure of the general contrast level. This leads to a very similar\n",
      "computation to what our statistical modelling proposes, although our model does not\n",
      "consider limited response ranges of neurons — linear RF’s have no such limitation.\n",
      "9.6 Effect of normalization on ICA\n",
      "Although we considered the dependencies after estimating ICA, it makes sense to\n",
      "do the variance normalization before ICA. This would be theoretically optimal be-\n",
      "cause then the ICA estimation would be performed on data whose distribution is\n",
      "closer to the distribution given by the ICA model. In fact, the method given above\n",
      "in Section 9.4 can actually normalize the patches without computing independent\n",
      "components.\n",
      "A valid question then is: does variance normalization affect the independent com-\n",
      "ponents? Let us now estimate ICA after variance normalization to see what effect\n",
      "there may be. The obtained Wi and Ai are shown in Figures 9.3 and 9.4. The simi-\n",
      "larity to the results obtained without variance normalization (Figs. 6.6 on page 151\n",
      "and 7.3 on page 169) is striking.3\n",
      "There is one important difference, however. Variance normalization makes the\n",
      "components less sparse. In Fig. 9.5 we have plotted the histogram of the kurtoses of\n",
      "the independent components, estimated either with and without variance normaliza-\n",
      "tion. Variance normalization clearly reduces the average kurtosis of the components.\n",
      "The components after variance normalization correspond to estimates ˜si.\n",
      "This reduction in kurtosis is not very surprising if we recall the results in Sec-\n",
      "tion 7.8.3. There, it was shown that changing variance of gaussian variables is one\n",
      "mechanism for creating sparse variables. The variance variable in Equation (9.8)\n",
      "does exactly that. Here, the variance variable is the same for all components, but\n",
      "that does not change the situation regarding the marginal distributions of the single\n",
      "components: multiplication by the variance variable makes them more sparse. Thus,\n",
      "if we cancel the effect of the variance variable, it is natural that the components be-\n",
      "come less sparse.\n",
      "3 However, there is some difference as well: the vectors Ai now have some spurious oscillations.\n",
      "The reason for this phenomenon remains to be investigated.\n",
      "\n",
      "218\n",
      "9 Energy correlation of linear features & normalization\n",
      "Fig. 9.3: The whole set of detector weights Wi obtained by ICA after the variances have been\n",
      "normalized as in Equations (9.11) and (9.12).\n",
      "In practice, normalization of the image patches only reduces the variance depen-\n",
      "dencies but does not eliminate them. The process described above for modelling the\n",
      "variances of the components was only a very rough approximation. Let us do the\n",
      "same measurements on the variance dependencies that we did above before normal-\n",
      "ization. The results are shown in Figure 9.6. We see that energy correlations still\n",
      "remain, although they are now smaller.\n",
      "\n",
      "9.7 Concluding remarks and References\n",
      "219\n",
      "Fig. 9.4: The whole set of features Ai obtained by ICA after the variances have been normalized.\n",
      "9.7 Concluding remarks and References\n",
      "This chapter focused on the simple empirical fact that the “independent compo-\n",
      "nents” estimated from natural images are not independent. This seemingly para-\n",
      "doxical statement is due to the slightly misleading way the expression “indepen-\n",
      "dent component” is used in the context of ICA. While ICA ﬁnds the most indepen-\n",
      "dent components possible by a linear transformation, there is no guarantee that they\n",
      "would be completely independent. The dependencies observed in the case of natural\n",
      "images can be partly explained by the concept of a global variance variable which\n",
      "changes from patch to patch. Attempts to cancel the dependencies generated by such\n",
      "a changing variance lead to divisive normalization or gain control models. However,\n",
      "\n",
      "220\n",
      "9 Energy correlation of linear features & normalization\n",
      "a)\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "b)\n",
      "−5\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "Fig. 9.5: Histograms of kurtoses of independent components. a) estimated without variance nor-\n",
      "malization, b) estimated with variance normalization.\n",
      "a)\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "b)\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "c)\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "Fig. 9.6: Histograms of correlation coefﬁcients of nonlinear functions of independent components\n",
      "estimated with variance normalization. a) f (s) = |s|, b) f (s) = s2, c) f (s) is a thresholding function\n",
      "that gives 0 between -1 and 1 and gives 1 elsewhere. Compare with a)-c) in Fig. 9.2.\n",
      "\n",
      "9.8 Exercices\n",
      "221\n",
      "this is merely the beginning of a new research direction — modelling dependencies\n",
      "of “independent” components — which will be continued in the following chapters.\n",
      "The results in this chapter also point out an interesting, and very important,\n",
      "change in the relationship between sparseness and independence. With linear mod-\n",
      "els, maximization of sparseness is equivalent to maximization of independence,\n",
      "if the linear projections are sparse (super-gaussian). But in Section 9.6, we saw\n",
      "that divisive normalization increases independence, as measured by correlations of\n",
      "squares, while decreasing sparseness as measured by kurtosis. Thus, sparseness and\n",
      "independence have a simple relation in linear models only; with nonlinear process-\n",
      "ing, we cannot hope to maximize both simultaneously. This point is elaborated in\n",
      "(Lyu and Simoncelli, 2008); we will also return to this point in Section 17.2.1.\n",
      "A seminal work on normalizing images to get more gaussian distributions for\n",
      "the components is (Ruderman and Bialek, 1994b). Simoncelli and coworkers have\n",
      "proposed sophisticated methods for modelling variance dependencies in large im-\n",
      "ages. They start with a ﬁxed wavelet transform (which is similar to the ICA de-\n",
      "composition, see Section 17.3.2). Since the linear basis is ﬁxed, it is much easier to\n",
      "build further models of the wavelet coefﬁcients, which can then be used in contrast\n",
      "gain control (Schwartz and Simoncelli, 2001a; Schwartz et al, 2005). More com-\n",
      "plex model include hidden Markov models (Wainwright et al, 2001; Romberg et al,\n",
      "2001) as well as Markov Random Fields (Gehler and Welling, 2005; Lyu and Si-\n",
      "moncelli, 2007). A recent experimental work which considers different alternatives\n",
      "for the functional form of divisive normalization is (Bonin et al, 2006); the authors\n",
      "conclude that something like the division by the norm is a good model of contrast\n",
      "gain control in the cat’s LGN.\n",
      "Finally, let us mention that some work considers the statistical properties of il-\n",
      "lumination itself, before its interaction with objects (Dror et al, 2004; Mury et al,\n",
      "2007).\n",
      "Gain control phenomena can be found in many different parts of the visual sys-\n",
      "tem. Some of the earliest quantitative work on the effect of natural stimuli statistics\n",
      "considered gain control in the retina (Laughlin, 1981; van Hateren, 1992). Recent\n",
      "work on gain control in the LGN can be found in (Bonin et al, 2006).\n",
      "9.8 Exercices\n",
      "Mathematical exercises\n",
      "1. Show that if two components s1 and s2 are created using a variance variable\n",
      "as in Eq. (9.4), then also their absolute values have a positive correlation, i.e.\n",
      "cov(|s1|,|s2|) > 0 unless d is constant.\n",
      "2. Consider a variance variable d which only takes two values: α with probability\n",
      "1/2 and β with probability 1/2. Assume s1 and s2 follow Eq. (9.4) with gaussian\n",
      "˜s1 and ˜s2.\n",
      "\n",
      "222\n",
      "9 Energy correlation of linear features & normalization\n",
      "a. Show that for α,β > 0, the resulting joint pdf of (s1,s2) is a sum of two\n",
      "gaussian pdf’s.\n",
      "b. Take α = 0 and β > 0. What is the distribution now like? Can you write the\n",
      "pdf?\n",
      "Computer assignments\n",
      "1. Take some images and sample patches from them. Then, build two edge detectors\n",
      "in orthogonal orientations. Compute the outputs of both edge detectors for all the\n",
      "patches. Normalize the outputs to unit variance. What are the\n",
      "a. the ordinary covariances and correlation coefﬁcients of edge detector outputs\n",
      "b. the covariances and correlation coefﬁcients of the squares of the edge detector\n",
      "outputs?\n",
      "\n",
      "Chapter 10\n",
      "Energy detectors and complex cells\n",
      "In preceding chapters we considered only linear features. In this chapter, we intro-\n",
      "duce nonlinear features. There is an inﬁnite variety of different kinds of nonlinear-\n",
      "ities that one might use for computing such features. We will consider a very basic\n",
      "form inspired by models previously used in computer vision and neuroscience. This\n",
      "approach is based on the concepts of subspaces and energy detectors. The resulting\n",
      "model called “independent subspace analysis” gives nonlinear features which turn\n",
      "out to be very similar to complex cells when the parameters are learned from natural\n",
      "images.\n",
      "10.1 Subspace model of invariant features\n",
      "10.1.1 Why linear features are insufﬁcient\n",
      "In the previous chapters, we assumed that each feature is basically a linear entity.\n",
      "Linearity worked in two directions: The coefﬁcient (strength) si of the feature in\n",
      "the image was computed by a linear feature detector as in Equation (7.4) on page\n",
      "162, and the image was composed of a linear superposition of the features Ai, as in\n",
      "Equation (7.3) on page 161.\n",
      "A problem with linear features is that they cannot represent invariances. For\n",
      "example, an ideal complex cell gives the same response for a grating irrespective of\n",
      "its phase. A linear feature detector cannot have such behaviour, because the response\n",
      "of a linear system to a grating depends on the match between the phase of the input\n",
      "and the phase of the detector, as discussed in Section 2.2.3. In higher levels of the\n",
      "visual system, there are cells which respond to complex object parts irrespective of\n",
      "their spatial location. This cannot be very well described by a single linear template\n",
      "(feature detector), either.\n",
      "223\n",
      "\n",
      "224\n",
      "10 Energy detectors and complex cells\n",
      "10.1.2 Subspaces or groups of linear features\n",
      "Linear combinations are a ﬂexible tool that is capable of computing and representing\n",
      "invariant features. Let us consider a feature that consist of several vectors and all\n",
      "their linear combinations. Thus, one such feature corresponds to a group of simple\n",
      "linear features which are mixed together with some coefﬁcients:\n",
      "invariant feature = set of\n",
      "q\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si for all values of si\n",
      "(10.1)\n",
      "where q is the number of vectors in a single group. This grouping of components is\n",
      "closely related to the concept of subspace in linear algebra. The subspace spanned\n",
      "by the vectors Ai for i = 1,...,q is deﬁned as the set of all the possible linear combi-\n",
      "nations of the vectors. Thus, the range of values that the invariant feature represents\n",
      "is a subspace.\n",
      "The point is that each such subspace is representing an invariant feature by taking\n",
      "different linear combinations of the vectors. Let us consider, for example, the prob-\n",
      "lem of constructing a detector for some shape, so that the output of the detector does\n",
      "not depend on location of that shape, i.e. it detects whether the shape occurs any-\n",
      "where in the image. You could approach the problem by linear vectors (templates)\n",
      "Ai that represent the shape in many different locations. Now, if these vectors are\n",
      "dense enough in the image space, so that all locations are covered, the occurrence\n",
      "of the shape in any location can be represented as a trivial linear combination of the\n",
      "templates: take the coefﬁcient at the right location to be equal to one, and all the\n",
      "other coefﬁcients si to be zero. Thus, the subspace can represent the shape in a way\n",
      "that is invariant with respect to the location, i.e. it does not depend on the location.\n",
      "In fact, subspaces are even more powerful, because to represent a shape that is not\n",
      "exactly in the locations given by the basic vectors, a satisfactory representation can\n",
      "often be obtained by taking the average of two or more templates in nearby locations\n",
      "(say, just to the left and just to the right of the actual location). It is this capacity of\n",
      "interpolation that makes the linear subspace representation so useful.\n",
      "The whole image (patch) can be expressed as a linear superposition of these\n",
      "nonlinear features. Let us denote by S(k) the set of indices i of those Ai that belong\n",
      "to the k-th group or subspace; for example, if all the subspaces have dimension\n",
      "two, we would have S(1) = {1,2},S(2) = {3,4} etc. Thus, we obtain the following\n",
      "model\n",
      "I(x,y) = ∑\n",
      "k ∑\n",
      "i∈S(k)\n",
      "Ai(x,y)si\n",
      "(10.2)\n",
      "The image is still a linear superposition of the vectors Ai(x,y), but the point is that\n",
      "these vectors are grouped together. The grouping is useful in deﬁning nonlinear\n",
      "feature detectors, as shown next.\n",
      "\n",
      "10.1 Subspace model of invariant features\n",
      "225\n",
      "10.1.3 Energy model of feature detection\n",
      "In Equation (10.2), there was no quantity that would directly say what the strength\n",
      "(value, output) of a subspace feature is. It is, of course, important to deﬁne such a\n",
      "measure, which is the counterpart of the si in the case of simple linear features. We\n",
      "now deﬁne the value of the feature, i.e. the output of a feature detector as a particular\n",
      "nonlinear function of the input image patch. We shall denote it by ek.\n",
      "First of all, since the model in Equation (10.2) is still a linear superposition\n",
      "model, we can invert the system given by the vectors Ai as in the linear case. To\n",
      "be able to do this easily, we assume, as in ICA, that the total number of vectors Ai\n",
      "is equal to the number of pixels (alternatively, equal to the number of dimensions\n",
      "after canonical preprocessing, as discussed below). Then, each linear coefﬁcient si\n",
      "can be computed by inverting the system, just as in equation (7.6):\n",
      "si = ∑\n",
      "x,y\n",
      "Wi(x,y)I(x,y).\n",
      "(10.3)\n",
      "for some detector weights Wi which are obtained just as with any linear image model\n",
      "(e.g. the ICA model).\n",
      "Now, how do we compute the strength of the subspace feature as a function of the\n",
      "coefﬁcients si that belong to that subspace? There are several reasons for choosing\n",
      "the square root of the sum of the squares:\n",
      "ek =\n",
      "s\n",
      "∑\n",
      "i∈S(k)\n",
      "s2\n",
      "i\n",
      "(10.4)\n",
      "The ﬁrst reason for using this deﬁnition is that the sum of squares is related to the\n",
      "norm of the vector ∑i∈S(k) Aisi. In fact, if the Ai form an orthogonal matrix i.e. they\n",
      "are orthogonal and have norms equal to one, the square root of the sum of squares\n",
      "is equal to the norm of that vector. The norm of the vector is an obvious measure\n",
      "of its strength; here it can be interpreted as the “total” response of all the si in the\n",
      "subspace.\n",
      "Second, a meaningful measure of the strength ek of a subspace feature in an\n",
      "image I could be formed by computing the distance of I from the best approximation\n",
      "(projection) that the subspace feature is able to provide:\n",
      "min\n",
      "si,i∈S(k)∑\n",
      "x,y\n",
      "[I(x,y)−∑\n",
      "i∈S(k)\n",
      "siAi(x,y)]2\n",
      "(10.5)\n",
      "Again, if the Ai form an orthogonal matrix, this can be shown to be equal to\n",
      "∑\n",
      "x,y\n",
      "I(x,y)2 −∑\n",
      "i∈S(k)\n",
      "s2\n",
      "i\n",
      "(10.6)\n",
      "which is closely related to the sum of squares, because the ﬁrst term does not depend\n",
      "on the coefﬁcients si at all.\n",
      "\n",
      "226\n",
      "10 Energy detectors and complex cells\n",
      "Third, a sum of squares is often used in Fourier analysis: the “energy” in a given\n",
      "frequency band is usually computed as the sum of squares of the Fourier coefﬁcients\n",
      "in the band. This is why a feature detector using sum of squares is often called an\n",
      "energy detector. Note, however, that the connection to energy in the sense of physics\n",
      "is quite remote.\n",
      "Fourth, the sum of squares seems to be a good model of the way complex cells\n",
      "in the visual cortex compute their outputs from outputs of simple cells, see Sec-\n",
      "tion 3.4.2. In the physiological model, a square root is not necessarily taken, but the\n",
      "basic idea of summing the squares is the same. In that context, the summation is\n",
      "often called “pooling”.\n",
      "Note that we could equally well talk about linear combinations of linear feature\n",
      "detectors Wi instead of linear combinations of the Ai. For an orthogonal basis, these\n",
      "are essentially the same thing, as shown in Section 19.6. Then, linear combinations\n",
      "of the Wi in the same subspace give the forms of all possible linear feature detectors\n",
      "associated with that subspace.\n",
      "Figure 10.1 illustrates such an energy pooling (summation) model.\n",
      "I\n",
      "Input\n",
      "8\n",
      "<w  , I>\n",
      "7\n",
      "<w  , I>\n",
      "(.)\n",
      "2\n",
      "(.)\n",
      "2\n",
      "6\n",
      "<w  , I>\n",
      "2\n",
      "<w  , I>\n",
      "1\n",
      "<w  , I>\n",
      "3\n",
      "<w  , I>\n",
      "5\n",
      "<w  , I>\n",
      "4\n",
      "<w  , I>\n",
      "(.)\n",
      "2\n",
      "(.)\n",
      "2\n",
      "(.)\n",
      "Σ\n",
      "2\n",
      "(.)\n",
      "2\n",
      "(.)\n",
      "2\n",
      "2\n",
      "(.)\n",
      "Σ\n",
      "Fig. 10.1: Illustration of computation of complex cell outputs by pooling squares of linear feature\n",
      "detectors. From (Hyv¨arinen and Hoyer, 2000), Copyright c⃝2000 MIT Press, used with permis-\n",
      "sion.\n",
      "\n",
      "10.2 Maximizing sparseness in the energy model\n",
      "227\n",
      "Canonically preprocessed data\n",
      "Invariant features can be directly applied to data whose dimension has been re-\n",
      "duced. Just as in the case of a basic linear decomposition, we can simply formu-\n",
      "late the linear model as in Equation (10.2) where the data on the left-hand side is\n",
      "the preprocessed data zi, and the linear feature vectors are in the reduced space.\n",
      "Nothing is changed in the concept of subspaces. Likewise, the energy detector in\n",
      "Equation (10.4) takes the same form.\n",
      "10.2 Maximizing sparseness in the energy model\n",
      "10.2.1 Deﬁnition of sparseness of output\n",
      "What we are now going to show is that we can learn invariant features from natu-\n",
      "ral images by maximization of sparseness of the energy detectors ek given by the\n",
      "subspace model. Sparseness can be measured in the same way as in the linear case.\n",
      "That is, we consider the expectation of a convex function of the square of the detec-\n",
      "tor output.\n",
      "First of all, we take a number of linear features that span a feature subspace. To\n",
      "keep things simple, let us take just two in the following. Let us denote the detector\n",
      "weight vectors, which work in the reduced space after canonical preprocessing, by\n",
      "v1 and v2. Since we are considering a single subspace, we can drop the index i of\n",
      "the subspace. So, what we want to maximize is a measure of sparseness of the form\n",
      "E{h(e2)} = E\n",
      "\b\n",
      "h((vT\n",
      "1 z)2 +(vT\n",
      "2 z)2)\n",
      "\t\n",
      "= E\n",
      "(\n",
      "h((\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "v1 jzj)2 +(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "v2 jzj)2)\n",
      ")\n",
      "(10.7)\n",
      "where h is a convex function just as in the linear feature case.\n",
      "An important point that must be considered is how the relation between v1 and\n",
      "v2 should be constrained. If they are not constrained at all, it may easily happen that\n",
      "these two linear detectors end up being the equal to each other. Then we lose the\n",
      "capability of representing a subspace. Mathematically speaking, such a situation is\n",
      "violating our assumption that the linear system given by the vectors Ai is invertible,\n",
      "because this assumption implies that the Wi (or the vi) are not linear combinations\n",
      "of each other.\n",
      "We constrain here v1 and v2 in the same way as in the linear feature case: the\n",
      "outputs of the linear feature detectors must be uncorrelated:\n",
      "E{(vT\n",
      "1 z) (vT\n",
      "2 z)} = 0\n",
      "(10.8)\n",
      "and, as before, we also constrain the output variances to be equal to one:\n",
      "E{(vT\n",
      "i z)2} = 1, for i = 1,2\n",
      "(10.9)\n",
      "\n",
      "228\n",
      "10 Energy detectors and complex cells\n",
      "Now, we can maximize the function in Equation (10.7) under the constraints in\n",
      "Equations (10.8) and (10.9).\n",
      "10.2.2 One feature learned from natural images\n",
      "To give some preview of what this kind of analysis means in practice, we show the\n",
      "results of estimation of a single four-dimensional subspace from natural images.\n",
      "The four vectors vi, converted back to the original image space (inverting the pre-\n",
      "processing), are shown in Figure 10.2.\n",
      "Fig. 10.2: A group of weight vectors Wi found by maximization of the nonlinear energy detector in\n",
      "natural images.\n",
      "What is the invariance represented by the subspace like? A simple way to analyze\n",
      "this is to plot a lot of linear combinations of the weight vectors Wi belonging to the\n",
      "same subspace. Thus, we see many instances of the different features that together\n",
      "deﬁne the invariant feature. This is shown in Fig. 10.3 for the weight vectors in\n",
      "Fig. 10.2, using random coefﬁcients inside the subspace.\n",
      "Fig. 10.3: Random combinations of the weight vectors Wi in the subspace shown in Fig. 10.2. These\n",
      "combinations are all particular instances of the feature set represented by the invariant feature\n",
      "The resulting invariance has a simple interpretation: The invariant feature ob-\n",
      "tained by the algorithm is maximally invariant with respect to the phase of the in-\n",
      "put. This is because all the four linear features Wi are similar to Gabor functions\n",
      "which have quite similar parameters otherwise, but with the major difference that\n",
      "the phases of the underlying oscillations are quite different. In the theory of space-\n",
      "frequency analysis (Section 2.4), and in complex cell models (Section 3.4.2), invari-\n",
      "\n",
      "10.3 Model of independent subspace analysis\n",
      "229\n",
      "ance to phase is achieved by using two different linear feature detectors which are\n",
      "in quadrature-phase (as sine and cosine functions). Here, we have four linear feature\n",
      "detectors, but the basic principle seems to be the same.\n",
      "Such phase-invariance does, in fact, practically always emerge for the feature\n",
      "subspaces estimated from natural image data, see Section 10.7 for a more detailed\n",
      "analysis. The invariant features are thus similar to complex cells in the visual cor-\n",
      "tex. This invariance appears because the linear features in the same subspace have\n",
      "similar orientations, and frequencies, whereas they have quite different phases, and\n",
      "slightly different positions. Note that it is not easy to distinguish the effects of dif-\n",
      "ferent phases and slightly different positions, since they result in very much the\n",
      "same transformations in the overall shape of the features (something that looks like\n",
      "a small displacement of the feature).\n",
      "These results indicate that from a statistical viewpoint, the invariance to phase\n",
      "is a more important feature of natural images that, say, invariance to orientation.\n",
      "Such invariance to phase has been considered very important in visual neuroscience\n",
      "because it is the function usually attributed to complex cells: phase-invariance is the\n",
      "hallmark property that distinguished simple and complex cells.\n",
      "To see that this “emergence” of phase-invariant features is not self-evident, we\n",
      "can consider some alternatives. A well-known alternative would be a feature sub-\n",
      "space invariant to orientation, called “steerable ﬁlters” in computer vision. Actually,\n",
      "by taking a subspace of Gabor-like vectors that are similar in all other parameters\n",
      "than orientation, one can obtain exactly orientation-invariant features (see Refer-\n",
      "ences and Exercises sections below). What our results show is that in representing\n",
      "natural images, invariance with respect to phase is more important in the sense that\n",
      "it gives a better statistical model of natural images. This claim will be justiﬁed in\n",
      "the next section, where we build a proper probabilistic model based on sparse, inde-\n",
      "pendent subspaces.\n",
      "10.3 Model of independent subspace analysis\n",
      "Maximization of sparseness can be interpreted as estimation of a statistical model\n",
      "just as in the case of linear features. Assume that the pdf of the sk is of the following\n",
      "form:\n",
      "log p(s1,...,sn) = ∑\n",
      "k\n",
      "h(e2\n",
      "k) = ∑\n",
      "k\n",
      "h( ∑\n",
      "i∈S(k)\n",
      "s2\n",
      "i )\n",
      "(10.10)\n",
      "(A constant needs to be added to make this a proper pdf if the function h is not prop-\n",
      "erly normalized, but it has little signiﬁcance in practice.) Denote by zt,t = 1,...,T a\n",
      "set of observed image patches after preprocessing. Then, the likelihood of the model\n",
      "can be obtained in very much the same way as in the case of ICA in Equation (7.15)\n",
      "on page 168. The log-likelihood is given by\n",
      "\n",
      "230\n",
      "10 Energy detectors and complex cells\n",
      "logL(v1,...,vn) = T log|det(V)|+∑\n",
      "k\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "h( ∑\n",
      "i∈S(k)\n",
      "(vT\n",
      "i zt)2)\n",
      "(10.11)\n",
      "Again, if we constrain the si to be uncorrelated and of unit variance, which is equiv-\n",
      "alent to orthogonality of the matrix V, the term log|det(V)| is constant. The re-\n",
      "maining term is just the sum of the sparseness measures of all the energy detectors.\n",
      "Thus, we see that maximization of the sparsenesses is equivalent to estimation of\n",
      "the statistical generative model by maximization of likelihood.\n",
      "As a concrete example, let us consider the case of two-dimensional subspaces,\n",
      "and choose h(y) = −√y. This deﬁnes a distribution inside each subspace for which\n",
      "log p(si,sj) = −\n",
      "q\n",
      "s2\n",
      "i +s2\n",
      "j. If we further normalize this pdf so that its integral is equal\n",
      "to one, and so that si and sj have unit variance, we get the following pdf for si and\n",
      "sj in the same subspace:\n",
      "p(si,sj) = 2\n",
      "3π exp(−\n",
      "√\n",
      "3\n",
      "q\n",
      "s2\n",
      "i +s2\n",
      "j)\n",
      "(10.12)\n",
      "This could be considered as a two-dimensional generalization of the Laplacian dis-\n",
      "tribution. If you assume sj is given as sj = 0, the conditional pdf of si is proportional\n",
      "to exp(−\n",
      "√\n",
      "3\n",
      "q\n",
      "s2\n",
      "i ), which is as in the Laplacian pdf in Equation (7.18) up to some\n",
      "scaling constants.\n",
      "What is the main difference between this statistical model and ICA? In ICA,\n",
      "the pdf was derived using the assumption of independence of the components si.\n",
      "Since we have here a rather different model, it must mean that some statistical de-\n",
      "pendencies exist among the components. In fact, the pdf above corresponds to a\n",
      "model where the nonlinear features ek are independent, but the components (i.e.\n",
      "linear features) in the same subspace are not. The independence of the nonlinear\n",
      "features can be seen from the fact that the log-density in Equation (10.10) is a sum\n",
      "of functions of the nonlinear features. By deﬁnition, the nonlinear features are then\n",
      "independent. This also implies that two components in two different subspaces are\n",
      "independent. Since the subspaces are independent in these two ways, this model is\n",
      "called independent subspace analysis (ISA).\n",
      "The more difﬁcult question is: What kind of dependencies exist between the com-\n",
      "ponents in a single subspace? This will be considered next.\n",
      "10.4 Dependency as energy correlation\n",
      "The basic result is that in the ISA model, the dependencies of the linear components\n",
      "in the same subspace take the form of energy correlations already introduced in\n",
      "Chapter 9. This result will be approached from different angles in the following.\n",
      "\n",
      "10.4 Dependency as energy correlation\n",
      "231\n",
      "10.4.1 Why energy correlations are related to sparseness\n",
      "To start this investigation on the statistical dependencies of components in ISA, we\n",
      "consider a simple intuitive explanation of why the sparseness of energy detectors is\n",
      "related to the correlations of energies of the underlying linear features.\n",
      "Let us consider the following two cases. First, consider just two linear feature\n",
      "detectors which have the same output distributions, and whose output energies are\n",
      "summed (pooled) in a nonlinear energy detector. If the outputs are statistically inde-\n",
      "pendent, the pooling reduces sparseness. This is because of the fundamental result\n",
      "given by the Central Limit Theorem (see Section 7.9.1). It says, roughly speaking,\n",
      "that the sum of independent random variables is closer to gaussian (and therefore,\n",
      "less sparse) than the original random variables themselves.\n",
      "Second, consider the contrasting extreme case where the linear detector outputs\n",
      "are perfectly dependent, that is, equal. This means that the distribution of the pooled\n",
      "energies is equal to the distribution of the original energies (up to a scaling constant),\n",
      "and therefore there is no reduction in sparseness.\n",
      "So, we see that maximization of the sparseness of the energy is related to maxi-\n",
      "mization of the energy correlations (dependencies) of the underlying linear features.\n",
      "10.4.2 Spherical symmetry and changing variance\n",
      "Next, we show how the ISA pdf can be interpreted in terms of a variance variable,\n",
      "already used in Chapter 9.\n",
      "The distribution inside each subspace, as deﬁned by Eq. (10.10), has the distin-\n",
      "guishing property of being spherically symmetric. This simply means that the pdf\n",
      "depends on the norm\n",
      "q\n",
      "∑i∈S(k) s2\n",
      "i only. Then, any rotation (orthogonal transforma-\n",
      "tion) of the variables in the subspace has exactly the same distribution.\n",
      "Spherically symmetric distributions constitute one of the simplest models of non-\n",
      "linear correlations. If h is nonlinear, the variables in the same subspace are depen-\n",
      "dent. In contrast, an important special case of spherically symmetric distributions is\n",
      "obtained when h(u) = u, in which case the distribution is just the ordinary gaussian\n",
      "distribution with no dependencies or correlations.\n",
      "Spherical symmetry is closely related to the the model in which a separate vari-\n",
      "ance variable multiplies two (or more) independent variables as in Equation (9.4)\n",
      "on page 213. If the independent variables ˜s1 and ˜s2 are gaussian, the distribution of\n",
      "the vector (s1,s2) is spherically symmetric. To show this, we use the basic principle\n",
      "that the marginal pdf of the vector (s1,s2) can be computed by integrating the joint\n",
      "pdf of (s1,s2,d) over d. First note that we have ˜s2\n",
      "i = s2\n",
      "i /d2. Since the ˜si are gaussian\n",
      "and independent (let us say they have unit variance), and independent of d, the pdf\n",
      "can be computed as:\n",
      "p(s1,s2) =\n",
      "Z\n",
      "p(s1,s2,d)dd =\n",
      "Z\n",
      "1\n",
      "2πd2 exp(−s2\n",
      "1 +s2\n",
      "2\n",
      "2d2 )p(d)dd\n",
      "(10.13)\n",
      "\n",
      "232\n",
      "10 Energy detectors and complex cells\n",
      "Even without actually computing the integral (“integrating d out”) in this formula,\n",
      "we see that the pdf only depends on the (square of the) norm s2\n",
      "1 + s2\n",
      "2. Thus, the\n",
      "distribution is spherically symmetric. This is because the distribution of (˜s1, ˜s2) was\n",
      "spherically symmetric to begin with. The distribution of d, given by p(d) in the\n",
      "equation above, determines what the distribution of the norm is like.\n",
      "In the model estimation interpretation, h is obtained as the logarithm of the pdf,\n",
      "when it is expressed as a function of the square of the norm. Thus, based in Equa-\n",
      "tion (10.13) we have\n",
      "h(e2) = log\n",
      "Z\n",
      "1\n",
      "2πd2 exp(−e2\n",
      "2d2 )p(d)dd\n",
      "(10.14)\n",
      "Note that we obtain a spherically symmetric distribution only if the ˜si are gaus-\n",
      "sian, because only gaussian variables can be both spherically symmetrically dis-\n",
      "tributed and independent. In Chapter 9, we did not assume that the ˜si are gaussian;\n",
      "in fact, when we normalized the data we saw the the estimated ˜si are still quite\n",
      "super-gaussian. This apparent contradiction arises because in the ISA model, we\n",
      "have a different variance variable dk for each subspace, whereas in Chapter 9 there\n",
      "was only one d for the whole image patch. If we estimated the ˜si in the ISA model,\n",
      "their distributions would presumably be much closer to gaussian than in Chapter 9.\n",
      "10.4.3 Correlation of squares and convexity of nonlinearity\n",
      "Next we consider the role of the nonlinearity h in Equation (10.11). In the model\n",
      "developed in this chapter, we don’t have just any h, but h is assumed to be convex\n",
      "because we are considering measures of sparseness. Actually, it turns out that the\n",
      "h that can be derived from the model with a variance variable as in the preceding\n",
      "section, are necessarily convex. A detailed mathematical analysis of this connection\n",
      "is given in Section 10.8.\n",
      "Conversely, if we deﬁne the pdf inside a subspace by taking a convex function h\n",
      "of the square of the norm, we usually get a positive covariance between the squares\n",
      "of the components. Again, a detailed mathematical analysis of this connection is\n",
      "given in Section 10.8, but we will discuss this connection here with an example.\n",
      "As an illustrative example, consider two-dimensional subspaces with pdf deﬁned\n",
      "as in Equation (10.12). The covariance of the squares of si and sj can be calculated,\n",
      "it is equal to 2/3. The kurtosis of either si or sj is equal to 2, and the variables\n",
      "are uncorrelated. (This density has been standardized to that its mean is zero and\n",
      "variance equal to one.) Using this pdf we can investigate the conditional distribution\n",
      "of sj for a given si:\n",
      "p(sj|si) = p(si,sj)\n",
      "p(si)\n",
      "=\n",
      "p(si,sj)\n",
      "R p(si,sj)dsj\n",
      "(10.15)\n",
      "\n",
      "10.4 Dependency as energy correlation\n",
      "233\n",
      "This can be easily computed for our pdf, and is plotted in Fig, 10.4 a). We see a shape\n",
      "that has been compared to a bow-tie: when going away from zero on the horizontal\n",
      "axis (si), the distribution on the vertical axis (sj) becomes wider and wider, i.e. its\n",
      "variance grows. This can be quantiﬁed by the conditional variance\n",
      "var(sj|si) =\n",
      "Z\n",
      "s2\n",
      "j p(sj|si)dsj\n",
      "(10.16)\n",
      "The actual conditional variance of si, given sj is shown in Fig. 10.4 b). We see that\n",
      "the conditional variance grows with the absolute value of si.\n",
      "What is the connection to energy correlations? Both increasing conditional vari-\n",
      "ance and energy correlations try to formalize the same intuitive idea: when one of\n",
      "the variables has a large absolute values, the other(s) is likely to have a large abso-\n",
      "lute values as well. Correlations of squares or energies is something we can easily\n",
      "compute, whereas conditional variance is a more faithful formalization of the same\n",
      "idea.\n",
      "Thus, we see that taking a convex h, or assuming the data to come from the\n",
      "variance variable model (with gaussian “original” variables ˜si) are closely related.\n",
      "a)\n",
      "−1.2\n",
      "−0.4\n",
      "0.4\n",
      "1.2\n",
      "2\n",
      "1.2\n",
      "0.4\n",
      "−0.4\n",
      "−1.2\n",
      "−2\n",
      "b)\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "Fig. 10.4: Illustration of the correlation of squares in the probability density in Equation (10.12).\n",
      "a) The two-dimensional conditional density of sj (vertical axis) given si (horizontal axis). The\n",
      "conditional density is obtained by taking vertical slices of the density function, and then normal-\n",
      "izing each slice so that it integrates to one, and thus deﬁnes a proper probability density function.\n",
      "Black means low probability density and white means high probability density. We see that the\n",
      "conditional distribution get broader as si goes further from zero in either direction. This leads to\n",
      "correlation of energies since the expectation of the square is nothing but the variance. b) The condi-\n",
      "tional variance of sj (vertical axis) for a given si (horizontal axis). Here we see that the conditional\n",
      "variance grows with the square (or absolute value) of si.\n",
      "\n",
      "234\n",
      "10 Energy detectors and complex cells\n",
      "10.5 Connection to contrast gain control\n",
      "Both the ISA model and the model we used to motivate divisive normalization in\n",
      "Chapter 9 lead to a similar kind of dependency. This may give the impression that\n",
      "the two are actually modelling the same thing. This is not so because in ISA, the\n",
      "variance variables d are different for each subspace, whereas in the contrast gain\n",
      "control there was a single d for the whole patch.\n",
      "In the ISA model, the variance variables di are actually closely related to the out-\n",
      "puts of nonlinear feature detectors. The sum of squares of the si inside one subspace\n",
      "(or rather, that sum divided by the dimension of the subspace) can be considered\n",
      "a very crude estimator of the d2\n",
      "i of that subspace, because, in general, the average\n",
      "of squares is an estimator of variance. No such interpretation of d can be made in\n",
      "the contrast gain control context, where the single d is considered an uninteresting\n",
      "“nuisance parameter”, something whose inﬂuence we want to cancel.\n",
      "Although the contrast gain control models could be generalized to the case where\n",
      "the patch is modelled using several variance variables, which possibly control the\n",
      "variances in different parts of the patch due to different illumination conditions,\n",
      "the basic idea is still that in ISA, there are many more energy detectors than there\n",
      "are variance variables due to illumination conditions in the contrast gain model in\n",
      "Chapter 9.\n",
      "Because the dependencies in the two models are so similar, one could envision\n",
      "a single model that encompasses both models. Steps towards such a model are dis-\n",
      "cussed in Section 11.8. On the other hand, we can use ISA to model the energy\n",
      "correlations that remain in the images after divisive normalization. In the image ex-\n",
      "periments below, we ﬁrst reduce energy correlation by divisive normalization using\n",
      "Equation (9.11), and then model the data by ISA. This has two different motivations:\n",
      "1. We want to model energy correlations, or in general the statistical structure of\n",
      "images, as precisely as possible. So it makes sense to ﬁrst reduce the overall value\n",
      "of energy correlations to be able better to see ﬁne details. This can be compared\n",
      "with removal of the DC component, which makes the details in second-order\n",
      "correlations more prominent. Just like in ICA, one ﬁnds that the dependencies\n",
      "between the subspaces are reduced by divisive normalization, so the ISA model\n",
      "is then simply a better model of image data.\n",
      "2. On a more intuitive level, one goal in image modelling is to ﬁnd some kind of\n",
      "“original” independent features. Reducing the dependencies of linear features by\n",
      "divisive normalization seems a reasonable step toward such a goal.\n",
      "10.6 ISA as a nonlinear version of ICA\n",
      "It is also possible to interpret the ISA model of independent subspace analysis as a\n",
      "nonlinear invertible transformation of the data. Obviously the transformation is non-\n",
      "linear, but how can we say that it is invertible? The point is to consider not just the\n",
      "\n",
      "10.7 Results on natural images\n",
      "235\n",
      "norms of the coefﬁcients in the subspaces, but also the angles inside the subspaces.\n",
      "That is, we look at what is called the polar coordinates inside each subspace. For\n",
      "simplicity, let us consider just two-dimensional subspaces, although this discussion\n",
      "also applies in higher-dimensional subspaces.\n",
      "The point is that if we express the coordinates s1,s2 in a two-dimensional sub-\n",
      "space as a function of the norm r =\n",
      "q\n",
      "s2\n",
      "1 +s2\n",
      "2 and the angle θ = arctans2/s1 with\n",
      "respect to one of the axes. This is an invertible transformation; the inverse is given\n",
      "by s1 = rcosθ and s2 = rsinθ.\n",
      "The fundamental point is that the two variables r and θ are independent under the\n",
      "ISA model. This is precisely because of the assumption that the pdf is spherically\n",
      "symmetric, i.e. it depends on the norm only. Intuitively, this is easy to see: since\n",
      "the pdf only depends on the norm, it can be factorized, as required by the deﬁni-\n",
      "tion of statistical independence, to two factors. The ﬁrst depends on the norm only,\n",
      "and the second, completely trivial factor is equal to 1. The constant factor can be\n",
      "interpreted to be a (constant) function of θ, corresponding to a uniform distribution\n",
      "of the angles. So, we see that the pdf can be factorized into a product of a function\n",
      "of r and a function θ, which proves the independence. (Note that this proof is not\n",
      "quite correct because we have to take into account the determinant of the Jacobian,\n",
      "as always when we transform pdf’s. The rigorous proof is left as an exercise for\n",
      "mathematically very sophisticated readers.)\n",
      "Thus we see that we can think of the generative model of ISA as a nonlinear,\n",
      "invertible transformation, which is, in the case of two-dimensional subspaces, as\n",
      "follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r1\n",
      "θ1\n",
      "...\n",
      "rn/2\n",
      "θn/2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "→\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "z1\n",
      "z2\n",
      "...\n",
      "zn−1\n",
      "zn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(10.17)\n",
      "where the components on the left-hand-side are all independent from each other.\n",
      "The same idea holds for subspaces of any dimensions; we just need to parameterize\n",
      "arbitrary rotations in those subspaces (which is rather complicated).\n",
      "10.7 Results on natural images\n",
      "10.7.1 Emergence of invariance to phase\n",
      "10.7.1.1 Data and preprocessing\n",
      "We took the same 50,000 natural image patches of size 32×32 as in the ICA case.\n",
      "We performed contrast gain control by divisive normalization as in Equation (9.11),\n",
      "\n",
      "236\n",
      "10 Energy detectors and complex cells\n",
      "as motivated in Section 10.5. Then, we preprocessed the normalized patches in the\n",
      "same (“canonical”) way as with ICA, reducing the dimension to 256.\n",
      "The nonlinearity h used in the likelihood or sparseness measure was chosen to\n",
      "be a smoothed version of the square root as in Eq. (6.14) on page 143. We then esti-\n",
      "mated the whole set of feature subspaces using subspace size 4 for natural images,\n",
      "which means 64 = 256/4 subspaces.\n",
      "10.7.1.2 Features obtained\n",
      "The results are shown in Fig. 10.5 and Fig. 10.6 for the Wi and the Ai, respectively.\n",
      "Again, the feature detectors are plotted so that the grey-scale value of a pixel means\n",
      "the value of the coefﬁcient at that pixel. Grey pixels mean zero coefﬁcients. As with\n",
      "linear independent components, the order of the subspaces is not deﬁned by the\n",
      "model. For further analysis, the subspaces are ordered according to the sparsenesses\n",
      "of the subspaces as measure by the term ∑T\n",
      "t=1 h(∑i∈S(k)(vT\n",
      "i zt)2) in the likelihood.\n",
      "Visually, one can see that these feature detectors have interesting localization\n",
      "properties. First, they are localized in space: most of the coefﬁcients are practically\n",
      "zero outside of a small receptive ﬁeld. This is true of the individual feature detectors\n",
      "in the same way as in the case of linear feature detectors estimated by sparse coding\n",
      "or ICA. What is important here is that it is also true with respect to the whole sub-\n",
      "space, because the non-zero coefﬁcients are more or less in the same spatial location\n",
      "for all feature detectors corresponding to the same subspace. The linear feature de-\n",
      "tectors and the invariant features are also oriented and multiscale in exactly the same\n",
      "way: the optimal orientations and frequencies seem to be the same for all the linear\n",
      "features in the same subspace.\n",
      "10.7.1.3 Analysis of tuning and invariance\n",
      "We can analyze these features further by ﬁtting Fourier gratings, just as in Sec-\n",
      "tion 6.4. In determining the optimal orientation and frequency for a subspace, we\n",
      "ﬁnd the grating that has maximum energy response, i.e. maximum sum of squares of\n",
      "linear dot-products inside the subspace. The analysis is made a bit more complicated\n",
      "by the fact that for these nonlinear features, we cannot ﬁnd the maximum response\n",
      "over all phases by using two ﬁlters in quadrature-phase and taking the square of the\n",
      "responses as we did in Section 6.4. We have to compute the responses over different\n",
      "values of orientation, frequency and phase. Thus we take many different values of\n",
      "α,β and θ in\n",
      "f(x,y) = sin(2πα(sin(θ)x+cos(θ)y)+β)\n",
      "(10.18)\n",
      "Then we compute the responses of the energy detectors and ﬁnd the α,θ that max-\n",
      "imize the sum of responses over the different β for each subspace.\n",
      "We can then investigate the selectivities of the features by changing one of the\n",
      "parameters, while the others are ﬁxed to the optimal values. This gives the tuning\n",
      "\n",
      "10.7 Results on natural images\n",
      "237\n",
      "Fig. 10.5: The whole set of vectors Wi obtained by independent subspace analysis. The four vectors\n",
      "in the same subspace are shown consecutively on the same row. The subspaces have been ordered\n",
      "so that the sparsest ones are ﬁrst (top rows).\n",
      "curves for each of the parameters. Note that when computing the responses for vary-\n",
      "ing orientation or frequency, we again take the sum over all possible phases to sim-\n",
      "ulate the total response to a drifting grating. On the other hand, when we compute\n",
      "the tuning curve for phase, we do not take a sum over different phases.\n",
      "In Figure 10.7 we have show the results of the analysis for the ﬁrst ten (i.e. the ten\n",
      "sparsest) subspaces in Fig. 10.5. We can clearly see that estimated energy detectors\n",
      "are still selective to orientation and frequency. However, they are less selective to\n",
      "phase. Some of the features are rather completely insensitive to phase, whereas in\n",
      "other, some selectivity is present. This shows that the model successfully produces\n",
      "the hallmark property of complex cells: invariance to phase—at least in some of the\n",
      "\n",
      "238\n",
      "10 Energy detectors and complex cells\n",
      "Fig. 10.6: The whole set of vectors Ai obtained by independent subspace analysis.\n",
      "cells.1 Thus, the invariance and selectivities that emerge from natural images by ISA\n",
      "is just the same kind that characterize complex cells.\n",
      "The selectivity to orientation and frequency is a simple consequence of the fact\n",
      "that the orientation and frequency selectivities of the underlying linear feature de-\n",
      "tectors are similar in a given subspace. This can be analyzed in more detail by vi-\n",
      "sualizing the correlations of the optimal parameters for two linear features in the\n",
      "1 It should be noted that the invariance to phase of the sum of squares of linear ﬁlter responses\n",
      "is not an interesting property in itself. Even taking receptive ﬁelds with random coefﬁcients gives\n",
      "similar phase-response curves as in Fig. 10.7 for the sum of squares. This is because the phase-\n",
      "responses are always sinusoids, and so are their squares, so if the phases of different ﬁlters are\n",
      "different enough, their sum often ends up being relatively constant. What is remarkable, and needs\n",
      "sophisticated learning, is the combination of selectivity to orientation and frequency with phase-\n",
      "invariance.\n",
      "\n",
      "10.7 Results on natural images\n",
      "239\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Fig. 10.7: Tuning curves of the ISA features Wi. Left: change in frequency (the unit is relative to the\n",
      "window size of 32 pixels, so that 16 means wavelength of 2 pixels). Middle: change in orientation.\n",
      "Right: change in phase.\n",
      "\n",
      "240\n",
      "10 Energy detectors and complex cells\n",
      "same subspace. In Fig. 10.8 we see that the orientations (b) are strongly correlated.\n",
      "In the case of frequencies the correlation is more difﬁcult to see because of the over-\n",
      "all concentration to high frequencies. As for phases, no correlation (or any kind of\n",
      "statistical dependency) can be seen.\n",
      "In this analysis, it is important to reduce the dimension by PCA quite a lot. This is\n",
      "because as explained in Section 5.3.3.2, the phase is not a properly deﬁned quantity\n",
      "at the very highest frequencies (the Nyquist frequency) that can be represented by\n",
      "the sampling lattice, i.e. the pixel resolution.\n",
      "a)\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "b)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "2.5\n",
      "3\n",
      "c)\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "d)\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Fig. 10.8: Correlation of parameters characterizing the linear features in the same independent\n",
      "subspace. In each plot, we have divided the subspaces into two pairs, and plotted the optimal\n",
      "parameter values for the two linear features in a scatter plot. a) scatter plot of frequencies, b)\n",
      "scatter plot of orientations, c) scatter plot of phases, d) scatter plot of locations (x-coordinate of\n",
      "centerpoints).\n",
      "Finally, we can analyze the distribution of the frequencies and orientations of the\n",
      "subspace features. The plot in Fig. 10.9 shows that while all orientations are rather\n",
      "equally present (except for the anisotropy seen even in ICA results), the frequency\n",
      "distribution is strongly skewed: most invariant features are tuned to high frequen-\n",
      "cies.\n",
      "\n",
      "10.7 Results on natural images\n",
      "241\n",
      "a)\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "b)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "Fig. 10.9: Histograms of the optimal a) frequencies and b) orientations of the independent sub-\n",
      "spaces.\n",
      "10.7.1.4 Image synthesis results\n",
      "Results of synthesizing image with the ISA model are shown in Figure 10.10. This\n",
      "is based on the interpretation as a nonlinear ICA in Section 10.6, but the model with\n",
      "variance dependencies in Section 10.4 would give the same results.\n",
      "Here, the norms ri, i.e. the values of the invariance features, were chosen to be\n",
      "equal to those actually observed in the data. The angles inside the subspace were\n",
      "then randomly generated.\n",
      "The synthesized images are quite similar to those obtained by ICA. The invari-\n",
      "ance is not really reﬂected in the visual quality of these synthesized images.\n",
      "Fig. 10.10: Image synthesis using ISA. Compare with the ICA results in Figure 7.4 on page 170.\n",
      "\n",
      "242\n",
      "10 Energy detectors and complex cells\n",
      "10.7.2 The importance of being invariant\n",
      "What is the point in features that are invariant to phase? In general, the variability\n",
      "of how objects are expressed in the retinal image is one of the greatest challenges,\n",
      "perhaps the very greatest, to the visual system. Objects can be seen in different\n",
      "locations in the visual space (= retinal space). They can appear at different distances\n",
      "to the observer, which changes their size in the retinal image. Objects can rotate,\n",
      "turn around, and transform in a myriad of ways. And that’s not all: the environment\n",
      "can change, moving light sources and changing their strength, casting shadows and\n",
      "even occluding parts of the object.\n",
      "The visual system has learned to recognize objects despite these difﬁculties. One\n",
      "approach used in the early parts of the system is to compute features that are in-\n",
      "variant to some of such changes. Actually, in Chapter 9 we already saw one such\n",
      "operation: contrast gain control attempts to cancel some effects of changes in light-\n",
      "ing, and removal of the DC component is doing something similar.\n",
      "With energy detectors, we ﬁnd phase-invariant features, similar to those in com-\n",
      "plex cells. It is usually assumed that the point in such an invariance is to make\n",
      "recognition of objects less dependent on the exact position where they appear. The\n",
      "point is that a change in phase is very closely related to a change in position. In fact,\n",
      "it is rather difﬁcult to distinguish between phase-invariance and position-invariance\n",
      "(which is often called translation- or shift-invariance.) If you look at the different\n",
      "feature vectors Ai inside the same subspace in Fig. 10.6, you might say that they are\n",
      "the same feature in slightly different positions.\n",
      "Changing the phase of a grating, and in particular of a Gabor function is in-\n",
      "deed very similar to moving the stimulus a bit. However, it is not movement in\n",
      "an arbitrary direction: It is always movement in the direction of oscillations. Thus,\n",
      "phase-invariance is rather a special case of position-invariance. And, of course, the\n",
      "position-invariance exhibited by these energy detectors is very limited. If the stim-\n",
      "ulus is spatially localized (say, a Gabor function as always!), only a small change\n",
      "in the position is allowed, otherwise the stimulus goes out of the receptive ﬁeld and\n",
      "the response goes to zero. Even this very limited position-invariance can be useful\n",
      "as a ﬁrst step, especially if combined with further invariant computations in the next\n",
      "processing layers.\n",
      "Figure 10.11 shows a number of Gabor stimuli that have all other parameters\n",
      "ﬁxed at the same values but the phase is changed systematically. An ideal phase-\n",
      "invariant feature detector would give the same response to all these stimuli.\n",
      "Fig. 10.11: A Gabor stimulus whose phase is changed.\n",
      "\n",
      "10.7 Results on natural images\n",
      "243\n",
      "10.7.3 Grouping of dependencies\n",
      "Next we analyze how grouping of dependencies can be seen in the ISA results on\n",
      "natural images. A simple approach is to compute the correlation coefﬁcients of the\n",
      "squares of components. This is done separately for components which belong to\n",
      "the same subspace, and for components which belong to different subspaces. When\n",
      "this is computed for all possible component pairs, we can plot the histogram of the\n",
      "correlation coefﬁcients in the two cases. This is shown in Figure 10.12. We see that\n",
      "square correlations are much stronger for components in the same subspace. Ac-\n",
      "cording to the model deﬁnition, square correlations should be zero for components\n",
      "in different subspaces, but again we see that the real data does not exactly respect\n",
      "the independence assumptions.\n",
      "a)\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "b)\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2x 10\n",
      "4\n",
      "Fig. 10.12: Correlation coefﬁcients of the squares of all possible pairs of components estimated by\n",
      "ISA. a) Components in the same subspace, b) Components in different subspaces.\n",
      "Another way of analyzing the results is to visualize the square correlations. This\n",
      "is done in Figure 10.13 for the ﬁrst 80 components, i.e. 20 ﬁrst subspaces. Visually,\n",
      "we can see a clear grouping of dependencies.\n",
      "10.7.4 Superiority of the model over ICA\n",
      "How do we know if the ISA model really is better for natural images when compared\n",
      "to the ICA model? The ﬁrst issue to settle is what it means to have a better model.\n",
      "Of course, ISA is better than ICA in the sense that it shows emergence of new\n",
      "kinds of phenomena. However, since we are building statistical models, it is impor-\n",
      "tant to ask if the new model we have introduced, in this case ISA, really is better\n",
      "than ICA in a purely statistical sense. One useful way of approaching this is to\n",
      "compute the maximum likelihood. In a Bayesian interpretation, the likelihood is the\n",
      "\n",
      "244\n",
      "10 Energy detectors and complex cells\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "Fig. 10.13: Correlation coefﬁcients of the squares of the 80 ﬁrst components in Figure 10.5. The\n",
      "correlation coefﬁcients are shown in grey-scale. To improve the visualization, values larger than\n",
      "0.3 have been set to 0.3.\n",
      "probability of the parameters given the data, if the prior is ﬂat. This helps us in com-\n",
      "paring ICA and ISA models because we can consider the subspace size as another\n",
      "parameter. The ICA model is obtained in the case where the subspace size equals\n",
      "one. So, we can plot the maximum likelihood as a function of subspace size, always\n",
      "recomputing the Wi so as to maximize the likelihood for each subspace size. If the\n",
      "maximum is obtained for a subspace size larger than one, we can say that ISA is a\n",
      "better model than ICA.2\n",
      "It is important to note that we need to use a measure which is in line with the\n",
      "theory of statistics. One might think that comparison of, say, sparsenesses of the\n",
      "ICA and ISA features could be used to compare the models, but such a compari-\n",
      "son would be more problematic. First, ISA has fewer features, so how to compare\n",
      "the total sparseness of the representations? Second, we would also encounter the\n",
      "more fundamental question: Which sparseness measure to use? If we use likeli-\n",
      "hood, statistical theory automatically shows how to compute the quantities used in\n",
      "the comparison.\n",
      "2 Comparison of models in this way is actually a bit more complicated. One problem is that if\n",
      "the models may have a different number of parameters, a direct comparison of the likelihoods is\n",
      "not possible because having more parameters can lead to overﬁtting. Here, this problem is not\n",
      "serious because the number of parameters in the two models is essentially the same (it may be a bit\n",
      "different if the nonlinearities hi are parameterized as well). Furthermore, Bayesian theory proposes\n",
      "a number of more sophisticated methods for comparing models; they consider the likelihood with\n",
      "many different parameter values and not only at the maximal point. Such methods are, however,\n",
      "computationally quite complicated, so we don’t use them here.\n",
      "\n",
      "10.8 Analysis of convexity and energy correlations*\n",
      "245\n",
      "In Figure 10.14, likelihood is given as a function of subspace size for the ISA\n",
      "model, for image patches is 24 ×24. What we see here is that the likelihood grows\n",
      "when the subspace size is made larger than one—a subspace size of one is the same\n",
      "as the ICA model. Thus, ISA gives a higher likelihood. In addition, the graph shows\n",
      "that the likelihood is maximized when the subspace size is 32, which is quite large.\n",
      "However, this maximum depends quite a lot on how contrast gain control is per-\n",
      "formed. Here, it was performed by dividing the image patches by their norms, but\n",
      "as noted in Chapter 9, this may not a very good normalization method. Thus, the re-\n",
      "sults in Figure 10.14 should not be taken too seriously. Combining a proper contrast\n",
      "gain control method with the ICA and ISA model is an important topic for future\n",
      "research.\n",
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "32  64  128 256\n",
      "−1.3\n",
      "−1.28\n",
      "−1.26\n",
      "−1.24\n",
      "−1.22\n",
      "Subspace size\n",
      "log−Likelihood\n",
      "Fig. 10.14: Maximum likelihood of natural image data as a function of subspace dimensionality\n",
      "in ISA. Subspace size equal to 1 corresponds to the ICA model. The error bars are computed by\n",
      "doing the estimation many times for different samples of patches. Adapted from (Hyv¨arinen and\n",
      "K¨oster, 2007).\n",
      "10.8 Analysis of convexity and energy correlations*\n",
      "In this section, we show more detailed mathematical analysis on the connection of\n",
      "the correlation of squares and convexity of h discussed in Section 10.4.3. It can be\n",
      "omitted by readers not interested in mathematical details.\n",
      "\n",
      "246\n",
      "10 Energy detectors and complex cells\n",
      "10.8.1 Variance variable model gives convex h\n",
      "First we show that the dependency implied by the model with a convex h typically\n",
      "takes the form of energy correlations. To prove that h in (10.14) is always convex, it\n",
      "is enough to show that the second derivative of h is always positive. We can ignore\n",
      "the factor 1/2π. Using simple derivation under the integral sign, we obtain\n",
      "h′′(u) =\n",
      "R\n",
      "1\n",
      "d6 exp(−u\n",
      "2d2 )p(d)dd\n",
      "R 1\n",
      "d2 exp(−u\n",
      "2d2 )p(d)dd −[\n",
      "R 1\n",
      "d4 exp(−u\n",
      "2d2 )p(d)dd]2\n",
      "[\n",
      "R\n",
      "1\n",
      "d2 exp(−u\n",
      "2d2 )p(d)dd]2\n",
      "(10.19)\n",
      "Since the denominator is always positive, it is enough to show that the numerator is\n",
      "always positive. Let is consider exp(−u\n",
      "2d2 )p(d) as a new pdf of d, for any ﬁxed u,\n",
      "after it has been normalized to have unit integral. Then, the numerator takes the form\n",
      "(E{1/d6}E{1/d2} −E{1/d4}2). Thus, it is enough that we prove the following\n",
      "general result: for any random variable z ≥0, we have\n",
      "(E{z2})2 ≤E{z3}E{z}\n",
      "(10.20)\n",
      "When we apply this on z = 1/d2, we have shown that the numerator is positive. The\n",
      "proof of Equation (10.20) is possible by the classic Cauchy-Schwarz inequality,\n",
      "which says that for any x,y ≥0, we have\n",
      "E{xy} ≤E{x2}1/2E{y2}1/2\n",
      "(10.21)\n",
      "Now, choose x = z3/2 and y = z1/2. Then, taking squares on both sides, Equa-\n",
      "tion (10.21) gives Equation (10.20).\n",
      "10.8.2 Convex h typically implies positive energy correlations\n",
      "Next we show why convexity of h implies energy correlations in the general case.\n",
      "We cannot show this exactly, we have to use a ﬁrst-order approximation. Let us\n",
      "consider two variables, and look at the conditional pdf of s2 near the point s2 = 0.\n",
      "This gives\n",
      "h(s2\n",
      "1 +s2\n",
      "2) = h(s2\n",
      "1)+h′(s2\n",
      "1)s2\n",
      "2 +smaller terms\n",
      "(10.22)\n",
      "Let us interpret this as the logarithm of the pdf of s2, given a ﬁxed s1. Some nor-\n",
      "malization term should then be added, corresponding to the denominator in (10.15),\n",
      "but it is a function of s1 alone. This ﬁrst-order approximation of the conditional pdf\n",
      "is gaussian, because only the gaussian distribution has a log-pdf that is a quadratic\n",
      "function. The variance of the distribution is equal to 2/|h′(s2\n",
      "1)|. Because of convex-\n",
      "ity, h′ is increasing. Usually, h′ is also negative, because the pdf must go to zero\n",
      "(and its log to −∞) when s2 goes inﬁnite. Thus, |h′(s2\n",
      "1)| is a decreasing function,\n",
      "and 2/|h′(s2\n",
      "1)| is increasing. This shows that the conditional variance of s2 increases\n",
      "\n",
      "10.9 Concluding remarks and References\n",
      "247\n",
      "with s2\n",
      "1, if h is convex. Of course, this was only an approximation, but it justiﬁes the\n",
      "intuitive idea that a convex h leads to positive energy correlations.\n",
      "Thus, we see that using a convex h in the ISA model is closely related to assuming\n",
      "that the si inside the same subspace have positive energy correlations.\n",
      "10.9 Concluding remarks and References\n",
      "Independent subspace analysis is for complex cells what ICA was for simple cells.\n",
      "When estimated from natural image data, it learns an energy detector model which\n",
      "is similar to what is computed by complex cells in V1. The resulting features have a\n",
      "relatively strong phase-invariance, while they retain the simple-cell selectivities for\n",
      "frequency, orientation, and to a lesser degree, location. A restriction in the model is\n",
      "that the pooling in the second layer is ﬁxed; relaxing this restriction is an important\n",
      "topic of current research and will be brieﬂy considered in Section 11.8. Another\n",
      "question is whether the squaring nonlinearity in computation of the features is better\n",
      "than, say, the absolute value; experiments in (Hyv¨arinen and K¨oster, 2007) indicate\n",
      "that it is.\n",
      "Steerable ﬁlters (orientation-invariant features) are discussed in the exercises and\n",
      "computer assignments below. The earliest references include (Koenderink and van\n",
      "Doorn, 1987; Freeman and Adelson, 1991; Simoncelli et al, 1992). An alterna-\n",
      "tive viewpoint on using quadratic models on natural images is in (Lindgren and\n",
      "Hyv¨arinen, 2007), which uses a different approach and ﬁnds very different features.\n",
      "Early and more recent work on energy detectors can be found in (Pollen and\n",
      "Ronner, 1983; Mel et al, 1998; Emerson et al, 1992; Gray et al, 1998). It is also\n",
      "possible to directly incorporate energy detectors in wavelets using complex-valued\n",
      "wavelets (Romberg et al, 2000). The idea of transforming the data into polar coor-\n",
      "dinates can be found in (Zetzsche et al, 1999). Using position-invariant features in\n",
      "pattern recognition goes back to at least (Fukushima, 1980), see e.g. (Fukushima\n",
      "et al, 1994; Riesenhuber and Poggio, 1999) for more recent developments.\n",
      "Only recently, reverse correlation methods have been extended to estimation en-\n",
      "ergy models (Touryan et al, 2005; Rust et al, 2005; Chen et al, 2007). These provide\n",
      "RF’s for linear subunits in an energy model. The obtained results are quite similar to\n",
      "those we learned in this chapter. However, such reverse-correlation are quite scarce\n",
      "at the moment, so a detailed comparison is hardly possible. Alternative approaches\n",
      "to characterizing complex cells is presented in (Felsen et al, 2005; Touryan et al,\n",
      "2002).\n",
      "\n",
      "248\n",
      "10 Energy detectors and complex cells\n",
      "10.10 Exercices\n",
      "Mathematical exercises\n",
      "1. Show Equation (10.6).\n",
      "2. This exercise considers the simplest case of steerable ﬁlters. Consider the gaus-\n",
      "sian function\n",
      "ϕ(x,y) = exp(−1\n",
      "2(x2 +y2))\n",
      "(10.23)\n",
      "a. Compute the partial derivatives of ϕ with respect to x and y. Denote them by\n",
      "ϕx and ϕy.\n",
      "b. Show that ϕx and ϕy are orthogonal:\n",
      "Z\n",
      "ϕx(x,y)ϕy(x,y)dxdy = 0\n",
      "(10.24)\n",
      "c. The two functions ϕx and ϕy deﬁne a pair of steerable ﬁlters. The subspace\n",
      "they span has an invariance property which will be shown next. Deﬁne an\n",
      "orientation angle parameter α. Consider a linear combination\n",
      "ϕα = ϕx cosα +ϕy sinα\n",
      "(10.25)\n",
      "The point is to show that ϕα has just the same shape as ϕx or ϕy, the only\n",
      "difference being that they are all rotated versions of each other. Thus, ϕx and\n",
      "ϕy form an orthogonal basis for a subspace which consists of simple edge\n",
      "detectors with all possible orientations. The proof can be obtained as follows.\n",
      "Deﬁne a rotated version of the variables as\n",
      "\u0012x′\n",
      "y′\n",
      "\u0013\n",
      "=\n",
      "\u0012 sinβ\n",
      "cosβ\n",
      "−cosβ sinβ\n",
      "\u0013\u0012x\n",
      "y\n",
      "\u0013\n",
      "(10.26)\n",
      "Express ϕ as a function of x′ and y′. Show that this is equivalent to ϕα for a\n",
      "suitably chosen β.\n",
      "Computer assignments\n",
      "1. Create two two-dimensional Gabor ﬁlters in quadrature-phase and plot random\n",
      "linear combinations of them.\n",
      "2. Next we consider steerable ﬁlters.\n",
      "a. Plot the partial derivatives ϕx and ϕy deﬁned in the mathematical exercise 2\n",
      "above.\n",
      "b. For a couple of different values of alpha, plot their linear combinations ϕα.\n",
      "Compare visually the shapes of the functions plotted.\n",
      "\n",
      "Chapter 11\n",
      "Energy correlations and topographic\n",
      "organization\n",
      "The energy detection model in the preceding chapter can easily be modiﬁed to in-\n",
      "corporate topography, i.e. an arrangement of the features on a two-dimensional grid.\n",
      "This is very interesting because such organization is one of the most prominent phe-\n",
      "nomena found in the primary visual cortex. In this chapter, we shall investigate such\n",
      "a topographic version of the ICA model. It is, mathematically, a rather simple mod-\n",
      "iﬁcation of the independent subspace analysis model.\n",
      "11.1 Topography in the cortex\n",
      "Topography means that the cells in the visual cortex are not in any random order;\n",
      "instead, they have a very speciﬁc spatial organization. When moving on the corti-\n",
      "cal surface, the response properties of the neurons change in systematic ways. The\n",
      "phenomenon can also be called topological organization, and sometimes the term\n",
      "“columnar organization” is used in almost the same sense.\n",
      "Fundamentally the cortex is, of course, three-dimensional. In addition to the sur-\n",
      "face coordinates, which we denote by xc and yc, there is the depth dimension zc. The\n",
      "depth “axis” goes from the very surface of the cortex through different layers of the\n",
      "grey matter to the white matter.\n",
      "However, the depth dimension is usually assumed to be different from the other\n",
      "two dimensions. In the most simplistic interpretations, the cells that are on the same\n",
      "surface location (xc,yc) are similar irrespective of how deep they are on the cortex.\n",
      "This is most clearly expressed in the classic “ice cube” model of V1. Such a simplis-\n",
      "tic view has been challenged, and it is now well-known that at least some properties\n",
      "of the cells are clearly different in different (depth) layers. In particular, input to V1\n",
      "is received in some of the layers and others are specialized in outputting the results.\n",
      "Still, it seems that the response properties which we consider in this book, such as\n",
      "location, frequency, and orientation selectivities, depend mainly on the coordinates\n",
      "(xc,yc) of the cell with respect to the surface.\n",
      "249\n",
      "\n",
      "250\n",
      "11 Energy correlations and topographic organization\n",
      "Looking at the spatial organization of response properties as a function of the\n",
      "surface coordinates xc and yc, the most striking aspect of topographic organization is\n",
      "retinotopy, which means that the location of the receptive ﬁeld in the retinal space is\n",
      "closely correlated with the xc and yc coordinates. The global correspondence of the\n",
      "retinal coordinates and the cortical coordinates is somewhat complicated due to such\n",
      "phenomena as the magniﬁcation factor (the area in the center of the visual ﬁeld has\n",
      "a relatively larger representation on the cortex), the division into two hemispheres,\n",
      "some unexpected discontinuities, and so on. The correlation is, therefore, more of a\n",
      "local nature.\n",
      "The second important topographic property is the gradual change of orienta-\n",
      "tion tuning. The preferred orientation of simple and complex cells mostly changes\n",
      "smoothly. This phenomenon is often referred to as orientation columns. They can be\n",
      "seen most clearly in optical imaging experiments where one takes a “photograph” of\n",
      "the cortex that shows which regions are active when the input consists of a grating\n",
      "of a given orientation. Such activity patterns take the form of stripes (columns).\n",
      "The third important property of spatial organization is that frequency selectivity\n",
      "seems to be arranged topographically into low-frequency blobs so that the blobs (or\n",
      "at least their centers) contain predominantly cells that prefer low-frequency cells\n",
      "and the interblob cells prefer higher frequencies. These low-frequency blobs seem\n",
      "to coincide with the well-known cytochrome oxidase blobs.\n",
      "A ﬁnal point to note is that phase is not arranged topographically. In fact, phase\n",
      "seems to be completely random: there is no correlation between the phase parame-\n",
      "ters in two neighbouring cells.\n",
      "11.2 Modelling topography by statistical dependence\n",
      "Now we show how to extend the models of natural image statistics to include to-\n",
      "pography. The key is to consider the dependencies of the components. The model is\n",
      "thus closely related to the model of independent subspace analysis in Chapter 10. In\n",
      "fact, ISA can be seen as a special case of this model.\n",
      "11.2.1 Topographic grid\n",
      "To model topographic organization, we have to ﬁrst deﬁne which features are “close\n",
      "to each other” on the cortical surface. This is done by arranging the features si on\n",
      "a two-dimensional grid or lattice. The restriction to 2D is motivated by cortical\n",
      "anatomy, but higher dimensions are equally possible. The spatial organization on\n",
      "the grid models the organization on the cortical surface. The arrangement on the\n",
      "lattice is illustrated in Figure 11.1.\n",
      "The topography is formally expressed by a neighbourhood function π(i, j) that\n",
      "gives the proximity of the features (components) with indices i and j. Typically, one\n",
      "\n",
      "11.2 Modelling topography by statistical dependence\n",
      "251\n",
      "independent\n",
      "dependent\n",
      "S1\n",
      "S\n",
      "3\n",
      "2\n",
      "S\n",
      "Si\n",
      "neighbour−\n",
      "hood of\n",
      "si\n",
      "Fig. 11.1: Illustration of topography and its statistical interpretation. The neurons (feature detec-\n",
      "tors) are arranged on a two-dimensional grid that deﬁnes which neurons are near to each other and\n",
      "which are far from each other. It also deﬁnes the neighbourhood of a cell as the set of cells which\n",
      "are closer than a certain radius. In the statistical model, neurons that are near to each other have\n",
      "statistically dependent outputs, neurons that are far from each other have independent outputs.\n",
      "deﬁnes that π(i, j) is 1 if the features are sufﬁciently close to each other (they are\n",
      "“neighbours”), and 0 otherwise. Typically, the neighbourhood function is chosen by\n",
      "deﬁning the neighbourhood of a feature to be square. For example, π(i, j) is 1 if the\n",
      "feature j is in a 5 ×5 square centered on feature i; otherwise π(i, j) is zero.\n",
      "11.2.2 Deﬁning topography by statistical dependencies\n",
      "Consider a number of features si,i = 1,...,n. How can we order the features on the\n",
      "topographic grid in a meaningful way? The starting point is to deﬁne a measure of\n",
      "similarity between two features, and then to order the features so that features that\n",
      "are similar are close to each other on the grid. This is a general principle that seems\n",
      "fair enough. But then, what is a meaningful way of deﬁning similarity between two\n",
      "features? There are, actually, a couple of different possibilities.\n",
      "In many models, the similarity of features is deﬁned by similarity of the features\n",
      "weights or receptive ﬁelds Wi. Typically, this means the dot-product (also called,\n",
      "somewhat confusingly, the correlation of the receptive ﬁelds). This is the case in\n",
      "Kohonen’s self-organizing map and related models. However, this seems rather in-\n",
      "\n",
      "252\n",
      "11 Energy correlations and topographic organization\n",
      "adequate in the case of the visual cortex. For example, two features of the same\n",
      "frequency need not exhibit large dot-products of weight vectors; in fact, the dot-\n",
      "product can be zero if the features are of orthogonal orientations with otherwise\n",
      "similar parameters. Yet, since the V1 exhibits low-frequency blobs, low-frequency\n",
      "features should be considered similar to each other even if they are quite differ-\n",
      "ent with respect to other parameters. What’s even worse is that since the phases\n",
      "change randomly when moving a bit on the cortical surface, the dot-products be-\n",
      "tween neighbouring components also change rather randomly since the phase has a\n",
      "large inﬂuence on the shape of the receptive ﬁelds and on the dot-products.\n",
      "Another candidate for a similarity measure would be correlation of the feature\n",
      "detector outputs si when the input consists of natural images. However, this is no\n",
      "good either, since the outputs (components) are typically constrained to be exactly\n",
      "uncorrelated in ICA and related models. Thus, they would all be maximally dissim-\n",
      "ilar if similarity is based on correlations.\n",
      "Yet, using correlations seems to be a step in the right direction. The central hy-\n",
      "pothesis used in this book — visual processing in the cortex is strongly inﬂuenced\n",
      "by the statistical structure of the natural input — would suggest that we have to look\n",
      "at the statistics of feature detector outputs in order to ﬁnd a meaningful measure of\n",
      "similarity to be used in a model of topography. We just need more information than\n",
      "the ordinary linear correlations.\n",
      "Our statistical approach to topography thus concentrates on the pattern of statis-\n",
      "tical dependencies between the si, assuming that the joint distribution of the si is\n",
      "dictated by the natural image input. The basic idea is that similarity is deﬁned by\n",
      "the statistical dependency of the outputs. Thus, features that have strong statistical\n",
      "dependencies are deﬁned to be similar, and features that are independent or weakly\n",
      "dependent are deﬁned to be dissimilar.\n",
      "The application of this principle is illustrated in Fig. 11.1. The linear feature\n",
      "detectors (simple cells) have been arranged on a grid (cortex) so that any two fea-\n",
      "ture detectors that are close to each other have dependent outputs, whereas feature\n",
      "detectors that are far from each other have independent outputs.\n",
      "Actually, from Chapters 9 and 10 we know what are the most prominent statis-\n",
      "tical dependencies that remains after ordinary ICA: the correlations of squares (or\n",
      "absolute values, which seems to be closely related). Thus, we do not need to model\n",
      "the whole dependency structure of the si, which would be most complicated. We can\n",
      "just concentrate on the dependencies of the squares s2\n",
      "i .\n",
      "11.3 Deﬁnition of topographic ICA\n",
      "As in the ICA and ISA models, we model the image as a linear superposition of\n",
      "features Ai with random coefﬁcients si:\n",
      "I(x,y) =\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si\n",
      "(11.1)\n",
      "\n",
      "11.3 Deﬁnition of topographic ICA\n",
      "253\n",
      "As in ICA and ISA, the si are obtained as the outputs of linear feature detectors as\n",
      "si = ∑\n",
      "x,y\n",
      "Wi(x,y)I(x,y) =\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "vijzj = vT\n",
      "i z\n",
      "(11.2)\n",
      "where the zj denotes the j-th variable obtained from the image patch by canonical\n",
      "preprocessing.\n",
      "The point is now to deﬁne the joint pdf of the si so that it expresses the topo-\n",
      "graphic ordering. First, we deﬁne the “local energies” as\n",
      "ci =\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "π(i, j)s2\n",
      "j.\n",
      "(11.3)\n",
      "This is basically the general activity level in the neighbourhood of the linear feature\n",
      "si. The weighting by π(i, j) means that we only sum over sj which are close to si in\n",
      "the topography.\n",
      "Next, we deﬁne the likelihood of the topographic ICA model by a simple mod-\n",
      "iﬁcation of the log-likelihood in the ISA model, given in Equation (10.11) on page\n",
      "230. We replace the subspace energies ek by these local energies. (The connection\n",
      "between the two models is discussed in more detail later.) Thus, deﬁne the pdf of\n",
      "the si as\n",
      "log p(s1,...,sn) =\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "h(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "π(i, j)s2\n",
      "j)\n",
      "(11.4)\n",
      "where h is a convex function as in the preceding chapters, e.g. Section 6.2.1. As-\n",
      "suming we have observed a set of image patches, represented by zt,t = 1,...,T\n",
      "after canonical preprocessing, we obtain the likelihood\n",
      "logL(v1,...,vn) = T log|det(V)|+\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "h(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "π(i, j)(vT\n",
      "j zt)2)\n",
      "(11.5)\n",
      "The topography given by π(i, j) is considered ﬁxed, and only the linear feature\n",
      "weights v j are estimated, so this likelihood is a function of the v j only. As in earlier\n",
      "models, the vectors v j are constrained to form an orthogonal matrix, so the determi-\n",
      "nant is constant (one) and the term T log|det(V)| can be ignored.\n",
      "The central feature of this model is that the responses si of near-by simple cells\n",
      "are not statistically independent in this model. The responses are still linearly uncor-\n",
      "related, but they have nonlinear dependencies. In fact, the energies s2\n",
      "i are strongly\n",
      "positively correlated for neighbouring cells. This property is directly inherited from\n",
      "the ISA model; that connection will be discussed next.\n",
      "\n",
      "254\n",
      "11 Energy correlations and topographic organization\n",
      "11.4 Connection to independent subspaces and invariant features\n",
      "Topographic ICA can be considered a generalization of the model of independent\n",
      "subspace analysis. The likelihood of ISA, see Equation (10.11), can be expressed as\n",
      "a special case of the likelihood in Equation (11.5) with a neighbourhood function\n",
      "which is one if the components are in the same subspace and zero otherwise, or\n",
      "more formally:\n",
      "π(i, j) =\n",
      "(\n",
      "1,\n",
      "if there is some subspace with index q so that i, j ∈S(q)\n",
      "0\n",
      "otherwise.\n",
      "This shows that topographic ICA is closely connected to the principle of invari-\n",
      "ant feature subspaces in Chapter 10. In topographic ICA, every component has its\n",
      "own neighbourhood, which corresponds to a subspace in ISA. Each of the local en-\n",
      "ergies ci could be considered as the counterpart of the energies ek in ISA. Thus the\n",
      "local energies, possibly after a nonlinear transform, can be interpreted as the values\n",
      "of invariant features. The pooling process is controlled by the neighbourhood func-\n",
      "tion π(i, j). This function directly gives the pooling weights, i.e. the connections\n",
      "between the linear features with index i and the invariant feature cell with index j.\n",
      "Note that the number of invariant features is here equal to the number of underlying\n",
      "linear features.\n",
      "The dependencies of the components can also be deduced from this analogy\n",
      "with ISA. In ISA, components which are in the same subspace have correlations\n",
      "of energies. In topographic ICA, components which are close to each other in the\n",
      "topographic grid have correlations of squares. Thus, all the features in the same\n",
      "neighbourhood tend to be active (non-zero) at the same time.\n",
      "In a biological interpretation, our deﬁnition of the pooling weights from simple\n",
      "cells to complex cells in topographic ICA is equivalent to the assumption that com-\n",
      "plex cells only pool outputs of simple cells that are near-by on the topographic grid.\n",
      "Neuroanatomic measurements indicate that the wiring of complex cells may indeed\n",
      "be so constrained, see References below. Such a two-layer network is illustrated in\n",
      "Fig. 11.2.\n",
      "11.5 Utility of topography\n",
      "What is the computational utility of a topographic arrangement? A widely used ar-\n",
      "gument is that such a spatial arrangement is useful to minimize wiring length. Wiring\n",
      "length means here the length of the physical connections (axons) needed to send sig-\n",
      "nals from one neuron to another. Consider, for example, the problem of designing\n",
      "the connections from simple cells to complex cells so that the “wires” are as short\n",
      "as possible. It is rather obvious that topographic ICA is related to minimizing that\n",
      "wiring length because in topographic ICA all such connections are very local in the\n",
      "sense that they are not longer that the radius of the neighbourhoods. A more general\n",
      "\n",
      "11.5 Utility of topography\n",
      "255\n",
      "task may be to pool of responses to reduce noise: if a cell in a higher area wants\n",
      "to “read”, say, the orientation of the stimulus, it could reduce noise in V1 cell re-\n",
      "sponses by looking at the average of the responses of many cells which have the\n",
      "same orientation selectivity.\n",
      "In general, if we assume that two cells need to communicate with each other\n",
      "if (and only if) their outputs are statistically dependent, topographic ICA provides\n",
      "optimal wiring. The same applies if the responses of two cells are combined by a\n",
      "third cell only if the outputs of the two cells are statistically dependent. Such as-\n",
      "sumptions are reasonable because if the cells represent pieces of information which\n",
      "are related (in some intuitive sense), it is likely that their outputs are statistically de-\n",
      "pendent, and vice versa; so, statistical dependence tells which cells contain related\n",
      "information which has to be combined in higher levels.\n",
      "Minimization of wiring length may be important for keeping the total brain vol-\n",
      "ume minimal: a considerable proportion of the brain volume is used up in intercon-\n",
      "necting axons. It would also speed up processing because the signal travels along\n",
      "the axons with limited speed.\n",
      "2\n",
      "( )\n",
      "( )\n",
      "Σ\n",
      "( )\n",
      "2\n",
      "( ) 2\n",
      "( ) 2\n",
      "2\n",
      "( )\n",
      "2\n",
      "( ) 2\n",
      "( ) 2\n",
      "( ) 2\n",
      "Fig. 11.2: Computation of invariant features in the topographic ICA model. Invariant features (com-\n",
      "plex cell outputs) are obtained by summing the squares of linear features (simple cell outputs) in\n",
      "a neighbourhood of the topographic grid. From (Hyv¨arinen et al, 2001a), Copyright c⃝2001 MIT\n",
      "Press, used with permission.\n",
      "\n",
      "256\n",
      "11 Energy correlations and topographic organization\n",
      "11.6 Estimation of topographic ICA\n",
      "A fundamental similarity to ISA is that we do not specify what parameters should\n",
      "be considered as deﬁning the topographic order. That is, the model does not specify,\n",
      "for example, that near-by neurons should have receptive ﬁelds that have similar\n",
      "locations, or similar orientations. Rather, we let the natural images decide what the\n",
      "topography should be like, based on their statistical structure.\n",
      "(locally pooled energies)\n",
      "(linear filters)\n",
      "square rectification\n",
      "PIXEL INPUT\n",
      "maximize\n",
      "sparseness\n",
      "SIMPLE CELLS\n",
      "COMPLEX CELLS\n",
      "fixed\n",
      "learned\n",
      "weights\n",
      "weights\n",
      "Fig. 11.3: Illustration of learning in the topographic ICA model. From (Hyv¨arinen and Hoyer,\n",
      "2001), Copyright c⃝2001 Elsevier, used with permission.\n",
      "Since we have already deﬁned the likelihood in Equation (11.5), estimation needs\n",
      "hardly any further comment. We use whitened (canonically preprocessed) data, so\n",
      "we constrain V to be orthogonal just like in ICA and ISA. We maximize the like-\n",
      "lihood under this constraint. The computational implementation of such maximiza-\n",
      "tion is discussed in detail in Chapter 18, in particular Section 18.5.\n",
      "The intuitive interpretation of such estimation is that we are maximizing the\n",
      "sparsenesses of the local energies. This is completely analogue to ISA, where we\n",
      "maximize sparsenesses of complex cell outputs. The learning process is illustrated\n",
      "in Fig. 11.3.\n",
      "\n",
      "11.7 Topographic ICA of natural images\n",
      "257\n",
      "11.7 Topographic ICA of natural images\n",
      "11.7.1 Emergence of V1-like topography\n",
      "11.7.1.1 Data and preprocessing\n",
      "We performed topographic ICA on the same data as in previous chapters. We took\n",
      "the same 50,000 natural image patches of size 32×32 as in the preceding chapters.\n",
      "We preprocessed the data in the same way as in the ISA case: This means divisive\n",
      "normalization using Equation (9.11), and reducing the dimension to 256 by PCA.\n",
      "The nonlinearity h was chosen to be a smoothed version of the square root as in\n",
      "Eq. (6.14), just like in the ISA experiments.\n",
      "The topography was chosen so that π(i, j) is 1 if the cell j is in a 5 × 5 square\n",
      "centered on cell i; otherwise π(i, j) is zero. Moreover, it was chosen to be cyclic\n",
      "(toroidal) so that the left edge of the grid is connected to the right edge, and the\n",
      "upper edge is connected to the lower edge. This was done to reduce border artifacts\n",
      "due to the limited size to the topographic grid.\n",
      "11.7.1.2 Results and analysis\n",
      "The linear detector weights Wi obtained by topographic ICA from natural images\n",
      "are shown in Fig. 11.4, and the corresponding feature vectors Ai are in Fig. 11.5.\n",
      "The topographic ordering is visually obvious. The underlying linear features are\n",
      "tuned for the three principal parameters: orientation, frequency and location. Visual\n",
      "inspection of the map shows that orientation and location mostly change smoothly\n",
      "as a function of position on the topographic grid. A striking feature of the map is a\n",
      "“blob” grouping low-frequency features. Thus the topography is determined by the\n",
      "same set of parameters for which the features are selectively tuned; these are just the\n",
      "same as in ICA and ISA. These are also the three parameters with respect to which\n",
      "a clear spatial organization has been observed in V1.\n",
      "The topography can be analyzed in more detail by either a global or a local anal-\n",
      "ysis. A local analysis is done by visualizing the correlations of the optimal Gabor\n",
      "parameters for two linear features that are immediate neighbours. In Fig. 11.6 we\n",
      "see that the locations (a,b) and orientations (c) are strongly correlated. In the case\n",
      "of frequencies (d) the correlation is more difﬁcult to see because of the overall con-\n",
      "centration to high frequencies. As for phases (e), no correlation (or any kind of sta-\n",
      "tistical dependency) can be seen, which is again similar to what has been observed\n",
      "in V1. Furthermore, all these correlations are similar to the correlations inside inde-\n",
      "pendent subspaces in Figure 10.8 on page 240. This is not surprising because of the\n",
      "intimate connection between the two models, explained above in Section 11.4.\n",
      "A global analysis is possible by colour-coding the Gabor parameters of linear\n",
      "features. This gives “maps” whose smoothness shows the smoothness of the under-\n",
      "lying parameter. The maps are shown in Figure 11.7. The locations (a and b) can be\n",
      "\n",
      "258\n",
      "11 Energy correlations and topographic organization\n",
      "Fig. 11.4: The whole set of vectors Wi obtained by topographic independent component analysis,\n",
      "in the topographic order.\n",
      "seen to change smoothly, which is not obvious from just looking at the features in\n",
      "Figure 11.4. The orientation and frequency maps (c and d) mainly change smoothly,\n",
      "which was rather obvious from Figure 11.4 anyway. In some points, the orientation\n",
      "seems to change abruptly, which may correspond to so-called “pinwheels”, which\n",
      "are points in which many different orientations can be found next to each other, and\n",
      "have been observed on the cortex. As for phases, the map in (e) shows that they\n",
      "really change randomly.\n",
      "We can also analyze the distribution of the frequencies and orientations of the\n",
      "features. The plot in Fig. 11.8 shows the histograms preferred orientations and\n",
      "frequencies for the linear features. We see that all orientations are almost equally\n",
      "present, but the horizontal orientation is slightly overrepresented. This is the same\n",
      "anisotropy we have seen all preceding models. In contrast, the frequency distribu-\n",
      "\n",
      "11.7 Topographic ICA of natural images\n",
      "259\n",
      "Fig. 11.5: The whole set of vectors Ai obtained by topographic independent component analysis.\n",
      "tion is very strongly skewed: most linear features are tuned to high frequencies.\n",
      "However, the distribution of frequencies is a bit closer to uniform than in the cases\n",
      "of ICA (Fig. 6.9) or ISA (Fig. 10.9).\n",
      "The connection of the model to ISA suggests that the local energies can be in-\n",
      "terpreted as invariant features. What kind of invariances do we see emerging from\n",
      "natural images? Not surprisingly, the invariances are similar to what we obtained\n",
      "with ISA, because the neighbourhoods have the same kinds of parameters correla-\n",
      "tions (Figure 11.6) as in ICA; we will not analyze them in more detail here. The\n",
      "main point is that local energies are like complex cells. That is, the topographic ICA\n",
      "model automatically incorporates a complex cell model.\n",
      "Basically, the conclusion to draw from these results is that the topographic ICA\n",
      "model produces a spatial topographic organization of linear features that is quite\n",
      "similar to the one observed in V1.\n",
      "\n",
      "260\n",
      "11 Energy correlations and topographic organization\n",
      "a)\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "b)\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "c)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "2.5\n",
      "3\n",
      "d)\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "e)\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Fig. 11.6: Correlation of parameters characterizing the linear features of two neighbouring features\n",
      "in Figure 11.4. An immediate neighbour for each cell chosen as the one immediately to the right.\n",
      "Each point in the scatter plots is based on one such couple. a) scatter plot of locations along x-axis,\n",
      "b) locations along y-axis, c) orientations, d) frequencies, and e) phases. The plots are very similar\n",
      "to corresponding plots for ISA in Fig. 10.8 on page 240; the main visual difference is simply due\n",
      "to the fact that here we have twice the number of dots in each plot.\n",
      "11.7.1.3 Image synthesis results & sketch of generative model\n",
      "Next, we will synthesize images from the topographic ICA model. This is a bit tricky\n",
      "because in fact, we did not yet introduce a proper generative model for topographic\n",
      "ICA. Such a model can be obtained as a special case of the framework introduced\n",
      "later in Section 11.8.2. We will here brieﬂy describe how such a generative model\n",
      "can be obtained.\n",
      "\n",
      "11.7 Topographic ICA of natural images\n",
      "261\n",
      "a)\n",
      "5\n",
      "10\n",
      "15\n",
      "5\n",
      "10\n",
      "15\n",
      "b)\n",
      "5\n",
      "10\n",
      "15\n",
      "5\n",
      "10\n",
      "15\n",
      "c)\n",
      "5\n",
      "10\n",
      "15\n",
      "5\n",
      "10\n",
      "15\n",
      "d)\n",
      "5\n",
      "10\n",
      "15\n",
      "5\n",
      "10\n",
      "15\n",
      "e)\n",
      "5\n",
      "10\n",
      "15\n",
      "5\n",
      "10\n",
      "15\n",
      "Fig. 11.7: Global structure of the topography estimated from natural images in Figure 11.4. Each\n",
      "parameter of the Gabor functions describing the features is plotted grey-scale or colour-coded.\n",
      "Colour-coding is used for parameters which are cyclic: orientation and phase, since the colour\n",
      "spectrum is also cyclic. The actual values of the parameters are not given because they have little\n",
      "importance. a) locations along x-axis, b) locations along y-axis, c) orientations, d) frequencies,\n",
      "and e) phases.\n",
      "Basically, the idea is a simple generalization of the framework using variance\n",
      "variables as in Section 10.4 and Section 9.3. Here, we have a separate variance\n",
      "variable di for each component si:\n",
      "si = ˜sidi\n",
      "(11.6)\n",
      "\n",
      "262\n",
      "11 Energy correlations and topographic organization\n",
      "a)\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "b)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "Fig. 11.8: Histograms of the optimal a) frequencies and b) orientations of the linear features in\n",
      "topographic ICA.\n",
      "where the ˜si are gaussian and independent from each other (and from the di). The\n",
      "point is to generate the di so that their dependencies incorporate the topography.This\n",
      "can be accomplished by generating them using a higher-order ICA model, where the\n",
      "mixing matrix is given by the neighbourhood function. Denoting the higher-order\n",
      "components by ui, we simply deﬁne\n",
      "di = ∑\n",
      "j\n",
      "π(i, j)ui\n",
      "(11.7)\n",
      "This produces approximately the same distribution as the pdf which we used to\n",
      "deﬁne the topographic ICA model earlier in this chapter. See Section 11.8.2 for\n",
      "details. A problem we encounter here is that it is not obvious how to estimate the\n",
      "distributions of the ui. So, we have to ﬁx them rather arbitrarily, which means the\n",
      "results are not quite directly comparable with those obtained by ISA and ICA where\n",
      "we could use the observed histograms of the features.\n",
      "Results of synthesizing images with this generative model are shown in Fig-\n",
      "ure 11.9. The ui were generated as the fourth powers of gaussian variables. The\n",
      "synthesized images seem to have more global structure than those obtained by ICA\n",
      "or ISA, but as we just pointed out, this may be related to the way we ﬁxed the\n",
      "distributions of the ui.\n",
      "11.7.2 Comparison with other models\n",
      "When compared with other models on V1 topography, we see three important prop-\n",
      "erties in the topographic ICA model:\n",
      "1. The topographic ICA model shows emergence of a topographic organization us-\n",
      "ing the above-mentioned three principal parameters: location, frequency and ori-\n",
      "entation. The use of these particular three parameters is not predetermined by the\n",
      "\n",
      "11.7 Topographic ICA of natural images\n",
      "263\n",
      "Fig. 11.9: Image synthesis using topographic ICA. Compare with the ICA results in Figure 7.4 on\n",
      "page 170 and ISA results in Figure 10.10 on page 241.\n",
      "model, but determined by the statistics of the input. This is in contrast to most\n",
      "models that only model topography with respect to one or two parameters (usu-\n",
      "ally orientation possibly combined with binocularity) that are chosen in advance.\n",
      "2. No other model has shown the emergence of a low-frequency blob.\n",
      "3. Topographic ICA may be the ﬁrst one to explicitly show a connection between\n",
      "topography and complex cells. The topographic, columnar organization of the\n",
      "simple cells is such that complex cell properties are automatically created when\n",
      "considering local activations. This is related to the randomness of phases, which\n",
      "means that in each neighbourhood, there are linear features with very different\n",
      "phases, like in the subspaces in ISA.\n",
      "It is likely that the two latter properties (blobs and complex cells) can only\n",
      "emerge in a model that is based on simultaneous activation (energy correlation)\n",
      "instead of similarity of receptive ﬁelds as measured by Euclidean distances or recep-\n",
      "tive ﬁeld correlations. This is because Euclidean distances or correlations between\n",
      "feature vectors of different frequencies, or of different phases, are quite arbitrary:\n",
      "they can obtain either large or small values depending on the other parameters. Thus,\n",
      "they do not offer enough information to qualitatively distinguish the effects of phase\n",
      "vs. frequency, so that phase can be random and frequency can produce a blob.\n",
      "\n",
      "264\n",
      "11 Energy correlations and topographic organization\n",
      "11.8 Learning both layers in a two-layer model *\n",
      "In this section, we discuss estimation of a two-layer model which is a generaliza-\n",
      "tion of the topographic ICA. The section is quite sophisticated mathematically, and\n",
      "presents ongoing work with a lot of open problems, so it can be skipped by readers\n",
      "not interested in mathematical details.\n",
      "11.8.1 Generative vs. energy-based approach\n",
      "Many of the results in the preceding chapters are related to a two-layer generative\n",
      "model. In the model, the observed variables z are generated as a linear transforma-\n",
      "tion of components s, just as in the basic ICA model: z = As. The point is to deﬁne\n",
      "the joint density of s so that it expresses the correlations of squares that seem to be\n",
      "dominant in image data.\n",
      "There are two approaches we can use. These parallel very much the sparse cod-\n",
      "ing and ICA approaches in Chapters 6 and 7. In the ﬁrst approach, typically called\n",
      "“energy-based” for historical reasons,1 we just deﬁne an objective function which\n",
      "expresses sparseness or some related statistical criterion, and maximize it. In the\n",
      "second approach, we formulate a generative model which describes how the data\n",
      "is generated starting from some elementary components. We shall consider here\n",
      "ﬁrst the generative-model approach; the energy-based model is considered in Sec-\n",
      "tion 11.8.5.\n",
      "11.8.2 Deﬁnition of the generative model\n",
      "In the generative-model approach, we deﬁne the joint density of s as follows. The\n",
      "variances d2\n",
      "i of the si are not constant, instead they are assumed to be random vari-\n",
      "ables. These random variables di are, in their turn, generated according to a model\n",
      "to be speciﬁed. After generating the variances d2\n",
      "i , the variables si are generated in-\n",
      "dependently from each other, using some conditional distributions to be speciﬁed.\n",
      "In other words, the si are independent given their variances. Dependence among the\n",
      "si is implied by the dependence of their variances.\n",
      "This is a generalization of the idea of a common variance variable presented\n",
      "in Section 7.8.3. Here, there is no single common variance variable, since there is\n",
      "a separate variance variable d2\n",
      "i corresponding to each si. However, these variance\n",
      "variables are correlated, which implies that the squares of the si are correlated. Con-\n",
      "sider the extreme case where the di are completely correlated. Then, the d2\n",
      "i are actu-\n",
      "ally the same variable, possibly multiplied by some constants. Thus, in this extreme\n",
      "1 Note that the word “energy” here has nothing to do with Fourier energy, it comes from a com-\n",
      "pletely different physical analogy.\n",
      "\n",
      "11.8 Learning both layers in a two-layer model *\n",
      "265\n",
      "case, we actually have just a single variance variable as in the divisive normalization\n",
      "model in Chapter 9.\n",
      "Many different models for the variances d2\n",
      "i could be used. We prefer here to use\n",
      "an ICA model followed by a nonlinearity:\n",
      "di = r(\n",
      "n\n",
      "∑\n",
      "k=1\n",
      "π(i,k)uk)\n",
      "(11.8)\n",
      "Here, the uk are the “higher-order” independent components used to generate the\n",
      "variances, and r is some scalar nonlinearity (possibly just the identity r(z) = z).\n",
      "The coefﬁcients π(i,k) are the entries of a higher-order feature matrix. It is closely\n",
      "related to the matrix deﬁning the topography in topographic ICA, which is why we\n",
      "use the same notation.\n",
      "This particular model can be motivated by two facts. First, taking sparse ui, we\n",
      "can model a two-layer generalization of sparse coding, where the activations (i.e. the\n",
      "variances) of the components si are sparse, and constrained to some groups of “re-\n",
      "lated” components. Related components means here components whose variances\n",
      "are strongly inﬂuenced by the same higher-order components ui.\n",
      "In the model, the distributions of the ui and the actual form of r are additional\n",
      "parameters; some suggestions will be given below. It seems natural to constrain the\n",
      "uk to be non-negative. The function r can then be constrained to be a monotonic\n",
      "transformation in the set of non-negative real numbers. This ensures that the di’s are\n",
      "non-negative, so is a natural constraint since they give the standard deviation of the\n",
      "components.\n",
      "The resulting two-layer model is summarized in Fig. 11.10. Note that the two\n",
      "stages of the generative model can be expressed as a single equation, analogously to\n",
      "(9.4), as follows:\n",
      "si = r(∑\n",
      "k\n",
      "π(i,k)uk)˜si\n",
      "(11.9)\n",
      "where ˜si is a random variable that has the same distribution as si given that di is\n",
      "ﬁxed to unity. The uk and the ˜si are all mutually independent.\n",
      "11.8.3 Basic properties of the generative model\n",
      "Here we discuss some basic properties of the generative model just deﬁned.\n",
      "11.8.3.1 The components si are uncorrelated\n",
      "This is because according to (11.9) we have\n",
      "E{sisj} = E{˜si}E{˜sj}E{r(∑\n",
      "k\n",
      "π(i,k)uk)r(∑\n",
      "k\n",
      "π(j,k)uk)} = 0\n",
      "(11.10)\n",
      "\n",
      "266\n",
      "11 Energy correlations and topographic organization\n",
      "due to the independence of the uk from ˜si and ˜sj. (Recall that ˜si and ˜sj are zero-\n",
      "mean.) To simplify things, one can deﬁne that the marginal variances (i.e. integrated\n",
      "over the distribution of di) of the si are equal to unity, as in ordinary ICA. In fact,\n",
      "we have\n",
      "E{s2\n",
      "i } = E{˜s2\n",
      "i }E{r(∑\n",
      "k\n",
      "π(i,k)uk)2},\n",
      "(11.11)\n",
      "so we only need to rescale π(i, j) (the variance of ˜si is equal to unity by deﬁnition).\n",
      "11.8.3.2 The components si are sparse\n",
      "This is true in the case where component si is assumed to have a gaussian distribu-\n",
      "tion when the variance is given. This follows from the proof given in Section 7.8.3:\n",
      "the logic developed there still applies in this two-layer model, when the marginal\n",
      "distribution of each component si is consider separately. Then, the marginal, uncon-\n",
      "ditional distributions of the components si are called gaussian scale mixtures.\n",
      "11.8.3.3 Topographic organization can be modelled\n",
      "This is possible simply by constraining the higher-order matrix π(i, j) to equal a\n",
      "topographic neighbourhood matrix as in Section 11. We can easily prove that com-\n",
      "ponents which are far from each other on the topography are then independent.\n",
      "Assume that that si and sj are such that their neighbourhoods have no overlap, i.e.\n",
      "there is no index k such that both π(i,k) and π(j,k) are non-zero. Then their vari-\n",
      "ances di and d j are independent because no higher-order component inﬂuences both\n",
      "of these variances. Thus, the components si and sj are independent as well.\n",
      "11.8.3.4 Independent subspaces are a special case\n",
      "This is more or less implied by the discussion in Section 11.4 where independent\n",
      "subspace analysis was shown to be a special case of topographic ICA. A more direct\n",
      "connection is seen by noting that each variance variable could determine the vari-\n",
      "ance inside a single subspace, with no interactions between the variance variables.\n",
      "Then we get the ISA model as explained in Section 10.4.\n",
      "11.8.4 Estimation of the generative model\n",
      "11.8.4.1 Integrating out\n",
      "In this section, we discuss the estimation of the two-layer model introduced in the\n",
      "previous section. In principle, this can be done by “integrating out” the latent vari-\n",
      "\n",
      "11.8 Learning both layers in a two-layer model *\n",
      "267\n",
      "u\n",
      "Σ\n",
      "Σ\n",
      "Σ\n",
      "2\n",
      "3\n",
      "1\n",
      "x\n",
      "x\n",
      "x\n",
      "A\n",
      "3\n",
      "r\n",
      "r\n",
      "r\n",
      "u\n",
      "u\n",
      "1\n",
      "2\n",
      "s\n",
      "s\n",
      "s\n",
      "2\n",
      "3\n",
      "1\n",
      "d\n",
      "d 1\n",
      "d\n",
      "2\n",
      "3\n",
      "Fig. 11.10: An illustration of the two-layer generative model. First, the “variance-generating” vari-\n",
      "ables ui are generated randomly. They are then mixed linearly. The resulting variables are then\n",
      "transformed using a nonlinearity r, thus giving the local variances d2\n",
      "i . Components si are then\n",
      "generated with variances d2\n",
      "i . Finally, the components si are mixed linearly to give the observed\n",
      "variables xi (which are subsequently whitened to give the zi).\n",
      "ables. Integrating out is an intuitive appealing method: since the likelihood depends\n",
      "on the values of the variance variables ui which we don’t know, why not just com-\n",
      "pute the likelihood averaged over all possible values of ui? Basically, if we have the\n",
      "joint density of the si and the ui, we could just compute the integral over the ui to\n",
      "get the density over si alone:\n",
      "p(s) =\n",
      "Z\n",
      "p(s,u)du\n",
      "(11.12)\n",
      "The problem is, as always with integration, that we may not be able to express this\n",
      "integral with a simple formula, and numerical integration may be computationally\n",
      "impossible.\n",
      "In our case, the joint density of s, i.e. the topographic components, and u, i.e. the\n",
      "higher-order independent components generating the variances, can be expressed as\n",
      "p(s,u) = p(s|u)p(u) = ∏\n",
      "i\n",
      "ps\n",
      "i(\n",
      "si\n",
      "r(∑k π(i,k)uk))\n",
      "1\n",
      "r(∑k π(i,k)uk) ∏\n",
      "j\n",
      "pu\n",
      "j(u j) (11.13)\n",
      "where the pu\n",
      "i are the marginal densities of the ui and the ps\n",
      "i are the densities of ps\n",
      "i for\n",
      "variance ﬁxed to unity. The marginal density of s could be obtained by integration:\n",
      "p(s) =\n",
      "Z\n",
      "∏\n",
      "i\n",
      "ps\n",
      "i(\n",
      "si\n",
      "r(∑k π(i,k)uk))\n",
      "∏j pu\n",
      "j(u j)\n",
      "r(∑k π(i,k)uk)du\n",
      "(11.14)\n",
      "Possibly, for some choices of the nonlinearity r and the distributions pu\n",
      "i , this integral\n",
      "could be computed easily, but no such choices are known to us.\n",
      "11.8.4.2 Approximating the likelihood\n",
      "One thing which we can do is to approximate the likelihood by an analytical expres-\n",
      "sion. This approximation actually turns out to be rather useless for the purpose of\n",
      "\n",
      "268\n",
      "11 Energy correlations and topographic organization\n",
      "estimating the two-layer model, but it shows an interesting connection to the likeli-\n",
      "hood of the topographic ICA model.\n",
      "To simplify the notation, we assume in the following that the densities pu\n",
      "i are\n",
      "equal for all i, and likewise for ps\n",
      "i. To obtain the approximation, we ﬁrst ﬁx the\n",
      "density ps\n",
      "i = ps to be gaussian, as discussed in Section 11.8.3, and we deﬁne the\n",
      "nonlinearity r as\n",
      "r(∑\n",
      "k\n",
      "π(i,k)uk) = (∑\n",
      "k\n",
      "π(i,k)uk)−1/2\n",
      "(11.15)\n",
      "The main motivation for these choices is algebraic simplicity that makes a sim-\n",
      "ple approximation possible. Moreover, the assumption of conditionally gaussian si,\n",
      "which implies that the unconditional distribution of si super-gaussian, is compatible\n",
      "with the preponderance of super-gaussian variables in ICA applications.\n",
      "With these deﬁnitions, the marginal density of s equals:\n",
      "p(s) =\n",
      "Z\n",
      "1\n",
      "√\n",
      "2π\n",
      "n exp(−1\n",
      "2 ∑\n",
      "i\n",
      "s2\n",
      "i [∑\n",
      "k\n",
      "π(i,k)uk])∏\n",
      "i\n",
      "pu(ui)\n",
      "r\n",
      "∑\n",
      "k\n",
      "π(i,k)uk du\n",
      "(11.16)\n",
      "which can be manipulated to give\n",
      "p(s) =\n",
      "Z\n",
      "1\n",
      "√\n",
      "2π\n",
      "n exp(−1\n",
      "2 ∑\n",
      "k\n",
      "uk[∑\n",
      "i\n",
      "π(i,k)s2\n",
      "i ])∏\n",
      "i\n",
      "pu(ui)\n",
      "r\n",
      "∑\n",
      "k\n",
      "π(i,k)uk du.\n",
      "(11.17)\n",
      "The interesting point in this form of the density is that it is a function of the “local\n",
      "energies” ∑i π(i,k)s2\n",
      "i only. The integral is still intractable, though. Therefore, we\n",
      "use the simple approximation:\n",
      "r\n",
      "∑\n",
      "k\n",
      "π(i,k)uk ≈\n",
      "p\n",
      "π(i,i)ui.\n",
      "(11.18)\n",
      "This is actually a lower bound, and thus our approximation will be a lower bound of\n",
      "the likelihood as well. This gives us the following approximation ˜p(s):\n",
      "˜p(s) = ∏\n",
      "k\n",
      "exp(G(∑\n",
      "i\n",
      "π(i,k)s2\n",
      "i ))\n",
      "(11.19)\n",
      "where the scalar function G is obtained from the pu by:\n",
      "G(y) = log\n",
      "Z\n",
      "1\n",
      "√\n",
      "2π exp(−1\n",
      "2uy)pu(u)\n",
      "p\n",
      "π(i,i)udu.\n",
      "(11.20)\n",
      "Recall that we assumed π(i,i) to be constant.\n",
      "Next, using the same derivation as in ICA, we obtain the likelihood of the data as\n",
      "log ˜L(V) =\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "G(\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "π(i, j)(vT\n",
      "i z(t))2)+T log|detV|.\n",
      "(11.21)\n",
      "\n",
      "11.8 Learning both layers in a two-layer model *\n",
      "269\n",
      "where V = (v1,...,vn)T = A−1, and the z(t),t = 1,...,T are the observations of z.\n",
      "It is here assumed that the neighbourhood function and the nonlinearity r as well\n",
      "as the densities pu\n",
      "i and ps\n",
      "i are known. This approximation is a function of local\n",
      "energies. Every term ∑n\n",
      "i=1 π(i, j)(vT\n",
      "i z(t))2 could be considered as the energy of a\n",
      "neighbourhood, related to the output of a higher-order neuron as in complex cell\n",
      "models. The function G has a similar role as the log-density of the independent\n",
      "components in ICA; the corresponding function h is basically obtained as h(u) =\n",
      "G(\n",
      "p\n",
      "|u|).\n",
      "The formula for G in (11.20) can be analytically evaluated only in special cases.\n",
      "One such case is obtained if the uk are obtained as squares of standardized gaussian\n",
      "variables. Straight-forward calculation then gives the following function\n",
      "G0(y) = −log(1 +y)+const.\n",
      "(11.22)\n",
      "However, in ICA, it is well-known that the exact form of the log-density does not\n",
      "affect the consistency of the estimators, as long as the overall shape of the function\n",
      "is correct. This is probably true in topographic ICA as well.\n",
      "11.8.4.3 Difﬁculty of estimating the model\n",
      "What we have really shown in deriving the approximation of likelihood in Equa-\n",
      "tion (11.21) is that the heuristically justiﬁed objective function in Equation (11.5)\n",
      "can be obtained from the two-layer generative model as an approximation. But we\n",
      "have not really got any closer to the goal of estimating both layers of weights. This\n",
      "is because the approximation used here approximates the dependence of the likeli-\n",
      "hood from π quite badly. To see why, consider maximization of the approximative\n",
      "likelihood in Equation (11.21) with respect to the π(i, j). Take G as in (11.22). Now,\n",
      "∑n\n",
      "i=1 π(i, j)(vT\n",
      "i z(t))2 is always non-negative. On the other hand, G attains its max-\n",
      "imum at zero. So, if we simply take π(i, j) = 0 for all i, j, G is actually always\n",
      "evaluated at zero and the approximative likelihood is maximized. So, taking all ze-\n",
      "ros in π is the maximum, which is absurd!\n",
      "One approach would be to ﬁnd the values of the latent variables ui which max-\n",
      "imize the likelihood, treating the ui like the parameters. Thus, we would not try to\n",
      "integrate out the ui, but rather just formulate the joint likelihood of V,π(i, j),ui(t)\n",
      "for all i, j and all t = 1,...,T. This is computationally very difﬁcult because the la-\n",
      "tent variables di are different for each image patch, so there is a very large number of\n",
      "them. The situation could be simpliﬁed by ﬁrst estimating the ﬁrst layer by ordinary\n",
      "ICA, and then ﬁxing V once and for all (Karklin and Lewicki, 2005). However, this\n",
      "does not reduce the number of dimensions.\n",
      "So, we see that the estimation of both layers in a generative two-layer model is\n",
      "quite difﬁcult. However, abandoning the generative-model approach simpliﬁes the\n",
      "situation, and provides a promising approach, which will be treated next.\n",
      "\n",
      "270\n",
      "11 Energy correlations and topographic organization\n",
      "11.8.5 Energy-based two-layer models\n",
      "A computationally simpler alternative to estimation of the two layers is provided by\n",
      "an “energy-based” approach. The idea is to take the likelihood in Equation (11.5) as\n",
      "the starting point. As pointed out above, it does not make sense to try to maximize\n",
      "this with respect to the π, because the maximum is obtained by taking all zeros as\n",
      "the second layer weights.\n",
      "There is a deep mathematical reason why we cannot maximize the likelihood\n",
      "in Equation (11.5) with respect to the π. The reason is that the likelihood is not\n",
      "normalized. That is, when we interpret the likelihood as a pdf, its integral over the\n",
      "data variables is not equal to one: the integral depends on the values of the π. This\n",
      "means it is not a properly deﬁned pdf, because a pdf must always integrate to one,\n",
      "so the likelihood is not a properly deﬁned likelihood either. To alleviate this, we\n",
      "have to introduce what is called a normalization constant or a partition function in\n",
      "the likelihood. The normalization constant, which is actually not a constant but a\n",
      "function of the model parameters, is chosen so that it makes the integral equal to\n",
      "one. Denoting the normalization constant by Z(π), we write\n",
      "logL(v1,...,vn) =\n",
      "T\n",
      "∑\n",
      "t=1∑\n",
      "i\n",
      "h(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "π(i, j)(vT\n",
      "i zt)2)−log|detV|−logZ(π)\n",
      "(11.23)\n",
      "See Section 13.1.5 and Chapter 21 for more discussion on the normalization con-\n",
      "stant.\n",
      "In principle, the normalization constant can be computed by computing the inte-\n",
      "gral of the underlying pdf over the space of the v, but this is extremely complicated\n",
      "numerically. Fortunately, there is a way around this problem, which is to use spe-\n",
      "cial estimation methods which do not require the normalization constant. Thus, we\n",
      "abandon maximization of likelihood, because it requires that we compute the nor-\n",
      "malization constant. See Chapter 21 for information on such methods.\n",
      "Attempts to estimate both layers in a two-layer model, using an energy-based\n",
      "approach, and estimation methods which circumvent the need for a normalization\n",
      "constant, can be found in (Osindero et al, 2006; K¨oster and Hyv¨arinen, 2007, 2008).\n",
      "This is a very active area of research (Karklin and Lewicki, 2008). Some more re-\n",
      "motely related work is in (K¨oster et al, 2009a).\n",
      "11.9 Concluding remarks and References\n",
      "A simple modiﬁcation of the model of independent subspace analysis leads to emer-\n",
      "gence of topography, i.e. the spatial arrangement of the features. This is in contrast\n",
      "to ICA and ISA, in which the features are in random order. (In ISA, it is the sub-\n",
      "spaces which are in random order, but the linear features have some organization\n",
      "because of their partition to subspaces.) The basic idea in modelling topography is\n",
      "to consider subspaces which are overlapping, so that the neighbourhood of each cell\n",
      "\n",
      "11.9 Concluding remarks and References\n",
      "271\n",
      "is one subspace. It is also possible to formulate a proper generative model which in-\n",
      "corporates the same kind of statistical dependencies using variance variables which\n",
      "are generated by a higher-order ICA model, but that approach is mathematically\n",
      "difﬁcult and still under construction.\n",
      "Basic though old papers on topography are (Hubel and Wiesel, 1968; DeValois\n",
      "et al, 1982). Optical imaging results are shown in (Blasdel, 1992), and a recent high-\n",
      "resolution imaging study is in (Ohki et al, 2005). Topography with respect to spatial\n",
      "frequency is investigated in (Tootell et al, 1988; Silverman et al, 1989; Edwards et al,\n",
      "1996). Seminal papers on pinwheels are (Bonhoeffer and Grinvald, 1991; Maldon-\n",
      "ado et al, 1997). A most interesting recent paper is (DeAngelis et al, 1999) that also\n",
      "shows that the phases are not correlated in neighbouring cells. The relationships of\n",
      "the topographic representation for different parameters are considered in (H¨ubener\n",
      "et al, 1997). An important point is made in (Yen et al, 2007), who show that the\n",
      "topography of responses is not so clear when the stimuli are complex, presumable\n",
      "due to nonlinear interactions. The connection between topography and complex cell\n",
      "pooling is discussed in (Blasdel, 1992; DeAngelis et al, 1999).\n",
      "The idea of minimum wiring length, or wiring economy, goes back to Ram´on\n",
      "y Cajal, cited in (Chen et al, 2006). The metabolic advantages of topography are\n",
      "further considered in (Durbin and Mitchison, 1990; Mitchison, 1992; Koulakov and\n",
      "Chklovskii, 2001; Attwell and Laughlin, 2001). Comparisons between white and\n",
      "grey matter volume also point out how brain (skull) size limits the connectivity\n",
      "(Zhang and Sejnowski, 2000).\n",
      "Original papers describing the topographic ICA models are (Hyv¨arinen and\n",
      "Hoyer, 2001; Hyv¨arinen et al, 2001a). Kohonen’s famous self-organizing map is\n",
      "also closely related (Kohonen, 1982, 2001), but it has not been shown to produce\n",
      "a realistic V1-like topography; reasons for this were discussed in Section 11.7.2.\n",
      "A model which produces more a realistic topography (but still no low-frequency\n",
      "blobs) is Kohonen’s ASSOM model (Kohonen, 1996; Kohonen et al, 1997). How-\n",
      "ever, in that model the nature of the topography is strongly inﬂuenced by an artiﬁcial\n",
      "manipulation of the input (a sampling window that moves smoothly in time), and it\n",
      "does not really emerge from the structure of images alone.\n",
      "A related idea on minimization of wiring length has been proposed in (Vincent\n",
      "and Baddeley, 2003; Vincent et al, 2005), in which it is proposed that the retinal\n",
      "coding minimizes wiring, whereas cortical coding maximizes sparseness of activi-\n",
      "ties.\n",
      "\n",
      "\n",
      "Chapter 12\n",
      "Dependencies of energy detectors: Beyond V1\n",
      "All the models in this book so far have dealt with the primary visual cortex (V1).\n",
      "In this chapter, we show how statistical models of natural images can be extended\n",
      "to deal with properties in the extrastriate cortex, i.e. those areas which are close to\n",
      "V1 (also called the striate cortex) and to which the visual information is transmitted\n",
      "from V1.\n",
      "12.1 Predictive modelling of extrastriate cortex\n",
      "Most of the experimental results in early cortical visual processing have considered\n",
      "V1. The function of most extrastriate areas is still rather much a mystery. Likewise,\n",
      "most research in modelling natural image statistics has been on low-level features,\n",
      "presumably corresponding to V1.\n",
      "However, the methodology that we used in this book could possibly be extended\n",
      "to such extrastriate areas as V2, V3(A), V4, and V5. Actually, since the function of\n",
      "most extrastriate areas is not well understood, it would be most useful if we could\n",
      "use this modelling endeavour in a predictive manner, so that we would be able to\n",
      "predict properties of cells in the visual cortex, in cases where the properties have\n",
      "not yet been demonstrated experimentally. This would give testable, quantitative\n",
      "hypotheses that might lead to great advances in visual neuroscience.\n",
      "In the next sections, we attempt to accomplish such predictive modelling in order\n",
      "to predict properties of a third processing step, following the simple and complex\n",
      "cell layers. The predictions should be based on the statistical properties of modelled\n",
      "complex-cell outputs. Our method is to apply ordinary independent component anal-\n",
      "ysis to modelled outputs of complex cells whose input consists of natural images.1\n",
      "1 This chapter is based on the article (Hyv¨arinen et al, 2005a), originally published in BMC Neu-\n",
      "roscience. The experiments were done by Michael Gutmann. Copyright retained by the authors.\n",
      "273\n",
      "\n",
      "274\n",
      "12 Dependencies of energy detectors: Beyond V1\n",
      "12.2 Simulation of V1 by a ﬁxed two-layer model\n",
      "The basic idea in this chapter is to ﬁx a model of complex cells and then learn\n",
      "a representation for complex cell outputs using a statistical model. The resulting\n",
      "three-layer network is depicted in Fig. 12.1.\n",
      "This approach is rather different from the one used in previous chapters, in which\n",
      "we learned ﬁrst the simple cells and then the complex cells from the data. Here, to\n",
      "simplify the model and the computations, we do not attempt to learn everything\n",
      "at the same time. Instead, we ﬁx the ﬁrst two layers (simple and complex cells)\n",
      "according to well-known models, and learn only the third layer.\n",
      "image\n",
      "complex cells\n",
      "contour cells\n",
      "simple cells\n",
      "i\n",
      "ij\n",
      "a\n",
      "s\n",
      "cj\n",
      "Fig. 12.1: The simpliﬁed hierarchical model investigated in this chapter. Modelled complex-cell\n",
      "responses are calculated in a feedforward manner, and these responses are subsequently analyzed\n",
      "by a higher-order feature layer in the network (“contour” layer). To emphasise that the lower layers\n",
      "are ﬁxed and not learned, these layers have been greyed out in the ﬁgure. The direction of the arrows\n",
      "is from higher features to lower ones which is in line with the interpretation of our analysis as a\n",
      "generative model.\n",
      "The classic complex-cell model is based on Gabor functions. As explained in\n",
      "Section 3.4.2, complex cells can be modelled as the sum of squares of two Gabor\n",
      "functions which are in quadrature phase. Quadrature phase means, simply, that if\n",
      "one of them is even-symmetric, the other one is odd-symmetric. This is related to\n",
      "computation of the Fourier energy locally, as explained in Section 2.4.\n",
      "Complex-cell responses ck to natural images were thus modelled with a Gabor\n",
      "energy model of the following form:\n",
      "ck =\n",
      " \n",
      "∑\n",
      "x,y\n",
      "W o\n",
      "k (x,y)I(x,y)\n",
      "!2\n",
      "+\n",
      " \n",
      "∑\n",
      "x,y\n",
      "W e\n",
      "k (x,y)I(x,y)\n",
      "!2\n",
      "(12.1)\n",
      "\n",
      "12.2 Simulation of V1 by a ﬁxed two-layer model\n",
      "275\n",
      "where W e\n",
      "k and W o\n",
      "k are even- and odd-symmetric Gabor receptive ﬁelds; the equa-\n",
      "tion shows that their squares (energies) are pooled together in the complex cell. The\n",
      "complex cells were arranged on a 6 × 6 spatial grid. They had 6 × 6 = 36 differ-\n",
      "ent spatial locations, and at each location, four different preferred orientations and\n",
      "three different frequency selectivities (“bands”). The aspect ratio (ratio of spatial\n",
      "length to width) was ﬁxed to 1.5. The frequency selectivities of the Gabor ﬁlters\n",
      "are shown in Figure 12.2, in which all the ﬁlters W were normalized to unit norm\n",
      "for visualization purposes. The actual normalization we used in the experiments\n",
      "consisted of standardizing the variances of the complex cell outputs so that they\n",
      "were equal to unity for natural image input. The number of complex cells totalled\n",
      "36 ×4 ×3 = 432.\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1\n",
      "Spatial Frequency [cycles/pixel]\n",
      "Filter Response\n",
      "Fig. 12.2: We used ﬁxed complex cells with three different frequency selectivities. The amplitudes\n",
      "of the Fourier Transforms of the odd-symmetric Gabor ﬁlters are shown here. The selectivities are\n",
      "such that each cell is sensitive to a certain frequency “band”. The underlying Gabor ﬁlters had\n",
      "logarithmically spaced frequency peaks. Peak spatial frequencies were chosen as follows: f1 = 0.1\n",
      "cycles/pixel, f2 = 0.21 cycles/pixel and f3 = 0.42 cycles/pixel.\n",
      "As the basic data, we used 1008 grey-scale natural images of size 1024 × 1536\n",
      "pixels from van Hateren’s database2. We manually chose natural images in the nar-\n",
      "rower sense, i.e. only wildlife scenes. From the source images, 50,000 image patches\n",
      "of size 24 × 24 pixels were randomly extracted. The mean grey value of each im-\n",
      "age patch was subtracted and the pixel values were rescaled to unit variance. The\n",
      "resulting image patch will be denoted by I(x,y).\n",
      "2 Available at http://hlab.phys.rug.nl/imlib/index.html , category “deblurred”\n",
      "(van Hateren and van der Schaaf, 1998).\n",
      "\n",
      "276\n",
      "12 Dependencies of energy detectors: Beyond V1\n",
      "12.3 Learning the third layer by another ICA model\n",
      "After ﬁxing the ﬁrst two layers, we learned the feature weights in the third layer\n",
      "by doing a simple ICA of the complex cell (second-layer) outputs denoted by ck.\n",
      "No PCA dimension reduction was done here, so the number of independent com-\n",
      "ponents equals the number of complex cells, K. Thus, ICA was performed on the\n",
      "vector c = (c1,...,cK) using the FastICA algorithm (see Section 18.7). In ICA, the\n",
      "orthogonalization approach was symmetric. Different nonlinearities g were used,\n",
      "see Table 12.1. (The nonlinearities are related to the non-gaussianity measures used,\n",
      "see Section 18.7.)\n",
      "Non-gaussianity measure\n",
      "FastICA nonlinearity Motivation\n",
      "G1(y) = logcoshy\n",
      "g1(y) = tanh(y)\n",
      "Basic sparseness measure\n",
      "G2(y) = −exp(−y2/2)\n",
      "g2(y) = yexp(−y2/2) More robust variant of g1\n",
      "G3(y) = 1\n",
      "3y3\n",
      "g3(y) = y2\n",
      "Skewness (asymmetry)\n",
      "G4(y) = Gaussian cum. distr. function g4(y) = exp(−y2/2)\n",
      "Robust variant of g3\n",
      "Table 12.1: The measures of non-gaussianity used, i.e. the different functions G = log ps used in\n",
      "the likelihood of the ICA model. These correspond to different nonlinearities g in the FastICA\n",
      "algorithm, and to different sparseness measures h. The measures probe the non-gaussianity of the\n",
      "estimated components in different ways.\n",
      "Thus we learned (estimated) a linear decomposition of the form\n",
      "ck =\n",
      "K\n",
      "∑\n",
      "i=1\n",
      "akisi for all k = 1,...,K\n",
      "(12.2)\n",
      "or in vector form\n",
      "c =\n",
      "K\n",
      "∑\n",
      "i=1\n",
      "aisi = As\n",
      "(12.3)\n",
      "where each vector ai = (a1i,...,aKi) gives a higher-order feature vector. The si de-\n",
      "ﬁne the values of the higher-order features in the third cortical processing stage.\n",
      "Recall that the input to the system was natural images, so the statistics of c reﬂect\n",
      "natural image statistics.\n",
      "Note that the signs of the feature vectors are not deﬁned by the ICA model, i.e.\n",
      "the model does not distinguish between ai and −ai because any change in sign of the\n",
      "feature vector can be cancelled by changing the sign of si accordingly. Here, unlike\n",
      "in the original natural images, the features will not be symmetric with respect to\n",
      "such a change of sign, so it makes sense to deﬁne the signs of the ai based on that\n",
      "asymmetry. We deﬁned the sign for each vector ai so that the sign of the element\n",
      "with the maximal absolute value was positive.\n",
      "This model can be interpreted as a generative model of image patches, following\n",
      "the interpretation of ISA as a nonlinear ICA in Section 10.6. The higher-order inde-\n",
      "pendent component (here denoted by si) are generated according to Equation (12.2).\n",
      "\n",
      "12.4 Methods for analysing higher-order components\n",
      "277\n",
      "Then, the activity of the complex cell is expressed as activities of simple cells with\n",
      "random division of the activity to the simple cells, using a random angle variable\n",
      "as in Equation (10.17) on page 235. Finally, the simple cell activities are linearly\n",
      "transformed to image patches as in ICA or ISA models. This provides a complete\n",
      "generative model from the higher-order features to image pixel values.\n",
      "12.4 Methods for analysing higher-order components\n",
      "We need to introduce some special methods to analyze the “higher-order” compo-\n",
      "nents obtained by this method, because the resulting higher-order feature vectors ai\n",
      "cannot be simply plotted in the form of image patches.\n",
      "We visualize the vectors ai by plotting an ellipse at the centerpoint of each com-\n",
      "plex cell. The orientation of the ellipse is the orientation of the complex cell with\n",
      "index k, and the brightness of the ellipse with index i is proportional to the coefﬁ-\n",
      "cient aki of the feature vector ai, using a grey-scale coding of coefﬁcient values. We\n",
      "plotted complex cells in each frequency band (i.e. with the same frequency selectiv-\n",
      "ity) separately.\n",
      "We are also interested in the frequency pooling of complex cells in different\n",
      "higher-order features. We quantiﬁed the pooling over frequencies using a simple\n",
      "measure deﬁned as follows. Let us denote by ai(x,y,θ, fn) the coefﬁcient in the\n",
      "higher-order feature vector ai that corresponds to the complex cell with spatial loca-\n",
      "tion (x,y), orientation θ and preferred frequency fn. We computed a quantity which\n",
      "is similar to the sums of correlations of the coefﬁcients over the three frequency\n",
      "bands, but normalized in a slightly different way. This measure Pi was deﬁned as\n",
      "follows:\n",
      "Pi = ∑\n",
      "m<n\n",
      "|∑x,y,θ ai(x,y,θ, fm)ai(x,y,θ, fn)|\n",
      "CmCn\n",
      "(12.4)\n",
      "where the normalization constant Cm is deﬁned as\n",
      "Cm =\n",
      "s\n",
      "1\n",
      "K ∑\n",
      "j,x,y,θ\n",
      "a j(x,y,θ, fm)2\n",
      "(12.5)\n",
      "and likewise for Cn.\n",
      "For further analysis of the estimated feature vectors, we deﬁned the preferred\n",
      "orientation of a higher-order feature. First, let us deﬁne for a higher-order fea-\n",
      "ture of index i the hot-spot (xi,yi)∗as the centre location (x,y) of complex cells\n",
      "where the higher-order component si generates the maximum amount of activity.\n",
      "That is, we sum the elements of ai that correspond to a single spatial location, and\n",
      "choose the largest sum. This allows us to deﬁne the tuning to a given orientation of\n",
      "a higher-order feature i by summing over the elements of ai that correspond to the\n",
      "spatial hotspot and a given orientation; the preferred orientation is the orientation for\n",
      "which this sum is maximized. We also computed the length of a higher-order feature\n",
      "\n",
      "278\n",
      "12 Dependencies of energy detectors: Beyond V1\n",
      "by least-squares ﬁtting a gaussian kernel to the patterns ai (Hoyer and Hyv¨arinen,\n",
      "2002).\n",
      "It is also possible to perform an image synthesis from a higher-order feature\n",
      "vector. However, the mapping from image to complex-cell outputs is not one-to-\n",
      "one. This means that the generation of the image is not uniquely deﬁned given the\n",
      "activities of higher-order features alone. A unique deﬁnition can be achieved by\n",
      "constraining the phases of the complex cells. For the purposes of image synthesis,\n",
      "we assume that only odd-symmetric Gabor ﬁlters are active. Furthermore, we make\n",
      "the simplifying assumptions that the receptive ﬁelds W in simple cells are equal to\n",
      "the corresponding feature vectors, and that all the elements in the higher-order fea-\n",
      "ture vector are non-negative (or small enough to be ignored). Then, the synthesized\n",
      "image Ii\n",
      "synth for higher-order feature vector ai is given by\n",
      "Ii\n",
      "synth(x,y) = ∑\n",
      "k∈H\n",
      "W o\n",
      "k (x,y)√aki\n",
      "(12.6)\n",
      "where the square root cancels the squaring operation in the computation of complex-\n",
      "cell responses, and H denotes the set of indices that correspond to complex cells of\n",
      "the preferred orientation at the hotspot. Negative values of aki were set to zero in\n",
      "this formula.\n",
      "Since we are applying ICA on data which has been heavily processed (by the\n",
      "complex cell model), we have to make sure that the model is not only analyzing the\n",
      "artefacts produced by that processing. To obtain a baseline with which to compare\n",
      "our results, and to show which part of the results is due to the statistical properties\n",
      "of natural images instead of some intrinsic properties of our ﬁlterbank and analysis\n",
      "methods, we did exactly the same kind of analysis for 24 × 24 image patches that\n",
      "consisted of white gaussian noise, i.e. the grey-scale value in each pixel was ran-\n",
      "domly and independently drawn from a gaussian distribution of zero mean and unit\n",
      "variance. The white gaussian noise input provides a “chance level” for any quantities\n",
      "computed from the ICA results. In a control experiment, such white noise patches\n",
      "were thus fed to the complex cell model, and the same kind of ICA was applied on\n",
      "the outputs.\n",
      "12.5 Results on natural images\n",
      "12.5.1 Emergence of collinear contour units\n",
      "In the ﬁrst experiment, we used only the output from complex cells in a single\n",
      "frequency band, f2 in Figure 12.2.\n",
      "The higher-order features are represented by their feature vectors ai which show\n",
      "the contribution of the third-stage feature of index i on the activities of complex\n",
      "cells. A collection of the obtained feature vectors is shown in Figure 12.3 for the\n",
      "nonlinearity g1 (see Table 12.1), visualized as described above. We can see emer-\n",
      "\n",
      "12.5 Results on natural images\n",
      "279\n",
      "gence of collinear features. That is, the higher-order features code for the simulta-\n",
      "neous activation of complex cells that together form something similar to a straight\n",
      "line segment.\n",
      "Those coefﬁcients that are clearly different from zero have almost always the\n",
      "same sign in a single feature vector. Deﬁning the sign as explained above, this means\n",
      "that the coefﬁcients are essentially non-negative.3\n",
      "Other measures of non-gaussianity (FastICA nonlinearities) led to similar feature\n",
      "vectors. However, some led to a larger number of longer contours. Figure 12.4 shows\n",
      "the distribution of lengths for different nonlinearities. The nonlinearity g4 (robust\n",
      "skewness) seems to lead to the largest number of long contours. The outputs of\n",
      "complex cells are skewed (non-symmetric), so it makes sense to use a skewness-\n",
      "based measure of non-gaussianity, as discussed in Section 7.9. In this experiment,\n",
      "the results were very similar to those obtained by sparseness, however.\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Fig. 12.3: Random selection of learned feature vectors ai when the complex cells are all in a single\n",
      "frequency band. ICA nonlinearity g was the tanh nonlinearity g1. Each patch gives the coefﬁcients\n",
      "of one higher-order feature. Each ellipse means that the complex cell in the corresponding location\n",
      "and orientation is present in the higher-order feature, the brightness of the ellipse is proportional to\n",
      "the coefﬁcient aki.\n",
      "12.5.2 Emergence of pooling over frequencies\n",
      "In the second experiment, the complex cell set was expanded to include cells of\n",
      "three different preferred frequencies. In total, there were now 432 complex cells.\n",
      "We performed ICA on the complex cell outputs when their input consisted of nat-\n",
      "ural images. Thus, we obtained 432 higher-order feature vectors (features) ai with\n",
      "corresponding activities si.\n",
      "3 In earlier work (Hoyer and Hyv¨arinen, 2002) we actually imposed a non-negativity constraint\n",
      "on the coefﬁcients, see Section 13.2. The results reported here show that those results can be\n",
      "replicated using ordinary ICA methods. The constraint of non-negativity of the feature vectors has\n",
      "little impact on the results: even without this constraint, the system learns feature vectors which\n",
      "are mainly non-negative.\n",
      "\n",
      "280\n",
      "12 Dependencies of energy detectors: Beyond V1\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Length\n",
      "Percentage of cell population\n",
      "g1\n",
      "g2\n",
      "g3\n",
      "g4\n",
      "Fig. 12.4: Comparison of different measures of non-gaussianity (FastICA nonlinearities) in the\n",
      "ﬁrst experiment. The histogram gives the lengths of the contour patterns for the four different\n",
      "nonlinearities g1,...,g4 in Table 12.1.\n",
      "We visualized a random selection of higher-order features learned from natural\n",
      "images in Figure 12.5. The visualization shows that the features tend to be spatially\n",
      "localized and oriented, and show collinearity as in the single-channel experiment\n",
      "above. What is remarkable in these results is that many cells pool responses over\n",
      "different frequencies. The pooling is coherent in the sense that the complex cells\n",
      "that are pooled together have similar locations and orientations. A smaller number of\n",
      "cells is shown in more detail in Figure 12.6, where the coefﬁcients in all orientations\n",
      "are shown separately.\n",
      "We computed the frequency pooling measure Pi in Equation (12.4) for the learned\n",
      "feature vectors. The distribution of this measure for natural image input and white\n",
      "gaussian noise input is shown in Figure 12.7. The ﬁgure shows that frequency pool-\n",
      "ing according to this measure was essentially nonexistent for white gaussian noise\n",
      "input, but relatively strong for many feature vectors when the input consisted of\n",
      "natural images. To express this more quantitatively, we computed the 99% quantile\n",
      "for the white gaussian noise input. Then, 59% of the basis vectors for natural image\n",
      "input had a pooling index Pi that was larger than this quantile. (For the 95% quantile\n",
      "the proportion was 63%.) Thus, we can say that more than half of the higher-order\n",
      "basis vectors, when learned from natural images, have a pooling over frequencies\n",
      "that is signiﬁcantly above chance level.\n",
      "To show that the pooling measure is valid, and to further visualize the frequency\n",
      "pooling in the higher-order features, we chose randomly feature vectors learned\n",
      "from natural images that have pooling signiﬁcantly over chance level (Pi above its\n",
      "99% quantile for white gaussian noise). These are plotted in Figure 12.8. Visual in-\n",
      "spection shows that in this subset, all basis vectors exhibit pooling over frequencies\n",
      "that respects the orientation tuning and collinearity properties.\n",
      "The corresponding results when the input is white gaussian noise are shown in\n",
      "Figure 12.9, for a smaller number of higher-order cells . (To make the comparison\n",
      "fair, these were randomly chosen among the 59% that had higher pooling mea-\n",
      "\n",
      "12.5 Results on natural images\n",
      "281\n",
      "sures, the same percentage as in Figure 12.8.) Pooling over frequencies as well as\n",
      "collinearity are minimal. Some weak reﬂections of these properties can be seen, pre-\n",
      "sumably due to the small overlap of the ﬁlters in space and frequency, which leads\n",
      "to weak statistical correlations between complex cells that are spatially close to each\n",
      "other or in neighbouring frequency bands.\n",
      "We also examined quantitatively whether the higher-order features are tuned to\n",
      "orientation. We investigated which complex cell has the maximum weight in ai for\n",
      "each i in each frequency band. When the data used in learning consisted of natural\n",
      "images, in 86% of the cells the maximally weighted complex cells were found to be\n",
      "located at the hot-spot (xi,yi)∗(i.e., point of maximum activity, see above) and tuned\n",
      "to the preferred orientation of the higher-order feature for every frequency f. This\n",
      "shows how the higher-order features are largely selective to a single orientation.\n",
      "When the data used in learning consisted of gaussian white noise, only 34% of the\n",
      "cells were found to be orientation-selective according to this criterion.\n",
      "Frequency [cycles/pixel]\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      "−0.05\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1\n",
      "Fig. 12.5: A random selection of higher-order feature vectors ai estimated from natural images\n",
      "using complex cells of multiple frequencies in the second experiment. ICA nonlinearity g was the\n",
      "tanh nonlinearity g1. Each display of three patches gives the coefﬁcients of one higher-order fea-\n",
      "ture. Each patch gives the coefﬁcients of one higher-order feature in one frequency band. Each\n",
      "ellipse means that the complex cell in the corresponding location, and of the corresponding orien-\n",
      "tation and frequency is present in the higher-order feature, brightness of ellipse is proportional to\n",
      "coefﬁcient aki.\n",
      "Finally, we synthesized images from higher-order feature activities. Figure 12.10\n",
      "shows a slice orthogonal to the preferred orientation of one higher-order feature\n",
      "vector (H209 in Figure 12.6). The intensity of the synthesized image shows no side-\n",
      "lobes (unnecessary oscillations), while representing a sharp, localized edge. In con-\n",
      "trast, synthesis in the white gaussian noise case (also shown in Figure 12.10) gives\n",
      "curves that have either side-lobes like the underlying Gabor ﬁlters, or do not give a\n",
      "\n",
      "282\n",
      "12 Dependencies of energy detectors: Beyond V1\n",
      "Orientation [degree]\n",
      "Frequency [cycles/pixel]\n",
      "H150\n",
      " 90\n",
      " 45\n",
      "  0\n",
      " 45\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Orientation [degree]\n",
      "Frequency [cycles/pixel]\n",
      "H209\n",
      " 90\n",
      " 45\n",
      "  0\n",
      " 45\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Orientation [degree]\n",
      "Frequency [cycles/pixel]\n",
      "H218\n",
      " 90\n",
      " 45\n",
      "  0\n",
      " 45\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Orientation [degree]\n",
      "Frequency [cycles/pixel]\n",
      "H235\n",
      " 90\n",
      " 45\n",
      "  0\n",
      " 45\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Fig. 12.6: Higher-order feature vectors of four selected higher-order features in the second ex-\n",
      "periment, shown in detail. The coefﬁcients in each orientation and frequency band are plotted\n",
      "separately.\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "2.5\n",
      "3\n",
      "3.5\n",
      "4\n",
      "4.5\n",
      "5\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Percentage of cell population\n",
      "Pooling Measure\n",
      "White Gaussian Noise\n",
      "Natural images\n",
      "Fig. 12.7: The distributions of the frequency pooling measure in Equation (12.4) for natural images\n",
      "and white gaussian noise.\n",
      "\n",
      "12.5 Results on natural images\n",
      "283\n",
      "Frequency [cycles/pixel]\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      "−0.08\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1\n",
      "Fig. 12.8: A selection of higher-order feature vectors ai estimated from natural images in the second\n",
      "experiment. These basis vectors were chosen randomly among those that have frequency pooling\n",
      "signiﬁcantly above chance level.\n",
      "Frequency [cycles/pixel]\n",
      " 0.1\n",
      "0.21\n",
      "0.42\n",
      "0\n",
      "0.25\n",
      "0.5\n",
      "0.75\n",
      "1\n",
      "Fig. 12.9: For comparison, higher-order feature vectors estimated from white gaussian noise, with\n",
      "each frequency band shown separately.\n",
      "sharp localized edge. Thus, the curve obtained from synthesis of the features learned\n",
      "from natural images corresponds better to the notion of an edge.\n",
      "\n",
      "284\n",
      "12 Dependencies of energy detectors: Beyond V1\n",
      "0\n",
      "Fig. 12.10: Local image synthesis from the three odd-symmetric Gabor elements that have pre-\n",
      "ferred orientation at the hotspot of a higher-order feature vector (H209 in Figure 12.6). The thick\n",
      "dotted curve shows the synthesis using coefﬁcients from natural images, and the solid curves show\n",
      "various synthesis results using coefﬁcients learned from white gaussian noise input.\n",
      "12.6 Discussion of results\n",
      "12.6.1 Why coding of contours?\n",
      "The result of the ﬁrst experiment, using a single frequency channel (Section 12.5.1),\n",
      "is that simple ICA of simulated complex cell outputs leads to emergence of units\n",
      "coding for collinear contours (Figure 12.3). First, we have to note that this result is\n",
      "not logically necessary: It is not obvious that the higher-order representation should\n",
      "necessarily code for contours. Multi-layer mechanisms similar to the one used here\n",
      "have been proposed in the context of texture segregation as well (Sperling, 1989;\n",
      "Malik and Perona, 1990). A priori, one could have expected such texture bound-\n",
      "ary detectors to emerge from this model. Our results seem to indicate that contour\n",
      "coding is, at least in this sparse coding sense, more fundamental than texture segre-\n",
      "gation.\n",
      "The higher-order neurons which represent long contours bear many similarities\n",
      "to ‘collator’ (or ‘collector’) units, proposed in the psychophysical literature (Mussap\n",
      "and Levi, 1996; Moulden, 1994). Such units are thought to integrate the responses\n",
      "of smaller, collinear ﬁlters, to give a more robust estimate of global orientation than\n",
      "could be achieved with elongated linear mechanisms.4\n",
      "4 In principle, long contours could be represented by long feature vectors on the level of simple\n",
      "cells as well. However, the representation by these higher-order contour coding cells has the ad-\n",
      "vantage of being less sensitive to small curvature and other departures from strict collinearity. Even\n",
      "very small curvature can completely change the response of an elongated linear ﬁlter (simple cell),\n",
      "but it does not change the representation on this higher level, assuming that the curvature is so small\n",
      "that the line stays inside the receptive ﬁelds of the same complex cells. Thus, higher-order contour\n",
      "cells give a more robust representation of the contours. Of course, the intermediate complex cell\n",
      "layer also confers some phase-invariance to the contour detectors.\n",
      "\n",
      "12.6 Discussion of results\n",
      "285\n",
      "12.6.2 Frequency channels and edges\n",
      "In the second experiment using multiple frequency channels (Section 12.5.2), we\n",
      "saw emergence of pooling of contour information across multiple frequencies (Fig-\n",
      "ures 12.5,12.6,12.8).What is the functional meaning of this frequency pooling? One\n",
      "possibility is that this spatially coherent pooling of multiple frequencies leads to a\n",
      "representation of an edge that is more realistic than the edges given by typical Ga-\n",
      "bor functions. Presumably, this is largely due to the fact that natural images contain\n",
      "many sharp, step-like edges that are not contained in a single frequency band. Thus,\n",
      "representation of such “broad-band” edges is difﬁcult unless information from dif-\n",
      "ferent frequency bands is combined.\n",
      "In terms of frequency channels, the model predicts that frequency channels\n",
      "should be pooled together after complex cell processing. Models based on frequency\n",
      "channels and related concepts have been most prominent in image coding litera-\n",
      "ture in recent years, both in biological and computer vision circles. The utility of\n",
      "frequency channels in the initial processing stages is widely acknowledged, and it\n",
      "is not put into question by these results — in fact, the results in Chapters 6–10\n",
      "show that using frequency-selective simple and complex cells is statistically opti-\n",
      "mal. However, the question of when the frequency channels should be pooled or\n",
      "otherwise combined has received little attention. The results in this chapter (second\n",
      "experiment) indicate that a statistically optimal way is to pool them together right\n",
      "after the complex cell “stage”, and this pooling should be done among cells of a\n",
      "given orientation which form a local, collinear conﬁguration.\n",
      "12.6.3 Towards predictive modelling\n",
      "As we explained in the beginning of the chapter, the present results are an instance of\n",
      "predictive modelling, where we attempt to predict properties of cells and cell assem-\n",
      "blies that have not yet been observed in experiments. To be precise, the prediction is\n",
      "that in V2 (or some related area) there should be cells whose optimal stimulus is a\n",
      "broad-band edge that has no sidelobes while being relatively sharp, i.e. the optimal\n",
      "stimulus is closer to a step-edge than the Gabor functions that tend to be optimal for\n",
      "V1 simple and complex cells. The optimal stimulus should also be more elongated\n",
      "(Polat and Tyler, 1999; Gilbert and Wiesel, 1985) than what is usually observed in\n",
      "V1, while being highly selective for orientation.\n",
      "Statistical models of natural images offer a framework that lends itself to pre-\n",
      "dictive modelling of the visual cortex. First, they offer a framework where we of-\n",
      "ten see emergence of new kinds of feature detectors — sometimes very different\n",
      "from what was expected when the model was formulated. Second, the framework\n",
      "is highly constrained and data-driven. The rigorous theory of statistical estimation\n",
      "makes it rather difﬁcult to insert the theorist’s subjective expectations in the model,\n",
      "and therefore the results are strongly determined by the data. Third, the framework\n",
      "\n",
      "286\n",
      "12 Dependencies of energy detectors: Beyond V1\n",
      "is very constructive. From just a couple of simple theoretical speciﬁcations, e.g.\n",
      "non-gaussianity, natural images lead to the emergence of complex phenomena.\n",
      "We hope that the present work as well as future results in the same direction will\n",
      "serve as a basis for a new kind of synergy between theoretical and experimental\n",
      "neuroscience.\n",
      "12.6.4 References and related work\n",
      "Several investigators have looked at the connection between natural image statistics,\n",
      "Gestalt grouping rules, and local interactions in the visual cortex (Geisler et al, 2001;\n",
      "Sigman and Gilbert, 2000; Elder and Goldberg, 2002; Kr¨uger, 1998). However, few\n",
      "have considered the statistical relations between features of different frequencies.\n",
      "It should be noted that some related work on interactions of different frequencies\n",
      "does exist in the models of contrast gain control, see Chapter 9 or (Schwartz and\n",
      "Simoncelli, 2001a).\n",
      "Recent measurements from cat area 18 (somewhat analogous to V2) emphasize\n",
      "responses to “second-order” or “non-Fourier” stimuli, typically sine-wave gratings\n",
      "whose amplitudes are modulated (Mareschal and Baker, 1998a,b). These results\n",
      "and the proposed models are related to our results and predictions, yet fundamen-\n",
      "tally different. In the model in (Mareschal and Baker, 1998b), a higher-order cell\n",
      "pools outputs of complex cells in the same frequency band to ﬁnd contours that are\n",
      "deﬁned by texture-like cues instead of luminance. The same cell also receives direct\n",
      "input from simple cells of a different frequency, which enables the cell to combine\n",
      "luminance and second-order cues. This is in stark contrast to higher-order cells in\n",
      "the model we used in this chapter, which pool outputs of complex cells of different\n",
      "frequencies. They can hardly ﬁnd contours deﬁned by second-order cues; instead\n",
      "they seem to be good for coding broad-band contours. Furthermore, in (Mareschal\n",
      "and Baker, 1998a,b), any collinearity of pooling seems to be absent. This naturally\n",
      "leads to the question: Why are our predictions so different from these results from\n",
      "area 18? We suspect this is because it is customary to think of visual processing\n",
      "in terms of division into frequency channels — “second-order” stimuli are just an\n",
      "extension of this conceptualization. Therefore, not much attempt has been made to\n",
      "ﬁnd cells that break the division into frequency channels according to our prediction.\n",
      "On the other hand, one can presume that the cells found in area 18 in (Mareschal\n",
      "and Baker, 1998a,b) are different from our predictions because they use a learning\n",
      "strategy which is different from sparse coding used in our model, perhaps related to\n",
      "the temporal aspects of natural image sequences, see Chapter 16.\n",
      "Another closely related line of work is by Zetzsche and coworkers (Zetzsche and\n",
      "Krieger, 1999; Zetzsche and R¨ohrbein, 2001) who emphasize the importance of de-\n",
      "composing the image information to local phase and amplitude information. The\n",
      "local amplitude is basically given by complex cell outputs, whereas the physiologi-\n",
      "cal coding of the local phases is not known. An important question for future work\n",
      "is how to incorporate phase information in the higher-order units. Some models by\n",
      "\n",
      "12.7 Conclusion\n",
      "287\n",
      "Zetzsche et al actually predict some kind of pooling over frequencies, but rather\n",
      "directly after the simple cell stage, see Fig. 16 in (Zetzsche and R¨ohrbein, 2001).\n",
      "Related models in which edge detection uses phase information pooled over dif-\n",
      "ferent frequencies are in (Morrone and Burr, 1988; Kovesi, 1999). An interesting\n",
      "investigation into the relation of edges and space-frequency analysis ﬁlter outputs in\n",
      "natural images is in (Grifﬁn et al, 2004). A psychophysical study on the integration\n",
      "of information over different frequencies is (Olzak and Wickens, 1997).\n",
      "The model in this chapter opens the way to highly nonlinear multilayer models\n",
      "of natural image statistics. While this seems like a most interesting direction of\n",
      "research, not much work has been done so far. Related attempts to construct very\n",
      "general, nonlinear models of natural image statistics include (Pedersen and Lee,\n",
      "2002; Lee et al, 2003; Malo and Guti´errez, 2006; Chandler and Field, 2007; Grifﬁn,\n",
      "2007).\n",
      "12.7 Conclusion\n",
      "Experiments in this chapter show that two different kinds of pooling over complex\n",
      "cells emerge when we model the statistical properties of natural images. First, the\n",
      "higher-order features group collinear complex cells which form a longer contour.\n",
      "Second, they group complex cells of different frequency preferences. This is ac-\n",
      "complished by applying ordinary ICA on a set of modelled complex cells with mul-\n",
      "tiple frequencies, and inputting natural images to the complex cells. Thus, statistical\n",
      "modelling of natural stimuli leads to an interesting hypothesis on the existence of a\n",
      "new kind of cells in the visual cortex.\n",
      "\n",
      "\n",
      "Chapter 13\n",
      "Overcomplete and non-negative models\n",
      "In this chapter, we discuss two generalizations of the basic ICA and sparse coding\n",
      "models. These do not reject the assumption of independence of the components\n",
      "but change some of the other assumptions in the model. Although the generative\n",
      "models are linear, the computation of the features is nonlinear. In the overcomplete\n",
      "basis model, the number of independent components is larger than the number of\n",
      "pixels. In the non-negative model, the components, as well as the feature vectors,\n",
      "are constrained to be non-negative.\n",
      "13.1 Overcomplete bases\n",
      "13.1.1 Motivation\n",
      "An important restriction of most of the models treated so far is that the number of\n",
      "features cannot be larger than the dimension of the data. The dimension of the data\n",
      "is at most equal to the number of pixels, and it is actually smaller after canonical\n",
      "preprocessing including PCA. This was for two reasons:\n",
      "1. In the sparse coding models the feature detector weights were constrained to be\n",
      "orthogonal. In a space with n dimensions, we can have at most n orthogonal\n",
      "vectors, so this constrains the number of features.\n",
      "2. In the generative models such as ICA, we had to assume that the matrix A, which\n",
      "has the features as its columns, is invertible. Again, a matrix can be invertible\n",
      "only if it is square: thus the number of features cannot be larger than the number\n",
      "of pixels.\n",
      "However, it can be argued that the number of features should be larger than the\n",
      "dimension of the image data. The computational justiﬁcation for such a claim goes\n",
      "as follows:\n",
      "289\n",
      "\n",
      "290\n",
      "13 Overcomplete and non-negative models\n",
      "1. The processing of an image part, corresponding perhaps to an object, should not\n",
      "depend on which location of the image it happens to occupy. That is, if a face\n",
      "is in on the left side of the visual ﬁeld, it should be processed in the same way\n",
      "as if it were on the right side; and if the object is moved one pixel to the left, its\n",
      "processing should not change either.1\n",
      "2. Thus, any feature the system computes should be computed at each possible loca-\n",
      "tion — at the minimum at the location corresponding to each pixel. For example,\n",
      "if we have an edge detector, the output of that edge detector should be computed\n",
      "at each possible (x,y) location possible. Denote their number by N.\n",
      "3. So, any feature should basically have N replicates in the system, one for each\n",
      "location. Possibly it could be a bit less because we may not want to replicate the\n",
      "feature very close to borders where they could not be replicated completely, but\n",
      "this does not change the basic argument.\n",
      "4. What all this implies is that if we just take one feature, say a vertical odd-\n",
      "symmetric Gabor of a given frequency and envelope, copy it in all different loca-\n",
      "tions, we already have N different features, supposedly the maximum number!\n",
      "5. Of course, we would actually like to have many different Gabors with different\n",
      "orientations, different phases, different frequencies and maybe something else\n",
      "as well. Actually, the argument in point 1 can be applied equally well to differ-\n",
      "ent orientations and frequencies, which should be processed equally well. So, in\n",
      "the end, the number of features must be many times greater than the number of\n",
      "pixels.\n",
      "A neuroanatomical justiﬁcation for the same phenomenon is the following cal-\n",
      "culation: the number of simple cells in V1 seems to be much larger than the number\n",
      "of retinal ganglion cells which send out the information on the retina, perhaps by\n",
      "a factor of 25 (Olshausen, 2003). So, if we consider the number of ganglion cells\n",
      "as the “dimension” of input to V1, the number of features seems to be much larger\n",
      "than the number of dimensions.2\n",
      "13.1.2 Deﬁnition of generative model\n",
      "Now we deﬁne a generative model which has more features than the data has di-\n",
      "mensions. In this context, to avoid any confusion, we call the feature vectors Ai\n",
      "basis vectors. A set of basis vectors which contains more vectors than the space has\n",
      "1 The resolution of the retinal image changes as a function of eccentricity (the distance from the\n",
      "centerpoint), so talking about moving “one pixel to the left” is an oversimpliﬁcation. However, this\n",
      "does not change the underlying logic very much, if one simply thinks of photoreceptors or ganglion\n",
      "cells instead of pixels.\n",
      "2 This point is a bit complicated by the fact that the number of photoreceptors in the retina is\n",
      "approximately 100 times larger than the number of ganglion cells. Thus, ganglion cells reduce\n",
      "the dimension of the data, and V1 seems to increase it again. Nevertheless, if we consider the\n",
      "computational problem faced by V1, it does seem justiﬁed to say that it uses an overcomplete basis\n",
      "because it can only receive the outputs of ganglion cells.\n",
      "\n",
      "13.1 Overcomplete bases\n",
      "291\n",
      "dimensions is called an overcomplete basis (Simoncelli et al, 1992; Olshausen and\n",
      "Field, 1997).\n",
      "The deﬁnition of a generative model with an overcomplete basis is rather straight-\n",
      "forward. We just need to express the image as a linear superposition\n",
      "I(x,y) =\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si\n",
      "(13.1)\n",
      "where the only difference to previous models is that the number of features m is\n",
      "arbitrarily large. We also need to specify the statistical properties of the components\n",
      "si. In the basic case, we assume that they are sparse and statistically independent.\n",
      "For technical reasons, another modiﬁcation is also usually introduced at this\n",
      "point: we assume that the image is not exactly a linear sum of the features, but\n",
      "there is noise as well. That is, gaussian noise N(x,y) is added to each pixel:\n",
      "I(x,y) =\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si +N(x,y)\n",
      "(13.2)\n",
      "This does not change the behaviour of the model very much, especially if the noise\n",
      "level is small, but it simpliﬁes the computations in this case. (In the case of basic\n",
      "ICA, introduction of noise in the model just complicates things, so it is usually\n",
      "neglected.)\n",
      "Note that the meaning of overcompleteness changes when the dimension is re-\n",
      "duced by PCA. From the viewpoint of statistical modelling, the dimension of the\n",
      "data is then the dimension given by PCA. So, even a basis which has the same num-\n",
      "ber of vectors as there are pixels can be called overcomplete, because the number of\n",
      "pixels is larger than the PCA-reduced dimension.\n",
      "Despite the simplicity of the deﬁnition of the model, the overcomplete basis\n",
      "model is much more complicated to estimate. What is interesting is that it has a\n",
      "richer behaviour than the basic sparse coding and ICA models because it leads to\n",
      "some nonlinearities in the computation of the features. We will treat this point ﬁrst.\n",
      "13.1.3 Nonlinear computation of the basis coefﬁcients\n",
      "Consider ﬁrst the case where the basis vectors Ai are given, and we want to compute\n",
      "the coefﬁcients si for an input image I. The fundamental problem is that the linear\n",
      "system given by the basis vectors Ai is not invertible: If one tries to solve for the si\n",
      "given an I, there are more unknowns si than there are equations. So, computation of\n",
      "the si seems impossible. Indeed, it is impossible in the sense that even if the image\n",
      "were created as a linear sum of the Ai for some coefﬁcient values si, we cannot\n",
      "recover those original coefﬁcients from the input image alone, without some further\n",
      "information.\n",
      "\n",
      "292\n",
      "13 Overcomplete and non-negative models\n",
      "As an illustration, consider an image with two pixels with values (1,1). Assume\n",
      "we use a basis with three vectors: (0,1), (1,0), and (1,1). Thus, we have\n",
      "(1,1) = (0,1)s1 +(1,0)s2 +(1,1)s3\n",
      "(13.3)\n",
      "Obviously, we could represent the image by setting s1 = 0, s2 = 0, and s3 = 1. But,\n",
      "equally well, we could set s1 = 1, s2 = 1, and s3 = 0. Even if the image was exactly\n",
      "generated using one of these choices for si, we cannot tell which one it was by using\n",
      "information in the image alone. Actually, there is an inﬁnite number of different\n",
      "solutions: you could take any weighted average with of the two solution just given,\n",
      "and it would be a solution as well.\n",
      "However, there is a partial solution to this problem. The key is to use sparseness.\n",
      "Since we know that the si are sparse, we can try decide to ﬁnd the sparsest solution.\n",
      "In the illustration above, we would choose the solution s1 = 0, s2 = 0, and s3 = 1\n",
      "because it is the sparsest possible in the sense that only one coefﬁcient is different\n",
      "from zero.3\n",
      "There is a clear probabilistic justiﬁcation for such a procedure. Basically, we can\n",
      "ﬁnd the most probable values for the coefﬁcients si, under the assumption that the\n",
      "si have sparse distributions. This is possible by using conditional probabilities in a\n",
      "manner similar to Bayes’ rule (see Section 4.7). Now we will derive the procedure\n",
      "based on probabilistic reasoning. By the deﬁnition of conditional pdf’s, we have\n",
      "p(s|I) = p(s,I)\n",
      "p(I) = p(I|s)p(s)\n",
      "p(I)\n",
      "(13.4)\n",
      "which is the basis for Bayes’ rule. The formula can be simpliﬁed because p(I) does\n",
      "not depend on s. Since our goal is to ﬁnd the s which maximizes p(s|I), we can\n",
      "just ignore this constant. We can also maximize its logarithm instead because it is\n",
      "often simpler, and equivalent because logarithm is a strictly increasing function.\n",
      "This gives us the following objective function to maximize:\n",
      "log p(I|s)+log p(s)\n",
      "(13.5)\n",
      "Such estimation of the s is called maximum a posteriori (MAP) estimation, as dis-\n",
      "cussed in Section 4.8.2.\n",
      "Now, we have to compute the probabilities log p(I|s) and log p(s) needed. The\n",
      "ﬁrst thing we consider is the prior distribution p(s) of the si. In Bayesian inference,\n",
      "the prior distribution (or prior for short) incorporates the knowledge we have before\n",
      "making any observations. What prior knowledge do we have here? First, we know\n",
      "that the components are sparse. Second, we assume that they are independent, which\n",
      "is a simple approximation although it is not terribly precise. Thus, log p(s) is similar\n",
      "to what was used in ordinary ICA estimation and linear sparse coding. It can be\n",
      "expressed as\n",
      "3 Another solution would be to use the Moore-Penrose pseudo-inverse, see Section 19.8. However,\n",
      "that method is less justiﬁed by statistical principles, and less useful in practice.\n",
      "\n",
      "13.1 Overcomplete bases\n",
      "293\n",
      "log p(s) =\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "G(si)\n",
      "(13.6)\n",
      "where the function G is the same kind of function we used in ICA estimation, see\n",
      "e.g. Equation (7.19) on page 172.\n",
      "To compute p(I|s), we will use the noisy version of the model in Equation (13.2).\n",
      "Assume that we know the variance of the gaussian noise, and denote it by by σ2.\n",
      "Then, the conditional probability of I(x,y) given all the si is the gaussian pdf of\n",
      "N(x,y) = ∑m\n",
      "i=1 Ai(x,y)si −I(x,y). By deﬁnition of the gaussian pdf, the pdf of a\n",
      "single noise variable is thus\n",
      "p(N(x,y)) =\n",
      "1\n",
      "√\n",
      "2π exp(−1\n",
      "2σ2 N(x,y)2)\n",
      "(13.7)\n",
      "So, the conditional log-pdf for one pixel is\n",
      "log p(I(x,y)|s) = −1\n",
      "2σ2 N(x,y)2 −1\n",
      "2 log2π\n",
      "= −1\n",
      "2σ2 [I(x,y)−\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si]2 −1\n",
      "2 log2π\n",
      "(13.8)\n",
      "We assume that the noise is independent in all pixels, so the conditional pdf of the\n",
      "whole image I is the sum of these log-pdf’s:\n",
      "log p(I|s) = −1\n",
      "2σ2 ∑\n",
      "x,y\n",
      "[I(x,y)−\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si]2 −1\n",
      "2 log2π\n",
      "(13.9)\n",
      "The constant 1\n",
      "2 log2π can be omitted for simplicity.\n",
      "Putting all this together: To ﬁnd the most probable s1,...,sm that generated the\n",
      "image, we maximize\n",
      "log p(s|I) = log p(I|s)+log p(s)+const.\n",
      "= −1\n",
      "2σ2 ∑\n",
      "x,y\n",
      "[I(x,y)−\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si]2 +\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "G(si)+const.\n",
      "(13.10)\n",
      "where the “const” means terms which do not depend on s. Maximization of this\n",
      "objective function is usually not possible in closed form, and numerical optimiza-\n",
      "tion methods have to be used. We have here assumed that the Ai are known; their\n",
      "estimation will be considered below.\n",
      "Maximization of such an objective function leads to a nonlinear computation of\n",
      "the cell activities si. This is in stark contrast to ordinary (non-overcomplete) mod-\n",
      "els, in which the si are a linear function of the I(x,y). The implications of such a\n",
      "nonlinearity will be considered in more detail in Chapter 14.\n",
      "\n",
      "294\n",
      "13 Overcomplete and non-negative models\n",
      "13.1.4 Estimation of the basis\n",
      "Estimation of the basis vectors Ai can be performed using the same principle as es-\n",
      "timation of the si. Basically, the solution is hidden in Equation (13.10). First, note\n",
      "that the pdf in Equation (13.9) depends on the Ai as well. So, that equation actu-\n",
      "ally describes p(I|s,A1,...,Am) instead of just p(I|s). Further, if we backtrack in\n",
      "the logic that lead us to Equation (13.10), we see that the conditional probability\n",
      "in Equation (13.10), when considered as a function of both s and the Ai, is equal\n",
      "to p(s,A1,...,Am|I), if we assume a ﬂat (constant) prior for the Ai. This is the con-\n",
      "ditional probability of both s and the Ai, given the image I. Thus, the conditional\n",
      "log-pdf can be interpreted as essentially the likelihood of the Ai.\n",
      "Estimation of the Ai can now be performed by maximizing the conditional pdf\n",
      "in Equation (13.10) for a sample of images I1,I2,...,IT. (Obviously, we cannot es-\n",
      "timate a basis from a single image.) As usual, we assume that the images in the\n",
      "sample have been collected independently from each other, in which case the log-\n",
      "pdf for the sample is simply the sum of the log-pdf. So, we obtain the ﬁnal objective\n",
      "function\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "log p(s(t),A1,...,Am|It)\n",
      "= −1\n",
      "2σ2\n",
      "T\n",
      "∑\n",
      "t=1∑\n",
      "x,y\n",
      "[It(x,y)−\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si(t)]2 +\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "G(si(t))+const.\n",
      "(13.11)\n",
      "When we maximize this objective function with respect to all the basis vectors Ai\n",
      "and cell outputs si(t) (the latter are different for each image), we obtain, at the same\n",
      "time, the estimates of the components and the basis vectors.4 In other words, we\n",
      "compute both the nonlinear cell outputs and the features Ai.\n",
      "Note that it is not straightforward to deﬁne the receptive ﬁelds of the cell any-\n",
      "more. This is because computation of the cell outputs is nonlinear, and receptive\n",
      "ﬁelds are simple to deﬁne for linear cells only. Actually, if we collect the basis\n",
      "vectors Ai into a matrix A as we did earlier in the ordinary ICA case, that matrix\n",
      "is simply not invertible, so we cannot deﬁne the receptive ﬁelds as the rows of its\n",
      "inverse, as we did earlier.\n",
      "13.1.5 Approach using energy-based models\n",
      "An alternative approach for estimating an overcomplete representation is the follow-\n",
      "ing: We give up a generative model and concentrate on generalizing the sparseness\n",
      "4 One technical problem with this procedure is that the scales of the independent components are\n",
      "not ﬁxed, which leads to serious problems. This problem can be solved simply by normalizing the\n",
      "variances of the independent components to be equal to unity at every optimization step. Alterna-\n",
      "tively, one can normalize the basis vector Ai to unit norm at every step.\n",
      "\n",
      "13.1 Overcomplete bases\n",
      "295\n",
      "criteria. Basically, we take the log-likelihood of the basic ICA model, and relax the\n",
      "constraint that there cannot be too many linear feature detectors. This approach is\n",
      "computationally more efﬁcient because we do not need to compute the nonlinear\n",
      "estimates of the components si which requires another optimization.\n",
      "Consider the log-likelihood of the basic ICA model in Equation (7.15), which we\n",
      "reproduce here for convenience:\n",
      "logL(v1,...,vn;z1,...,zT) = T log|det(V)|+\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "Gi(vT\n",
      "i zt)\n",
      "(13.12)\n",
      "where zt is the canonically preprocessed data sample, and the vi are the feature de-\n",
      "tector vectors in the preprocessed space. We have changed the number of feature\n",
      "detectors to m in line with the notation in this section. Moreover, we use here gen-\n",
      "eral functions Gi, which in the case of basic ICA is equal to log pi, the log-pdf of\n",
      "the independent component. (In this section, we revert to using canonically prepro-\n",
      "cessed data, but this does not really change anything in the mathematical develop-\n",
      "ments. Overcompleteness then means that the number of features is larger than the\n",
      "PCA-reduced dimension.)\n",
      "Now, could we just use the formula in Equation (13.12) with more features than\n",
      "dimensions? Let us denote the dimension of the data by n. Then, this means that we\n",
      "just take m > n to achieve an overcomplete representation.\n",
      "Unfortunately, this is not possible. The problem is the term log|det(V)|. The\n",
      "simple reason is that if m > n, the matrix V, which collects the vi as its rows, would\n",
      "not be square, and the determinant is only deﬁned for a square matrix.\n",
      "On the other hand, the second term on the right-hand side in Equation (13.12)\n",
      "is just a sum of measures of sparseness of the features, so this term need not be\n",
      "changed if we want to have an overcomplete representation.\n",
      "So, we have to understand the real meaning of the term log|det(V)| to obtain\n",
      "a model with an overcomplete representation. This term is actually the logarithm\n",
      "of what is called the normalization constant or a partition function. It is a function\n",
      "of the model parameters which makes the pdf of the data fulﬁll the fundamental\n",
      "constraint that the integral of the pdf is equal to one—a constraint that every pdf\n",
      "must fulﬁll. A likelihood is nothing else than a pdf interpreted as a function of the\n",
      "parameters, and computed for the whole sample instead of one observation. So, the\n",
      "likelihood must fulﬁll this constraint as well.\n",
      "The normalization constant is, in theory, obtained in a straightforward manner.\n",
      "Let us deﬁne the pdf (for one observation) by replacing the ﬁrst term in Equa-\n",
      "tion (13.12) by the proper normalization constant, which we denote by Z:\n",
      "logL(z;v1,...,vn) = −logZ(V)+\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "Gi(vT\n",
      "i z)\n",
      "(13.13)\n",
      "Normalization of the pdf means that we should have\n",
      "Z\n",
      "L(z;v1,...,vn)dz = 1\n",
      "(13.14)\n",
      "\n",
      "296\n",
      "13 Overcomplete and non-negative models\n",
      "In the present case, this means\n",
      "Z\n",
      "L(z;v1,...,vn)dz =\n",
      "1\n",
      "Z(V)\n",
      "Z\n",
      "n\n",
      "∏\n",
      "i=1\n",
      "exp(Gi(vT\n",
      "i z))dz = 1\n",
      "(13.15)\n",
      "So, in principle, we just need to take\n",
      "Z(V) =\n",
      "Z\n",
      "n\n",
      "∏\n",
      "i=1\n",
      "exp(Gi(vT\n",
      "i z))dz\n",
      "(13.16)\n",
      "because this makes the integral in Equation (13.15) equal to one.\n",
      "However, in practice, evaluation of the integral in Equation (13.16) is extremely\n",
      "difﬁcult even with the best numerical integration methods. So, the real problem\n",
      "when we take more feature detector vectors than there are dimensions in the data, is\n",
      "the computation of the normalization constant.\n",
      "Estimation of the model by maximization of likelihood requires that we know\n",
      "Z. If we omit Z and maximize only the ﬁrst term in Equation (13.13), the estima-\n",
      "tion goes completely wrong: If the Gi have a single peak at zero (like the negative\n",
      "log cosh function), as we have assumed in earlier chapters, the maximum of such a\n",
      "truncated likelihood is obtained when the Wi(x,y) are all zero, which is quite absurd!\n",
      "So, the model becomes much more complicated to estimate since we don’t know\n",
      "how to normalize the pdf as a function of the vectors vi. This is in stark contrast to\n",
      "the basic case where the number of feature detector vectors equals the number of\n",
      "input variables: the function Z is simply obtained from the determinant of the matrix\n",
      "collecting all the vectors vi, as seen in Equation (7.15).\n",
      "Fortunately, there are methods for estimating models in the case where Z can-\n",
      "not be easily computed. First of all, there is a number of methods for computing Z\n",
      "approximately, so that the maximum likelihood estimation is computationally pos-\n",
      "sible. However, in our case, it is probably more useful to look at methods which\n",
      "estimate the model directly, avoiding the computation of the normalization con-\n",
      "stant. Score matching and contrastive divergence are two methods for estimating\n",
      "such “non-normalized” models. The mathematical details of score matching are de-\n",
      "scribed in Chapter 21.\n",
      "One point to note is that we are really estimating linear receptive ﬁelds Wi us-\n",
      "ing this method. Thus, the result is not really an overcomplete basis but rather an\n",
      "overcomplete representation using an overcomplete set of receptive ﬁelds.\n",
      "This approach is sometimes called “energy-based” due to complicated historical\n",
      "reasons. The model in Equation (13.13) has also been called a “Products of Experts”\n",
      "model (Hinton, 2002). Further related methods are considered in (Hyv¨arinen and\n",
      "Inki, 2002). See (Utsugi, 2001) for an overcomplete version of the ISA model.\n",
      "\n",
      "13.1 Overcomplete bases\n",
      "297\n",
      "13.1.6 Results on natural images\n",
      "We estimated an overcomplete representation from natural images using the method\n",
      "in Section 13.1.5. Thus, we deﬁned the model using the non-normalized log-\n",
      "likelihood in Equation 13.13. We basically used the classic (negative) log cosh func-\n",
      "tion as G, but we allowed a bit more ﬂexibility by allowing rescaling of the Gi by\n",
      "deﬁning Gi(u) = −αi logcosh(u), where αi are parameters that are estimated at the\n",
      "same time as the vi. We also constrained the norms of the vi to be equal to one.\n",
      "We used the score matching approach (see above or Chapter 21) to estimate the\n",
      "parameters without computation of the normalization constant.\n",
      "To reduce the computational load, we took patches of 16×16 pixels. We prepro-\n",
      "cessed the data just like with ICA in Chapter 7, but the dimension reduction was\n",
      "less strong: we retained 128 principal components, i.e. one half of the dimensions.\n",
      "Then, we estimated a representation with 512 receptive ﬁelds. The representation is\n",
      "thus 4 times overcomplete when compared to the PCA dimension, and two times\n",
      "overcomplete when compared with the number of pixels.\n",
      "The resulting receptive ﬁelds are shown in Figure 13.1. To save space, only a\n",
      "random sample of 192 receptive ﬁelds is shown. The receptive ﬁelds are quite sim-\n",
      "ilar to those estimated by basic ICA or sparse coding. Some are more oscillatory,\n",
      "though.\n",
      "13.1.7 Markov Random Field models *\n",
      "The approach of energy-based overcomplete representations can be readily extended\n",
      "to models which cover the whole image using the principle of Markov Random\n",
      "Fields. Here, we provide a very brief description of this extension for readers with\n",
      "some background in MRF’s.\n",
      "A very important question for any image-processing application is how the mod-\n",
      "els for image patches can be used for whole images which have tens of thousands,\n",
      "or even millions, of pixels. One approach for this is to use Markov random ﬁelds\n",
      "(MRF). What this means is that we deﬁne what is called in that theory a neighbour-\n",
      "hood for each pixel, and deﬁne the probability density for the image as a function\n",
      "of each pixel value and the values of the pixels in the neighbourhood. The central\n",
      "idea is that we compute the same function in all possible locations of the image.\n",
      "In our context, the neighbourhood of a pixel can be deﬁned to be an image patch\n",
      "taken so that the pixel in question is in the very middle. To extend our models to a\n",
      "MRF, we can also use the outputs of linear feature detectors to deﬁne the pdf.\n",
      "This leads to a pdf of the following form:\n",
      "log p(I;W1,...,Wn) = ∑\n",
      "x,y\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "G(∑\n",
      "ξ,η\n",
      "Wi(ξ,η)I(x+ξ,y+η))−logZ(W1,...,Wn)\n",
      "(13.17)\n",
      "\n",
      "298\n",
      "13 Overcomplete and non-negative models\n",
      "Fig. 13.1: Receptive ﬁelds Wi in a four times overcomplete basis for canonically preprocessed data,\n",
      "estimated using the model in Equation (13.13) and score matching estimation. Only a random\n",
      "sample of the Wi is shown to save space.\n",
      "\n",
      "13.1 Overcomplete bases\n",
      "299\n",
      "Here, the ﬁrst sum over x,y goes over all possible image locations and neighbour-\n",
      "hoods. For each location, we compute the outputs of n linear feature detectors so\n",
      "that they are always centered around the location x,y. The function G is the same\n",
      "kind of function, for example logcosh, as used in sparse coding.\n",
      "An important point is that the indices ξ,η only take values inside a small range,\n",
      "which is the neighbourhood size. For example, we could deﬁne that they belong to\n",
      "the range −5,...,5, in which case the patch size would be 11 ×11 pixels.\n",
      "One interpretation of this pdf is that we are sliding a window over the whole\n",
      "image and computing the outputs of the feature detectors in those windows. In other\n",
      "words, we compute the convolution of each of the Wi with the image, and then apply\n",
      "the nonlinear function G on the results of the convolution. Summation over x,y and\n",
      "over i then simply means that the log-pdf is the sum over the whole convolved,\n",
      "nonlinearly processed image, and all the ﬁlters.\n",
      "As in the case of the model in Section 13.1.5, the log-pdf includes a normaliza-\n",
      "tion constant Z, which is a function of the feature detector weights Wi. Again, the\n",
      "computation of the normalization constant is most difﬁcult, and the model is prob-\n",
      "ably best estimated using methods which avoid computation of the normalization\n",
      "constant (see e.g. Chapter 21).\n",
      "In fact, we can see a direct connection with the overcomplete basis framework\n",
      "as follows. Deﬁne the translated feature detector W (a,b) as a feature detector whose\n",
      "weights have been translated by the amount given by a and b, so that W (a,b)(x,y) =\n",
      "W(x−a,y−b). Also, redeﬁne indices as x+ξ = x′, y+η = y′. Then we can write\n",
      "the log-pdf as\n",
      "log p(I;W1,...,Wn) =\n",
      "n\n",
      "∑\n",
      "i=1∑\n",
      "x,y\n",
      "G(∑\n",
      "x′,y′\n",
      "W (x,y)\n",
      "i\n",
      "(x′,y′)I(x′,y′))−logZ\n",
      "(13.18)\n",
      "This model is just like the overcomplete model in Section 13.1.5, but the feature\n",
      "weights are constrained so that they are copies of a small number of feature weights\n",
      "Wi in all the different locations, obtained by the translation operation Wi(x,y). Due\n",
      "to the summation over the translation parameters x,y, each weight vector is copied\n",
      "to all different locations. (We are here neglecting any border effects which appear\n",
      "because for those weight in the Wi which go over the edges of the image.) Further-\n",
      "more, the normalization constant is computed in a slightly different way because the\n",
      "integration is over the whole image.\n",
      "Learning feature detector weights of MRF’s was proposed in (Roth and Black,\n",
      "2005). A related approach was proposed in (Zhu et al, 1997). At the time of this\n",
      "writing, the ﬁrst successful attempt to estimate MRF’s in the sense that we obtain\n",
      "Gabor-like features was obtained in (K¨oster et al, 2009b). A review of classic MRF\n",
      "models, i.e. models in which the features are not learned but manually tuned, is in\n",
      "(Li, 2001); a more mathematical treatise is (Winkler, 2003).\n",
      "Let us ﬁnally mention some completely different approaches to modelling whole\n",
      "images or scenes. One is to extract some global statistics, i.e. feature histograms,\n",
      "which can then be further analyzed by various statistical models, as in , e.g., (Liu and\n",
      "Cheng, 2003; Lindgren and Hyv¨arinen, 2004). Yet another alternative is to compute\n",
      "\n",
      "300\n",
      "13 Overcomplete and non-negative models\n",
      "a low-dimensional holistic representation by techniques related to PCA, as in e.g.,\n",
      "(Torralba and Oliva, 2003).\n",
      "13.2 Non-negative models\n",
      "13.2.1 Motivation\n",
      "Neural ﬁring rates are never negative. Even if we consider the spontaneous ﬁring\n",
      "rate as the baseline and deﬁne it to be zero in our scale, the ﬁring in cortical cells\n",
      "cannot go much below zero because the spontaneous ﬁring rates are so low; so,\n",
      "it may be useful to consider them non-negative anyway. It has been argued that\n",
      "this non-negativity of ﬁring rates should be taken into account in statistical models.\n",
      "Non-negative matrix factorization (NMF) (Lee and Seung, 1999) is a recent method\n",
      "for ﬁnding such a representation. It was originally introduced in a different context\n",
      "and called positive matrix factorization (Paatero and Tapper, 1994), but the acronym\n",
      "NMF is now more widely used.5\n",
      "13.2.2 Deﬁnition\n",
      "Let us assume that our data consists of T of n-dimensional vectors, denoted by\n",
      "x(t) (t = 1,...,T). These are collected to a non-negative data matrix X which has\n",
      "x(t) as its columns. NMF ﬁnds an approximate factorization of X into non-negative\n",
      "factors A and S. Thus, non-negative matrix factorization is a linear, non-negative\n",
      "approximate data representation, given by\n",
      "x(t) ≈\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "aisi(t) = As(t)\n",
      "or\n",
      "X ≈AS\n",
      "where A is an n × m matrix containing the basis vectors ai as its columns. This\n",
      "representation is, of course, similar in many respects to PCA and ICA. In particular,\n",
      "the dimension of the representation m can be smaller than the dimension of the data,\n",
      "in which the the dimension is reduced as in PCA.\n",
      "Whereas PCA and ICA do not in any way restrict the signs of the entries of A and\n",
      "S, NMF requires all entries of both matrices to be non-negative. What this means\n",
      "is that the data is described by using additive components only. This constraint has\n",
      "been motivated in a couple of ways: First, in many applications one knows (for ex-\n",
      "ample by the rules of physics) that the quantities involved cannot be negative—ﬁring\n",
      "rates are one example. In such cases, it can be difﬁcult to interpret the results of PCA\n",
      "5 This section is based on the article (Hoyer, 2004), originally published in Journal of Machine\n",
      "Learning Research. Copyright retained by the author.\n",
      "\n",
      "13.2 Non-negative models\n",
      "301\n",
      "and ICA (Paatero and Tapper, 1994; Parra et al, 2000). Second, non-negativity has\n",
      "been argued for based on the intuition that parts are generally combined additively\n",
      "(and not subtracted) to form a whole; hence, these constraints might be useful for\n",
      "learning parts-based representations (Lee and Seung, 1999).\n",
      "Given a data matrix X, the optimal choice of matrices A and S are deﬁned to\n",
      "be those non-negative matrices that minimize the reconstruction error between X\n",
      "and AS. Various error functions have been proposed (Paatero and Tapper, 1994; Lee\n",
      "and Seung, 2001), perhaps the most widely used is the squared error (Euclidean\n",
      "distance) function\n",
      "D(A,S) = ∥X−AS∥2 = ∑\n",
      "i,j\n",
      "(xij −[AS]ij)2.\n",
      "A gradient algorithm for this optimization was proposed by (Paatero and Tapper,\n",
      "1994), whereas in (Lee and Seung, 2001) a multiplicative algorithm was devised\n",
      "that is somewhat simpler to implement and also showed good performance.\n",
      "Although some theoretical work on the properties of the NMF representation ex-\n",
      "ists (Donoho and Stodden, 2004), much of the appeal of NMF comes from its em-\n",
      "pirical success in learning meaningful features from a diverse collection of real-life\n",
      "datasets. It was shown in (Lee and Seung, 1999) that, when the dataset consisted of a\n",
      "collection of face images, the representation consisted of basis vectors encoding for\n",
      "the mouth, nose, eyes, etc; the intuitive features of face images. In Figure 13.2a) we\n",
      "have reproduced that basic result using the same dataset. Additionally, they showed\n",
      "that meaningful topics can be learned when text documents are used as data. Sub-\n",
      "sequently, NMF has been successfully applied to a variety of datasets (Buchsbaum\n",
      "and Bloch, 2002; Brunet et al, 2004; Jung and Kim, 2004; Kim and Tidor, 2003).\n",
      "Despite this success, there also exist datasets for which NMF does not give an\n",
      "intuitive decomposition into parts that would correspond to our idea of the ‘building\n",
      "blocks’ of the data. It was shown by (Li et al, 2001) that when NMF was applied\n",
      "to a different facial image database, the representation was global rather than local,\n",
      "qualitatively different from that reported by (Lee and Seung, 1999). Again, we have\n",
      "rerun that experiment and conﬁrm those results, see Figure 13.2b). The difference\n",
      "was mainly attributed to how well the images were hand-aligned (Li et al, 2001).\n",
      "Another case where the decomposition found by NMF does not match the under-\n",
      "lying elements of the data is shown in Figure 13.2c). In this experiment, natural im-\n",
      "age patches were whitened and subsequently split into positive (‘ON’) and negative\n",
      "(‘OFF’) contrast channels, simply by separating positive and negative values into\n",
      "separate channels (variables). This is somewhat similar to how visual information is\n",
      "processed by the retina. Each image patch of 12×12 pixels was thus represented by\n",
      "a 2 × 12 × 12 = 288 -dimensional vector, each element of which mimics the activ-\n",
      "ity of an ON- or OFF-center neuron to the input patch. These vectors made up the\n",
      "columns of X. When NMF is applied to such a dataset, the resulting decomposition\n",
      "does not consist of the oriented ﬁlters which form the cornerstone of most of visual\n",
      "models and modern image processing. Rather, NMF represents these images using\n",
      "simple, dull, circular ‘blobs’.\n",
      "\n",
      "302\n",
      "13 Overcomplete and non-negative models\n",
      "a\n",
      "c\n",
      "b\n",
      "Fig.\n",
      "13.2:\n",
      "NMF\n",
      "applied\n",
      "to\n",
      "various\n",
      "image\n",
      "dataset.\n",
      "a)\n",
      "Basis\n",
      "images\n",
      "given\n",
      "by\n",
      "NMF\n",
      "applied\n",
      "to\n",
      "face\n",
      "image\n",
      "data\n",
      "from\n",
      "the\n",
      "CBCL\n",
      "database\n",
      "(http://cbcl.mit.edu/cbcl/software-datasets/FaceData2.html),\n",
      "following\n",
      "(Lee\n",
      "and\n",
      "Seung,\n",
      "1999).\n",
      "In\n",
      "this case\n",
      "NMF produces\n",
      "a parts-based\n",
      "repre-\n",
      "sentation of the data. b) Basis images derived from the ORL face image database\n",
      "(http://www.uk.research.att.com/facedatabase.html), following (Li et al,\n",
      "2001). Here, the NMF representation is global rather than parts-based. c) Basis vectors from NMF\n",
      "applied to ON/OFF-contrast ﬁltered natural image data. Top: Weights for the ON-channel. Each\n",
      "patch represents the part of one basis vector ai corresponding to the ON-channel. (White pixels\n",
      "denote zero weight, darker pixels are positive weights.) Middle: Corresponding weights for the\n",
      "OFF-channel. Bottom: Weights for ON minus weights for OFF. (Here, grey pixels denote zero.)\n",
      "NMF represents this natural image data using simple blobs.\n",
      "13.2.3 Adding sparseness constraints\n",
      "Now we show, following (Hoyer, 2004), how explicitly controlling the sparseness of\n",
      "the representation leads to representations that are parts-based and match the intu-\n",
      "itive features of the data. Here we use a sparseness measure based on the relationship\n",
      "between the sum of absolute values and the sum of squares (Euclidean norm):\n",
      "sparseness(s) =\n",
      "√m−(∑|si|)/\n",
      "q\n",
      "∑s2\n",
      "i\n",
      "√m−1\n",
      ",\n",
      "\n",
      "13.2 Non-negative models\n",
      "303\n",
      "where m is the dimensionality of s. This function evaluates to unity if and only if\n",
      "s contains only a single non-zero component, and takes a value of zero if and only\n",
      "if all components are equal (up to signs), interpolating smoothly between the two\n",
      "extremes.\n",
      "Our aim is to constrain NMF to ﬁnd solutions with desired degrees of sparseness.\n",
      "The ﬁrst question to answer is then: what exactly should be sparse? The basis vectors\n",
      "A or the coefﬁcients S? This is a question that cannot be given a general answer; it\n",
      "all depends on the speciﬁc application in question. Further, just transposing the data\n",
      "matrix switches the role of the two, so it is easy to see that the choice of which to\n",
      "constrain (or both, or none) must be made by the experimenter.\n",
      "When trying to learn useful features from images, it might make sense to require\n",
      "both A and S to be sparse, signifying that any given object is present in few im-\n",
      "ages and affects only a small part of the image. Or, we could take the approach in\n",
      "Chapter 6 and only require the si to be sparse.\n",
      "These considerations lead us to deﬁning NMF with sparseness constraints as\n",
      "follows: Given a non-negative data matrix X of size n × T, ﬁnd the non-negative\n",
      "matrices A and S of sizes n ×m and m×T (respectively) such that\n",
      "D(A,S) = ∥X−AS∥2\n",
      "(13.19)\n",
      "is minimized, under optional constraints\n",
      "sparseness(ai) = Sa, ∀i\n",
      "sparseness(si) = Ss, ∀i,\n",
      "where ai is the i-th column of A and si is the i-th row of S. Here, m denotes the\n",
      "number of components, and Sa and Ss are the desired sparsenesses of A and S (re-\n",
      "spectively). These three parameters are set by the user.\n",
      "Note that we did not constrain the scales of ai or si yet. However, since aisi =\n",
      "(aiλ)(si/λ) for any λ, we are free to arbitrarily ﬁx any norm of either one. In our\n",
      "algorithm, we thus choose to ﬁx the Euclidean norm (sum of squares) of s to unity,\n",
      "as a matter of convenience.\n",
      "An algorithm for learning NMF with sparseness constraints is described in\n",
      "(Hoyer, 2004). In Figure 13.2c we showed that standard NMF applied to natu-\n",
      "ral image data produces only circular features, not oriented features as have been\n",
      "observed in the cortex. Now, let us see the result of using additional sparseness\n",
      "constraints. Figure 13.3 shows the basis vectors obtained by putting a sparseness\n",
      "constraint on the coefﬁcients (Ss = 0.85) but leaving the sparseness of the basis\n",
      "vectors unconstrained. In this case, NMF learns oriented Gabor-like features that\n",
      "represent edges and lines. This example illustrates how it is often useful to combine\n",
      "sparseness and non-negativity constraints to obtain a method which combines the\n",
      "biologically plausible results of low-level features with the purely additive learn-\n",
      "ing of NMF. Such combinations may be useful in future models which attempt to\n",
      "go beyond the primary visual cortex, because non-negativity may be an important\n",
      "\n",
      "304\n",
      "13 Overcomplete and non-negative models\n",
      "property of complex cell outputs and other higher-order features, as was already\n",
      "pointed out in Chapter 12.\n",
      "Fig. 13.3: Basis vectors from ON/OFF-ﬁltered natural images obtained using NMF with sparse-\n",
      "ness constraints. The sparseness of the coefﬁcients was ﬁxed at 0.85, and the sparseness of the\n",
      "basis images was unconstrained. Top: weights in ON channel. Middle: weights in OFF channel.\n",
      "Bottom: weights in ON channel minus weights in OFF channel. As opposed to standard NMF (cf\n",
      "Figure 13.2c), the representation is based on oriented, Gabor-like, features.\n",
      "\n",
      "13.3 Conclusion\n",
      "305\n",
      "13.3 Conclusion\n",
      "In this chapter, we saw two quite different extensions of the basic linear ICA model.\n",
      "The model with overcomplete basis is well motivated as a model of simple cells,\n",
      "and the next chapter will show some more implications of the principle.\n",
      "In contrast, the utility of non-negative models for feature extraction is still to\n",
      "be explored. Possibly, non-negative models can be useful in learning higher-order\n",
      "features, which can be considered either to be “there” (positive values) or “not there”\n",
      "(zero value), negative values being less meaningful. On the other hand, negative\n",
      "values can often be interpreted as meaning that the feature is there “less strongly” or\n",
      "“less likely”, possibly related to some baseline. In fact, after our initial work (Hoyer\n",
      "and Hyv¨arinen, 2002) learning the third layer as in Chapter 12 using non-negativity\n",
      "constraints, we found out that the non-negativity constraints had little effect on the\n",
      "results, and the results in Chapter 12 do not use any such contraint.\n",
      "Moreover, it is not clear if both the basis vectors and their coefﬁcients should\n",
      "be constrains non-negative: A partly non-negative model in which either the basis\n",
      "vectors or the components are constrained non-negative may also be more meaning-\n",
      "ful. Non-negativity may, in the end, ﬁnd its utility as one of the many properties of\n",
      "(some of the) parameters in a statistical model, instead of being very useful in itself.\n",
      "\n",
      "\n",
      "Chapter 14\n",
      "Lateral interactions and feedback\n",
      "So far, we have almost exclusively considered a “bottom-up” or feedforward frame-\n",
      "work, in which the incoming image is processed in a number of successive stages,\n",
      "the information ﬂowing in one direction only. However, it is widely appreciated in\n",
      "visual neuroscience that the brain is doing something much more complicated than\n",
      "just feedforward processing. There is a lot of evidence for\n",
      "1. feedback from “higher” areas to lower areas, e.g., “top-down” connections from\n",
      "V2 back to V1, as well as\n",
      "2. lateral (horizontal) interactions, by which we mean here connections between\n",
      "features in the same stage, e.g., connections between simple cells.\n",
      "In this chapter, we will see how such phenomena are rather natural consequences\n",
      "of Bayesian inference in the models we have introduced. First, we will introduce a\n",
      "model of feedback based on thresholding, or shrinkage, of coefﬁcients in the higher\n",
      "stage. Second, we will consider a lateral interaction phenomenon: end-stopping in\n",
      "simple cells. Finally, we will discuss the relationship of the principle of predictive\n",
      "coding to these phenomena.\n",
      "14.1 Feedback as Bayesian inference\n",
      "A central question in visual neuroscience concerns the computational role of feed-\n",
      "back connections. It has been suggested that the purpose of feedback is that of using\n",
      "information from higher-order units to modulate lower-level outputs, so as to se-\n",
      "lectively enhance responses which are consistent with the broader visual context\n",
      "(Lamme, 1995; Hup´e et al, 1998). In hierarchical generative models, this is natu-\n",
      "rally understood as part of the inference process: ﬁnding the most likely conﬁgura-\n",
      "tion of the network requires integrating incoming (bottom-up) sensory information\n",
      "with priors stored in higher areas (top-down) at each layer of the network (Hinton\n",
      "and Ghahramani, 1997).\n",
      "307\n",
      "\n",
      "308\n",
      "14 Lateral interactions and feedback\n",
      "Why would this kind of feedback inference be useful? In many cases, there can\n",
      "be multiple conﬂicting interpretations of the stimulus even on the lowest level, and\n",
      "top-down feedback is needed to resolve such conﬂicts. In essence, feedback infer-\n",
      "ence computes the most likely interpretation of the scene (Knill and Richards, 1996;\n",
      "Lee and Mumford, 2003; Yuille and Kersten, 2006), combining bottom-up sensory\n",
      "information with top-down priors.\n",
      "14.1.1 Example: contour integrator units\n",
      "An example of Bayesian feedback inference can be constructed based on the model\n",
      "of higher-order units that integrate outputs of complex cells, introduced in Chap-\n",
      "ter 12. Basically, the idea is as follows: if enough collinear complex cells are active,\n",
      "they will activate a higher-order contour-coding unit. The activation of such a unit\n",
      "is then evidence for a contour at that location, and this evidence will strengthen re-\n",
      "sponses of all complex cells lying on the contour, especially those whose bottom-up\n",
      "input is relatively weak.\n",
      "The structure of the network was depicted in Figure 12.1 in Chapter 12. In that\n",
      "chapter, we interpreted this network as performing feedforward computations only:\n",
      "ﬁrst the energy model for complex cells, and then a linear transformation. How can\n",
      "we then simulate the full network inference process to model feedback?\n",
      "One approach is reduction of noise (Hup´e et al, 1998). “Noise” in this context\n",
      "refers to any activity that is not consistent with the learned statistical model and\n",
      "is thus not only neural or photoreceptor noise. Such noise reduction essentially\n",
      "suppresses responses which are not typical of the training data, while retaining re-\n",
      "sponses that do ﬁt the learned statistical model. Denoting the complex cell responses\n",
      "by ck, we model them by a linear generative model which includes a noise term:\n",
      "ck =\n",
      "K\n",
      "∑\n",
      "i=1\n",
      "akisi +nk for all k\n",
      "(14.1)\n",
      "where nk is gaussian noise of zero mean and variance σ2. The outputs of higher-\n",
      "order contour-coding units are still denoted by si.\n",
      "We postulate that the outputs si of higher-order cells are computed by Bayesian\n",
      "inference in this generative model. Given an image, the complex-cell outputs are\n",
      "ﬁrst computed in a feedforward manner; these initial values are denoted by ck. (It\n",
      "is here assumed that the feature weights aik have already been learned.) Next, the\n",
      "outputs of higher-order cells are computed by ﬁnding the si which have the high-\n",
      "est posterior probability — we use the Bayesian terminology “posterior probability\n",
      "(distribution)”, which simply means the conditional probability given the observa-\n",
      "tions. Let us denote the computed outputs as ˆs:\n",
      "ˆs = argmax\n",
      "s\n",
      "log p(s|c)\n",
      "(14.2)\n",
      "\n",
      "14.1 Feedback as Bayesian inference\n",
      "309\n",
      "As is typical in Bayesian inference (see Section 4.7), we can formulate the posterior\n",
      "log-probability as the sum of two terms:\n",
      "log p(s|c) = log p(c|s)+log p(s)−const.\n",
      "(14.3)\n",
      "where p(s) is the prior pdf of s. It incorporates our knowledge of the structure of\n",
      "the world, e.g. that the cell outputs are sparse. The term log p(c|s) incorporates our\n",
      "knowledge of the image generation process; an example will be given below.\n",
      "The important point here is that the outputs ˆsi of higher-order units are nonlinear\n",
      "functions of the complex cell outputs. We will discuss below why this is so. This\n",
      "opens up the possibility of reducing noise in the complex cell outputs by recon-\n",
      "structing them using the linear generative model in Equation (14.1), ignoring the\n",
      "noise. The obtained reconstructions, i.e. the outputs of the complex cells after they\n",
      "have received feedback, are denoted by ˆck, and computed as\n",
      "ˆck =\n",
      "K\n",
      "∑\n",
      "i=1\n",
      "aki ˆsi for all k\n",
      "(14.4)\n",
      "Nonlinearity is essential in these models. If the outputs ˆsi were simply linear trans-\n",
      "formations of the complex cell outputs, little would be gained by such feedback.\n",
      "This is because the reconstructed values ˆck would still be linear transformations of\n",
      "the original feed-forward ck. Thus, one could wonder why any feedback would re-\n",
      "ally be needed to compute the ˆck because a linear transformation could certainly be\n",
      "easily incorporated in the feedforward process which computes the ck in the ﬁrst\n",
      "place. However, the nonlinear computations that emerge from the Bayesian infer-\n",
      "ence process do need more complicated computing circuitry, so it is natural that\n",
      "feedback is needed.\n",
      "The effect of this inference is that the top-down connections from the contour-\n",
      "coding units to the complex cells seek to adjust the complex cell responses towards\n",
      "that predicted by the contour units. To be more precise, such an effect can be ob-\n",
      "tained, for example, by sending a dynamic feedback signal of the form\n",
      "uki = [\n",
      "K\n",
      "∑\n",
      "i=1\n",
      "aki ˆsi]−ck\n",
      "(14.5)\n",
      "from the i-th higher-order cell to the k-th complex cell. When ck is equal to its\n",
      "denoised estimate, this signal is zero and equilibrium is achieved. Of course, this\n",
      "feedback signal is just one possibility and it is not known how this computation\n",
      "is actually achieved in the visual system. What is important here is that Bayesian\n",
      "inference gives an exact proposal on what the purpose of such feedback signals\n",
      "should be, thus providing a normative model.\n",
      "In Fig. 14.1, we show a very basic example of how feedback noise reduction in\n",
      "this model results in the emphasis of smooth contours. We generated image patches\n",
      "by placing Gabor functions at random locations and orientations (for simplicity,\n",
      "we consider only a single frequency band here). In one case, there was a collinear\n",
      "\n",
      "310\n",
      "14 Lateral interactions and feedback\n",
      "alignment of three consecutive Gabors; in the other these same Gabors had random\n",
      "orientations. These image patches are shown in Fig. 14.1 a). Next, we processed\n",
      "these by our model complex cells, as we had processed the natural image patches in\n",
      "our experiments in Chapter 12. The resulting ck are shown in Fig. 14.1 b). Finally,\n",
      "we calculated the contour-coding unit activities si (the actual method is discussed\n",
      "in the next subsection), and plotted the noise-reduced complex cell activity in Fig.\n",
      "14.1 c).\n",
      "a\n",
      "b\n",
      "c\n",
      "+\n",
      "0\n",
      "Fig. 14.1: Noise reduction and contour integration. a) Two image patches containing Gabors at\n",
      "random locations and orientations. In the top patch there is a collinear set of three Gabors, whereas\n",
      "in the bottom patch these same Gabors had random orientations. b) The response of the model\n",
      "complex cells to the images in a). c) The response of the complex cells after feedback noise reduc-\n",
      "tion using the learned network model. Note that the reduction of noise has left the activations of\n",
      "the collinear stimuli but suppressed activity that did not ﬁt the learned sparse coding model well.\n",
      "From (Hoyer and Hyv¨arinen, 2002), Copyright c⃝2002 Elsevier, used with permission.\n",
      "Note how the noise-reduction step suppresses responses to “spurious” edges,\n",
      "while emphasizing the responses that are part of the collinear arrangement. Such\n",
      "response enhancement to contours is the deﬁning characteristic of many proposed\n",
      "computational models of contour integration, see for example (Grossberg and Min-\n",
      "golla, 1985; Li, 1999; Neumann and Sepp, 1999). Comparing the denoised re-\n",
      "sponses (Fig. 12c) with each other one can also observe collinear contextual in-\n",
      "teractions in the model. The response to the central Gabor is stronger when it is\n",
      "ﬂanked by collinear Gabors (upper row) than when the ﬂankers have random orien-\n",
      "tations (bottom row), even though the ﬂankers fall well outside the receptive ﬁeld\n",
      "of the central neuron. This type of contextual interaction has been the subject of\n",
      "much study recently (Polat and Sagi, 1993; Polat et al, 1998; Polat and Tyler, 1999;\n",
      "Kapadia et al, 1995, 2000; Kurki et al, 2006); see (Fitzpatrick, 2000) for a review. It\n",
      "is hypothesized to be related to contour integration, although such a relation is not\n",
      "certain (Williams and Hess, 1998).\n",
      "\n",
      "14.1 Feedback as Bayesian inference\n",
      "311\n",
      "14.1.2 Thresholding (shrinkage) of a sparse code\n",
      "What is the nonlinearity in the inference of the si like in Equation (14.2)? Because\n",
      "the code is sparse, it turns out to be something like a thresholding of individual cell\n",
      "activities, as we will show next.\n",
      "14.1.2.1 Decoupling of estimates\n",
      "Inference in an ICA model which contains gaussian noise, as in Equation (14.1), is\n",
      "a special case of the principle in Section 13.1.3, in which the coefﬁcients in an over-\n",
      "complete basis were estimated. We will see that the noise alone leads to nonlinear\n",
      "computations even if the basis is not overcomplete as it was in Section 13.1.3. We\n",
      "can directly use the posterior pdf we calculated there, in Equation (13.10) on page\n",
      "293; instead of the original image I the observed data is the vector of complex cell\n",
      "outputs c. Thus, we have\n",
      "log p(s|c) = −1\n",
      "2σ2 ∑\n",
      "k\n",
      "[ck −\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "akisi]2 +\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "G(si)+const.\n",
      "(14.6)\n",
      "where aki is the matrix of higher-order features weights, and the constant does not\n",
      "depend on s. Now, let us assume that the number of complex cells equals the number\n",
      "of higher-order features. This is just the classic assumption that we usually make\n",
      "with ICA (with the exception of the overcomplete basis model in Section 13.1).\n",
      "Then, the matrix A, which has the aki as its entries, is invertible. Second, let us\n",
      "make the assumption that the matrix A is orthogonal. This assumption is a bit more\n",
      "intricate. It can be interpreted as saying that that the noise is added on the whitened\n",
      "data, because A is orthogonal after whitening. Since the noise is, in our case, an\n",
      "abstract kind of noise whose structure is not very well known in any case, this may\n",
      "not be an unreasonable assumption.\n",
      "After these simplifying assumptions, the inference deﬁned in Equation (14.6)\n",
      "becomes quite simple. First, note that the sum of squares is, in matrix notation, equal\n",
      "to ∥c−As∥2. Because an orthogonal transformation does not change the norm, we\n",
      "can multiply the vector c −As by AT without changing the norm. Thus, we can\n",
      "replace the sum of squares in Equation (14.6) by ∥ATc−s∥2, obtaining\n",
      "log p(s|c) = −1\n",
      "2σ2\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "[\n",
      "K\n",
      "∑\n",
      "k=1\n",
      "akick −si]2 +\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "G(si)+const.\n",
      "(14.7)\n",
      "Now we see the remarkable fact that this posterior log-pdf is a sum of functions\n",
      "of the form\n",
      "log p(si|c) = −1\n",
      "2σ2 [\n",
      "K\n",
      "∑\n",
      "k=1\n",
      "akick −si]2 +G(si)+const.\n",
      "(14.8)\n",
      "\n",
      "312\n",
      "14 Lateral interactions and feedback\n",
      "which are functions of single si (higher-order features) only. Thus, we can maxi-\n",
      "mize this posterior pdf separately for each si: we only need to do one-dimensional\n",
      "optimization. Each such one-dimensional maximum depends only on ∑K\n",
      "k=1 akick.\n",
      "This means that the estimates of the si which maximize this pdf are obtained by\n",
      "applying some one-dimensional nonlinear function f on linear transformations of\n",
      "the complex cell outputs:\n",
      "ˆsi = f(\n",
      "K\n",
      "∑\n",
      "k=1\n",
      "akick)\n",
      "(14.9)\n",
      "where the nonlinear function f depends on G, the log-pdf of the si.\n",
      "14.1.2.2 Sparseness leads to shrinkage\n",
      "What kind of nonlinearity f does noise reduction lead to? Intuitively, there are two\n",
      "forces at play in the posterior log-density log p(si|c). The ﬁrst term, squared error,\n",
      "says that si should be close to ∑K\n",
      "k=1 akick, which can be thought of as the feed-\n",
      "forward linear estimate for si. The really interesting part is the prior given by the\n",
      "function G in Equation (14.8). Now, for a sparse density, the log-density G is a\n",
      "peaked function. For example, it equals G(s) = −\n",
      "√\n",
      "2|s| (plus some constant) for\n",
      "the Laplacian density which we have used previously (see Equation (7.18)). This\n",
      "peakedness makes the inference nonlinear so that if the linear estimate ∑K\n",
      "k=1 akick\n",
      "is sufﬁciently close to zero, the maximum of p(si|c) is obtained at zero. This is\n",
      "illustrated in Figure 14.2.\n",
      "Actually, the form of the function f can be obtained analytically for some choices\n",
      "of G. In particular, assume that G is the log-pdf of the Laplacian distribution. Then,\n",
      "the function f becomes what is called a “shrinkage” function:\n",
      "f(y) = sign(y)max(|y|−\n",
      "√\n",
      "2σ2,0)\n",
      "(14.10)\n",
      "What this function means is that the linear transformation of complex cell outputs\n",
      "∑K\n",
      "k=1 akick is “shrunk” towards zero by an amount which depends on noise level\n",
      "σ2. Such a function can be considered a “soft” form of thresholding. In fact, for\n",
      "some other choices of G, such as the one in Equation (7.22), which correspond\n",
      "to much sparser pdf’s, the nonlinearity becomes very close to thresholding. See\n",
      "Fig. 14.3 for plots of such functions. For more details in shrinkage functions, see\n",
      "(Hyv¨arinen, 1999b; Simoncelli and Adelson, 1996; Johnstone and Silverman, 2005)\n",
      "in a Bayesian context, and (Donoho, 1995) for a related method.\n",
      "The nonlinear behaviour obtained by a sparse prior is in stark contrast to the case\n",
      "where the distribution of si is gaussian: then G is quadratic, and so is log p(si|c).\n",
      "Minimization of a quadratic function leads to a linear function of the parameters. In\n",
      "fact, we can take the derivative of log p(si|c) = −1\n",
      "2σ2 [∑K\n",
      "k=1 akick −si]2 −s2\n",
      "i /2 with\n",
      "respect to si and set it to zero, which gives as the solution ˆsi =\n",
      "1\n",
      "1+σ2 ∑K\n",
      "k=1 akick.\n",
      "This is a simple linear function of the feed-forward estimate. So, we see that it is\n",
      "sparseness, or non-gaussianity, which leads to interesting nonlinear phenomena.\n",
      "\n",
      "14.1 Feedback as Bayesian inference\n",
      "313\n",
      "Thus, we see that the function of Bayesian inference in this kind of a model\n",
      "is to reduce small cell activities in the higher processing area to zero. If there is\n",
      "not enough evidence that the feature encoded by a higher-order cell is there, the\n",
      "cell activity is considered pure noise, and set to zero. Feedback from higher ar-\n",
      "eas modulates activity in lower areas by suppressing cells which are not consistent\n",
      "with the cell activity which is left after noise reduction on the higher level. In other\n",
      "words, activities of some cells in the lower-level area are suppressed because they\n",
      "are considered purely noise. At the same time, activities of some cells may even be\n",
      "enhanced so as to make them consistent with higher-level activities. Such a mech-\n",
      "anism can work on many different levels of hierarchy. However, the mathematical\n",
      "difﬁculties constrain most analysis to a network where the feedback is only between\n",
      "two levels.\n",
      "a)\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "−5\n",
      "−4\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "b)\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "−5\n",
      "−4\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "Fig. 14.2: Illustration of why noise reduction with a sparse prior for si leads to shrinkage. In both\n",
      "plots, the dashed line gives the Laplacian prior pdf G(s) = −\n",
      "√\n",
      "2|s|. The dash-dotted line gives\n",
      "the squared error term in Equation (14.8). The solid line gives the sum of these two terms, i.e.\n",
      "the posterior log-probability log(si|c). The variance is ﬁxed to σ 2 = 0.5. a) The case where the\n",
      "feedforward signal is weak: ∑K\n",
      "k=1 akick = 0.25. We can see that the peak at zero of the Laplacian\n",
      "pdf dominates, and the maximum of the posterior is obtained at zero. This leads to a kind of\n",
      "thresholding. b) The case where the feedforward signal is strong: ∑K\n",
      "k=1 akick = 1.5. Now, the sparse\n",
      "prior does not dominate anymore. The maximum of the posterior is obtained at a value which is\n",
      "clearly different from zero, but a bit smaller than the value given by the feedforward signal.\n",
      "14.1.3 Categorization and top-down feedback\n",
      "The shrinkage feedback treated above is only one example of Bayesian inference\n",
      "in a noisy linear generative model. Different variants can be obtained depending\n",
      "on the assumptions of the marginal distributions of the latent variables, and their\n",
      "dependencies. Actually, even the generative model in Equation (14.1) is applicable\n",
      "\n",
      "314\n",
      "14 Lateral interactions and feedback\n",
      "Fig. 14.3: Plots of the shrinkage functions f which modify the outputs of the higher-order contour\n",
      "cells. The effect of the functions is to reduce the absolute value of its argument by a certain amount\n",
      "which depends on the noise level. Small arguments are set to zero. This reduces gaussian noise\n",
      "for sparse random variables. Solid line: shrinkage corresponding to Laplace density. Dash-dotted\n",
      "line: a thresholding function corresponding to the highly sparse density in Equation (7.22). The\n",
      "line x = y is given as the dotted line. The linear estimate to which this nonlinearity is applied was\n",
      "normalized to unit variance, and noise variance was ﬁxed to .3.\n",
      "to any two groups of cells on two levels of hierarchy; the ck need not be complex\n",
      "cells and si need not be contour-coding cells.\n",
      "For example, consider the latent variables si as indicating category membership.\n",
      "Each of them is zero or one depending on whether the object in the visual ﬁeld\n",
      "belongs to a certain category. For example, assume one of them, s1, signals the\n",
      "category “face”.\n",
      "Thus, Bayesian inference based on Equation (14.2) can again be used to infer\n",
      "the most probable values for the ck variables for s1. What is interesting here is that\n",
      "the binary nature of s1 means that when the visual input is sufﬁciently close to\n",
      "the prototype of a face, the most likely value of s1 will be exactly 1; at a certain\n",
      "threshold, it will jump from 0 to 1. This will affect the denoised estimates of the\n",
      "complex cell outputs ck. The top-down feedback will now say that they should be\n",
      "similar to the ﬁrst basis vector (a11,...,a1n). Thus, a combination of excitatory and\n",
      "inhibitory feedback will be sent down to complex cells to drive the complex cell\n",
      "outputs in this direction.\n",
      "For example, if the input is a face in which some of the contours have very low\n",
      "contrast, due to lighting conditions, this feedback will try to enhance them (Lee and\n",
      "Mumford, 2003). Such feedback will be triggered if the evidence for s1 being 1 is\n",
      "above the threshold needed. Otherwise, possibly the feedback from another category\n",
      "unit is activated.\n",
      "\n",
      "14.2 Overcomplete basis and end-stopping\n",
      "315\n",
      "14.2 Overcomplete basis and end-stopping\n",
      "A second kind of phenomenon which emerges from Bayesian inference and goes\n",
      "beyond a basic feedforward model is competitive interactions. This happens espe-\n",
      "cially in the model with overcomplete basis, see Section 13.1, and can explain the\n",
      "phenomenon of end-stopping.\n",
      "End-stopping refers to a phenomenon, already described by Hubel and Wiesel in\n",
      "the 1960’s, in which the cell output is reduced when the optimal stimulus is made\n",
      "longer. That is, you ﬁrst ﬁnd a Gabor stimulus which gives maximum response in a\n",
      "simple cell. Then, you simply make that Gabor longer, that is, more elongated, with-\n",
      "out changing anything else. You would expect that the response of the cell does not\n",
      "change because the new stuff that appears in the stimulus is outside of the receptive\n",
      "ﬁeld. However, some cells (both simple and complex) actually reduce their ﬁring\n",
      "rate when the stimulus is made more elongated. This is what is called end-stopping.\n",
      "As discussed in Section 13.1, in an overcomplete basis there are often many\n",
      "different combinations of coefﬁcients si which can give rise to the same image in a\n",
      "linear generative model I(x,y) = ∑i Ai(x,y)si. Using Bayesian inference, the most\n",
      "likely coefﬁcients si can be found, and this may provide a more accurate model for\n",
      "how simple cells in V1 compute their responses. This is related to end-stopping\n",
      "because such Bayesian inference in an overcomplete basis leads to dynamics which\n",
      "can be conceptualized as competition.\n",
      "Here is an example of such competition. Consider only three Gabor-shaped ba-\n",
      "sis vectors which are of the same shape but in slightly different locations, so that\n",
      "together they form an elongated Gabor. It is important that the Gabors are overlap-\n",
      "ping; this is necessary for the competition to arise. The three Gabors are depicted in\n",
      "Figure 14.4.\n",
      "First assume that the stimulus is a Gabor which is exactly the same as the feature\n",
      "coded by the cell in the middle. Then, obviously, the sparsest possible representation\n",
      "of the stimulus is to set the coefﬁcients of the left- and right-most features to zero\n",
      "(s1 = s3 = 0), and use only the feature in the middle. Next, assume that the stimulus\n",
      "is a more elongated Gabor, which is actually exactly the sum of the two Gabors on\n",
      "the left and the right sides. Now, the sparsest representation is such that the middle\n",
      "feature has zero activity (s2 = 0), and the other two are used with equal coefﬁcients.\n",
      "Thus, the cell in the middle is ﬁrst highly activated, but when the stimulus be-\n",
      "comes more elongated, its activity is reduced, and eventually becomes zero. We can\n",
      "interpret this in terms of competition: The three cells are competing for the “right”\n",
      "to represent the stimulus, and with the ﬁrst stimulus, the cell in the middle wins,\n",
      "whereas when the stimulus is elongated, the other two win. This competition pro-\n",
      "vides a perfect example of end-stopping.\n",
      "This kind of experiments also show that the classical concept of receptive ﬁeld\n",
      "may need to be redeﬁned, as already discussed in Section 13.1.4. After all, the con-\n",
      "cept of receptive ﬁeld is based on the idea that the response of the cell only depends\n",
      "on the light pattern in a particular part of the retinal space. Now, end-stopping, and\n",
      "other phenomena such as contrast gain control, show that the cell response depends\n",
      "on stimulation outside of what is classically called the receptive ﬁeld. Hence, the\n",
      "\n",
      "316\n",
      "14 Lateral interactions and feedback\n",
      "Cell receptive ﬁelds\n",
      "Stimuli\n",
      "Fig. 14.4: The receptive ﬁelds and stimuli used in the end-stopping illustration. When the stimulus\n",
      "on the left is input to the system, the sparsest, i.e. the most probable pattern of coefﬁcients is such\n",
      "that only the cell in the middle is activated si > 0. In contrast, when the stimulus is made longer,\n",
      "i.e. the stimulus on the right is input to the system, the inference leads to a representation in which\n",
      "only the cells on the left and the right are used s1,s3 > 0 whereas the cell in the middle has zero\n",
      "activity s2 = 0.\n",
      "expression classical receptive ﬁeld is used for the part which roughly corresponds\n",
      "to non-zero weights in W(x,y), and the area from which signals of contrast gain\n",
      "control and end-stopping come is called the non-classical receptive ﬁeld. See (An-\n",
      "gelucci et al, 2002) for an investigation of different kinds of receptive ﬁelds.\n",
      "14.3 Predictive coding\n",
      "A closely related idea on the relation between feedback and feedforward processing\n",
      "is predictive coding. There are actually rather different ideas grouped under this\n",
      "title.\n",
      "Firstly, one can consider prediction in time or in space, where “space” means\n",
      "different parts of a static image. Some of the earliest work in predictive coding\n",
      "considered prediction in both (Srivanivasan et al, 1982). Secondly, prediction can be\n",
      "performed between different processing stages (Mumford, 1992; Rao and Ballard,\n",
      "1999) or inside a single stage (Srivanivasan et al, 1982; Hosoya et al, 2005). There\n",
      "is also a large body of engineering methods in which time signals, such as speech,\n",
      "are predicted in time in order to compress the signal (Spanias, 1994); we shall not\n",
      "consider such methods here.\n",
      "We constrain ourselves here to the case where prediction happens between dif-\n",
      "ferent levels of neural processing and for static stimuli. The key idea here is that\n",
      "each neural level tries to predict the activity in the lower processing level. This is\n",
      "\n",
      "14.4 Conclusion\n",
      "317\n",
      "usually coupled with the idea that the lower level sends to the higher level the error\n",
      "in that prediction.\n",
      "Prediction of the activities in lower levels is, in fact, implicit in the noisy gen-\n",
      "erative model we have been using. As we saw in Section 13.1, estimation of the\n",
      "model in Equation (14.1) can be accomplished by maximization of the objective\n",
      "function (the posterior probability) in Equation (14.6) with respect to both aki and\n",
      "si. We can interpret ∑i akisi as the prediction that the higher level makes of lower-\n",
      "level activities. (In Section 14.1 we interpreted it as a denoised estimate which is\n",
      "closely related.) Then, the ﬁrst term in Equation (14.6) can be interpreted as the pre-\n",
      "diction that the higher level makes of the lower level activities ck. Thus, estimation\n",
      "of the model is, indeed, based on minimization of a prediction error as in predictive\n",
      "coding.\n",
      "The idea that the lower level sends only the prediction error to the higher level\n",
      "needs some reinterpretation of the model. In Section 14.1, we showed how inference\n",
      "of si can, under some assumptions, be interpreted as shrinkage. Let us approach the\n",
      "maximization of the posterior probability in Equation (14.8) by a basic gradient\n",
      "method. The partial derivative of the objective function with respect to si equals:\n",
      "∂log p(s|c)\n",
      "∂si\n",
      "= 1\n",
      "σ2 ∑\n",
      "k\n",
      "aki[ck −\n",
      "m\n",
      "∑\n",
      "i=1\n",
      "akisi]+G′(si)\n",
      "(14.11)\n",
      "This derivative actually contains the prediction errors ck −∑m\n",
      "i=1 akisi of the lower-\n",
      "level activities ck, and no other information on the ck. Thus, if the higher level imple-\n",
      "ments a gradient descent method to infer the most likely si, the information which\n",
      "it needs from the lower level can be conveyed by sending these prediction errors\n",
      "(multiplied by the weights aki which can be considered as feedforward connection\n",
      "weights).\n",
      "The main difference between predictive coding and the generative modelling\n",
      "framework may thus be small from the viewpoint of statistical inference. The es-\n",
      "sential difference is in the interpretation of how the abstract quantities are computed\n",
      "and coded in the cortex. In the predictive modelling framework, it is assumed that\n",
      "the prediction errors ck −∑m\n",
      "i=1 akisi are actually the activities (ﬁring rates) of the\n",
      "neurons on the lower level. This is a strong departure from the framework used in\n",
      "this book, where the ck are considered as the activities of the neurons. Which one\n",
      "of these interpretations is closer to the neural reality is an open question which has\n",
      "inspired some experimental work, see (Murray et al, 2002). Something like a syn-\n",
      "thesis of these views is to posit that there are two different kinds of neurons, each\n",
      "sending one of these signals (Roelfsema et al, 2002).\n",
      "14.4 Conclusion\n",
      "In this chapter, we have shown that, in contrast to the impression one might get from\n",
      "preceding chapters, current models of natural images are not at all bound to a strict\n",
      "\n",
      "318\n",
      "14 Lateral interactions and feedback\n",
      "feed-forward thinking which neglects top-down inﬂuence. Quite on the contrary,\n",
      "Bayesian inference in these models leads to different kind of lateral interactions and\n",
      "feedback from higher cortical areas.\n",
      "We have barely scratched the surface here. In many cases where connections be-\n",
      "tween latent variables and images are not completely deterministic and one-to-one\n",
      "in both directions, such phenomena emerge. For example, the two-layer generative\n",
      "model in Section 11.8 would also give rise to such phenomena: If the latent vari-\n",
      "ables are properly inferred from the input stimuli, some interesting dynamics might\n",
      "emerge.\n",
      "Another very important case is contour integration by lateral (horizontal) connec-\n",
      "tions between simple or complex cells. Basic dependencies between cells signalling\n",
      "contours which are typically part of a longer contour were pointed out in (Kr¨uger,\n",
      "1998; Geisler et al, 2001; Sigman et al, 2001; Elder and Goldberg, 2002). Proba-\n",
      "bilistic models incorporating horizontal connections can be found in (Garrigues and\n",
      "Olshausen, 2008; Osindero and Hinton, 2008).\n",
      "A very deep question related to feedback concerns the very deﬁnition of natural\n",
      "images. Any sufﬁciently sophisticated organism has an active mechanism, related to\n",
      "attention, which selects what kind of information it receives by its sensory organs.\n",
      "This introduces a complicated feedback loop between perception and action. It has\n",
      "been pointed out that the statistics in those image parts to which people attend, or\n",
      "direct their gaze, are different from the overall statistics (Reinagel and Zador, 1999;\n",
      "Krieger et al, 2000); see (Henderson, 2003) for a review. The implications of this\n",
      "difference can be quite deep. A related line of work considers contours labelled by\n",
      "human subjects in natural images (Martin et al, 2004).\n",
      "\n",
      "Part IV\n",
      "Time, colour and stereo\n",
      "\n",
      "\n",
      "Chapter 15\n",
      "Colour and stereo images\n",
      "In this chapter, we show how we can model some other visual modalities, colour and\n",
      "stereopsis using ICA. We will see that ICA still ﬁnds features that are quite similar\n",
      "to those computed in the visual cortex.1\n",
      "15.1 Colour image experiments\n",
      "In this section, we extend the ICA image model from grey-scale (achromatic) to\n",
      "colour (chromatic) images. Thus, for each pixel we have three values (red, green\n",
      "and blue), instead of one (grey-scale). The corresponding ICA model is illustrated\n",
      "in Figure 15.1. First, we discuss the selection of data, then we analyse its second-\n",
      "order statistics and ﬁnally show the features found using ICA.\n",
      "= s1·\n",
      "+ s2·\n",
      "+ ···+ sn·\n",
      "Fig. 15.1: The colour image ICA model. As with grey-scale patches, we model the data as a linear\n",
      "combination of feature vectors Ai. Here, each feature vector consists of the three colour planes\n",
      "(red, green and blue), shown separately to clearly illustrate the linear model.\n",
      "1 This chapter was originally published as the article (Hoyer and Hyv¨arinen, 2000) in Network:\n",
      "Computation in Neural Systems. Copyright c⃝2000 Institute of Physics, used with permission.\n",
      "321\n",
      "\n",
      "322\n",
      "15 Colour and stereo images\n",
      "15.1.1 Choice of data\n",
      "Obviously we should select as input data as “natural” images as possible if we wish\n",
      "to make any connection between our results and properties of neurons in the visual\n",
      "cortex. When analysing colours, the spectral composition of the images becomes\n",
      "important in addition to the spatial structure.\n",
      "It is clear that the colour content of images varies widely with the environment\n",
      "in which the images are taken. Thus we do not pretend to ﬁnd some universally\n",
      "optimal features in which to code all natural colour images. Rather, we seek the\n",
      "general qualitative properties of an ICA model of such images. In other words, we\n",
      "hope to ﬁnd answers to questions such as: “How are colours coded in using such\n",
      "features; separate from, or mixed with achromatic channels?” and “What kind of\n",
      "spatial conﬁguration do colour-coding feature vectors have?”\n",
      "We hope that, as with grey-scale images, the ICA features are not too sensitive\n",
      "to the particular choice of colour images, and that our data is realistic enough.\n",
      "Neurons, of course, receive their information ultimately from the outputs of the\n",
      "photoreceptors in the retina. Colour vision is made possible by the existence of\n",
      "photoreceptors called “cones” which come in three types, each sensitive to light\n",
      "of different wavelengths. Thus our data should consist of the hypothetical outputs\n",
      "of the three types of cones in response to our images. However, any three linear\n",
      "combinations of these outputs is just as good an input data, since we are applying\n",
      "ICA: Linearly transforming the data transforms the feature matrix A, but does not\n",
      "alter the independent components.\n",
      "We choose to use standard red/green/blue (RGB) values as inputs, assuming the\n",
      "transformation to cone outputs to be roughly linear. This has the advantage that the\n",
      "features found are directly comparable to features currently in use in image pro-\n",
      "cessing operations such as compression or denoising, and could straightforwardly\n",
      "be applied in such tasks. The drawback of using RGB values as inputs is of course\n",
      "that any nonlinearities inherent in the conversion from RGB to cone responses will\n",
      "affect the ICA result and a comparison to properties of neurons may not be war-\n",
      "ranted. To test the effect of nonlinearities, we have experimented with transforming\n",
      "the RGB values using the well-known gamma-nonlinearity2 of cathode ray tubes\n",
      "used in computer screens. This did not qualitatively change the results, and there-\n",
      "fore we are conﬁdent that our results would be similar if we had used estimated cone\n",
      "outputs as inputs.\n",
      "Our main data consists of colour versions of natural scenes (depicting forest,\n",
      "wildlife, rocks, etc.) which we have used in previous work as well. The data is in\n",
      "the form of 20 RGB images (of size 384 ×256-pixels) in standard TIFF format.\n",
      "2 The gamma-nonlinearity is the most signiﬁcant nonlinearity of the CRT monitor. After gamma-\n",
      "correction the transform from RGB to cone responses is roughly linear; see the appendix in (Wan-\n",
      "dell, 1995).\n",
      "\n",
      "15.1 Colour image experiments\n",
      "323\n",
      "15.1.2 Preprocessing and PCA\n",
      "From the images, a total of 50,000 12-by-12 pixel image patches were sampled ran-\n",
      "domly. Since each channel yields 144 pixels, the dimensionality was now 3×144 =\n",
      "432. Next, the mean value of each variable (pixel/colour pair) was subtracted from\n",
      "that component, centering the dataset on the origin. Note that the DC component\n",
      "was not subtracted.\n",
      "Then, we calculated the covariance matrix and its eigenvectors, which gave us\n",
      "the principal components. These are shown in Figure 15.2. The eigenvectors con-\n",
      "sist of global features, resembling 2D Fourier features. The variance decreases with\n",
      "increasing spatial frequency, and when going from grey-scale to blue/yellow to\n",
      "red/green features.3 These results were established by (Ruderman et al, 1998) who\n",
      "used hyperspectral images (i.e. data with many more than the three spectral compo-\n",
      "nent in RGB data) as their original input data.\n",
      "To analyze the colour content of the PCA ﬁlters in more detail, we will show\n",
      "the pixels of a few ﬁlters plotted in a coloured hexagon. In particular, each pixel\n",
      "(RGB-triplet) is projected onto a plane given by\n",
      "R+G+B = constant.\n",
      "(15.1)\n",
      "In other words, the luminance is ignored, and only the colour content is used in\n",
      "the display. Figure 15.3 shows the colours in this hexagon. Note that this is a very\n",
      "simple 2D projection of the RGB colour cube and should not directly be compared\n",
      "to any neural or psychophysical colour representations.\n",
      "Figure 15.4 shows a bright/dark ﬁlter (no. 3), a blue/yellow ﬁlter (no. 15), a\n",
      "red/green ﬁlter (no. 432, the last one), and a mixture (no. 67). Most ﬁlters are in-\n",
      "deed exclusively opponent colours, as was found in (Ruderman et al, 1998). How-\n",
      "ever, there are also some mixtures of these in the transition zones of main opponent\n",
      "colours.\n",
      "As described earlier, we project the data onto the n ﬁrst principal components\n",
      "before whitening (we have experimented with n = 100, 160, 200, and 250). As can\n",
      "be seen from Figure 15.2, dropping the dimension mostly discards blue/yellow fea-\n",
      "tures of high spatial frequency and red/green features of medium to high frequency.\n",
      "This already gives a hint as to why the blue/yellow and the red/green systems have\n",
      "a much lower resolution than the bright/dark system, as has been observed in psy-\n",
      "chophysical experiments (Mullen, 1985).\n",
      "15.1.3 ICA results and discussion\n",
      "The feature vectors Ai estimated by ICA are shown in Figure 15.5. Examining Fig-\n",
      "ure 15.5 closely reveals that the features found are very similar to earlier results on\n",
      "3 It should be noted that chromatic aberration in the eye might have an effect of additionally\n",
      "reducing signal energy at high spatial frequencies.\n",
      "\n",
      "324\n",
      "15 Colour and stereo images\n",
      "Fig. 15.2: PCA features of colour images. These are the eigenvectors of the covariance matrix of\n",
      "the data, from left-to-right and top-to-bottom in order of decreasing corresponding eigenvalues. As\n",
      "explained in the main text, we projected the data on the ﬁrst 160 principal components (top 8 rows)\n",
      "before performing ICA.\n",
      "grey-scale image data, i.e. the features resemble Gabor-functions. Note that most\n",
      "units are (mainly) achromatic, so they only represent brightness (luminance) vari-\n",
      "ations. This is in agreement with the ﬁnding that a large part of the neurons in the\n",
      "primary visual cortex seem to respond equally well to different coloured stimuli, i.e.\n",
      "are not selective to colour (Hubel and Wiesel, 1968; Livingstone and Hubel, 1984).\n",
      "In addition, there is a small number of red/green and blue/yellow features. These are\n",
      "also oriented, but of much lower spatial frequency, similar to the grey-scale features\n",
      "\n",
      "15.1 Colour image experiments\n",
      "325\n",
      "Fig. 15.3: The colour hexagon used for analyzing the colour content of the PCA and ICA features.\n",
      "The hexagon is the projection of the RGB cube onto a plane orthogonal to the luminance (R+G+\n",
      "B) vector. Thus achromatic RGB triplets map to the center of the hexagon while highly saturated\n",
      "ones are projected close to the edges.\n",
      "Fig. 15.4: Colour content of four PCA ﬁlters. From left to right: Component no. 3, 15, 432, and\n",
      "67. All pixels of each ﬁlter have been projected onto the colour hexagon shown in Figure 15.3. See\n",
      "main text for a discussion of the results.\n",
      "Fig. 15.5: ICA features of colour images. Each patch corresponds to one feature Ai. Note that\n",
      "each feature is equally well represented by its negation, i.e. switching each pixel to its opponent\n",
      "colour in any one patch is equivalent to changing the sign of ai and does not change the ICA model\n",
      "(assuming components with symmetric distributions).\n",
      "\n",
      "326\n",
      "15 Colour and stereo images\n",
      "of lowest frequency. One could think that the low frequency features together form\n",
      "a “colour” (including brightness) system, and the high-frequency grey-scale fea-\n",
      "tures a channel analysing form. Also note that the average colour (DC-value) of the\n",
      "patches is represented by 3 separate feature vectors, just as the average brightness\n",
      "in an ICA decomposition of grey-scale images is usually separate from the other\n",
      "feature vectors.\n",
      "We now show typical ICA features plotted in the colour-hexagon (Figure 15.6),\n",
      "as we did with the PCA features. The ﬁgure shows a bright/dark feature, a blue-\n",
      "yellow feature, and a red/green feature. There were no “mixtures” of the type seen\n",
      "for PCA; in other words each feature clearly belonged to one of these groups. (Note\n",
      "that the bright/dark features also contained blue/yellow to a quite small degree.)\n",
      "Fig. 15.6: Colour content of three ICA ﬁlters, projected onto the colour hexagon of Figure 15.3.\n",
      "From left to right: no. 24, 82, and 12.\n",
      "The dominance of bright/dark features is largely due to the dimension reduction\n",
      "performed while whitening. To test the dependence of the group sizes on the value\n",
      "of n used, we estimated the ICA features for different values of n and counted the\n",
      "group sizes in each case. The results can be seen in Figure 15.7. Clearly, when n is\n",
      "increased, the proportion of colour-selective units increases. However, even in the\n",
      "case of keeping over half of the dimensions of the original space (n = 250), the\n",
      "bright/dark features still make up over 60% of all units.\n",
      "Another thing to note is that each ICA feature is “double-opponent”: For blue-\n",
      "yellow features stimulating with a blue spot always gives an opposite sign in the\n",
      "response compared to stimulating with a yellow spot. Red/green and bright/dark\n",
      "features behave similarly. This is in fact a direct consequence of the linear ICA\n",
      "model. It would be impossible to have completely linear ﬁlters function in any other\n",
      "way.\n",
      "Although early results (Livingstone and Hubel, 1984) on the chromatic prop-\n",
      "erties of neurons suggested that most colour-sensitive cells were unoriented, and\n",
      "exhibited center-surround receptive ﬁelds, more recent studies have indicated that\n",
      "there are also oriented colour-selective neurons (Ts’o and Gilbert, 1988). The fact\n",
      "that our colour features are mostly oriented is thus at least in partial agreement with\n",
      "neurophysiological data.\n",
      "\n",
      "15.1 Colour image experiments\n",
      "327\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Grays\n",
      "B/Y\n",
      "R/G\n",
      "100 dim.\n",
      "160 dim.\n",
      "200 dim.\n",
      "250 dim.\n",
      "Fig. 15.7: Percentages of achromatic, blue/yellow, and red/green feature vectors for different num-\n",
      "bers of retained PCA components (100, 160, 200 and 250). (In each case, the three features giving\n",
      "the mean colour have been left out of this count.)\n",
      "In any case, there is some agreement that most neurons are not selective to\n",
      "chromatic contrast, rather are more concerned about luminance (Hubel and Wiesel,\n",
      "1968; Livingstone and Hubel, 1984; Ts’o and Roe, 1995). Our basis is in agreement\n",
      "with these ﬁndings. In addition, the cytochrome oxidase blobs which have been\n",
      "linked to colour processing (Livingstone and Hubel, 1984) have also been associ-\n",
      "ated with low spatial frequency tuning (Tootell et al, 1988; Shoham et al, 1997). In\n",
      "other words, colour selective cells should be expected to be tuned to lower spatial\n",
      "frequencies. This is also seen in our features.\n",
      "As stated earlier, we do not pretend that our main image set is representative of\n",
      "all natural environments. To check that the results obtained do not vary wildly with\n",
      "the image set used, we have performed the same experiments on another dataset:\n",
      "single-eye colour versions of the 11 stereo images described in Section 15.2.1. The\n",
      "found ICA features (not shown) are in most aspects quite similar to that shown in\n",
      "Figure 15.5: Features are divided into bright/dark, blue/yellow and red/green chan-\n",
      "nels, of which the bright/dark group is the largest, containing Gabor-like ﬁlters of\n",
      "mostly higher frequency than the features coding colours. The main differences are\n",
      "that (a) there is a slightly higher proportion of colour-coding units, and (b) the oppo-\n",
      "nent colours they code are slightly shifted in colour space from those found from our\n",
      "main data. In other words, the qualitative aspects, answering questions such as those\n",
      "proposed in Section 15.1.1, are quite similar. However, quantitative differences do\n",
      "exist.\n",
      "\n",
      "328\n",
      "15 Colour and stereo images\n",
      "15.2 Stereo image experiments\n",
      "Another interesting extension of the basic grey-scale image ICA model can be made\n",
      "by modelling stereopsis, which means the extraction of depth information from\n",
      "binocular disparity. (Binocular disparity refers to the difference in image location\n",
      "of an object seen by the left and right eyes, resulting from the eyes’ horizontal sepa-\n",
      "ration.) Now, our artiﬁcial neurons are attempting to learn the dependencies of cor-\n",
      "responding patches from natural stereo images. The model is shown in Figure 15.8.\n",
      "15.2.1 Choice of data\n",
      "Again, the choice of data is an important step for us to get realistic results. Different\n",
      "approaches are possible here. In some early work, a binocular correlation function\n",
      "was estimated from actual stereo image data, and subsequently analysed (Li and\n",
      "Atick, 1994). In addition, at least one investigation of receptive ﬁeld development\n",
      "used artiﬁcially generated disparity from monocular images (Shouval et al, 1996).\n",
      "Here, we have chosen to use 11 images from a commercial collection of stereo\n",
      "images of natural scenes; a typical image is given in Figure 15.9.\n",
      "To simulate the workings of the eyes, we selected 5 focus points at random from\n",
      "each image and estimated the disparities at these points. We then randomly sampled\n",
      "16 ×16-pixel corresponding image patches in an area of 300 ×300 pixels centered\n",
      "on each focus point, obtaining a total of 50,000 samples. Because of the local ﬂuctu-\n",
      "ations in disparity (due to the 3D imaging geometry) corresponding image patches\n",
      "often contained similar, but horizontally shifted features; this is of course the basis\n",
      "of stereopsis.\n",
      "Note that in reality the “sampling” is quite different. Each neuron sees a certain\n",
      "area of the visual ﬁeld which is relatively constant with respect to the focus point.\n",
      "Thus a more realistic sampling would be to randomly select 50,000 focus points\n",
      "and from each take corresponding image patches at some given constant positional\n",
      "offset. However, the binocular matching is computationally slow and we thus opted\n",
      "for the easier approach, which should give the same distribution of disparities.\n",
      "= s1·\n",
      "+ s2·\n",
      "+ ···+ sn·\n",
      "Fig. 15.8: The ICA model for corresponding stereo image patches. The top row contains the patches\n",
      "from left-eye image and the bottom row corresponding patches from the right-eye image. Just as\n",
      "for grey-scale and colour patches, we model the data as a linear combination of feature vectors Ai\n",
      "with independent coefﬁcients si.\n",
      "\n",
      "15.2 Stereo image experiments\n",
      "329\n",
      "Fig. 15.9: One of the stereo images used in the experiments. The left image should be seen with\n",
      "the left eye, and the right image with the right eye (so-called uncrossed viewing).\n",
      "15.2.2 Preprocessing and PCA\n",
      "The same kind of preprocessing was used in these experiments as for colour, in\n",
      "Section 15.1. Since each sample consisted of corresponding left and right 16 ×16-\n",
      "patches our original data was 512-dimensional. First, the mean was removed from\n",
      "each variable, to center the data on the origin. Next, we calculated the covariance\n",
      "matrix of the data, and its eigenvalue decomposition. In order not to waste space,\n",
      "we show here (in Figure 15.10) the principal components for a window size of 8×8\n",
      "pixels (the result for 16 ×16 is qualitatively very similar).\n",
      "The most signiﬁcant feature is that the principal components are roughly or-\n",
      "dered according to spatial frequency, just as in PCA on standard (monocular) image\n",
      "patches. However, in addition early components (low spatial frequency) are more\n",
      "binocular than late ones (high frequency). Also note that binocular components gen-\n",
      "erally consist of features of identical or opposite phases. This is in agreement with\n",
      "the binocular correlation function described in (Li and Atick, 1994).\n",
      "As before, we select the ﬁrst 160 principal components for further analysis by\n",
      "ICA. Again, this is plausible as a coding strategy for neurons, but is mainly done to\n",
      "lower the computational expenses and thus running time and memory consumption.\n",
      "Due to the structure of the covariance matrix, dropping the dimension to 160 is\n",
      "similar to low-pass ﬁltering.\n",
      "\n",
      "330\n",
      "15 Colour and stereo images\n",
      "Fig. 15.10: PCA features of stereo images, i.e. the eigenvectors of the covariance matrix of the\n",
      "data, from left-to-right and top-to-bottom in order of decreasing corresponding eigenvalues. See\n",
      "main text for discussion.\n",
      "15.2.3 ICA results and discussion\n",
      "Figure 15.11 shows the estimated ICA feature vectors Ai. Each pair of patches rep-\n",
      "resents one feature. First, note that pairs have varying degrees of binocularity. Many\n",
      "of our “model neurons” respond equally well to stimulation from both eyes, but\n",
      "there are also many which respond much better to stimulation of one eye than to\n",
      "stimulation of the other. This is shown quantitatively in Figure 15.12, which gives\n",
      "an “ocular-dominance” histogram of the features. Ocular dominance thus means\n",
      "whether the neuron prefers input from one of the eyes (monocular) or combines\n",
      "information from both eyes (binocular).\n",
      "The histogram depends strongly on the area of the sampling around the focus\n",
      "points (which in these experiments was 300 ×300 pixels). Sampling a smaller area\n",
      "implies that the correlation between the patches is higher and a larger number of\n",
      "features fall into the middle bin of the histogram. In theory, if we chose to sample\n",
      "only exactly at the ﬁxation point, we would obtain (ignoring factors such as occlu-\n",
      "sion) identical left-right image patches; this would in turn make all feature vectors\n",
      "completely binocular with identical left-right patches, as there would be no signal\n",
      "variance in the other directions of the data space. On the other hand, sampling a\n",
      "larger area leads to a spreading of the histogram towards the edge bins. As the area\n",
      "\n",
      "15.2 Stereo image experiments\n",
      "331\n",
      "Fig. 15.11: ICA of stereo images. Each pair of patches represents one feature vector Ai. Note the\n",
      "similarity of these features to those obtained from standard image data (Figure 7.3 on page 169).\n",
      "In addition, these exhibit various degrees of binocularity and varying relative positions and phases.\n",
      "gets larger, the dependencies between the left and right patches get weaker. In the\n",
      "limit of unrelated left and right windows, all features fall into bins 1 and 7 of the\n",
      "histogram. This was conﬁrmed in experiments (results not shown).\n",
      "Taking a closer look at the binocular pairs reveals that for most pairs the left\n",
      "patch is similar to the right patch both in orientation and spatial frequency. The po-\n",
      "sitions of the features inside the patches are close, when not identical. In some pairs\n",
      "the phases are very similar, while in others they are quite different, even completely\n",
      "opposite. These properties make the features sensitive to different degrees of binoc-\n",
      "ular disparity. Identical left-right receptive ﬁelds make the feature most responsive\n",
      "to zero disparity, while receptive ﬁelds that are identical except for a phase reversal\n",
      "show strong inhibition (a response smaller than the “base-line” response given by\n",
      "an optimal monocular stimulus) to zero disparity.\n",
      "To analyse the disparity tuning we ﬁrst estimated several ICA bases using dif-\n",
      "ferent random number seeds. We then selected only relatively high frequency, well\n",
      "localized, binocular features which had a clear Gabor ﬁlter structure. This was nec-\n",
      "essary because ﬁlters of low spatial frequency were not usually well conﬁned within\n",
      "\n",
      "332\n",
      "15 Colour and stereo images\n",
      "the patch and thus cannot be analysed as complete neural receptive ﬁelds. The set\n",
      "of selected feature vectors is shown in Figure 15.13.\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "Fig. 15.12: Ocular dominance histogram of the ICA features. For each pair, we cal-\n",
      "culated the value of (∥Aleft\n",
      "i\n",
      "∥−∥Aright\n",
      "i\n",
      "∥)/(∥Aleft\n",
      "i\n",
      "∥+ ∥Aleft\n",
      "i\n",
      "∥), and used the bin boundaries\n",
      "[−0.85,−0.5,−0.15,0.15,0.5,0.85] as suggested in (Shouval et al., 1996). Although many units\n",
      "where quite monocular (as can be seen from Figure 15.11), no units fell into bins 1 or 7. This\n",
      "histogram is quite dependent on the sampling window around ﬁxation points, as discussed in the\n",
      "main text.\n",
      "Fig. 15.13: Units selected for disparity tuning analysis. These were selected from bases such as\n",
      "the one in Figure 15.11 on the basis of binocularity, frequency content and localization (only well-\n",
      "localized Gabor ﬁlters were suitable for further analysis).\n",
      "\n",
      "15.2 Stereo image experiments\n",
      "333\n",
      "For each stereo pair, we presented an identical stimulus at different disparities\n",
      "to both the left and right parts of the ﬁlter corresponding to the pair. For each dis-\n",
      "parity, the maximum over translations was taken as the response of the pair at that\n",
      "disparity. This gave a disparity tuning curve. For stimuli we used the feature vectors\n",
      "themselves, ﬁrst presenting the left patch of the pair to both “eyes”, then the right.\n",
      "The tuning curves were usually remarkably similar, and we took the mean of these\n",
      "as the ﬁnal curve.\n",
      "We then classiﬁed each curve as belonging to one of the types “tuned excitatory”,\n",
      "“tuned inhibitory”, “near”, or “far”, which have been identiﬁed in physiological ex-\n",
      "periments (Poggio and Fischer, 1977; Fischer and Kruger, 1979; LeVay and Voigt,\n",
      "1988). Tuned excitatory units showed a strong peak at zero, usually with smaller\n",
      "inhibition at either side. Tuned inhibitory units on the other hand showed a marked\n",
      "inhibition (cancelling) at zero disparity, with excitation at small positive or nega-\n",
      "tive disparities. Features classiﬁed as “near” showed a clear positive peak at crossed\n",
      "(positive) disparity while those grouped as “far” a peak for uncrossed (negative) dis-\n",
      "parity. Some tuning curves that did not clearly ﬁt any of these classes were grouped\n",
      "into “others”.\n",
      "In Figure 15.14 we give one example from each class. Shown are the feature\n",
      "vectors and the corresponding tuning curves. It is fairly easy to see how the orga-\n",
      "nization of the patches gives the tuning curves. The tuned excitatory (top) unit has\n",
      "almost identical left-right proﬁles and thus shows a strong preference for stimuli\n",
      "at zero disparities. The tuned inhibitory (second) unit has nearly opposite polarity\n",
      "patches which implies strong inhibition at zero disparity. The near (third) unit’s right\n",
      "receptive ﬁeld is slightly shifted (positional offset) to the left compared with the left\n",
      "ﬁeld, giving it a positive preferred disparity. On the other hand, the far unit (bottom)\n",
      "has an opposite positional offset and thus responds best to negative disparities.\n",
      "Figure 15.15 shows the relative number of units in the different classes. Note that\n",
      "the most common classes are “tuned excitatory” and “near”. One would perhaps\n",
      "have expected a greater dominance of the tuned excitatory over the other groups.\n",
      "The relative number of tuned vs. untuned units probably depends to a great deal on\n",
      "the performance of the disparity estimation algorithm in the sampling procedure.\n",
      "We suspect that with a more sophisticated algorithm (we have used a very simple\n",
      "window-matching technique) one would get a larger number of tuned cells. The\n",
      "clear asymmetry between the “near” and “far” groups is probably due to the much\n",
      "larger range of possible disparities for near than for far stimuli: Disparities for ob-\n",
      "jects closer than ﬁxation can in principle grow arbitrarily large whereas disparities\n",
      "for far objects are limited (Barlow et al, 1967).\n",
      "It is important to note that completely linear units (simple cells) cannot have\n",
      "very selective disparity tuning. Also, since the disparity tuning curves vary with the\n",
      "stimulus, the concept “disparity tuning curve” is not very well-deﬁned (Zhu and\n",
      "Qian, 1996). However, disparity tuning is still measurable so long as one keeps in\n",
      "mind that the curve depends on the stimulus. Our tuning curves are “simulations” of\n",
      "experiments where a moving stimulus is swept across the receptive ﬁeld at differ-\n",
      "ent binocular disparities, and the responses of the neuron in question is measured.\n",
      "As such, it is appropriate to use the estimated feature vectors as input. To obtain\n",
      "\n",
      "334\n",
      "15 Colour and stereo images\n",
      "−10\n",
      "−8\n",
      "−6\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "−10\n",
      "−8\n",
      "−6\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "−10\n",
      "−8\n",
      "−6\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "−10\n",
      "−8\n",
      "−6\n",
      "−4\n",
      "−2\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "Fig. 15.14: Disparity tuning curves for units belonging to different classes. Top row: A “tuned\n",
      "excitatory” unit (no. 4 in Figure 15.13). Second row: a “tuned inhibitory” unit (12). Third row:\n",
      "a “near” unit (38). Bottom row: a “far” unit (47). Crossed disparity (“near”) is labelled positive\n",
      "and uncrossed (“far”) negative in the ﬁgures. The horizontal dotted line gives the “base-line” re-\n",
      "sponse (the optimal response to one-eye only) and the vertical dotted line the position of maximum\n",
      "deviation from that response.\n",
      "stimulus-invariant disparity tuning curves (as well as more complex binocular inter-\n",
      "actions than those seen here) one would need to model nonlinear (complex) cells.\n",
      "Overall, the properties of the found features correspond quite well to those of re-\n",
      "ceptive ﬁelds measured for neurons in the visual cortex. The features show varying\n",
      "degrees of ocular dominance, just as neuronal receptive ﬁelds (Hubel and Wiesel,\n",
      "1962). Binocular units have interocularly matched orientations and spatial frequen-\n",
      "cies, as has been observed for real binocular neurons (Skottun and Freeman, 1984).\n",
      "It is easy by visual inspection to see that there exist both interocular position and\n",
      "\n",
      "15.3 Further references\n",
      "335\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "Far\n",
      "Tuned\n",
      "Near\n",
      "Fig. 15.15: Disparity tuning histogram. The histogram shows the relative amounts of “tuned exci-\n",
      "tatory” (44), “near” (44), “far” (17) units (in black) and “tuned inhibitory” units (25) in white. Not\n",
      "shown are those which did not clearly ﬁt into any of these categories (15).\n",
      "phase differences, which seems to be the case for receptive ﬁelds of cortical neurons\n",
      "(Anzai et al, 1999a). Finally, simulated disparity tuning curves of the found features\n",
      "are also similar to tuning curves measured in physiological experiments (Poggio and\n",
      "Fischer, 1977).\n",
      "15.3 Further references\n",
      "15.3.1 Colour and stereo images\n",
      "Work concerning the second-order statistics of colour include (Atick et al, 1992; van\n",
      "Hateren, 1993; Ruderman et al, 1998). In addition, coloured input was used in (Bar-\n",
      "row et al, 1996) to emerge a topographic map of receptive ﬁelds. Again, that work\n",
      "basically concerns only the second-order structure of the data, as the correlation-\n",
      "based learning used relies only on this information. Application of ICA on colour\n",
      "images has been reported in (Hoyer and Hyv¨arinen, 2000; Wachtler et al, 2001;\n",
      "Doi et al, 2003; Caywood et al, 2004; Wachtler et al, 2007). Related work on LGN\n",
      "neurons can be found in (Mante et al, 2005).\n",
      "In addition to learning chromatic receptive ﬁelds, it is also possible to investigate\n",
      "the statistical properties of the chromatic spectra if single pixels (Wachtler et al,\n",
      "2001). That is, one measures the spectral content of single pixels with a high reso-\n",
      "lution which gives more than the conventional three dimensions. This can shed light\n",
      "on the optimality of the three-cone dimensionality reduction used in the retina.\n",
      "Emerging receptive ﬁelds from stereo input has been considered in (Li and Atick,\n",
      "1994; Shouval et al, 1996; Erwin and Miller, 1996, 1998). As with colour, most\n",
      "\n",
      "336\n",
      "15 Colour and stereo images\n",
      "studies have explicitly or implicitly used only second-order statistics (Li and Atick,\n",
      "1994; Erwin and Miller, 1996, 1998). The exception is (Shouval et al, 1996) which\n",
      "used the BCM learning rule (Bienenstock et al, 1982) which is a type of projection\n",
      "pursuit learning closely linked to ICA. The main difference between their work and\n",
      "the one reported in this chapter is that here we use data from actual stereo images\n",
      "whereas they used horizontally shifted (misaligned) data from regular images. In\n",
      "addition, we estimate a complete basis for the data, whereas they studied only single\n",
      "receptive ﬁelds.\n",
      "15.3.2 Other modalities, including audition\n",
      "Further investigations into the statistical structure of other sensory modalities have\n",
      "been made especially in the context of audition, in which ICA yields interesting\n",
      "receptive ﬁelds whether applied on raw audio data (Bell and Sejnowski, 1996;\n",
      "Lewicki, 2002; Cavaco and Lewicki, 2007) or spectrograms (Klein et al, 2003).\n",
      "See also (Schwartz et al, 2003) for work related to music perception, and (Schwartz\n",
      "and Simoncelli, 2001b) for work on divisive normalization for auditory signals.\n",
      "Further topics which have been addressed using the statistical structure of the\n",
      "ecologically valid environment include visual space (Yang and Purves, 2003), so-\n",
      "matosensory system (Hafner et al, 2003), and place cells (L¨orincz and Buzs´aki,\n",
      "2000). Motion in image sequences is considered in Section 16.2.\n",
      "For some work on multimodal integration and natural image statistics, see (Hurri,\n",
      "2006; Kr¨uger and W¨org¨otter, 2002) (the latter is on image sequences). An image-\n",
      "processing application combining spatial, temporal, and chromatic information is in\n",
      "(Bergner and Drew, 2005).\n",
      "15.4 Conclusion\n",
      "In this chapter, we have investigated the use of independent component analysis for\n",
      "decomposing natural colour and stereo images. ICA applied to colour images yields\n",
      "features which resemble Gabor functions, with most features achromatic, and the\n",
      "rest red/green- or blue/yellow-opponent. When ICA is applied on stereo images\n",
      "we obtain feature pairs which exhibit various degrees of ocular dominance and are\n",
      "tuned to various disparities. Thus, ICA seems to be a plausible model also for these\n",
      "modalities and not just grey-scale images.\n",
      "\n",
      "Chapter 16\n",
      "Temporal sequences of natural images\n",
      "Up to this point this book has been concerned with static natural images. However,\n",
      "in natural environments the scene changes over time. In addition, the observer may\n",
      "move, or the observer may move its eyes. Temporal sequences of natural images,\n",
      "temporal properties of the visual system, and temporal models of processing are the\n",
      "topics of this chapter.\n",
      "16.1 Natural image sequences and spatiotemporal ﬁltering\n",
      "Fig. 16.1: An example of an image sequence (van Hateren and Ruderman, 1998) with 5 frames.\n",
      "Here time proceeds from left to right.\n",
      "In digital systems, dynamical (time-varying) images are often processed as image\n",
      "sequences, which consist of frames, each frame being one static image. Figure 16.1\n",
      "shows an example of an image sequence with lateral camera movement.\n",
      "Previous chapters have made clear the importance of linear operators as tools and\n",
      "models in image processing. In the case of image sequence data, the fundamental\n",
      "linear operation is spatiotemporal linear ﬁltering, which is a straightforward exten-\n",
      "sion of the spatial linear ﬁltering discussed in Chapter 2. Remember that in spatial\n",
      "linear ﬁltering a two-dimensional ﬁlter is slid across the image, and the output is\n",
      "formed by computing a weighted sum of the pixels in the area of the ﬁlter, with\n",
      "the weights given by the elements of the ﬁlter. In spatiotemporal linear ﬁltering, a\n",
      "337\n",
      "\n",
      "338\n",
      "16 Temporal sequences of natural images\n",
      "three-dimensional ﬁlter is slid across the image sequence, and the output is formed\n",
      "by computing a weighted sum of the pixels in the spatiotemporal area of the ﬁlter,\n",
      "with the weights given by the elements of the ﬁlter.\n",
      "Mathematically, let W(x,y,t) denote the ﬁlter weights, I(x,y,t) denote the input\n",
      "image sequence, and O(x,y,t) denote the output image sequence. The index t is\n",
      "time. Then linear spatiotemporal ﬁltering is given by\n",
      "O(x,y,t) =\n",
      "∞\n",
      "∑\n",
      "x∗=−∞\n",
      "∞\n",
      "∑\n",
      "y∗=−∞\n",
      "∞\n",
      "∑\n",
      "t∗=−∞\n",
      "W(x∗,y∗,t∗)I(x+x∗,y+y∗,t +t∗),\n",
      "(16.1)\n",
      "where the upper and lower limits of the sums are in practical situations ﬁnite. Typ-\n",
      "ically only ﬁlters which do not use future time points are used; mathematically we\n",
      "will denote this causality restriction by W(x,y,t) = 0 when t > 0.\n",
      "The concepts of frequency-based representations, presented in Section 2.2 (p. 29)\n",
      "are applicable also in the three-dimensional, spatiotemporal case. An image se-\n",
      "quence can be represented as a sum of spatiotemporal sinusoidal components\n",
      "I(x,y,t) = ∑\n",
      "ωx ∑\n",
      "ωy ∑\n",
      "ωt\n",
      "Aωx,ωy,ωt cos\n",
      "\u0000ωxx+ωyy+ωtt +ψωx,ωy,ωt\n",
      "\u0001\n",
      ",\n",
      "(16.2)\n",
      "where ωx and ωy are spatial frequencies and ωt is a temporal frequency, Aωx,ωy,ωt\n",
      "is the amplitude associated with the frequency triple, and ψωx,ωy,ωt is the phase\n",
      "of the frequency triple. You may want to compare Equation (16.2) with its spatial\n",
      "counterpart, Equation (2.9) on page 33. A spatiotemporal convolution operation is\n",
      "deﬁned by\n",
      "H(x,y,t)∗I(x,y,t) =\n",
      "∞\n",
      "∑\n",
      "x∗=−∞\n",
      "∞\n",
      "∑\n",
      "y∗=−∞\n",
      "∞\n",
      "∑\n",
      "t∗=−∞\n",
      "I(x−x∗,y−y∗,t −t∗)H(x∗,y∗,t∗), (16.3)\n",
      "where H(x,y,t) is the impulse response, which has a straightforward relationship\n",
      "with the linear ﬁlter W(x,y,t)\n",
      "H(x,y,t) = W(−x,−y,−t).\n",
      "(16.4)\n",
      "This impulse response has a complex-valued three-dimensional discrete Fourier\n",
      "transform ˜H(u,v,w), the magnitude of which reveals the amplitude response of the\n",
      "ﬁlter, and the angle reveals the phase response.\n",
      "16.2 Temporal and spatiotemporal receptive ﬁelds\n",
      "With the inclusion of time, we get two new kinds of receptive ﬁelds: spatiotemporal\n",
      "and temporal; these are illustrated in Figure 16.2 in the case of a neuron from the\n",
      "lateral geniculate nucleus. A spatiotemporal receptive ﬁeld W(x,y,t) (Figure 16.2a)\n",
      "corresponds to a causal spatiotemporal ﬁlter: it deﬁnes a linear model that relates the\n",
      "\n",
      "16.2 Temporal and spatiotemporal receptive ﬁelds\n",
      "339\n",
      "a)\n",
      "t (ms)\n",
      "0.0\n",
      "x\n",
      "y\n",
      "−15.6\n",
      "−31.2\n",
      "−46.8\n",
      "−62.4\n",
      "−78.0\n",
      "−93.6\n",
      "−109.2\n",
      "−124.8\n",
      "−140.4\n",
      "−156.0\n",
      "−171.6\n",
      "b)\n",
      "x\n",
      "t (ms)\n",
      "5\n",
      "10\n",
      "15\n",
      "−160\n",
      "−140\n",
      "−120\n",
      "−100\n",
      "−80\n",
      "−60\n",
      "−40\n",
      "−20\n",
      "0\n",
      "c)\n",
      "−200\n",
      "−150\n",
      "−100\n",
      "−50\n",
      "0\n",
      "−0.2\n",
      "−0.1\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "t (ms)\n",
      "W(t)\n",
      "Fig. 16.2: Spatiotemporal and temporal receptive ﬁelds of a neuron in the lateral geniculate nu-\n",
      "cleus (LGN), estimated from measurement data from the neuron. a) A spatiotemporal recep-\n",
      "tive ﬁeld W(x,y,t), the equivalent of a causal linear spatiotemporal ﬁlter. b) A two-dimensional\n",
      "visualization of the RF in a), obtained by summing the spatiotemporal RF along the y-axis:\n",
      "W(x,t) = ∑yW(x,y,t). c) A temporal receptive ﬁeld, which is a single time-slice of the spatiotem-\n",
      "poral RF: W(t) = W(xconst,yconst,t). For the description of the original measurement data and its\n",
      "source see (Dayan and Abbott, 2001; Kara et al, 2000).\n",
      "history of all pixels in the image sequence to the output of a neuron. These inher-\n",
      "ently three-dimensional spatiotemporal receptive ﬁelds are often visualized in two\n",
      "dimensions with one spatial dimension and a temporal dimension W(x,t) by taking\n",
      "either a slice at where y is constant (y = yconst) or summing over the y-dimension\n",
      "(Figure 16.2b).1 A temporal receptive ﬁeld (Figure 16.2c) is the time course of a\n",
      "single spatial location in a spatiotemporal receptive ﬁeld: W(t) = W(xconst,yconst,t).\n",
      "It deﬁnes a linear model that relates the history of a single pixel to the output of a\n",
      "neuron.\n",
      "Spatiotemporal receptive ﬁelds are divided into two qualitatively different types\n",
      "based on whether or not they can be described as a cascade of a spatial and a tempo-\n",
      "ral ﬁltering operation. In the case where this is possible, the spatiotemporal ﬁlter is\n",
      "called is called space-time separable. Let us denote again the output of the ﬁltering\n",
      "by O(x,y,t), the spatial ﬁlter by Wspat(x,y) and the temporal ﬁlter by Wtemp(t). Then,\n",
      "the cascade can be combined into a single spatiotemporal ﬁlter as follows:\n",
      "1 When the RF is selective to a certain spatial orientation of the stimulus, this visualization can be\n",
      "improved by rotating the RF spatially so that the preferred orientation becomes the y-axis.\n",
      "\n",
      "340\n",
      "16 Temporal sequences of natural images\n",
      "O(x,y,t) =\n",
      "∞\n",
      "∑\n",
      "x∗=−∞\n",
      "∞\n",
      "∑\n",
      "y∗=−∞\n",
      "Wspat(x∗,y∗)\n",
      "∞\n",
      "∑\n",
      "t∗=−∞\n",
      "Wtemp(t∗)I(x+x∗,y+y∗,t +t∗)\n",
      "=\n",
      "∞\n",
      "∑\n",
      "x∗=−∞\n",
      "∞\n",
      "∑\n",
      "y∗=−∞\n",
      "∞\n",
      "∑\n",
      "t∗=−∞\n",
      "Wspat(x∗,y∗)Wtemp(t∗)\n",
      "|\n",
      "{z\n",
      "}\n",
      "=W(x∗,y∗,t∗)\n",
      "I(x+x∗,y+y∗,t +t∗)\n",
      "=\n",
      "∞\n",
      "∑\n",
      "x∗=−∞\n",
      "∞\n",
      "∑\n",
      "y∗=−∞\n",
      "∞\n",
      "∑\n",
      "t∗=−∞\n",
      "W(x∗,y∗,t∗)I(x+x∗,y+y∗,t +t∗).\n",
      "(16.5)\n",
      "Thus, the spatiotemporal ﬁlter is obtained as a product of the spatial and temporal\n",
      "parts as\n",
      "W(x,y,t) = Wspat(x,y)Wtemp(t)\n",
      "(16.6)\n",
      "By changing the ordering of the sums in Equation (16.5) it is easy to see that in\n",
      "the space-time separable case, the order in which the spatial and the temporal ﬁl-\n",
      "tering are done is irrelevant. A spatiotemporal receptive ﬁeld that is not space-time\n",
      "separable is called space-time inseparable.\n",
      "a)\n",
      "Wspat(x,y)\n",
      "b)\n",
      "−200\n",
      "−150\n",
      "−100\n",
      "−50\n",
      "0\n",
      "−0.4\n",
      "−0.2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "t (ms)\n",
      "Wtemp(t)\n",
      "c)\n",
      "0.0\n",
      "x\n",
      "y\n",
      "−15.6\n",
      "−31.2\n",
      "−46.8\n",
      "−62.4\n",
      "−78.0\n",
      "−93.6\n",
      "−109.2\n",
      "−124.8\n",
      "−140.4\n",
      "−156.0\n",
      "−171.6\n",
      "Fig. 16.3: A space-time-separable representation of the spatiotemporal RF of Figure 16.2. a),\n",
      "b) The optimal spatial RF Wspat(x,y) and temporal RF Wtemp(t), estimated using the sep-\n",
      "arability condition W(x,y,t) = Wspat(x,y)Wtemp(t). c) The resulting space-time-separable RF\n",
      "Wspat(x,y)Wtemp(t); comparison of this with Figure 16.2a) demonstrates the good match provided\n",
      "by the separable model for this neuron.\n",
      "The spatiotemporal receptive ﬁeld shown in Figure 16.2 is approximately space-\n",
      "time separable: Figure 16.3 shows the decomposition of the receptive ﬁeld into the\n",
      "spatial part and the temporal part, and the resulting space-time-separable approxi-\n",
      "\n",
      "16.3 Second-order statistics\n",
      "341\n",
      "mation.2 This suggests that the linear model of the neuron can be divided into a spa-\n",
      "tial ﬁlter and a temporal ﬁlter. Intuitively speaking, space-time separability means\n",
      "that the RF does not contain anything that “moves” from one place to another, be-\n",
      "cause the spatial proﬁle is all the time in the same place: only its magnitude (and\n",
      "possibly sign) changes.\n",
      "16.3 Second-order statistics\n",
      "16.3.1 Average spatiotemporal power spectrum\n",
      "Now, we begin the investigation of the statistical structure of natural image se-\n",
      "quences by characterizing the spatiotemporal correlations between two pixels in\n",
      "an image sequence. As was discussed in Section 5.6 on p. 116, a characterization of\n",
      "the average power spectrum is equivalent to an examination of these second-order\n",
      "statistics. Therefore, following (Dong and Atick, 1995a), we proceed to analyze the\n",
      "average spatiotemporal power spectrum of natural image sequences.\n",
      "The natural image sequences used as data were a subset of those used in (van\n",
      "Hateren and Ruderman, 1998). The original data set consisted of 216 monochrome,\n",
      "non-calibrated video clips of 192 seconds each, taken from television broadcasts.\n",
      "More than half of the videos feature wildlife, the rest show various topics such as\n",
      "sports and movies. Sampling frequency in the data is 25 frames per second, and\n",
      "each frame had been block-averaged to a resolution of 128 × 128 pixels. For our\n",
      "experiments this data set was pruned to remove the effect of human-made objects\n",
      "and artifacts. First, many of the videos feature human-made objects, such as houses,\n",
      "furniture etc. Such videos were removed from the data set, leaving us with 129\n",
      "videos. Some of these 129 videos had been grabbed from television broadcasts so\n",
      "that there was a wide black bar with height 15 pixels at the top of each image, prob-\n",
      "ably because the original broadcast had been in wide screen format. Our sampling\n",
      "procedure never took samples from this topmost part of the videos.\n",
      "The results of this section are based on the following procedure. We ﬁrst took\n",
      "10,000 samples of size 64 × 64 × 64 = ∆x × ∆y × ∆t from the natural image se-\n",
      "quence. We then computed the spatiotemporal power spectrum of each of these\n",
      "samples by computing the squared amplitudes of three-dimensional discrete Fourier\n",
      "transform of the sample. These power spectra were averaged over all of the sam-\n",
      "ples to obtain the average power spectrum R(ωx,ωy,ωt). Image data is often as-\n",
      "sumed to be approximately rotationally invariant (isotropic, see Section 5.7), so\n",
      "a two-dimensional average power spectrum was computed as a function of spa-\n",
      "tial frequency ωs =\n",
      "q\n",
      "ω2x +ω2y by averaging over all spatial orientations, yielding\n",
      "R(ωs,ωt).\n",
      "2 The decomposition has been obtained by minimizing the squared Euclidean distance between the\n",
      "original RF and its space-time-separable version. This can be solved by employing the singular-\n",
      "value decomposition approximation of matrices.\n",
      "\n",
      "342\n",
      "16 Temporal sequences of natural images\n",
      "a)\n",
      "10\n",
      "−2\n",
      "10\n",
      "−1\n",
      "10\n",
      "0\n",
      "10\n",
      "6\n",
      "10\n",
      "8\n",
      "10\n",
      "10\n",
      "10\n",
      "12\n",
      "ωs (c/p)\n",
      "R(ωs,ωt)\n",
      "ωt = 0 Hz\n",
      "ωt = 3.1 Hz\n",
      "ωt = 6.2 Hz\n",
      "ωt = 9.4 Hz\n",
      "ωt = 12 Hz\n",
      "b)\n",
      "10\n",
      "0\n",
      "10\n",
      "1\n",
      "10\n",
      "6\n",
      "10\n",
      "8\n",
      "10\n",
      "10\n",
      "10\n",
      "12\n",
      "10\n",
      "14\n",
      "ωt (Hz)\n",
      "R(ωs,ωt)\n",
      "ωs = 0 c/p\n",
      "ωs = 0.12 c/p\n",
      "ωs = 0.25 c/p\n",
      "ωs = 0.38 c/p\n",
      "ωs = 0.5 c/p\n",
      "Fig. 16.4: One-dimensional slices of the two-dimensional average spatiotemporal power spectrum\n",
      "R(ωs,ωt) of natural image sequences. a) Plots in which ωt is held constant. b) Plots in which ωs\n",
      "is held constant.\n",
      "a)\n",
      "10\n",
      "−2\n",
      "10\n",
      "−1\n",
      "10\n",
      "0\n",
      "10\n",
      "0\n",
      "10\n",
      "5\n",
      "10\n",
      "10\n",
      "10\n",
      "15\n",
      "ωs (c/p)\n",
      "R(ωs,ωt)\n",
      "observed R(ωs,ωt)\n",
      "best separable Rs(ωs)Rt(ωt)\n",
      "b)\n",
      "10\n",
      "0\n",
      "10\n",
      "1\n",
      "10\n",
      "0\n",
      "10\n",
      "5\n",
      "10\n",
      "10\n",
      "10\n",
      "15\n",
      "ωt (Hz)\n",
      "R(ωs,ωt)\n",
      "observed R(ωs,ωt)\n",
      "best separable Rs(ωs)Rt(ωt)\n",
      "Fig. 16.5: The average spatiotemporal power spectrum of natural image sequences R(ωs,ωt) is\n",
      "not well approximated by a space-time separable Rs(ωs)Rt(ωt). These plots show the observed\n",
      "curves plotted in Figure 16.4 along with plots from the best separable spatiotemporal spectrum\n",
      "(here “best” is deﬁned by minimal least mean square distance). The uppermost curves contain both\n",
      "the observed and best separated curves on almost exactly on top of each other, which shows that in\n",
      "the case of the lowest frequencies, the approximation is very good.\n",
      "One way to visualize the resulting two-dimensional function R(ωs,ωt) is to plot\n",
      "curves of the function while keeping one of the variables ﬁxed. This has been done in\n",
      "Figures 16.4a) and b), keeping ωt constant in the former and ωs in the latter. In order\n",
      "to analyze the form of this power spectrum in more detail, one can ﬁrst ﬁt a space-\n",
      "time separable power spectrum Rs(ωs)Rt(ωt); the best ﬁt (in terms of least mean\n",
      "square) is visualized in Figure 16.5 in similar plots as those in Figures 16.4, but this\n",
      "time plotting curves from both the observed and the best space-time separable power\n",
      "spectrum. As can be seen, at higher frequencies the best separable power spectrum\n",
      "provides a relatively poor match to the observed one.\n",
      "\n",
      "16.3 Second-order statistics\n",
      "343\n",
      "a)\n",
      "ωs\n",
      "ωt\n",
      "−2.9 −1.5\n",
      "0\n",
      "1.5\n",
      "2.9\n",
      "−2.9\n",
      "−1.5\n",
      "0\n",
      "1.5\n",
      "2.9\n",
      "s\n",
      "t\n",
      "20\n",
      "40\n",
      "60\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "b)\n",
      "ωs\n",
      "ωt\n",
      "−2.9 −1.5\n",
      "0\n",
      "1.5\n",
      "2.9\n",
      "−2.9\n",
      "−1.5\n",
      "0\n",
      "1.5\n",
      "2.9\n",
      "s\n",
      "t\n",
      "20\n",
      "40\n",
      "60\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "Fig. 16.6: In the frequency-based representation of the s–t-space, the direction of the frequency\n",
      "vector ω = [ωs ωt]T is equivalent to the speed of the pixels of a moving spatial grating in the image\n",
      "sequence. This is illustrated here for two different (ωs,ωt)-pairs (a, b), for which the frequency-\n",
      "based representation is shown on the left, and the s–t-representation on the right. One can see that\n",
      "the pixels in the spatial gratings move with the same speed: for example, looking at the pixel which\n",
      "starts at the corner position (s,t) = (64,1), it can be seen that in both cases (a, b), the pixel moves\n",
      "across the whole image when time t runs from 1 to 64, indicating similar speed of movement.\n",
      "In order to proceed to a more accurate model of the spatiotemporal power spec-\n",
      "trum of natural image sequences, let us reconsider the frequency representation\n",
      "of the s–t-space. Referring to our presentation of the two-dimensional frequency-\n",
      "based representation in Section 2.2.2 – in particular, see Figure 2.5 on page 32 – let\n",
      "ω = [ωs ωt]T . The vector ω has two important properties: direction and magnitude\n",
      "(length). Now consider the direction of the vector. In the case of a two-dimensional\n",
      "spatial frequency-based representation, the direction of the vector [ωx ωy]T deter-\n",
      "mines the spatial orientation of the sinusoidal in the x–y-space. Analogously, in the\n",
      "spatiotemporal case, the direction of the vector ω = [ωs ωt]T determines the ori-\n",
      "entation of the sinusoidal in the s–t-space. We are able to provide a more intuitive\n",
      "interpretation for orientation in the s–t-space: it is the speed of the spatial pattern\n",
      "that is moving. Figure 16.6 illustrates this. Points in the (ωs,ωt)-space that have the\n",
      "same speed (direction) lie on a line ωt = cωs, where c is a constant. Therefore, the\n",
      "set of (ωs,ωt)-points have the same speed when ωt\n",
      "ωs = constant.\n",
      "It was observed by Dong and Atick (1995b) that the power spectrum has a par-\n",
      "ticularly simple form as a function of spatial frequency ωs when the speed ωt\n",
      "ωs is\n",
      "held constant. Figure 16.7a) shows plots of R(ωs,ωt) as a function of spatial ωs for\n",
      "different constant values of speed ωt\n",
      "ωs . As can be seen, in this log-log-plot all the\n",
      "curves are similar to straight lines with the same slope but different intercepts for\n",
      "different values of ωt\n",
      "ωs . Denoting the common slope by −a, a > 0, and the intercepts\n",
      "by b\n",
      "\u0010\n",
      "ωt\n",
      "ωs\n",
      "\u0011\n",
      ", this suggests that\n",
      "\n",
      "344\n",
      "16 Temporal sequences of natural images\n",
      "a)\n",
      "10\n",
      "−1\n",
      "10\n",
      "0\n",
      "10\n",
      "6\n",
      "10\n",
      "7\n",
      "10\n",
      "8\n",
      "10\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "11\n",
      "ωs (c/p)\n",
      "R(ωs,ωt)\n",
      "ωt/ωs = 1.9 p/s\n",
      "ωt/ωs = 3.3 p/s\n",
      "ωt/ωs = 7.1 p/s\n",
      "ωt/ωs = 17 p/s\n",
      "ωt/ωs = 33 p/s\n",
      "b)\n",
      "10\n",
      "−1\n",
      "10\n",
      "0\n",
      "10\n",
      "1\n",
      "10\n",
      "2\n",
      "10\n",
      "3\n",
      "10\n",
      "0\n",
      "10\n",
      "2\n",
      "10\n",
      "4\n",
      "10\n",
      "6\n",
      "10\n",
      "8\n",
      "ωt/ωs (p/s)\n",
      "R(ωs,ωt)ωs\n",
      "3.7\n",
      "Fig. 16.7: The average spatiotemporal power spectrum R(ωs,ωt) of natural image sequences can\n",
      "be separated into functions depending on spatial frequency ωs and speed ωt\n",
      "ωs . a) Log-log plots of\n",
      "R(ωs,ωt) as a function of ωs are straight lines, suggesting that R(ωs,ωt) ≈ω−a\n",
      "s\n",
      "f\n",
      "\u0010\n",
      "ωt\n",
      "ωs\n",
      "\u0011\n",
      ", where\n",
      "a > 0 and f\n",
      "\u0010\n",
      "ωt\n",
      "ωs\n",
      "\u0011\n",
      "is a function of speed. b) A plot of f\n",
      "\u0010\n",
      "ωt\n",
      "ωs\n",
      "\u0011\n",
      "≈ω3.7\n",
      "s\n",
      "R(ωs,ωt). See text for details.\n",
      "logR(ωs,ωt) ≈−alogωs +b\n",
      "\u0012ωt\n",
      "ωs\n",
      "\u0013\n",
      "(16.7)\n",
      "R(ωs,ωt) ≈ω−a\n",
      "s\n",
      "exp\n",
      "\u0014\n",
      "b\n",
      "\u0012ωt\n",
      "ωs\n",
      "\u0013\u0015\n",
      "|\n",
      "{z\n",
      "}\n",
      "=f( ωt\n",
      "ωs )\n",
      "(16.8)\n",
      "R(ωs,ωt) ≈ω−a\n",
      "s\n",
      "f\n",
      "\u0012ωt\n",
      "ωs\n",
      "\u0013\n",
      ",\n",
      "(16.9)\n",
      "where f(·) is an unknown function of speed. When an estimate of the slope a has\n",
      "been computed (e.g., from the data shown in Figure 16.7a), an approximate plot of\n",
      "function f(·) can be obtained from\n",
      "f\n",
      "\u0012ωt\n",
      "ωs\n",
      "\u0013\n",
      "≈ωa\n",
      "s R(ωs,ωt);\n",
      "(16.10)\n",
      "this plot is shown in Figure 16.7b) for a = 3.7.\n",
      "Dong and Atick (1995a) went two steps further in the characterization of R(ωs,ωt).\n",
      "First, they derived Equation (16.9) from the average power spectrum of static images\n",
      "and a model in which objects move with different velocities at different distances.\n",
      "Second, by assuming a distribution of object velocities they also derived a para-\n",
      "metric form for function f(·) which agrees well with the observed R(ωs,ωt) with\n",
      "reasonable parameter values. See their paper for more details.\n",
      "The spatiotemporal power spectrum seems to exhibit some anisotropies (see Sec-\n",
      "tion 5.7, i.e. it is not the same in all orientations). This can be used to explain some\n",
      "psychophysical phenomena (Dakin et al, 2005).\n",
      "\n",
      "16.3 Second-order statistics\n",
      "345\n",
      "16.3.2 The temporally decorrelating ﬁlter\n",
      "In Section 5.9 (page 126) we saw that the removal of linear correlations – that is,\n",
      "whitening – forms the basis of a model that results in the emergence of spatial\n",
      "center-surround receptive ﬁelds from natural data. In this section we apply similar\n",
      "theory to the case of temporal data and temporal receptive ﬁelds (see Figure 16.2c on\n",
      "page 339). We are examining the statistical properties of purely temporal data here,\n",
      "that is, samples consisting of time courses of individual pixels (which are sampled\n",
      "from different spatial locations in the image sequences).\n",
      "We proceed similarly as in the spatial case. Let Ri(ωt) denote the temporal power\n",
      "spectrum of natural image sequence data (time courses of individual pixels). As in\n",
      "the spatial case, we assume that noise power Rn(ωt) is constant, and given by\n",
      "Rn(ωt) = Ri(ωt,c)\n",
      "2\n",
      "for all ωt,\n",
      "(16.11)\n",
      "where ωt,c is the characteristic frequency at which the data and noise have the same\n",
      "power. As in the spatial case (see Equation (5.49) on page 133), we deﬁne the am-\n",
      "plitude response of the ﬁlter\n",
      "\f\f ˜W(ωt)\n",
      "\f\f as the product of the amplitude responses of a\n",
      "whitening ﬁlter and a noise-suppressive ﬁlter:\n",
      "\f\f ˜W(ωt)\n",
      "\f\f =\n",
      "1\n",
      "p\n",
      "Ri(ωt)\n",
      "Ri(ωt)−Rn(ωt)\n",
      "Ri(ωt)\n",
      ".\n",
      "(16.12)\n",
      "As was mentioned in Section 5.9, a shortcoming of the decorrelation theory is that\n",
      "it does not predict the phase response of the ﬁlter. Here we use the principle of\n",
      "minimum energy delay: the phases are speciﬁed so that the energy in the impulse\n",
      "response of the resulting causal ﬁlter is delayed the least. The phase response of a\n",
      "minimum energy delay ﬁlter is given by the Hilbert transform of the logarithm of\n",
      "the amplitude response; see (Oppenheim and Schafer, 1975) for details. After the\n",
      "amplitude and the phase responses have been deﬁned, the temporal ﬁlter itself can\n",
      "be obtained by taking the inverse Fourier transform.\n",
      "The ﬁlter properties that result from the application of Equations (16.11) and\n",
      "(16.12) and the minimum energy delay principle are illustrated in Figure 16.8 for\n",
      "a characteristic frequency value of ωs,t = 7Hz (the same value was used in (Dong\n",
      "and Atick, 1995b)). For this experiment, 100,000 signals of spatial size 1×1 pixels\n",
      "and a duration of 64 time points (≈2.5s) were sampled from the image sequence\n",
      "data of van Hateren and Ruderman (1998). The average temporal power spectrum\n",
      "of these signals was then computed and is shown in Figure 16.8a). The squared am-\n",
      "plitude response of the whitening ﬁlter, obtained from Equation (16.12), is shown in\n",
      "Figure 16.8b). The power spectrum of the ﬁltered data is shown in Figure 16.8c); it\n",
      "is approximately ﬂat at lower frequencies and drops off at high frequencies because\n",
      "of the higher relative noise power at high frequencies. The resulting ﬁlter is shown\n",
      "in Figure 16.8d); for comparison, a measured temporal receptive ﬁeld of an LGN\n",
      "neuron is shown in Figure 16.8e). Please observe the difference in the time scales\n",
      "in Figures 16.8d) and e). Here the match between the two linear ﬁlters is only qual-\n",
      "\n",
      "346\n",
      "16 Temporal sequences of natural images\n",
      "a)\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "Ri(ωt)\n",
      "10−1\n",
      "100\n",
      "101\n",
      "102\n",
      "ωt (Hz)\n",
      "b)\n",
      "10−4\n",
      "10−3\n",
      "10−2\n",
      "10−1\n",
      "|W (ωt)|2\n",
      "10−1\n",
      "100\n",
      "101\n",
      "102\n",
      "ωt (Hz)\n",
      "c)\n",
      "101\n",
      "102\n",
      "103\n",
      "|W (ωt)|2 Ri(ωt)\n",
      "10−1\n",
      "100\n",
      "101\n",
      "102\n",
      "ωt (Hz)\n",
      "d)\n",
      "−400\n",
      "−300\n",
      "−200\n",
      "−100\n",
      "0\n",
      "−0.1\n",
      "−0.05\n",
      "0\n",
      "0.05\n",
      "0.1\n",
      "0.15\n",
      "W(t)\n",
      "t (ms)\n",
      "e)\n",
      "−200\n",
      "−150\n",
      "−100\n",
      "−50\n",
      "0\n",
      "−0.2\n",
      "−0.1\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "t (ms)\n",
      "W(t)\n",
      "Fig. 16.8: The application of the whitening principle, combined with noise reduction and minimum\n",
      "energy delay phase response, leads to the emergence of ﬁlters resembling the temporal receptive\n",
      "ﬁelds of neurons in the retina and the LGN. a) The temporal power spectrum Ri(ωt) of natural\n",
      "image sequence data. b) The squared amplitude response of a whitening ﬁlter which suppresses\n",
      "noise: this curve follows the inverse of the data power spectrum at low frequencies, but drops off\n",
      "at high frequencies, because the proportion of noise is larger at high frequencies. c) The power\n",
      "spectrum of the resulting (ﬁltered) data, showing approximately ﬂat (white) power at low frequen-\n",
      "cies, and dropping off at high frequencies. d) The resulting ﬁlter which has been obtained from\n",
      "the amplitude response in b) and by specifying a minimum energy delay phase response; see text\n",
      "for details. e) For comparison, the temporal receptive ﬁeld of an LGN neuron. Please note the\n",
      "differences in the time scales in d) and e).\n",
      "\n",
      "16.4 Sparse coding and ICA of natural image sequences\n",
      "347\n",
      "itative; in experimental animals, the latencies of LGN cells seem to vary from tens\n",
      "to hundreds milliseconds (Saul and Humphrey, 1990). Similar temporal processing\n",
      "properties are often attributed to retinal ganglion cells (Meister and Berry II, 1999),\n",
      "although Dong and Atick (1995a) argue that the temporal frequency response of\n",
      "retinal cells is typically ﬂat when compared with the response of neurons in the\n",
      "LGN.\n",
      "Dong and Atick (1995a) proceed by showing that when combined with basic\n",
      "neural nonlinearities (rectiﬁcation), the temporally decorrelating ﬁlter theory yields\n",
      "response properties that match the timing and phase response properties of LGN\n",
      "neurons. For additional experimental evaluation of the model, see (Dan et al, 1996b).\n",
      "Here we have used a linear neuron model with a constant ﬁlter (static receptive\n",
      "ﬁeld). In reality the temporal receptive ﬁeld of a visual neuron may change, and this\n",
      "adaptation may be related to the short-term changes in stimulus statistics (Hosoya\n",
      "et al, 2005).\n",
      "16.4 Sparse coding and ICA of natural image sequences\n",
      "To analyze the spatiotemporal statistics beyond covariances, the ICA model can be\n",
      "applied directly to natural image sequences. Instead of vectorizing image patches\n",
      "(windows), and using them as data in the ICA model, spatiotemporal image se-\n",
      "quence blocks can be vectorized to form the data x. After a spatiotemporal feature\n",
      "detector weight vector w or, alternatively, a spatiotemporal feature vector a has been\n",
      "learned from the data, it can be visualized as an image sequence after “unvectoriz-\n",
      "ing” it, just like in the basic spatial case.\n",
      "Results of estimating spatiotemporal features by ICA are shown in in Figure 16.9.\n",
      "The data consisted of image sequence blocks of size (11,11,9), where the two ﬁrst\n",
      "values are in pixels and the third value is in time steps. We used two different sam-\n",
      "pling rates, 25Hz and 3.125Hz because that parameter has a visible inﬂuence on the\n",
      "results. The number of spatiotemporal patches was 200,000, and the dimension was\n",
      "reduced by approximately 50% by PCA. The data set was the same van Hateren\n",
      "and Ruderman (1998) dataset as used above. FastICA was run in the symmetric\n",
      "mode with nonlinearity g(α) = tanh(α), which corresponds to the familiar log cosh\n",
      "measure of sparseness (see Section 18.7).\n",
      "The estimated features shown in Figure 16.9 are spatially Gabor-like, some of\n",
      "them are separable and other are not. The results clearly depend on the sampling\n",
      "rate: if the sampling rate is high (a), the features tend to be static, i.e., there is hardly\n",
      "any temporal change. This is intuitively comprehensible: if the time resolution in\n",
      "the data is too high, there is simply not enough time for any changes to occur. When\n",
      "the sampling rate is lower (b), there is much more temporal change in the features.\n",
      "The results are thus quite well in line with those measured in single-cell record-\n",
      "ings, in e.g. (DeAngelis et al, 1993a,b, 1995).\n",
      "Further results on estimating spatiotemporal features vectors obtained by apply-\n",
      "ing\n",
      "FastICA\n",
      "to\n",
      "natural\n",
      "image\n",
      "sequence\n",
      "data\n",
      "can\n",
      "be\n",
      "found\n",
      "at\n",
      "\n",
      "348\n",
      "16 Temporal sequences of natural images\n",
      "http://hlab.phys.rug.nl/demos/ica/index.html, and the paper\n",
      "(van Hateren and Ruderman, 1998).\n",
      "a)\n",
      "b)\n",
      "Fig. 16.9: Spatiotemporal features estimated by ICA. Each row in each display (a or b) corresponds\n",
      "to one feature vector, i.e. one column of the matrix A in the ICA model. On a given row, each\n",
      "frame corresponds to one spatial frame with time index ﬁxed, so that time goes from left to right.\n",
      "Thus, each feature is basically obtained by “playing” the frames on one row one after the other. a)\n",
      "Sampling rate 25Hz, i.e., sampled every 40ms. b) Sampling rate 3.125Hz, i.e. sample every 320ms.\n",
      "\n",
      "16.5 Temporal coherence in spatial features\n",
      "349\n",
      "16.5 Temporal coherence in spatial features\n",
      "16.5.1 Temporal coherence and invariant representation\n",
      "Our visual environment has inertia: during a short time interval, the scene we see\n",
      "tends to remain similar in the sense that the same objects persist in our ﬁeld of\n",
      "vision, the lighting conditions usually change slowly etc. Could our visual system\n",
      "utilize this property of our environment?\n",
      "In particular, it has been proposed that those properties which change more\n",
      "quickly are often less important for pattern recognition: The identities of the ob-\n",
      "jects in our visual ﬁeld change slower than their appearance. For example, when\n",
      "you talk with somebody, you see the same face for a long time, but its appearance\n",
      "undergoes various transformations due to the change in the facial expression and the\n",
      "muscle actions related to speech. So, if you consider those features which change\n",
      "the slowest, they might be directly related to the identity of the interlocutor.\n",
      "Thus, it has been proposed that a good internal representation for sensory input\n",
      "would be one that changes slowly. The term temporal coherence refers to a repre-\n",
      "sentation principle in which, when processing temporal input, the representation in\n",
      "the computational system is optimized so that it changes as slowly as possible over\n",
      "time (Hinton, 1989; F¨oldi´ak, 1991).\n",
      "In this section, we will take a look at a model of temporal coherence which\n",
      "results in the emergence of simple-cell-like RF’s from natural image sequence data.\n",
      "In the next section, this will be extended to a model that exhibit complex-cell-like\n",
      "behaviour and topographical organization of RF’s.\n",
      "16.5.2 Quantifying temporal coherence\n",
      "It has been argued that the neural output in the visual system is characterized tem-\n",
      "porally as short, intense ﬁring events, or bursts of spikes (Reinagel, 2001). Here we\n",
      "present a model which optimizes a measure of such temporal coherence of activity\n",
      "levels – or energy – and which, when applied to a set of natural image sequences,\n",
      "leads to the emergence of RF’s which resemble simple-cell RF’s.3\n",
      "We use a set of spatial feature detectors (weight vectors) w1,...,wK to relate in-\n",
      "put to output. While it may ﬁrst sound weird to use purely spatial features with\n",
      "spatiotemporal data, this simpliﬁcation will make better sense below when we in-\n",
      "troduce the temporal ﬁltering used in preprocessing; this combination of temporal\n",
      "and spatial features is equivalent to space-time separable spatiotemporal features\n",
      "(see Section 16.2, page 339). Let vector x(t) denote the (preprocessed) input to the\n",
      "system at time t. The output of the kth feature detector at time t, denoted by sk(t),\n",
      "is given by sk(t) = wT\n",
      "k x(t). Let matrix W = [w1 ···wK]T denote a matrix with all\n",
      "3 This section is based on the article (Hurri and Hyv¨arinen, 2003a) originally published in Neural\n",
      "Computation. Copyright c⃝2003 MIT Press, used with permission.\n",
      "\n",
      "350\n",
      "16 Temporal sequences of natural images\n",
      "the feature detector weights as rows. Then the input-output relationship can be ex-\n",
      "pressed in vector form by s(t) = Wx(t), where vector s(t) = [s1(t)···sK(t)]T .\n",
      "To proceed to the objective function, we ﬁrst deﬁne a nonlinearity g(·) that mea-\n",
      "sures the strength (amplitude) of the feature, and emphasizes large responses over\n",
      "small ones: we require that g is strictly convex, even-symmetric (rectifying), and\n",
      "differentiable. Examples of choices for this nonlinearity are g1(x) = x2, which mea-\n",
      "sures the energy of the response, and g2(x) = logcoshx, which is a robustiﬁed ver-\n",
      "sion of g1 (less sensitive to outliers). Let the symbol ∆t denote a delay in time.\n",
      "Temporal response strength correlation, the objective function, is deﬁned by\n",
      "f(W) =\n",
      "K\n",
      "∑\n",
      "k=1\n",
      "T\n",
      "∑\n",
      "t=1+∆t\n",
      "g(sk(t))g(sk(t −∆t)).\n",
      "(16.13)\n",
      "A set of feature detectors which has a large temporal response strength correlation\n",
      "is such that the same features often respond strongly at consecutive time points,\n",
      "outputting large (either positive or negative) values. This means that the same fea-\n",
      "tures will respond strongly over short periods of time, thereby expressing temporal\n",
      "coherence of activity levels in the neuronal population.\n",
      "To keep the outputs of the features bounded we enforce the unit variance\n",
      "constraint on each of the output signals sk(t), that is, we enforce the constraint\n",
      "Et\n",
      "\b\n",
      "s2\n",
      "k(t)\n",
      "\t\n",
      "= wT\n",
      "k Cxwk = 1 for all k, where Cx is the covariance matrix Et{x(t)xT(t)},\n",
      "and Et means average over t. Additional constraints are needed to keep the feature\n",
      "detectors from converging to the same solution. Standard methods are either to force\n",
      "the set of feature weights to be orthogonal, or to force their outputs to be uncorre-\n",
      "lated, from which we choose the latter, as in preceding chapters. This introduces\n",
      "additional constraints wT\n",
      "i Cxwj = 0, i = 1,...,K, j = 1,...,K, j ̸= i. These uncorre-\n",
      "latedness constraints limit the number of features K we can ﬁnd so that if the image\n",
      "data has spatial size N × N pixels, then K ≤N2. The unit variance constraints and\n",
      "the uncorrelatedness constraints can be expressed by a single matrix equation\n",
      "WCxWT = I.\n",
      "(16.14)\n",
      "Note that if we use a nonlinearity g(x) = x2, and ∆t = 0, the objective function\n",
      "becomes f(W) = ∑K\n",
      "k=1 Et\n",
      "\b\n",
      "s4\n",
      "k(t)\n",
      "\t\n",
      ". In this case the optimization of the objective\n",
      "function under the unit variance constraint is equivalent to optimizing the sum of\n",
      "kurtoses of the outputs. As was discussed in Section 6.2.1 on page 139, kurtosis is\n",
      "a commonly used measure in sparse coding. Similarly, in the case of nonlinearity\n",
      "g(x) = logcoshx and ∆t = 0, the objective function can be interpreted as a non-\n",
      "quadratic measure of the non-gaussianity of features.\n",
      "Thus, the receptive ﬁelds are learned in the model by maximizing the objective\n",
      "function in Equation (16.13) under the constraint in Equation (16.14). The opti-\n",
      "mization algorithm used for this constrained optimization problem is a variant of\n",
      "the gradient projection method described in Section 18.2.4. The optimization ap-\n",
      "proach employs whitening, that is, a temporary change of coordinates, to transform\n",
      "the constraint (16.14) into an orthogonality constraint. Then a gradient projection\n",
      "\n",
      "16.5 Temporal coherence in spatial features\n",
      "351\n",
      "algorithm employing optimal symmetric orthogonalization can be used. See (Hurri\n",
      "and Hyv¨arinen, 2003a) for details.\n",
      "16.5.3 Interpretation as generative model *\n",
      "An interpretation of maximization of objective function (16.13) as estimation of a\n",
      "generative model is possible, based on the concept of sources with non-stationary\n",
      "(non-constant)variances (Matsuoka et al, 1995; Pham and Cardoso, 2001; Hyv¨arinen,\n",
      "2001a). The linear generative model for x(t) is similar to the one in previous chap-\n",
      "ters:\n",
      "x(t) = As(t).\n",
      "(16.15)\n",
      "Here A = [a1···aK] denotes a matrix which relates the image sequence patch x(t)\n",
      "to the activities of the simple cells, so that each column ak, k = 1,...,K, gives the\n",
      "feature that is coded by the corresponding simple cell. The dimension of x(t) is\n",
      "typically larger than the dimension of s(t), so that (16.15) is generally not invertible.\n",
      "A one-to-one correspondence between W and A can be established by using the\n",
      "pseudo-inverse solution (see Section 19.8):\n",
      "A = WT(WWT)−1.\n",
      "(16.16)\n",
      "a)\n",
      "−3\n",
      "0\n",
      "3\n",
      "0\n",
      "200\n",
      "400\n",
      "time index\n",
      "s(t)\n",
      "b)\n",
      "0\n",
      "3\n",
      "6\n",
      "9\n",
      "0\n",
      "200\n",
      "400\n",
      "time index\n",
      "s2(t)\n",
      "Fig. 16.10: Illustration of non-stationarity of variance. a) A temporally uncorrelated signal s(t)\n",
      "with non-stationary variance. b) Plot of s2(t).\n",
      "The non-stationarity of the variances of sources s(t) means that their variances\n",
      "change over time, and the variance of a signal is correlated at nearby time points.\n",
      "An example of a signal with non-stationary variance is shown in Figure 16.10. It\n",
      "can be shown (Hyv¨arinen, 2001a) that optimization of a cumulant-based criterion,\n",
      "similar to Equation (16.13), can separate independent sources with non-stationary\n",
      "variances. Thus, the maximization of the objective function can also be interpreted\n",
      "as estimation of generative models in which the activity levels of the sources vary\n",
      "over time, and are temporally correlated over time. This situation is analogous to\n",
      "\n",
      "352\n",
      "16 Temporal sequences of natural images\n",
      "the application of measures of sparseness to estimate linear generative models with\n",
      "independent non-gaussian sources, i.e. the ICA model treated in Chapter 7.\n",
      "16.5.4 Experiments on natural image sequences\n",
      "16.5.4.1 Data and preprocessing\n",
      "The natural image data used in the experiments was described in Section 16.3.1\n",
      "(page 341). The ﬁnal, preprocessed (see below) data set consisted of 200,000 pairs\n",
      "of consecutive 11 × 11 image patches at the same spatial position, but ∆t mil-\n",
      "liseconds apart from each other. In the main experiment, ∆t = 40ms; other values\n",
      "were used in the control experiments. However, because of the temporal ﬁltering\n",
      "used in preprocessing, initially 200,000 longer image sequences with a duration of\n",
      "∆t + 400ms, and the same spatial size 11 × 11, were sampled with the same sam-\n",
      "pling rate.\n",
      "The preprocessing in the main experiment consisted of three steps: temporal\n",
      "decorrelation, subtraction of local mean, and normalization. The same preprocess-\n",
      "ing steps were applied in the control experiments; whenever preprocessing was var-\n",
      "ied in control experiments it is explained separately below. Temporal decorrelation\n",
      "can be motivated in two different ways. First, as was discussed in Section 16.3.2\n",
      "(page 345) it can be motivated biologically as a model of temporal processing in\n",
      "the early visual system. Second, as discussed above, for ∆t = 0 the objective func-\n",
      "tion can be interpreted as a measure of sparseness. Therefore it is important to rule\n",
      "out the possibility that there is hardly any change in short intervals in video data,\n",
      "since this would imply that our results could be explained in terms of sparse cod-\n",
      "ing or ICA. To make the distinction between temporal response strength correlation\n",
      "and measures of sparseness clear, temporal decorrelation was applied because it en-\n",
      "hances temporal changes. Note, however, that this still does not remove all of the\n",
      "static part in the video – this issue is addressed in the control experiments below.\n",
      "Temporal decorrelation was performed with the temporal ﬁlter shown in Fig-\n",
      "ure 16.8d (page 346). As was already mentioned above, the use of such a temporal\n",
      "ﬁlter in conjunction with the learned spatial features makes the overall model spa-\n",
      "tiotemporal (to be more exact, space-time separable).\n",
      "16.5.4.2 Results and analysis\n",
      "In the main experiment, nonlinearity g in objective function (16.13) was chosen to\n",
      "be g(x) = logcoshx. A set of feature detector weights (rows of W) learned by the\n",
      "model is shown in Figure 16.11a). The features resemble Gabor functions. They are\n",
      "localized, oriented, and have different scales, and thus have the main properties of\n",
      "simple-cell receptive ﬁelds.\n",
      "\n",
      "16.5 Temporal coherence in spatial features\n",
      "353\n",
      "a)\n",
      "b)\n",
      "Fig. 16.11: Simple-cell-like ﬁlters emerge when temporal response strength correlation is opti-\n",
      "mized in natural image sequences. a) Feature weights wk, k = 1,...,120, which maximize temporal\n",
      "response strength correlation (Equation (16.13)); here the nonlinearity g(x) = logcoshx. The fea-\n",
      "tures have been ordered according to Et\n",
      "\b\n",
      "g(sk(t))g(sk(t −∆t))\n",
      "\t\n",
      ", that is, according to their “con-\n",
      "tribution” into the ﬁnal objective value (features with largest values top left). b) A corresponding\n",
      "set of feature vectors ak, k = 1,...,120, from a generative-model-based interpretation of the results\n",
      "(see Equations (16.15) and (16.16)).\n",
      "\n",
      "354\n",
      "16 Temporal sequences of natural images\n",
      "temporal\n",
      "coherence\n",
      "ICA\n",
      "a)\n",
      "0\n",
      "1000\n",
      "2000\n",
      "0.1\n",
      "0.2\n",
      "0.4 0.7\n",
      "peak spatial frequency (c/pixel)\n",
      "occurrences\n",
      "b)\n",
      "0\n",
      "1000\n",
      "2000\n",
      "0.1\n",
      "0.2\n",
      "0.4 0.7\n",
      "peak spatial frequency (c/pixel)\n",
      "occurrences\n",
      "c)\n",
      "0\n",
      "500\n",
      "1000\n",
      "−90 −45\n",
      "0\n",
      "45\n",
      "90\n",
      "peak orientation (degrees)\n",
      "occurrences\n",
      "d)\n",
      "0\n",
      "500\n",
      "1000\n",
      "−90 −45\n",
      "0\n",
      "45\n",
      "90\n",
      "peak orientation (degrees)\n",
      "occurrences\n",
      "e)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "spatial freq. bandwidth (octaves)\n",
      "occurrences\n",
      "f)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "spatial freq. bandwidth (octaves)\n",
      "occurrences\n",
      "g)\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "0\n",
      "1000\n",
      "2000\n",
      "orientation bandwidth (degrees)\n",
      "occurrences\n",
      "h)\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "0\n",
      "1000\n",
      "2000\n",
      "orientation bandwidth (degrees)\n",
      "occurrences\n",
      "Fig. 16.12: Comparison of properties of receptive ﬁelds obtained by optimizing temporal response\n",
      "strength correlation (left column, histograms a, c, e and g) and estimating ICA (right column,\n",
      "histograms b, d, f and h). See text for details.\n",
      "To compare the results obtained with this model against those obtained with ICA,\n",
      "we ran both this algorithm and the symmetric version of FastICA with nonlinear-\n",
      "ity tanh 50 times with different initial values and compared the resulting two sets\n",
      "of 6000 (= 50 × 120) features against each other. The results are shown in Fig-\n",
      "ure 16.12. The measured properties were peak spatial frequency (Figures 16.12a and\n",
      "16.12b, note logarithmic scale, units cycles/pixel), peak orientation (Figures 16.12c\n",
      "and 16.12d), spatial frequency bandwidth (Figures 16.12e and 16.12f), and orienta-\n",
      "tion bandwidth (Figures 16.12g and 16.12h). Peak orientation and peak frequency\n",
      "are simply the orientation and frequency of the highest value in the Fourier power\n",
      "\n",
      "16.5 Temporal coherence in spatial features\n",
      "355\n",
      "spectrum. Bandwidths measure the sharpness of the tuning and were computed from\n",
      "the tuning curve as the full width at at the point were half the maximum response\n",
      "was attained (full width at half maximum, FWHM); this measure is widely used in\n",
      "vision science. See (van Hateren and van der Schaaf, 1998) for more details.\n",
      "Although there are some differences between the two feature sets, the most im-\n",
      "portant observation here is the similarity of the histograms. This supports the idea\n",
      "that ICA / sparse coding and temporal coherence are complementary theories, in\n",
      "that they both result in the emergence of simple-cell-like receptive ﬁelds. As for\n",
      "the differences, the results obtained using temporal response strength correlation\n",
      "have a slightly smaller number of high-frequency receptive ﬁelds. Also, temporal\n",
      "response strength correlation seems to produce receptive ﬁelds that are somewhat\n",
      "more localized with respect to both spatial frequency and orientation.4\n",
      "16.5.5 Why Gabor-like features maximize temporal coherence\n",
      "A simpliﬁed intuitive illustration of why the outputs of Gabor-like feature have such\n",
      "strong energy correlation over time is shown in Figure 16.13. Most transformations\n",
      "of objects in the 3D world result in something similar to local translations of lines\n",
      "and edges in image sequences. This is obvious in the case of 3D translations, and\n",
      "is illustrated in Figure 16.13a) for two other types of transformations: rotation and\n",
      "bending. In the case of a local translation, a suitably oriented simple-cell-like RF re-\n",
      "sponds strongly at consecutive time points, but the sign of the response may change.\n",
      "Note that when the output of a feature detector is considered as a continuous sig-\n",
      "nal, the change of sign implies that the signal reaches zero at some intermediate\n",
      "time point, which can lead to a weak measured correlation. Thus, a better model of\n",
      "the dependencies would be to consider dependencies of variances (Matsuoka et al,\n",
      "1995; Pham and Cardoso, 2001), as in the generative-model interpretation described\n",
      "above. However, for simplicity, we consider here the magnitude that is a crude ap-\n",
      "proximation of the underlying variance.\n",
      "In order to further visualize the correlation of rectiﬁed responses at consecutive\n",
      "time points, we will consider the interaction of features in one dimension (orthog-\n",
      "onal to the orientation of the feature). This allows us to consider the effect of local\n",
      "translations in a simpliﬁed setting. Figure 16.14 illustrates, in a simpliﬁed case, why\n",
      "the temporal response strengths of lines and edges correlate positively as a result of\n",
      "Gabor-like feature structure. Prototypes of two different types of image elements\n",
      "– the proﬁles of a line and an edge – which both have a zero DC component, are\n",
      "shown in the topmost row of the ﬁgure. The leftmost column shows the proﬁles of\n",
      "4 When these results are compared against the results in (van Hateren and van der Schaaf, 1998),\n",
      "the most important difference is the peak at zero bandwidth in Figures 16.12e and 16.12f. This\n",
      "difference is probably a consequence of the fact that no dimensionality reduction, anti-aliasing or\n",
      "noise reduction was performed here, which results in the appearance of very small, checkerboard-\n",
      "like receptive ﬁelds. This effect is more pronounced in ICA, which also explains the stronger peak\n",
      "at the 45◦angle in Figure 16.12d).\n",
      "\n",
      "356\n",
      "16 Temporal sequences of natural images\n",
      "a)\n",
      "t −∆t\n",
      "t\n",
      "t −∆t\n",
      "t\n",
      "b)\n",
      "t−∆t\n",
      "t\n",
      "Fig. 16.13: A simpliﬁed illustration of temporal activity level dependencies of simple-cell-like\n",
      "features when the input consists of image sequences. a) Transformations of objects induce local\n",
      "translations of edges and lines in local regions in image sequences: rotation (left) and bending\n",
      "(right). The solid line shows the position/shape of a line in the image sequence at time t −∆t, and\n",
      "the dotted line shows its new position/shape at time t. The dashed square indicates the sampling\n",
      "window. b) Temporal activity level dependencies: in the case of a local translation of an edge or a\n",
      "line, the response of a simple-cell-like features with a suitable position and orientation is strong at\n",
      "consecutive time points, but the sign may change. The ﬁgure shows a translating line superimposed\n",
      "on an oriented and localized receptive ﬁeld at two different time instances (time t −∆t, solid line,\n",
      "left; time t, dotted line, right).\n",
      "three different features with unit norm and zero DC component: a Gabor-like fea-\n",
      "ture, a sinusoidal (Fourier basis -like) feature, and an impulse feature. The rest of the\n",
      "ﬁgure shows the square rectiﬁed responses of the features to the inputs as functions\n",
      "of spatial displacement of the input.\n",
      "Consider the rectiﬁed response of the Gabor-like feature to the line and the edge,\n",
      "that is, the ﬁrst row of responses in Figure 16.14. The squared response at time t −∆t\n",
      "(spatial displacement zero) is strongly positively correlated with response at time t,\n",
      "even if the line or edge is displaced slightly. This shows how small local transla-\n",
      "tions of basic image elements still yield large values of temporal response strength\n",
      "correlation for Gabor-like features. If you compare the responses of the Gabor-like\n",
      "feature to the responses of the sinusoidal feature – that is, the second row of re-\n",
      "sponses in Figure 16.14 – you can see that the responses to the sinusoidal feature\n",
      "are typically much smaller. This leads to a lower value of our measure of temporal\n",
      "response strength correlation that emphasizes large values. Also, in the third row of\n",
      "responses in Figure 16.14 we can see that while the response of an impulse feature\n",
      "to an edge correlates quite strongly over small spatial displacements, when the input\n",
      "consists of a line even a very small displacement will take the correlation to almost\n",
      "zero.\n",
      "Thus we can see that when considering three important classes of features –\n",
      "features which are maximally localized in space, maximally localized in frequency,\n",
      "or localized in both – the optimal feature is a Gabor-like feature, which is localized\n",
      "both in space and in frequency. If the feature is maximally localized in space, it fails\n",
      "to respond over small spatial displacements of very localized image elements. If the\n",
      "\n",
      "16.5 Temporal coherence in spatial features\n",
      "357\n",
      "input\n",
      "−2−1 0 1 2\n",
      "−2−1 0 1 2 3\n",
      "0 1\n",
      "−2−1 0 1 2 3\n",
      "ﬁlter\n",
      "square-rectiﬁed output\n",
      "−1 0 1\n",
      "−2−1 0 1 2 3\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−2−1 0 1 2 3\n",
      "−2−1 0 1 2 3\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−2−1 0 1 2\n",
      "−2−1 0 1 2 3\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "Fig. 16.14: A simpliﬁed illustration of why a Gabor-like feature, localized in both space and fre-\n",
      "quency, yields larger values of temporal response strength correlation than a feature localized only\n",
      "in space or only in frequency. Top row: cross sections of a line (left) and an edge (right) as func-\n",
      "tions of spatial position. Leftmost column: cross sections of three features with unit norm and zero\n",
      "DC component – a Gabor-like feature (top), a sinusoidal feature (middle), and an impulse feature\n",
      "(bottom). The other plots in the ﬁgure show the responses of the feature detectors to the inputs as\n",
      "a function of spatial displacement of the input. The Gabor-like feature yields fairly large positively\n",
      "correlated values for both types of input. The sinusoidal feature yields small response values. The\n",
      "impulse feature yields fairly large positively correlated values when the input consists of an edge,\n",
      "but when the input consists of a line even a small displacement yields a correlation of almost zero.\n",
      "the feature is maximally localized in frequency, its responses to the localized image\n",
      "features are not strong enough.\n",
      "Figure 16.15 shows why we need nonlinear correlations instead of linear ones:\n",
      "raw output values might correlate either positively or negatively, depending on the\n",
      "displacement. Thus we see why ordinary linear correlation is not maximized for\n",
      "Gabor-like features, whereas the rectiﬁed (nonlinear) correlation is.\n",
      "\n",
      "358\n",
      "16 Temporal sequences of natural images\n",
      "input\n",
      "−2−1 0 1 2\n",
      "−2−1 0 1 2 3\n",
      "0 1\n",
      "−2−1 0 1 2 3\n",
      "ﬁlter\n",
      "raw output\n",
      "−1 0 1\n",
      "−2−1 0 1 2 3\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "−1 0 1\n",
      "Fig. 16.15: A simpliﬁed illustration of why nonlinear correlation is needed for the emergence of\n",
      "the phenomenon. Raw response values of the Gabor-like feature to the line and edge may correlate\n",
      "positively or negatively, depending on the displacement. (See Figure 16.14 for an explanation of\n",
      "the layout of the ﬁgure.)\n",
      "16.5.6 Control experiments\n",
      "To validate the novelty of the results obtained with this model when compared with\n",
      "ICA and sparse coding, and to examine the effect of different factors in the results, a\n",
      "number of control experiments were made. These experiments are summarized here,\n",
      "details can be found in (Hurri and Hyv¨arinen, 2003a). The control experiments show\n",
      "that\n",
      "• the results are qualitatively similar when the static part of the video is removed\n",
      "altogether by employing Gram-Schmidt orthogonalization, which strengthens the\n",
      "novelty of this model when compared with static models\n",
      "• the results are qualitatively similar when no temporal decorrelation is performed\n",
      "• the results are qualitatively similar when ∆t = 120ms; when ∆is further in-\n",
      "creased to ∆t = 480ms and ∆t = 960ms, the resulting features start to lose their\n",
      "spatial localization and gradually also their orientation selectivity; ﬁnally, when\n",
      "the consecutive windows have to temporal relationship (consecutive windows\n",
      "chosen randomly), the resulting features correspond to noise patterns\n",
      "• the results are qualitatively similar when observer (camera) movement is com-\n",
      "pensated by a tracking mechanism in video sampling.\n",
      "Finally, one further control experiment was made in which the linear correlation\n",
      "fℓ(wk) = Et\n",
      "\b\n",
      "sk(t)sk(t −∆t)\n",
      "\t\n",
      "was maximized. The unit variance constraint is used\n",
      "here again, so the problem is equivalent to minimizing Et\n",
      "n\u0000sk(t)−sk(t −∆t)\n",
      "\u00012o\n",
      "with the same constraint; we will return to this objective function below in Sec-\n",
      "tion 16.8. The resulting features resemble Fourier basis vectors, and not simple-cell\n",
      "receptive ﬁelds. This shows that nonlinear, higher-ordercorrelation is indeed needed\n",
      "for the emergence of simple-cell-like features.\n",
      "\n",
      "16.6 Spatiotemporal energy correlations in linear features\n",
      "359\n",
      "16.6 Spatiotemporal energy correlations in linear features\n",
      "16.6.1 Deﬁnition of the model\n",
      "Temporal response strength correlation, deﬁned in Equation (16.13) on page 350,\n",
      "maximizes the “temporal coherence” in the outputs of individual simple cells. Note\n",
      "that in terms of the generative model described above, the objective functions says\n",
      "nothing about the interdependencies in different sk(t)’s – that is, different cells.\n",
      "Thus, there is an implicit assumption of independence in the model, at least if it\n",
      "is interpreted as a probabilistic generative model. In this section we add another\n",
      "layer to the generative model to extend the theory to simple-cell interactions, and to\n",
      "the level of complex cells.5\n",
      "×\n",
      "abs(s(t))\n",
      "s(t)\n",
      "sign generation\n",
      "P\n",
      "\u0000sk(t) > 0\n",
      "\f\f\f\fsk(t−∆t) > 0\n",
      "\u0001\n",
      "= Pret\n",
      "x(t) = As(t)\n",
      "v(t)\n",
      "abs(s(t)) = Mabs(s(t−∆t))+v(t)\n",
      "x(t)\n",
      "Fig. 16.16: The two layers of the generative model. Let abs(s(t)) = [|s1(t)|···|sK(t)|]T denote\n",
      "the amplitudes of simple cell responses. In the ﬁrst layer, the driving noise signal v(t) generates\n",
      "the amplitudes of simple cell responses via an autoregressive model. The signs of the responses\n",
      "are generated randomly between the ﬁrst and second layer to yield signed responses s(t). In the\n",
      "second layer, natural video x(t) is generated linearly from simple cell responses. In addition to\n",
      "the relations shown here, the generation of v(t) is affected by Mabs(s(t −∆t)) to ensure non-\n",
      "negativity of abs(s(t)). See text for details.\n",
      "Like in the many generative models discussed in this book, the output layer of\n",
      "the new model (see Figure 16.16) is linear, and maps cell responses s(t) to image\n",
      "features x(t), but we do not assume that the components of s(t) are independent.\n",
      "Instead, we model the temporal dependencies between these components in the ﬁrst\n",
      "layer of our model. Let abs(s(t)) = [|s1(t)|···|sK(t)|]T denote the activities of the\n",
      "cells, and let v(t) denote a driving noise signal and M denote a K × K matrix; the\n",
      "modelled interdependencies will be “coded” in M.. Our model is a multidimensional\n",
      "ﬁrst-order autoregressive process, deﬁned by\n",
      "abs(s(t)) = Mabs(s(t −∆t))+v(t).\n",
      "(16.17)\n",
      "Again, we also need to ﬁx the scale of the latent variables by deﬁning Et\n",
      "n\n",
      "s\n",
      "2\n",
      "k(t)\n",
      "o\n",
      "= 1\n",
      "for k = 1,...,K.\n",
      "5 This section is based on the article (Hurri and Hyv¨arinen, 2003b) originally published in Network:\n",
      "Computation in Neural Systems. Copyright c⃝2003 Institute of Physics, used with permission.\n",
      "\n",
      "360\n",
      "16 Temporal sequences of natural images\n",
      "There are dependencies between the driving noise v(t) and output strengths\n",
      "abs(s(t)), caused by the non-negativity of abs(s(t)). To take these dependencies\n",
      "into account, we use the following formalism. Let u(t) denote a random vector with\n",
      "components which are statistically independent of each other. To ensure the non-\n",
      "negativity of abs(s(t)), we deﬁne\n",
      "v(t) = max(−Mabs(s(t −∆t)),u(t)),\n",
      "(16.18)\n",
      "where, for vectors a and b, max(a,b) = [max(a1,b1) ··· max(an,bn)]T . We assume\n",
      "that u(t) and abs(s(t)) are uncorrelated. The point in this deﬁnition is to make sure\n",
      "that the noise does not drive the absolute values of the sK(t) negative, which would\n",
      "be absurd.\n",
      "To make the generative model complete, a mechanism for generating the signs\n",
      "of cell responses s(t) must be included. We specify that the signs are generated ran-\n",
      "domly with equal probability for plus or minus after the strengths of the responses\n",
      "have been generated. Note that one consequence of this is that the different sk(t)’s\n",
      "are uncorrelated. In the estimation of the model this uncorrelatedness property is\n",
      "used as a constraint. When this is combined with the unit variance (scale) constraints\n",
      "described above, the resulting set of constraints is the same as in the approach de-\n",
      "scribed above in Section 16.5 (page 349).\n",
      "In Equation (16.17), a large positive matrix element M(i, j), or M(j,i), indicates\n",
      "that there is strong temporal dependency between the output strengths of cells i and\n",
      "j. Thinking in terms of grouping temporally coherent cells together, matrix M can\n",
      "be thought of as containing similarities (reciprocals of distances) between different\n",
      "cells. We will use this property below to derive a topography of simple-cell receptive\n",
      "ﬁelds from M.\n",
      "16.6.2 Estimation of the model\n",
      "To estimate the model deﬁned above we need to estimate both M and A. Instead of\n",
      "estimating A directly, we estimate W which maps image sequence data to responses\n",
      "s(t) = Wx(t),\n",
      "(16.19)\n",
      "and use the pseudo-inverse relationship – that is, Equation (16.16) on page 351 – to\n",
      "compute A. In what follows, we ﬁrst show how to estimate M, given W. We then\n",
      "describe an objective function which can be used to estimate W, given M. Each\n",
      "iteration of the estimation algorithm consists of two steps. During the ﬁrst step M is\n",
      "updated, and W is kept constant; during the second step these roles are reversed.\n",
      "First, regarding the estimation of M, consider a situation in which W is kept\n",
      "constant. It can be shown (Hurri and Hyv¨arinen, 2003b) that M can be estimated by\n",
      "using approximative method of moments, and that the estimate is given by\n",
      "\n",
      "16.6 Spatiotemporal energy correlations in linear features\n",
      "361\n",
      "M ≈βEt\n",
      "n\n",
      "(abs(s(t))−Et {abs(s(t))})(abs(s(t −∆t))−Et {abs(s(t))})To\n",
      "×Et\n",
      "n\n",
      "(abs(s(t))−Et {abs(s(t))})(abs(s(t)) −Et {abs(s(t))})To−1\n",
      ",\n",
      "where β > 1. Since this multiplier does not change the relative strengths of the\n",
      "elements of M, and since it has a constant linear effect in the objective function of\n",
      "W given below, its value does not affect the optimal W, so we can simply set β = 1\n",
      "in the optimization. The resulting estimator for M is the same as the optimal least\n",
      "mean squares linear predictor in the case of unconstrained v(t).\n",
      "The estimation of W is more complicated. A rigorous derivation of an objec-\n",
      "tive function based on well-known estimation principles is very difﬁcult, because\n",
      "the statistics involved are non-gaussian, and the processes have difﬁcult interdepen-\n",
      "dencies. Therefore, instead of deriving an objective function from ﬁrst principles,\n",
      "we derived an objective function heuristically (Hurri and Hyv¨arinen, 2003b), and\n",
      "veriﬁed through simulations that the objective function is capable of estimating the\n",
      "two-layer model. The objective function is a weighted sum of the covariances of\n",
      "feature output strengths at times t −∆t and t, deﬁned by\n",
      "f(W,M) =\n",
      "K\n",
      "∑\n",
      "i=1\n",
      "K\n",
      "∑\n",
      "j=1\n",
      "M(i, j)cov\n",
      "\b\n",
      "|si(t)|,\n",
      "\f\fsj(t −∆t)\n",
      "\f\f\t\n",
      ".\n",
      "(16.20)\n",
      "In the actual estimation algorithm, W is updated by employing a gradient projection\n",
      "approach to the optimization of f in Equation (16.20) under the constraints. The\n",
      "initial value of W is selected randomly.\n",
      "The fact that the algorithm described above is able to estimate the two-layer\n",
      "model has been veriﬁed through extensive simulations. These simulations show that\n",
      "matrix W can be estimated fairly reliably, and that the relative error of the estimate\n",
      "of matrix M also decreases reliably in the estimation, but the remaining error for M\n",
      "is larger than in the case of matrix W. This difference is probably due to the approx-\n",
      "imation made in the estimation of M; see (Hurri and Hyv¨arinen, 2003b). However,\n",
      "the simulations suggest that the error in the estimate of M is largely due to a sys-\n",
      "tematic, monotonic, nonlinear element-wise bias, which does not affect greatly our\n",
      "interpretation of the elements of M, since we are mostly interested in their relative\n",
      "magnitudes. See (Hurri and Hyv¨arinen, 2003b) for details. A very closely related\n",
      "model which can be analyzed in detail is in (Hyv¨arinen and Hurri, 2004), which\n",
      "shows that a rigorous justiﬁcation for our objective function above can be found in\n",
      "the case where we use the quadratic function instead of the absolute value function.\n",
      "See also (Valpola et al, 2003) for related theoretical work.\n",
      "16.6.3 Experiments on natural images\n",
      "The estimation algorithm was run on the same data set as for the basic temporal\n",
      "coherence model in Section 16.5 to obtain estimates for M and A. Figure 16.17\n",
      "\n",
      "362\n",
      "16 Temporal sequences of natural images\n",
      "Fig. 16.17: The estimation of the two-layer generative model from natural visual stimuli results\n",
      "in the emergence of localized, oriented receptive ﬁelds with multiple scales. The feature vectors\n",
      "(columns of A) shown here are in no particular order.\n",
      "shows the resulting feature vectors – that is, columns of A. As can be seen, the\n",
      "resulting features are localized, oriented, and have multiple scales, thereby fulﬁlling\n",
      "the most important deﬁning criteria of simple-cell receptive ﬁelds. This suggests\n",
      "that, as far as receptive-ﬁeld structure is concerned, the method yields receptive\n",
      "ﬁelds with similar qualitative properties to those obtained with sparse coding, ICA,\n",
      "or temporal response strength correlation.\n",
      "What is truly novel in this model is the estimation of matrix M, which captures\n",
      "the temporal and spatiotemporal activity-level dependencies between the feature\n",
      "vectors shown in Figure 16.17. The extracted matrices A and M can be visual-\n",
      "ized simultaneously by using the interpretation of M as a similarity matrix (see\n",
      "page 360). Figure 16.18 illustrates the feature vectors – that is, columns of A – laid\n",
      "out at spatial coordinates derived from M in a way explained below. The resulting\n",
      "feature vectors are again oriented, localized and multiscale, as in the basic temporal\n",
      "coherence model in Section 16.5.\n",
      "In the resulting planar representation shown in Figure 16.18, the temporal coher-\n",
      "ence between the outputs of two cells i and j is reﬂected in the distance between\n",
      "the corresponding receptive ﬁelds: the larger the elements M(i, j) and M(j,i) are,\n",
      "the closer the receptive ﬁelds are to each other. We can see that local topography\n",
      "emerges in the results: those basis vectors which are close to each other seem to be\n",
      "mostly coding for similarly oriented features at nearby spatial positions. This kind\n",
      "of grouping is characteristic of pooling of simple cell outputs at complex cell level\n",
      "(Palmer, 1999). Some global topography also emerges: those basis vectors which\n",
      "\n",
      "16.6 Spatiotemporal energy correlations in linear features\n",
      "363\n",
      "Fig. 16.18: Results of estimating the two-layer generative model from natural image sequences.\n",
      "Features (columns of A) plotted at spatial coordinates given by applying multidimensional scal-\n",
      "ing to M. Matrix M was ﬁrst converted to a non-negative similarity matrix Ms by subtracting\n",
      "mini, j M(i, j) from each of its elements, and by setting each of the diagonal elements at value 1.\n",
      "Multidimensional scaling was then applied to Ms by interpreting entries Ms(i, j) and Ms(j,i) as\n",
      "similarity measures between cells i and j. Some of the resulting coordinates were very close to\n",
      "each other, so tight cell clusters were magniﬁed for purposes of visual display. Details are given in\n",
      "(Hurri and Hyv¨arinen, 2003b).\n",
      "code for horizontal features are on the left in the ﬁgure, while those that code for\n",
      "vertical features are on the right.\n",
      "Thus, the estimation of our two-layer model from natural image sequences yields\n",
      "both simple-cell-like receptive ﬁelds, and grouping similar to the pooling of simple\n",
      "cell outputs. Linear receptive ﬁelds emerge in the second layer (matrix A), and cell\n",
      "output grouping emerges in the ﬁrst layer (matrix M). Both of these layers are es-\n",
      "timated simultaneously. This is an important property when compared with other\n",
      "\n",
      "364\n",
      "16 Temporal sequences of natural images\n",
      "statistical models of early vision, because no a priori ﬁxing of either of these lay-\n",
      "ers is needed. The results thus compare with the two-layer models for static images\n",
      "discussed in Section 11.8 and (K¨oster and Hyv¨arinen, 2007, 2008). The main differ-\n",
      "ence is that here, M describes “lateral” interactions between the features sk, whereas\n",
      "in Section 11.8 considered another stage in hierarchical processing.\n",
      "16.6.4 Intuitive explanation of results\n",
      "The results shown in Figure 16.18 suggest that features which prefer similar orien-\n",
      "tation but different spatial location have spatiotemporal activity dependencies. Why\n",
      "is this the case?\n",
      "temporal\n",
      "spatial\n",
      "spatiotemporal\n",
      "cell 1\n",
      "cell 2\n",
      "t −∆t\n",
      "t\n",
      "Fig. 16.19: A simpliﬁed illustration of static and short-time activity-level dependencies of simple-\n",
      "cell-like receptive ﬁelds. For a translating edge or line, the responses of two similar receptive ﬁelds\n",
      "with slightly different positions (cell 1, top row; cell 2, bottom row) are large at nearby time in-\n",
      "stances (time t −∆t, solid line, left column; time t, dotted line, right column). Each subﬁgure\n",
      "shows the translating line superimposed on a receptive ﬁeld. The magnitudes of the responses of\n",
      "both cells are large at both time instances. This introduces three types of activity-level depen-\n",
      "dencies: temporal (in the output of a single cell at nearby time instances), spatial (between two\n",
      "different cells at a single time instance) and spatiotemporal (between two different cells at nearby\n",
      "time instances). The multivariate autoregressive model discussed in this section includes temporal\n",
      "and spatiotemporal activity-level dependencies (marked with solid lines). Spatial activity-level de-\n",
      "pendency (dashed line) is an example of energy dependencies modelled in work on static images\n",
      "in Chapters 9–11.\n",
      "Temporal activity-level dependencies, illustrated in Figure 16.13, are not the only\n",
      "type of activity-level dependencies in a set of simple-cell-like features. Figure 16.19\n",
      "illustrates how two different cells with similar receptive ﬁeld proﬁles – having the\n",
      "same orientation but slightly different positions – respond at consecutive time in-\n",
      "\n",
      "16.7 Unifying model of spatiotemporal dependencies\n",
      "365\n",
      "stances when the input is a translating line. The receptive ﬁelds are otherwise iden-\n",
      "tical, except that one is a slightly translated version of the other. It can be seen that\n",
      "both cells are highly active at both time instances, but again, the signs of the outputs\n",
      "vary. This means that in addition to temporal activity dependencies (the activity of\n",
      "a cell is large at time t −∆t and time t), there are two other kinds of activity-level\n",
      "dependencies.\n",
      "spatial (static) dependencies\n",
      "Both cells are highly active at a single time instance.\n",
      "This kind of dependency is an example of the energy dependencies earlier mod-\n",
      "elled in static images in Chapters 9–11.\n",
      "spatiotemporal dependencies\n",
      "The activity levels of different cells are also related\n",
      "over time. For example, the activity of cell 1 at time t −∆t is related to the activity\n",
      "of cell 2 at time t.\n",
      "What makes these dependencies important is that they seem to be reﬂected in\n",
      "the structure of the topography in the primary visual cortex. The results presented\n",
      "in this section suggest that combining temporal activity level dependencies with\n",
      "spatiotemporal dependencies yields both simple-cell-like receptive ﬁelds and a set\n",
      "of connections between these receptive ﬁelds. These connections can be related to\n",
      "both the way in which complex cells seem to pool simple-cell outputs, and to the\n",
      "topographic organization observed in the primary visual cortex, in the same way\n",
      "as described in Chapter 11. Therefore, the principle of activity level dependencies\n",
      "seems to explain both receptive ﬁeld structure and their organization.\n",
      "16.7 Unifying model of spatiotemporal dependencies\n",
      "In order to motivate the development of a model which uniﬁes a number of statistical\n",
      "properties in natural image sequences, let us summarize the key results on proba-\n",
      "bilistic modelling of the properties of the neural representation at the simple-cell\n",
      "level.\n",
      "1. Results obtained using sparse coding / independent component analysis suggest\n",
      "that, on the average, at a single time instant relatively few simple cells are active\n",
      "on the cortex (see Chapter 6); furthermore, each cell is active only rarely.\n",
      "2. In this chapter, we have described a complementary model, which suggests that\n",
      "simple cells tend to be highly active at consecutive time instants – that is, their\n",
      "outputs are burst-like (see Section 16.5).\n",
      "3. Models on static dependencies between simple-cell-like features, and the rela-\n",
      "tionship between these dependencies and cortical topography, suggest that the\n",
      "active cells tend to be located close to each other on the cortex, as in Chapter 11.\n",
      "4. As we saw in the preceding section, temporal correlations also lead to topo-\n",
      "graphic properties resembling cortical topography, based on a model which uti-\n",
      "lizes temporal correlations between the outputs of different features.\n",
      "These four different principles – sparseness, temporal coherence of activity levels,\n",
      "spatial activity level dependencies, and spatiotemporal activity level dependencies –\n",
      "\n",
      "366\n",
      "16 Temporal sequences of natural images\n",
      "are not conﬂicting. That is, none of the principles excludes the existence of another.\n",
      "Perhaps, then, each of these models offers just a limited view to a more complete\n",
      "model of cortical coding at the simple-cell level. In fact, the following description\n",
      "of simple-cell activation is in accordance with all of the principles: when an animal\n",
      "is viewing a natural scene, a relatively small number of patches of cortical area are\n",
      "highly active in the primary visual cortex, and the activity in these areas tends to be\n",
      "sustained for a while. That is, activity is sparse, and contiguous both in space and\n",
      "time. This is the bubble coding model (Hyv¨arinen et al, 2003).\n",
      "In the bubble coding model, the ﬁnal generative mapping from latent components\n",
      "to natural image sequence data is linear, like in the previous sections: x(t) = As(t).\n",
      "The main idea in the bubble coding model is generation of the s(t) so that they\n",
      "have bubble-like activity. This is accomplished by introducing a bubble-like vari-\n",
      "ance signal for s(t), as illustrated by an example in Figure 16.20. The spatiotem-\n",
      "poral locations of the variance bubbles are determined by a sparse process u(t)\n",
      "(Figure 16.20a). A temporal ﬁlter φ and spatial pooling function h, both of which\n",
      "are ﬁxed a priori in the model, spread the variance bubbles temporally and spa-\n",
      "tially (Figures 16.20b and c). The resulting variance bubbles can also overlap each\n",
      "other, in which case the variance in the overlapping area is obtained as a sum of\n",
      "the variances in each bubble; in Figure 16.20, however, the variance bubbles are\n",
      "non-overlapping for illustrative purposes. It is also possible that at this point a ﬁxed\n",
      "static nonlinearity f is applied to rescale the magnitudes of the variance bubbles.\n",
      "These steps yield the variance signals\n",
      "vk(t) = f\n",
      "\u0012\n",
      "∑\n",
      "ℓ\n",
      "h(k,ℓ)[φ(t)∗uℓ(t)]\n",
      "\u0013\n",
      ".\n",
      "(16.21)\n",
      "where ∗denotes temporal convolution. The burst-like oscillating nature of the com-\n",
      "ponents inside the bubbles is introduced through a gaussian temporally uncorrelated\n",
      "(white noise) process z(t) (Figure 16.20d). Thus, the components sk(t) are gener-\n",
      "ated from the variance bubbles and the noise signals by multiplying the two together\n",
      "(Figure 16.20e):\n",
      "sk(t) = vk(t)zk(t).\n",
      "(16.22)\n",
      "Note that all three different types of activity level dependencies – temporal, spatial,\n",
      "and spatiotemporal (see Figure 16.19 on page 364) – are present in the bubble-\n",
      "coding model, as well as sparseness. To complete this generative model, the sk(t)\n",
      "are ﬁnally linearly transformed to the image using a linear transformation, as in\n",
      "almost all models in this book.\n",
      "An approximative maximum likelihood scheme can be used to estimate the bub-\n",
      "ble coding model; details can be found in (Hyv¨arinen et al, 2003). Note that because\n",
      "the pooling function h is ﬁxed, it enforces the spatial pooling, while in the two-layer\n",
      "model described in the previous section, this pooling was learned from the data. The\n",
      "temporal smoothing (low-pass) ﬁlter φ is also ﬁxed in the model.\n",
      "Figure 16.21 shows the resulting spatial basis vectors, obtained when the bubble\n",
      "coding model was estimated from natural image sequence data. The basis consists\n",
      "of simple-cell-like linear receptive-ﬁeld models, similar to those obtained by topo-\n",
      "\n",
      "16.8 Features with minimal average temporal change\n",
      "367\n",
      "graphic ICA from static images (Figure 11.4 on page 258), or using the temporal\n",
      "models in Section 16.6. The orientation and the location of the feature coded by\n",
      "the vectors change smoothly when moving on the topographic grid. Low-frequency\n",
      "basis vectors are spatially segregated from the other vectors, so there also seems to\n",
      "be some ordering based on preferred spatial frequency. Such an organization with\n",
      "respect to orientation, location, and spatial frequency is similar to the topographic\n",
      "ordering of simple cells in the primary visual cortex, as was discussed in Chapter 11.\n",
      "One can also estimate spatiotemporal features with this model. An animated ex-\n",
      "ample\n",
      "of\n",
      "the\n",
      "resulting\n",
      "spatiotemporal\n",
      "features\n",
      "can\n",
      "be\n",
      "found\n",
      "at\n",
      "www.cs.helsinki.fi/group/nis/animations/bubbleanimation.gif.\n",
      "The features obtained by the bubble coding models are thus hardly any different\n",
      "from what were obtained by the topographic ICA model, for example. The signiﬁ-\n",
      "cance of the model is mainly theoretical in the sense that it gives a uniﬁed framework\n",
      "for understanding the different models involved.\n",
      "16.8 Features with minimal average temporal change\n",
      "16.8.1 Slow feature analysis\n",
      "16.8.1.1 Motivation and history\n",
      "All of the models of temporal coherence discussed above – temporal response\n",
      "strength correlation, the two-layer autoregressive model, and the bubble-coding\n",
      "model – are based on temporal patterns in output energies (variances) s2(t). What\n",
      "happens if we just minimize a measure of the temporal change in the outputs of the\n",
      "model neurons? That is, if s(t) is the output of a model at time t, and w is our vector\n",
      "of model parameters, we could for example minimize the squared difference of the\n",
      "output at close-by time points\n",
      "fSFA(w) = Et\n",
      "n\n",
      "(s(t)−s(t −∆t))2o\n",
      ".\n",
      "(16.23)\n",
      "An explicit formalization of this principle was given by Mitchison (1991) who sim-\n",
      "ply described it as “removal of time variation” (see also (Hinton, 1989; F¨oldi´ak,\n",
      "1991; Stone, 1996)). The principle was also used in blind source separation, in\n",
      "which a number of sophisticated methods for its algorithmic implementation have\n",
      "been developed (Tong et al, 1991; Belouchrani et al, 1997; Hyv¨arinen et al, 2001b).\n",
      "Recently the principle has been given the name slow feature analysis (Wiskott and\n",
      "Sejnowski, 2002), thus the subscript SFA in the deﬁnition of the objective function.\n",
      "In order to relate the SFA objective function to the models we have discussed\n",
      "above, let us analyze it in more detail. Expanding the square and taking the expec-\n",
      "tations of the resulting terms we get\n",
      "\n",
      "368\n",
      "16 Temporal sequences of natural images\n",
      "a)\n",
      "signal index k →\n",
      "uk(t)\n",
      "time t →\n",
      "b)\n",
      "signal index k →\n",
      "ξk(t) = φ(t)∗uk(t)\n",
      "time t →\n",
      "c)\n",
      "signal index k →\n",
      "vk(t) = f (∑ℓh(k,ℓ)ξℓ(t))\n",
      "time t →\n",
      "d)\n",
      "signal index k →\n",
      "zk(t)\n",
      "time t →\n",
      "e)\n",
      "signal index k →\n",
      "sk(t) = vk(t)zk(t)\n",
      "time t →\n",
      "\n",
      "16.8 Features with minimal average temporal change\n",
      "369\n",
      "fSFA(w) = Et\n",
      "\b\n",
      "s2(t)−2s(t)s(t −∆t)+s2(t −∆t)\n",
      "\t\n",
      "= Et\n",
      "\b\n",
      "s2(t)\n",
      "\t\n",
      "−2Et {s(t)s(t −∆t)}+Et\n",
      "\b\n",
      "s2(t −∆t)\n",
      "\t\n",
      "|\n",
      "{z\n",
      "}\n",
      "=Et{s2(t)}\n",
      "= 2\n",
      "\u0000Et\n",
      "\b\n",
      "s2(t)\n",
      "\t\n",
      "−Et {s(t)s(t −∆t)}\n",
      "\u0001\n",
      "(16.24)\n",
      "The objective function is non-negative,and a trivial way to minimize it is to compute\n",
      "a zero output s(t) = 0 for all t. A standard way to avoid this anomaly is to constrain\n",
      "the “energy” (second moment) of the output signal to unity, that is, deﬁne constraint\n",
      "Et\n",
      "\b\n",
      "s2(t)\n",
      "\t\n",
      "= 1,\n",
      "(16.25)\n",
      "in which case the objective function become simpler:\n",
      "fSFA(w) = 2(1 −Et {s(t)s(t −∆t)}),\n",
      "(16.26)\n",
      "which shows that under the unit energy constraint, SFA is equivalent to maximiza-\n",
      "tion of the linear temporal correlation in the output. This is in contrast to the model\n",
      "in Section 16.5 (page 349), which was based on maximization of nonlinear tem-\n",
      "poral correlation. Also note that if mean output is zero, that is, if Et {s(t)} = 0,\n",
      "then the unit energy constraint is equivalent to the unit variance constraint, since\n",
      "var(s(t)) = Et\n",
      "\b\n",
      "s2(t)\n",
      "\t\n",
      "−(Et {s(t)})2 .\n",
      "16.8.1.2 SFA in a linear neuron model\n",
      "In Section 16.5 (page 349) we mentioned that in a linear neuron model, maximiza-\n",
      "tion of linear temporal correlation results in receptive ﬁelds which resemble fre-\n",
      "quency components and not simple-cell-like receptive ﬁelds or Gabor functions. In\n",
      "Fig. 16.20: (opposite page) Illustration of the generation of components sk(t) in the bubble coding\n",
      "model. For simplicity of illustration, we use a one-dimensional topography instead of the more\n",
      "conventional two-dimensional one. a) The starting point is the set of sparse signals uk(t). b) Each\n",
      "sparse signal uk(t) is ﬁltered with a temporal low-pass ﬁlter φ(t), yielding signals φ(t)∗uk(t). In\n",
      "this example, the ﬁlter φ(t) simply spreads the impulses uniformly over an interval. c) In the next\n",
      "step, a neighbourhood function h(k,ℓ) is applied to spread the bubbles spatially; this is like a spatial\n",
      "low-pass ﬁlter. A static nonlinearity f may also be applied at this point to rescale the magnitudes\n",
      "of the variance bubbles. This yields variance bubble signals vk(t) = f\n",
      "\u0000∑ℓh(k,ℓ)[φ(t)∗uℓ(t)]\n",
      "\u0001\n",
      ". In\n",
      "this example, the neighbourhood function h is simply 1 close-by and 0 elsewhere, and the static\n",
      "nonlinearity f is just the identity mapping f (α)= α. d) Next, we generate gaussian temporally un-\n",
      "correlated (white noise) signals zk(t). e) Linear components (responses) are deﬁned as products of\n",
      "the gaussian white noise signals and the spatiotemporally spread bubble signals: sk(t) = zk(t)vk(t).\n",
      "These are transformed linearly by the matrix A to give the observed image data (not shown). (Note\n",
      "that in subﬁgures a)–c), white denotes value zero and black denotes value 1, while in subﬁgures\n",
      "d) and e), medium grey denotes zero, and black and white denote negative and positive values,\n",
      "respectively.)\n",
      "\n",
      "370\n",
      "16 Temporal sequences of natural images\n",
      "Fig. 16.21: A set of spatial features, estimated from natural image using the bubble coding estima-\n",
      "tion method, and laid out at spatial coordinates deﬁned by the topographic grid in the bubble coding\n",
      "model. The topographic organization of the features exhibits ordering with respect to orientation,\n",
      "location, and spatial frequency of the vectors, being very similar to that obtained by topographic\n",
      "ICA.\n",
      "such a linear model, slow feature analysis can be analyzed mathematically in detail.\n",
      "Let s(t) denote the output of the unit at time t :\n",
      "s(t) = wTx(t).\n",
      "(16.27)\n",
      "Assume that the input x(t) has zero mean (Et {x(t)} = 0), and that we impose the\n",
      "unit variance constraint to avoid the trivial solution w = 0. Then the unit energy\n",
      "constraint also holds, and instead of minimizing the SFA objective function (16.26)\n",
      "we can just maximize linear temporal correlation\n",
      "fLTC(w) = Et {s(t)s(t −∆t)} = Et\n",
      "\b\n",
      "wT x(t)x(t −∆t)Tw\n",
      "\t\n",
      "= wTEt\n",
      "\b\n",
      "x(t)x(t −∆t)T\t\n",
      "w\n",
      "(16.28)\n",
      "\n",
      "16.8 Features with minimal average temporal change\n",
      "371\n",
      "with the constraint\n",
      "Et\n",
      "\b\n",
      "s2(t)\n",
      "\t\n",
      "= 1 ⇔wTEt\n",
      "\b\n",
      "x(t)x(t)T\t\n",
      "w = 1\n",
      "(16.29)\n",
      "A solution can be derived by adapting the mathematics of PCA described in Sec-\n",
      "tion 5.8.1. The connection becomes clear if we ﬁrst whiten the data x(t) (spatially,\n",
      "i.e. in the same way as in Chapter 5, ignoring the temporal dependencies). For sim-\n",
      "plicity, we denote the whitened data by x(t) in the following. For the whitened data,\n",
      "the constraint of unit variance is equivalent to the constraint that w has unit norm,\n",
      "because Et\n",
      "\b\n",
      "x(t)x(t)T\t\n",
      "is the identity matrix.\n",
      "Thus, we have a maximization of a quadratic function under unit norm constraint,\n",
      "just as in PCA. There is a small difference, though: The matrix Et\n",
      "\b\n",
      "x(t)x(t −∆t)T\t\n",
      "deﬁning the quadratic function is not necessarily symmetric. Fortunately, it is not\n",
      "difﬁcult to prove that actually, this maximization is equivalent to maximization using\n",
      "a symmetric version of the matrix:\n",
      "fLTC(w) = wT\n",
      "\u00141\n",
      "2Et\n",
      "\b\n",
      "x(t)x(t −∆t)T\t\n",
      "+ 1\n",
      "2Et\n",
      "\b\n",
      "x(t −∆t)x(t)T\t\u0015\n",
      "w\n",
      "(16.30)\n",
      "Thus, the same principle of computing the eigenvalue decomposition applies here\n",
      "(Tong et al, 1991). The optimal vector w is obtained as the one corresponding to the\n",
      "largest eigenvalue of this matrix. If we want to extract a set of RF’s, we can use the\n",
      "following result: assuming that the output of the next selected RF has to be uncor-\n",
      "related with the outputs of the previously selected ones, then the next maximum is\n",
      "the eigenvector with the next largest eigenvalue.\n",
      "Figure 16.22 shows the ﬁlters that result from such optimization in a linear neu-\n",
      "ron model. The data set and preprocessing in this experiment was identical to the\n",
      "one in Section 16.5. As can be seen, the resulting ﬁlters correspond to frequency\n",
      "(Fourier) features, and not the localized RF’s in the early visual system.\n",
      "16.8.2 Quadratic slow feature analysis\n",
      "As shown above, SFA in the case of a linear neuron model does not produce very\n",
      "interesting results. In contrast, application of the principle in the nonlinear case has\n",
      "proven more promising, although results for real natural image sequences have not\n",
      "been reported.\n",
      "A straightforward and computationally simple way to design nonlinear models\n",
      "is the basis expansion approach. As a simple example, assume that out original\n",
      "input data is a single scalar x. This data can be expanded by computing the square\n",
      "of the data point x2. We can then design a model that is linear in the parameters\n",
      "a = (a1 a2)T\n",
      "y = a1x+a2x2 = aT\n",
      "\u0012 x\n",
      "x2\n",
      "\u0013\n",
      "(16.31)\n",
      "\n",
      "372\n",
      "16 Temporal sequences of natural images\n",
      "Fig. 16.22: The set of ﬁlters which optimize the objective function in slow feature analysis from\n",
      "natural image data in the case of a linear neuron model. As can be seen, the resulting ﬁlters do not\n",
      "resemble localized receptive ﬁelds of either retina/LGN or V1.\n",
      "but obviously nonlinear in the data x (here, it is quadratic). A very nice property\n",
      "of this approach is that the analysis developed for the linear case is immediately\n",
      "applicable: all we need to do is to replace the original data with the expanded data.\n",
      "In the SFA case, let f(x(t)) = [f1(x(t)) f2(x(t)) ··· fM(x(t))]T denote a nonlinear\n",
      "expansion of the data. Then the output is\n",
      "s(t) = wTf(x(t)),\n",
      "(16.32)\n",
      "and all of the optimization results apply after x(t) is replaced with f(x(t)).\n",
      "The form of basis expansion we are particularly interested in is that of a quadratic\n",
      "basis. Let x(t) = [x1(t) x2(t) ··· xK(t)]T denote our original data. Then a data vector\n",
      "in our quadratic data set f(x(t)) includes, in addition to the original components\n",
      "x1(t),x2(t),...,xK(t), the products of all pairs of components xk(t)xℓ(t), k = 1,...,K,\n",
      "ℓ= 1,...,K; note that this also includes the squares x2\n",
      "1(t),x2\n",
      "2(t),...,x2\n",
      "K(t).\n",
      "\n",
      "16.8 Features with minimal average temporal change\n",
      "373\n",
      "While the computation of the optimum in the case of basis-expanded SFA is\n",
      "straightforward, the interpretation of the results is more difﬁcult: unlike in the case\n",
      "of linear data, the obtained parameters can not simply be interpreted as a template\n",
      "of weights at different positions in the image, because some of the parameters corre-\n",
      "spond to the quadratic terms xk(t)xℓ(t), k = 1,...,K, ℓ= 1,...,K. One way to analyze\n",
      "the learned parameter vectors w1,...,wM is to compute the input images that elicit\n",
      "maximal and minimal responses, while constraining the norm (energy) of the im-\n",
      "ages to make the problem well-posed; that is, if c > 0 is a constant\n",
      "xmax = argmax∥x∥=c wTf(x)\n",
      "(16.33)\n",
      "xmin = argmin∥x∥=c wTf(x).\n",
      "(16.34)\n",
      "A procedure for ﬁnding xmax and xmin is described in (Berkes and Wiskott, 2007).\n",
      "a)\n",
      "b)\n",
      "Fig. 16.23: Quadratic SFA of image sequence data generated from static image data. a) Input\n",
      "images xmax that correspond to maximum output. b) Input images xmin that correspond to min-\n",
      "imum output. The maximum and minimum input images are at corresponding locations in the\n",
      "lattices. Most of the optimal input images are oriented and bandpass, and also spatially localized\n",
      "to some degree. The maximum and minimum input images of the units have interesting relation-\n",
      "ships; for example, they may have different preferred orientations, different locations, or one can\n",
      "be orientation-selective while the other is not.\n",
      "Berkes and Wiskott (2005) applied quadratic SFA to simulated image sequence\n",
      "data: the image sequences x(t) were obtained from static natural images by selecting\n",
      "an initial image location, obtaining as x(0) from the location with random orienta-\n",
      "tion and zoom factor, and then obtaining x(t), t > 0, by applying all of the following\n",
      "transformations at the location of x(t −1) : translation, rotation, and zooming. An\n",
      "important property of the results obtained by Berkes and Wiskott (2005) using SFA\n",
      "with simulated image sequence data is the phase invariance of the quadratic units.\n",
      "This has lead to an association between SFA and complex cells; in fact, Berkes and\n",
      "Wiskott (2005) report a number of observed properties in quadratic SFA models\n",
      "that match those of complex cells. See Figure 16.23 for a reproduction of those re-\n",
      "sults. However, the results by Berkes and Wiskott were obtained by simulated image\n",
      "\n",
      "374\n",
      "16 Temporal sequences of natural images\n",
      "sequences, whose temporal correlations were basically determined by the experi-\n",
      "menters themselves. Thus, they do not really provide a basis for making conclusions\n",
      "about the connection between ecologically valid stimuli and visual processing.\n",
      "Hashimoto (2003) applied quadratic SFA to real natural image sequences. She\n",
      "found that the obtained features were only weakly related to complex cells, and\n",
      "proposed that better results could be found by a sparse variant of SFA. This will be\n",
      "treated next.\n",
      "16.8.3 Sparse slow feature analysis\n",
      "In sparse SFA, the measure of change is changed to one that emphasizes sparse-\n",
      "ness. In the original objective in Equation (16.23), it is not necessary to take the\n",
      "squared error. The squared error is used here for algebraic and computational sim-\n",
      "plicity only: it allows us to maximize the objective function using the eigenvalue\n",
      "decomposition. In general, we can consider objective function of the form\n",
      "fSSFA(w) = Et {G(s(t)−s(t −∆t))}.\n",
      "(16.35)\n",
      "where G is some even-symmetric function. A statistically optimal choice of G is\n",
      "presumably one that corresponds to a sparse pdf, because changes in images are\n",
      "usually abrupt, as in edges. We call resulting model Sparse SFA. The statistical\n",
      "justiﬁcation is based on modelling the data with an autoregressive model in which\n",
      "the driving noise term (innovation process) is super-gaussian or sparse (Hyv¨arinen,\n",
      "2001b).\n",
      "The same sparse model (using a G which is not the square function) can be used\n",
      "in the context of quadratic SFA because quadratic SFA simply means deﬁning the\n",
      "input data in a new way. This leads to the concept of sparse quadratic SFA. It is\n",
      "important not to confuse the two ways in which an SFA model can be quadratic: It\n",
      "can use squared error (i.e. take G(u) = u2), or it can use a quadratic expansion of\n",
      "the input data (using products of the original input variables as new input variables).\n",
      "Using sparse quadratic SFA, Hashimoto (2003) obtained energy detectors which\n",
      "seem to be much closer to quadrature-phase ﬁlter pairs and complex cells than those\n",
      "obtained by ordinary quadratic SFA. Those results were obtained on real natural\n",
      "image sequences, from which ordinary quadratic SFA does not seem to learn very\n",
      "complex-cell like energy detectors.\n",
      "Thus, we see how sparseness is ubiquitous in natural image statistics modelling,\n",
      "and seems to be necessary even in the context of SFA.\n",
      "\n",
      "16.9 Conclusion\n",
      "375\n",
      "16.9 Conclusion\n",
      "Different models of temporal coherence have been applied to simulated and natural\n",
      "visual data. The results that emerge from these models agree with neurophysiologi-\n",
      "cal observations to varying degrees. Thus far the principle has mostly been applied\n",
      "to model V1 cells – that is, simple and complex cells. In this chapter we focused\n",
      "on models which have resulted in the emergence of spatially localized ﬁlters with\n",
      "multiple scales (responding to different frequencies) from natural image sequence\n",
      "data. That is, we required that the spatial localization has not been forced in the\n",
      "model, but emerges from learning, as happened in all the sparse coding and ICA-\n",
      "related models treated in this book so far; this is in contrast to some temporal coher-\n",
      "ence models, in which spatial localization is enforced by sampling with a gaussian\n",
      "weighting window, so that the RF’s are then necessarily localized in the center of\n",
      "the patch. Also, we required that the image sequences come from a video camera\n",
      "or a similar device, which is in contrast to some work in which one takes static im-\n",
      "ages and then artiﬁcially creates sequence by sampling from them. Further work on\n",
      "temporal coherence, in addition to the work already cited above, include (Kohonen,\n",
      "1996; Kohonen et al, 1997; Bray and Martinez, 2003; Kayser et al, 2003; K¨ording\n",
      "et al, 2004; Wyss et al, 2006).\n",
      "A more philosophical question concerns the priorities between models of static\n",
      "images and image sequences. We have seen models which produce quite similar\n",
      "results in the two cases. For example, simple cell RF’s can be learned by sparse\n",
      "coding and ICA from static natural images, or, alternatively, using temporal coher-\n",
      "ence from natural image sequences. Which model is, then, more “interesting”? This\n",
      "is certainly a deep question which depends very much of the justiﬁcation of the\n",
      "assumptions in the models. Yet, one argument can be put forward in general: We\n",
      "should always prefer the simpler model, if both models have the same explanatory\n",
      "power. This is a general principle in modelling, called parsimony, or Occam’s ra-\n",
      "zor. In our case, it could be argued that since static images are necessarily simpler\n",
      "than image sequences, we should prefer models which use static images—at least\n",
      "if the models have similar conceptual simplicity. Thus, one could argue that models\n",
      "on image sequences are mainly interesting if they enable learning of aspects which\n",
      "cannot be learned with static images. This may not have been the case with many\n",
      "models we considered in this chapter; however, the principles introduced may very\n",
      "well to lead to discoveries of new properties which cannot be easily, if at all, found\n",
      "in static images.\n",
      "An important related question concerns learning image transformations (Memi-\n",
      "sevic and Hinton, 2007). One can view image sequences from a viewpoint where\n",
      "each image (frame) is a transformation of the preceding one. This is, in a sense,\n",
      "complementary to the viewpoint of temporal coherence, in which one tries to cap-\n",
      "ture features which are not transformed. It also seems to be closely related to the\n",
      "idea of predictive coding, see Section 14.3.\n",
      "\n",
      "\n",
      "Part V\n",
      "Conclusion\n",
      "\n",
      "\n",
      "Chapter 17\n",
      "Conclusion and future prospects\n",
      "In this chapter, we ﬁrst provide a short overview of this book. Then, we discuss\n",
      "some open questions in the ﬁeld, as well as alternative approaches to natural image\n",
      "statistics which we did not consider in detail in this book. We conclude with some\n",
      "remarks on possible future developments.\n",
      "17.1 Short overview\n",
      "We started this book in Chapter 1 by motivating the research on natural image\n",
      "statistics from an ecological-statistical viewpoint: The visual system is assumed to\n",
      "be adapted to the statistical structure of natural images, because it needs to use\n",
      "Bayesian inference. Next, we prepared the reader by introducing well-known math-\n",
      "ematical tools which are needed in natural image statistics models (Part I, i.e. Chap-\n",
      "ters 2–4). The rest of the book, up to this chapter, was mainly a succession of differ-\n",
      "ent statistical models for natural images.\n",
      "Part II was dedicated to models using purely linear receptive ﬁelds. The ﬁrst\n",
      "model we considered was principal component analysis in Chapter 5. It is an im-\n",
      "portant model for historical reasons, and also because it provides a preprocessing\n",
      "method (dimension reduction accompanied by whitening) which is used in most\n",
      "subsequent models. However, it does not provide a proper model for receptive ﬁelds\n",
      "in the primary visual cortex (V1).\n",
      "In Chapter 6, the failure of principal component analysis was explained as the\n",
      "failure to consider the sparse, non-gaussian structure of the data. In fact, natural\n",
      "images have a very sparse structure; the outputs of typical linear ﬁlters all have\n",
      "strongly positive kurtosis. Based on this property, we developed a method in which\n",
      "we ﬁnd the feature weights by maximizing the sparseness of the output when the\n",
      "input to the feature detectors is natural images. Thus, we obtained a fundamental\n",
      "result: sparse coding ﬁnds receptive ﬁelds which are quite similar to those in V1\n",
      "simple cells in the sense that they are spatially localized, oriented, and band-pass\n",
      "(localized in Fourier space).\n",
      "379\n",
      "\n",
      "380\n",
      "17 Conclusion and future prospects\n",
      "Chapter 7 further elaborated on the linear sparse coding model and brought it\n",
      "ﬁrmly into the realm of generative probabilistic models. It also brought the view-\n",
      "point of independence: instead of maximizing the sparseness of single features, we\n",
      "can maximize their statistical independence. A most fundamental theoretical results\n",
      "says that these two goals are equivalent for linear features. The resulting model has\n",
      "been named independent component analysis (ICA) in the signal-processing litera-\n",
      "ture. An information-theoreticinterpretation of the model was considered Chapter 8,\n",
      "as an alternative to the Bayesian one.\n",
      "Part III took a clear step forward by introducing nonlinear feature detectors. It\n",
      "turns out that independent component analysis is not able to cancel all the depen-\n",
      "dencies between the components, despite the name of the method. If we measure\n",
      "the dependencies of components given by ICA by computing different kinds of cor-\n",
      "relations, we see that the squares of the components tend to be strongly correlated\n",
      "(Chapter 9). Such squares are called “energies” for historical reasons. We can model\n",
      "such dependencies by introducing a random variable which controls the variances\n",
      "of all the components at the same time. This enables the reduction of the depen-\n",
      "dencies based on processing which is quite similar to neurophysiological models of\n",
      "interactions between cells in V1, based on divisive normalization.\n",
      "In Chapter 10, we used the same kind of energy dependencies to model strongly\n",
      "nonlinear features. Here, the nonlinearities took the form of computing the squares\n",
      "of linear feature detectors and summing (“pooling”) such squares together. Just as\n",
      "with linear features, we can maximize the sparseness of such nonlinear features\n",
      "when the input is natural images. The resulting features are quite similar to complex\n",
      "cells in V1. Again, we can build a probabilistic model, independent subspace anal-\n",
      "ysis, based on this maximization of sparseness. Interestingly, the model can also be\n",
      "considered a nonlinear version of independent component analysis.\n",
      "The same idea of maximization of sparseness of energies was extended to model\n",
      "the spatial (“topographic”) arrangement of cells in the cortex in Chapter 11. This\n",
      "model is really a simple modiﬁcation of the complex cells model of the preceding\n",
      "chapter. We order the simple cells or linear feature detectors on a regular grid, which\n",
      "thus deﬁnes which cells are close to each other. Then, we maximize the sparsenesses\n",
      "of energy detectors which pool the energies of close-by simple cells; we take the\n",
      "sum of such sparsenesses over all grid locations. This leads to a spatial arrangement\n",
      "of linear features which is similar to the one in V1 in the sense that preferred ori-\n",
      "entations and frequencies of the cells change smoothly when we move on the grid\n",
      "(or cortex); the same applies to the locations of the centres of the features. Because\n",
      "of the close connection to the complex cell model, the pooled energies of close-by\n",
      "cells (i.e. sums of squares of feature detectors which are close to each other on the\n",
      "grid) have the properties of complex cells just like in the preceding chapter.\n",
      "In Chapter 12 even more complicated nonlinear features were learned, although\n",
      "we didn’t introduce any new probabilistic models. The trick was to ﬁx the initial\n",
      "feature extraction to computation of energies as in complex cell models. Then we\n",
      "can just estimate a linear model, basic ICA, of the outputs of such nonlinear feature\n",
      "detectors. Effectively, we are then estimating a hierarchical three-layer model. The\n",
      "results show that there are strong dependencies between outputs of complex cells\n",
      "\n",
      "17.2 Open, or frequently asked, questions\n",
      "381\n",
      "which are collinear, even if they are in different frequency bands. Thus, the learned\n",
      "features can be interpreted as short line segments which are, in contrast to the fea-\n",
      "tures computed by simple or complex cells, not restricted to a single frequency band\n",
      "(and they are also more elongated).\n",
      "Chapter 13 we went back to the basic linear models such as ICA, and introduced\n",
      "two important extensions. First, we considered the case where the number of com-\n",
      "ponents is arbitrarily large, which results in what is called an overcomplete basis.\n",
      "Overcomplete bases seem to be important for building a good probabilistic model,\n",
      "although the receptive ﬁelds learned are not unlike those learned by basic ICA. A\n",
      "related extension is Markov random ﬁelds which may allow extension of the models\n",
      "to whole images instead of image patches.\n",
      "To conclude Part III, we showed how the concept of feedback emerges naturally\n",
      "from Bayesian inference in models of natural image statistics (Chapter 14). Feed-\n",
      "back can be interpreted as a communication between different feature sets to com-\n",
      "pute the best estimates of the feature values. Computing the values of the features\n",
      "was straightforward in models considered earlier, but if we assume that there is noise\n",
      "in the system or we have an overcomplete basis, things are much more complicated.\n",
      "The features are interpreted latent (hidden) random variables, and computing the op-\n",
      "timal Bayesian estimates of the features is a straightforward application of Bayesian\n",
      "inference. However, computationally it can be quite complicated, and requires up-\n",
      "dating estimates of some features based on estimates of others; hence the need for\n",
      "feedback from one cortical area to another, or between groups of cells inside the\n",
      "same cortical area. This topic is not yet well developed, but holds great promise to\n",
      "explain the complicated phenomena of cortical feedback which are wide-spread in\n",
      "the brain.\n",
      "Part IV considered images which are not simple static grey-scale images. For\n",
      "colour images and stereo images (mimicking the capture of visual information by\n",
      "the two eyes), ICA gives features which are similar to the corresponding processing\n",
      "in V1, as shown in Chapter 15. For motion (Chapter 16), the same is true, at least to\n",
      "some extent; more interestingly, motion leads to a completely new kind of statistical\n",
      "property, or learning principle. This is temporal coherence or stability, which is\n",
      "based on ﬁnding features which change slowly.\n",
      "17.2 Open, or frequently asked, questions\n",
      "Next, we consider some questions on the general framework and fundamental as-\n",
      "sumptions adopted in this book.\n",
      "\n",
      "382\n",
      "17 Conclusion and future prospects\n",
      "17.2.1 What is the real learning principle in the brain?\n",
      "There has been some debate on what is the actual learning principle which the vi-\n",
      "sual cortex “follows”, or which it should follow. There are really two questions\n",
      "here: What is the learning principle which the brain should follow according to the\n",
      "ecological-statistical approach, and what is the learning principle which best ex-\n",
      "plains the functioning of the brain. Answering the latter question seems impossible\n",
      "considering our modest knowledge of the workings of the visual cortex, but the for-\n",
      "mer question needs some comment because it may seem the existing theory provides\n",
      "several contradictory answers.\n",
      "In fact, in this book, we saw a few different proposals for the learning principle:\n",
      "sparseness in Chapter 6, independence in Chapter 7, and temporal coherence in\n",
      "Chapter 16. However, in our view, there is no need to argue which one of these is\n",
      "the best since they are all subsumed under the greater principle of describing the\n",
      "statistical structure of natural images as well as possible.\n",
      "Having a good statistical model of the input is what the visual system needs in\n",
      "order to perform Bayesian inference. Yet, it is true that Bayesian inference may\n",
      "not be the only goal for which the system needs input statistics. Sparse coding,\n",
      "as well as topography, may be useful for reducing metabolic costs (Section 6.5 and\n",
      "Section 11.5). Information theoretic approaches (Chapter 8) assume that the ultimate\n",
      "goal is to store and to transmit the data in noisy channels of limited capacity — the\n",
      "limited capacity being presumably due to metabolic costs.\n",
      "Our personal viewpoint is that the image analysis and pattern recognition are so\n",
      "immensely difﬁcult tasks that the visual system needs to be optimized to perform\n",
      "them. Metabolic costs may not be such a major factor in the design of the brain.\n",
      "However, we admit that this is, at best, an educated guess, and future research may\n",
      "prove it to be wrong.\n",
      "In principle, we can compare different models and learning principles as the the-\n",
      "ory of statistical estimation gives us clear guidelines on how to measure how well a\n",
      "model describes a data set. There may not be a single answer, because one could use,\n",
      "for example, either the likelihood or the score matching distance. However, these\n",
      "different measures of model ﬁt are likely to give very similar answers on which\n",
      "models are good and which are not. In the future, such calculations may shed some\n",
      "light on the optimality of various learning principles.\n",
      "17.2.2 Nature vs. nurture\n",
      "One question which we have barely touched is whether the formation of receptive\n",
      "ﬁelds is governed by genes or the input from the environment. One answer to this\n",
      "question is simply that we don’t care: the statistical models are modelling the ﬁnal\n",
      "result of genetic instructions and individual development, and we don’t even try\n",
      "to ﬁgure out which part has what kind of contribution. The question of nature vs.\n",
      "\n",
      "17.2 Open, or frequently asked, questions\n",
      "383\n",
      "nurture seems to be highly complex in the case of the visual system, and trying to\n",
      "disentangle the two effects has not produced very conclusive results.\n",
      "What makes the situation even more complicated in vision research is that there is\n",
      "ample evidence that pre-natal experience in the uterus has an important effect on the\n",
      "receptive ﬁeld properties, see (Wong, 1999) for a review. In fact, the retinal ganglion\n",
      "cells exhibit spontaneous activity which is characterized by synchronised bursts,\n",
      "and they generate waves of activity that periodically sweep across the retina. If such\n",
      "“travelling waves” are disrupted by experimental manipulations, the development\n",
      "of the visual cortex suffers considerably (Cang et al, 2005).\n",
      "Spontaneous retinal waves may, in fact, be considered as a primitive form of vi-\n",
      "sual stimuli from which the visual cortex might learn in rather similar ways as it\n",
      "learns from natural images. Application of ICA on such travelling waves can gen-\n",
      "erate something similar to ICA of natural images. Thus, travelling waves may be\n",
      "a method of enabling the rudimentary learning of some basic receptive ﬁeld prop-\n",
      "erties even before the eyes receive any input. One might speculate that such waves\n",
      "are a clever method, devised by evolution, of simulating some of the most funda-\n",
      "mental statistical properties of natural images. Such learning would bridge the gap\n",
      "between nature and nurture, since it is both innate (present at birth) and learned from\n",
      "“stimuli” external to the visual cortex (Albert et al, 2008).\n",
      "17.2.3 How to model whole images\n",
      "Our approach was based on the idea of considering images as random vectors. This\n",
      "means, in particular, that we neglect their two-dimensional structure, and the fact\n",
      "that different parts of the image tend to have rather similar statistical regularities.\n",
      "Our approach was motivated by the desire to be sure that the properties we esti-\n",
      "mate are really about the statistics of images and not due to our assumptions. The\n",
      "downside is that this is computationally a very demanding approach: the number of\n",
      "parameters can be very large even for small image patches, which means that we\n",
      "need large amounts of data and the computational resources needed can be near the\n",
      "limit of what is available — at the time of this writing.\n",
      "The situation can be greatly simpliﬁed if we assume that the dependencies of\n",
      "pixels are just the same regardless of whether the pixels considered are in, say, the\n",
      "upper-left corner of the image, or in the centre. We have already considered one\n",
      "approach based on this idea, Markov random ﬁelds in Section 13.1.7, and wavelet\n",
      "approaches to be considered below in Section 17.3.2 are another.\n",
      "Wavelet theory has been successfully used in many practical engineering tasks\n",
      "to model whole images. A major problem is that it does not really answer the ques-\n",
      "tion of what are the statistically optimal receptive ﬁelds; the receptive ﬁelds are\n",
      "determined largely by mathematical convenience, the desire to imitate V1 receptive\n",
      "ﬁelds, or, more recently, the desire to imitate ICA results.\n",
      "On the other hand, the theory of Markov random ﬁelds offers a promising alter-\n",
      "native in which we presumably can estimate receptive ﬁelds from natural images, as\n",
      "\n",
      "384\n",
      "17 Conclusion and future prospects\n",
      "well as obtain a computationally feasible probability model for Bayesian inference.\n",
      "However, at present, the theory is really not developed enough to see whether that\n",
      "promise will be fulﬁlled.\n",
      "17.2.4 Are there clear-cut cell types?\n",
      "There has been a lot of debate about the categorization of V1 cells into simple and\n",
      "complex cells. Some investigators argue that the cells cannot be meaningfully di-\n",
      "vided into two classes. They argue that there is a continuum of cell types, meaning\n",
      "that there are many cells which are between the stereotypical simple cells and com-\n",
      "plex cells.\n",
      "Consider some quantity (such as phase-variance) which can be measured from\n",
      "cells in the primary visual cortex. The basic point in the debate is whether we can\n",
      "ﬁnd a quantity such that its distribution is bimodal. This is illustrated in Figure 17.1.\n",
      "In some authors’ view, only such bimodality can justify classiﬁcation to simple and\n",
      "complex cells. Thus, there is not much debate on whether there are some cells which\n",
      "ﬁt the classical picture of simple cells, and others which ﬁt the complex cell cate-\n",
      "gory. The debate is mainly on whether there is a clear distinction or division between\n",
      "these two classes.\n",
      "Value of quantity\n",
      "Number of cells\n",
      "Fig. 17.1: Hypothetical histogram of some quantity for cells in the primary visual cortex. Some au-\n",
      "thors argue that the histogram should be bimodal (solid curve) to justify classiﬁcation of cells into\n",
      "simple and complex cells. On the other hand, even if the distribution is ﬂat (dashed curve), charac-\n",
      "terizing the cells at the two ends of the distribution may be an interesting approach, especially in\n",
      "computational models which always require some level of abstraction.\n",
      "\n",
      "17.2 Open, or frequently asked, questions\n",
      "385\n",
      "This debate is rather complicated because there are very different dimensions in\n",
      "which one could assume simple and complex cells to be form two classes. One can\n",
      "consider, for example, their response properties as measured by phase-invariance, or\n",
      "some more basic physiological or anatomical quantities in the cells. It has, in fact,\n",
      "been argued that even marked differences in response properties need not imply any\n",
      "fundamental physiological difference which would justify considering two different\n",
      "cell types (Mechler and Ringach, 2002).\n",
      "A related debate is on the validity of the hierarchical model, in which complex\n",
      "cell responses are computed from simple cell responses. It has been argued that\n",
      "complex cell responses might be due to lateral connections in a system with no hi-\n",
      "erarchical distinction between simple and complex cells (Chance et al, 1999). This\n",
      "dimension, hierarchy vs. lateral connections, might be considered another dimen-\n",
      "sion along which the bimodality or ﬂatness of the distribution could be considered.\n",
      "We would argue that this debate is not necessarily very relevant for natural image\n",
      "statistics. Even if the distribution of simple and complex cells is not bimodal with\n",
      "respect to any interesting quantity, it still makes sense to model the two ends of the\n",
      "distribution. This is a useful abstraction even if it neglects the cells in the middle\n",
      "of the distribution. Furthermore, if we have models of the cells at the ends of the\n",
      "“spectrum”, it may not be very difﬁcult to combine them into one to provide a more\n",
      "complete model. In any case, mathematical and computational modelling always re-\n",
      "quire some level of abstraction; this includes classiﬁcation of objects into categories\n",
      "which are not strictly separated in reality.1\n",
      "17.2.5 How far can we go?\n",
      "So far, the research on natural image statistics has mainly been modelling V1, using\n",
      "the classic distinction of simple and complex cells. Chapter 12 presented an attempt\n",
      "to go beyond these two processing layers. How far is it possible to go with this\n",
      "modelling approach?\n",
      "A central assumption in natural image statistics research is that learning is unsu-\n",
      "pervised. In the terminology of machine learning, this means learning in which we\n",
      "do not know what is good and what is bad; nor do we know what is the right output\n",
      "of the system in contrast to classic regression methods. Thus, if the system knows\n",
      "that bananas are good (in the sense of increasing some objective function), we are\n",
      "in a domain which is perhaps outside of natural image statistics. So, the question\n",
      "really is: How much of the visual system is involved in processing which applies\n",
      "equally to all stimuli, and does not require knowledge of what the organism needs?\n",
      "Unsupervised learning may be enough for typical signal-processing tasks such\n",
      "as noise reduction and compression. Noise reduction should be taken here in a\n",
      "1 In fact, when we talk about response properties of a cell, there is always a certain amount of\n",
      "abstraction involves since the response properties change (adapt) depending on various parame-\n",
      "ters. For example, the contrast level may change the classiﬁcation of a cell to simple or complex\n",
      "(Crowder et al, 2007).\n",
      "\n",
      "386\n",
      "17 Conclusion and future prospects\n",
      "very general sense, including operations such as contour completion. More sophisti-\n",
      "cated tasks which may be possible in an unsupervised setting include segmentation,\n",
      "amodal completion (completion of occluded contours), and various kinds of ﬁlling-\n",
      "in of image areas which are not seen due to anatomical restrictions or pathologies.\n",
      "Certainly, there need not be any clear-cut distinction between processing based\n",
      "on unsupervised learning and the rest. For example, the system might be able to\n",
      "perform a rudimentary segmentation based on generic knowledge of natural image\n",
      "statistics; if that results in recognition of, say, a banana, the prior knowledge about\n",
      "the banana can be used to reﬁne the segmentation. That is, knowledge of the general\n",
      "shapes of objects can be complemented by knowledge about speciﬁc objects, the\n",
      "latter being perhaps outside of the domain of natural image statistics.\n",
      "The greatest obstacle in answering the question in the title of this section is our\n",
      "lack of knowledge of the general functioning of the visual system. We simply don’t\n",
      "know enough to make a reasonable estimate on which parts could be modelled by\n",
      "natural image statistics. So, it may be better to leave this question entirely to future\n",
      "research.\n",
      "17.3 Other mathematical models of images\n",
      "In this book, data-driven analysis was paramount: We took natural images and an-\n",
      "alyzed them with models which are as general as possible. A complementary ap-\n",
      "proach is to construct mathematical models of images based on some theoretical\n",
      "assumptions, and then ﬁnd the best representation. The obvious advantage is that\n",
      "the features can be found in a more elegant mathematical form, although usually\n",
      "not as a simple formula. The equally obvious disadvantage is that the utility of such\n",
      "a model crucially depends on how realistic the assumptions are. An ancestor of this\n",
      "line of research is Fourier analysis, as well as the more elaborate Gabor analysis,\n",
      "which were discussed in Chapter 2.\n",
      "In this section, we consider some of the most important models developed using\n",
      "this approach.\n",
      "17.3.1 Scaling laws\n",
      "Most of the mathematical models in this section are related to scaling laws, which\n",
      "are one of the oldest observations of the statistical structure of natural images. Scal-\n",
      "ing laws basically describe the approximately 1/ f 2 behaviourof the power spectrum\n",
      "which we discussed in Section 5.6.1. As reviewed in (Srivastava et al, 2003), they\n",
      "were ﬁrst found by television engineers in the 1950’s (Deriugin, 1956; Kretzmer,\n",
      "1952).\n",
      "The observed scaling is very closely related to scale-invariance. The idea is to\n",
      "consider how natural image statistics change when you look at natural images at dif-\n",
      "\n",
      "17.3 Other mathematical models of images\n",
      "387\n",
      "ferent scales (resolutions). The basic observation, or assumption, is that they don’t:\n",
      "natural images look just the same if you zoom in or zoom out. Such scale-invariance\n",
      "is one of the basic motivations of a highly inﬂuential theory of signal and image\n",
      "analysis: wavelet theory, which we consider next.\n",
      "17.3.2 Wavelet theory\n",
      "Beginning from the 1980’s the theory of wavelets became very prominent in signal\n",
      "and image processing. Wavelets provide a basis for one-dimensional signals; the\n",
      "basis is typically orthogonal. The key idea is that all the basis vectors (or functions,\n",
      "since the original formulation uses a continuous formalism) are based on a single\n",
      "prototype function called the mother wavelet φ(x). The functions in the wavelet\n",
      "basis are obtained by translations φ(x+l) and “dilations” (rescalings) φ(2−sx):\n",
      "φs,l(x) = 2−s/2φ(2−sx−l)\n",
      "(17.1)\n",
      "where s and l are integers that represent scale and translation, respectively. The fun-\n",
      "damental property of a wavelet basis is self-similarity, which means that the same\n",
      "function is used in different scales without changing it shape. This is motivated\n",
      "by the scale-invariance of natural images. Wavelet theory can be applied on sig-\n",
      "nals sampled with a ﬁnite resolution by considering discretized versions of these\n",
      "functions, just like in Fourier analysis we can move from a continuous-time repre-\n",
      "sentation to a discretized one.\n",
      "Much of the excitement around wavelets is based on mathematical analysis which\n",
      "shows that the representation is optimal for several statistical signal-processing tasks\n",
      "such as denoising (Donoho et al, 1995). However, such theoretical results always as-\n",
      "sume that the input data comes from a certain theoretical distribution (in this case,\n",
      "from certain function spaces deﬁned using sophisticated functional analysis theory).\n",
      "Another great advantage is the existence of fast algorithms for computing the coef-\n",
      "ﬁcients in such a basis (Mallat, 1989).\n",
      "This classic formulation of wavelets is for one-dimensional signals, which is\n",
      "a major disadvantage for image analysis. Although it is straightforward to apply a\n",
      "one-dimensional analysis on images by ﬁrst doing the analysis in, say, the horizontal\n",
      "direction, and then, the vertical direction, this is not very satisfactory because then\n",
      "the analysis does not properly contain features of different orientations. In practical\n",
      "image processing, the fact that basic wavelets form an orthogonal basis may also\n",
      "be problematic: It implies that the number of features equals the number of pixels,\n",
      "whereas in engineering applications, an overcomplete basis is usually required. Var-\n",
      "ious bases which are similar to wavelets, but better for images, have therefore been\n",
      "developed. In fact, the theory of multiresolution decompositions of images was one\n",
      "of the original motivations for the general theory of wavelets (Burt and Adelson,\n",
      "1983).\n",
      "\n",
      "388\n",
      "17 Conclusion and future prospects\n",
      "Wavelet-like bases speciﬁcally developed for images typically include features\n",
      "of different orientations, as well as some overcompleteness. “Steerable pyramids”\n",
      "are based on steerable ﬁlters, and therefore provide implicitly all possible orienta-\n",
      "tions, see, e.g. (Freeman and Adelson, 1991; Simoncelli et al, 1992). One of the\n",
      "most recent systems is “curvelets”. Curvelets can be shown to provide an optimally\n",
      "sparse representation of edges (Cand`es et al, 2005), thus providing a basis set which\n",
      "is mathematically well-deﬁned and statistically optimal. However, such strong theo-\n",
      "retical optimality results only come at the cost of considering edge representation in\n",
      "an artiﬁcial setting, and their relevance to natural images remains to be investigated.\n",
      "In any case, such ready-made bases may be very useful in engineering applications.\n",
      "An interesting hybrid approach is to use a wavelet basis which is partly learned\n",
      "(Olshausen et al, 2001; Sallee and Olshausen, 2003), thus bridging the wavelet the-\n",
      "ory and the theory in this book; see also (Turiel and Parga, 2000).\n",
      "17.3.3 Physically inspired models\n",
      "Another line of research models the process which generated natural images in the\n",
      "ﬁrst place. As with wavelet analysis, scale-invariance plays a very important role in\n",
      "these models. In the review by (Srivastava et al, 2003), these models were divided\n",
      "into two classes, superposition models and occlusion models.\n",
      "In the superposition models (Mumford and Gidas, 2001; Grenander and Srivas-\n",
      "tava, 2001), it is assumed that the images are a linear sum of many independent\n",
      "“objects”. In spirit, the models are not very different from the linear superposition\n",
      "we have encountered ever since the ICA model in Chapter 7. What is different from\n",
      "ICA is that ﬁrst, the objects come from a predeﬁned model which is not learned, and\n",
      "second, the predeﬁned model is typically richer than the one used in ICA. In fact,\n",
      "the objects can be from a space which deﬁnes different sizes, shapes and textures\n",
      "(Grenander and Srivastava, 2001). One of the basic results in this line of research is\n",
      "to show that such superposition models can exhibit both scale-invariance and non-\n",
      "gaussianity for well-chosen distributions of the sizes of the objects (Mumford and\n",
      "Gidas, 2001).\n",
      "In occlusion models, the objects are not added linearly; they can occlude each\n",
      "other if placed close to each other. An example is the “dead leaves” model, which\n",
      "was originally proposed in mathematical morphology, see (Srivastava et al, 2003).\n",
      "It can be shown that scale-invariance can be explained with this class of models as\n",
      "well (Ruderman, 1997; Lee et al, 2001).\n",
      "17.4 Future work\n",
      "Modern research in natural statistics essentially started in the mid-1990’s with the\n",
      "publication of the seminal sparse coding paper by Olshausen and Field (1996). It co-\n",
      "\n",
      "17.4 Future work\n",
      "389\n",
      "incided with a tremendous increase of interest in independent component analysis\n",
      "(Comon, 1994; Bell and Sejnowski, 1995; Delfosse and Loubaton, 1995; Cardoso\n",
      "and Laheld, 1996; Amari et al, 1996; Hyv¨arinen and Oja, 1997) and the highly in-\n",
      "ﬂuential work by Donoho, Johnstone and others on application of wavelets to statis-\n",
      "tical signal processing (Donoho et al, 1995; Donoho, 1995; Donoho and Johnstone,\n",
      "1995). What we have tried to capture in this book is the developments of these ideas\n",
      "in the last 10-15 years.\n",
      "What might be the next wave in natural image statistics? Multilayer models are\n",
      "seen by many as the Holy Grail, especially if we were able to estimate an arbitrary\n",
      "number of layers, as in classical multilayer perceptrons. Markov random ﬁelds may\n",
      "open the way to new successful engineering applications even if their impact of\n",
      "neuroscientiﬁc modelling may be modest. Some would argue that image sequences\n",
      "are the key because their structure is much richer than those of static images.\n",
      "People outside of the mainstream natural image statistics research might put for-\n",
      "ward arguments in favour of embodiment, i.e., we cannot dissociate information pro-\n",
      "cessing from behaviour, and possibly not from metabolic needs either. This would\n",
      "mean we need research on robots, or simulated robot-like agents, which interact\n",
      "with their environment. On the other hand, science has often advanced faster when\n",
      "it has dissociated one problem from the rest; it may be that using robots makes\n",
      "modelling technically too difﬁcult.\n",
      "Whatever future research may bring, natural image statistics seems to have con-\n",
      "solidated its place as the dominant functional explanation of why V1 receptive ﬁelds\n",
      "are as they are. Hopefully, it will lead to new insights on how the rest of the visual\n",
      "system works. Combined with more high-level theories of pattern recognition by\n",
      "Bayesian inference, it has the potential of providing a “grand uniﬁed theory” of\n",
      "visual processing in the brain.\n",
      "\n",
      "\n",
      "Part VI\n",
      "Appendix: Supplementary mathematical\n",
      "tools\n",
      "\n",
      "\n",
      "Chapter 18\n",
      "Optimization theory and algorithms\n",
      "In this book, we have considered features which are deﬁned by some optimality\n",
      "properties, such as maximum sparseness. In this chapter, we brieﬂy explain how\n",
      "those optimal features can be numerically computed. The solutions are based ei-\n",
      "ther on general-purpose optimization methods, such as gradient methods, or speciﬁc\n",
      "tailor-made methods such as ﬁxed-point algorithms.\n",
      "18.1 Levels of modelling\n",
      "First, it is important to understand the different levels on which we can model vision.\n",
      "A well-known classiﬁcation is due to Marr (1982), who distinguished between the\n",
      "computational, algorithmic, and implementation levels. In our context, we can actu-\n",
      "ally distinguish even more levels. We can consider, at least, the following different\n",
      "levels:\n",
      "1. Abstract principle. The modelling begins by formulating an abstract optimality\n",
      "principle for learning. For example, we assume that the visual system should\n",
      "have a good model of the statistical properties of the typical input, or that the\n",
      "representation should be sparse to decrease metabolic costs.\n",
      "2. Probabilistic model. Typically, the abstract principle leads to a number of con-\n",
      "crete quantitative models. For example, independent component analysis is one\n",
      "model which tries to give a good model of the statistics of typical input.\n",
      "3. Objective function. Based on the probabilistic model, or sometimes directly us-\n",
      "ing the abstract principle, we formulate an objective function which we want to\n",
      "optimize. For example, we formulate the likelihood of a probabilistic model.\n",
      "4. Optimization algorithm. This is the focus of this Chapter. The algorithm allows\n",
      "us to ﬁnd the maximum or minimum of the objective function.\n",
      "5. Physical implementation. This is the detailed physical implementation of the op-\n",
      "timization algorithm. The same algorithm can be implemented in different kinds\n",
      "of hardware: a digital computer or a brain, for example. Actually, this level is\n",
      "393\n",
      "\n",
      "394\n",
      "18 Optimization theory and algorithms\n",
      "quite complex and could be further divided into a number of levels: the phys-\n",
      "ical implementation can be described at the level of networks, single cells, or\n",
      "molecules, whereas the detailed implementation of the numerical operations (e.g.\n",
      "matrix multiplication and nonlinear scalar functions) is an interesting issue in it-\n",
      "self. We will not go into details regarding this level.\n",
      "Some of the levels may be missing in some cases. For example, in the basic\n",
      "sparse coding approach in Chapter 6, we don’t have a probabilistic model: We go\n",
      "directly from the level of principles to the level of objective functions. However,\n",
      "the central idea of this book is that we should include the probabilistic modelling\n",
      "level—which was one motivation for going from sparse coding to ICA.\n",
      "An important choice made in this book is that the level of objective function is\n",
      "always present. All our learning was based on optimization of objective functions—\n",
      "which are almost always based on probabilistic modelling principles. In some other\n",
      "approaches, one may go directly from the representational principle to an algorithm.\n",
      "The danger with such an approach is that it may be difﬁcult to understand what\n",
      "such algorithms actually do. Going systematically through the levels of probabilis-\n",
      "tic modelling, and objective function formulation, we gain a deeper understanding\n",
      "of what the algorithm does based on the theory of statistics. Also, this approach\n",
      "constrains the modelling because we have to respect the rules of probabilistic mod-\n",
      "elling, and avoids completely ad hoc methods.\n",
      "Since we always have an objective function, we always need an optimization\n",
      "algorithm. In preceding chapters, we omitted any discussion on how such optimiza-\n",
      "tion algorithms can be constructed. One reason for this is that it is possible to use\n",
      "general-purpose optimization methods readily implemented in many scientiﬁc com-\n",
      "puting environments. So, one could numerically optimize the objective functions\n",
      "without knowing anything, or at least not much, on the theory of optimization.\n",
      "However, it is of course very useful to understand optimization theory when do-\n",
      "ing natural image statistics modelling for several reasons:\n",
      "• One can better choose a suitable optimization method, and ﬁne-tune its parame-\n",
      "ters.\n",
      "• Some optimization methods have interesting neurophysiological interpretations\n",
      "(in particular, Hebbian learning in Section 18.4).\n",
      "• Some methods have tailor-made optimization methods (FastICA in Section 18.7).\n",
      "That is why in this chapter, we review the optimization theory needed for under-\n",
      "standing how to optimize the objective functions obtained in this book.\n",
      "\n",
      "18.2 Gradient method\n",
      "395\n",
      "18.2 Gradient method\n",
      "18.2.1 Deﬁnition and meaning of gradient\n",
      "The gradient method is the most fundamental method for maximizing a continuous-\n",
      "valued, smooth function in a multidimensional space.\n",
      "We consider the general problem of ﬁnding the maximum of a function that takes\n",
      "real values in an n-dimensional real space. Finding the minimum is just ﬁnding the\n",
      "maximum of the negative of the function, so the same theory is directly applicable\n",
      "to both cases. We consider here maximization because that is what we needed in\n",
      "preceding chapters. Let us denote the function to be maximized by f(w) where\n",
      "w = (w1,...,wn) is just an n-dimensional vector. The function to be optimized is\n",
      "usually called the objective function.\n",
      "The gradient of f, denoted by ∇f is deﬁned as the vector of the partial deriva-\n",
      "tives:\n",
      "∇f(w) =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "∂f(w)\n",
      "∂w1...\n",
      "∂f(w)\n",
      "∂wn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(18.1)\n",
      "The meaning of the gradient is that it points in the direction where the function\n",
      "grows the fastest. More precisely, suppose we want to ﬁnd a vector v which is such\n",
      "that f(w+v) is as large as possible when we constrain the norm of v to be ﬁxed and\n",
      "very small. Then the optimal v is given by a suitably short vector in the direction\n",
      "of the gradient vector. Likewise, the vector that reduces the value of f as much as\n",
      "possible is given by −∇f(w), multiplied by a small constant. Thus, the gradient is\n",
      "the direction of “steepest ascent”, and −∇f(w) is the direction of steepest descent.\n",
      "Geometrically, the gradient is always orthogonal to the curves in a contour plot\n",
      "of the function (i.e. to the curves that show where f has the same value), pointing in\n",
      "the direction of growing f.\n",
      "For illustration, let us consider the following function:\n",
      "f(w) = exp(−5(x−1)2 −10(y−1)2)\n",
      "(18.2)\n",
      "which is, incidentally, like a gaussian pdf. The function is plotted in Figure 18.1 a).\n",
      "Its maximum is at the point (1,1). The gradient is equal to\n",
      "∇f(w) =\n",
      "\u0012\n",
      "−10(x−1)exp(−5(x−1)2 −10(y−1)2)\n",
      "−20(y−1)exp(−5(x−1)2 −10(y−1)2)\n",
      "\u0013\n",
      "(18.3)\n",
      "Some contours where the function is constant are shown in Fig. 18.1 b). Also, the\n",
      "gradient at one point is shown. We can see that taking a small step in the direction\n",
      "of the gradient, one gets closer to the maximizing point. However, if one takes a big\n",
      "enough step, one actually misses the maximizing point, so the step really has to be\n",
      "small.\n",
      "\n",
      "396\n",
      "18 Optimization theory and algorithms\n",
      "a)\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0.5\n",
      "1\n",
      "b)\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "2\n",
      "Fig. 18.1: The geometrical meaning of the gradient. Consider the function in Equation (18.3),\n",
      "plotted in a). In b), the closed curves are the sets where the function f has a constant value. The\n",
      "gradient at point (0.5,1.42) is shown and it is orthogonal to the curve.\n",
      "18.2.2 Gradient and optimization\n",
      "The gradient method for ﬁnding the maximum of a function consists of repeatedly\n",
      "taking small steps in the direction of the gradient, ∇f(w), recomputing the gradient\n",
      "at the current point after each step. We have to take small steps because we know\n",
      "that this direction leads to an increase in the value of f only locally — actually,\n",
      "we can be sure of this only when the steps are inﬁnitely small. The direction of the\n",
      "gradient is, of course, different at each point and needs to be computed again in the\n",
      "new point. The method can then be expressed as\n",
      "w ←w+ µ∇f(w)\n",
      "(18.4)\n",
      "where the parameter µ is a small step size, typically much smaller than 1. The\n",
      "iteration in (18.4) is repeated over and over again until the algorithm converges to\n",
      "a point. This can be tested by looking at the change in w between two subsequent\n",
      "iterations: if it is small enough, we assume the algorithm has converged.\n",
      "When does such an algorithm converge? Obviously, if it arrives at a point where\n",
      "the gradient it zero, it will not move away from it. This is not surprising because\n",
      "the basic principles of optimization theory tell that at the maximizing points, the\n",
      "gradient is zero; this is a generalization of the elementary calculus result which says\n",
      "that in one dimension, the minima or maxima of a function are obtained at those\n",
      "points where the derivative is zero.\n",
      "If the gradient method is used for minimization of the function, as is more con-\n",
      "ventional in the literature, the sign of the increment in (18.4) is negative, i.e.\n",
      "w ←w−µ∇f(w)\n",
      "(18.5)\n",
      "Choosing a good step size parameter µ is crucial. If it is too large, the algorithm\n",
      "will not work at all; if it is too small, the algorithm will be too slow. One method,\n",
      "\n",
      "18.2 Gradient method\n",
      "397\n",
      "which we used in the ISA and topographic ICA experiments in this book, is to adapt\n",
      "the step size during the iterations. At each step, we consider the step size used in\n",
      "the previous iteration (say µ0), and a larger one (say 2µ0) and a smaller one (µ0/2).\n",
      "Then, we compute the value of the objective function that results from using any\n",
      "of these three step sizes in the current step, and choose the step size which gives\n",
      "the largest value for the objective function, and use it as the µ0 in the next iteration.\n",
      "Such adaptation makes each step a bit slower, but it makes sure that the step sizes\n",
      "are reasonable.\n",
      "18.2.3 Optimization of function of matrix\n",
      "Many functions we want to maximize are actually functions of a matrix. However,\n",
      "in this context, such matrices are treated just like vectors. That is, a n ×n matrix is\n",
      "treated as an ordinary n2-dimensional vector. Just like we vectorize image patches\n",
      "and consider them as very long vectors, we consider the parameter matrices as if\n",
      "they had been vectorized. In practice, we don’t need to concretely vectorize the\n",
      "parameter matrices we optimize, but that is the underlying idea.\n",
      "For example, the gradient of the likelihood of the ICA model in Equation (7.15)\n",
      "is given by (see (Hyv¨arinen et al, 2001b) for derivation):\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "g(Vzt)zT\n",
      "t +(V−1)T\n",
      "(18.6)\n",
      "where the latter term is the gradient of log|detV|. Here, g is a function of the pdf\n",
      "of the independent components: g = p′\n",
      "i/pi where pi is the pdf of an independent\n",
      "component. Thus, a gradient method for maximizing the likelihood of ICA is given\n",
      "by:\n",
      "V ←V+ µ[\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "g(Vzt)zT\n",
      "t +(V−1)T ]\n",
      "(18.7)\n",
      "where µ is the learning rate, not necessarily constant in time. Actually, in this case\n",
      "it is possible to use a very simple trick to speed up computation. If the gradient is\n",
      "multiplied by VTV from the right, we obtain a simpler version\n",
      "V ←V+ µ\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "[I+g(yt)yT\n",
      "t ]V.\n",
      "(18.8)\n",
      "where yt = Vzt. This turns out to be a valid method for maximizing likelihood.\n",
      "Simply, the algorithm can be assumed to converge to the same points as the one in\n",
      "Equation (18.7) because VTV is invertible, and thus the points where the change\n",
      "in V is zero are the same. A more rigorous justiﬁcation of this natural or relative\n",
      "gradient method is given in (Cardoso and Laheld, 1996; Amari, 1998).\n",
      "\n",
      "398\n",
      "18 Optimization theory and algorithms\n",
      "18.2.4 Constrained optimization\n",
      "It is often necessary to maximize a function under some constraints. That is, the\n",
      "vector w is not allowed to take any value in the n-dimensional real space. The most\n",
      "common constraint that we will encounter is that the norm of w is ﬁxed to be con-\n",
      "stant, typically equal to one: ∥w∥= 1. The set of allowed values is called the con-\n",
      "straint set. Some changes are needed in the gradient method to take such constraints\n",
      "into account, but in the cases that we are interested in, the changes are actually quite\n",
      "simple.\n",
      "18.2.4.1 Projecting back to constraint set\n",
      "The basic idea of the gradient method can be used in the constrained case as well.\n",
      "Only a simple modiﬁcation is needed: after each iteration of (18.4), we project the\n",
      "vector w onto the constraint set. Projecting means going to the point in the constraint\n",
      "set which is closest. Projection to the constraint set is illustrated in Figure 18.2 a).\n",
      "a)\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "b)\n",
      "−1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "−0.5\n",
      "0\n",
      "0.5\n",
      "1\n",
      "1.5\n",
      "Fig. 18.2: Projection onto the constraint set (a), and projection of the gradient (b). A function (not\n",
      "shown explicitly) is to be minimized on the unit sphere. a) Starting at the point marked with “o”, a\n",
      "small gradient step is taken, as shown by the arrow. Then, the point is projected to the closest point\n",
      "on the unit sphere, which is marked by “x”. This is one iteration of the method. b) The gradient\n",
      "(dashed arrow) points in a direction in which is it dangerous to take a big step. The projected\n",
      "gradient (solid arrow) points in a better direction, which is “tangential” to the constraint set. Then,\n",
      "a small step in this projected direction is taken (the step is not shown here).\n",
      "In general, computing the projection can be very difﬁcult, but in some special\n",
      "cases, it is a simple operation. For example, if the constraint set consists of vectors\n",
      "with norm equal to one, the projection is performed simply by the division:\n",
      "w ←w/∥w∥\n",
      "(18.9)\n",
      "\n",
      "18.3 Global and local maxima\n",
      "399\n",
      "Another common constraint is orthogonality of a matrix. In that case, the projec-\n",
      "tion onto the constraint set is given by\n",
      "W ←(WWT)−1/2W\n",
      "(18.10)\n",
      "Here, we see a rather involved operation: the inverse of the square root of the matrix.\n",
      "We shall not go into details on how it can be computed; sufﬁce it to say that most nu-\n",
      "merical software can compute it quite efﬁciently.1 This operation often called sym-\n",
      "metric orthogonalization, and it is the way that symmetric decorrelation in sparse\n",
      "coding and other algorithms is usually implemented.\n",
      "18.2.4.2 Projection of the gradient\n",
      "Actually, an even better method is obtained if we ﬁrst project the gradient onto the\n",
      "“tangent space” of the constraint set, and then take a step in that direction instead\n",
      "of the ordinary gradient direction. What this means is that we compute a direction\n",
      "that is “inside” the constraint set in the sense that inﬁnitely small changes along that\n",
      "direction do not get us out of the constraint set, yet the movement in that direction\n",
      "maximally increases the value of the objective function. This improves the method\n",
      "because then we can usually take larger step sizes and obtain larger increases in the\n",
      "objective function without going in a completely wrong direction as is always the\n",
      "danger when taking large steps. The projection onto the constraint set has to done\n",
      "even in this case. Projection of the gradient is illustrated in Figure 18.2 b).\n",
      "This is quite useful in the case where we are maximizing with respect to a pa-\n",
      "rameter matrix that is constrained to be orthogonal. The projection can be shown to\n",
      "equal (Edelman et al, 1998):\n",
      "˜∇f(W) = ∇f(W)−WT∇f(W)WT\n",
      "(18.11)\n",
      "where ∇f(W) is the ordinary gradient of the function f.\n",
      "In Section 18.5 below we will see an example of how these ideas of constrained\n",
      "optimization are used in practice.\n",
      "18.3 Global and local maxima\n",
      "An important distinction is between global and local maxima. Consider the one-\n",
      "dimensional function in Figure 18.3. The global maximum of the function is at the\n",
      "point x = 6; this is the “real” maximum point where the function attains its very\n",
      "largest value. However, there are also two local maxima, at x = 2 and x = 9. A local\n",
      "1 If you really want to know: the inverse square root (WWT )−1/2 of the symmetric matrix WWT\n",
      "is obtained from the eigenvalue decomposition of WWT = Ediag(λ1,...,λn)ET as (WWT )−1/2 =\n",
      "Ediag(1/\n",
      "√\n",
      "λ1,...,1/√λn)ET . It is easy to see that if you multiply this matrix with itself, you get\n",
      "the inverse of the original matrix. See also Section 5.9.2\n",
      "\n",
      "400\n",
      "18 Optimization theory and algorithms\n",
      "maximum is a point in which the function obtains a value which is greater than the\n",
      "values in all neighbouring points close-by.\n",
      "An important point to understand is that the result of a gradient algorithm de-\n",
      "pends on the initial point, that is, the point where the algorithm starts in the ﬁrst\n",
      "iteration. The algorithm only sees the local behaviour of the function, so it will ﬁnd\n",
      "the closest local maximum. Thus, if in Figure 18.3, the algorithm is started in the\n",
      "point marked by circles, it will ﬁnd the global maximum. In contrast, if it is started\n",
      "in one of the points marked by crosses, it will converge to one of the local maxima.\n",
      "In many cases, we are only interested in the global maximum. Then, the be-\n",
      "haviour of the gradient method can be rather unsatisfactory, because it only ﬁnds\n",
      "a local optimum. This is, actually, the case with most optimization algorithms. So,\n",
      "when running optimization algorithms, we have to always keep in mind that that\n",
      "algorithm only gives a local optimum, not the global one.\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "Fig. 18.3: Local vs. global maxima. The function has a global maximum at x = 6 and two local\n",
      "maxima at x = 2 and x = 9. If a gradient algorithm starts near one of the local maxima (e.g. at the\n",
      "points marked by crosses), it will get stuck at one of the local maxima and it will not ﬁnd the global\n",
      "maximum. Only if the algorithm starts sufﬁciently close to the global maximum (e.g. at the points\n",
      "marked by circles), it will ﬁnd the global maximum.\n",
      "18.4 Hebb’s rule and gradient methods\n",
      "18.4.1 Hebb’s rule\n",
      "Hebb’s rule, or Hebbian learning, is a principle which is central in modern research\n",
      "on learning and memory. It attempts to explain why certain synaptic connections get\n",
      "strengthened as a result of experience, and others don’t; this is called plasticity in\n",
      "neuroscience, and learning is a more cognitive context. Donald Hebb proposed in\n",
      "1949 that\n",
      "\n",
      "18.4 Hebb’s rule and gradient methods\n",
      "401\n",
      "When an axon of cell A (...) excites cell B and repeatedly or persistently takes part in ﬁring\n",
      "it, some growth process of metabolic change takes place in one or both cells so that A’s\n",
      "efﬁciency as one of the cells ﬁring B is increased. (Quoted in (Kandel et al, 2000))\n",
      "This proposal can be readily considered in probabilistic terms: A statistical analysis\n",
      "is about things which happen “repeatedly or persistently”.\n",
      "A basic interpretation of Hebb’s rule is in terms of the covariance of the ﬁring\n",
      "rates of cells A and B: the change in the synaptic connection should be proportional\n",
      "to that covariance. This is because if the ﬁring rates of A and B are both high at\n",
      "the same time, their covariance is typically large. The covariance interpretation is\n",
      "actually stronger because it would also imply that if both cells are silent (ﬁring rate\n",
      "below average) at the same time, the synaptic connection is strengthened. Even more\n",
      "than that: if one of the cells is typically silent when the other one ﬁres strongly, this\n",
      "has a negative contribution to the covariance, and the synaptic connection should be\n",
      "decreased. Such an extension of Hebb’s rule seems to be quite in line with Hebb’s\n",
      "original idea (Dayan and Abbott, 2001).\n",
      "Note a difference between this covariance interpretation, in which only the cor-\n",
      "relation of the ﬁring rates matters, and the original formulation, in which cell A is\n",
      "assumed to “take part in ﬁring [cell B]”, i.e. to have a causal inﬂuence on cell B’s\n",
      "ﬁring. This difference may be partly resolved by recent research which shows that\n",
      "the exact timing of the action potentials in cell A and cell B is important, a phe-\n",
      "nomenon called spike-timing dependent plasticity (Dan and Poo, 2004), but we will\n",
      "not consider this difference here.\n",
      "18.4.2 Hebb’s rule and optimization\n",
      "Hebb’s rule can be readily interpreted as an optimization process, closely related\n",
      "to gradient methods. Consider an objective function of the form which we have\n",
      "extensively used in this book:\n",
      "J(w) = ∑\n",
      "t\n",
      "G(\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "wixi(t))\n",
      "(18.12)\n",
      "To compute the gradient, we use two elementary rules. First, the derivative of a sum\n",
      "is the sum of the derivatives, so we just need to take the derivative of G(∑n\n",
      "i=1 wixi(t))\n",
      "and take its sum over t. Second, we use the chain rule which gives the deriva-\n",
      "tive of a compound function f1(f2(w)) as f ′\n",
      "2(w)f ′\n",
      "1(f2(w)). Now, the derivative of\n",
      "∑n\n",
      "i=1 wixi(t) with respect to wi is simply xi(t), and we denote by g = G′ the deriva-\n",
      "tive of G. Thus, the partial derivatives are obtained as\n",
      "∂J\n",
      "∂wi\n",
      "= ∑\n",
      "t\n",
      "xi(t)g(\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "wixi(t))\n",
      "(18.13)\n",
      "So, a gradient method to maximize this function would be of the form\n",
      "\n",
      "402\n",
      "18 Optimization theory and algorithms\n",
      "wi ←wi + µ∑\n",
      "t\n",
      "xi(t)g(\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "wixi(t)), for all i.\n",
      "(18.14)\n",
      "Now, let us interpret the terms in Equation (18.14). Assume that\n",
      "1. the xi(t) are the inputs to the i-th dendrite of a neuron at time point t,\n",
      "2. the wi are the strengths of the synapses at those dendrites, and\n",
      "3. the ﬁring rate of the neuron at time point t is equal to ∑n\n",
      "i=1 wixi(t)\n",
      "4. the inputs xi have zero mean, i.e. they describe changes around the mean ﬁring\n",
      "rate.\n",
      "Further, let us assume that the function g is increasing.\n",
      "Then, the gradient method in Equation (18.14) is quite similar to a Hebbian learn-\n",
      "ing process. Consider the connection strength of one of the synapses i. Then, the\n",
      "connection wi is increased if xi(t) is repeatedly high at the same time as the ﬁring\n",
      "rate of the neuron in question. In fact, the term multiplied by the learning rate µ is\n",
      "nothing else that the covariance between the input to the i-th dendrite and an increas-\n",
      "ing function of the ﬁring rate of the neuron, as in the covariance-based extension of\n",
      "Hebb’s rule.\n",
      "Such a “learning rule” would be incomplete, however. The reason is that we\n",
      "have to constrain w somehow, otherwise it might simply go to zero or inﬁnity. In\n",
      "preceding chapters, we usually constrained the norm of w to be equal to unity. This\n",
      "is quite a valid constraint here as well. So, we assume that in addition to Hebb’s\n",
      "rule, some kind of normalization process is operating in cell B.\n",
      "18.4.3 Stochastic gradient methods\n",
      "The form of Hebb’s rule in Equation (18.14) uses the statistics of the data in the\n",
      "sense that it computes the correlation over many observations of x. This is not very\n",
      "realistic in terms of neurobiological modelling. A simple solution for this problem\n",
      "is offered by the theory of stochastic gradient methods (Kushner and Clark, 1978).\n",
      "The idea in a stochastic gradient is simple and very general. Assume we want to\n",
      "maximize some expectation, say E{g(w,x)} where x is a random vector, and w is a\n",
      "parameter vector. The gradient method for maximizing this with respect to w gives\n",
      "w ←w+ µE{∇wg(w,x)}\n",
      "(18.15)\n",
      "where the gradient is computed with respect to w, as emphasized by the subscript in\n",
      "the ∇operator. Note that we have taken the gradient inside the expectation operator,\n",
      "which is valid because expectation is basically a sum, and the derivative of a sum is\n",
      "the sum of the derivatives as noted above.\n",
      "The stochastic gradient method now proposes that we don’t need to compute the\n",
      "expectation before taking the gradient step. For each observation x, we can use the\n",
      "gradient iteration given that single observation:\n",
      "\n",
      "18.4 Hebb’s rule and gradient methods\n",
      "403\n",
      "w ←w+ µ∇wg(w,x)\n",
      "(18.16)\n",
      "So, when given a sample of observations of x, we compute the update in Equa-\n",
      "tion (18.16) for each observation separately. This is reasonable because the update\n",
      "in the gradient will be, on the average, equal to the update in the original gradient\n",
      "with the expectation given in Equation (18.15). The step size has to be much smaller,\n",
      "though, because of the large random ﬂuctuations in this “instantaneous” gradient.\n",
      "So, we can consider Hebbian learning in Equation (18.14) so that the sum over\n",
      "t is omitted, and each incoming observation, i.e. stimulus, is immediately used in\n",
      "learning:\n",
      "wi ←wi + µxi(t)g(\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "wixi(t))\n",
      "(18.17)\n",
      "Such a learning method still performs maximization of the objective function, but\n",
      "is more realistic in terms of neurophysiological modelling: at each time point, the\n",
      "input and output of the neuron make a small change in the synaptic weights.\n",
      "18.4.4 Role of the Hebbian nonlinearity\n",
      "By changing the nonlinearity g in the learning rule, and thus the G in the objective\n",
      "function, we see that Hebb’s rule is quite ﬂexible and allows different kinds of learn-\n",
      "ing to take place. If we assume that g is linear the original function G is quadratic.\n",
      "Then, Hebb’s rule is actually doing PCA (Oja, 1982), since it is simply maximizing\n",
      "the variance of wTx under the constraint that w has unit norm.\n",
      "On the other hand, if g is nonlinear, G is non-quadratic. Then, we can go back to\n",
      "the framework of basic sparse coding in Chapter 6. There, we used the expression\n",
      "h(s2) instead of G(s) in order to investigate the convexity of h. So, if G is such that\n",
      "it corresponds to a convex h, Hebb’s rule can be interpreted as doing sparse coding!\n",
      "The is no contradiction in that almost same rule is able to do both PCA and sparse\n",
      "coding, because in Chapter 6 we also assumed that the data is whitened. So, we see\n",
      "that the operation of Hebb’s rule depends very much on the preprocessing of the\n",
      "data.\n",
      "What kind of nonlinearities does sparse coding require? Consider the widely-\n",
      "used choice G1(s) = −logcoshs. This would give\n",
      "g1(s) = −tanhs\n",
      "(18.18)\n",
      "This function (plotted in Fig. 18.4) would be rather odd to use as such in Hebb’s\n",
      "rule, because it is decreasing, and the whole idea of Hebb’s rule would be inverted.\n",
      "(Actually, such “anti-Hebbian” learning has been observed in some contexts (Bell\n",
      "et al, 1993), and is considered important in some computational models (F¨oldi´ak,\n",
      "1990)).\n",
      "However, because the data is whitened, we can ﬁnd a way of interpreting this\n",
      "maximization as Hebbian learning. The point is that for whitened data, we can add\n",
      "\n",
      "404\n",
      "18 Optimization theory and algorithms\n",
      "a quadratic term to G, and consider\n",
      "G2(s) = 1\n",
      "2s2 −logcoshs\n",
      "(18.19)\n",
      "Since the data is whitened and w is constrained to unit norm, the expectation of\n",
      "s2 = (wTx)2 is constant, and thus the maximization of G2 produces just the same\n",
      "result as maximization of G1. Now, the derivative of G2 is\n",
      "g2(s) = s−tanhs\n",
      "(18.20)\n",
      "which is an increasing function, see Fig. 18.4.\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "−3\n",
      "−2\n",
      "−1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Fig. 18.4: Two nonlinearities: g1 in Equation (18.18), dash-dotted line, and g2 in Equation (18.20),\n",
      "solid line. For comparison, the line x = y is given as dotted line.\n",
      "So, using a nonlinearity such as g2, sparse coding does have a meaningful inter-\n",
      "pretation as a special case of Hebb’s rule. The nonlinearity g2 even makes intuitive\n",
      "sense: it is a kind of a thresholding function (actually, a shrinkage function, see\n",
      "Section 14.1.2.2), which ignores activations which are small.\n",
      "18.4.5 Receptive ﬁelds vs. synaptic strengths\n",
      "In the Hebbian learning context, the feature weights Wi are related to synaptic\n",
      "strengths in the visual cells. However, the visual input reaches the cortex only after\n",
      "passing through several neurons in the retina and the thalamus. Thus, the Wi actually\n",
      "model the compound effect of transformations in all those processing stages. How\n",
      "can we then interpret optimization of a function such as G(∑x,yW(x,y)I(x,y)) in\n",
      "terms of Hebb’s rule?\n",
      "In Section 5.9 we discussed the idea that the retina and LGN perform some-\n",
      "thing similar to a whitening of the data. Thus, as a rough approximation, we could\n",
      "consider the canonically preprocessed data as the input to the visual cortex. Then\n",
      "maximization of a function such as G(vTz), where z is the preprocessed data, is in\n",
      "\n",
      "18.5 Optimization in topographic ICA *\n",
      "405\n",
      "fact modelling the plasticity of synapses of the cells the primary visual cortex. So,\n",
      "Hebbian learning in that stage can be modelled just as we did above.\n",
      "18.4.6 The problem of feedback\n",
      "In the Hebbian implementation of ICA and related learning rules, there is one more\n",
      "problem which needs to be solved. This is the implementation of the constraint of\n",
      "orthogonality. The constraint is necessary to prevent the neurons from all learning\n",
      "the same feature. A simple approach would be to consider the minimization of some\n",
      "measure of the covariance of the outputs (assuming the data is whitened as a pre-\n",
      "processing stage):\n",
      "Q(v1,...,vn) = −M∑\n",
      "j̸=i\n",
      "[E{sisj}]2 = −M∑\n",
      "i̸=j\n",
      "[E{(vT\n",
      "i z)(vT\n",
      "j z)}]2\n",
      "(18.21)\n",
      "where M is a large constant (say, M = 100). We can add this function as a so-called\n",
      "penalty to the measures of sparseness. If we then consider the gradient with respect\n",
      "to vi, this leads to the addition of a term of the form\n",
      "∇viQ = −2M∑\n",
      "j̸=i\n",
      "E{zsj}E{sisj}\n",
      "(18.22)\n",
      "to the learning rule for vi. Because M is large, after maximization the sum of the\n",
      "[E{sisj}]2 will be very close to zero—corresponding to the case where the si are all\n",
      "uncorrelated. Thus, this penalty approximately reinforces the constraint of uncorre-\n",
      "latedness.\n",
      "The addition of Q to the sparseness measure thus results in the addition of a\n",
      "feedback term of the form in Equation (18.22).\n",
      "18.5 Optimization in topographic ICA *\n",
      "As an illustration of the gradient method and constrained optimization, we con-\n",
      "sider in this section maximization of likelihood of the topographic ICA in Equa-\n",
      "tion (11.5). This section can be skipped by readers not interested in mathematical\n",
      "details.\n",
      "Because independent subspace analysis is formally a special case of topographic\n",
      "ICA, obtained by a special deﬁnition of the neighbourhood function, the obtained\n",
      "learning rule is also the gradient method for independent subspace analysis.\n",
      "First note that we constrain V to be orthogonal, so detV is constant (equal to one),\n",
      "and can be ignored in this optimization. Another simple trick to simplify the problem\n",
      "is to note is that we can ignore the sum over t and just compute the “instantaneous”\n",
      "gradient as in stochastic gradient methods. We can always go back to the sum over\n",
      "\n",
      "406\n",
      "18 Optimization theory and algorithms\n",
      "t by just summing the gradient over t, because the gradient of a sum is the sum of\n",
      "the gradients. In fact, we can simplify the problem even further by computing the\n",
      "gradient of the likelihood for each term of in the sum over i in the likelihood in\n",
      "Equation (11.5), and taking the sum afterwards.\n",
      "So, the computation of the gradient is essentially reduced to computing the gra-\n",
      "dient of\n",
      "Li(v1,...,vn) = h(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "π(i, j)(vT\n",
      "j zt)2)\n",
      "(18.23)\n",
      "Denote by vl\n",
      "k the l-th component of vk. By the chain rule, applied twice, we obtain\n",
      "∂Li\n",
      "∂vl\n",
      "k\n",
      "= 2zl\n",
      "tπ(i,k)(vT\n",
      "k zt)h′(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "π(i, j)(vT\n",
      "j zt)2)\n",
      "(18.24)\n",
      "This can be written in vector form by simply collecting these partial derivatives for\n",
      "all l in a single vector:\n",
      "∇vkLi = 2ztπ(i,k)(vT\n",
      "k zt)h′(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "π(i, j)(vT\n",
      "j zt)2)\n",
      "(18.25)\n",
      "(This is not really the whole gradient because it is just the partial derivatives with\n",
      "respect to some of the entries in V, but the notation using ∇is still often used.)\n",
      "Since the log-likelihood is simply the sum of the Li’s we obtain\n",
      "∇vk logL =\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "∇vkLi = 2\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "zt(vT\n",
      "k zt)\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "π(i,k)h′(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "π(i, j)(vT\n",
      "j zt)2)\n",
      "(18.26)\n",
      "We can omit the constant 2 which does not change the direction of the gradient.\n",
      "So, the algorithm for maximizing the likelihood in topographic ICA is ﬁnally as\n",
      "follows:\n",
      "1. Compute the gradients in Equation (18.26) for all k. Collect them in a matrix\n",
      "∇V logL which has the ∇vk logL as its rows.\n",
      "2. Compute the projection of this matrix on the tangent space of the constraint\n",
      "space, using the formula in Equation (18.11). Denote the projected matrix as\n",
      "˜G. (This projection step is optional, but usually it speeds up the algorithm.)\n",
      "3. Do a gradient step as\n",
      "V ←V+ µ ˜G\n",
      "(18.27)\n",
      "4. Orthogonalize the matrix V. For example, this can be done by the formula in\n",
      "Equation (18.10).\n",
      "To see a connection of such an algorithm with Hebbian learning, consider a gra-\n",
      "dient update for each vk separately. We obtain the gradient learning rule\n",
      "vk ←vk + µ\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "zt(vT\n",
      "k zt)rk\n",
      "t\n",
      "(18.28)\n",
      "\n",
      "18.6 Beyond basic gradient methods *\n",
      "407\n",
      "where\n",
      "rk\n",
      "t =\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "π(i,k)h′(\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "π(i, j)(vT\n",
      "j zt)2).\n",
      "(18.29)\n",
      "Equally well, we could use a stochastic gradient method, ignoring the sum over t.\n",
      "In a neural interpretation, the Hebbian learning rule in (18.28) can be considered a\n",
      "“modulated” Hebbian learning, since the ordinary Hebbian learning term zt(vT\n",
      "k zt)\n",
      "is modulated by the term rk\n",
      "t . This term could be considered as top-down feedback,\n",
      "since it is a function of the local energies which could be the outputs of higher-order\n",
      "neurons (complex cells).\n",
      "18.6 Beyond basic gradient methods *\n",
      "This section can be skipped by readers not interested in mathematical theory. Here,\n",
      "we brieﬂy describe two further well-known classes of optimization methods. Actu-\n",
      "ally, in our context, these are not very often better than the basic gradient method,\n",
      "so our description is very brief.\n",
      "18.6.1 Newton’s method\n",
      "As discussed above, the optima of an objective function are found in points where\n",
      "the gradient is zero. So, optimization can be approached as the problem of solving\n",
      "a system of equations given by\n",
      "∂f\n",
      "∂w1\n",
      "(w) = 0\n",
      "(18.30)\n",
      "...\n",
      "(18.31)\n",
      "∂f\n",
      "∂wn\n",
      "(w) = 0\n",
      "(18.32)\n",
      "A classic method for solving such a system of equations is Newton’s method. It\n",
      "can be used to solve any system of equations, but we consider here the case of the\n",
      "gradient only.\n",
      "Basically, the idea is to approximate the function linearly using its derivatives. In\n",
      "one dimension, the idea is simply to approximate the graph of the function using its\n",
      "tangent, whose slope is given by the derivative. That is, for a general function g:\n",
      "g(w) ≈g(w0)+g′(w0)(w−w0)\n",
      "(18.33)\n",
      "This very general idea of ﬁnding the point where the function attains the value zero\n",
      "is illustrated in Figure 18.5.\n",
      "\n",
      "408\n",
      "18 Optimization theory and algorithms\n",
      "w\n",
      "w0\n",
      "Fig. 18.5: Illustration of Newton’s method for solving an equation (which we use in optimization\n",
      "to solve the equation which says that the gradient is zero). The function is linearly approximated by\n",
      "its tangent. The point where the tangent intersects with the x-axis is taken as the next approximation\n",
      "of the point where the function is zero.\n",
      "In our case, g corresponds to the gradient, so we use the derivatives of the gradi-\n",
      "ents, which are second partial derivatives of the original function f. Also, we need\n",
      "to use a multidimensional version of this approximation. This gives\n",
      "∇f(w) = ∇f(w0)+H(w0)(w−w0)\n",
      "(18.34)\n",
      "where the function H, called the Hessian matrix, is the matrix of second partial\n",
      "derivatives:\n",
      "Hij =\n",
      "∂2 f\n",
      "∂wi∂wj\n",
      "(18.35)\n",
      "Now, we can at every step of the method, ﬁnd the new point as the one for which\n",
      "this linear approximation is zero. Thus, we solve\n",
      "∇f(w0)+H(w0)(w−w0) = 0\n",
      "(18.36)\n",
      "which gives\n",
      "w = w0 −H(w0)−1(∇f(w0))\n",
      "(18.37)\n",
      "This is the idea in the Newton iteration. Starting from a random point, we iteratively\n",
      "update w according to Equation (18.37), i.e. compute the right-hand side for the\n",
      "current value of w, and take that as the new value of w. Using the same notation as\n",
      "with the gradient methods, we have the iteration\n",
      "w ←w−H(w)−1(∇f(w))\n",
      "(18.38)\n",
      "Note that this iteration is related to the gradient method. If the matrix H(w0)−1\n",
      "in Equation (18.37) is replaced by a scalar step size µ, we actually get the gradient\n",
      "method. So, the difference between the methods is threefold:\n",
      "1. In Newton’s method the direction where w “moves” is not given by the gradient\n",
      "directly, but the gradient multiplied by the inverse of the Hessian.\n",
      "\n",
      "18.6 Beyond basic gradient methods *\n",
      "409\n",
      "2. This “step size” is not always very small: It is directly given by the inverse of the\n",
      "Hessian matrix, and can be quite large.\n",
      "3. In the gradient method, one can choose between minimization and maximiza-\n",
      "tion of the objective function, by choosing the sign in the algorithm (cf. Equa-\n",
      "tions (18.4) and (18.5)). In the Newton method, no such choice is possible. The\n",
      "algorithm just tries to ﬁnd a local extremum in which the gradient is zero, and\n",
      "this can be either a minimum or a maximum.\n",
      "The Newton iteration has some advantages and disadvantages compared to the\n",
      "basic gradient method. It usually requires a smaller number of steps to converge.\n",
      "However, the computations needed at each step are much more demanding, because\n",
      "one has to ﬁrst compute the Hessian matrix, and then compute H(w)−1(∇f(w))\n",
      "(which is best obtained by solving the linear system H(w)v = ∇f(w)).\n",
      "In practice, however, the main problem with the Newton method is that its be-\n",
      "haviour can be quite erratic. There is no guarantee any one iteration gives a w which\n",
      "increases f(w). In fact, a typical empirical observation is that for some functions\n",
      "this does not happen, and the algorithm may completely diverge, i.e. go to arbitrary\n",
      "values of w, eventually reaching inﬁnity. This is because the step size can be arbi-\n",
      "trarily large, unlike in the gradient methods. This lack of robustness is why Newton’s\n",
      "method is not often used in practice.\n",
      "As an example of this phenomenon, consider the function\n",
      "f(w) = exp(−1\n",
      "2w2)\n",
      "(18.39)\n",
      "which has a single maximum as w = 0. The ﬁrst and second derivatives, which\n",
      "are the one-dimensional equivalents of the gradient and the Hessian, can be easily\n",
      "calculated as\n",
      "f ′(w) = −wexp(−1\n",
      "2w2)\n",
      "(18.40)\n",
      "f ′′(w) = (w2 −1)exp(−1\n",
      "2w2)\n",
      "(18.41)\n",
      "which gives the Newton iteration as\n",
      "w ←w+\n",
      "w\n",
      "w2 −1\n",
      "(18.42)\n",
      "Now, assume that we start the iteration at any point where w > 1. Then, the change\n",
      "w\n",
      "w2−1 is positive, which means that w is increased and it moves further and further\n",
      "away from zero! In this case, the method fails completely and w goes to inﬁnity\n",
      "without ﬁnding the maximum at zero. (In contrast, a gradient method, with a rea-\n",
      "sonably small step size, would ﬁnd the maximum.)\n",
      "However, different variants of the Newton method have proven useful. For ex-\n",
      "ample, methods which do something between the gradient method and Newton’s\n",
      "method (e.g. the Levenberg-Marquardt algorithm) have proven useful in some ap-\n",
      "plications. In ICA, the FastICA algorithm (see below) uses the basic iteration of\n",
      "\n",
      "410\n",
      "18 Optimization theory and algorithms\n",
      "Newton’s method but with a modiﬁcation which takes the special structure of the\n",
      "objective function into account.\n",
      "18.6.2 Conjugate gradient methods\n",
      "Conjugate gradient methods are often considered as the most efﬁcient general-\n",
      "purpose optimization methods. The theory is rather complicated and non-intuitive,\n",
      "so we do not try to explain it in detail.\n",
      "Conjugate gradient methods try to ﬁnd a direction which is better than the gra-\n",
      "dient direction. The idea is illustrated in Figure. 18.6. While the gradient direction\n",
      "is good for a very small step size (actually, it is still the best for an inﬁnitely small\n",
      "step size), it is not very good for a moderately large step size. The conjugate gra-\n",
      "dient method tries to ﬁnd a better direction based on information on the gradient\n",
      "directions in previous iterations. In this respect, the method is similar to Newton’s\n",
      "method, which also modiﬁes the gradient direction.\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "1.2\n",
      "1.4\n",
      "1.6\n",
      "1.8\n",
      "2\n",
      "0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1\n",
      "1.2\n",
      "1.4\n",
      "1.6\n",
      "1.8\n",
      "2\n",
      "Fig. 18.6: A problem with the gradient method. The gradient direction may be very bad for anything\n",
      "but the very smallest step sizes. Here, the gradient goes rather completely in the wrong direction\n",
      "due to the strongly non-circular (non-symmetric) structure of the objective function. The conjugate\n",
      "gradient method tries to ﬁnd a better direction.\n",
      "In fact, conjugate gradient methods do not just take a step of a ﬁxed size in the di-\n",
      "rection they have found. An essential ingredient, which is actually necessary for the\n",
      "method to work, is a one-dimension line search. This means that once the direction,\n",
      "say d, in which w should move has been chosen (using the complicated theory of\n",
      "conjugate gradient methods), many different step sizes µ are tried out, and the best\n",
      "one is chosen. In other words, a one-dimensional optimization is performed on the\n",
      "function h(µ) = f(w + µd), and µ maximizing this function is chosen. (Such line\n",
      "search could also be used in the basic gradient method. However, in the conjugate\n",
      "gradient method it is completely necessary.)\n",
      "Conjugate gradient methods are thus much more complicated than ordinary gra-\n",
      "dient methods. This is not a major problem if one uses a scientiﬁc computing en-\n",
      "\n",
      "18.7 FastICA, a ﬁxed-point algorithm for ICA\n",
      "411\n",
      "vironment in which the method is already programmed. Sometimes, the method is\n",
      "much more efﬁcient than the ordinary gradient methods, but this is not always the\n",
      "case.\n",
      "18.7 FastICA, a ﬁxed-point algorithm for ICA\n",
      "Development of tailor-made algorithms for solving the optimization problems in\n",
      "ICA is a subject of an extensive literature. Here we explain brieﬂy one popular\n",
      "algorithm for performing the maximization, more information and details can be\n",
      "found in the ICA book (Hyv¨arinen et al, 2001b).\n",
      "18.7.1 The FastICA algorithm\n",
      "Assume that the data t,t = 1,...,T, is whitened and has zero mean. The basic form\n",
      "of the FastICA algorithm is as follows:\n",
      "1. Choose an initial (e.g. random) weight vector w.\n",
      "2. Let w ←∑t ztg(wTzt)−w∑t g′(wT zt)\n",
      "3. Let w ←w/∥w∥\n",
      "4. If not converged, go back to 2.\n",
      "Note that the sign of w may change from one iteration to the next; this is in line\n",
      "with the fact that the signs of the components in ICA are not well-deﬁned. Thus,\n",
      "the convergence of the algorithm must use a criterion which is immune to this. For\n",
      "example, one might stop the iteration if |wTwold| is sufﬁciently close to one, where\n",
      "wold is the value of w at the previous iteration.\n",
      "To use FastICA for several features, the iteration step 2 is applied separately\n",
      "for the weight vector of each unit. After updating all the weight vectors, they are or-\n",
      "thogonalized (assuming whitened data). This means projecting the matrix W, which\n",
      "contains the vectors wi as its rows, on the space of orthogonal matrices, which can be\n",
      "accomplished, for example, by the classical method involving matrix square roots,\n",
      "given in Eq. (18.10). See Chapter 6 of (Hyv¨arinen et al, 2001b) for more information\n",
      "on orthogonalization.\n",
      "18.7.2 Choice of the FastICA nonlinearity\n",
      "The FastICA algorithm uses a nonlinearity, usually denoted by g. This comes from\n",
      "a measure of non-gaussianity. Non-gaussianity is measured as E{G(s)} for some\n",
      "non-quadratic function. The function g is then the derivative of G.\n",
      "\n",
      "412\n",
      "18 Optimization theory and algorithms\n",
      "Note that in Chapter 6 we measured non-gaussianity (or sparseness) as E{h(s2)}.\n",
      "Then, we have G(s) = h(s2) which implies g(s) = 2h′(s2)s. So, we must make a\n",
      "clear distinction between the nonlinearities h and the functions G and g; they are all\n",
      "different functions but they can be derived from one another.\n",
      "The choice of the measure of non-gaussianity, or the nonlinearity, is actually\n",
      "quite free in FastICA. We are not restricted to functions such that maximization of\n",
      "G corresponds to maximization of sparseness, or such that G corresponds to the log-\n",
      "pdf of the components. We can use, for example, measures of skewness, i.e. the lack\n",
      "of symmetry of the pdf.\n",
      "In practice, it has been found that G(s) = logcoshs works quite well in a variety\n",
      "of domains; it corresponds to the tanh nonlinearity as g. (In FastICA, it makes no\n",
      "difference if we take tanh or −tanh, the algorithm is immune to the change of sign.)\n",
      "18.7.3 Mathematics of FastICA *\n",
      "Here, we present the derivation of the FastICA algorithm, and show its connection\n",
      "to gradient methods. This can be skipped by readers not interested in mathematical\n",
      "details.\n",
      "18.7.3.1 Derivation of the ﬁxed-point iteration\n",
      "To begin with, we shall derive the ﬁxed-point algorithm for one feature, using an ob-\n",
      "jective function motivated by projection pursuit, see (Hyv¨arinen, 1999a) for details.\n",
      "Denote the weight vector corresponding to one feature detector by w, and the canon-\n",
      "ically preprocessed input by z. The goal is to ﬁnd the extrema of E{G(wTz)} for\n",
      "a given non-quadratic function G, under the constraint E{(wTz)2} = 1. According\n",
      "to the Lagrange conditions (Luenberger, 1969), the extrema are obtained at points\n",
      "where\n",
      "E{zg(wTz)} −βCw = 0\n",
      "(18.43)\n",
      "where C = E{zzT}, and β is a constant that can be easily evaluated to give β =\n",
      "E{wT\n",
      "0 zg(wT\n",
      "0 z)}, where w0 is the value of w at the optimum. Let us try to solve this\n",
      "equation by the classical Newton’s method, see Section 18.6.1 above. Denoting the\n",
      "function on the left-hand side of (18.43) by F, we obtain its Jacobian matrix, i.e. the\n",
      "matrix of partial derivatives, JF(w) as\n",
      "JF(w) = E{zzTg′(wT z)} −βC\n",
      "(18.44)\n",
      "To simplify the inversion of this matrix, we decide to approximate the ﬁrst term in\n",
      "(18.44). A reasonable approximation in this context seems to be E{zzTg′(wTz)} ≈\n",
      "E{zzT}E{g′(wTz)} = E{g′(wT z)}C. The obtained approximation of the Jacobian\n",
      "matrix can be inverted easily:\n",
      "\n",
      "18.7 FastICA, a ﬁxed-point algorithm for ICA\n",
      "413\n",
      "JF(w)−1 ≈C−1/(E{g′(wTz)} −β).\n",
      "(18.45)\n",
      "We also approximate β using the current value of w instead of w0. Thus we obtain\n",
      "the following approximative Newton iteration:\n",
      "w ←w−[C−1E{zg(wTz)} −βw]/[E{g′(wT z)} −β]\n",
      "(18.46)\n",
      "where w+ denotes the new value of w, and β = E{wTzg(wTz)}. After every step,\n",
      "w+ is normalized by dividing it by\n",
      "p\n",
      "(w+)TCw+ to improve stability. This algo-\n",
      "rithm can be further algebraically simpliﬁed (see (Hyv¨arinen, 1999a)) to obtain the\n",
      "original form the ﬁxed-point algorithm:\n",
      "w ←C−1E{zg(wTz)} −E{g′(wTz)}w.\n",
      "(18.47)\n",
      "These two forms are equivalent. Note that for whitened data, C−1 disappears, giving\n",
      "an extremely simple form of the Newton iteration. In (Hyv¨arinen and Oja, 1997),\n",
      "this learning rule was derived as a ﬁxed-point iteration of a gradient method max-\n",
      "imizing kurtosis, hence the name of the algorithm. However, image analysis must\n",
      "use the more general form in (Hyv¨arinen, 1999a) because of the non-robustness of\n",
      "kurtosis.\n",
      "18.7.3.2 Connection to gradient methods\n",
      "There is a simple an interesting connection between the FastICA algorithm and\n",
      "gradient algorithms for ICA.\n",
      "Let us assume that the number of independent components to be estimated equals\n",
      "the number of observed variables, i.e. n = m and A is square. Denote by W the\n",
      "estimate of the inverse of A.\n",
      "Now, consider the preliminary form of the algorithm in (18.46). To avoid the\n",
      "inversion of the covariance matrix, we can approximate it as C−1 ≈WTW, since\n",
      "C = AAT. Thus, collecting the updates for all the rows of W into a single equation,\n",
      "we obtain the following form of the ﬁxed-point algorithm:\n",
      "W ←W+D[diag(−βi)+E{g(y)y}]W\n",
      "(18.48)\n",
      "where y = Wz, βi = E{yig(yi)}, and D = diag(1/(βi −E{g′(yi)})). This can be\n",
      "compared to the natural gradient algorithm for maximization of the likelihood in\n",
      "Eq. (18.8). We can see that the algorithms are very closely related. First, the expec-\n",
      "tation in (18.48) is in practice computed as a sample average as in (18.8). So, the\n",
      "main difference is that in the natural gradient algorithm, the βi are all set to one,\n",
      "and D is replaced by identity times the step size µ. So, D is actually like a step size,\n",
      "although in the form of a matrix here, but it does not affect to the point where the\n",
      "algorithm converges (i.e. the update is zero). So, the only real difference is the βi.\n",
      "Now, it can be proven that if the g really is the derivative of the log-likelihood, then\n",
      "the βi are also (for inﬁnite sample) equal to one (Hyv¨arinen et al, 2001b). In theory,\n",
      "\n",
      "414\n",
      "18 Optimization theory and algorithms\n",
      "then, even this difference vanishes and the algorithms really converge to the same\n",
      "points.\n",
      "It must be noted that the FastICA algorithm does not maximize sparseness but\n",
      "non-gaussianity. Thus, in the case of sub-gaussian features, it may actually be min-\n",
      "imizing sparseness, see Section 7.9.3.\n",
      "\n",
      "Chapter 19\n",
      "Crash course on linear algebra\n",
      "This chapter explains basic linear algebra on a very elementary level. This is mainly\n",
      "meant as a reminder: The readers hopefully already know this material.\n",
      "19.1 Vectors\n",
      "A vector in an n-dimensional real space is an ordered collection of n real numbers.\n",
      "In this book, a vector is typically either the grey-scale values of pixels in an image\n",
      "patch, or the weights in a linear ﬁlter or feature detector. The number of pixels is\n",
      "n in the former case, and the number of weights is n in the latter case. We denote\n",
      "images by I(x,y) and the weights of a feature detector typically by W(x,y). It is\n",
      "assumed that the index x takes values from 1 to nx and the index y takes values from\n",
      "1 to ny, where the dimensions fulﬁll n = nx ×ny. In all the sums that follow, this is\n",
      "implicitly assumed and not explicitly written to simplify notation.\n",
      "One of the main points in linear algebra is to provide a notation in which many\n",
      "operations take a simple form. In linear algebra, the vectors such as I(x,y) and\n",
      "W(x,y) are expressed as one-dimensional columns or rows of numbers. Thus, we\n",
      "need to index all the pixels by a single index i that goes from 1 to n. This is obviously\n",
      "possible by scanning the image row by row, or column by column (see Section 4.1\n",
      "for details on such vectorization). It does not make any difference which method is\n",
      "used. A vector is usually expressed in column form as\n",
      "v =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "v1\n",
      "v2\n",
      "...\n",
      "vn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(19.1)\n",
      "In this book, the vector containing image data (typically after some preprocessing\n",
      "steps) will be usually denoted by z, and the vector giving the weights of a feature de-\n",
      "415\n",
      "\n",
      "416\n",
      "19 Crash course on linear algebra\n",
      "tector by v. In the following, we will use both the vector- and image-based notations\n",
      "side-by-side.\n",
      "The (Euclidean) norm of a vector is deﬁned as\n",
      "∥W(x,y)∥=\n",
      "r\n",
      "∑\n",
      "x,y\n",
      "W(x,y)2, or ∥v∥=\n",
      "r\n",
      "∑\n",
      "i\n",
      "v2\n",
      "i\n",
      "(19.2)\n",
      "The norm gives the length (or “size”) of a vector. There are also other ways of\n",
      "deﬁning the norm, but the Euclidean one is the most common.\n",
      "The dot-product (or inner product between two vectors is deﬁned as\n",
      "⟨W,I⟩= ∑\n",
      "x,y\n",
      "W(x,y)I(x,y)\n",
      "(19.3)\n",
      "If W is a feature detector, this could express the value of the feature when the input\n",
      "image is I. It basically computes a match between I and W. In vector notation, we\n",
      "use the transpose operator, given by vT, to express the same operation:\n",
      "vTz =\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "vizi\n",
      "(19.4)\n",
      "If the dot-product is zero, the vectors W and I are called orthogonal. The dot-\n",
      "product of a vector with itself equals the square of its norm.\n",
      "19.2 Linear transformations\n",
      "A linear transformation is the simplest kind of transformation in an n-dimensional\n",
      "vector space. A vector I is transformed to a vector J by taking weighted sums:\n",
      "J(x,y) = ∑\n",
      "x′y′\n",
      "m(x,y,x′,y′)I(x′,y′), for all x,y\n",
      "(19.5)\n",
      "The weights in the sum are different for every point (x,y). The indices x′ and y′ take\n",
      "all the same values as x and y. Typical linear transformations include smoothing and\n",
      "edge detection.\n",
      "We can compound linear transformations by taking a linear transformation of J\n",
      "using weights denoted by n(x,y,x′,y′). This gives the new vector as\n",
      "\n",
      "19.3 Matrices\n",
      "417\n",
      "K(x,y) = ∑\n",
      "x′′y′′\n",
      "n(x,y,x′′,y′′)J(x′′,y′′)\n",
      "= ∑\n",
      "x′′y′′\n",
      "n(x,y,x′′,y′′)∑\n",
      "x′y′\n",
      "m(x′′,y′′,x′,y′)I(x′,y′)\n",
      "= ∑\n",
      "x′y′\n",
      " \n",
      "∑\n",
      "x′′y′′\n",
      "n(x,y,x′′,y′′)m(x′′,y′′,x′,y′)\n",
      "!\n",
      "I(x′,y′)\n",
      "(19.6)\n",
      "Deﬁning\n",
      "p(x,y,x′,y′) = ∑\n",
      "x′′y′′\n",
      "n(x,y,x′′,y′′)m(x′′,y′′,x′,y′)\n",
      "(19.7)\n",
      "we see that the compounded transformation is a linear transformation with the\n",
      "weights given by p.\n",
      "19.3 Matrices\n",
      "In matrix algebra, linear transformations and linear systems of equations (see below)\n",
      "can be succinctly expressed by products (multiplications). In this book we avoid\n",
      "using too much linear algebra to keep things as simple as possible. However, it\n",
      "is necessary to understand how matrices are used to express linear transformation,\n",
      "because in some cases, the notation becomes just too complicated, and also because\n",
      "most numerical software takes matrices as input.\n",
      "A matrix M of size n1 ×n2 is a collection of real numbers arranged into n1 rows\n",
      "and n2 columns. The single entries are denoted by mij where i is the row and j is the\n",
      "column. We can convert the weights m(x,y,x′,y′) expressing a linear transformation\n",
      "by the same scanning process as was done with vectors. Thus,\n",
      "M =\n",
      "\n",
      "\n",
      "m11 m12 ... m1m\n",
      "...\n",
      "...\n",
      "mn1 mn2 ... mnm\n",
      "\n",
      "\n",
      "(19.8)\n",
      "The linear transformation of a vector z is then denoted by\n",
      "y = Mz\n",
      "(19.9)\n",
      "which is basically a short-cut notation for\n",
      "yi =\n",
      "n2\n",
      "∑\n",
      "j=1\n",
      "mijzj, for all i\n",
      "(19.10)\n",
      "This operation is also the deﬁnition of the product of a matrix and a vector.\n",
      "If we concatenate two linear transformations, deﬁning\n",
      "\n",
      "418\n",
      "19 Crash course on linear algebra\n",
      "s = Ny\n",
      "(19.11)\n",
      "we get another linear transformation. The matrix P that expresses this linear trans-\n",
      "formation is obtained by\n",
      "pij =\n",
      "n1\n",
      "∑\n",
      "k=1\n",
      "nikmk j\n",
      "(19.12)\n",
      "This is the deﬁnition of the product of two matrices: the new matrix P is denoted by\n",
      "P = MN\n",
      "(19.13)\n",
      "This is the matrix version of Equation (19.7). The deﬁnition is quite useful, because\n",
      "it means we can multiply matrices and vectors in any order when we compute s. In\n",
      "fact, we have\n",
      "s = Ny = N(Mz) = (NM)z\n",
      "(19.14)\n",
      "Another important operation with matrices is the transpose. The transpose MT\n",
      "of a matrix M is the matrix where the indices are exchanged: the i, j-th entry of MT\n",
      "is mji. A matrix M is called symmetric if mij = mji, i.e., if M equals its transpose.\n",
      "19.4 Determinant\n",
      "The determinant answers the question: how are volumes changed when the data\n",
      "space is transformed by the linear transformation m? That is, if I takes values in a\n",
      "cube whose edges are all of length one, what is the volume of the set of the values J\n",
      "in Equation (19.5)?. The answer is given by the absolute value of the determinant,\n",
      "denoted by |det(M)| where M is the matrix form of m.\n",
      "Two basic properties of the determinant are very useful.\n",
      "1. The determinant of a product is the product of the determinants: det(MN) =\n",
      "det(M)det(N). If you think that the ﬁrst transformation changes the volume by\n",
      "a factor or 2 and the second by a factor of 3, it is obvious that when you do both\n",
      "transformation, the change in volume is by a factor of 2 ×3 = 6.\n",
      "2. The determinant of a diagonal matrix equals the product of the diagonal elements.\n",
      "If you think is two dimensions, a diagonal matrix simply stretches one coordinate\n",
      "by a factor of, say 2, and the other coordinate by a factor of, say 3, so the volume\n",
      "of a square of area equal to 1 then becomes 2 ×3 = 6.\n",
      "(In Section 19.7 we will see a further important result on the determinant of an\n",
      "orthogonal matrix).\n",
      "\n",
      "19.6 Basis representations\n",
      "419\n",
      "19.5 Inverse\n",
      "If a linear transformation in Equation (19.5) does not change the dimension of the\n",
      "data, i.e. the number of pixels, the transformation can usually be inverted. That is,\n",
      "Equation (19.5) can usually be solved for I: if we know J and m, we can compute\n",
      "what was the original I. This is the case if the linear transformation is invertible — a\n",
      "technical condition that is almost always true. In this book, we will always assume\n",
      "that a linear transformation is invertible if not otherwise mentioned.\n",
      "In fact, we can then ﬁnd a matrix of coefﬁcients n(x,y), so that\n",
      "I(x,y) = ∑\n",
      "x′y′\n",
      "n(x,y,x′,y′)J(x′,y′), for all x,y.\n",
      "(19.15)\n",
      "This is the inverse transformation of m. In matrix algebra, the coefﬁcient are ob-\n",
      "tained by computing the inverse of the matrix M, denoted by M−1. So, solving for\n",
      "y in (19.9) we have\n",
      "y = M−1z\n",
      "(19.16)\n",
      "A multitude of numerical methods for computing the inverse of the matrix exist.\n",
      "Note that the determinant of the inverse matrix is simply the inverse of the deter-\n",
      "minant: det(M−1) = 1/det(M). Logically, if the transformation changes the volume\n",
      "by a factor of 5 (say), then the inverse must change the volume by a factor of 1/5.\n",
      "The product of a matrix with its inverse equals the identity matrix I:\n",
      "MM−1 = M−1M = I\n",
      "(19.17)\n",
      "The identity matrix is a matrix whose diagonal elements are all ones and the off-\n",
      "diagonal elements are all zero. It corresponds to the idenity transformation, i.e., a\n",
      "transformation which does not change the vector. This means we have\n",
      "Iz = z\n",
      "(19.18)\n",
      "for any z.\n",
      "19.6 Basis representations\n",
      "An important interpretation of the mathematics in the preceding sections is the rep-\n",
      "resentation of an image in a basis. Assume we have a number of features Ai(x,y)\n",
      "where i goes from 1 to n. Given an image I(x,y), we want to represent it as a linear\n",
      "sum of these feature vectors:\n",
      "I(x,y) =\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "Ai(x,y)si\n",
      "(19.19)\n",
      "\n",
      "420\n",
      "19 Crash course on linear algebra\n",
      "The si are the coefﬁcients of the feature vectors Ai. They can be considered as the\n",
      "values of the features in the image I, since they tell “to what extent” the features are\n",
      "in the image. If, for example, s1 = 0, that means that the feature A1 is not present in\n",
      "the image.\n",
      "Using vector notation, the basis representation can be given as\n",
      "z =\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "aisi\n",
      "(19.20)\n",
      "Interestingly, this equation can be further simpliﬁed by putting all the si into a single\n",
      "vector s, and forming a matrix A so that the columns of that matrix are the vectors\n",
      "ai, that is:\n",
      "A = [a1,a2,...,an] =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a11\n",
      "...\n",
      "an1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a12\n",
      "...\n",
      "an2\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "a1n\n",
      "...\n",
      "an3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(19.21)\n",
      "Then we have equivalently\n",
      "z = As\n",
      "(19.22)\n",
      "From this equation, we see how we can apply all the linear algebra machinery to\n",
      "answer the following questions:\n",
      "• How do we compute the coefﬁcients si? This is done by computing the inverse\n",
      "matrix of A (hoping that one exists), and then multiplying z with the inverse,\n",
      "since s = A−1z.\n",
      "• When is it possible to represent any z using the given ai? This question was\n",
      "already posed in the preceding section. The answer is: if the number of basis\n",
      "vectors equals the dimension of z, the matrix A is invertible practically always.\n",
      "In such a case, we say that the ai (or the Ai) form a basis.\n",
      "A further important question is: What happens then if the number of vectors ai is\n",
      "smaller than the dimension of the vector z? Then, we cannot represent all the possi-\n",
      "ble z’s using those features. However, we can ﬁnd the best possible approximation\n",
      "for any z based on those features, which is treated in Section 19.8.\n",
      "The opposite case is when we have more vectors ai than the dimension of the\n",
      "data. Then, we can represent any vector z using those features; in fact, there are\n",
      "usually many ways of representing any z, and the coefﬁcients si are not uniquely\n",
      "deﬁned. This case is called overcomplete basis and treated in Section 13.1.\n",
      "19.7 Orthogonality\n",
      "A linear transformation is called orthogonal if it does not change the norm of the\n",
      "vector. Likewise, a matrix A is called orthogonal if the corresponding transformation\n",
      "is orthogonal. An equivalent condition for orthogonality is\n",
      "\n",
      "19.8 Pseudo-inverse *\n",
      "421\n",
      "ATA = I\n",
      "(19.23)\n",
      "If you think about the meaning of this equation in detail, you will realize that it says\n",
      "two things: the column vectors of the matrix A are orthogonal, and all normalized to\n",
      "unit norm. This is because the entries in the matrix AT A are the dot-products aT\n",
      "i a j\n",
      "between the column vectors of the matrix A.\n",
      "An orthogonal basis is nothing else than a basis in which the basis vectors are\n",
      "orthogonal and have unit norm; in other words, if we collect the basis vectors into a\n",
      "matrix as in Equation (19.21), that matrix is orthogonal.\n",
      "Equation (19.23) shows that the inverse of an orthogonal matrix (or an orthogonal\n",
      "transformation) is trivial to compute: we just need to rearrange the entries by taking\n",
      "the transpose. This means that si = aT\n",
      "i z, or\n",
      "si = ∑\n",
      "x,y\n",
      "Ai(x,y)I(x,y)\n",
      "(19.24)\n",
      "So, in an orthogonal basis we obtain the coefﬁcients as simple dot-products with the\n",
      "basis vectors. Note that this is not true unless the basis is orthogonal.\n",
      "The compound transformation of two orthogonal transformation is orthogonal.\n",
      "This is natural since if neither of the transformations changes the norm of the image,\n",
      "then doing one transformation after the other does not change the norm either.\n",
      "The determinant of an orthogonal matrix is equal to plus or minus one. This\n",
      "is because because an orthogonal transformation does not change volumes, so the\n",
      "absolute value has to be one. The change in sign is related to reﬂections. Think of\n",
      "multiplying one-dimenasional data by −1: This does not change the “volumes”, but\n",
      "“reﬂects” the data with respect to 0, and corresponds to a determinant of −1.\n",
      "19.8 Pseudo-inverse *\n",
      "Sometimes transformations change the dimension, and the inversion is more com-\n",
      "plicated. If there are more variables in y than in z in Equation (19.9), there are\n",
      "basically more equations than free variables, so there is no solution in general. That\n",
      "is, we cannot ﬁnd a matrix ˜M so that for any given y, z = ˜My is a solution for\n",
      "Equation (19.9). However, in many cases it is useful to consider an approximative\n",
      "solution: Find a matrix M+ so that for z = M+y, the error ∥y −Mz∥is as small\n",
      "as possible. In this case, the optimal “approximative inverse” matrix can be easily\n",
      "computed as:\n",
      "M+ = (MTM)−1MT\n",
      "(19.25)\n",
      "On the other hand, if the matrix M has fewer rows than columns (fewer variables\n",
      "in y than in z), there are more free variables than there are constraining equations.\n",
      "Thus, there are many solutions z for (19.9) for a given y, and we have to choose one\n",
      "of them. One option is to choose the solution that has the smallest Euclidean norm.\n",
      "The matrix that gives this solution as M+y is given by\n",
      "\n",
      "422\n",
      "19 Crash course on linear algebra\n",
      "M+ = MT(MMT )−1\n",
      "(19.26)\n",
      "The matrix M+ in both of these cases is called the (Moore-Penrose) pseudo-inverse\n",
      "of M. (A more sophisticated solution for the latter case, using sparseness, is consid-\n",
      "ered in Section 13.1.3.)\n",
      "\n",
      "Chapter 20\n",
      "The discrete Fourier transform\n",
      "This chapter is a mathematically sophisticated treatment of the theory of Fourier\n",
      "analysis. It concentrates on the discrete Fourier transform which is the variant used\n",
      "in image analysis practice.\n",
      "It is not necessary to know this material to understand the developments in this\n",
      "book; this is meant as supplementary material.\n",
      "20.1 Linear shift-invariant systems\n",
      "Let us consider a system H operating on one-dimensional input signals I(x). The\n",
      "system is linear if for inputs I1(x) and I2(x), and scalar α\n",
      "H {I1(x)+I2(x)} = H {I1(x)} +H {I2(x)}\n",
      "(20.1)\n",
      "H {αI1(x)} = αH {I1(x)};\n",
      "(20.2)\n",
      "similar deﬁnitions apply in the two-dimensional case. A system H is shift-invariant\n",
      "if a shift in the input results in a shift of the same size in the output; that is, if\n",
      "H {I(x)} = O(x), then for any integer m\n",
      "H {I(x+m)} = O(x+m);\n",
      "(20.3)\n",
      "or, in the two-dimensional case, for any integers m and n,\n",
      "H {I(x+m,y+n)} = O(x+m,y+n).\n",
      "(20.4)\n",
      "A linear shift-invariant system H operating on signals (or, in the two-dimensional\n",
      "case, on images) can be implemented by either linear ﬁltering with a ﬁlter, or another\n",
      "operation, the convolution of the input and the impulse response of the system. The\n",
      "impulse response H(x) is the response of the system to an impulse\n",
      "423\n",
      "\n",
      "424\n",
      "20 The discrete Fourier transform\n",
      "δ(x) =\n",
      "(\n",
      "1,\n",
      "if x = 0\n",
      "0,\n",
      "otherwise,\n",
      "(20.5)\n",
      "that is\n",
      "H(x) = H {δ(x)}.\n",
      "(20.6)\n",
      "By noting that I(x) = ∑∞\n",
      "k=−∞I(k)δ(x −k), and by applying linearity and shift-\n",
      "invariance properties (Equations (20.1)–(20.3)) it is easy to show that\n",
      "O(x) = H {I(x)} =\n",
      "∞\n",
      "∑\n",
      "k=−∞\n",
      "I(k)H(x−k) = I(x)∗H(x),\n",
      "(20.7)\n",
      "where the last equality sign deﬁnes convolution ∗. Note that convolution is a sym-\n",
      "metric operator since by making the change in summation index ℓ= x−k (implying\n",
      "k = x−ℓ)\n",
      "I(x)∗H(x) =\n",
      "∞\n",
      "∑\n",
      "k=−∞\n",
      "I(k)H(x−k) =\n",
      "∞\n",
      "∑\n",
      "ℓ=−∞\n",
      "H(ℓ)I(x−ℓ) = H(x)∗I(x).\n",
      "(20.8)\n",
      "20.2 One-dimensional discrete Fourier transform\n",
      "20.2.1 Euler’s formula\n",
      "For purposes of mathematical convenience, in Fourier analysis the frequency rep-\n",
      "resentation is complex-valued: both the basis images and the weights consist of\n",
      "complex numbers; this is called the representation of an image in the Fourier space.\n",
      "The fundamental reason for this is Euler’s formula, which states that\n",
      "eai = cosa +isina\n",
      "(20.9)\n",
      "where i is the imaginary unit. Thus, a complex exponential contains both the sin and\n",
      "cos function in a way that turns out to be algebraically very convenient. One of the\n",
      "basic reasons for this is that the absolute value of a complex number contains the\n",
      "sum-of-squares operation:\n",
      "|a +bi| =\n",
      "p\n",
      "a2 +b2\n",
      "(20.10)\n",
      "which is related to the formula in Equation (2.16) on page 41 which gives the power\n",
      "of a sinusoidal component. We will see below that we can indeed compute the\n",
      "Fourier power as the absolute value (modulus) of some complex numbers.\n",
      "In fact, we will see that the argument of a complex number on the complex plane\n",
      "is related to the phase in signal processing. The argument of a complex number c is\n",
      "a real number φ ∈(−π,π] such that\n",
      "c = |c|eφi\n",
      "(20.11)\n",
      "\n",
      "20.2 One-dimensional discrete Fourier transform\n",
      "425\n",
      "We will use here the signal-processing notation ∠c for the argument.\n",
      "We will also use the complex conjugate of a complex number c = a+bi, denoted\n",
      "by ¯c, which can be obtained either as a −bi or, equivalently, as |c|e−φi. Thus, the\n",
      "complex conjugate has the same absolute value, but opposite argument (“phase”).\n",
      "20.2.2 Representation in complex exponentials\n",
      "In signal processing theory, sinusoidals are usually represented in the form of the\n",
      "following complex exponential signal\n",
      "eiωx = cos(ωx)+isin(ωx),\n",
      "x = 1,...,M,\n",
      "(20.12)\n",
      "A fundamental mathematical reason for this is that these signals are eigensignals\n",
      "of linear shift-invariant systems. An eigensignal is a generalization of the concept\n",
      "of an eigenvector in linear algebra (see Section 5.8.1). Denote by H(x) the impulse\n",
      "response of a linear shift-invariant system H . Then\n",
      "H\n",
      "\b\n",
      "eiωx\t\n",
      "= H(x)∗eiωx =\n",
      "∞\n",
      "∑\n",
      "k=−∞\n",
      "H(k)eiω(x−k) = eiωx\n",
      "∞\n",
      "∑\n",
      "k=−∞\n",
      "H(k)e−iωk\n",
      "|\n",
      "{z\n",
      "}\n",
      "= ˜H(ω)\n",
      "= ˜H(ω)eiωx,\n",
      "(20.13)\n",
      "where we have assumed that the sum ∑∞\n",
      "k=−∞H(k)e−iωk converges, and have de-\n",
      "noted this complex number by ˜H(ω). Equation (20.13) shows that when a complex\n",
      "exponential is input into a linear shift-invariant system, the output is the same com-\n",
      "plex exponential multiplied by ˜H(ω); the complex exponential is therefore called\n",
      "an eigensignal of the system.\n",
      "To illustrate the usefulness of the representation in complex exponentials in an-\n",
      "alytic calculations, let us derive the response of a linear shift-invariant system to a\n",
      "sinusoidal. This derivation uses the identity\n",
      "cos(φ) = 1\n",
      "2\n",
      "\u0000eiφ +e−iφ\u0001\n",
      ",\n",
      "(20.14)\n",
      "which can be veriﬁed by applying Equation (20.12). Let H be a linear shift-\n",
      "invariant system, and Acos(ωx+ψ) be an input signal; then\n",
      "H {Acos(ωx+ψ)} = A\n",
      "2 H\n",
      "n\n",
      "ei(ωx+ψ) +e−i(ωx+ψ)o\n",
      "= A\n",
      "2\n",
      "\u0000eiψH\n",
      "\b\n",
      "eiωx\t\n",
      "+e−iψH\n",
      "\b\n",
      "e−iωx\t\u0001\n",
      "= A\n",
      "2\n",
      "\u0000eiψ ˜H(ω)eiωx +e−iψ ˜H(−ω)e−iωx\u0001\n",
      ".\n",
      "(20.15)\n",
      "\n",
      "426\n",
      "20 The discrete Fourier transform\n",
      "By the deﬁnition of ˜H(ω) (see Equation 20.13), ˜H(−ω) = ˜H(ω) =\n",
      "\f\f ˜H(ω)\n",
      "\f\fe−i∠˜H(ω).\n",
      "Thus\n",
      "H {Acos(ωx+ψ)} =\n",
      "\f\f ˜H(ω)\n",
      "\f\fA1\n",
      "2\n",
      "\u0010\n",
      "ei(ωx+ψ+∠˜H(ω)) +e−i(ωx+ψ+∠˜H(ω))\u0011\n",
      "=\n",
      "\f\f ˜H(ω)\n",
      "\f\fA\n",
      "|\n",
      "{z\n",
      "}\n",
      "amplitude\n",
      "cos(ωx+ψ +∠˜H(ω)\n",
      "|\n",
      "{z\n",
      "}\n",
      "phase\n",
      ").\n",
      "(20.16)\n",
      "Equation (20.16) is a one-dimensional formal version of the two statements made\n",
      "in Section 2.2.3 (page 33):\n",
      "• When a sinusoidal is input into a linear shift-invariant system, the output is a\n",
      "sinusoidal with the same frequency.\n",
      "• The change in amplitude and phase depend only on the frequency ω.\n",
      "Furthermore, the equation contains another important result, namely that both the\n",
      "amplitude and the phase response can be read out from ˜H(ω) : the amplitude re-\n",
      "sponse is\n",
      "\f\f ˜H(ω)\n",
      "\f\f, and the phase response ∠˜H(ω). This also explains the notation\n",
      "introduced for amplitude and phase responses on page 33.\n",
      "In order to examine further the use of complex exponentials eiωx, let us derive\n",
      "the representation of a real-valued signal in terms of these complex-valued signals.\n",
      "Thus, the imaginary parts have to somehow disappear in the ﬁnal representation. De-\n",
      "riving such a representation from the representation in sinusoidals (Equation (2.6),\n",
      "page 30) can be done by introducing negative frequencies ω < 0, and using Equa-\n",
      "tion (20.14). Let us denote the coefﬁcient of the complex exponential eiωx by ˜I∗(ω).\n",
      "The representation can be calculated as follows:\n",
      "I(x) =\n",
      "ωM\n",
      "∑\n",
      "ω=0\n",
      "Aω cos(ωx+ψω) =\n",
      "ωM\n",
      "∑\n",
      "ω=0\n",
      "Aω\n",
      "2\n",
      "\u0010\n",
      "ei(ωx+ψω) +e−i(ωx+ψω)\u0011\n",
      "=\n",
      "ωM\n",
      "∑\n",
      "ω=0\n",
      "Aω\n",
      "2\n",
      "\u0010\n",
      "eiψω eiωx +ei(−ψω)ei(−ω)x\u0011\n",
      "= A0\n",
      "|{z}\n",
      "=˜I∗(0)\n",
      "+\n",
      "ωM\n",
      "∑\n",
      "ω=−ωM\n",
      "ω̸=0\n",
      "A|ω|\n",
      "2 eisgn(ω)ψ|ω|\n",
      "|\n",
      "{z\n",
      "}\n",
      "=˜I∗(ω)\n",
      "when ω ̸= 0\n",
      "eiωx =\n",
      "ωM\n",
      "∑\n",
      "ω=−ωM\n",
      "˜I∗(ω)eiωx.\n",
      "(20.17)\n",
      "Note the following properties of the coefﬁcients ˜I∗(ω) :\n",
      "• In general, the coefﬁcients ˜I∗(ω) are complex-valued, except for ˜I∗(0) which is\n",
      "always real.\n",
      "• For ω ≥0, a coefﬁcient ˜I∗(ω) contains the information about both the amplitude\n",
      "and the phase of the sinusoidal representation – amplitude information is given\n",
      "by the magnitude\n",
      "\f\f˜I∗(ω)\n",
      "\f\f and phase information by the angle ∠˜I∗(ω) :\n",
      "\n",
      "20.2 One-dimensional discrete Fourier transform\n",
      "427\n",
      "Aω =\n",
      "(\n",
      "˜I∗(0),\n",
      "if ω = 0\n",
      "2\n",
      "\f\f˜I∗(ω)\n",
      "\f\f,\n",
      "otherwise\n",
      "(20.18)\n",
      "ψω =\n",
      "(\n",
      "undeﬁned,\n",
      "if ω = 0\n",
      "∠˜I∗(ω),\n",
      "otherwise.\n",
      "(20.19)\n",
      "• A closer look at the derivation (20.17) shows that the magnitude and the angle of\n",
      "the positive and negative frequencies are related to each other as follows:\n",
      "\f\f˜I∗(−ω)\n",
      "\f\f =\n",
      "\f\f˜I∗(ω)\n",
      "\f\f\n",
      "(20.20)\n",
      "∠˜I∗(−ω) = −∠˜I∗(ω)\n",
      "(20.21)\n",
      "Thus ˜I∗(w) and ˜I∗(−w) form a complex-conjugate pair. This also means that\n",
      "knowing only the coefﬁcients of the positive frequencies – or only the coefﬁ-\n",
      "cients of the negative frequencies – is sufﬁcient to reconstruct the whole rep-\n",
      "resentation. (This is true only for real-valued signals; if one wants to represent\n",
      "complex-valued signals, the whole set of coefﬁcients is needed. However, such\n",
      "representations are not needed in this book.)\n",
      "Above it was assumed that a frequency-based representation of a signal I(x) ex-\n",
      "ists:\n",
      "I(x) =\n",
      "ωM\n",
      "∑\n",
      "ω=0\n",
      "Aω cos(ωx+ψω).\n",
      "(20.22)\n",
      "From that we derived a representation in complex exponentials\n",
      "I(x) =\n",
      "ωM\n",
      "∑\n",
      "ω=−ωM\n",
      "˜I∗(ω)eiωx\n",
      "(20.23)\n",
      "˜I∗(ω) =\n",
      "(\n",
      "A0\n",
      "when ω = 0\n",
      "A|ω|\n",
      "2 eisgn(ω)ψ|ω|\n",
      "otherwise.\n",
      "(20.24)\n",
      "This derivation can be reversed: assuming that a representation in complex expo-\n",
      "nentials exists – so that the coefﬁcients of the negative and positive frequencies are\n",
      "complex-conjugate pairs – a frequency-based representation also exists:\n",
      "I(x) =\n",
      "ωM\n",
      "∑\n",
      "ω=−ωM\n",
      "˜I∗(ω)eiωx =\n",
      "ωM\n",
      "∑\n",
      "ω=−ωM\n",
      "\f\f˜I∗(ω)\n",
      "\f\fei(ωx+∠˜I∗(ω))\n",
      "= ˜I∗(0)+\n",
      "ωM\n",
      "∑\n",
      "ω=ω1\n",
      "2\n",
      "\f\f˜I∗(ω)\n",
      "\f\fcos\n",
      "\u0000ωx+∠˜I∗(ω)\n",
      "\u0001\n",
      ".\n",
      "(20.25)\n",
      "\n",
      "428\n",
      "20 The discrete Fourier transform\n",
      "20.2.3 The discrete Fourier transform and its inverse\n",
      "Next, we introduce the discrete Fourier transform (DFT) and its inverse, which are\n",
      "the tools that are used in practice to convert signals to their representation in com-\n",
      "plex exponentials and back. We will ﬁrst give a deﬁnition of the transforms, and\n",
      "then relate the properties of these transforms to the discussion we had above.\n",
      "The word “discrete” refers here to the fact that the signal (or image) is sampled\n",
      "at a discrete set of points, i.e. the index x is not continuous. This is in contrast to\n",
      "the general mathematical deﬁnition of the Fourier transform which is deﬁned for\n",
      "functions which take values in a real-valued space. Another point is that the DFT\n",
      "is in a sense closer to what is called the Fourier series in mathematics because the\n",
      "set of frequencies used is discrete as well. Thus, the theory of DFT has a number of\n",
      "differences to the general mathematical deﬁnitions used in differential calculus.\n",
      "The discrete Fourier transformation is used to compute the coefﬁcients of the\n",
      "signal’s representation in complex exponentials: this set of coefﬁcients is called the\n",
      "discrete Fourier transform. The inverse discrete Fourier transformation (IDFT) is\n",
      "used to compute the signal from its representation in complex exponentials. Let\n",
      "I(x) be a signal of length N. The discrete Fourier transform pair is deﬁned by\n",
      "DFT:\n",
      "˜I(u) =\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "I(x)e−i 2πx\n",
      "N u, u = 0,...,N −1.\n",
      "(20.26)\n",
      "IDFT:\n",
      "I(x) = 1\n",
      "N\n",
      "N−1\n",
      "∑\n",
      "u=0\n",
      "˜I(u)ei 2πu\n",
      "N x, x = 0,...,N −1,\n",
      "(20.27)\n",
      "Notice that the frequencies utilized in the representation in complex exponentials of\n",
      "the IDFT (20.27) are\n",
      "ωu = 2πu\n",
      "N , u = 0,...,N −1.\n",
      "(20.28)\n",
      "The fact that Equations (20.27) and (20.26) form a valid transform pair – that\n",
      "is, that the IDFT of ˜I(k) is I(x) – can be shown as follows. Let ˜I(u) be deﬁned as\n",
      "in Equation (20.26). Then – redeﬁning the sum in Equation (20.26) to be over x∗\n",
      "instead of x to avoid using the same index twice – the IDFT gives\n",
      "1\n",
      "N\n",
      "N−1\n",
      "∑\n",
      "u=0\n",
      "˜I(u)ei 2πu\n",
      "N x = 1\n",
      "N\n",
      "N−1\n",
      "∑\n",
      "u=0\n",
      " \n",
      "N−1\n",
      "∑\n",
      "x∗=0\n",
      "I(x∗)e−i 2πx∗\n",
      "N u\n",
      "!\n",
      "ei 2πu\n",
      "N x\n",
      "= 1\n",
      "N\n",
      "N−1\n",
      "∑\n",
      "x∗=0\n",
      "I(x∗)\n",
      " \n",
      "N−1\n",
      "∑\n",
      "u=0\n",
      "ei 2πu(x−x∗)\n",
      "N\n",
      "!\n",
      "= 1\n",
      "N\n",
      "N−1\n",
      "∑\n",
      "x∗=0\n",
      "I(x∗)\n",
      "\"\n",
      "N−1\n",
      "∑\n",
      "u=0\n",
      "\u0010\n",
      "ei 2π(x−x∗)\n",
      "N\n",
      "\u0011u#\n",
      "|\n",
      "{z\n",
      "}\n",
      "term A\n",
      ".\n",
      "(20.29)\n",
      "\n",
      "20.2 One-dimensional discrete Fourier transform\n",
      "429\n",
      "If x∗= x, then term A in Equation (20.29) equals N; when x∗̸= x, the value of this\n",
      "geometric sum is\n",
      "N−1\n",
      "∑\n",
      "u=0\n",
      "\u0010\n",
      "ei 2π(x−x∗)\n",
      "N\n",
      "\u0011u\n",
      "=\n",
      "=1\n",
      "z\n",
      "}|\n",
      "{\n",
      "\u0010\n",
      "ei 2π(x−x∗)\n",
      "N\n",
      "\u0011N\n",
      "−1\n",
      "ei 2π(x−x∗)\n",
      "N\n",
      "−1\n",
      "= 0.\n",
      "(20.30)\n",
      "Therefore, the IDFT gives\n",
      "1\n",
      "N\n",
      "N−1\n",
      "∑\n",
      "u=0\n",
      "˜I(u)ei 2πu\n",
      "N x = 1\n",
      "N I(x)N = I(x).\n",
      "(20.31)\n",
      "We now discuss several of the properties of the discrete Fourier transform pair.\n",
      "Negative frequencies and periodicity in the DFT\n",
      "The representation in complex\n",
      "exponentials in the DFT employs the following frequencies:\n",
      "ωu = 2πu\n",
      "N , u = 0,...,N −1.\n",
      "(20.32)\n",
      "In the previous section we discussed the use of negative frequencies in represen-\n",
      "tations based on complex exponentials. At ﬁrst sight it looks like no negative fre-\n",
      "quencies are utilized in the DFT. However, the representation used by the DFT\n",
      "(Equation 20.27) is periodic: as a function of the frequency index u, both the\n",
      "complex exponentials and their coefﬁcients have a period of N. That is, for any\n",
      "integer ℓ, for the complex exponentials we have\n",
      "ei 2π(u+ℓN)\n",
      "N\n",
      "x = ei 2πu\n",
      "N x ei2πℓx\n",
      "| {z }\n",
      "=1\n",
      "= ei 2πu\n",
      "N x,\n",
      "(20.33)\n",
      "and for the coefﬁcients\n",
      "˜I(u +ℓN) =\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "I(x)e−i 2πx\n",
      "N (u+ℓN) =\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "I(x)e−i 2πx\n",
      "N u e−i2πuℓ\n",
      "| {z }\n",
      "=1\n",
      "=\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "I(x)e−i 2πx\n",
      "N u = ˜I(u).\n",
      "(20.34)\n",
      "Therefore, for example, the coefﬁcient ˜I(N −1) corresponding to frequency\n",
      "2π(N−1)\n",
      "N\n",
      "is the same as the coefﬁcient ˜I(−1) corresponding to frequency −2π\n",
      "N\n",
      "would be. In general, the latter half of the DFT can be considered to correspond\n",
      "to the negative frequencies. To be more precise, for a real-valued I(x), the DFT\n",
      "equivalent of the complex-conjugate relationships (20.20) and (20.21) is\n",
      "\n",
      "430\n",
      "20 The discrete Fourier transform\n",
      "˜I(N −u) =\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "I(x)e−i 2πx\n",
      "N (N−u) =\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "I(x)e−i2πx\n",
      "| {z }\n",
      "=1\n",
      "ei 2πx\n",
      "N u\n",
      "=\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "I(x)e−i 2πx\n",
      "N u = ˜I(u).\n",
      "(20.35)\n",
      "This relation also explains why the DFT seems to have “too many numbers” for\n",
      "real-valued signals. It consists of N complex-valued numbers, which seems to\n",
      "contain twice the amount of information as the original signal, which has N real-\n",
      "valued numbers. The reason is that half the information in DFT is redundant, due\n",
      "to the relation in Equation (20.35). For example, if you know all the values of\n",
      "˜I(u) for u from 0 to (N −1)/2 (assuming N is odd), you can compute all the rest\n",
      "by just taking complex conjugates.\n",
      "Periodicity of the IDFT and the convolution theorem\n",
      "The\n",
      "Fourier-transform\n",
      "pair implicitly assumes that the signal I(x) is periodic: applying a derivation sim-\n",
      "ilar to (20.34) to the IDFT (20.27) gives\n",
      "I(x+ℓN) = I(x).\n",
      "(20.36)\n",
      "This assumption of periodicity is also important for perhaps the most important\n",
      "mathematical statement about the discrete Fourier transform, namely the convo-\n",
      "lution theorem. Loosely speaking, the convolution theorem states that the Fourier\n",
      "transform of the convolution of two signals is the product of the discrete Fourier\n",
      "transforms of the signals. To be more precise, we have to take border effects into\n",
      "account, i.e. what happens near the beginning and the end of signals, and this is\n",
      "where the periodicity comes into play.\n",
      "Now we shall derive the convolution theorem. Let I(x) and H(x) be two signals of\n",
      "the same length N (if they initially have different lengths, one of them can always\n",
      "be extended by “padding” zeros, i.e. adding a zero signal the end). Denote by ˜I(u)\n",
      "and ˜H(u) the Fourier transforms of the signals. Then the product of the Fourier\n",
      "transforms is\n",
      "˜H(u)˜I(u) =\n",
      " \n",
      "N−1\n",
      "∑\n",
      "ℓ=0\n",
      "H(ℓ)e−i2πℓu/N\n",
      "! \n",
      "N−1\n",
      "∑\n",
      "k=0\n",
      "I(k)e−i2πku/N\n",
      "!\n",
      "=\n",
      "N−1\n",
      "∑\n",
      "ℓ=0\n",
      "N−1\n",
      "∑\n",
      "k=0\n",
      "H(ℓ)I(k)e−i2π(ℓ+k)u/N.\n",
      "(20.37)\n",
      "Making a change of index x = ℓ+k yields\n",
      "\n",
      "20.2 One-dimensional discrete Fourier transform\n",
      "431\n",
      "˜H(u)˜I(u) =\n",
      "N−1\n",
      "∑\n",
      "ℓ=0\n",
      "ℓ+N−1\n",
      "∑\n",
      "x=ℓ\n",
      "H(ℓ)I(x−ℓ)e−i2πxu/N\n",
      "=\n",
      "N−1\n",
      "∑\n",
      "ℓ=0\n",
      "\u0014N−1\n",
      "∑\n",
      "x=ℓ\n",
      "H(ℓ)I(x−ℓ)e−i2πxu/N\n",
      "+\n",
      "ℓ+N−1\n",
      "∑\n",
      "x=N\n",
      "H(ℓ)I(x−ℓ)e−i2πxu/N\n",
      "|\n",
      "{z\n",
      "}\n",
      "sum A\n",
      "\u0015\n",
      ".\n",
      "(20.38)\n",
      "If we assume that I(x) is periodic with a period of N, then what has been denoted\n",
      "by sum A in equation (20.38) can be made simpler: since in that sum H(ℓ) is\n",
      "constant and e−i2πxu/N is periodic with a period of N, the lower and upper limits\n",
      "in the sum can simply be changed to 0 and ℓ−1, respectively, yielding\n",
      "˜H(u)˜I(u)=\n",
      "N−1\n",
      "∑\n",
      "ℓ=0\n",
      "\u0014N−1\n",
      "∑\n",
      "x=ℓ\n",
      "H(ℓ)I(x−ℓ)e−i2πxu/N\n",
      "+\n",
      "ℓ−1\n",
      "∑\n",
      "x=0\n",
      "H(ℓ)I(x−ℓ)e−i2πxu/N\n",
      "\u0015\n",
      "=\n",
      "N−1\n",
      "∑\n",
      "ℓ=0\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "H(ℓ)I(x−ℓ)e−i2πxu/N\n",
      "=\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "N−1\n",
      "∑\n",
      "ℓ=0\n",
      "H(ℓ)I(x−ℓ)\n",
      "|\n",
      "{z\n",
      "}\n",
      "=O(x)\n",
      "e−i2πxu/N =\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "O(x)e−i2πxu/N\n",
      "= ˜O(u),\n",
      "(20.39)\n",
      "where ˜O(u) is the discrete Fourier transform of O(x). Notice that O(x) is obtained\n",
      "as a convolution of H(x) and I(x) under the assumption of periodicity. That is,\n",
      "we deﬁne the values of the signal outside of its actual range by assuming that it is\n",
      "periodic. (If we want to use the basic deﬁnition of convolution, we actually have\n",
      "to deﬁne the values of the signal up to inﬁnite values of the indices, because the\n",
      "deﬁnition assumes that the signals have inﬁnite length.) We call such an operation\n",
      "cyclic convolution.\n",
      "Equation (20.39) proves the cyclic version of the convolution theorem: the DFT\n",
      "of the cyclic convolution of two signals is the product of the DFTs of the signals.\n",
      "Note that the assumption of cyclicity is not needed in the general continuous-\n",
      "space version of the convolution theorem; it is a special property of the discrete\n",
      "transform.\n",
      "In practice, when computing the convolution of two ﬁnite-length signals, the\n",
      "deﬁnition of cyclic convolution is often not what one wants, because it means that\n",
      "values of the signals near x = 0 can have an effect on the values of the convolution\n",
      "near x = N −1. In most cases, one would like to deﬁne the convolution so that\n",
      "the effect of ﬁnite length is more limited. Usually, this is done by modifying the\n",
      "\n",
      "432\n",
      "20 The discrete Fourier transform\n",
      "signals so that the difference between cyclic convolution and other ﬁnite-length\n",
      "versions disappear. For example, this can lead to adding (“padding”) zeros at the\n",
      "edges. Such zero-padding makes it simple to compute convolutions using DFT’s,\n",
      "which is usually much faster than using the deﬁnition.1\n",
      "Real- and complex-valued DFT coefﬁcients\n",
      "In general, the coefﬁcients ˜I(u) are\n",
      "complex-valued, except for ˜I(0) which is always real-valued. However, if the\n",
      "signal has an even length so that N\n",
      "2 is an integer, then\n",
      "˜I\n",
      "\u0012N\n",
      "2\n",
      "\u0013\n",
      "= ˜I\n",
      "\u0012\n",
      "N −N\n",
      "2\n",
      "\u0013\n",
      "= ˜I\n",
      "\u0012N\n",
      "2\n",
      "\u0013\n",
      ",\n",
      "(20.40)\n",
      "where in the last step we have applied Equation (20.35). Therefore, when N is\n",
      "even, ˜I\n",
      "\u0000 N\n",
      "2\n",
      "\u0001\n",
      "is also real-valued.\n",
      "The sinusoidal representation from the DFT\n",
      "If N is odd, then starting from\n",
      "Equation (20.27), a derivation similar to (20.25) gives\n",
      "I(x) =\n",
      "˜I(0)\n",
      "N\n",
      "|{z}\n",
      "=A0\n",
      "+\n",
      "N−1\n",
      "2\n",
      "∑\n",
      "u=1\n",
      "2\n",
      "\f\f˜I(u)\n",
      "\f\f\n",
      "N\n",
      "| {z }\n",
      "=Au\n",
      "when u ̸= 0\n",
      "cos\n",
      "\u0012\n",
      "2πu\n",
      "N\n",
      "|{z}\n",
      "=ωu\n",
      "when u ̸= 0\n",
      "x+ ∠˜I(u)\n",
      "| {z }\n",
      "=ψu\n",
      "when u ̸= 0\n",
      "\u0013\n",
      ".\n",
      "(20.41)\n",
      "If N is even, then\n",
      "I(x) =\n",
      "˜I(0)\n",
      "N\n",
      "|{z}\n",
      "=A0\n",
      "+\n",
      "N\n",
      "2 −1\n",
      "∑\n",
      "u=1\n",
      "2\n",
      "\f\f˜I(u)\n",
      "\f\f\n",
      "N\n",
      "| {z }\n",
      "=Au\n",
      "when u ̸= 0\n",
      "and u ̸= N\n",
      "2\n",
      "cos\n",
      "\u0012\n",
      "2πu\n",
      "N\n",
      "|{z}\n",
      "=ωu\n",
      "when u ̸= 0\n",
      "and u ̸= N\n",
      "2\n",
      "x+ ∠˜I(u)\n",
      "| {z }\n",
      "=ψu\n",
      "when u ̸= 0\n",
      "and u ̸= N\n",
      "2\n",
      "\u0013\n",
      "+\n",
      "˜I( N\n",
      "2 )\n",
      "N\n",
      "| {z }\n",
      "=A N\n",
      "2\n",
      "cos( π\n",
      "|{z}\n",
      "=ω N\n",
      "2\n",
      "x).\n",
      "(20.42)\n",
      "Comparing Equation (20.41) to Equation (20.25), we can see that the magni-\n",
      "tudes of the DFT coefﬁcients are divided by N to get the amplitudes of the si-\n",
      "nusoidals. This corresponds to the 1\n",
      "N coefﬁcient in front of the IDFT (20.27),\n",
      "which is needed so that the DFT and the IDFT form a valid transform pair. How-\n",
      "ever, the placement of this coefﬁcient is ultimately a question of convention: the\n",
      "derivation in Equations (20.29)–(20.31) is still valid if the coefﬁcient 1\n",
      "N would\n",
      "be moved in front of the DFT in Equation (20.26), or even if both the IDFT and\n",
      "DFT equations would have a coefﬁcient of\n",
      "1\n",
      "√\n",
      "N in front. The convention adopted\n",
      "here in the DFT-IDFT equation pair (equations (20.27) and (20.26)) is the same\n",
      "as in MATLAB.\n",
      "1 See, for example, the MATLAB reference manual entry for the function conv for details.\n",
      "\n",
      "20.3 Two- and three-dimensional discrete Fourier transforms\n",
      "433\n",
      "The basis is orthogonal, perhaps up to scaling\n",
      "In terms of a basis representa-\n",
      "tion, the calculations in Equation (20.29) show that the complex basis vectors\n",
      "used in DFT are orthogonal to each other. In fact, the dot-product of two basis\n",
      "vectors with frequencies u and u∗is\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "e−i 2πx\n",
      "N uei 2πx\n",
      "N u∗=\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "\u0010\n",
      "ei 2π\n",
      "N (u∗−u)\u0011x\n",
      "(20.43)\n",
      "where we have taken the conjugate of the latter term because that is how the dot-\n",
      "product of complex-valued vectors is deﬁned. Now, this is almost like the “term\n",
      "A” in Equation (20.29) with the roles of u and x exchanged (as well as the signs\n",
      "of u and u∗ﬂipped and the scaling of u changed). So, the calculations given there\n",
      "can be simply adapted to show that for u ̸= u∗, this dot-product is zero. However,\n",
      "the norms of these basis vectors are not equal to one in this deﬁnition. This does\n",
      "not change much because it simply means that the inverse transform rescales the\n",
      "coefﬁcients accordingly. The coefﬁcients in the basis are still obtained by just\n",
      "taking dot-products with the basis vectors (and some rescaling if needed). As\n",
      "pointed out above, different deﬁnitions of DFT exist, and in some of them, the\n",
      "basis vectors are normalized to unit norm, so the basis is exactly orthogonal. (In\n",
      "such a deﬁnition, it is the convolution theorem which needs a scaling coefﬁcient.)\n",
      "DFT can be computed by the Fast Fourier Transformation\n",
      "A basic way of\n",
      "computing the DFT would be to use the deﬁnition in Equation (20.26). That\n",
      "would mean that we have to do something like N2 operations, because com-\n",
      "puting each coefﬁcient needs a sum with N terms, and there are N coefﬁcients.\n",
      "A most important algorithm in signal processing is the Fast Fourier Transform\n",
      "(FFT), which computes the DFT using operations which are of the order N logN,\n",
      "based on a recursive formula. This is much faster than N2 because the logarithm\n",
      "grows very slowly as a function of N. Using FFT, one can compute the DFT\n",
      "for very long signals. Practically all numerical software implementing DFT use\n",
      "some variant of FFT, and usually the function is called fft.\n",
      "20.3 Two- and three-dimensional discrete Fourier transforms\n",
      "The two- and three-dimensional discrete Fourier transforms are conceptually similar\n",
      "to the one-dimensional transform. The inverse transform can be thought of as a\n",
      "representation of the image in complex exponentials\n",
      "I(x,y) =\n",
      "1\n",
      "MN\n",
      "M−1\n",
      "∑\n",
      "u=0\n",
      "N−1\n",
      "∑\n",
      "v=0\n",
      "˜I(u,v)ei2π( ux\n",
      "M + vy\n",
      "N ),\n",
      "x = 0,...,M −1, y = 0,...,N −1,\n",
      "(20.44)\n",
      "\n",
      "434\n",
      "20 The discrete Fourier transform\n",
      "and the coefﬁcients ˜I(u,v) in this representation are determined by the (forward)\n",
      "transform\n",
      "˜I(u,v) =\n",
      "M−1\n",
      "∑\n",
      "x=0\n",
      "N−1\n",
      "∑\n",
      "y=0\n",
      "I(x,y)e−i2π( ux\n",
      "M + vy\n",
      "N ),\n",
      "u = 0,...,M −1, v = 0,...,N −1\n",
      "(20.45)\n",
      "The horizontal and vertical frequencies (see Section 2.2.2 on page 30) in the repre-\n",
      "sentation in complex exponentials (Equation (20.44)) are\n",
      "ωx,u = 2πu\n",
      "M , u = 0,...,M −1\n",
      "(20.46)\n",
      "ωy,v = 2πv\n",
      "N , v = 0,...,N −1,\n",
      "(20.47)\n",
      "and the amplitude Au,v and phase ψu,v of the corresponding frequency components\n",
      "Au,v cos(ωx,ux+ωy,vy+ψu,v), u = 0,...,M −1, v = 0,...,N −1,\n",
      "(20.48)\n",
      "can be “read out” from the magnitude and angle of the complex-valued coefﬁcient\n",
      "˜I(u,v). In a basis interpretation, the DFT thus uses a basis with different frequencies,\n",
      "phases, and orientations.\n",
      "Computationally, the two-dimensional DFT can be obtained as follows. First,\n",
      "compute a one-dimensional DFT along for each row, i.e. for the one-dimensional\n",
      "slice given by ﬁxing y. For each row, replace the original values I(x,y) by the DFT\n",
      "coefﬁcients. Denote these by I(u,y). Then, just compute a one-dimensional DFT for\n",
      "each column, i.e. for each ﬁxed u. This gives the ﬁnal two-dimensional DFT I(u,v).\n",
      "Thus, the two-dimensional DFT is obtained by applying the one-dimensional DFT\n",
      "twice; typically, an FFT algorithm is used. The reason why this is possible is the\n",
      "following relation, which can be obtained by simple rearrangement of the terms in\n",
      "the deﬁnition in Equation (20.45):\n",
      "˜I(u,v) =\n",
      "M−1\n",
      "∑\n",
      "y=0\n",
      "\"\n",
      "N−1\n",
      "∑\n",
      "x=0\n",
      "I(x,y)e−i2π ux\n",
      "N\n",
      "#\n",
      "e−i2π vy\n",
      "M\n",
      "(20.49)\n",
      "in which the term in brackets is just the one-dimensional DFT for a ﬁxed y.\n",
      "The three-dimensional discrete Fourier transform pair is deﬁned similarly:\n",
      "I(x,y,t) =\n",
      "1\n",
      "MNT\n",
      "M−1\n",
      "∑\n",
      "u=0\n",
      "N−1\n",
      "∑\n",
      "v=0\n",
      "T−1\n",
      "∑\n",
      "w=0\n",
      "˜I(u,v,w)ei2π( ux\n",
      "M + vy\n",
      "N + wt\n",
      "N ),\n",
      "x = 0,...,M −1, y = 0,...,N −1, t = 0,...,T −1,\n",
      "(20.50)\n",
      "\n",
      "20.3 Two- and three-dimensional discrete Fourier transforms\n",
      "435\n",
      "˜I(u,v,w) =\n",
      "M−1\n",
      "∑\n",
      "x=0\n",
      "N−1\n",
      "∑\n",
      "y=0\n",
      "T−1\n",
      "∑\n",
      "t=0\n",
      "I(x,y,y)e−i2π( ux\n",
      "M + vy\n",
      "N + wt\n",
      "N ),\n",
      "u = 0,...,M −1, v = 0,...,N −1, w = 0,...,T −1,\n",
      "(20.51)\n",
      "The two- and three-dimensional discrete Fourier transforms enjoy a number of\n",
      "similar properties as the one-dimensional transform. For example, the properties of\n",
      "two-dimensional transform pair include:\n",
      "Complex-conjugate symmetry\n",
      "˜I(−u,−v) = I(u,v)\n",
      "Convolution theorem holds when the convolution is deﬁned as the cyclic variant\n",
      "Periodicity of the transform\n",
      "˜I(u,v) = ˜I(u+M,v) = ˜I(u,v+N) = ˜I(u+N,v+M)\n",
      "Periodicity of the inverse\n",
      "I(x,y) = I(x+M,y) = I(x,y+N) = I(x+N,y+M)\n",
      "\n",
      "\n",
      "Chapter 21\n",
      "Estimation of non-normalized statistical models\n",
      "Statistical models are often based on non-normalized probability densities. That\n",
      "is, the model contains an unknown normalization constant whose computation is\n",
      "too difﬁcult for practical purposes. Such models were encountered, for example, in\n",
      "Sections 13.1.5 and 13.1.7. Maximum likelihood estimation is not possible with-\n",
      "out computation of the normalization constant. In this chapter, we show how such\n",
      "models can be estimated using a different estimation method. 1\n",
      "It is not necessary to know this material to understand the developments in this\n",
      "book; this is meant as supplementary material.\n",
      "21.1 Non-normalized statistical models\n",
      "To ﬁx the notation, assume we observe a random vector x ∈Rn which has a probabil-\n",
      "ity density function (pdf) denoted by px(.). We have a parametrized density model\n",
      "p(.;θ), where θ is an m-dimensional vector of parameters. We want to estimate the\n",
      "parameter θ from observations of x, i.e. we want to approximate px(.) by p(.; ˆθ) for\n",
      "the estimated parameter value ˆθ. (To avoid confusion between the random variable\n",
      "and an integrating variable, we use ξ as the integrating variable instead of x in what\n",
      "follows.)\n",
      "The problem we consider here is that we only are able to compute the pdf given\n",
      "by the model up to a multiplicative constant 1/Z(θ):\n",
      "p(ξ;θ) =\n",
      "1\n",
      "Z(θ)q(ξ;θ)\n",
      "That is, we do know the functional form of q as an analytical expression (or any\n",
      "form that can be easily computed), but we do not know how to easily compute Z\n",
      "which is given by an integral that is often analytically intractable:\n",
      "1 This chapter is based on (Hyv¨arinen, 2005), ﬁrst published in Journal of Machine Learning\n",
      "Research. Copyright retained by the author.\n",
      "437\n",
      "\n",
      "438\n",
      "21 Estimation of non-normalized statistical models\n",
      "Z(θ) =\n",
      "Z\n",
      "ξ∈Rn q(ξ;θ)dξ\n",
      "In higher dimensions (in fact, for almost any n > 2), the numerical computation of\n",
      "this integral is practically impossible as well.\n",
      "Thus, maximum likelihood estimation cannot be easily performed. One solution\n",
      "is to approximate the normalization constant Z using Monte Carlo methods, see e.g.\n",
      "(Mackay, 2003). In this chapter, we discuss a simpler method called score matching.\n",
      "21.2 Estimation by score matching\n",
      "In the following, we use extensively the gradient of the log-density with respect to\n",
      "the data vector. For simplicity, we call this the score function, although according\n",
      "the conventional deﬁnition, it is actually the score function with respect to a hypo-\n",
      "thetical location parameter (Schervish, 1995). For the model density, we denote the\n",
      "score function by ψ(ξ;θ):\n",
      "ψ(ξ;θ) =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "∂log p(ξ;θ)\n",
      "∂ξ1...\n",
      "∂log p(ξ;θ)\n",
      "∂ξn\n",
      "\n",
      "\n",
      "\n",
      "=\n",
      "\n",
      "\n",
      "\n",
      "ψ1(ξ;θ)\n",
      "...\n",
      "ψn(ξ;θ)\n",
      "\n",
      "\n",
      "= ∇ξ log p(ξ;θ)\n",
      "The point in using the score function is that it does not depend on Z(θ). In fact we\n",
      "obviously have\n",
      "ψ(ξ;θ) = ∇ξ logq(ξ;θ)\n",
      "(21.1)\n",
      "Likewise, we denote by ψx(.) = ∇ξ log px(.) the score function of the distribution\n",
      "of observed data x. This could in principle be estimated by computing the gradient\n",
      "of the logarithm of a non-parametric estimate of the pdf—but we will see below that\n",
      "no such computation is necessary. Note that score functions are mappings from Rn\n",
      "to Rn.\n",
      "We now propose that the model is estimated by minimizing the expected squared\n",
      "distance between the model score function ψ(.;θ) and the data score function ψx(.).\n",
      "We deﬁne this squared distance as\n",
      "J(θ) = 1\n",
      "2\n",
      "Z\n",
      "ξ∈Rn px(ξ)∥ψ(ξ;θ)−ψx(ξ)∥2dξ\n",
      "(21.2)\n",
      "Thus, our score matching estimator of θ is given by\n",
      "ˆθ = argmin\n",
      "θ J(θ)\n",
      "The motivation for this estimator is that the score function can be directly com-\n",
      "puted from q as in Equation (21.1), and we do not need to compute Z. However,\n",
      "this may still seem to be a very difﬁcult way of estimating θ, since we might have to\n",
      "\n",
      "21.2 Estimation by score matching\n",
      "439\n",
      "compute an estimator of the data score function ψx from the observed sample, which\n",
      "is basically a non-parametric estimation problem. However, no such non-parametric\n",
      "estimation is needed. This is because we can use a simple trick of partial integration\n",
      "to compute the objective function very easily, as shown by the following theorem:\n",
      "Theorem 1. Assume that the model score function ψ(ξ;θ) is differentiable, as well\n",
      "as some weak regularity conditions.2\n",
      "Then, the objective function J in Equation (21.2) can be expressed as\n",
      "J(θ) =\n",
      "Z\n",
      "ξ∈Rn px(ξ)\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "\u0014\n",
      "∂iψi(ξ;θ)+ 1\n",
      "2ψi(ξ;θ)2\n",
      "\u0015\n",
      "dξ +const.\n",
      "(21.3)\n",
      "where the constant does not depend on θ,\n",
      "ψi(ξ;θ) = ∂logq(ξ;θ)\n",
      "∂ξi\n",
      "is the i-th element of the model score function, and\n",
      "∂iψi(ξ;θ) = ∂ψi(ξ;θ)\n",
      "∂ξi\n",
      "= ∂2 logq(ξ;θ)\n",
      "∂ξ 2\n",
      "i\n",
      "is the partial derivative of the i-th element of the model score function with respect\n",
      "to the i-th variable.\n",
      "The proof, given in (Hyv¨arinen, 2005), is based on a simple trick of partial integra-\n",
      "tion.\n",
      "The theorem shows the remarkable fact that the squared distance of the model\n",
      "score function from the data score function can be computed as a simple expecta-\n",
      "tion of certain functions of the non-normalized model pdf. If we have an analytical\n",
      "expression for the non-normalized density function q, these functions are readily\n",
      "obtained by derivation using Equation (21.1) and taking further derivatives.\n",
      "In practice, we have T observations of the random vector x, denoted by\n",
      "x(1),...,x(T). The sample version of J is obviously obtained from Equation (21.3)\n",
      "as\n",
      "˜J(θ) = 1\n",
      "T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "\u0014\n",
      "∂iψi(x(t);θ)+ 1\n",
      "2ψi(x(t);θ)2\n",
      "\u0015\n",
      "+const.\n",
      "(21.4)\n",
      "which is asymptotically equivalent to J due to the law of large numbers. We propose\n",
      "to estimate the model by minimization of ˜J in the case of a real, ﬁnite sample.\n",
      "One may wonder whether it is enough to minimize J to estimate the model, or\n",
      "whether the distance of the score functions can be zero for different parameter val-\n",
      "ues. Obviously, if the model is degenerate in the sense that two different values of θ\n",
      "give the same pdf, we cannot estimate θ. If we assume that the model is not degen-\n",
      "2\n",
      "Namely: the data pdf\n",
      "px(ξ) is differentiable, the expectations Ex{∥ψ(x;θ)∥2} and\n",
      "Ex{∥ψx(x)∥2} are ﬁnite for any θ, and px(ξ)ψ(ξ;θ) goes to zero for any θ when ∥ξ∥→∞.\n",
      "\n",
      "440\n",
      "21 Estimation of non-normalized statistical models\n",
      "erate, and that q > 0 always, we have local consistency as shown by the following\n",
      "theorem and the corollary:\n",
      "Theorem 2. Assume the pdf of x follows the model: px(.) = p(.;θ ∗) for some θ ∗.\n",
      "Assume further that no other parameter value gives a pdf that is equal3 to p(.;θ ∗),\n",
      "and that q(ξ;θ) > 0 for all ξ,θ. Then\n",
      "J(θ) = 0 ⇔θ = θ∗\n",
      "For a proof, see (Hyv¨arinen, 2005).\n",
      "Corollary 1. Under the assumptions of the preceding Theorems, the score matching\n",
      "estimator obtained by minimization of ˜J is consistent, i.e. it converges in probability\n",
      "towards the true value of θ when sample size approaches inﬁnity, assuming that the\n",
      "optimization algorithm is able to ﬁnd the global minimum.\n",
      "The corollary is proven by applying the law of large numbers.4\n",
      "This result of consistency assumes that the global minimum of ˜J is found by the\n",
      "optimization algorithm used in the estimation. In practice, this may not be true, in\n",
      "particular because there may be several local minima. Then, the consistency is of\n",
      "local nature, i.e., the estimator is consistent if the optimization iteration is started\n",
      "sufﬁciently close to the true value.\n",
      "21.3 Example 1: Multivariate gaussian density\n",
      "As a very simple illustrative example, consider estimation of the parameters of the\n",
      "multivariate gaussian density:\n",
      "p(x;M,µ) =\n",
      "1\n",
      "Z(M,µ) exp(−1\n",
      "2(x −µ)TM(x −µ))\n",
      "where M is a symmetric positive-deﬁnite matrix (the inverse of the covariance ma-\n",
      "trix). Of course, the expression for Z is well-known in this case, but this serves as\n",
      "an illustration of the method. As long as there is no chance of confusion, we use x\n",
      "here as the general n-dimensional vector. Thus, here we have\n",
      "q(x) = exp(−1\n",
      "2(x −µ)T M(x −µ))\n",
      "(21.5)\n",
      "and we obtain\n",
      "ψ(x;M,µ) = −M(x −µ)\n",
      "3 In this theorem, equalities of pdf’s are to be taken in the sense of equal almost everywhere with\n",
      "respect to the Lebesgue measure.\n",
      "4 As sample size approaches inﬁnity, ˜J converges to J (in probability). Thus, the estimator con-\n",
      "verges to a point where J is globally minimized. By Theorem 2, the global minimum is unique and\n",
      "found at the true parameter value (obviously, J cannot be negative).\n",
      "\n",
      "21.3 Example 1: Multivariate gaussian density\n",
      "441\n",
      "and\n",
      "∂iψi(x;M,µ) = −mii\n",
      "Thus, we obtain\n",
      "˜J(M,µ) = 1\n",
      "T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "[∑\n",
      "i\n",
      "−mii + 1\n",
      "2(x(t)−µ)T MM(x(t)−µ)]\n",
      "(21.6)\n",
      "To minimize this with respect to µ, it is enough to compute the gradient\n",
      "∇µ ˜J = MMµ −MM 1\n",
      "T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "x(t)\n",
      "which is obviously zero if and only if µ is the sample average 1\n",
      "T ∑T\n",
      "t=1 x(t). This is\n",
      "truly a minimum because the matrix MM that deﬁnes the quadratic form is positive-\n",
      "deﬁnite.\n",
      "Next, we compute the gradient with respect to M, which gives\n",
      "∇M ˜J = −I+M 1\n",
      "2T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "(x(t)−µ)(x(t)−µ)T + 1\n",
      "2T [\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "(x(t)−µ)(x(t)−µ)T]M\n",
      "which is zero if and only if M is the inverse of the sample covariance matrix\n",
      "1\n",
      "T ∑T\n",
      "t=1(x(t)−µ)(x(t)−µ)T, which thus gives the score matching estimate.\n",
      "Interestingly, we see that score matching gives exactly the same estimator as\n",
      "maximum likelihood estimation. In fact, the estimators are identical for any sample\n",
      "(and not just asymptotically). The maximum likelihood estimator is known to be\n",
      "consistent, so the score matching estimator is consistent as well.\n",
      "This example also gives some intuitive insight into the principle of score match-\n",
      "ing. Let us consider what happened if we just maximized the non-normalized log-\n",
      "likelihood, i.e., log of q in Equation (21.5). It is maximized when the scale param-\n",
      "eters in M are zero, i.e., the model variances are inﬁnite and the pdf is completely\n",
      "ﬂat. This is because then the model assigns the same probability to all possible val-\n",
      "ues of x(t), which is equal to 1. In fact, the same applies to the second term in\n",
      "Equation (21.6), which thus seems to be closely connected to maximization of the\n",
      "non-normalized log-likelihood.\n",
      "Therefore, the ﬁrst term in Equation (21.3) and Equation (21.6), involving second\n",
      "derivatives of the logarithm of q, seems to act as a kind of a normalization term.\n",
      "Here it is equal to −∑i mii. To minimize this, the mii should be made as large (and\n",
      "positive) as possible. Thus, this term has the opposite effect to the second term.\n",
      "Since the ﬁrst term is linear and the second term polynomial in M, the minimum of\n",
      "the sum is different from zero.\n",
      "A similar interpretation applies to the general non-gaussian case. The second\n",
      "term in Equation (21.3), expectation of the norm of score function, is closely re-\n",
      "lated to maximization of non-normalized likelihood: if the norm of this gradient is\n",
      "zero, then in fact the data point is in a local extremum of the non-normalized log-\n",
      "likelihood. The ﬁrst term then measures what kind of an extremum this is. If it is\n",
      "\n",
      "442\n",
      "21 Estimation of non-normalized statistical models\n",
      "a minimum, the ﬁrst term is positive and the value of J is increased. To minimize\n",
      "J, the ﬁrst term should be negative, in which case the extremum is a maximum.\n",
      "In fact, the extremum should be as steep a maximum (as opposed to a ﬂat maxi-\n",
      "mum) as possible to minimize J. This counteracts, again, the tendency to assign the\n",
      "same probability to all data points that is often inherent in the maximization of the\n",
      "non-normalized likelihood.\n",
      "21.4 Example 2: Estimation of basic ICA model\n",
      "Next, we show how score matching can be used in the estimation of the basic ICA\n",
      "model, deﬁned as\n",
      "log p(x) =\n",
      "n\n",
      "∑\n",
      "k=1\n",
      "G(wT\n",
      "k x)+Z(w1,...,wn)\n",
      "(21.7)\n",
      "Again, the normalization constant is well-known and equal to log|detW| where the\n",
      "matrix W has the vectors wi as rows, but this serves as an illustration of our method.\n",
      "Here, we choose the distribution of the components si to be so-called logistic\n",
      "with\n",
      "G(s) = −2logcosh( π\n",
      "2\n",
      "√\n",
      "3\n",
      "s)−4\n",
      "√\n",
      "3\n",
      "π\n",
      "This distribution is normalized to unit variance as typical in the theory of ICA. The\n",
      "score function of the model in (21.7) is given by\n",
      "ψ(x;W) =\n",
      "n\n",
      "∑\n",
      "k=1\n",
      "wkg(wT\n",
      "k x)\n",
      "(21.8)\n",
      "where the scalar nonlinear function g is given by\n",
      "g(s) = −π\n",
      "√\n",
      "3 tanh( π\n",
      "2\n",
      "√\n",
      "3s)\n",
      "The relevant derivatives of the score function are given by:\n",
      "∂iψi(x) =\n",
      "n\n",
      "∑\n",
      "k=1\n",
      "w2\n",
      "kig′(wT\n",
      "k x)\n",
      "and the sample version of the objective function ˜J is given by\n",
      "\n",
      "21.6 Conclusion\n",
      "443\n",
      "˜J = 1\n",
      "T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "n\n",
      "∑\n",
      "i=1\n",
      "\"\n",
      "n\n",
      "∑\n",
      "k=1\n",
      "w2\n",
      "kig′(wT\n",
      "k x(t))+ 1\n",
      "2\n",
      "n\n",
      "∑\n",
      "j=1\n",
      "wjig(wT\n",
      "j x(t))\n",
      "n\n",
      "∑\n",
      "k=1\n",
      "wkig(wT\n",
      "k x(t))\n",
      "#\n",
      "=\n",
      "n\n",
      "∑\n",
      "k=1\n",
      "∥wk∥2 1\n",
      "T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "g′(wT\n",
      "k x(t))+ 1\n",
      "2\n",
      "n\n",
      "∑\n",
      "j,k=1\n",
      "wT\n",
      "j wk\n",
      "1\n",
      "T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "g(wT\n",
      "k x(t))g(wT\n",
      "j x(t))\n",
      "(21.9)\n",
      "21.5 Example 3: Estimation of an overcomplete ICA model\n",
      "Finally, we how score matching can be applied in the case of the overcomplete basis\n",
      "model in Section 13.1.5. The likelihood is deﬁned almost as in Equation (21.7), but\n",
      "the number of components m is larger than the dimension of the data n, and we\n",
      "introduce some extra parameters. The likelihood is given by\n",
      "log p(x) =\n",
      "m\n",
      "∑\n",
      "k=1\n",
      "αkG(wT\n",
      "k x)+Z(w1,...,wn,α1,...,αn)\n",
      "(21.10)\n",
      "where the vectors wk = (wk1,...,wkn) are constrained to unit norm (unlike in the\n",
      "preceding example), and the αk are further parameters. We introduce here the extra\n",
      "parameters αk to account for different distributions for different projections. Con-\n",
      "straining αk = 1 and m = n and allowing the wk to have any norm, this becomes the\n",
      "basic ICA model.\n",
      "We have the score function\n",
      "ψ(x;W,α1,...,αm) =\n",
      "m\n",
      "∑\n",
      "k=1\n",
      "αkwkg(wT\n",
      "k x)\n",
      "where g is the ﬁrst derivative of G. Going through similar developments as in the\n",
      "case of the basic ICA model, the sample version of the objective function ˜J can be\n",
      "shown to equal\n",
      "˜J =\n",
      "m\n",
      "∑\n",
      "k=1\n",
      "αk\n",
      "1\n",
      "T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "g′(wT\n",
      "k x(t))+ 1\n",
      "2\n",
      "m\n",
      "∑\n",
      "j,k=1\n",
      "αjαkwT\n",
      "j wk\n",
      "1\n",
      "T\n",
      "T\n",
      "∑\n",
      "t=1\n",
      "g(wT\n",
      "k x(t))g(wT\n",
      "j x(t))\n",
      "(21.11)\n",
      "Minimization of this function thus enables estimation of the overcomplete ICA\n",
      "model using the energy-based formulation. This is how we obtained the results in\n",
      "Figure 13.1 on page 298.\n",
      "21.6 Conclusion\n",
      "Score matching is a simple method to estimate statistical models in the case where\n",
      "the normalization constant is unknown. Although the estimation of the score func-\n",
      "\n",
      "444\n",
      "21 Estimation of non-normalized statistical models\n",
      "tion is computationally difﬁcult, we showed that the distance of data and model\n",
      "score functions is very easy to compute. The main assumptions in the method are:\n",
      "1) all the variables are continuous-valued and deﬁned over Rn, 2) the model pdf is\n",
      "smooth enough.\n",
      "We have seen how the method gives an objective function whose minimization\n",
      "enables estimation of the model. The objective function is typically given as an\n",
      "analytical formula, so any classic optimization method, such as gradient methods,\n",
      "can be used to minimize it.\n",
      "Two related methods are contrastive divergence (Hinton, 2002) and pseudo-\n",
      "likelihood (Besag, 1975). The relationships between these methods are considered\n",
      "in (Hyv¨arinen, 2007).\n",
      "\n",
      "Index\n",
      "action potential, 51\n",
      "aliasing, 355\n",
      "and rectangular sampling grid, 111\n",
      "of phases of highest frequencies, 112\n",
      "reducing it by dimension reduction, 113\n",
      "amodal completion, 386\n",
      "amplitude, 29\n",
      "amplitude response, 33, 426\n",
      "analysis by synthesis, 8\n",
      "anisotropy, 121, 152, 240, 258, 344\n",
      "argument (of Fourier coefﬁcient), 424\n",
      "aspect ratio, 58, 275\n",
      "attention, 318\n",
      "audition, 336\n",
      "autocorrelation function, 119\n",
      "axon, 51\n",
      "basis\n",
      "deﬁnition, 419\n",
      "illustration, 39\n",
      "orthogonal, 40\n",
      "overcomplete, see overcomplete basis\n",
      "undercomplete, 40\n",
      "basis vector, 290\n",
      "Bayes’ rule, 85\n",
      "Bayesian inference, 10\n",
      "and cortical feedback, 307\n",
      "as higher-order learning principle, 382\n",
      "deﬁnition, 83\n",
      "in overcomplete basis, 292\n",
      "blue sky effect, 216\n",
      "bottom-up, 307, 308\n",
      "bubble coding, 366\n",
      "canonical preprocessing, see preprocessing,\n",
      "canonical\n",
      "category, 314\n",
      "causal inﬂuence, 401\n",
      "central limit theorem, 178, 231\n",
      "channel\n",
      "colour (chromatic), 322\n",
      "frequency, see frequency channels\n",
      "information, 192\n",
      "limited capacity, 193, 203\n",
      "ON and OFF, 301\n",
      "chromatic aberration, 323\n",
      "coding, 13, 185\n",
      "bubble, 366\n",
      "predictive, 316\n",
      "sparse, see sparse coding\n",
      "collator units, 284\n",
      "collector units, 284\n",
      "colour, 321\n",
      "colour hexagon, 323\n",
      "competitive interactions, 315\n",
      "complex cells, 62, 223, 226, 228\n",
      "energy model, see energy model\n",
      "hierarchical model critisized, 385\n",
      "in ISA, 238\n",
      "in topographic ICA, 254, 259\n",
      "interactions between, 63\n",
      "complex exponentials, 425\n",
      "compression, 13, 185\n",
      "cones, 322\n",
      "contours, 284, 308\n",
      "contrast, 56\n",
      "contrast gain control, 64, 236\n",
      "and normalization of variance, 214\n",
      "relationship to ISA, 234\n",
      "contrastive divergence, 296\n",
      "convexity, 172, 173, 227, 245\n",
      "deﬁnition, 141\n",
      "convolution, 28, 423\n",
      "correlation\n",
      "445\n",
      "\n",
      "446\n",
      "Index\n",
      "and Hebb’s rule, 401\n",
      "between pixels, 99\n",
      "of squares (energies), see energy correlations\n",
      "correlation coefﬁcient, 80\n",
      "cortex, 54\n",
      "extrastriate, 273\n",
      "striate, see V1\n",
      "covariance\n",
      "and Hebb’s rule, 401\n",
      "deﬁnition, 80\n",
      "covariance matrix, 80\n",
      "and PCA, 104\n",
      "connection to power spectrum, 119\n",
      "curvelets, 388\n",
      "cytochrome oxidase blobs, 250, 327\n",
      "DC component, 97\n",
      "is not always sparse, 176, 180\n",
      "removal, 65, 97, 214\n",
      "as part of canonical preprocessing, 113\n",
      "dead leaves model, 388\n",
      "decorrelation\n",
      "deﬂationary, 146\n",
      "symmetric, 147\n",
      "dendrite, 53\n",
      "derivatives, 395, 408\n",
      "determinant\n",
      "considered constant in likelihoods, 171\n",
      "deﬁnition, 418\n",
      "in density of linear transform, 167\n",
      "dimension reduction\n",
      "as part of canonical preprocessing, 114\n",
      "by PCA, 108\n",
      "Dirac ﬁlter, 19\n",
      "discrete cosine transform, 194\n",
      "disparity, 328\n",
      "divisive normalization, 65, 215, 236\n",
      "dot-product, 416\n",
      "double opponent, 326\n",
      "ecological adaptation, 6\n",
      "eigensignal, 425\n",
      "eigenvalue decomposition\n",
      "and Fourier analysis, 125\n",
      "and PCA, 106, 123\n",
      "and translation-invariant data, 125\n",
      "deﬁnition, 123\n",
      "ﬁnds maximum of quadratic form, 124\n",
      "of covariance matrix, 123\n",
      "eigenvectors / eigenvalues, see eigenvalue\n",
      "decomposition\n",
      "embodiment, 389\n",
      "end-stopping, 315\n",
      "energy correlations, 211, 230, 245, 355\n",
      "spatiotemporal, 359, 365\n",
      "temporal, 350\n",
      "energy model, 62, 253, 274\n",
      "as subspace feature in ISA, 225\n",
      "learning by sparseness, 227\n",
      "entropy\n",
      "and coding length, 189\n",
      "deﬁnition, 188\n",
      "differential, 191\n",
      "as measure of non-gaussianity, 192\n",
      "maximum, 192\n",
      "minimum, 194\n",
      "of neuron outputs, 197\n",
      "estimation, 89\n",
      "maximum a posteriori (MAP), 90\n",
      "maximum likelihood, see likelihood,\n",
      "maximum\n",
      "Euler’s formula, 424\n",
      "excitation, 54\n",
      "expectation\n",
      "deﬁnition, 79\n",
      "linearity, 80\n",
      "exponential distribution, 176\n",
      "extrastriate cortex, 66, 307, 385\n",
      "FastICA, 150, 347\n",
      "deﬁnition, 411\n",
      "feature, 18\n",
      "output statistics, 19\n",
      "feedback, 307, 405\n",
      "FFT, 433\n",
      "ﬁlling-in, 386\n",
      "ﬁlter\n",
      "linear, 25\n",
      "spatiotemporal, 337\n",
      "temporally decorrelating, 345\n",
      "ﬁring rate, 53\n",
      "modelled as a function of stimulus, 55\n",
      "spontaneous, 53\n",
      "Fourier amplitude (see power spectrum)\n",
      "1/ f behaviour, 116, 386\n",
      "Fourier analysis, see Fourier transform\n",
      "Fourier energy, 33, 63\n",
      "Fourier power spectrum, see power spectrum\n",
      "Fourier space, 424\n",
      "Fourier transform, 29, 38, 423\n",
      "connection to PCA, 107\n",
      "deﬁnition, 423\n",
      "discrete, 38, 428\n",
      "fast, 433\n",
      "spatiotemporal, 338\n",
      "two-dimensional, 433\n",
      "frame (in image sequences), 337\n",
      "frequency\n",
      "\n",
      "Index\n",
      "447\n",
      "negative, 426\n",
      "frequency channels, 59, 279, 285\n",
      "produced by ICA, 182\n",
      "frequency-based representation, 29, 338\n",
      "as a basis, 40\n",
      "function\n",
      "log cosh, 143, 150, 172, 198, 276, 297, 299,\n",
      "347, 350, 403, 412\n",
      "neighbourhood, 250\n",
      "nonlinear, see nonlinearity\n",
      "tanh, see nonlinearity, tanh\n",
      "Gabor ﬁlter, 44, 274\n",
      "Gabor function, 44, 58, 285, 324, 355\n",
      "in complex cell model, 63\n",
      "gain control, contrast, see contrast gain control\n",
      "gain control, luminance, 65\n",
      "ganglion cells, 54, 65, 127\n",
      "learning receptive ﬁelds, 130\n",
      "number compared to V1, 290\n",
      "receptive ﬁelds, 56\n",
      "gaussian distribution\n",
      "and PCA, 115\n",
      "and score matching estimation, 440\n",
      "generalized, 173\n",
      "multidimensional, 115\n",
      "one-dimensional, 72\n",
      "spherical symmetry when whitened, 164\n",
      "standardized, 72, 81\n",
      "uncorrelatedness implies independence, 165\n",
      "gaze direction, 318\n",
      "gradient, 395\n",
      "gradient method, 396\n",
      "conjugate, 410\n",
      "stochastic, 402\n",
      "with constraints, 398\n",
      "Gram-Schmidt orthogonalization, 156\n",
      "grandmother cell, 54\n",
      "Hebb’s rule, 400\n",
      "and correlation, 401\n",
      "and orthogonality, 405\n",
      "Hessian, 408\n",
      "horizontal interactions\n",
      "see lateral interactions 307\n",
      "ice cube model, 249\n",
      "image, 3\n",
      "image space, 11\n",
      "image synthesis\n",
      "by ICA, 169\n",
      "by ISA, 241\n",
      "by PCA, 116\n",
      "impulse response, 28, 423\n",
      "independence\n",
      "as nonlinear uncorrelatedness, 160\n",
      "deﬁnition, 77\n",
      "implies uncorrelatedness, 81\n",
      "increased by divisive normalization, 218\n",
      "of components, 161\n",
      "independent component analysis, 159\n",
      "after variance normalization, 217\n",
      "and Hebb’s rule, 402\n",
      "and mutual information, 195\n",
      "and non-gaussianity, 161\n",
      "and optimal sparseness measures, 172\n",
      "connection to sparse coding, 170\n",
      "deﬁnition, 161\n",
      "for preprocessed data, 162\n",
      "image synthesis, 169\n",
      "impossibility for gaussian data, 164\n",
      "indeterminacies, 161\n",
      "likelihood, see likelihood, of ICA\n",
      "maximum likelihood, 167\n",
      "need not give independent components, 209\n",
      "nonlinear, 234\n",
      "of complex cell outputs, 276\n",
      "of colour images, 323, 330\n",
      "of image sequences, 347\n",
      "of natural images, 168\n",
      "optimization in, 405\n",
      "pdf deﬁned by, 166\n",
      "score matching estimation, 442\n",
      "topographic, 252\n",
      "vs. whitening, 163\n",
      "independent subspace analysis, 223\n",
      "as nonlinear ICA, 234\n",
      "generative model deﬁnition, 229\n",
      "image synthesis, 241\n",
      "of natural images, 235\n",
      "special case of topographic ICA, 254\n",
      "special case of two-layer model, 266\n",
      "superiority over ICA, 243\n",
      "infomax, 196\n",
      "basic, 197\n",
      "nonlinear neurons, 198\n",
      "with non-constant noise variance, 199\n",
      "information ﬂow\n",
      "maximization, see infomax\n",
      "information theory, 13, 185\n",
      "critique of application, 202\n",
      "inhibition, 54\n",
      "integrating out, 266\n",
      "invariance\n",
      "modelling by subspace features, 224\n",
      "not possible with linear features, 223\n",
      "of features, importance, 242\n",
      "of ISA features, 238\n",
      "\n",
      "448\n",
      "Index\n",
      "rotational (of image), see anisotropy\n",
      "shift (of a system), 423\n",
      "to orientation, 229, 247\n",
      "to phase, of a feature\n",
      "and sampling grid, 113\n",
      "in complex cells, 63\n",
      "in ISA, 228, 235\n",
      "to position, 224, 242\n",
      "to scale, 386, 388\n",
      "and 1/ f 2 power spectrum, 118\n",
      "to translation, of an image, 125\n",
      "and relation to PCA, 106\n",
      "inverse of matrix, 419\n",
      "kurtosis, 173, 174, 350\n",
      "and classiﬁcation of distributions, 175\n",
      "and estimation of ICA, 180\n",
      "deﬁnition, 140\n",
      "Laplacian distribution, 172, 312\n",
      "generalized, 173\n",
      "two-dimensional generalization, 230\n",
      "lateral geniculate nucleus, see LGN\n",
      "lateral interactions, 307, 315, 318, 364, 385\n",
      "LGN, 54, 203\n",
      "learning receptive ﬁelds, 130, 345\n",
      "receptive ﬁelds characterized, 56\n",
      "likelihood, 89, 91, 167\n",
      "and divisive normalization, 215\n",
      "maximum, 90, 382, 438\n",
      "obtained by integrating out, 267\n",
      "of ICA, 168, 183\n",
      "and infomax, 198\n",
      "and differential entropy, 195\n",
      "and optimal sparseness measures, 172\n",
      "as a sparseness measure, 170\n",
      "optimization, 397\n",
      "of ISA, 229\n",
      "of topographic ICA, 253, 256\n",
      "of two-layer model, 267\n",
      "used for deciding between models, 244\n",
      "linear features\n",
      "cannot be invariant, 223\n",
      "linear-nonlinear model, 61\n",
      "local maximum, 399\n",
      "local minimum, 399\n",
      "localization\n",
      "simultaneous, 47\n",
      "log cosh, see function, log cosh\n",
      "Markov random ﬁeld, 297, 384, 389\n",
      "matrix\n",
      "deﬁnition, 417\n",
      "identity, 419\n",
      "inverse, 419\n",
      "of orthogonal matrix, 421\n",
      "optimization of a function of, 397\n",
      "orthogonal, 111\n",
      "matrix square root, 129\n",
      "and orthogonalization, 399\n",
      "and whitening, 130\n",
      "maximum entropy, 192\n",
      "maximum likelihood, see likelihood, maximum\n",
      "metabolic economy, 155, 382\n",
      "minimum entropy, 194\n",
      "coding in cortex, 196\n",
      "model\n",
      "descriptive, 16\n",
      "different levels, 393\n",
      "energy, see energy model\n",
      "energy-based, 264, 294\n",
      "generative, 8, 264\n",
      "normative, 16, 309, 382\n",
      "physically inspired, 388\n",
      "predictive, 273, 285\n",
      "statistical, 88, 393\n",
      "two-layer, 264\n",
      "multilayer models, 389\n",
      "multimodal integration, 336\n",
      "music, 336\n",
      "mutual information, 192, 195, 197\n",
      "natural images\n",
      "as random vectors, 69\n",
      "deﬁnition, 12\n",
      "sequences of, 337\n",
      "transforming to a vector, 69\n",
      "nature vs. nurture, 382\n",
      "neighbourhood function, 250\n",
      "neuron, 51\n",
      "Newton’s method, 407\n",
      "noise\n",
      "added to pixels, 291\n",
      "reduction and feedback, 308\n",
      "reduction by thresholding, 311\n",
      "white, see white noise\n",
      "non-gaussianity\n",
      "and independence, 178\n",
      "different forms, 175\n",
      "maximization and ICA, 177\n",
      "non-negative matrix factorization, 300\n",
      "with sparseness constraints, 302\n",
      "nonlinearity, 139\n",
      "convex, 141, 172, 173, 227, 232\n",
      "gamma, 322\n",
      "Hebbian, 403\n",
      "in FastICA, 276, 411\n",
      "in overcomplete basis, 293\n",
      "\n",
      "Index\n",
      "449\n",
      "in three-layer model, 309\n",
      "square root, 142, 172, 236\n",
      "tanh, 276, 347, 403, 412\n",
      "norm, 416\n",
      "normal distribution, see gaussian distribution\n",
      "normalization constant, 270, 295, 438\n",
      "objective function, 394\n",
      "ocular dominance, 330\n",
      "optimization, 393\n",
      "constrained, 398\n",
      "under orthogonality constraint, 398\n",
      "orientation columns, 250\n",
      "orthogonality\n",
      "and Hebb’s rule, 405\n",
      "equivalent to uncorrelatedness, 111\n",
      "of matrix or transformation, 420\n",
      "of vectors, 416\n",
      "of vectors vs. of matrix, 111\n",
      "prevents overcompleteness, 289\n",
      "orthogonalization\n",
      "as decorrelation, 146\n",
      "Gram-Schmidt, 156\n",
      "symmetric, 399\n",
      "orthonormality, 111\n",
      "overcomplete basis\n",
      "and end-stopping, 315\n",
      "and PCA, 291\n",
      "deﬁnition, 291\n",
      "energy-based model, 294\n",
      "score matching estimation, 443\n",
      "generative model, 290\n",
      "partition function, see normalization constant\n",
      "PCA, see principal component analysis\n",
      "pdf, 71\n",
      "non-normalized, 437\n",
      "phase, 29, 424\n",
      "its importance in natural images, 120\n",
      "phase response, 33, 426\n",
      "photoreceptors, 54\n",
      "and colour, 322\n",
      "pinwheels, 258, 271\n",
      "place cells, 336\n",
      "plasticity, 400\n",
      "spike-time dependent, 401\n",
      "pooling, 226, 254\n",
      "frequency, 277\n",
      "positive matrix factorization, 300\n",
      "posterior distribution, 84, 85, 308\n",
      "power spectrum, 33\n",
      "1/ f 2 behaviour, 116, 386\n",
      "and covariances, 119\n",
      "and gaussian model, 120\n",
      "and PCA, 120\n",
      "its importance in natural images, 120\n",
      "of natural images, 116\n",
      "spatiotemporal, 341\n",
      "Wiener-Khinchin theorem, 119\n",
      "preprocessing\n",
      "by DC removal, see DC component, removal\n",
      "canonical, 113, 146, 150, 181, 227, 236, 253\n",
      "how it changes the models, 162\n",
      "in sparse coding, 144\n",
      "input to visual cortex, 404\n",
      "inversion of, 181\n",
      "primary visual cortex, see V1\n",
      "principal component analysis\n",
      "and Hebb’s rule, 403\n",
      "and whitening, 109\n",
      "as anti-aliasing, 111, 113\n",
      "as dimension reduction, 108\n",
      "as generative model, 115\n",
      "as part of canonical preprocessing, 113\n",
      "as preprocessing, 107\n",
      "components are uncorrelated, 124\n",
      "computation of, 104, 122\n",
      "connection to Fourier analysis, 107\n",
      "deﬁnition, 100\n",
      "deﬁnition is unsatisfactory, 103\n",
      "lack of uniqueness, 103, 125\n",
      "mathematics of, 122\n",
      "of colour images, 323\n",
      "of natural images, 102, 104\n",
      "of stereo images, 329\n",
      "principal subspace, 108\n",
      "prior distribution, 6, 10, 85, 86, 166, 292\n",
      "non-informative, 86\n",
      "prior information, see prior distribution\n",
      "probabilistic model, see model, statistical\n",
      "probability\n",
      "conditional, 75\n",
      "joint, 73\n",
      "marginal, 73\n",
      "probability density (function), see pdf\n",
      "products of experts, 296\n",
      "pseudoinverse, 421\n",
      "pyramids, 387\n",
      "quadrature phase, 44, 150, 274\n",
      "in complex cell model, 63\n",
      "random vector, 70\n",
      "receptive ﬁeld, 55, 57\n",
      "center-surround, 56, 127, 326, 345\n",
      "classical and non-classical, 316\n",
      "deﬁnition is problematic, 294, 315\n",
      "Gabor model, 58\n",
      "\n",
      "450\n",
      "Index\n",
      "linear model, 56\n",
      "space-time inseparable, 340\n",
      "space-time separable, 339\n",
      "spatiotemporal, 338\n",
      "temporal, 338\n",
      "vs. feature (basis) vector, 180\n",
      "vs. synaptic strength, 404\n",
      "rectiﬁcation, 65, 347, 355\n",
      "half-wave, 61\n",
      "redundancy, 13, 190\n",
      "as predictability, 14\n",
      "problems with, 190\n",
      "reduction, 15\n",
      "representation, 18\n",
      "frequency-based, see frequency-based\n",
      "representation\n",
      "linear, 18, 38\n",
      "retina, 54\n",
      "learning receptive ﬁelds, 130\n",
      "receptive ﬁelds characterized, 56\n",
      "retinotopy, 65, 250\n",
      "reverse correlation, 57\n",
      "RGB data, 322\n",
      "sample, 88\n",
      "two different meanings, 88\n",
      "sampling, 3, 111, 328, 347\n",
      "saturation, 61, 217\n",
      "scale mixture, 177, 266\n",
      "scaling laws, 386\n",
      "score matching, 296, 382, 438\n",
      "segmentation, 386\n",
      "selectivities\n",
      "of ISA features, 238\n",
      "of simple cells, 58\n",
      "of sparse coding features, 152\n",
      "sequences\n",
      "of natural images, 337\n",
      "shrinkage, 312, 404\n",
      "simple cells, 56, 254\n",
      "distinct from complex cells?, 384\n",
      "Gabor models, 58\n",
      "interactions between, 63\n",
      "linear models, 56\n",
      "nonlinear responses, 59\n",
      "selectivities, 58\n",
      "sinusoidal, 425\n",
      "skewness, 175, 176\n",
      "in natural images, 176\n",
      "slow feature analysis, 367\n",
      "linear, 369\n",
      "quadratic, 371\n",
      "sparse, 374\n",
      "space-frequency analysis, 41\n",
      "sparse coding, 137\n",
      "and compression, 194\n",
      "and Hebb’s rule, 402, 403\n",
      "connection to ICA, 170\n",
      "metabolic economy, 155, 382\n",
      "optimization in, 405\n",
      "results with natural images, 145, 150\n",
      "special case of ICA, 179\n",
      "utility, 155\n",
      "sparseness\n",
      "as non-gaussianity, 175\n",
      "deﬁnition, 137\n",
      "lifetime vs. population, 148\n",
      "measure, 139\n",
      "absolute value, 143\n",
      "by convex function of square, 141\n",
      "kurtosis, 140\n",
      "log cosh, 143\n",
      "optimal, 159, 172\n",
      "relation to tanh function, 403\n",
      "minimization of, 180\n",
      "of feature vs. of representation, 148\n",
      "why present in images, 175\n",
      "spherical symmetry, 231\n",
      "spike, 51\n",
      "square root\n",
      "of a matrix, see matrix square root\n",
      "statistical-ecological approach, 21\n",
      "steerable ﬁlters, 229, 247, 388\n",
      "step size, 396, 408\n",
      "stereo vision, 328\n",
      "stereopsis, 328\n",
      "striate cortex, see V1\n",
      "sub-gaussianity, 175\n",
      "subgaussianity\n",
      "in natural images, 176\n",
      "subspace features, 224, 225\n",
      "super-gaussianity, 175\n",
      "temporal coherence, 349\n",
      "and spatial energy correlations, 359, 365\n",
      "temporal response strength correlation, see\n",
      "temporal coherence\n",
      "thalamus, 54\n",
      "theorem\n",
      "central limit, 178, 231\n",
      "of density of linear transform, 167\n",
      "Wiener-Khinchin, 119\n",
      "three-layer model, 276, 308\n",
      "thresholding\n",
      "and feedback, 312, 314\n",
      "in simple cell response, 61\n",
      "top-down, 307, 308\n",
      "topographic grid, 250\n",
      "\n",
      "Index\n",
      "451\n",
      "topographic ICA, 252\n",
      "connection to ISA, 254\n",
      "optimization in, 405\n",
      "topographic lattice, 250\n",
      "topographic organization, 65, 249\n",
      "utility, 254\n",
      "transmission\n",
      "of data, 187\n",
      "transpose, 418\n",
      "tuning curve, 59\n",
      "disparity, 333\n",
      "of ISA features, 236\n",
      "of sparse coding features, 152\n",
      "two-layer model\n",
      "energy-based, 270\n",
      "ﬁxed, 274\n",
      "generative, 264\n",
      "uncertainty principle, 47\n",
      "uncorrelatedness\n",
      "deﬁnition, 80\n",
      "equivalent to orthogonality, 111\n",
      "implied by independence, 81\n",
      "nonlinear, 160\n",
      "uniform distribution, 78\n",
      "is sub-gaussian, 176\n",
      "maximum entropy, 192\n",
      "unsupervised learning, 385\n",
      "V1, see also simple cells and complex cells, 55\n",
      "V2, 203, 286, 307\n",
      "V4, 66\n",
      "V5, 66\n",
      "variance\n",
      "as basis for PCA, 100\n",
      "changing (non-constant), 177, 199, 213,\n",
      "231, 265, 351\n",
      "deﬁnition, 80\n",
      "variance variable, 177, 213, 231, 265\n",
      "vector, 415\n",
      "vectorization, 70\n",
      "vision, 3\n",
      "visual space, 336\n",
      "wavelets, 221, 387\n",
      "learning them partly, 388\n",
      "waves\n",
      "retinal travelling, 383\n",
      "white noise, 57, 197, 198, 281, 366\n",
      "deﬁnition, 82\n",
      "whitening, 109\n",
      "and center-surround receptive ﬁelds, 126\n",
      "and LGN, 126, 130\n",
      "and retina, 126, 130\n",
      "as part of canonical preprocessing, 114\n",
      "by matrix square root, 130\n",
      "by PCA, 109\n",
      "center-surround receptive ﬁelds, 130\n",
      "ﬁlter, 130\n",
      "patch-based and ﬁlter-based, 127\n",
      "symmetric, 130\n",
      "Wiener-Khinchin theorem, 119\n",
      "wiring length minimization, 135, 254\n",
      "\n",
      "\n",
      "References\n",
      "Albert MV, Schnabel A, Field DJ (2008) Innate visual learning through spontaneous activity pat-\n",
      "terns. PLoS Computational Biology 4(8)\n",
      "Albright TD, Stoner GR (2002) Contextual inﬂuences on visual processing. Annual Reviews in\n",
      "Neurosciences 25:339–379\n",
      "Amari SI (1998) Natural gradient works efﬁciently in learning. Neural Computation 10(2):251–\n",
      "276\n",
      "Amari SI, Cichocki A, Yang H (1996) A new learning algorithm for blind source separation. In:\n",
      "Advances in Neural Information Processing Systems 8, MIT Press, pp 757–763\n",
      "Angelucci A, Levitt JB, Walton EJS, Hup´e JM, Bullier J, Lund JS (2002) Circuits for local and\n",
      "global signal integration in primary visual cortex. J of Neuroscience 22(19):8633–8646\n",
      "Anzai A, Ohzawa I, Freeman RD (1999a) Neural mechanisms for encoding binocular disparity:\n",
      "Receptive ﬁeld position vs. phase. Journal of Neurophysiology 82(2):874–890\n",
      "Anzai A, Ohzawa I, Freeman RD (1999b) Neural mechanisms for processing binocular informa-\n",
      "tion: Simple cells. Journal of Neurophysiology 82(2):891–908\n",
      "Atick JJ, Redlich AN (1992) What does the retina know about natural scenes? Neural Computation\n",
      "4(2):196–210\n",
      "Atick JJ, Li Z, Redlich AN (1992) Understanding retinal color coding from ﬁrst principles. Neural\n",
      "Computation 4:559–572\n",
      "Attneave F (1954) Some informational aspects of visual perception. Psychological Review 61:183–\n",
      "193\n",
      "Attwell D, Laughlin SB (2001) An energy budget for signaling in the grey matter of the brain.\n",
      "Journal of Cerebral Blood Flow & Metabolism 21:1133–1145\n",
      "Baddeley RJ, Hancock PJB (1991) A statistical analysis of natural images matches psychophysi-\n",
      "cally derived orientation tuning curves. Proceedings of the Royal Society B 246(1317):219–223\n",
      "Baddeley RJ, Abbott LF, Booth MC, Sengpiel F, Freeman T, Wakeman EA, Rolls ET (1997)\n",
      "Responses of neurons in primary and inferior temporal visual cortices to natural scenes.\n",
      "Proc Royal Society ser B 264(1389):1775–1783\n",
      "Bak P, Tang C, Wiesenfeld K (1987) Self-organized criticality: an explanation of 1/f noise. Physical\n",
      "Review Letters 59:381–384.\n",
      "Balasubramaniam V, Kimber D, II MJB (2001) Metabolically efﬁcient information processing.\n",
      "Neural Computation 13:799–815\n",
      "Barlow HB (1961) Possible principles underlying the transformations of sensory messages. In:\n",
      "Rosenblith WA (ed) Sensory Communication, MIT Press, pp 217–234\n",
      "Barlow HB (1972) Single units and sensation: A neuron doctrine for perceptual psychology? Per-\n",
      "ception 1:371–394\n",
      "Barlow HB (2001a) The exploitation of regularities in the environment by the brain. Behavioral\n",
      "and Brain Sciences 24(3)\n",
      "453\n",
      "\n",
      "454\n",
      "References\n",
      "Barlow HB (2001b) Redundancy reduction revisited. Network: Computation in Neural Systems\n",
      "12:241–253\n",
      "Barlow HB, Blakemore C, Pettigrew JD (1967) The neural mechanism of binocular depth discrim-\n",
      "ination. Journal of Physiology 193:327–342\n",
      "Barrow HG, Bray AJ, Budd JML (1996) A self-organized model of ‘color blob’ formation. Neural\n",
      "Computation 8:1427–1448\n",
      "Bell AJ, Sejnowski TJ (1995) An information-maximization approach to blind separation and blind\n",
      "deconvolution. Neural Computation 7:1129–1159\n",
      "Bell AJ, Sejnowski TJ (1996) Learning higher-order structure of a natural sound. Network 7:261–\n",
      "266\n",
      "Bell AJ, Sejnowski TJ (1997) The ‘independent components’ of natural scenes are edge ﬁlters.\n",
      "Vision Research 37:3327–3338\n",
      "Bell CC, Caputi A, Grant K, Serrier J (1993) Storage of a sensory pattern by anti-hebbian synaptic\n",
      "plasticity in an electric ﬁsh. Proceedings of the National Academy of Science (USA) 90:4650–\n",
      "4654\n",
      "Belouchrani A, Meraim KA, Cardoso JF, Moulines E (1997) A blind source separation technique\n",
      "based on second order statistics. IEEE Trans on Signal Processing 45(2):434–444\n",
      "Bergner S, Drew MS (2005) Spatiotemporal-chromatic structure of natural scenes. In: Proc. IEEE\n",
      "Int. Conf. on Image Processing (ICIP2005), pp II–301–4\n",
      "Berkes P, Wiskott L (2005) Slow feature analysis yields a rich repertoire of complex cell properties.\n",
      "Journal of Vision 5(6):579–602\n",
      "Berkes P, Wiskott L (2007) Analysis and interpretation of quadratic models of receptive ﬁelds.\n",
      "Nature Protocols 2(2):400–407\n",
      "Besag J (1975) Statistical analysis of non-lattice data. The Statistician 24:179–195\n",
      "Bienenstock EL, Cooper LN, Munro PW (1982) Theory for the development of neuron selectivity:\n",
      "orientation speciﬁcity and binocular interation in visual cortex. Journal of Neuroscience 2:32–\n",
      "48\n",
      "Bishop CM (2006) Pattern Recognition and Machine Learning. Springer\n",
      "Blasdel GG (1992) Orientation selectivity, preference, and continuity in monkey striate cortex.\n",
      "Journal of Neuroscience 12(8):3139–3161\n",
      "Bonhoeffer T, Grinvald A (1991) Iso-orientation domains in cat visual cortex are arranged in\n",
      "pinwheel-like patterns. Nature 353:429–431\n",
      "Bonin V, Mante V, Carandini M (2006) The statistical computation underlying contrast gain con-\n",
      "trol. Journal of Neuroscience 26:6346–6353\n",
      "Boynton G, Hedg´e J (2004) Visual cortex: The continuing puzzle of area V2. Current Biology\n",
      "14:R523–R524\n",
      "Bray A, Martinez D (2003) Kernel-based extraction of slow features: complex cells learn disparity\n",
      "and translation-invariance from natural images. In: Becker S, Thrun S, Obermayer K (eds)\n",
      "Advances in Neural Information Processing Systems, The MIT Press, vol 15, pp 253–260\n",
      "Brunet JP, Tamayo P, Golub TR, Mesirov JP (2004) Metagenes and molecular pattern discovery\n",
      "using matrix factorization. Proceedings of the National Academy of Sciences 101:4164–4169\n",
      "Buchsbaum G, Bloch O (2002) Color categories revealed by non-negative matrix factorization of\n",
      "munsell color spectra. Vision Research 42:559–563\n",
      "Burt PJ, Adelson EH (1983) The laplacian pyramid as a compact image code. IEEE Trans on\n",
      "Communications 4:532–540\n",
      "Burton GJ, Moorehead TR (1987) Color and spatial structure in natural scenes. Applied Optics\n",
      "26:157–170\n",
      "Cai D, DeAngelis GC, Freeman RD (1997) Spatiotemporal receptive ﬁeld organization in the lat-\n",
      "eral geniculate nucleus of cats and kittens. Journal of Neurophysiology 78(2):1045–1061\n",
      "Cand`es EJ, Demanet L, Donoho DL, Ying L (2005) Fast discrete curvelet transforms. Multiscale\n",
      "Model Simul 5:861–899\n",
      "Cang J, Renter´ıa RC, M K, X L, Copenhagen ea D R (2005) Development of precise maps in visual\n",
      "cortex requires patterned spontaneous activity in the retina. Neuron 48:797–809\n",
      "\n",
      "References\n",
      "455\n",
      "Carandini M (2004) Receptive ﬁelds and suppressive ﬁelds in the early visual system. In: Gaz-\n",
      "zaniga MS (ed) The Cognitive Neurosciences III, MIT Press\n",
      "Carandini M, Heeger DJ, Movshon JA (1997) Linearity and normalization in simple cells of the\n",
      "macaque primary visual cortex. Journal of Neuroscience 17:8621–8644\n",
      "Carandini M, Heeger DJ, Movshon JA (1999) Linearity and gain control in V1 simple cells. In:\n",
      "Jones EG, Ulinski PS (eds) Models of cortical function, Cerebral Cortex, vol 13, Plenum Press,\n",
      "pp 401–443\n",
      "Carandini M, Demb JB, Mante V, Tolhurst DJ, Dan Y, Olshausen BA, Gallant JL, Rust NC (2005)\n",
      "Do we know what the early visual system does? J of Neuroscience 25(46):10,577–10,597\n",
      "Cardoso JF (1989) Source separation using higher order moments. In: Proc. IEEE Int. Conf. on\n",
      "Acoustics, Speech and Signal Processing (ICASSP’89), Glasgow, UK, pp 2109–2112\n",
      "Cardoso JF, Laheld BH (1996) Equivariant adaptive source separation. IEEE Trans on Signal Pro-\n",
      "cessing 44(12):3017–3030\n",
      "Cavaco S, Lewicki MS (2007) Statistical modeling of intrinsic structures in impacts sounds. J of\n",
      "the Acoustical Society of America 121:3558–3568\n",
      "Caywood MS, Willmore B, Tolhurst DJ (2004) Independent components of color natural scenes\n",
      "resemble V1 neurons in their spatial and color tuning. J of Neurophysiology 91:2859–2873\n",
      "Chance FS, Nelson SB, Abbott LF (1999) Complex cells as cortically ampliﬁed simple cells. Na-\n",
      "ture Neuroscience 2(3):277–282\n",
      "Chandler DM, Field DJ (2007) Estimates of the information content and dimensionality of natural\n",
      "scenes from proximity distributions. Journal of the Optical Society of America A 24(4):922–\n",
      "941\n",
      "Chen BL, Hall DH, Chklovskii DB (2006) Wiring optimization can relate neuronal structure and\n",
      "function. Proc National Academy of Sciences (USA) 103(1):4723–4728\n",
      "Chen X, Han F, Poo MM, Dan Y (2007) Excitatory and suppressive receptive ﬁeld subunits\n",
      "in awake monkey primary visual cortex (v1). Proc National Academy of Sciences (USA)\n",
      "104:19,120–19,125\n",
      "Cohen L (1995) Time-Frequency Analysis. Prentice Hall Signal Processing Series, Prentice Hall\n",
      "Comon P (1994) Independent component analysis—a new concept? Signal Processing 36:287–314\n",
      "Cover TM, Thomas JA (2006) Elements of Information Theory, 2nd edn. Wiley\n",
      "Crowder NA, van Kleef J, Dreher B, Ibbotson MR (2007) Complex cells increase their phase\n",
      "sensitivity at low contrasts and following adaptation. Journal of Neurophysiology 98:1155–\n",
      "1166\n",
      "Dakin SC, Mareschal I, Bex PJ (2005) An oblique effect for local motion: Psychophysics and\n",
      "natural movie statistics. Journal of Vision 5:878–887\n",
      "Dan Y, Poo MM (2004) Spike timing-dependent plasticity of neural circuits. Neuron 44:23–30\n",
      "Dan Y, Atick JJ, Reid RC (1996a) Efﬁcient coding of natural scenes in the lateral geniculate nu-\n",
      "cleus: Experimental test of a computational theory. Journal of Neuroscience 16:3351–3362\n",
      "Dan Y, Atick JJ, Reid RC (1996b) Efﬁcient coding of natural scenes in the lateral geniculate nu-\n",
      "cleus: experimental test of a computational theory. The Journal of Neuroscience 16(10):3351–\n",
      "3362\n",
      "Davis GW, Naka K (1980) Spatial organization of catﬁsh retinal neurons. I. single- and random-bar\n",
      "stimulation. J of Neurophysiology 43:807–831\n",
      "Dayan P, Abbott LF (2001) Theoretical Neuroscience. The MIT Press\n",
      "DeAngelis GC, Ohzawa I, Freeman RD (1993a) Spatiotemporal organization of simple-cell re-\n",
      "ceptive ﬁelds in the cat’s striate cortex. I. General characteristics and postnatal development.\n",
      "Journal of Neurophysiology 69(4):1091–1117\n",
      "DeAngelis GC, Ohzawa I, Freeman RD (1993b) Spatiotemporal organization of simple-cell recep-\n",
      "tive ﬁelds in the cat’s striate cortex. II. Linearity of temporal and spatial summation. Journal of\n",
      "Neurophysiology 69(4):1118–1135\n",
      "DeAngelis GC, Ohzawa I, Freeman RD (1995) Receptive-ﬁeld dynamics in the central visual path-\n",
      "ways. Trends in Neurosciences 18(10):451–458\n",
      "\n",
      "456\n",
      "References\n",
      "DeAngelis GC, Ghose GM, Ohzawa I, Freeman RD (1999) Functional micro-organization of\n",
      "primary visual cortex: Receptive ﬁeld analysis of nearby neurons. Journal of Neuroscience\n",
      "19(10):4046–4064\n",
      "Delfosse N, Loubaton P (1995) Adaptive blind separation of independent sources: a deﬂation ap-\n",
      "proach. Signal Processing 45:59–83\n",
      "Deriugin NG (1956) The power spectrum and the correlation function of the television signal.\n",
      "Telecommunications 1:1–12\n",
      "DeValois RL, Albrecht DG, Thorell LG (1982) Spatial frequency selectivity of cells in macaque\n",
      "visual cortex. Vision Research 22:545–559\n",
      "Doi E, Inui T, Lee TW, Wachtler T, Sejnowski TJ (2003) Spatiochromatic receptive ﬁeld properties\n",
      "derived from information-theoretic analyses of cone mosaic responses to natural scenes. Neural\n",
      "Computation 15:397–417\n",
      "Dong DW, Atick JJ (1995a) Statistics of natural time-varying images. Network: Computation in\n",
      "Neural Systems 6(3):345–358\n",
      "Dong DW, Atick JJ (1995b) Temporal decorrelation: a theory of lagged and nonlagged responses\n",
      "in the lateral geniculate nucleus. Network: Computation in Neural Systems 6(2):159–178\n",
      "Donoho DL (1995) De-noising by soft-thresholding. IEEE Trans on Information Theory 41:613–\n",
      "627\n",
      "Donoho DL, Johnstone IM (1995) Adapting to unknown smoothness via wavelet shrinkage. J of\n",
      "the American Statistical Association 90:1200–1224\n",
      "Donoho DL, Stodden V (2004) When does non-negative matrix factorization give a correct de-\n",
      "composition into parts? In: Advances in Neural Information Processing 16 (Proc. NIPS*2003),\n",
      "MIT Press\n",
      "Donoho DL, Johnstone IM, Kerkyacharian G, Picard D (1995) Wavelet shrinkage: asymptopia?\n",
      "Journal of the Royal Statistical Society, Ser B 57:301–337\n",
      "Dror RO, Willsky AS, Adelson EH (2004) Statistical characterization of real-world illumination.\n",
      "Journal of Vision 4(9):821–837\n",
      "Durbin R, Mitchison G (1990) A dimension reduction framework for understanding cortical maps.\n",
      "Nature 343:644–647\n",
      "Edelman A, Arias TA, Smith ST (1998) The geometry of algorithms with orthogonality constraints.\n",
      "SIAM Journal on Matrix Analysis and Applications 20(2):303–353\n",
      "Edwards DP, Purpura HP, Kaplan E (1996) Contrast sensitivity and spatial frequency response\n",
      "of primate cortical neurons in and around the cytochromase oxidase blobs. Vision Research\n",
      "35(11):1501–23\n",
      "Elder JH, Goldberg RM (2002) Ecological statistics of gestalt laws for the perceptual organization\n",
      "of contours. Journal of Vision 2(4):324–353\n",
      "Embrechts P, Maejima M (2000) An introduction to the theory of self-similar stochastic processes.\n",
      "International Journal of Modern Physics B 14:1399–1420\n",
      "Emerson RC, Bergen JR, Adelson EH (1992) Directionally selective complex cells and the com-\n",
      "putation of motion energy in cat visual cortex. Vision Research 32:203–218\n",
      "Erwin E, Miller KD (1996) Modeling joint development of ocular dominance and orientation maps\n",
      "in primary visual cortex. In: Bower JM (ed) Computational neuroscience: Trends in research\n",
      "1995, New York: Academic Press, pp 179–184\n",
      "Erwin E, Miller KD (1998) Correlation-based development of ocularly-matched orientation and\n",
      "ocular dominance maps: determination of required input activities. Journal of Neuroscience\n",
      "18:5908–5927\n",
      "Fairhall AL, Lewen GD, Bialek W, de Ruyter van Steveninck RR (2001) Efﬁciency and ambiguity\n",
      "in an adaptive neural code. Nature 412:787–792\n",
      "Felsen G, Touryan J, Han F, Dan Y (2005) Cortical sensitivity to visual features in natural scenes.\n",
      "PLoS Biology 3(10):e342\n",
      "Field DJ (1987) Relations between the statistics of natural images and the response properties of\n",
      "cortical cells. Journal of the Optical Society of America A 4:2379–2394\n",
      "Field DJ (1994) What is the goal of sensory coding? Neural Computation 6:559–601\n",
      "\n",
      "References\n",
      "457\n",
      "Field DJ (1999) Wavelets, vision and the statistics of natural scenes. Philosophical Transactions of\n",
      "the Royal Society A 357(1760):2527 – 2542\n",
      "Fischer B, Kruger J (1979) Disparity tuning and binocularity of single neurons in cat visual cortex.\n",
      "Experimental Brain Research 35:1–8\n",
      "Fitzpatrick D (2000) Seeing beyond the receptive ﬁeld in primary visual cortex. Current Opinion\n",
      "in Neurobiology 10(4):438–443\n",
      "F¨oldi´ak P (1990) Forming sparse representations by local anti-Hebbian learning. Biological Cy-\n",
      "bernetics 64:165–170\n",
      "F¨oldi´ak P (1991) Learning invariance from transformation sequences. Neural Computation 3:194–\n",
      "200\n",
      "Freeman WT, Adelson EH (1991) The design and use of steerable ﬁlters. IEEE Transactions on\n",
      "Pattern Analysis and Machine Intelligence 13(9):891–906\n",
      "Fukushima K (1980) Neocognitron: A self-organizing neural network model for a mechanism of\n",
      "pattern recognition unaffected by shift in position. Biological Cybernetics 36(4):193–202\n",
      "Fukushima K, Okada M, Hiroshige K (1994) Neocognitron with dual C-cell layers. Neural Net-\n",
      "works 7(1):41–47\n",
      "Gallant J, Connor C, van Essen D (1998) Neural activity in areas V1, V2 and V4 during free\n",
      "viewing of natural scenes compared to controlled viewing. NeuroReport 9:85–90\n",
      "Gallant JL, Braun J, van Essen DC (1993) Selectivity for polar, hyperbolic, and cartesian gratings\n",
      "in macaque visual cortex. Science 259:100–103\n",
      "Gardner JL, Sun P, Waggoner RA, Ueno K, Tanaka K, Cheng K (2005) Contrast adaptation and\n",
      "representation in human early visual cortex. Neuron 47:607–620\n",
      "Garrigues PJ, Olshausen BA (2008) Learning horizontal connections in a sparse coding model of\n",
      "natural images. In: Advances in Neural Information Processing Systems, vol 20, MIT Press\n",
      "Gehler P, Welling M (2005) Products of ”edge-perts”. In: Advances in Neural Information Pro-\n",
      "cessing Systems, vol 17, MIT Press\n",
      "Geisler WS, Perry JS, Super BJ, Gallogly DP (2001) Edge co-occurrence in natural images predicts\n",
      "contour grouping performance. Vision Research 41:711–724\n",
      "Gilbert CD, Wiesel TN (1985) Intrinsic connectivity and receptive ﬁeld properties in visual cortex.\n",
      "Vision Research 25(3):365–374\n",
      "Gonzales RC, Woods RE (2002) Digital Image Processing, 2nd edn. Prentice Hall\n",
      "Gray MS, Pouget A, Zemel RS, Nowlan SJ, Sejnowski TJ (1998) Reliable disparity estimation\n",
      "through selective integration. Visual Neuroscience 15(3):511–528\n",
      "Grenander U (1976–1981) Lectures in Pattern Theory I, II, and III: Pattern analysis, pattern syn-\n",
      "thesis and regular structures. Springer-Verlag\n",
      "Grenander U, Srivastava A (2001) Probability models for clutter in natural images. IEEE Transac-\n",
      "tions on Pattern Analysis and Machine Intelligence 23:424–429\n",
      "Grifﬁn LD (2007) The second order local-image-structure solid. IEEE Transaction on Pattern Anal-\n",
      "ysis and Machine Intelligence 29(8):1355–1366\n",
      "Grifﬁn LD, Lillholm M, Nielsen M (2004) Natural image proﬁles are most likely to be step edges.\n",
      "Vision Research 44(4):407–421\n",
      "Grossberg S, Mingolla E (1985) Neural dynamics of perceptual grouping: textures, boundaries and\n",
      "emergent segmentations. Perception and psychophysics 38(2):141–171\n",
      "Hafner VV, Fend M, Lungarella M, Pfeifer R, K¨onig P, K¨ording KP (2003) Optimal coding for\n",
      "naturally occurring whisker deﬂections. In: Proc. Int. Conf. on Artiﬁcial Neural Networks\n",
      "(ICANN/ICONIP2002), pp 805–812\n",
      "Hancock PJB, Baddeley RJ, Smith LS (1992) The principal components of natural images. Net-\n",
      "work: Computation in Neural Systems 3:61–72\n",
      "Hansel D, van Vreeswijk C (2002) How noise contributes to contrast invariance of orientation\n",
      "tuning in cat visual cortex. J of Neuroscience 22:5118–5128\n",
      "Hansen BC, Hess RF (2007) Structural sparseness and spatial phase alignment in natural scenes.\n",
      "Journal of the Optical Society of America A 24(7):1873–1885\n",
      "Hashimoto W (2003) Quadratic forms in natural images. Network: Computation in Neural Systems\n",
      "14(4):765–788\n",
      "\n",
      "458\n",
      "References\n",
      "van Hateren JH (1992) Real and optimal neural images in early vision. Nature 360:68–70\n",
      "van Hateren JH (1993) Spatial, temporal and spectral pre-processing for colour vision. Proc Royal\n",
      "Society, Ser B 251:61–68\n",
      "van Hateren JH, Ruderman DL (1998) Independent component analysis of natural image sequences\n",
      "yields spatiotemporal ﬁlters similar to simple cells in primary visual cortex. Proc Royal Society,\n",
      "Ser B 265:2315–2320\n",
      "van Hateren JH, van der Schaaf A (1998) Independent component ﬁlters of natural images com-\n",
      "pared with simple cells in primary visual cortex. Proc Royal Society, Ser B 265:359–366\n",
      "Heeger D (1992) Normalization of cell responses in cat striate cortex. Visual Neuroscience 9:181–\n",
      "198\n",
      "Hegd´e J, Essen DCV (2007) A comparative study of shape representation in macaque visual areas\n",
      "V2 and V4. Cerebral Cortex 17(5):1100–16\n",
      "Helmholtz H (1867) Handbuch der physiologischen Optik. Dritter Abschnitt. Die Lehre von den\n",
      "Gesichtswahrnehmungen. (Physiological Optics. Volume III. The theory of the perceptions of\n",
      "vision.). Leipzig: Leopold Voss\n",
      "Henderson JM (2003) Human gaze control during real-world scene perception. Trends in Cognitive\n",
      "Science 7(11):498–504\n",
      "H´erault J, Ans B (1984) Circuits neuronaux `a synapses modiﬁables: d´ecodage de messages com-\n",
      "posites par apprentissage non supervis´e. Comptes-Rendus de l’Acad´emie des Sciences 299(III-\n",
      "13):525–528\n",
      "Hinton GE (1989) Connectionist learning procedures. Artiﬁcial Intelligence 40(1–3):185–234\n",
      "Hinton GE (2002) Training products of experts by minimizing contrastive divergence. Neural Com-\n",
      "putation 14(8):1771–1800\n",
      "Hinton GE, Ghahramani Z (1997) Generative models for discovering sparse distributed represen-\n",
      "tations. Phil Trans R Soc Lond B 352:1177–1190\n",
      "Hosoya T, Baccus SA, Meister M (2005) Dynamic predictive coding in the retina. Nature\n",
      "436(10):71–77\n",
      "Hoyer PO (2004) Non-negative matrix factorization with sparseness constraints. Journal of Ma-\n",
      "chine Learning Research 5:1457–1469\n",
      "Hoyer PO, Hyv¨arinen A (2000) Independent component analysis applied to feature extraction from\n",
      "colour and stereo images. Network: Computation in Neural Systems 11(3):191–210\n",
      "Hoyer PO, Hyv¨arinen A (2002) A multi-layer sparse coding network learns contour coding from\n",
      "natural images. Vision Research 42(12):1593–1605\n",
      "Hubel DH, Wiesel TN (1962) Receptive ﬁelds, binocular interaction and functional architecture in\n",
      "the cat’s visual cortex. Journal of Physiology 73:218–226\n",
      "Hubel DH, Wiesel TN (1963) Receptive ﬁelds of cells in striate cortex of very young, visually\n",
      "inexperienced kittens. Journal of Neurophysiology 26:994–1002\n",
      "Hubel DH, Wiesel TN (1968) Receptive ﬁelds and functional architecture of monkey striate cortex.\n",
      "J of Physiology (London) 195:215–243\n",
      "Hubel DH, Wiesel TN (1977) Functional architecture of macaque monkey visual cortex (Ferrier\n",
      "Lecture). Proc Royal Soc London, Ser B 198:1–59\n",
      "H¨ubener M, Shoham D, Grinvald A, Bonhoeffer T (1997) Spatial relationships among three colum-\n",
      "nar systems in cat area 17. J of Neuroscience 17(23):9270–9284\n",
      "Hup´e JM, James AC, Payne BR, Lomber SG, Girard P, Bullier J (1998) Cortical feedback improves\n",
      "discrimination between ﬁgure and background by V1, V2 and V3 neurons. Nature 394:784–787\n",
      "Hurri J (2006) Learning cue-invariant visual responses. In: Advances in Neural Information Pro-\n",
      "cessing Systems, vol 18, MIT Press\n",
      "Hurri J, Hyv¨arinen A (2003a) Simple-cell-like receptive ﬁelds maximize temporal coherence in\n",
      "natural video. Neural Computation 15(3):663–691\n",
      "Hurri J, Hyv¨arinen A (2003b) Temporal and spatiotemporal coherence in simple-cell responses:\n",
      "A generative model of natural image sequences. Network: Computation in Neural Systems\n",
      "14(3):527–551\n",
      "Hyv¨arinen A (1999a) Fast and robust ﬁxed-point algorithms for independent component analysis.\n",
      "IEEE Transactions on Neural Networks 10(3):626–634\n",
      "\n",
      "References\n",
      "459\n",
      "Hyv¨arinen A (1999b) Sparse code shrinkage: Denoising of nongaussian data by maximum likeli-\n",
      "hood estimation. Neural Computation 11(7):1739–1768\n",
      "Hyv¨arinen A (2001a) Blind source separation by nonstationarity of variance: A cumulant-based\n",
      "approach. IEEE Transactions on Neural Networks 12(6):1471–1474\n",
      "Hyv¨arinen A (2001b) Complexity pursuit: Separating interesting components from time-series.\n",
      "Neural Computation 13(4):883–898\n",
      "Hyv¨arinen A (2002) An alternative approach to infomax and independent component analysis.\n",
      "Neurocomputing 44-46(C):1089–1097\n",
      "Hyv¨arinen A (2005) Estimation of non-normalized statistical models using score matching. J of\n",
      "Machine Learning Research 6:695–709\n",
      "Hyv¨arinen A (2007) Connections between score matching, contrastive divergence, and pseudolike-\n",
      "lihood for continuous-valued variables. IEEE Transactions on Neural Networks 18:1529–1531\n",
      "Hyv¨arinen A, Hoyer PO (2000) Emergence of phase and shift invariant features by decomposition\n",
      "of natural images into independent feature subspaces. Neural Computation 12(7):1705–1720\n",
      "Hyv¨arinen A, Hoyer PO (2001) A two-layer sparse coding model learns simple and complex cell\n",
      "receptive ﬁelds and topography from natural images. Vision Research 41(18):2413–2423\n",
      "Hyv¨arinen A, Hurri J (2004) Blind separation of sources that have spatiotemporal variance depen-\n",
      "dencies. Signal Processing 84(2):247–254\n",
      "Hyv¨arinen A, Inki M (2002) Estimating overcomplete independent component bases from image\n",
      "windows. J of Mathematical Imaging and Vision 17:139–152\n",
      "Hyv¨arinen A, K¨oster U (2007) Complex cell pooling and the statistics of natural images. Network:\n",
      "Computation in Neural Systems 18:81–100\n",
      "Hyv¨arinen A, Oja E (1997) A fast ﬁxed-point algorithm for independent component analysis. Neu-\n",
      "ral Computation 9(7):1483–1492\n",
      "Hyv¨arinen A, Oja E (2000) Independent component analysis: Algorithms and applications. Neural\n",
      "Networks 13(4-5):411–430\n",
      "Hyv¨arinen A, Hoyer PO, Inki M (2001a) Topographic independent component analysis. Neural\n",
      "Computation 13(7):1527–1558\n",
      "Hyv¨arinen A, Karhunen J, Oja E (2001b) Independent Component Analysis. Wiley Interscience\n",
      "Hyv¨arinen A, Hurri J, V¨ayrynen J (2003) Bubbles: A unifying framework for low-level statistical\n",
      "properties of natural image sequences. J of the Optical Society of America A 20(7):1237–1252\n",
      "Hyv¨arinen A, Gutmann M, Hoyer PO (2005a) Statistical model of natural stimuli predicts edge-\n",
      "like pooling of spatial frequency channels in V2. BMC Neuroscience 6(12)\n",
      "Hyv¨arinen A, Hoyer PO, Hurri J, Gutmann M (2005b) Statistical models of images and early vi-\n",
      "sion. In: Proc. Int. Conf. Adaptive Knowledge Representation and Reasoning, Espoo, Finland,\n",
      "pp 1–14\n",
      "Johnstone IM, Silverman BW (2005) Empirical Bayes selection of wavelet thresholds. Annals of\n",
      "Statistics 33(4):1700–1752\n",
      "Jones JP, Palmer LA (1987a) An evaluation of the two-dimensional gabor ﬁlter model of simple\n",
      "receptive ﬁelds in cat striate cortex. Journal of Neurophysiology 58:1233–1258\n",
      "Jones JP, Palmer LA (1987b) The two-dimensional spatial structure of simple receptive ﬁelds in\n",
      "cat striate cortex. Journal of Neurophysiology 58:1187–1211\n",
      "Jones JP, Stepnoski A, Palmer LA (1987) The two-dimensional spectral structure of simple recep-\n",
      "tive ﬁelds in cat striate cortex. Journal of Neurophysiology 58:1212–1232\n",
      "Jung K, Kim EY (2004) Automatic text extraction for content-based image indexing. Advances in\n",
      "Knowledge Discovery and Data Mining: Proceedings Lecture Notes in Artiﬁcial Intelligence\n",
      "3056:497–507\n",
      "Jutten C, H´erault J (1991) Blind separation of sources, part I: An adaptive algorithm based on\n",
      "neuromimetic architecture. Signal Processing 24:1–10\n",
      "Kandel ER, Schwartz JH, Jessell TM (2000) Principles of neural science, 4th edn. McGraw-Hill\n",
      "Kapadia MK, Ito M, Gilbert CD, Westheimer G (1995) Improvement in visual sensitivity by\n",
      "changes in local context: parallel studies in human observers and in V1 of alert monkeys. Neu-\n",
      "ron 15(4):843–856\n",
      "\n",
      "460\n",
      "References\n",
      "Kapadia MK, Westheimer G, Gilbert CD (2000) Spatial distribution of contextual interactions in\n",
      "primary visual cortex and in visual perception. Journal of Neurophysiology 84:2048–2062\n",
      "Kara P, Reinagel P, Reid C (2000) Low response variability in simultaneously recorded retinal,\n",
      "thalamic, and cortical neurons. Neuron 27(3):635–646\n",
      "Karklin Y, Lewicki MS (2005) A hierarchical Bayesian model for learning nonlinear statistical\n",
      "regularities in natural signals. Neural Computation 17:397–423\n",
      "Karklin Y, Lewicki MS (2008) Emergence of complex cell properties by learning to generalize in\n",
      "natural scenes. Nature\n",
      "Kayser C, K¨ording K, K¨onig P (2003) Learning the nonlinearity of neurons from natural visual\n",
      "stimuli. Neural Computation 15(8):1751–1759\n",
      "Kersten D, Schrater P (2002) Pattern inference theory: A probabilistic approach to vision. In:\n",
      "Mausfeld R, Heyer D (eds) Perception and the Physical World, Wiley & Sons\n",
      "Kim PM, Tidor B (2003) Subsystem identiﬁcation through dimensionality reduction of large-scale\n",
      "gene expression data. Genome Research 13:1706–1718\n",
      "Klein DJ, K¨onig P, K¨ording KP (2003) Sparse spectrotemporal coding of sounds. EURASIP Jour-\n",
      "nal of Applied Signal Processing 3:659–667\n",
      "Knill DC, Richards W (eds) (1996) Perception as Bayesian Inference. Cambridge University Press\n",
      "Koenderink JJ, van Doorn AJ (1987) Representation of local geometry in the visual system. Bio-\n",
      "logical Cybernetics 55:367–375\n",
      "Kohonen T (1982) Self-organized formation of topologically correct feature maps. Biological Cy-\n",
      "bernetics 43(1):56–69\n",
      "Kohonen T (1996) Emergence of invariant-feature detectors in the adaptive-subspace self-\n",
      "organizing map. Biological Cybernetics 75:281–291\n",
      "Kohonen T (2001) Self-Organizing Maps, 3rd edn. Springer\n",
      "Kohonen T, Kaski S, Lappalainen H (1997) Self-organized formation of various invariant-feature\n",
      "ﬁlters in the adaptive-subspace SOM. Neural Computation 9(6):1321–1344\n",
      "K¨ording K, Kayser C, Einh¨auser W, K¨onig P (2004) How are complex cell properties adapted to\n",
      "the statistics of natural stimuli? Journal of Neurophysiology 91(1):206–212\n",
      "K¨oster U, Hyv¨arinen A (2007) A two-layer ICA-like model estimated by score matching. In: Proc.\n",
      "Int. Conf. on Artiﬁcial Neural Networks (ICANN2007), Porto, Portugal, pp 798–807\n",
      "K¨oster U, Hyv¨arinen A (2008) A two-layer model of natural stimuli estimated with score matching\n",
      "Submitted manuscript\n",
      "K¨oster U, Lindgren JT, Gutmann M, Hyv¨arinen A (2009a) Learning natural image structure with a\n",
      "horizontal product model. In: Proc. Int. Conference on Independent Component Analysis and\n",
      "Blind Signal Separation (ICA2009), Paraty, Brazil\n",
      "K¨oster U, Lindgren JT, Hyv¨arinen A (2009b) Estimating markov random ﬁeld potentials for nat-\n",
      "ural images. In: Proc. Int. Conference on Independent Component Analysis and Blind Signal\n",
      "Separation (ICA2009), Paraty, Brazil\n",
      "Koulakov AA, Chklovskii DB (2001) Orientation preference patterns in mammalian visual cortex:\n",
      "a wire length minimization approach. Neuron 29:519–27\n",
      "Kovesi P (1999) Image features from phase congruency. Videre: Journal of Computer Vision Re-\n",
      "search 1(3)\n",
      "Kretzmer ER (1952) Statistics of television signals. Bell Syst Tech Journal 31:751–763\n",
      "Krieger G, Rentschler I, Hauske G, Schill1 K, Zetzsche C (2000) Object and scene analysis by\n",
      "saccadic eye-movements: an investigation with higher-order statistics. Spatial Vision 13:201–\n",
      "214\n",
      "Kr¨uger N (1998) Collinearity and parallelism are statistically signiﬁcant second order relations of\n",
      "complex cell responses. Neural Processing Letters 8:117–129\n",
      "Kr¨uger N, W¨org¨otter F (2002) Multi-modal estimation of collinearity and parallelism in natural\n",
      "image sequences. Network: Computation in Neural Systems 13:553–576\n",
      "Kurki I, Hyv¨arinen A, Laurinen P (2006) Collinear context (and learning) change the proﬁle of the\n",
      "perceptual ﬁlter. Vision Research 46(13):2009–2014\n",
      "Kushner H, Clark D (1978) Stochastic approximation methods for constrained and unconstrained\n",
      "systems. Springer-Verlag\n",
      "\n",
      "References\n",
      "461\n",
      "Lamme VAF (1995) The neurophysiology of ﬁgure-ground segregation in primary visual cortex. J\n",
      "of Neuroscience 15:1605–1615\n",
      "Laughlin S (1981) A simple coding procedure enhances a neuron’s information capacity.\n",
      "Zeitschrift f¨ur Naturforschung 36 C:910–912\n",
      "Lee AB, Mumford D, Huang J (2001) Occlusion models for natural images: A statistical study of\n",
      "scale-invariant dead leaves model. International Journal of Computer Vision 41:35–59\n",
      "Lee AB, Pedersen KS, Mumford D (2003) The nonlinear statistics of high-contrast patches in\n",
      "natural images. International Journal of Computer Vision 54:83–103\n",
      "Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization.\n",
      "Nature 401:788–791\n",
      "Lee DD, Seung HS (2001) Algorithms for non-negative matrix factorization. In: Advances in Neu-\n",
      "ral Information Processing 13 (Proc. NIPS*2000), MIT Press\n",
      "Lee TS, Mumford D (2003) Hierarchical Bayesian inference in the visual cortex. J of the Optical\n",
      "Society of America A 20(7):1434–1448\n",
      "Lennie P (2003) The cost of cortical computation. Current Biology 13:493–497\n",
      "LeVay S, Voigt T (1988) Ocular dominance and disparity coding in cat visual cortex. Visual Neu-\n",
      "roscience 1:395–414\n",
      "Levy WB, Baxter RA (1996) Energy efﬁcient neural codes. Neural Computation 8:531–43\n",
      "Lewicki MS (2002) Efﬁcient coding of natural sounds. Nature Neuroscience 5(4):356–363\n",
      "Li SZ (2001) Markov Random Field Modeling in Image Analysis, 2nd edn. Springer\n",
      "Li SZ, Hou X, Zhang H, Cheng Q (2001) Learning spatially localized parts-based representations.\n",
      "In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR 2001), Hawaii, USA\n",
      "Li Z (1999) Pre-attentive segmentation in the primary visual cortex. Spatial Vision 13(1):25–50\n",
      "Li Z, Atick JJ (1994) Efﬁcient stereo coding in the multiscale representation. Network: Computa-\n",
      "tion in Neural Systems 5:157–174\n",
      "Lindgren JT, Hyv¨arinen A (2004) Learning high-level independent components of images through\n",
      "a spectral representation. In: Proc. Int. Conf. on Pattern Recognition (ICPR2004), Cambridge,\n",
      "UK, pp 72–75\n",
      "Lindgren JT, Hyv¨arinen A (2007) Emergence of conjunctive visual features by quadratic indepen-\n",
      "dent component analysis. In: Advances in Neural Information Processing Systems, vol 19, MIT\n",
      "Press\n",
      "Linsker R (1988) Self-organization in a perceptual network. Computer 21:105–117\n",
      "Liu X, Cheng L (2003) Independent spectral representations of images for recognition. J of the\n",
      "Optical Society of America A 20:1271–1282\n",
      "Livingstone MS, Hubel DH (1984) Anatomy and physiology of a color system in the primate visual\n",
      "cortex. Journal of Neuroscience 4:309–356\n",
      "L¨orincz A, Buzs´aki G (2000) Two-phase computational model training long-term memories in the\n",
      "entorhinal-hippocampal region. Annals of the New York Academy of Sciences 911(1):83–111\n",
      "Luenberger D (1969) Optimization by Vector Space Methods. Wiley\n",
      "Lyu S, Simoncelli EP (2007) Statistical modeling of images with ﬁelds of gaussian scale mixtures.\n",
      "In: Advances in Neural Information Processing Systems, vol 19, MIT Press\n",
      "Lyu S, Simoncelli EP (2008) Nonlinear extraction of ’independent components’ of elliptically\n",
      "symmetric densities using radial gaussianization. Tech. rep., Courant Inst. of Mathematical\n",
      "Sciences, New York University\n",
      "Mach E (1886) Die Analyse der Empﬁndungen und das Verh¨altnis des Physischen zum Psychis-\n",
      "chen. (The analysis of sensations, and the relation of the physical to the psychical.). Jena:\n",
      "Gustav Fischer\n",
      "Mackay DJC (2003) Information Theory, Inference and Learning Algorithms. Cambridge Univer-\n",
      "sity Press\n",
      "Maldonado PE, G¨odecke I, Gray CM, Bonhoeffer T (1997) Orientation selectivity in pinwheel\n",
      "centers in cat striate cortex. Science 276:1551–1555\n",
      "Malik J, Perona P (1990) Preattentive texture discrimination with early vision mechanisms. Journal\n",
      "of the Optical Society of America A 7(5):923–932\n",
      "\n",
      "462\n",
      "References\n",
      "Mallat SG (1989) A theory for multiresolution signal decomposition: The wavelet representation.\n",
      "IEEE Trans on Pattern Analysis and Machine Intelligence 11:674–693\n",
      "Malo J, Guti´errez J (2006) V1 non-linear properties emerge from local-to-global non-linear ICA.\n",
      "Network: Computation in Neural Systems 17:85–102\n",
      "Mandelbrot BB, van Ness JW (1968) Fractional Brownian motions, fractional noises and applica-\n",
      "tions. SIAM Review 10:422–437\n",
      "Mante V, Frazor RA, Bonin V, Geisler WS, Carandini M (2005) Independence of luminance and\n",
      "contrast in natural scenes and in the early visual system. Nature Neuroscience 8:1690–1697\n",
      "Mareschal I, Baker CL (1998a) A cortical locus for the processing of contrast-deﬁned contours.\n",
      "Nature Neuroscience 1(2):150–154\n",
      "Mareschal I, Baker CL (1998b) Temporal and spatial response to second-order stimuli in cat area\n",
      "18. J of Neurophysiology 80:2811–2823\n",
      "Marr D (1982) Vision. W. H. Freeman and Company, New York\n",
      "Martin DR, Fowlkes CC, Malik J (2004) Learning to detect natural image boundaries using lo-\n",
      "cal brightness, color, and texture cues. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence 26:530–549\n",
      "Matsuoka K, Ohya M, Kawamoto M (1995) A neural net for blind separation of nonstationary\n",
      "signals. Neural Networks 8(3):411–419\n",
      "Mechler F, Ringach DL (2002) On the classiﬁcation of simple and complex cells. Vision Research\n",
      "42:1017–1033\n",
      "Meister M, Berry II MJ (1999) The neural code of the retina. Neuron 22(3):435–450\n",
      "Mel BW, Ruderman DL, Archie KA (1998) Translation-invariant orientation tuning in visual ”com-\n",
      "plex” cells could derive from intradendritic computations. Journal of Neuroscience 18:4325–\n",
      "4334\n",
      "Memisevic RF, Hinton GE (2007) Unsupervised learning of image transformations. In: Computer\n",
      "Vision and Pattern Recognition\n",
      "Miller KD, Troyer TW (2002) Neural noise can explain expansive, power-law nonlinearities in\n",
      "neural response functions. J of Neurophysiology 87:653–659\n",
      "Mitchison G (1991) Removing time variation with the anti-Hebbian differential synapse. Neural\n",
      "Computation 3(3):312–320\n",
      "Mitchison G (1992) Axonal trees and cortical architecture. Trends in Neurosciences 15:122–126\n",
      "Mooijaart A (1985) Factor analysis for non-normal variables. Psychometrica 50:323–342\n",
      "Morrone MC, Burr DC (1988) Feature detection in human vision: a phase-dependent energy\n",
      "model. Proceedings of the Royal Society of London B 235:221–224\n",
      "Moulden B (1994) Collator units: Second-stage orientational ﬁlters. In: Higher-order processing in\n",
      "the visual system. Ciba Foundation Symposium 184, John Wiley, Chichester, U.K., pp 170–192\n",
      "Mountcastle VB (1997) The columnar organization of the neocortex. Brain 120:701–722\n",
      "Mullen KT (1985) The contrast sensitivity of human colour vision to red-green and blue-yellow\n",
      "chromatic gratings. Journal of Physiology 359:381–400\n",
      "Mumford D (1992) On the computational architecture of the neocortex. II. The role of cortico-\n",
      "cortical loops. Biological Cybernetics 66(3):241–251\n",
      "Mumford D (1994) Neuronal architectures for pattern-theoretic problems. In: Koch C, Davis JL\n",
      "(eds) Large-Scale Neuronal Theories of the Brain, The MIT Press\n",
      "Mumford D, Gidas B (2001) Stochastic models for generic images. Quarterly of Applied Mathe-\n",
      "matics 59:85–111\n",
      "Murray SO, Kersten D, Olshausen BA, Schrater P, Woods D (2002) Shape perception reduces\n",
      "activity in human primary visual cortex. Proc Natl Acad Sci (USA) 99:15,164–9\n",
      "Mury AA, Pont SC, Koenderink JJ (2007) Light ﬁeld constancy within natural scenes. Applied\n",
      "Optics 46:7308–7316\n",
      "Mussap AJ, Levi DM (1996) Spatial properties of ﬁlters underlying vernier acuity revealed by\n",
      "masking: Evidence for collator mechanisms. Vision Research 36(16):2459–2473\n",
      "Nadal JP, Parga N (1994) Non-linear neurons in the low noise limit: a factorial code maximizes\n",
      "information transfer. Network 5:565–581\n",
      "\n",
      "References\n",
      "463\n",
      "Neumann H, Sepp W (1999) Recurrent V1-V2 interaction in early visual boundary processing.\n",
      "Biological Cybernetics 81:425–444\n",
      "Nikias C, Mendel J (1993) Signal processing with higher-order spectra. IEEE Signal Processing\n",
      "Magazine pp 10–37\n",
      "Nikias C, Petropulu A (1993) Higher-Order Spectral Analysis - A Nonlinear Signal Processing\n",
      "Framework. Prentice Hall\n",
      "Nykamp DQ, Ringach DL (2002) Full identiﬁcation of a linear-nonlinear system via cross-\n",
      "correlation analysis. Journal of Vision 2(1):1–11\n",
      "Ohki K, Chung S, Ch’ng YH, Kara P, Reid RC (2005) Functional imaging with cellular resolution\n",
      "reveals precise micro-architecture in visual cortex. Nature 433:597–603\n",
      "Oja E (1982) A simpliﬁed neuron model as a principal component analyzer. J of Mathematical\n",
      "Biology 15:267–273\n",
      "Olshausen BA (2002) Sparse codes and spikes. In: Rao R, Olshausen B, Lewicki M (eds) Proba-\n",
      "bilistic Models of the Brain, MIT Press, pp 257–272\n",
      "Olshausen BA (2003) Principles of image representation in visual cortex. In: Chalupa LM, Werner\n",
      "JS (eds) The Visual Neurosciences, MIT Press\n",
      "Olshausen BA, Field DJ (1996) Emergence of simple-cell receptive ﬁeld properties by learning a\n",
      "sparse code for natural images. Nature 381:607–609\n",
      "Olshausen BA, Field DJ (1997) Sparse coding with an overcomplete basis set: A strategy employed\n",
      "by V1? Vision Research 37:3311–3325\n",
      "Olshausen BA, Field DJ (2004) Sparse coding of sensory inputs. Current Opinion in Neurobiology\n",
      "14:481–487\n",
      "Olshausen BA, Sallee P, Lewicki MS (2001) Learning sparse image codes using a wavelet pyramid\n",
      "architecture. In: Advances in Neural Information Processing Systems, vol 13, MIT Press, pp\n",
      "887–893\n",
      "Olzak LA, Wickens TD (1997) Discrimination of complex patterns: orientation information is\n",
      "integrated across spatial scale; spatial-frequency and contrast information are not. Perception\n",
      "26:1101–1120\n",
      "Oppenheim A, Schafer R (1975) Digital Signal Processing. Prentice-Hall\n",
      "Osindero S, Hinton GE (2008) Modeling image patches with a directed hierarchy of markov ran-\n",
      "dom ﬁelds. In: Advances in Neural Information Processing Systems, vol 20, MIT Press\n",
      "Osindero S, Welling M, Hinton GE (2006) Topographic product models applied to natural scene\n",
      "statistics. Neural Computation 18(2):381–414\n",
      "Paatero P, Tapper U (1994) Positive matrix factorization: A non-negative factor model with optimal\n",
      "utilization of error estimates of data values. Environmetrics 5:111–126\n",
      "Palmer SE (1999) Vision Science – Photons to Phenomenology. The MIT Press\n",
      "Papoulis A, Pillai SU (2001) Probability, Random Variables, and Stochastic Processes, 4th edn.\n",
      "McGraw-Hill\n",
      "Parra L, Spence CD, Sajda P, Ziehe A, M¨uller KR (2000) Unmixing hyperspectral data. In: Ad-\n",
      "vances in Neural Information Processing Systems 12, MIT Press, pp 942–948\n",
      "Pasupathy A, Connor CE (1999) Responses to contour features in macaque area V4. Journal of\n",
      "Neurophysiology 82:2490–2502\n",
      "Pasupathy A, Connor CE (2001) Shape representation in area V4: Position-speciﬁc tuning for\n",
      "boundary conformation. Journal of Neurophysiology 86:2505–2519\n",
      "Pearson K (1892) The grammar of science. Scott, London\n",
      "Pedersen KS, Lee AB (2002) Toward a full probability model of edges in natural images. In:\n",
      "Proceedings of 7th European Conference on Computer Vision, pp 328–342\n",
      "Pham DT, Cardoso JF (2001) Blind separation of instantaneous mixtures of non stationary sources.\n",
      "IEEE Trans Signal Processing 49(9):1837–1848\n",
      "Pham DT, Garrat P (1997) Blind separation of mixture of independent sources through a quasi-\n",
      "maximum likelihood approach. IEEE Trans on Signal Processing 45(7):1712–1725\n",
      "Pham DT, Garrat P, Jutten C (1992) Separation of a mixture of independent sources through a\n",
      "maximum likelihood approach. In: Proc. EUSIPCO, pp 771–774\n",
      "\n",
      "464\n",
      "References\n",
      "Poggio GF, Fischer B (1977) Binocular interaction and depth sensitivity in striate and prestriate\n",
      "cortex of behaving rhesus monkey. Journal of Neurophysiology 40:1392–1405\n",
      "Polat U, Sagi D (1993) Lateral interactions between spatial channels: Suppression and facilitation\n",
      "revealed by lateral masking experiments. Vision Research 33:993–999\n",
      "Polat U, Tyler CW (1999) What pattern the eye sees best. Vision Research 39(5):887–895\n",
      "Polat U, Mizobe K, Pettet MW, Kasamatsu T, Norcia AM (1998) Collinear stimuli regulate visual\n",
      "responses depending on cell’s contrast threshold. Nature 391:580–584\n",
      "Pollen D, Ronner S (1983) Visual cortical neurons as localized spatial frequency ﬁlters. IEEE Trans\n",
      "on Systems, Man, and Cybernetics 13:907–916\n",
      "Pollen D, Przybyszewski A, Rubin M, Foote W (2002) Spatial receptive ﬁeld organization of\n",
      "macaque V4 neurons. Cerebral Cortex 12:601–616\n",
      "Rao RPN, Ballard DH (1999) Predictive coding in the visual cortex: a functional interpretation of\n",
      "some extra-classical receptive ﬁeld effects. Nature Neuroscience 2(1):79–87\n",
      "Reinagel P (2001) How do visual neurons respond in the real world? Current Opinion in Neurobi-\n",
      "ology 11(4):437–442\n",
      "Reinagel P, Zador A (1999) Natural scenes at the center of gaze. Network: Computation in Neural\n",
      "Systems 10:341–350\n",
      "Rieke F, Warland D, de R van Steveninck R, Bialek W (1997) Spikes: Exploring the neural code.\n",
      "MIT Press\n",
      "Riesenhuber M, Poggio T (1999) Hierarchical models of object recognition in cortex. Nature Neu-\n",
      "roscience 2:1019–1025\n",
      "Ringach DL, Malone BJ (2007) The operating point of the cortex: Neurons as large deviation\n",
      "detectors. J of Neuroscience 27:7673–7683\n",
      "Ringach DL, Shapley R (2004) Reverse correlation in neurophysiology. Cognitive Science 28:147–\n",
      "166\n",
      "Roelfsema PR, Lamme VAF, Spekreijse H, Bosch H (2002) Figure-ground segregation in a recur-\n",
      "rent network architecture˙J of Cognitive Neuroscience 14:525–537\n",
      "Romberg J, Choi H, Baraniuk R, Kingsbury N (2000) Hidden markov tree modelling of complex\n",
      "wavelet transforms. In: Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing\n",
      "(ICASSP2000), Istanbul, Turkey, pp 133–136\n",
      "Romberg J, Choi H, Baraniuk R (2001) Bayesian tree-structured image modeling using wavelet-\n",
      "domain hidden markov models. IEEE Trans on Image Processing 10:1056–1068\n",
      "Roth S, Black MJ (2005) Fields of experts: a framework for learning image priors. In: Proc. IEEE\n",
      "Int. Conf. on Computer Vision and Pattern Recognition (CVRP2005), pp 860–867\n",
      "Ruderman DL (1997) Origins of scaling in natural images. Vision Research 37:3358–3398\n",
      "Ruderman DL, Bialek W (1994a) Statistics of natural images: scaling in the woods. Phys Rev Lett\n",
      "73:814–817\n",
      "Ruderman DL, Bialek W (1994b) Statistics of natural images: Scaling in the woods. Physics Re-\n",
      "view Letters 73(6):814–817\n",
      "Ruderman DL, Cronin TW, Chiao C (1998) Statistics of cone responses to natural images: Impli-\n",
      "cations for visual coding. Journal of the Optical Society of America A 15(8):2036–2045\n",
      "Rust NC, Schwartz O, Movshon JA, Simoncelli EP (2005) Spatiotemporal elements of macaque\n",
      "V1 receptive ﬁelds. Neuron 46:945–956\n",
      "Sallee P, Olshausen BA (2003) Learning sparse multiscale image representations. In: Advances in\n",
      "Neural Information Processing Systems, vol 15, MIT Press, pp 1327–1334\n",
      "Sanger T (1989) Optimal unsupervised learning in a single-layered linear feedforward network.\n",
      "Neural Networks 2:459–473\n",
      "Saul AB, Humphrey AL (1990) Spatial and temporal response properties of lagged and nonlagged\n",
      "cells in cat lateral geniculate nucleus. Journal of Neurophysiology 64(1):206–224\n",
      "van der Schaaf A, van Hateren JH (1996) Modelling the power spectra of natural images: statistics\n",
      "and information. Vision Research 36:2759–2770\n",
      "Schervish M (1995) Theory of Statistics. Springer\n",
      "Schwartz DA, Howe CQ, Purves D (2003) The statistical structure of human speech sounds pre-\n",
      "dicts musical universals. J of Neuroscience 23:7160–7168\n",
      "\n",
      "References\n",
      "465\n",
      "Schwartz O, Simoncelli EP (2001a) Natural signal statistics and sensory gain control. Nature Neu-\n",
      "roscience 4(8):819–825\n",
      "Schwartz O, Simoncelli EP (2001b) Natural sound statistics and divisive normalization in the au-\n",
      "ditory system. In: Advances in Neural Information Processing Systems, vol 13, MIT Press\n",
      "Schwartz O, Sejnowski TJ, Dayan P (2005) Assignment of multiplicative mixtures in natural im-\n",
      "ages. In: Advances in Neural Information Processing Systems, vol 17, MIT Press, Cambridge,\n",
      "MA\n",
      "Shannon CE (1948) A mathematical theory of communication. Bell System Technical Journal\n",
      "27:379–423\n",
      "Shoham D, H¨ubener M, Schulze S, Grinvald A, Bonhoeffer T (1997) Spatio-temporal frequency\n",
      "domains and their relation to cytochrome oxidase staining in cat visual cortex. Nature 385:529–\n",
      "533\n",
      "Shouval H, Intrator N, Law CC, Cooper LN (1996) Effect of binocular cortical misalignment on\n",
      "ocular dominance and orientation selectivity. Neural Computation 8:1021–1040\n",
      "Sigman M, Gilbert CD (2000) Learning to ﬁnd a shape. Nature Neuroscience 3(3):264–269\n",
      "Sigman M, Cecchi GA, Gilbert CD, Magnasco MO (2001) On a common circle: Natural scenes\n",
      "and gestalt rules. Proceedings of the National Academy of Science, USA 98:1935–1940\n",
      "Silverman MS, Grosof DH, DeValois RL, Elfar SD (1989) Spatial-frequency organization in pri-\n",
      "mate striate cortex. Proc National Academy of Sciences (USA) 86(2):711–715\n",
      "Simoncelli EP (2005) Statistical modeling of photographic images. In: Bovik A (ed) Handbook of\n",
      "Image and Video Processing (2nd Edition), Academic Press\n",
      "Simoncelli EP, Adelson EH (1996) Noise removal via Bayesian wavelet coring. In: Proc. Third\n",
      "IEEE Int. Conf. on Image Processing, Lausanne, Switzerland, pp 379–382\n",
      "Simoncelli EP, Heeger D (1998) A model of neuronal responses in visual area MT. Vision Research\n",
      "38(5):743–761\n",
      "Simoncelli EP, Olshausen BA (2001) Natural image statistics and neural representation. Annual\n",
      "Review of Neuroscience 24:1193–216\n",
      "Simoncelli EP, Freeman WT, Adelson EH, Heeger DJ (1992) Shiftable multiscale transforms. IEEE\n",
      "Transactions on Information Theory 38:587–607\n",
      "Simoncelli EP, Paninski L, Pillow J, Schwartz O (2004) Characterization of neural responses with\n",
      "stochastic stimuli. In: Gazzaniga M (ed) The New Cognitive Neurosciences, 3rd edn, MIT Press\n",
      "Skottun BC, Freeman RD (1984) Stimulus speciﬁcity of binocular cells in the cat’s visual cor-\n",
      "tex: Ocular dominance and the matching of left and right eyes. Experimental Brain Research\n",
      "56(2):206–216\n",
      "Smith E, Lewicki MS (2005) Efﬁcient coding of time-relative structure using spikes. Neural Com-\n",
      "putation 17:19–45\n",
      "Smith E, Lewicki MS (2006) Efﬁcient auditory coding. Nature 439:978–982\n",
      "Sonka M, Hlavac V, Boyle R (1998) Image processing, analysis, and machine vision. Brooks/Cole\n",
      "publishing company, Paciﬁc Grove, CA\n",
      "Spanias AS (1994) Speech coding: a tutorial review. Proceedings of the IEEE 82(10):1541–1582\n",
      "Sperling G (1989) Three stages and two systems of visual processing. Spatial Vision 4(2/3):183–\n",
      "207\n",
      "Srivanivasan MV, Laughlin SB, Dubs A (1982) Predictive coding: a fresh view of inhibition in the\n",
      "retina. Proceedings of the Royal Society of London B 216:427–459\n",
      "Srivastava A, Lee AB, Simoncelli EP, Chu SC (2003) On advances in statistical modelling of\n",
      "natural images. Journal of Mathematical Imaging and Vision 18:17–33\n",
      "Stone J (1996) Learning visual parameters using spatiotemporal smoothness constraints. Neural\n",
      "Computation 8(7):1463–1492\n",
      "Strang G, Nguyen T (1996) Wavelets and Filter Banks. Wellesley-Cambridge Press, Wellesley,\n",
      "MA\n",
      "Thomson MGA (1999) Higher-order structure in natural scenes. J of the Optical Society of Amer-\n",
      "ica A 16:1549–1553\n",
      "Thomson MGA (2001) Beats, kurtosis and visual coding. Network: Computation in Neural Sys-\n",
      "tems 12:271–87\n",
      "\n",
      "466\n",
      "References\n",
      "Tolhurst DJ, Tadmor Y, Chao T (1992) Amplitude spectra of natural images. Ophthalmic & Phys-\n",
      "iological Optics 12(2):229–232\n",
      "Tong L, Liu RW, Soon VC, Huang YF (1991) Indeterminacy and identiﬁability of blind identiﬁca-\n",
      "tion. IEEE Trans on Circuits and Systems 38:499–509\n",
      "Tootell RBH, Silverman MS, Hamilton SL, Switkes E, Valois RLD (1988) Functional anatomy of\n",
      "macaque striate cortex. V. Spatial frequency. Journal of Neuroscience 8:1610–1624\n",
      "Torralba A, Oliva A (2003) Statistics of natural image categories. Network: Computation in Neural\n",
      "Systems 14:391–412\n",
      "Touryan J, Lau B, Dan Y (2002) Isolation of relevant visual features from random stimuli for\n",
      "cortical complex cells. Journal of Neuroscience 22:10,811–10,818\n",
      "Touryan J, Felsen G, Han F, Dan Y (2005) Spatial structure of complex cell receptive ﬁelds mea-\n",
      "sured with natural images. Neuron 45(5):781–791\n",
      "Ts’o DY, Gilbert CD (1988) The organization of chromatic and spatial interactions in the primate\n",
      "striate cortex. Journal of Neuroscience 8(5):1712–1727\n",
      "Ts’o DY, Roe AW (1995) Functional compartments in visual cortex: Segregation and interaction.\n",
      "In: Gazzaniga MS (ed) The Cognitive Neurosciences, MIT Press, pp 325–337\n",
      "Turiel A, Parga N (2000) The multi-fractal structure of contrast changes in natural images: From\n",
      "sharp edges to textures. Neural Computation 12:763–793\n",
      "Utsugi A (2001) Ensemble of independent factor analyzers with application to natural image anal-\n",
      "ysis. Neural Processing Letters 14:49–60\n",
      "Valpola H, Harva M, Karhunen J (2003) Hierarchical models of variance sources. In: Proceedings\n",
      "of the Fourth International Symposium on Independent Component Analysis and Blind Signal\n",
      "Separation, pp 83–88\n",
      "Vetterli M, Kovaˇcevi´c J (1995) Wavelets and Subband Coding. Prentice Hall Signal Processing\n",
      "Series, Prentice Hall, Englewood Cliffs, New Jersey\n",
      "Vincent B, Baddeley RJ (2003) Synaptic energy efﬁciency in retinal processing. Network: Com-\n",
      "putation in Neural Systems 43:1283–1290\n",
      "Vincent B, Baddeley RJ, Troscianko T, Gilchrist ID (2005) Is the early visual system optimised to\n",
      "be energy efﬁcient? Network: Computation in Neural Systems 16:175–190\n",
      "Vinje WE, Gallant JL (2000) Sparse coding and decorrelation in primary visual cortex during\n",
      "natural vision. Science 287(5456):1273–1276\n",
      "Vinje WE, Gallant JL (2002) Natural stimulation of the nonclassical receptive ﬁeld increases in-\n",
      "formation transmission efﬁciency in v1. J of Neuroscience 22:2904–2915\n",
      "van Vreeswijk C (2001) Whence sparseness? In: Advances in Neural Information Processing Sys-\n",
      "tems, vol 13, MIT Press, pp 180–186\n",
      "Wachtler T, Lee TW, Sejnowski TJ (2001) Chromatic structure of natural scenes. Journal of the\n",
      "Optical Society of America A 18(1):65–77\n",
      "Wachtler T, Doi E, Lee TW, Sejnowski TJ (2007) Cone selectivity derived from the responses of\n",
      "the retinal cone mosaic to natural scenes. Journal of Vision 7(8):1–14\n",
      "Wainwright MJ, Simoncelli EP, Willsky AS (2001) Random cascades on wavelet trees and their\n",
      "use in analyzing and modeling natural images. Applied Computational and Harmonic Analysis\n",
      "11:89–123\n",
      "Wandell BA (1995) Foundations of Vision. Sinauer Associates, Inc., Sunderland (Mass.)\n",
      "Weliky M, Fiser J, Hunt RH, Wagner DN (2003) Coding of natural scenes in primary visual cortex.\n",
      "Neuron 37(4):703–718\n",
      "Wichmann FA, Braun DI, Gegenfurtner KR (2006) Phase noise and the classiﬁcation of natural\n",
      "images. Vision Research 46:1520–1529\n",
      "Wilkinson F, James T, Wilson H, Gati J, Menon R, Goodale M (2000) An fMRI study of the\n",
      "selective activation of human extrastriate form vision areas by radial and concentric gratings.\n",
      "Current Biology 10:1455–8\n",
      "Williams CB, Hess RF (1998) Relationship between facilitation at threshold and suprathreshold\n",
      "contour integration. J Opt Soc Am A 15(8):2046–2051\n",
      "Willmore B, Tolhurst DJ (2001) Characterizing the sparseness of neural codes. Network: Compu-\n",
      "tation in Neural Systems 12:255–270\n",
      "\n",
      "References\n",
      "467\n",
      "Winkler G (2003) Image Analysis, Random Field and Markov Chain Monte Carlo Methods, 2nd\n",
      "edn. Springer\n",
      "Wiskott L, Sejnowski TJ (2002) Slow feature analysis: Unsupervised learning of invariances. Neu-\n",
      "ral Computation 14(4):715–770\n",
      "Wong ROL (1999) Retinal waves and visual system development. Annual Review of Neuroscience\n",
      "22:29–47\n",
      "Wyss R, K¨onig P, Verschure PFMJ (2006) A model of the ventral visual system based on temporal\n",
      "stability and local memory. PLoS Biology 4(5):0836–0843\n",
      "Yang Z, Purves D (2003) A statistical explanation of visual space. Nature Neuroscience 6:632 –\n",
      "640\n",
      "Yen SC, Baker J, Gray CM (2007) Heterogeneity in the responses of adjacent neurons to natural\n",
      "stimuli in cat striate cortex. J of Neurophysiology 97:1326–1341\n",
      "Yuille A, Kersten D (2006) Vision as Bayesian inference: analysis by synthesis? Trends in Cogni-\n",
      "tive Science 10(7):301–308\n",
      "Zetzsche C, Krieger G (1999) Nonlinear neurons and high-order statistics: New approaches to\n",
      "human vision and electronic image processing. In: Rogowitz B, Pappas T (eds) Human Vision\n",
      "and Electronic Imaging IV (Proc. SPIE vol. 3644), SPIE, pp 2–33\n",
      "Zetzsche C, R¨ohrbein F (2001) Nonlinear and extra-classical receptive ﬁeld properties and the\n",
      "statistics of natural scenes. Network: Computation in Neural Systems 12:331–350\n",
      "Zetzsche C, Krieger G, Wegmann B (1999) The atoms of vision: Cartesian or polar? J of the Optical\n",
      "Society of America A 16:1554–1565\n",
      "Zhang K, Sejnowski TJ (2000) A universal scaling law between gray matter and white matter of\n",
      "cerebral cortex. Proc National Academy of Sciences (USA) 97:5621–5626\n",
      "Zhu SC, Wu ZN, Mumford D (1997) Minimax entropy principle and its application to texture\n",
      "modeling. Neural Computation 9:1627–1660\n",
      "Zhu Y, Qian N (1996) Binocular receptive ﬁeld models, disparity tuning, and characteristic dispar-\n",
      "ity. Neural Computation 8:1611–1641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path1 = \"../../data/mcelreath_2020_statistical-rethinking.pdf\"\n",
    "pdf_path2 = \"../../data/Theory of Statistic.pdf\"\n",
    "pdf_path3 = \"../../data/Deep Learning with Python.pdf\"\n",
    "pdf_path4 = \"../../data/Natural_Image_Statistics.pdf\"\n",
    "pdf_path5 = \"../../data/mml-book.pdf\"\n",
    "\n",
    "pdf_path = pdf_path4\n",
    "\n",
    "pages_data = extract_page_data_fitz(pdf_path)\n",
    "start_chapter = correct_page_numbers(pages_data, sequence_length=10)\n",
    "text = extract_text(pdf_path, start_chapter)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1e12a",
   "metadata": {},
   "source": [
    "### Set-up ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"punkt\")  \n",
    "\n",
    "\n",
    "def paragraphs_chunking(text, max_words=750):\n",
    "    \"\"\"\n",
    "    Splits text into structured chunks, preserving paragraph integrity and avoiding unnatural breaks.\n",
    "    - Uses paragraph-based splitting first.\n",
    "    - Splits long paragraphs into smaller chunks based on sentence boundaries.\n",
    "    \"\"\"\n",
    "    # Split text into paragraphs first\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        words = para.split()\n",
    "        \n",
    "        # If paragraph is within limit, keep as a single chunk\n",
    "        if len(words) <= max_words:\n",
    "            chunks.append(para)\n",
    "            continue\n",
    "        \n",
    "        # Sentence-based chunking for large paragraphs\n",
    "        sentences = sent_tokenize(para)\n",
    "        chunk, chunk_word_count = [], 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_word_count = len(sentence.split())\n",
    "            \n",
    "            # If adding this sentence keeps chunk within word limit, add it\n",
    "            if chunk_word_count + sentence_word_count <= max_words:\n",
    "                chunk.append(sentence)\n",
    "                chunk_word_count += sentence_word_count\n",
    "            else:\n",
    "                # Finalize current chunk and start a new one\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = [sentence]\n",
    "                chunk_word_count = sentence_word_count\n",
    "\n",
    "        # Append any remaining chunk\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def lines_chunking(text, max_words=200):\n",
    "    \"\"\"\n",
    "    Splits a text into semantically meaningful chunks without breaking sentences or paragraphs abruptly.\n",
    "    - Preserves paragraph boundaries by detecting empty lines as paragraph breaks.\n",
    "    - Further splits long paragraphs into sentence-based chunks, ensuring each chunk stays within a maximum word limit.\n",
    "    \"\"\"\n",
    "    # Split text into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Group lines into paragraphs\n",
    "    paragraphs = []\n",
    "    current_paragraph = []\n",
    "    for line in lines:\n",
    "        if line.strip():  \n",
    "            current_paragraph.append(line.strip())\n",
    "        else:  # Empty line indicates end of paragraph\n",
    "            if current_paragraph:\n",
    "                paragraphs.append(\" \".join(current_paragraph))\n",
    "                current_paragraph = []\n",
    "    if current_paragraph: \n",
    "        paragraphs.append(\" \".join(current_paragraph))\n",
    "\n",
    "    # Process paragraphs\n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        words = para.split()\n",
    "        if len(words) <= max_words:\n",
    "            chunks.append(para)\n",
    "        else:\n",
    "            sentences = sent_tokenize(para)\n",
    "            chunk, chunk_word_count = [], 0\n",
    "            for sentence in sentences:\n",
    "                sentence_word_count = len(sentence.split())\n",
    "                if chunk_word_count + sentence_word_count <= max_words:\n",
    "                    chunk.append(sentence)\n",
    "                    chunk_word_count += sentence_word_count\n",
    "                else:\n",
    "                    chunks.append(\" \".join(chunk))\n",
    "                    chunk = [sentence]\n",
    "                    chunk_word_count = sentence_word_count\n",
    "            if chunk:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fecc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "def get_database_directory():\n",
    "    \"\"\"\n",
    "    Get the directory for storing the database.\n",
    "    \"\"\"\n",
    "    # Use an absolute path for better reliability\n",
    "    parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "    persist_dir = os.path.join(parent_dir, \"database\")\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "    \n",
    "    return persist_dir\n",
    "\n",
    "\n",
    "def get_chroma_client():\n",
    "    \"\"\"\n",
    "    Get a ChromaDB client.\n",
    "    \"\"\"\n",
    "    persist_dir = get_database_directory()\n",
    "    return chromadb.Client()\n",
    "\n",
    "\n",
    "def initialize_chromadb(EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Initialize ChromaDB client and embedding function.\n",
    "    \"\"\"\n",
    "    # Create a persistent directory for storing the database\n",
    "    client = get_chroma_client()\n",
    "\n",
    "    # Initialize an embedding function (using a Sentence Transformer model)\n",
    "    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=EMBEDDING_MODEL\n",
    "    )\n",
    "\n",
    "    return client, embedding_func\n",
    "\n",
    "\n",
    "def initialize_collection(client, embedding_func, collection_name):\n",
    "    \"\"\"\n",
    "    Initialize a collection in ChromaDB.\n",
    "    \"\"\"\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_func,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "\n",
    "def update_collection(collection, text, max_words=750):\n",
    "    \"\"\"\n",
    "    Update the ChromaDB collection with new files.\n",
    "    \"\"\"\n",
    "    # Tokenize text into chunks\n",
    "    max_words = 200\n",
    "    chunks = lines_chunking(text, max_words=max_words)\n",
    "\n",
    "    # Store chunks in the collection\n",
    "    filename = \"uploaded_book\"\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        ids=[f\"id{filename[:-4]}.{j}\" for j in range(len(chunks))],\n",
    "        metadatas=[{\"source\": filename, \"part\": n} for n in range(len(chunks))],\n",
    "    )\n",
    "    \n",
    "    return collection\n",
    "\n",
    "def update_collection(collection, text, book_title=\"textbook\", max_words=750):\n",
    "    chunks = lines_chunking(text, max_words=max_words)\n",
    "    \n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        ids=[f\"{book_title}_chunk_{j:04d}\" for j in range(len(chunks))],  # Zero-padded\n",
    "        metadatas=[{\n",
    "            \"source\": book_title,\n",
    "            \"chunk_index\": j,\n",
    "            \"page_estimate\": j * max_words // 250,  # Rough page estimate\n",
    "        } for j in range(len(chunks))],\n",
    "    )\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebad8519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davide/miniconda3/envs/llm_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  \n",
    "client, embedding_func = initialize_chromadb(EMBEDDING_MODEL)\n",
    "\n",
    "# Create two collections with different purposes\n",
    "whole_text_collection = initialize_collection(\n",
    "    client, embedding_func, \"whole_text_chunks\"\n",
    ")\n",
    "\n",
    "chapter_collection = initialize_collection(\n",
    "    client, embedding_func, \"chapter_chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_text(collection, query='', nresults=2, sim_th=None):\n",
    "    \"\"\"Get relevant text from a collection for a given query\"\"\"\n",
    "\n",
    "    query_result = collection.query(query_texts=query, n_results=nresults)\n",
    "    docs = query_result.get('documents')[0]\n",
    "\n",
    "    if sim_th is not None:\n",
    "        similarities = [1 - d for d in query_result.get(\"distances\")[0]]\n",
    "        relevant_docs = [d for d, s in zip(docs, similarities) if s >= sim_th]\n",
    "        return ''.join(relevant_docs)\n",
    "    return ''.join([doc for doc in docs if doc is not None])\n",
    "\n",
    "\n",
    "def generate_answer(base_url, model, prompt, context=[], top_k=5, top_p=0.9, temp=0.5):\n",
    "    url = base_url + \"/generate\"\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"stream\": False,\n",
    "        \"context\": context,\n",
    "        \"options\": {\"temperature\": temp, \"top_p\": top_p, \"top_k\": top_k},\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, json=data)\n",
    "        response.raise_for_status()\n",
    "        response_dict = response.json()\n",
    "        return response_dict.get('response', ''), response_dict.get('context', [])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "        return \"\", []\n",
    "\n",
    "\n",
    "def get_contextual_prompt(question, context):\n",
    "    contextual_prompt = (\n",
    "        \"You are a helpful assistant. Use the information provided in the context below to answer the question. \"\n",
    "        \"Ensure your answer is accurate, concise, and directly addresses the question. \"\n",
    "        \"If the context does not provide enough information to answer the question, state that explicitly.\\n\\n\"\n",
    "        \"### Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"### Question:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Answer:\"\n",
    "    )\n",
    "    return contextual_prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
