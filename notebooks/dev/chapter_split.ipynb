{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4192c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "pdf_path = \"../../data/mcelreath_2020_statistical-rethinking.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "content_page_range = range(5,7)  # Adjust the range as needed\n",
    "\n",
    "# Get content \n",
    "chapters_content = []\n",
    "for page_num in content_page_range:\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text(\"text\")\n",
    "    chapters_content.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570777f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contents\\nPreface to the Second Edition\\nix\\nPreface\\nxi\\nAudience\\nxi\\nTeaching strategy\\nxii\\nHow to use this book\\nxii\\nInstalling the rethinking R package\\nxvi\\nAcknowledgments\\nxvi\\nChapter 1.\\nThe Golem of Prague\\n1\\n1.1.\\nStatistical golems\\n1\\n1.2.\\nStatistical rethinking\\n4\\n1.3.\\nTools for golem engineering\\n10\\n1.4.\\nSummary\\n17\\nChapter 2.\\nSmall Worlds and Large Worlds\\n19\\n2.1.\\nThe garden of forking data\\n20\\n2.2.\\nBuilding a model\\n28\\n2.3.\\nComponents of the model\\n32\\n2.4.\\nMaking the model go\\n36\\n2.5.\\nSummary\\n46\\n2.6.\\nPractice\\n46\\nChapter 3.\\nSampling the Imaginary\\n49\\n3.1.\\nSampling from a grid-approximate posterior\\n52\\n3.2.\\nSampling to summarize\\n53\\n3.3.\\nSampling to simulate prediction\\n61\\n3.4.\\nSummary\\n68\\n3.5.\\nPractice\\n68\\nChapter 4.\\nGeocentric Models\\n71\\n4.1.\\nWhy normal distributions are normal\\n72\\n4.2.\\nA language for describing models\\n77\\n4.3.\\nGaussian model of height\\n78\\n4.4.\\nLinear prediction\\n91\\n4.5.\\nCurves from lines\\n110\\n4.6.\\nSummary\\n120\\n4.7.\\nPractice\\n120\\nChapter 5.\\nThe Many Variables & The Spurious Waffles\\n123\\n5.1.\\nSpurious association\\n125\\n5.2.\\nMasked relationship\\n144\\nv\\n',\n",
       " 'vi\\nCONTENTS\\n5.3.\\nCategorical variables\\n153\\n5.4.\\nSummary\\n158\\n5.5.\\nPractice\\n159\\nChapter 6.\\nThe Haunted DAG & The Causal Terror\\n161\\n6.1.\\nMulticollinearity\\n163\\n6.2.\\nPost-treatment bias\\n170\\n6.3.\\nCollider bias\\n176\\n6.4.\\nConfronting confounding\\n183\\n6.5.\\nSummary\\n189\\n6.6.\\nPractice\\n189\\nChapter 7.\\nUlysses’ Compass\\n191\\n7.1.\\nThe problem with parameters\\n193\\n7.2.\\nEntropy and accuracy\\n202\\n7.3.\\nGolem taming: regularization\\n214\\n7.4.\\nPredicting predictive accuracy\\n217\\n7.5.\\nModel comparison\\n225\\n7.6.\\nSummary\\n235\\n7.7.\\nPractice\\n235\\nChapter 8.\\nConditional Manatees\\n237\\n8.1.\\nBuilding an interaction\\n239\\n8.2.\\nSymmetry of interactions\\n250\\n8.3.\\nContinuous interactions\\n252\\n8.4.\\nSummary\\n260\\n8.5.\\nPractice\\n260\\nChapter 9.\\nMarkov Chain Monte Carlo\\n263\\n9.1.\\nGood King Markov and his island kingdom\\n264\\n9.2.\\nMetropolis algorithms\\n267\\n9.3.\\nHamiltonian Monte Carlo\\n270\\n9.4.\\nEasy HMC: ulam\\n279\\n9.5.\\nCare and feeding of your Markov chain\\n287\\n9.6.\\nSummary\\n296\\n9.7.\\nPractice\\n296\\nChapter 10.\\nBig Entropy and the Generalized Linear Model\\n299\\n10.1.\\nMaximum entropy\\n300\\n10.2.\\nGeneralized linear models\\n312\\n10.3.\\nMaximum entropy priors\\n321\\n10.4.\\nSummary\\n321\\nChapter 11.\\nGod Spiked the Integers\\n323\\n11.1.\\nBinomial regression\\n324\\n11.2.\\nPoisson regression\\n345\\n11.3.\\nMultinomial and categorical models\\n359\\n11.4.\\nSummary\\n365\\n11.5.\\nPractice\\n366\\nChapter 12.\\nMonsters and Mixtures\\n369\\n12.1.\\nOver-dispersed counts\\n369\\n12.2.\\nZero-inflated outcomes\\n376\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapters_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436a441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "page_number_map = {}\n",
    "\n",
    "for i, page in enumerate(doc):\n",
    "    text = page.get_text(\"dict\")\n",
    "    blocks = text[\"blocks\"]\n",
    "\n",
    "    for block in blocks:\n",
    "        if \"lines\" not in block:\n",
    "            continue\n",
    "        for line in block[\"lines\"]:\n",
    "            for span in line[\"spans\"]:\n",
    "                # Heuristic: page numbers are small and centered at the bottom\n",
    "                y0 = span[\"bbox\"][1]\n",
    "                if y0 > page.rect.height * 0.85:  # Bottom 15% of page\n",
    "                    clean_text = span[\"text\"].strip()\n",
    "                    if clean_text.isdigit():\n",
    "                        page_number_map[i] = int(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_page_data_fitz(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_data = []\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        height = page.rect.height\n",
    "        width = page.rect.width\n",
    "\n",
    "        top_rect = fitz.Rect(0, 0, width, height * 0.15)\n",
    "        bottom_rect = fitz.Rect(0, height * 0.85, width, height)\n",
    "\n",
    "        top_text = page.get_text(\"text\", clip=top_rect).split()\n",
    "        bottom_text = page.get_text(\"text\", clip=bottom_rect).split()\n",
    "\n",
    "        found_number = None\n",
    "        for text in top_text + bottom_text:\n",
    "            if text.isdigit():\n",
    "                found_number = int(text)\n",
    "                break\n",
    "\n",
    "        full_text = page.get_text(\"text\")\n",
    "\n",
    "        pages_data.append({\n",
    "            \"index\": i,\n",
    "            \"number\": found_number,\n",
    "            \"content\": full_text\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "    return pages_data\n",
    "\n",
    "\n",
    "def correct_page_numbers(pages_data, sequence_length=10):\n",
    "    # Find first sequence of 'sequence_length' consecutive page numbers\n",
    "    seen = [(i, d[\"number\"]) for i, d in enumerate(pages_data) if isinstance(d[\"number\"], int)]\n",
    "\n",
    "    for start in range(len(seen) - sequence_length + 1):\n",
    "        valid = True\n",
    "        for j in range(sequence_length):\n",
    "            if seen[start + j][1] != seen[start][1] + j:\n",
    "                valid = False\n",
    "                break\n",
    "        if valid:\n",
    "            base_index, base_number = seen[start]\n",
    "            break\n",
    "    else:\n",
    "        # No sequence found, return original data\n",
    "        raise ValueError(\"No valid sequence of page numbers found.\")\n",
    "\n",
    "    # Forward fill from base_index\n",
    "    for offset, page in enumerate(pages_data[base_index:], start=0):\n",
    "        page[\"number\"] = base_number + offset\n",
    "\n",
    "    # Backward fill before base_index\n",
    "    for offset in range(1, base_index + 1):\n",
    "        page = pages_data[base_index - offset]\n",
    "        page[\"number\"] = base_number - offset\n",
    "    \n",
    "    # Set pages < 1 == None\n",
    "    for page in pages_data:\n",
    "        if page[\"number\"] < 1:\n",
    "            page[\"number\"] = None\n",
    "\n",
    "    return pages_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path1 = \"../../data/mcelreath_2020_statistical-rethinking.pdf\"\n",
    "pdf_path2 = \"../../data/Theory of Statistic.pdf\"\n",
    "pdf_path3 = \"../../data/Deep Learning with Python.pdf\"\n",
    "pdf_path4 = \"../../data/mcelreath_2020_statistical-rethinking.pdf\"\n",
    "pdf_path5 = \"../../data/mml-book.pdf\"\n",
    "\n",
    "\n",
    "\n",
    "pages_data = extract_page_data_fitz(pdf_path1)\n",
    "\n",
    "try:\n",
    "    corrected_pages = correct_page_numbers(pages_data, sequence_length=10)\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}. Using original page data without correction.\")\n",
    "    corrected_pages = pages_data  # fallback to original\n",
    "\n",
    "for p in corrected_pages:\n",
    "    print(f\"Page index: {p['index']}, Page number: {p['number']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa20a279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 0, 'number': None, 'content': ''},\n",
       " {'index': 1, 'number': None, 'content': 'Statistical Rethinking\\n'},\n",
       " {'index': 2,\n",
       "  'number': None,\n",
       "  'content': 'CHAPMAN & HALL/CRC\\nTexts in Statistical Science Series\\nJoseph K. Blitzstein, Harvard University, USA  \\nJulian J. Faraway, University of Bath, UK  \\nMartin Tanner, Northwestern University, USA \\nJim Zidek, University of British Columbia, Canada\\nRecently Published Titles\\nTheory of Spatial Statistics \\nA Concise Introduction \\nM.N.M van Lieshout\\nBayesian Statistical Methods \\nBrian J. Reich and Sujit K. Ghosh\\nSampling \\nDesign and Analysis, Second Edition \\nSharon L. Lohr\\nThe Analysis of Time Series \\nAn Introduction with R, Seventh Edition \\nChris Chatfield and Haipeng Xing\\nTime Series \\nA Data Analysis Approach Using R \\nRobert H. Shumway and David S. Stoffer\\nPractical Multivariate Analysis, Sixth Edition \\nAbdelmonem Afifi, Susanne May, Robin A. Donatello, and Virginia A. Clark\\nTime Series: A First Course with Bootstrap Starter \\nTucker S. McElroy and Dimitris N. Politis\\nProbability and Bayesian Modeling \\nJim Albert and Jingchen Hu\\nSurrogates \\nGaussian Process Modeling, Design, and Optimization for the Applied Sciences  \\nRobert B. Gramacy\\nStatistical Analysis of Financial Data \\nWith Examples in R \\nJames Gentle\\nStatistical Rethinking \\nA Bayesian Course with Examples in R and Stan, Second Edition \\nRichard McElreath\\nFor more information about this series, please visit: https://www.crcpress.com/Chapman– \\nHallCRC-Texts-in-Statistical-Science/book-series/CHTEXSTASCI\\n'},\n",
       " {'index': 3,\n",
       "  'number': None,\n",
       "  'content': 'Statistical Rethinking\\nA Bayesian Course with Examples  \\nin R and Stan\\nSecond Edition\\nRichard McElreath\\n'},\n",
       " {'index': 4,\n",
       "  'number': None,\n",
       "  'content': 'Second edition published 2020\\nby CRC Press\\n6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742\\nand by CRC Press\\n2 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN\\n© 2020 Taylor & Francis Group, LLC \\nFirst edition published by CRC Press 2015\\nCRC Press is an imprint of Taylor & Francis Group, LLC\\nReasonable efforts have been made to publish reliable data and information, but the author and publisher cannot \\nassume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have \\nattempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders \\nif permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please \\nwrite and let us know so we may rectify in any future reprint.\\nExcept as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized \\nin any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, \\nmicrofilming, and recording, or in any information storage or retrieval system, without written permission from the \\npublishers.\\nFor permission to photocopy or use material electronically from this work, access www.copyright.com or contact the \\nCopyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. For works that are \\nnot available on CCC please contact mpkbookspermissions@tandf.co.uk\\nTrademark notice: Product or corporate names may be trademarks or registered trademarks, and are used only for \\nidentification and explanation without intent to infringe.\\nLibrary of Congress Cataloging‑in‑Publication Data\\nLibrary of Congress Control Number:2019957006\\nISBN: 978-0-367-13991-9 (hbk)\\nISBN: 978-0-429-02960-8 (ebk)\\n'},\n",
       " {'index': 5,\n",
       "  'number': None,\n",
       "  'content': 'Contents\\nPreface to the Second Edition\\nix\\nPreface\\nxi\\nAudience\\nxi\\nTeaching strategy\\nxii\\nHow to use this book\\nxii\\nInstalling the rethinking R package\\nxvi\\nAcknowledgments\\nxvi\\nChapter 1.\\nThe Golem of Prague\\n1\\n1.1.\\nStatistical golems\\n1\\n1.2.\\nStatistical rethinking\\n4\\n1.3.\\nTools for golem engineering\\n10\\n1.4.\\nSummary\\n17\\nChapter 2.\\nSmall Worlds and Large Worlds\\n19\\n2.1.\\nThe garden of forking data\\n20\\n2.2.\\nBuilding a model\\n28\\n2.3.\\nComponents of the model\\n32\\n2.4.\\nMaking the model go\\n36\\n2.5.\\nSummary\\n46\\n2.6.\\nPractice\\n46\\nChapter 3.\\nSampling the Imaginary\\n49\\n3.1.\\nSampling from a grid-approximate posterior\\n52\\n3.2.\\nSampling to summarize\\n53\\n3.3.\\nSampling to simulate prediction\\n61\\n3.4.\\nSummary\\n68\\n3.5.\\nPractice\\n68\\nChapter 4.\\nGeocentric Models\\n71\\n4.1.\\nWhy normal distributions are normal\\n72\\n4.2.\\nA language for describing models\\n77\\n4.3.\\nGaussian model of height\\n78\\n4.4.\\nLinear prediction\\n91\\n4.5.\\nCurves from lines\\n110\\n4.6.\\nSummary\\n120\\n4.7.\\nPractice\\n120\\nChapter 5.\\nThe Many Variables & The Spurious Waffles\\n123\\n5.1.\\nSpurious association\\n125\\n5.2.\\nMasked relationship\\n144\\nv\\n'},\n",
       " {'index': 6,\n",
       "  'number': None,\n",
       "  'content': 'vi\\nCONTENTS\\n5.3.\\nCategorical variables\\n153\\n5.4.\\nSummary\\n158\\n5.5.\\nPractice\\n159\\nChapter 6.\\nThe Haunted DAG & The Causal Terror\\n161\\n6.1.\\nMulticollinearity\\n163\\n6.2.\\nPost-treatment bias\\n170\\n6.3.\\nCollider bias\\n176\\n6.4.\\nConfronting confounding\\n183\\n6.5.\\nSummary\\n189\\n6.6.\\nPractice\\n189\\nChapter 7.\\nUlysses’ Compass\\n191\\n7.1.\\nThe problem with parameters\\n193\\n7.2.\\nEntropy and accuracy\\n202\\n7.3.\\nGolem taming: regularization\\n214\\n7.4.\\nPredicting predictive accuracy\\n217\\n7.5.\\nModel comparison\\n225\\n7.6.\\nSummary\\n235\\n7.7.\\nPractice\\n235\\nChapter 8.\\nConditional Manatees\\n237\\n8.1.\\nBuilding an interaction\\n239\\n8.2.\\nSymmetry of interactions\\n250\\n8.3.\\nContinuous interactions\\n252\\n8.4.\\nSummary\\n260\\n8.5.\\nPractice\\n260\\nChapter 9.\\nMarkov Chain Monte Carlo\\n263\\n9.1.\\nGood King Markov and his island kingdom\\n264\\n9.2.\\nMetropolis algorithms\\n267\\n9.3.\\nHamiltonian Monte Carlo\\n270\\n9.4.\\nEasy HMC: ulam\\n279\\n9.5.\\nCare and feeding of your Markov chain\\n287\\n9.6.\\nSummary\\n296\\n9.7.\\nPractice\\n296\\nChapter 10.\\nBig Entropy and the Generalized Linear Model\\n299\\n10.1.\\nMaximum entropy\\n300\\n10.2.\\nGeneralized linear models\\n312\\n10.3.\\nMaximum entropy priors\\n321\\n10.4.\\nSummary\\n321\\nChapter 11.\\nGod Spiked the Integers\\n323\\n11.1.\\nBinomial regression\\n324\\n11.2.\\nPoisson regression\\n345\\n11.3.\\nMultinomial and categorical models\\n359\\n11.4.\\nSummary\\n365\\n11.5.\\nPractice\\n366\\nChapter 12.\\nMonsters and Mixtures\\n369\\n12.1.\\nOver-dispersed counts\\n369\\n12.2.\\nZero-inflated outcomes\\n376\\n'},\n",
       " {'index': 7,\n",
       "  'number': None,\n",
       "  'content': 'CONTENTS\\nvii\\n12.3.\\nOrdered categorical outcomes\\n380\\n12.4.\\nOrdered categorical predictors\\n391\\n12.5.\\nSummary\\n397\\n12.6.\\nPractice\\n397\\nChapter 13.\\nModels With Memory\\n399\\n13.1.\\nExample: Multilevel tadpoles\\n401\\n13.2.\\nVarying effects and the underfitting/overfitting trade-off\\n408\\n13.3.\\nMore than one type of cluster\\n415\\n13.4.\\nDivergent transitions and non-centered priors\\n420\\n13.5.\\nMultilevel posterior predictions\\n426\\n13.6.\\nSummary\\n431\\n13.7.\\nPractice\\n431\\nChapter 14.\\nAdventures in Covariance\\n435\\n14.1.\\nVarying slopes by construction\\n437\\n14.2.\\nAdvanced varying slopes\\n447\\n14.3.\\nInstruments and causal designs\\n455\\n14.4.\\nSocial relations as correlated varying effects\\n462\\n14.5.\\nContinuous categories and the Gaussian process\\n467\\n14.6.\\nSummary\\n485\\n14.7.\\nPractice\\n485\\nChapter 15.\\nMissing Data and Other Opportunities\\n489\\n15.1.\\nMeasurement error\\n491\\n15.2.\\nMissing data\\n499\\n15.3.\\nCategorical errors and discrete absences\\n516\\n15.4.\\nSummary\\n521\\n15.5.\\nPractice\\n521\\nChapter 16.\\nGeneralized Linear Madness\\n525\\n16.1.\\nGeometric people\\n526\\n16.2.\\nHidden minds and observed behavior\\n531\\n16.3.\\nOrdinary differential nut cracking\\n536\\n16.4.\\nPopulation dynamics\\n541\\n16.5.\\nSummary\\n550\\n16.6.\\nPractice\\n550\\nChapter 17.\\nHoroscopes\\n553\\nEndnotes\\n557\\nBibliography\\n573\\nCitation index\\n585\\nTopic index\\n589\\n'},\n",
       " {'index': 8, 'number': None, 'content': ''},\n",
       " {'index': 9,\n",
       "  'number': None,\n",
       "  'content': 'Preface to the Second Edition\\nIt came as a complete surprise to me that I wrote a statistics book. It is even more sur-\\nprising how popular the book has become. But I had set out to write the statistics book that\\nI wish I could have had in graduate school. No one should have to learn this stuff the way I\\ndid. I am glad there is an audience to benefit from the book.\\nIt consumed five years to write it. There was an initial set of course notes, melted down\\nand hammered into a first 200-page manuscript. I discarded that first manuscript. But it\\ntaught me the outline of the book I really wanted to write. Then, several years of teaching\\nwith the manuscript further refined it.\\nReally, I could have continued refining it every year. Going to press carries the penalty of\\nfreezing a dynamic process of both learning how to teach the material and keeping up with\\nchanges in the material. As time goes on, I see more elements of the book that I wish I had\\ndone differently. I’ve also received a lot of feedback on the book, and that feedback has given\\nme ideas for improving it.\\nSo in the second edition, I put those ideas into action. The major changes are:\\nThe R package has some new tools. The map tool from the first edition is still here, but\\nnow it is named quap. This renaming is to avoid misunderstanding. We just used it to get\\na quadratic approximation to the posterior. So now it is named as such. A bigger change is\\nthat map2stan has been replaced by ulam. The new ulam is very similar to map2stan, and\\nin many cases can be used identically. But it is also much more flexible, mainly because it\\ndoes not make any assumptions about GLM structure and allows explicit variable types. All\\nthe map2stan code is still in the package and will continue to work. But now ulam allows for\\nmuch more, especially in later chapters. Both of these tools allow sampling from the prior\\ndistribution, using extract.prior, as well as the posterior. This helps with the next change.\\nMuch more prior predictive simulation. A prior predictive simulation means simulating\\npredictions from a model, using only the prior distribution instead of the posterior distri-\\nbution. This is very useful for understanding the implications of a prior. There was only a\\nvestigial amount of this in the first edition. Now many modeling examples have some prior\\npredictive simulation. I think this is one of the most useful additions to the second edition,\\nsince it helps so much with understanding not only priors but also the model itself.\\nMore emphasis on the distinction between prediction and inference. Chapter 5, the chap-\\nter on multiple regression, has been split into two chapters. The first chapter focuses on\\nhelpful aspects of regression; the second focuses on ways that it can mislead. This allows as\\nwell a more direct discussion of causal inference. This means that DAGs—directed acyclic\\nix\\n'},\n",
       " {'index': 10,\n",
       "  'number': None,\n",
       "  'content': 'x\\nPREFACE TO THE SECOND EDITION\\ngraphs—make an appearance. The chapter on overfitting, Chapter 7 now, is also more di-\\nrect in cautioning about the predictive nature of information criteria and cross-validation.\\nCross-validation and importance sampling approximations of it are now discussed explicitly.\\nNew model types. Chapter 4 now presents simple splines. Chapter 7 introduces one kind\\nor robust regression. Chapter 12 explains how to use ordered categorical predictor variables.\\nChapter 13 presents a very simple type of social network model, the social relations model.\\nChapter 14 has an example of a phylogenetic regression, with a somewhat critical and hetero-\\ndox presentation. And there is an entirely new chapter, Chapter 16, that focuses on models\\nthat are not easily conceived of as GLMMs, including ordinary differential equation models.\\nSome new data examples. There are some new data examples, including the Japanese cherry\\nblossoms time series on the cover and a larger primate evolution data set with 300 species\\nand a matching phylogeny.\\nMore presentation of raw Stan models. There are many more places now where raw Stan\\nmodel code is explained. I hope this makes a transition to working directly in Stan easier.\\nBut most of the time, working directly in Stan is still optional.\\nKindness and persistence. As in the first edition, I have tried to make the material as kind as\\npossible. None of this stuff is easy, and the journey into understanding is long and haunted.\\nIt is important that readers expect that confusion is normal. This is also the reason that I\\nhave not changed the basic modeling strategy in the book.\\nFirst, I force the reader to explicitly specify every assumption of the model. Some readers\\nof the first edition lobbied me to use simplified formula tools like brms or rstanarm. Those\\nare fantastic packages, and graduating to use them after this book is recommended. But\\nI don’t see how a person can come to understand the model when using those tools. The\\npriors being hidden isn’t the most limiting part. Instead, since linear model formulas like\\ny ~ (1|x) + z don’t show the parameters, nor even all of the terms, it is not easy to see\\nhow the mathematical model relates to the code. It is ultimately kinder to be a bit cruel and\\nrequire more work. So the formula lists remain. You’ll thank me later.\\nSecond, half the book goes by before MCMC appears. Some readers of the first edi-\\ntion wanted me to start instead with MCMC. I do not do this because Bayes is not about\\nMCMC. We seek the posterior distribution, but there are many legitimate approximations\\nof it. MCMC is just one set of strategies. Using quadratic approximation in the first half also\\nallows a clearer tie to non-Bayesian algorithms. And since finding the quadratic approxima-\\ntion is fast, it means readers don’t have to struggle with too many things at once.\\nThanks. Many readers and colleagues contributed comments that improved upon the first\\nedition. There are too many to name individually. Several anonymous reviewers provided\\nmany pages of constructive criticism. Bret Beheim and Aki Vehtari commented on multi-\\nple chapters. My colleagues at the Max Planck Institute for Evolutionary Anthropology in\\nLeipzig made the largest contributions, by working through draft chapters and being relent-\\nlessly honest.\\nRichard McElreath\\nLeipzig, 14 December 2019\\n'},\n",
       " {'index': 11,\n",
       "  'number': None,\n",
       "  'content': 'Preface\\nMasons, when they start upon a building,\\nAre careful to test out the scaffolding;\\nMake sure that planks won’t slip at busy points,\\nSecure all ladders, tighten bolted joints.\\nAnd yet all this comes down when the job’s done\\nShowing off walls of sure and solid stone.\\nSo if, my dear, there sometimes seem to be\\nOld bridges breaking between you and me\\nNever fear. We may let the scaffolds fall\\nConfident that we have built our wall.\\n(“Scaffolding” by Seamus Heaney, 1939–2013)\\nThis book means to help you raise your knowledge of and confidence in statistical mod-\\neling. It is meant as a scaffold, one that will allow you to construct the wall that you need,\\neven though you will discard it afterwards. As a result, this book teaches the material in of-\\nten inconvenient fashion, forcing you to perform step-by-step calculations that are usually\\nautomated. The reason for all the algorithmic fuss is to ensure that you understand enough\\nof the details to make reasonable choices and interpretations in your own modeling work.\\nSo although you will move on to use more automation, it’s important to take things slow at\\nfirst. Put up your wall, and then let the scaffolding fall.\\nAudience\\nThe principle audience is researchers in the natural and social sciences, whether new\\nPhD students or seasoned professionals, who have had a basic course on regression but\\nnevertheless remain uneasy about statistical modeling. This audience accepts that there is\\nsomething vaguely wrong about typical statistical practice in the early twenty-first century,\\ndominated as it is by p-values and a confusing menagerie of testing procedures. They see al-\\nternative methods in journals and books. But these people are not sure where to go to learn\\nabout these methods.\\nAs a consequence, this book doesn’t really argue against p-values and the like. The prob-\\nlem in my opinion isn’t so much p-values as the set of odd rituals that have evolved around\\nxi\\n'},\n",
       " {'index': 12,\n",
       "  'number': None,\n",
       "  'content': 'xii\\nPREFACE\\nthem, in the wilds of the sciences, as well as the exclusion of so many other useful tools. So\\nthe book assumes the reader is ready to try doing statistical inference without p-values. This\\nisn’t the ideal situation. It would be better to have material that helps you spot common mis-\\ntakes and misunderstandings of p-values and tests in general, as all of us have to understand\\nsuch things, even if we don’t use them. So I’ve tried to sneak in a little material of that kind,\\nbut unfortunately cannot devote much space to it. The book would be too long, and it would\\ndisrupt the teaching flow of the material.\\nIt’s important to realize, however, that the disregard paid to p-values is not a uniquely\\nBayesian attitude. Indeed, significance testing can be—and has been—formulated as a Bayes-\\nian procedure as well. So the choice to avoid significance testing is stimulated instead by\\nepistemological concerns, some of which are briefly discussed in the first chapter.\\nTeaching strategy\\nThe book uses much more computer code than formal mathematics. Even excellent\\nmathematicians can have trouble understanding an approach, until they see a working algo-\\nrithm. This is because implementation in code form removes all ambiguities. So material of\\nthis sort is easier to learn, if you also learn how to implement it.\\nIn addition to any pedagogical value of presenting code, so much of statistics is now com-\\nputational that a purely mathematical approach is anyways insufficient. As you’ll see in later\\nparts of this book, the same mathematical statistical model can sometimes be implemented\\nin different ways, and the differences matter. So when you move beyond this book to more\\nadvanced or specialized statistical modeling, the computational emphasis here will help you\\nrecognize and cope with all manner of practical troubles.\\nEvery section of the book is really just the tip of an iceberg. I’ve made no attempt to be\\nexhaustive. Rather I’ve tried to explain something well. In this attempt, I’ve woven a lot of\\nconcepts and material into data analysis examples. So instead of having traditional units on,\\nfor example, centering predictor variables, I’ve developed those concepts in the context of a\\nnarrative about data analysis. This is certainly not a style that works for all readers. But it\\nhas worked for a lot of my students. I suspect it fails dramatically for those who are being\\nforced to learn this information. For the internally motivated, it reflects how we really learn\\nthese skills in the context of our research.\\nHow to use this book\\nThis book is not a reference, but a course. It doesn’t try to support random access.\\nRather, it expects sequential access. This has immense pedagogical advantages, but it has\\nthe disadvantage of violating how most scientists actually read books.\\nThis book has a lot of code in it, integrated fully into the main text. The reason for this is\\nthat doing model-based statistics in the twenty-first century requires simple programming.\\nThe code is really not optional. Everyplace, I have erred on the side of including too much\\ncode, rather than too little. In my experience teaching scientific programming, novices learn\\nmore quickly when they have working code to modify, rather than needing to write an algo-\\nrithm from scratch. My generation was probably the last to have to learn some programming\\nto use a computer, and so coding has gotten harder and harder to teach as time goes on. My\\nstudents are very computer literate, but they sometimes have no idea what computer code\\nlooks like.\\n'},\n",
       " {'index': 13,\n",
       "  'number': None,\n",
       "  'content': 'HOW TO USE THIS BOOK\\nxiii\\nWhat the book assumes. This book does not try to teach the reader to program, in the most\\nbasic sense. It assumes that you have made a basic effort to learn how to install and process\\ndata in R. In most cases, a short introduction to R programming will be enough. I know\\nmany people have found Emmanuel Paradis’ R for Beginners helpful. You can find it and\\nmany other beginner guides here:\\nhttp://cran.r-project.org/other-docs.html\\nTo make use of this book, you should know already that y<-7 stores the value 7 in the sym-\\nbol y. You should know that symbols which end in parentheses are functions. You should\\nrecognize a loop and understand that commands can be embedded inside other commands\\n(recursion). Knowing that R vectorizes a lot of code, instead of using loops, is important. But\\nyou don’t have to yet be confident with R programming.\\nInevitably you will come across elements of the code in this book that you haven’t seen\\nbefore. I have made an effort to explain any particularly important or unusual programming\\ntricks in my own code. In fact, this book spends a lot of time explaining code. I do this\\nbecause students really need it. Unless they can connect each command to the recipe and\\nthe goal, when things go wrong, they won’t know whether it is because of a minor or major\\nerror. The same issue arises when I teach mathematical evolutionary theory—students and\\ncolleagues often suffer from rusty algebra skills, so when they can’t get the right answer, they\\noften don’t know whether it’s because of some small mathematical misstep or instead some\\nproblem in strategy. The protracted explanations of code in this book aim to build a level of\\nunderstanding that allows the reader to diagnose and fix problems.\\nWhy R. This book uses R for the same reason that it uses English: Lots of people know it\\nalready. R is convenient for doing computational statistics. But many other languages are\\nequally fine. I recommend Python (especially PyMC) and Julia as well. The first edition\\nended up with code translations for various languages and styles. Hopefully, the second\\nedition will as well.\\nUsing the code. Code examples in the book are marked by a shaded box, and output from\\nexample code is often printed just beneath a shaded box, but marked by a fixed-width type-\\nface. For example:\\nR code\\nprint( \"All models are wrong, but some are useful.\" )\\n0.1\\n[1] \"All models are wrong, but some are useful.\"\\nNext to each snippet of code, you’ll find a number that you can search for in the accompa-\\nnying code snippet file, available from the book’s website. The intention is that the reader\\nfollow along, executing the code in the shaded boxes and comparing their own output to that\\nprinted in the book. I really want you to execute the code, because just as one cannot learn\\nmartial arts by watching Bruce Lee movies, you can’t learn to program statistical models by\\nonly reading a book. You have to get in there and throw some punches and, likewise, take\\nsome hits.\\nIf you ever get confused, remember that you can execute each line independently and\\ninspect the intermediate calculations. That’s how you learn as well as solve problems. For\\nexample, here’s a confusing way to multiply the numbers 10 and 20:\\n'},\n",
       " {'index': 14,\n",
       "  'number': None,\n",
       "  'content': 'xiv\\nPREFACE\\nR code\\nx <- 1:2\\n0.2\\nx <- x*10\\nx <- log(x)\\nx <- sum(x)\\nx <- exp(x)\\nx\\n200\\nIf you don’t understand any particular step, you can always print out the contents of the sym-\\nbol x immediately after that step. For the code examples, this is how you come to understand\\nthem. For your own code, this is how you find the source of any problems and then fix them.\\nOptional sections. Reflecting realism in how books like this are actually read, there are two\\nkinds of optional sections: (1) Rethinking and (2) Overthinking. The Rethinking sections\\nlook like this:\\nRethinking: Think again. The point of these Rethinking boxes is to provide broader context for the\\nmaterial. They allude to connections to other approaches, provide historical background, or call out\\ncommon misunderstandings. These boxes are meant to be optional, but they round out the material\\nand invite deeper thought.\\nThe Overthinking sections look like this:\\nOverthinking: Getting your hands dirty. These sections, set in smaller type, provide more detailed\\nexplanations of code or mathematics. This material isn’t essential for understanding the main text.\\nBut it does have a lot of value, especially on a second reading. For example, sometimes it matters how\\nyou perform a calculation. Mathematics tells that these two expressions are equivalent:\\np1 = log(0\\n200\\n.01\\n)\\np2 = 200 × log(0.01)\\nBut when you use R to compute them, they yield different answers:\\nR code\\n0.3\\n( log( 0.01^200 ) )\\n( 200 * log(0.01) )\\n[1] -Inf\\n[1] -921.034\\nThe second line is the right answer. This problem arises because of rounding error, when the computer\\nrounds very small decimal values to zero. This loses precision and can introduce substantial errors in\\ninference. As a result, we nearly always do statistical calculations using the logarithm of a probability,\\nrather than the probability itself.\\nYou can ignore most of these Overthinking sections on a first read.\\nThe command line is the best tool. Programming at the level needed to perform twenty-\\nfirst century statistical inference is not that complicated, but it is unfamiliar at first. Why\\nnot just teach the reader how to do all of this with a point-and-click program? There are\\nbig advantages to doing statistics with text commands, rather than pointing and clicking on\\nmenus.\\n'},\n",
       " {'index': 15,\n",
       "  'number': None,\n",
       "  'content': 'HOW TO USE THIS BOOK\\nxv\\nEveryone knows that the command line is more powerful. But it also saves you time\\nand fulfills ethical obligations. With a command script, each analysis documents itself, so\\nthat years from now you can come back to your analysis and replicate it exactly. You can\\nre-use your old files and send them to colleagues. Pointing and clicking, however, leaves\\nno trail of breadcrumbs. A file with your R commands inside it does. Once you get in the\\nhabit of planning, running, and preserving your statistical analyses in this way, it pays for\\nitself many times over. With point-and-click, you pay down the road, rather than only up\\nfront. It is also a basic ethical requirement of science that our analyses be fully documented\\nand repeatable. The integrity of peer review and the cumulative progress of research depend\\nupon it. A command line statistical program makes this documentation natural. A point-\\nand-click interface does not. Be ethical.\\nSo we don’t use the command line because we are hardcore or elitist (although we might\\nbe). We use the command line because it is better. It is harder at first. Unlike the point-and-\\nclick interface, you do have to learn a basic set of commands to get started with a command\\nline interface. However, the ethical and cost saving advantages are worth the inconvenience.\\nHow you should work. But I would be cruel, if I just told the reader to use a command-line\\ntool, without also explaining something about how to do it. You do have to relearn some\\nhabits, but it isn’t a major change. For readers who have only used menu-driven statistics\\nsoftware before, there will be some significant readjustment. But after a few days, it will\\nseem natural to you. For readers who have used command-driven statistics software like\\nStata and SAS, there is still some readjustment ahead. I’ll explain the overall approach first.\\nThen I’ll say why even Stata and SAS users are in for a change.\\nThe sane approach to scripting statistical analyses is to work back and forth between\\ntwo applications: (1) a plain text editor of your choice and (2) the R program running in a\\nterminal. There are several applications that integrate the text editor with the R console. The\\nmost popular of these is RStudio. It has a lot of options, but really it is just an interface that\\nincludes both a script editor and an R terminal.\\nA plain text editor is a program that creates and edits simple formatting-free text files.\\nCommon examples include Notepad (in Windows) and TextEdit (in Mac OS X) and Emacs\\n(in most *NIX distributions, including Mac OS X). There is also a wide selection of fancy\\ntext editors specialized for programmers. You might investigate, for example, RStudio and\\nthe Atom text editor, both of which are free. Note that MSWord files are not plain text.\\nYou will use a plain text editor to keep a running log of the commands you feed into the\\nR application for processing. You absolutely do not want to just type out commands directly\\ninto R itself. Instead, you want to either copy and paste lines of code from your plain text\\neditor into R, or instead read entire script files directly into R. You might enter commands\\ndirectly into R as you explore data or debug or merely play. But your serious work should be\\nimplemented through the plain text editor, for the reasons explained in the previous section.\\nYou can add comments to your R scripts to help you plan the code and remember later\\nwhat the code is doing. To make a comment, just begin a line with the # symbol. To help clar-\\nify the approach, below I provide a very short complete script for running a linear regression\\non one of R’s built-in sets of data. Even if you don’t know what the code does yet, hopefully\\nyou will see it as a basic model of clarity of formatting and use of comments.\\nR code\\n# Load the data:\\n0.4\\n# car braking distances in feet paired with speeds in km/h\\n'},\n",
       " {'index': 16,\n",
       "  'number': None,\n",
       "  'content': 'xvi\\nPREFACE\\n# see ?cars for details\\ndata(cars)\\n# fit a linear regression of distance on speed\\nm <- lm( dist ~ speed , data=cars )\\n# estimated coefficients from the model\\ncoef(m)\\n# plot residuals against speed\\nplot( resid(m) ~ speed , data=cars )\\nEven those who are familiar with scripting Stata or SAS will be in for some readjust-\\nment. Programs like Stata and SAS have a different paradigm for how information is pro-\\ncessed. In those applications, procedural commands like PROC GLM are issued in imitation\\nof menu commands. These procedures produce a mass of default output that the user then\\nsifts through. R does not behave this way. Instead, R forces the user to decide which bits of\\ninformation she wants. One fits a statistical model in R and then must issue later commands\\nto ask questions about it. This more interrogative paradigm will become familiar through the\\nexamples in the text. But be aware that you are going to take a more active role in deciding\\nwhat questions to ask about your models.\\nInstalling the rethinking R package\\nThe code examples require that you have installed the rethinking R package. This\\npackage contains the data examples and many of the modeling tools that the text uses. The\\nrethinking package itself relies upon another package, rstan, for fitting the more advanced\\nmodels in the second half of the book.\\nYou should install rstan first. Navigate your internet browser to mc-stan.org and\\nfollow the instructions for your platform. You will need to install both a C++ compiler\\n(also called the “tool chain”) and the rstan package. Instructions for doing both are at\\nmc-stan.org. Then from within R, you can install rethinking with this code:\\nR code\\ninstall.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"dagitty\"))\\n0.5\\nlibrary(devtools)\\ndevtools::install_github(\"rmcelreath/rethinking\")\\nNote that rethinking is not on the CRAN package archive, at least not yet. You’ll always be\\nable to perform a simple internet search and figure out the current installation instructions\\nfor the most recent version of the rethinking package. If you encounter any bugs while us-\\ning the package, you can check github.com/rmcelreath/rethinking to see if a solution\\nis already posted. If not, you can leave a bug report and be notified when a solution becomes\\navailable. In addition, all of the source code for the package is found there, in case you aspire\\nto do some tinkering of your own. Feel free to “fork” the package and bend it to your will.\\nAcknowledgments\\nMany people have contributed advice, ideas, and complaints to this book. Most impor-\\ntant among them have been the graduate students who have taken statistics courses from\\n'},\n",
       " {'index': 17,\n",
       "  'number': None,\n",
       "  'content': 'ACKNOWLEDGMENTS\\nxvii\\nme over the last decade, as well as the colleagues who have come to me for advice. These\\npeople taught me how to teach them this material, and in some cases I learned the material\\nonly because they needed it. A large number of individuals donated their time to comment\\non sections of the book or accompanying computer code. These include: Rasmus Bååth,\\nRyan Baldini, Bret Beheim, Maciek Chudek, John Durand, Andrew Gelman, Ben Goodrich,\\nMark Grote, Dave Harris, Chris Howerton, James Holland Jones, Jeremy Koster, Andrew\\nMarshall, Sarah Mathew, Karthik Panchanathan, Pete Richerson, Alan Rogers, Cody Ross,\\nNoam Ross, Aviva Rossi, Kari Schroeder, Paul Smaldino, Rob Trangucci, Shravan Vasishth,\\nAnnika Wallin, and a score of anonymous reviewers. Bret Beheim and Dave Harris were\\nbrave enough to provide extensive comments on an early draft. Caitlin DeRango and Kot-\\nrina Kajokaite invested their time in improving several chapters and problem sets. Mary\\nBrooke McEachern provided crucial opinions on content and presentation, as well as calm\\nsupport and tolerance. A number of anonymous reviewers provided detailed feedback on\\nindividual chapters. None of these people agree with all of the choices I have made, and all\\nmistakes and deficiencies remain my responsibility. But especially when we haven’t agreed,\\ntheir opinions have made the book stronger.\\nThe book is dedicated to Dr. Parry M. R. Clarke (1977–2012), who asked me to write\\nit. Parry’s inquisition of statistical and mathematical and computational methods helped\\neveryone around him. He made us better.\\n'},\n",
       " {'index': 18, 'number': None, 'content': ''},\n",
       " {'index': 19,\n",
       "  'number': 1,\n",
       "  'content': '1 The Golem of Prague\\nIn the sixteenth century, the House of Habsburg controlled much of Central Europe, the\\nNetherlands, and Spain, as well as Spain’s colonies in the Americas. The House was maybe\\nthe first true world power. The Sun shone always on some portion of it. Its ruler was also\\nHoly Roman Emperor, and his seat of power was Prague. The Emperor in the late sixteenth\\ncentury, Rudolph II, loved intellectual life. He invested in the arts, the sciences (including\\nastrology and alchemy), and mathematics, making Prague into a world center of learning\\nand scholarship. It is appropriate then that in this learned atmosphere arose an early robot,\\nthe Golem of Prague.\\nA golem (goh-lem) is a clay robot from Jewish folklore, constructed from dust and fire\\nand water. It is brought to life by inscribing emet, Hebrew for “truth,” on its brow. Animated\\nby truth, but lacking free will, a golem always does exactly what it is told. This is lucky,\\nbecause the golem is incredibly powerful, able to withstand and accomplish more than its\\ncreators could. However, its obedience also brings danger, as careless instructions or unex-\\npected events can turn a golem against its makers. Its abundance of power is matched by its\\nlack of wisdom.\\nIn some versions of the golem legend, Rabbi Judah Loew ben Bezalel sought a way to\\ndefend the Jews of Prague. As in many parts of sixteenth century Central Europe, the Jews of\\nPrague were persecuted. Using secret techniques from the Kabbalah, Rabbi Judah was able\\nto build a golem, animate it with “truth,” and order it to defend the Jewish people of Prague.\\nNot everyone agreed with Judah’s action, fearing unintended consequences of toying with\\nthe power of life. Ultimately Judah was forced to destroy the golem, as its combination of\\nextraordinary power with clumsiness eventually led to innocent deaths. Wiping away one\\nletter from the inscription emet to spell instead met, “death,” Rabbi Judah decommissioned\\nthe robot.\\n1.1. Statistical golems\\nScientists also make golems.1 Our golems rarely have physical form, but they too are\\noften made of clay, living in silicon as computer code. These golems are scientific models.\\nBut these golems have real effects on the world, through the predictions they make and the\\nintuitions they challenge or inspire. A concern with “truth” enlivens these models, but just\\nlike a golem or a modern robot, scientific models are neither true nor false, neither prophets\\nnor charlatans. Rather they are constructs engineered for some purpose. These constructs\\nare incredibly powerful, dutifully conducting their programmed calculations.\\n1\\n'},\n",
       " {'index': 20,\n",
       "  'number': 2,\n",
       "  'content': '2\\n1. THE GOLEM OF PRAGUE\\nFigure 1.1. Example decision tree, or flowchart, for selecting an appropri-\\nate statistical procedure. Beginning at the top, the user answers a series of\\nquestions about measurement and intent, arriving eventually at the name\\nof a procedure. Many such decision trees are possible.\\nSometimes their unyielding logic reveals implications previously hidden to their design-\\ners. These implications can be priceless discoveries. Or they may produce silly and dan-\\ngerous behavior. Rather than idealized angels of reason, scientific models are powerful clay\\nrobots without intent of their own, bumbling along according to the myopic instructions\\nthey embody. Like with Rabbi Judah’s golem, the golems of science are wisely regarded with\\nboth awe and apprehension. We absolutely have to use them, but doing so always entails\\nsome risk.\\nThere are many kinds of statistical models. Whenever someone deploys even a simple\\nstatistical procedure, like a classical t-test, she is deploying a small golem that will obediently\\ncarry out an exact calculation, performing it the same way (nearly2) every time, without\\ncomplaint. Nearly every branch of science relies upon the senses of statistical golems. In\\nmany cases, it is no longer possible to even measure phenomena of interest, without making\\nuse of a model. To measure the strength of natural selection or the speed of a neutrino or\\nthe number of species in the Amazon, we must use models. The golem is a prosthesis, doing\\nthe measuring for us, performing impressive calculations, finding patterns where none are\\nobvious.\\nHowever, there is no wisdom in the golem. It doesn’t discern when the context is inap-\\npropriate for its answers. It just knows its own procedure, nothing else. It just does as it’s told.\\n'},\n",
       " {'index': 21,\n",
       "  'number': 3,\n",
       "  'content': '1.1. STATISTICAL GOLEMS\\n3\\nAnd so it remains a triumph of statistical science that there are now so many diverse golems,\\neach useful in a particular context. Viewed this way, statistics is neither mathematics nor a\\nscience, but rather a branch of engineering. And like engineering, a common set of design\\nprinciples and constraints produces a great diversity of specialized applications.\\nThis diversity of applications helps to explain why introductory statistics courses are so\\noften confusing to the initiates. Instead of a single method for building, refining, and cri-\\ntiquing statistical models, students are offered a zoo of pre-constructed golems known as\\n“tests.” Each test has a particular purpose. Decision trees, like the one in Figure 1.1, are\\ncommon. By answering a series of sequential questions, users choose the “correct” proce-\\ndure for their research circumstances.\\nUnfortunately, while experienced statisticians grasp the unity of these procedures, stu-\\ndents and researchers rarely do. Advanced courses in statistics do emphasize engineering\\nprinciples, but most scientists never get that far. Teaching statistics this way is somewhat\\nlike teaching engineering backwards, starting with bridge building and ending with basic\\nphysics. So students and many scientists tend to use charts like Figure 1.1 without much\\nthought to their underlying structure, without much awareness of the models that each proce-\\ndure embodies, and without any framework to help them make the inevitable compromises\\nrequired by real research. It’s not their fault.\\nFor some, the toolbox of pre-manufactured golems is all they will ever need. Provided\\nthey stay within well-tested contexts, using only a few different procedures in appropriate\\ntasks, a lot of good science can be completed. This is similar to how plumbers can do a lot\\nof useful work without knowing much about fluid dynamics. Serious trouble begins when\\nscholars move on to conducting innovative research, pushing the boundaries of their spe-\\ncialties. It’s as if we got our hydraulic engineers by promoting plumbers.\\nWhy aren’t the tests enough for research? The classical procedures of introductory sta-\\ntistics tend to be inflexible and fragile. By inflexible, I mean that they have very limited ways\\nto adapt to unique research contexts. By fragile, I mean that they fail in unpredictable ways\\nwhen applied to new contexts. This matters, because at the boundaries of most sciences,\\nit is hardly ever clear which procedure is appropriate. None of the traditional golems has\\nbeen evaluated in novel research settings, and so it can be hard to choose one and then to\\nunderstand how it behaves. A good example is Fisher’s exact test, which applies (exactly) to\\nan extremely narrow empirical context, but is regularly used whenever cell counts are small.\\nI have personally read hundreds of uses of Fisher’s exact test in scientific journals, but aside\\nfrom Fisher’s original use of it, I have never seen it used appropriately. Even a procedure like\\nordinary linear regression, which is quite flexible in many ways, being able to encode a large\\ndiversity of interesting hypotheses, is sometimes fragile. For example, if there is substan-\\ntial measurement error on prediction variables, then the procedure can fail in spectacular\\nways. But more importantly, it is nearly always possible to do better than ordinary linear\\nregression, largely because of a phenomenon known as overfitting (Chapter 7).\\nThe point isn’t that statistical tools are specialized. Of course they are. The point is that\\nclassical tools are not diverse enough to handle many common research questions. Every\\nactive area of science contends with unique difficulties of measurement and interpretation,\\nconverses with idiosyncratic theories in a dialect barely understood by other scientists from\\nother tribes. Statistical experts outside the discipline can help, but they are limited by lack of\\nfluency in the empirical and theoretical concerns of the discipline.\\nFurthermore, no statistical tool does anything on its own to address the basic problem\\nof inferring causes from evidence. Statistical golems do not understand cause and effect.\\n'},\n",
       " {'index': 22,\n",
       "  'number': 4,\n",
       "  'content': '4\\n1. THE GOLEM OF PRAGUE\\nThey only understand association. Without our guidance and skepticism, pre-manufactured\\ngolems may do nothing useful at all. Worse, they might wreck Prague.\\nWhat researchers need is some unified theory of golem engineering, a set of principles for\\ndesigning, building, and refining special-purpose statistical procedures. Every major branch\\nof statistical philosophy possesses such a unified theory. But the theory is never taught in\\nintroductory—and often not even in advanced—courses. So there are benefits in rethinking\\nstatistical inference as a set of strategies, instead of a set of pre-made tools.\\n1.2. Statistical rethinking\\nA lot can go wrong with statistical inference, and this is one reason that beginners are\\nso anxious about it. When the goal is to choose a pre-made test from a flowchart, then the\\nanxiety can mount as one worries about choosing the “correct” test. Statisticians, for their\\npart, can derive pleasure from scolding scientists, making the psychological battle worse.\\nBut anxiety can be cultivated into wisdom. That is the reason that this book insists on\\nworking with the computational nuts and bolts of each golem. If you don’t understand how\\nthe golem processes information, then you can’t interpret the golem’s output. This requires\\nknowing the model in greater detail than is customary, and it requires doing the computa-\\ntions the hard way, at least until you are wise enough to use the push-button solutions.\\nThere are conceptual obstacles as well, obstacles with how scholars define statistical ob-\\njectives and interpret statistical results. Understanding any individual golem is not enough,\\nin these cases. Instead, we need some statistical epistemology, an appreciation of how sta-\\ntistical models relate to hypotheses and the natural mechanisms of interest. What are we\\nsupposed to be doing with these little computational machines, anyway?\\nThe greatest obstacle that I encounter among students and colleagues is the tacit belief\\nthat the proper objective of statistical inference is to test null hypotheses.3 This is the proper\\nobjective, the thinking goes, because Karl Popper argued that science advances by falsifying\\nhypotheses. Karl Popper (1902–1994) is possibly the most influential philosopher of science,\\nat least among scientists. He did persuasively argue that science works better by developing\\nhypotheses that are, in principle, falsifiable. Seeking out evidence that might embarrass our\\nideas is a normative standard, and one that most scholars—whether they describe themselves\\nas scientists or not—subscribe to. So maybe statistical procedures should falsify hypotheses,\\nif we wish to be good statistical scientists.\\nBut the above is a kind of folk Popperism, an informal philosophy of science common\\namong scientists but not among philosophers of science. Science is not described by the falsi-\\nfication standard, and Popper recognized that.4 In fact, deductive falsification is impossible\\nin nearly every scientific context. In this section, I review two reasons for this impossibility.\\n(1) Hypotheses are not models. The relations among hypotheses and different kinds of\\nmodels are complex. Many models correspond to the same hypothesis, and many\\nhypotheses correspond to a single model. This makes strict falsification impossible.\\n(2) Measurement matters. Even when we think the data falsify a model, another ob-\\nserver will debate our methods and measures. They don’t trust the data. Sometimes\\nthey are right.\\nFor both of these reasons, deductive falsification never works. The scientific method cannot\\nbe reduced to a statistical procedure, and so our statistical methods should not pretend. Sta-\\ntistical evidence is part of the hot mess that is science, with all of its combat and egotism and\\nmutual coercion. If you believe, as I do, that science does often work, then learning that it\\n'},\n",
       " {'index': 23,\n",
       "  'number': 5,\n",
       "  'content': '1.2. STATISTICAL RETHINKING\\n5\\ndoesn’t work via falsification shouldn’t change your mind. But it might help you do better\\nscience. It might open your eyes to many legitimately useful functions of statistical golems.\\nRethinking: Is NHST falsificationist? Null hypothesis significance testing, NHST, is often identified\\nwith the falsificationist, or Popperian, philosophy of science. However, usually NHST is used to falsify\\na null hypothesis, not the actual research hypothesis. So the falsification is being done to something\\nother than the explanatory model. This seems the reverse from Karl Popper’s philosophy.5\\n1.2.1. Hypotheses are not models. When we attempt to falsify a hypothesis, we must work\\nwith a model of some kind. Even when the attempt is not explicitly statistical, there is always\\na tacit model of measurement, of evidence, that operationalizes the hypothesis. All models\\nare false,6 so what does it mean to falsify a model? One consequence of the requirement\\nto work with models is that it’s no longer possible to deduce that a hypothesis is false, just\\nbecause we reject a model derived from it.\\nLet’s explore this consequence in the context of an example from population biology\\n(Figure 1.2). Beginning in the 1960s, evolutionary biologists became interested in the pro-\\nposal that the majority of evolutionary changes in gene frequency are caused not by natural\\nselection, but rather by mutation and drift. No one really doubted that natural selection is re-\\nsponsible for functional design. This was a debate about genetic sequences. So began several\\nproductive decades of scholarly combat over “neutral” models of molecular evolution.7 This\\ncombat is most strongly associated with Motoo Kimura (1924–1994), who was perhaps the\\nstrongest advocate of neutral models. But many other population geneticists participated.\\nAs time has passed, related disciplines such as community ecology8 and anthropology9 have\\nexperienced (or are currently experiencing) their own versions of the neutrality debate.\\nLet’s use the schematic in Figure 1.2 to explore connections between motivating hy-\\npotheses and different models, in the context of the neutral evolution debate. On the left,\\nthere are two stereotyped, informal hypotheses: Either evolution is “neutral” (H0) or natu-\\nral selection matters somehow (H1). These hypotheses have vague boundaries, because they\\nbegin as verbal conjectures, not precise models. There are hundreds of possible detailed pro-\\ncesses that can be described as “neutral,” depending upon choices about population struc-\\nture, number of sites, number of alleles at each site, mutation rates, and recombination.\\nOnce we have made these choices, we have the middle column in Figure 1.2, detailed\\nprocess models of evolution. P0A and P0B differ in that one assumes the population size\\nand structure have been constant long enough for the distribution of alleles to reach a steady\\nstate. The other imagines instead that population size fluctuates through time, which can\\nbe true even when there is no selective difference among alleles. The “selection matters”\\nhypothesis H1 likewise corresponds to many different process models. I’ve shown two big\\nplayers: a model in which selection always favors certain alleles and another in which selec-\\ntion fluctuates through time, favoring different alleles.10\\nAn important feature of these process models is that they express causal structure. Dif-\\nferent process models formalize different cause and effect relationships. Whether analyzed\\nmathematically or through simulation, the direction of time in a model means that some\\nthings cause other things, but not the reverse. You can use such models to perform experi-\\nments and probe their causal implications. Sometimes these probes reveal, before we even\\nturn to statistical inference, that the model cannot explain a phenomenon of interest.\\nIn order to challenge process models with data, they have to be made into statistical\\nmodels. Unfortunately, statistical models do not embody specific causal relationships. A\\n'},\n",
       " {'index': 24,\n",
       "  'number': 6,\n",
       "  'content': '6\\n1. THE GOLEM OF PRAGUE\\nH0\\nH1\\n“Evolution\\n   is neutral”\\n“Selection\\n   matters”\\nP0A\\nNeutral,\\nnon-equilibrium\\nP0B\\nNeutral,\\nequilibrium\\nP1B\\nFluctuating\\nselection\\nP1A\\nConstant\\nselection\\nMI\\nMII\\nMIII\\nHypotheses\\nProcess models\\nStatistical models\\nFigure 1.2. Relations among hypotheses (left), detailed process models\\n(middle), and statistical models (right), illustrated by the example of “neu-\\ntral” models of evolution. Hypotheses (H) are typically vague, and so cor-\\nrespond to more than one process model (P). Statistical evaluations of hy-\\npotheses rarely address process models directly. Instead, they rely upon\\nstatistical models (M), all of which reflect only some aspects of the process\\nmodels. As a result, relations are multiple in both directions: Hypotheses\\ndo not imply unique models, and models do not imply unique hypotheses.\\nThis fact greatly complicates statistical inference.\\nstatistical model expresses associations among variables. As a result, many different process\\nmodels may be consistent with any single statistical model.\\nHow do we get a statistical model from a causal model? One way is to derive the ex-\\npected frequency distribution of some quantity—a “statistic”—from the causal model. For\\nexample, a common statistic in this context is the frequency distribution (histogram) of the\\nfrequency of different genetic variants (alleles). Some alleles are rare, appearing in only a\\nfew individuals. Others are very common, appearing in very many individuals in the popu-\\nlation. A famous result in population genetics is that a model like P0A produces a power law\\ndistribution of allele frequencies. And so this fact yields a statistical model, MII, that pre-\\ndicts a power law in the data. In contrast the constant selection process model P1A predicts\\nsomething quite different, MIII.\\nUnfortunately, other selection models (P1B) imply the same statistical model, MII, as the\\nneutral model. They also produce power laws. So we’ve reached the uncomfortable lesson:\\n(1) Any given statistical model (M) may correspond to more than one process model\\n(P).\\n(2) Any given hypothesis (H) may correspond to more than one process model (P).\\n(3) Any given statistical model (M) may correspond to more than one hypothesis (H).\\n'},\n",
       " {'index': 25,\n",
       "  'number': 7,\n",
       "  'content': '1.2. STATISTICAL RETHINKING\\n7\\nNow look what happens when we compare the statistical models to data. The classical ap-\\nproach is to take the “neutral” model as a null hypothesis. If the data are not sufficiently\\nsimilar to the expectation under the null, then we say that we “reject” the null hypothesis.\\nSuppose we follow the history of this subject and take P0A as our null hypothesis. This implies\\ndata corresponding to MII. But since the same statistical model corresponds to a selection\\nmodel P1B, it’s not clear what to make of either rejecting or accepting the null. The null\\nmodel is not unique to any process model nor hypothesis. If we reject the null, we can’t\\nreally conclude that selection matters, because there are other neutral models that predict\\ndifferent distributions of alleles. And if we fail to reject the null, we can’t really conclude that\\nevolution is neutral, because some selection models expect the same frequency distribution.\\nThis is a huge bother. Once we have the diagram in Figure 1.2, it’s easy to see the prob-\\nlem. But few of us are so lucky. While population genetics has recognized this issue, scholars\\nin other disciplines continue to test frequency distributions against power law expectations,\\narguing even that there is only one neutral model.11 Even if there were only one neutral\\nmodel, there are so many non-neutral models that mimic the predictions of neutrality, that\\nneither rejecting nor failing to reject the null model carries much inferential power.\\nSo what can be done? Well, if you have multiple process models, a lot can be done. If\\nit turns out that all of the process models of interest make very similar predictions, then\\nyou know to search for a different description of the evidence, a description under which\\nthe processes look different. For example, while P0A and P1B make very similar power law\\npredictions for the frequency distribution of alleles, they make very dissimilar predictions\\nfor the distribution of changes in allele frequency over time. Explicitly compare predictions\\nof more than one model, and you can save yourself from some ordinary kinds of folly.\\nStatistical models can be confused in other ways as well, such as the confusion caused by\\nunobserved variables and sampling bias. Process models allow us to design statistical models\\nwith these problems in mind. The statistical model alone is not enough.\\nRethinking: Entropy and model identification. One reason that statistical models routinely corre-\\nspond to many different detailed process models is because they rely upon distributions like the nor-\\nmal, binomial, Poisson, and others. These distributions are members of a family, the exponential\\nfamily. Nature loves the members of this family. Nature loves them because nature loves entropy,\\nand all of the exponential family distributions are maximum entropy distributions. Taking the nat-\\nural personification out of that explanation will wait until Chapter 10. The practical implication is\\nthat one can no more infer evolutionary process from a power law than one can infer developmental\\nprocess from the fact that height is normally distributed. This fact should make us humble about what\\ntypical regression models—the meat of this book—can teach us about mechanistic process. On the\\nother hand, the maximum entropy nature of these distributions means we can use them to do useful\\nstatistical work, even when we can’t identify the underlying process.\\n1.2.2. Measurement matters. The logic of falsification is very simple. We have a hypothesis\\nH, and we show that it entails some observation D. Then we look for D. If we don’t find it,\\nwe must conclude that H is false. Logicians call this kind of reasoning modus tollens, which\\nis Latin shorthand for “the method of destruction.” In contrast, finding D tells us nothing\\ncertain about H, because other hypotheses might also predict D.\\nA compelling scientific fable that employs modus tollens concerns the color of swans.\\nBefore discovering Australia, all swans that any European had ever seen had white feathers.\\nThis led to the belief that all swans are white. Let’s call this a formal hypothesis:\\nH0: All swans are white.\\n'},\n",
       " {'index': 26,\n",
       "  'number': 8,\n",
       "  'content': '8\\n1. THE GOLEM OF PRAGUE\\nWhen Europeans reached Australia, however, they encountered swans with black feathers.\\nThis evidence seemed to instantly prove H0 to be false. Indeed, not all swans are white. Some\\nare certainly black, according to all observers. The key insight here is that, before voyaging\\nto Australia, no number of observations of white swans could prove H0 to be true. However\\nit required only one observation of a black swan to prove it false.\\nThis is a seductive story. If we can believe that important scientific hypotheses can be\\nstated in this form, then we have a powerful method for improving the accuracy of our the-\\nories: look for evidence that disconfirms our hypotheses. Whenever we find a black swan,\\nH0 must be false. Progress!\\nSeeking disconfirming evidence is important, but it cannot be as powerful as the swan\\nstory makes it appear. In addition to the correspondence problems among hypotheses and\\nmodels, discussed in the previous section, most of the problems scientists confront are not so\\nlogically discrete. Instead, we most often face two simultaneous problems that make the swan\\nfable misrepresentative. First, observations are prone to error, especially at the boundaries\\nof scientific knowledge. Second, most hypotheses are quantitative, concerning degrees of\\nexistence, rather than discrete, concerning total presence or absence. Let’s briefly consider\\neach of these problems.\\n1.2.2.1. Observation error. All observers agree under most conditions that a swan is ei-\\nther black or white. There are few intermediate shades, and most observers’ eyes work simi-\\nlarly enough that there will be little disagreement about which swans are white and which are\\nblack. But this kind of example is hardly commonplace in science, at least in mature fields.\\nInstead, we routinely confront contexts in which we are not sure if we have detected a dis-\\nconfirming result. At the edges of scientific knowledge, the ability to measure a hypothetical\\nphenomenon is often in question as much as the phenomenon itself. Here are two examples.\\nIn 2005, a team of ornithologists from Cornell claimed to have evidence of an individual\\nIvory-billed Woodpecker (Campephilus principalis), a species thought extinct. The hypothe-\\nsis implied here is:\\nH0: The Ivory-billed Woodpecker is extinct.\\nIt would only take one observation to falsify this hypothesis. However, many doubted the\\nevidence. Despite extensive search efforts and a $50,000 cash reward for information leading\\nto a live specimen, no satisfying evidence has yet (by 2020) emerged. Even if good physical\\nevidence does eventually arise, this episode should serve as a counterpoint to the swan story.\\nFinding disconfirming cases is complicated by the difficulties of observation. Black swans\\nare not always really black swans, and sometimes white swans are really black swans. There\\nare mistaken confirmations (false positives) and mistaken disconfirmations (false negatives).\\nAgainst this background of measurement difficulties, scientists who already believe that the\\nIvory-billed Woodpecker is extinct will always be suspicious of a claimed falsification. Those\\nwho believe it is still alive will tend to count the vaguest evidence as falsification.\\nAnother example, this one from physics, focuses on the detection of faster-than-light\\n(FTL) neutrinos.12 In September 2011, a large and respected team of physicists announced\\ndetection of neutrinos—small, neutral sub-atomic particles able to pass easily and harm-\\nlessly through most matter—that arrived from Switzerland to Italy in slightly faster-than-\\nlightspeed time. According to Einstein, neutrinos cannot travel faster than the speed of light.\\nSo this seems to be a falsification of special relativity. If so, it would turn physics on its head.\\n'},\n",
       " {'index': 27,\n",
       "  'number': 9,\n",
       "  'content': '1.2. STATISTICAL RETHINKING\\n9\\nThe dominant reaction from the physics community was not “Einstein was wrong!” but\\ninstead “How did the team mess up the measurement?” The team that made the measure-\\nment had the same reaction, and asked others to check their calculations and attempt to\\nreplicate the result.\\nWhat could go wrong in the measurement? You might think measuring speed is a sim-\\nple matter of dividing distance by time. It is, at the scale and energy you live at. But with\\na fundamental particle like a neutrino, if you measure when it starts its journey, you stop\\nthe journey. The particle is consumed by the measurement. So more subtle approaches are\\nneeded. The detected difference from light-speed, furthermore, is quite small, and so even\\nthe latency of the time it takes a signal to travel from a detector to a control room can be\\norders of magnitude larger. And since the “measurement” in this case is really an estimate\\nfrom a statistical model, all of the assumptions of the model are now suspect. By 2013, the\\nphysics community was unanimous that the FTL neutrino result was measurement error.\\nThey found the technical error, which involved a poorly attached cable.13 Furthermore, neu-\\ntrinos clocked from supernova events are consistent with Einstein, and those distances are\\nmuch larger and so would reveal differences in speed much better.\\nIn both the woodpecker and neutrino dramas, the key dilemma is whether the falsifi-\\ncation is real or spurious. Measurement is complicated in both cases, but in quite different\\nways, rendering both true-detection and false-detection plausible. Popper was aware of this\\nlimitation inherent in measurement, and it may be one reason that Popper himself saw sci-\\nence as being broader than falsification. But the probabilistic nature of evidence rarely ap-\\npears when practicing scientists discuss the philosophy and practice of falsification.14 My\\nreading of the history of science is that these sorts of measurement problems are the norm,\\nnot the exception.15\\n1.2.2.2. Continuous hypotheses. Another problem for the swan story is that most inter-\\nesting scientific hypotheses are not of the kind “all swans are white” but rather of the kind:\\nH0: 80% of swans are white.\\nOr maybe:\\nH0: Black swans are rare.\\nNow what are we to conclude, after observing a black swan? The null hypothesis doesn’t\\nsay black swans do not exist, but rather that they have some frequency. The task here is\\nnot to disprove or prove a hypothesis of this kind, but rather to estimate and explain the\\ndistribution of swan coloration as accurately as we can. Even when there is no measurement\\nerror of any kind, this problem will prevent us from applying the modus tollens swan story\\nto our science.16\\nYou might object that the hypothesis above is just not a good scientific hypothesis, be-\\ncause it isn’t easy to disprove. But if that’s the case, then most of the important questions\\nabout the world are not good scientific hypotheses. In that case, we should conclude that the\\ndefinition of a “good hypothesis” isn’t doing us much good. Now, nearly everyone agrees\\nthat it is a good practice to design experiments and observations that can differentiate com-\\npeting hypotheses. But in many cases, the comparison must be probabilistic, a matter of\\ndegree, not kind.17\\n1.2.3. Falsification is consensual. The scientific community does come to regard some hy-\\npotheses as false. The caloric theory of heat and the geocentric model of the universe are no\\n'},\n",
       " {'index': 28,\n",
       "  'number': 10,\n",
       "  'content': '10\\n1. THE GOLEM OF PRAGUE\\nlonger taught in science courses, unless it’s to teach how they were falsified. And evidence\\noften—but not always—has something to do with such falsification.\\nBut falsification is always consensual, not logical. In light of the real problems of measure-\\nment error and the continuous nature of natural phenomena, scientific communities argue\\ntowards consensus about the meaning of evidence. These arguments can be messy. After the\\nfact, some textbooks misrepresent the history so it appears like logical falsification.18 Such\\nhistorical revisionism may hurt everyone. It may hurt scientists, by rendering it impossible\\nfor their own work to live up to the legends that precede them. It may make science an easy\\ntarget, by promoting an easily attacked model of scientific epistemology. And it may hurt\\nthe public, by exaggerating the definitiveness of scientific knowledge.19\\n1.3. Tools for golem engineering\\nSo if attempting to mimic falsification is not a generally useful approach to statistical\\nmethods, what are we to do? We are to model. Models can be made into testing procedures—\\nall statistical tests are also models20—but they can also be used to design, forecast, and argue.\\nDoing research benefits from the ability to produce and manipulate models, both because\\nscientific problems are more general than “testing” and because the pre-made golems you\\nmaybe met in introductory statistics courses are ill-fit to many research contexts. You may\\nnot even know which statistical model to use, unless you have a generative model in addition.\\nIf you want to reduce your chances of wrecking Prague, then some golem engineering\\nknow-how is needed. Make no mistake: You will wreck Prague eventually. But if you are a\\ngood golem engineer, at least you’ll notice the destruction. And since you’ll know a lot about\\nhow your golem works, you stand a good chance to figure out what went wrong. Then your\\nnext golem won’t be as bad. Without engineering training, you’re always at someone’s mercy.\\nWe want to use our models for several distinct purposes: designing inquiry, extracting\\ninformation from data, and making predictions. In this book I’ve chosen to focus on tools\\nto help with each purpose. These tools are:\\n(1) Bayesian data analysis\\n(2) Model comparison\\n(3) Multilevel models\\n(4) Graphical causal models\\nThese tools are deeply related to one another, so it makes sense to teach them together. Un-\\nderstanding of these tools comes, as always, only with implementation—you can’t compre-\\nhend golem engineering until you do it. And so this book focuses mostly on code, how to\\ndo things. But in the remainder of this chapter, I provide introductions to these tools.\\n1.3.1. Bayesian data analysis. Supposing you have some data, how should you use it to learn\\nabout the world? There is no uniquely correct answer to this question. Lots of approaches,\\nboth formal and heuristic, can be effective. But one of the most effective and general answers\\nis to use Bayesian data analysis. Bayesian data analysis takes a question in the form of a model\\nand uses logic to produce an answer in the form of probability distributions.\\nIn modest terms, Bayesian data analysis is no more than counting the numbers of ways\\nthe data could happen, according to our assumptions. Things that can happen more ways\\nare more plausible. Probability theory is relevant because probability is just a calculus for\\ncounting. This allows us to use probability theory as a general way to represent plausibility,\\nwhether in reference to countable events in the world or rather theoretical constructs like\\n'},\n",
       " {'index': 29,\n",
       "  'number': 11,\n",
       "  'content': '1.3. TOOLS FOR GOLEM ENGINEERING\\n11\\nparameters. The rest follows logically. Once we have defined the statistical model, Bayesian\\ndata analysis forces a purely logical way of processing the data to produce inference.\\nChapter 2 explains this in depth. For now, it will help to have another approach to com-\\npare. Bayesian probability is a very general approach to probability, and it includes as a\\nspecial case another important approach, the frequentist approach. The frequentist ap-\\nproach requires that all probabilities be defined by connection to the frequencies of events\\nin very large samples.21 This leads to frequentist uncertainty being premised on imaginary\\nresampling of data—if we were to repeat the measurement many many times, we would end\\nup collecting a list of values that will have some pattern to it. It means also that parameters\\nand models cannot have probability distributions, only measurements can. The distribution\\nof these measurements is called a sampling distribution. This resampling is never done,\\nand in general it doesn’t even make sense—it is absurd to consider repeat sampling of the\\ndiversification of song birds in the Andes. As Sir Ronald Fisher, one of the most important\\nfrequentist statisticians of the twentieth century, put it:22\\n[...] the only populations that can be referred to in a test of significance\\nhave no objective reality, being exclusively the product of the statistician’s\\nimagination [...]\\nBut in many contexts, like controlled greenhouse experiments, it’s a useful device for describ-\\ning uncertainty. Whatever the context, it’s just part of the model, an assumption about what\\nthe data would look like under resampling. It’s just as fantastical as the Bayesian gambit of\\nusing probability to describe all types of uncertainty, whether empirical or epistemological.23\\nBut these different attitudes towards probability do enforce different trade-offs. Con-\\nsider this simple example where the difference between Bayesian and frequentist probability\\nmatters. In the year 1610, Galileo turned a primitive telescope to the night sky and became\\nthe first human to see Saturn’s rings. Well, he probably saw a blob, with some smaller blobs\\nattached to it (Figure 1.3). Since the telescope was primitive, it couldn’t really focus the im-\\nage very well. Saturn always appeared blurred. This is a statistical problem, of a sort. There’s\\nuncertainty about the planet’s shape, but notice that none of the uncertainty is a result of vari-\\nation in repeat measurements. We could look through the telescope a thousand times, and\\nit will always give the same blurred image (for any given position of the Earth and Saturn).\\nSo the sampling distribution of any measurement is constant, because the measurement is\\ndeterministic—there’s nothing “random” about it. Frequentist statistical inference has a lot\\nof trouble getting started here. In contrast, Bayesian inference proceeds as usual, because\\nthe deterministic “noise” can still be modeled using probability, as long as we don’t identify\\nprobability with frequency. As a result, the field of image reconstruction and processing is\\ndominated by Bayesian algorithms.24\\nIn more routine statistical procedures, like linear regression, this difference in proba-\\nbility concepts has less of an effect. However, it is important to realize that even when a\\nBayesian procedure and frequentist procedure give exactly the same answer, our Bayesian\\ngolems aren’t justifying their inferences with imagined repeat sampling. More generally,\\nBayesian golems treat “randomness” as a property of information, not of the world. Nothing\\nin the real world—excepting controversial interpretations of quantum physics—is actually\\nrandom. Presumably, if we had more information, we could exactly predict everything. We\\njust use randomness to describe our uncertainty in the face of incomplete knowledge. From\\nthe perspective of our golem, the coin toss is “random,” but it’s really the golem that is ran-\\ndom, not the coin.\\n'},\n",
       " {'index': 30,\n",
       "  'number': 12,\n",
       "  'content': '12\\n1. THE GOLEM OF PRAGUE\\nFigure 1.3. Saturn, much like Galileo must have seen it. The true shape\\nis uncertain, but not because of any sampling variation. Probability theory\\ncan still help.\\nNote that the preceding description doesn’t invoke anyone’s “beliefs” or subjective opin-\\nions. Bayesian data analysis is just a logical procedure for processing information. There is\\na tradition of using this procedure as a normative description of rational belief, a tradition\\ncalled Bayesianism.25 But this book neither describes nor advocates it. In fact, I’ll argue\\nthat no statistical approach, Bayesian or otherwise, is by itself sufficient.\\nBefore moving on to describe the next two tools, it’s worth emphasizing an advantage of\\nBayesian data analysis, at least when scholars are learning statistical modeling. This entire\\nbook could be rewritten to remove any mention of “Bayesian.” In places, it would become\\neasier. In others, it would become much harder. But having taught applied statistics both\\nways, I have found that the Bayesian framework presents a distinct pedagogical advantage:\\nmany people find it more intuitive. Perhaps the best evidence for this is that very many sci-\\nentists interpret non-Bayesian results in Bayesian terms, for example interpreting ordinary\\np-values as Bayesian posterior probabilities and non-Bayesian confidence intervals as Bayes-\\nian ones (you’ll learn posterior probability and confidence intervals in Chapters 2 and 3).\\nEven statistics instructors make these mistakes.26 Statisticians appear doomed to republish\\nthe same warnings about misinterpretation of p-values forever. In this sense then, Bayesian\\nmodels lead to more intuitive interpretations, the ones scientists tend to project onto sta-\\ntistical results. The opposite pattern of mistake—interpreting a posterior probability as a\\np-value—seems to happen only rarely.\\nNone of this ensures that Bayesian analyses will be more correct than non-Bayesian anal-\\nyses. It just means that the scientist’s intuitions will less commonly be at odds with the actual\\nlogic of the framework. This simplifies some of the aspects of teaching statistical modeling.\\nRethinking: Probability is not unitary. It will make some readers uncomfortable to suggest that\\nthere is more than one way to define “probability.” Aren’t mathematical concepts uniquely correct?\\nThey are not. Once you adopt some set of premises, or axioms, everything does follow logically in\\nmathematical systems. But the axioms are open to debate and interpretation. So not only is there\\n“Bayesian” and “frequentist” probability, but there are different versions of Bayesian probability even,\\n'},\n",
       " {'index': 31,\n",
       "  'number': 13,\n",
       "  'content': '1.3. TOOLS FOR GOLEM ENGINEERING\\n13\\nrelying upon different arguments to justify the approach. In more advanced Bayesian texts, you’ll\\ncome across names like Bruno de Finetti, Richard T. Cox, and Leonard “Jimmie” Savage. Each of\\nthese figures is associated with a somewhat different conception of Bayesian probability. There are\\nothers. This book mainly follows the “logical” Cox (or Laplace-Jeffreys-Cox-Jaynes) interpretation.\\nThis interpretation is presented beginning in the next chapter, but unfolds fully only in Chapter 10.\\nHow can different interpretations of probability theory thrive? By themselves, mathematical en-\\ntities don’t necessarily “mean” anything, in the sense of real world implication. What does it mean to\\ntake the square root of a negative number? What does it mean to take a limit as something approaches\\ninfinity? These are essential and routine concepts, but their meanings depend upon context and an-\\nalyst, upon beliefs about how well abstraction represents reality. Mathematics doesn’t access the real\\nworld directly. So answering such questions remains a contentious and entertaining project, in all\\nbranches of applied mathematics. So while everyone subscribes to the same axioms of probability,\\nnot everyone agrees in all contexts about how to interpret probability.\\nRethinking: A little history. Bayesian statistical inference is much older than the typical tools of\\nintroductory statistics, most of which were developed in the early twentieth century. Versions of\\nthe Bayesian approach were applied to scientific work in the late 1700s and repeatedly in the nine-\\nteenth century. But after World War I, anti-Bayesian statisticians, like Sir Ronald Fisher, succeeded\\nin marginalizing the approach. All Fisher said about Bayesian analysis (then called inverse probabil-\\nity) in his influential 1925 handbook was:27\\n[...] the theory of inverse probability is founded upon an error, and must be wholly\\nrejected.\\nBayesian data analysis became increasingly accepted within statistics during the second half of the\\ntwentieth century, because it proved not to be founded upon an error. All philosophy aside, it worked.\\nBeginning in the 1990s, new computational approaches led to a rapid rise in application of Bayesian\\nmethods.28 Bayesian methods remain computationally expensive, however. And so as data sets have\\nincreased in scale—millions of rows is common in genomic analysis, for example—alternatives to or\\napproximations to Bayesian inference remain important, and probably always will.\\n1.3.2. Model comparison and prediction. Bayesian data analysis provides a way for models\\nto learn from data. But when there is more than one plausible model—and in most mature\\nfields there should be—how should we choose among them? One answer is to prefer models\\nthat make good predictions. This answer creates a lot of new questions, since knowing which\\nmodel will make the best predictions seems to require knowing the future. We’ll look at two\\nrelated tools, neither of which knows the future: cross-validation and information\\ncriteria. These tools aim to compare models based upon expected predictive accuracy.\\nComparing models by predictive accuracy can be useful in itself. And it will be even\\nmore useful because it leads to the discovery of an amazing fact: Complex models often\\nmake worse predictions than simpler models. The primary paradox of prediction is over-\\nfitting.29 Future data will not be exactly like past data, and so any model that is unaware\\nof this fact tends to make worse predictions than it could. And more complex models tend\\ntowards more overfitting than simple ones—the smarter the golem, the dumber its predic-\\ntions. So if we wish to make good predictions, we cannot judge our models simply on how\\nwell they fit our data. Fitting is easy; prediction is hard.\\nCross-validation and information criteria help us in three ways. First, they provide use-\\nful expectations of predictive accuracy, rather than merely fit to sample. So they compare\\nmodels where it matters. Second, they give us an estimate of the tendency of a model to\\n'},\n",
       " {'index': 32,\n",
       "  'number': 14,\n",
       "  'content': '14\\n1. THE GOLEM OF PRAGUE\\noverfit. This will help us to understand how models and data interact, which in turn helps\\nus to design better models. We’ll take this point up again in the next section. Third, cross-\\nvalidation and information criteria help us to spot highly influential observations.\\nBayesian data analysis has been worked on for centuries. Information criteria are com-\\nparatively very young and the field is evolving quickly. Many statisticians have never used\\ninformation criteria in an applied problem, and there is no consensus about which metrics\\nare best and how best to use them. Still, information criteria are already in frequent use\\nin the sciences, appearing in prominent publications and featuring in prominent debates.30\\nTheir power is often exaggerated, and we will be careful to note what they cannot do as well\\nas what they can.\\nRethinking: The Neanderthal in you. Even simple models need alternatives. In 2010, a draft genome\\nof a Neanderthal demonstrated more DNA sequences in common with non-African contemporary\\nhumans than with African ones. This finding is consistent with interbreeding between Neanderthals\\nand modern humans, as the latter dispersed from Africa. However, just finding DNA in common\\nbetween modern Europeans and Neanderthals is not enough to demonstrate interbreeding. It is also\\nconsistent with ancient structure in the African continent.31 In short, if ancient northeast Africans\\nhad unique DNA sequences, then both Neanderthals and modern Europeans could possess these\\nsequences from a common ancestor, rather than from direct interbreeding. So even in the seemingly\\nsimple case of estimating whether Neanderthals and modern humans share unique DNA, there is\\nmore than one process-based explanation. Model comparison is necessary.\\n1.3.3. Multilevel models. In an apocryphal telling of Hindu cosmology, it is said that the\\nEarth rests on the back of a great elephant, who in turn stands on the back of a massive turtle.\\nWhen asked upon what the turtle stands, a guru is said to reply, “it’s turtles all the way down.”\\nStatistical models don’t contain turtles, but they do contain parameters. And parameters\\nsupport inference. Upon what do parameters themselves stand? Sometimes, in some of\\nthe most powerful models, it’s parameters all the way down. What this means is that any\\nparticular parameter can be usefully regarded as a placeholder for a missing model. Given\\nsome model of how the parameter gets its value, it is simple enough to embed the new model\\ninside the old one. This results in a model with multiple levels of uncertainty, each feeding\\ninto the next—a multilevel model.\\nMultilevel models—also known as hierarchical, random effects, varying effects, or mixed\\neffects models—are becoming de rigueur in the biological and social sciences. Fields as di-\\nverse as educational testing and bacterial phylogenetics now depend upon routine multilevel\\nmodels to process data. Like Bayesian data analysis, multilevel modeling is not particularly\\nnew. But it has only been available on desktop computers for a few decades. And since\\nsuch models have a natural Bayesian representation, they have grown hand-in-hand with\\nBayesian data analysis.\\nOne reason to be interested in multilevel models is because they help us deal with over-\\nfitting. Cross-validation and information criteria measure overfitting risk and help us to\\nrecognize it. Multilevel models actually do something about it. What they do is exploit an\\namazing trick known as partial pooling that pools information across units in the data\\nin order to produce better estimates for all units. The details will wait until Chapter 13.\\nPartial pooling is the key technology, and the contexts in which it is appropriate are\\ndiverse. Here are four commonplace examples.\\n'},\n",
       " {'index': 33,\n",
       "  'number': 15,\n",
       "  'content': '1.3. TOOLS FOR GOLEM ENGINEERING\\n15\\n(1) To adjust estimates for repeat sampling. When more than one observation arises\\nfrom the same individual, location, or time, then traditional, single-level models\\nmay mislead us.\\n(2) To adjust estimates for imbalance in sampling. When some individuals, locations, or\\ntimes are sampled more than others, we may also be misled by single-level models.\\n(3) To study variation. If our research questions include variation among individuals\\nor other groups within the data, then multilevel models are a big help, because they\\nmodel variation explicitly.\\n(4) To avoid averaging. Pre-averaging data to construct variables can be dangerous.\\nAveraging removes variation, manufacturing false confidence. Multilevel models\\npreserve the uncertainty in the original, pre-averaged values, while still using the\\naverage to make predictions.\\nAll four apply to contexts in which the researcher recognizes clusters or groups of measure-\\nments that may differ from one another. These clusters or groups may be individuals such\\nas different students, locations such as different cities, or times such as different years. Since\\neach cluster may well have a different average tendency or respond differently to any treat-\\nment, clustered data often benefit from being modeled by a golem that expects such variation.\\nBut the scope of multilevel modeling is much greater than these examples. Diverse\\nmodel types turn out to be multilevel: models for missing data (imputation), measurement\\nerror, factor analysis, some time series models, types of spatial and network regression, and\\nphylogenetic regressions all are special applications of the multilevel strategy. And some\\ncommonplace procedures, like the paired t-test, are really multilevel models in disguise.\\nGrasping the concept of multilevel modeling may lead to a perspective shift. Suddenly single-\\nlevel models end up looking like mere components of multilevel models. The multilevel\\nstrategy provides an engineering principle to help us to introduce these components into a\\nparticular analysis, exactly where we think we need them.\\nI want to convince the reader of something that appears unreasonable: multilevel regres-\\nsion deserves to be the default form of regression. Papers that do not use multilevel models\\nshould have to justify not using a multilevel approach. Certainly some data and contexts do\\nnot need the multilevel treatment. But most contemporary studies in the social and natural\\nsciences, whether experimental or not, would benefit from it. Perhaps the most important\\nreason is that even well-controlled treatments interact with unmeasured aspects of the indi-\\nviduals, groups, or populations studied. This leads to variation in treatment effects, in which\\nindividuals or groups vary in how they respond to the same circumstance. Multilevel mod-\\nels attempt to quantify the extent of this variation, as well as identify which units in the data\\nresponded in which ways.\\nThese benefits don’t come for free, however. Fitting and interpreting multilevel mod-\\nels can be considerably harder than fitting and interpreting a traditional regression model.\\nIn practice, many researchers simply trust their black-box software and interpret multilevel\\nregression exactly like single-level regression. In time, this will change. There was a time\\nin applied statistics when even ordinary multiple regression was considered cutting edge,\\nsomething for only experts to fiddle with. Instead, scientists used many simple procedures,\\nlike t-tests. Now, almost everyone uses multivariate tools. The same will eventually be true\\nof multilevel models. Scholarly culture and curriculum still have some catching up to do.\\n'},\n",
       " {'index': 34,\n",
       "  'number': 16,\n",
       "  'content': '16\\n1. THE GOLEM OF PRAGUE\\nRethinking: Multilevel election forecasting. One of the older applications of multilevel modeling is\\nto forecast the outcomes of elections. In the 1960s, John Tukey (1915–2000) began working for the\\nNational Broadcasting Company (NBC) in the United States, developing real-time election prediction\\nmodels that could exploit diverse types of data: polls, past elections, partial results, and complete re-\\nsults from related districts. The models used a multilevel framework similar to the models presented\\nin Chapters 13 and 14. Tukey developed and used such models for NBC through 1978.32 Contempo-\\nrary election prediction and poll aggregation remains an active topic for multilevel modeling.33\\n1.3.4. Graphical causal models. When the wind blows, branches sway. If you are human,\\nyou immediately interpret this statement as causal: The wind makes the branches move. But\\nall we see is a statistical association. From the data alone, it could also be that the branches\\nswaying makes the wind. That conclusion seems foolish, because you know trees do not\\nsway their own branches. A statistical model is an amazing association engine. It makes\\nit possible to detect associations between causes and their effects. But a statistical model\\nis never sufficient for inferring cause, because the statistical model makes no distinction\\nbetween the wind causing the branches to sway and the branches causing the wind to blow.\\nFacts outside the data are needed to decide which explanation is correct.\\nCross-validation and information criteria try to guess predictive accuracy. When I in-\\ntroduced them above, I described overfitting as the primary paradox in prediction. Now we\\nturn to a secondary paradox in prediction: Models that are causally incorrect can make better\\npredictions than those that are causally correct. As a result, focusing on prediction can system-\\natically mislead us. And while you may have heard that randomized controlled experiments\\nallow causal inference, randomized experiments entail the same risks. No one is safe.\\nI will call this the identification problem and carefully distinguish it from the prob-\\nlem of raw prediction. Consider two different meanings of “prediction.” The simplest applies\\nwhen we are external observers simply trying to guess what will happen next. In that case,\\ntools like cross-validation are very useful. But these tools will happily recommend models\\nthat contain confounding variables and suggest incorrect causal relationships. Why? Con-\\nfounded relationships are real associations, and they can improve prediction. After all, if\\nyou look outside and see branches swaying, it really does predict wind. Successful predic-\\ntion does not require correct causal identification. In fact, as you’ll see later in the book,\\npredictions may actually improve when we use a model that is causally misleading.\\nBut what happens when we intervene in the world? Then we must consider a second\\nmeaning of “prediction.” Suppose we recruit many people to climb into the trees and sway\\nthe branches. Will it make wind? Not much. Often the point of statistical modeling is to pro-\\nduce understanding that leads to generalization and application. In that case, we need more\\nthan just good predictions, in the absence of intervention. We also need an accurate causal\\nunderstanding. But comparing models on the basis of predictive accuracy—or p-values or\\nanything else—will not necessarily produce it.\\nSo what can be done? What is needed is a causal model that can be used to design one or\\nmore statistical models for the purpose of causal identification. As I mentioned in the neu-\\ntral molecular evolution example earlier in this chapter, a complete scientific model contains\\nmore information than a statistical model derived from it. And this additional information\\ncontains causal implications. These implications make it possible to test alternative causal\\nmodels. The implications and tests depend upon the details. Newton’s laws of motion for\\n'},\n",
       " {'index': 35,\n",
       "  'number': 17,\n",
       "  'content': '1.4. SUMMARY\\n17\\nexample precisely predict the consequences of specific interventions. And these precise pre-\\ndictions tell us that the laws are only approximately right.\\nUnfortunately, much scientific work lacks such precise models. Instead we must work\\nwith vaguer hypotheses and try to estimate vague causal effects. Economics for example has\\nno good quantitative model for predicting the effect of changing the minimum wage. But the\\nvery good news is that even when you don’t have a precise causal model, but only a heuristic\\none indicating which variables causally influence others, you can still do useful causal infer-\\nence. Economics might, for example, be able to estimate the causal effect of changing the\\nminimum wage, even without a good scientific model of the economy.\\nFormal methods for distinguishing causal inference from association date from the first\\nhalf of the twentieth century, but they have more recently been extended to the study of\\nmeasurement, experimental design, and the ability to generalize (or transport) results across\\nsamples.34 We’ll meet these methods through the use of a graphical causal model.\\nThe simplest graphical causal model is a directed acyclic graph, usually called a DAG.\\nDAGs are heuristic—they are not detailed statistical models. But they allow us to deduce\\nwhich statistical models can provide valid causal inferences, assuming the DAG is true.\\nBut where does a DAG itself come from? The terrible truth about statistical inference\\nis that its validity relies upon information outside the data. We require a causal model with\\nwhich to design both the collection of data and the structure of our statistical models. But\\nthe construction of causal models is not a purely statistical endeavor, and statistical analysis\\ncan never verify all of our assumptions. There will never be a golem that accepts naked data\\nand returns a reliable model of the causal relations among the variables. We’re just going to\\nhave to keep doing science.\\nRethinking: Causal salad. Causal inference requires a causal model that is separate from the statisti-\\ncal model. The data are not enough. Every philosophy agrees upon that much. Responses, however,\\nare diverse. The most conservative response is to declare “causation” to be unprovable mental candy,\\nlike debating the nature of the afterlife.35 Slightly less conservative is to insist that cause can only be\\ninferred under strict conditions of randomization and experimental control. This would be very lim-\\niting. Many scientific questions can never be studied experimentally—human evolution, for example.\\nMany others could in principle be studied experimentally, but it would be unethical to do so. And\\nmany experiments are really just attempts at control—patients do not always take their medication.\\nBut the approach which dominates in many parts of biology and the social sciences is instead\\ncausal salad.36 Causal salad means tossing various “control” variables into a statistical model,\\nobserving changes in estimates, and then telling a story about causation. Causal salad seems founded\\non the notion that only omitted variables can mislead us about causation. But included variables can\\njust as easily confound us. When tossing a causal salad, a model that makes good predictions may still\\nmislead about causation. If we use the model to plan an intervention, it will get everything wrong.\\nThere will be examples in later chapters.\\n1.4. Summary\\nThis first chapter has argued for a rethinking of popular statistical and scientific phi-\\nlosophy. Instead of choosing among various black-box tools for testing null hypotheses,\\nwe should learn to build and analyze multiple non-null models of natural phenomena. To\\nsupport this goal, the chapter introduced Bayesian inference, model comparison, multilevel\\nmodels, and graphical causal models. The remainder of the book is organized into four parts.\\n'},\n",
       " {'index': 36,\n",
       "  'number': 18,\n",
       "  'content': '18\\n1. THE GOLEM OF PRAGUE\\n(1) Chapters 2 and 3 are foundational. They introduce Bayesian inference and the basic\\ntools for performing Bayesian calculations. They move quite slowly and emphasize\\na purely logical interpretation of probability theory.\\n(2) The next five chapters, 4 through 8, build multiple linear regression as a Bayesian\\ntool. This tool supports causal inference, but only when we analyze separate causal\\nmodels that help us determine which variables to include. For this reason, you’ll\\nlearn basic causal reasoning supported by causal graphs. These chapters emphasize\\nplotting results instead of attempting to interpret estimates of individual parame-\\nters. Problems of model complexity—overfitting—also feature prominently. So\\nyou’ll also get an introduction to information theory and predictive model com-\\nparison in Chapter 7.\\n(3) The third part of the book, Chapters 9 through 12, presents generalized linear mod-\\nels of several types. Chapter 9 introduces Markov chain Monte Carlo, used to fit the\\nmodels in later chapters. Chapter 10 introduces maximum entropy as an explicit\\nprocedure to help us design and interpret these models. Then Chapters 11 and 12\\ndetail the models themselves.\\n(4) The last part, Chapters 13 through 16, gets around to multilevel models, as well as\\nspecialized models that address measurement error, missing data, and spatial co-\\nvariation. This material is fairly advanced, but it proceeds in the same mechanistic\\nway as earlier material. Chapter 16 departs from the rest of the book in deploying\\nmodels which are not of the generalized linear type but are rather scientific models\\nexpressed directly as statistical models.\\nThe final chapter, Chapter 17, returns to some of the issues raised in this first one.\\nAt the end of each chapter, there are practice problems ranging from easy to hard. These\\nproblems help you test your comprehension. The harder ones expand on the material, intro-\\nducing new examples and obstacles. Some of the hard problems are quite hard. Don’t worry,\\nif you get stuck from time to time. Working in groups is a good way to get unstuck, just like\\nin real research.\\n'},\n",
       " {'index': 37,\n",
       "  'number': 19,\n",
       "  'content': '2 Small Worlds and Large Worlds\\nWhen Cristoforo Colombo (Christopher Columbus) infamously sailed west in the year\\n1492, he believed that the Earth was spherical. In this, he was like most educated people of\\nhis day. He was unlike most people, though, in that he also believed the planet was much\\nsmaller than it actually is—only 30,000 km around its middle instead of the actual 40,000\\nkm (Figure 2.1).37 This was one of the most consequential mistakes in European history. If\\nColombo had believed instead that the Earth was 40,000 km around, he would have correctly\\nreasoned that his fleet could not carry enough food and potable water to complete a journey\\nall the way westward to Asia. But at 30,000 km around, Asia would lie a bit west of the coast\\nof California. It was possible to carry enough supplies to make it that far. Emboldened in\\npart by his unconventional estimate, Colombo set sail, eventually landing in the Bahamas.\\nColombo made a prediction based upon his view that the world was small. But since he\\nlived in a large world, aspects of the prediction were wrong. In his case, the error was lucky.\\nHis small world model was wrong in an unanticipated way: There was a lot of land in the\\nway. If he had been wrong in the expected way, with nothing but ocean between Europe and\\nAsia, he and his entire expedition would have run out of supplies long before reaching the\\nEast Indies.\\nColombo’s small and large worlds provide a contrast between model and reality. All sta-\\ntistical modeling has these two frames: the small world of the model itself and the large world\\nwe hope to deploy the model in.38 Navigating between these two worlds remains a central\\nchallenge of statistical modeling. The challenge is greater when we forget the distinction.\\nThe small world is the self-contained logical world of the model. Within the small\\nworld, all possibilities are nominated. There are no pure surprises, like the existence of a huge\\ncontinent between Europe and Asia. Within the small world of the model, it is important to\\nbe able to verify the model’s logic, making sure that it performs as expected under favorable\\nassumptions. Bayesian models have some advantages in this regard, as they have reasonable\\nclaims to optimality: No alternative model could make better use of the information in the\\ndata and support better decisions, assuming the small world is an accurate description of the\\nreal world.39\\nThe large world is the broader context in which one deploys a model. In the large\\nworld, there may be events that were not imagined in the small world. Moreover, the model\\nis always an incomplete representation of the large world, and so will make mistakes, even\\nif all kinds of events have been properly nominated. The logical consistency of a model in\\nthe small world is no guarantee that it will be optimal in the large world. But it is certainly a\\nwarm comfort.\\n19\\n'},\n",
       " {'index': 38,\n",
       "  'number': 20,\n",
       "  'content': '20\\n2. SMALL WORLDS AND LARGE WORLDS\\nFigure 2.1. Illustration of Martin Behaim’s\\n1492 globe, showing the small world that\\nColombo anticipated. Europe lies on the right-\\nhand side. Asia lies on the left. The big island\\nlabeled “Cipangu” is Japan.\\nIn this chapter, you will begin to build Bayesian models. The way that Bayesian models\\nlearn from evidence is arguably optimal in the small world. When their assumptions approx-\\nimate reality, they also perform well in the large world. But large world performance has to\\nbe demonstrated rather than logically deduced. Passing back and forth between these two\\nworlds allows both formal methods, like Bayesian inference, and informal methods, like peer\\nreview, to play an indispensable role.\\nThis chapter focuses on the small world. It explains probability theory in its essential\\nform: counting the ways things can happen. Bayesian inference arises automatically from\\nthis perspective. Then the chapter presents the stylized components of a Bayesian statistical\\nmodel, a model for learning from data. Then it shows you how to animate the model, to\\nproduce estimates.\\nAll this work provides a foundation for the next chapter, in which you’ll learn to sum-\\nmarize Bayesian estimates, as well as begin to consider large world obligations.\\nRethinking: Fast and frugal in the large world. The natural world is complex, as trying to do science\\nserves to remind us. Yet everything from the humble tick to the industrious squirrel to the idle sloth\\nmanages to frequently make adaptive decisions. But it’s a good bet that most animals are not Bayesian,\\nif only because being Bayesian is expensive and depends upon having a good model. Instead, animals\\nuse various heuristics that are fit to their environments, past or present. These heuristics take adaptive\\nshortcuts and so may outperform a rigorous Bayesian analysis, once costs of information gathering\\nand processing (and overfitting, Chapter 7) are taken into account.40 Once you already know which\\ninformation to ignore or attend to, being fully Bayesian is a waste. It’s neither necessary nor sufficient\\nfor making good decisions, as real animals demonstrate. But for human animals, Bayesian analysis\\nprovides a general way to discover relevant information and process it logically. Just don’t think that\\nit is the only way.\\n2.1. The garden of forking data\\nOur goal in this section will be to build Bayesian inference up from humble beginnings,\\nso there is no superstition about it. Bayesian inference is really just counting and comparing\\nof possibilities. Consider by analogy Jorge Luis Borges’ short story “The Garden of Forking\\nPaths.” The story is about a man who encounters a book filled with contradictions. In most\\nbooks, characters arrive at plot points and must decide among alternative paths. A protag-\\nonist may arrive at a man’s home. She might kill the man, or rather take a cup of tea. Only\\n'},\n",
       " {'index': 39,\n",
       "  'number': 21,\n",
       "  'content': '2.1. THE GARDEN OF FORKING DATA\\n21\\none of these paths is taken—murder or tea. But the book within Borges’ story explores all\\npaths, with each decision branching outward into an expanding garden of forking paths.\\nThis is the same device that Bayesian inference offers. In order to make good inference\\nabout what actually happened, it helps to consider everything that could have happened.\\nA Bayesian analysis is a garden of forking data, in which alternative sequences of events\\nare cultivated. As we learn about what did happen, some of these alternative sequences are\\npruned. In the end, what remains is only what is logically consistent with our knowledge.\\nThis approach provides a quantitative ranking of hypotheses, a ranking that is maximally\\nconservative, given the assumptions and data that go into it. The approach cannot guarantee\\na correct answer, on large world terms. But it can guarantee the best possible answer, on\\nsmall world terms, that could be derived from the information fed into it.\\nConsider the following toy example.\\n2.1.1. Counting possibilities. Suppose there’s a bag, and it contains four marbles. These\\nmarbles come in two colors: blue and white. We know there are four marbles in the bag,\\nbut we don’t know how many are of each color. We do know that there are five possibilities:\\n(1) [\\n], (2) [\\n], (3) [\\n], (4) [\\n], (5) [\\n]. These are the only\\npossibilities consistent with what we know about the contents of the bag. Call these five\\npossibilities the conjectures.\\nOur goal is to figure out which of these conjectures is most plausible, given some evi-\\ndence about the contents of the bag. We do have some evidence: A sequence of three mar-\\nbles is pulled from the bag, one at a time, replacing the marble each time and shaking the bag\\nbefore drawing another marble. The sequence that emerges is:\\n, in that order. These\\nare the data.\\nSo now let’s plant the garden and see how to use the data to infer what’s in the bag.\\nLet’s begin by considering just the single conjecture, [\\n], that the bag contains one\\nblue and three white marbles. On the first draw from the bag, one of four things could\\nhappen, corresponding to one of four marbles in the bag. So we can visualize the possibilities\\nbranching outward:\\nNotice that even though the three white marbles look the same from a data perspective—\\nwe just record the color of the marbles, after all—they are really different events. This is\\nimportant, because it means that there are three more ways to see\\nthan to see\\n.\\nNow consider the garden as we get another draw from the bag. It expands the garden\\nout one layer:\\nNow there are 16 possible paths through the garden, one for each pair of draws. On the\\nsecond draw from the bag, each of the paths above again forks into four possible paths. Why?\\n'},\n",
       " {'index': 40,\n",
       "  'number': 22,\n",
       "  'content': '22\\n2. SMALL WORLDS AND LARGE WORLDS\\nFigure 2.2. The 64 possible paths generated by assuming the bag contains\\none blue and three white marbles.\\nBecause we believe that our shaking of the bag gives each marble a fair chance at being drawn,\\nregardless of which marble was drawn previously. The third layer is built in the same way,\\nand the full garden is shown in Figure 2.2. There are 43 = 64 possible paths in total.\\nAs we consider each draw from the bag, some of these paths are logically eliminated.\\nThe first draw tuned out to be\\n, recall, so the three white paths at the bottom of the garden\\nare eliminated right away. If you imagine the real data tracing out a path through the garden,\\nit must have passed through the one blue path near the origin. The second draw from the\\nbag produces\\n, so three of the paths forking out of the first blue marble remain. As the\\ndata trace out a path, we know it must have passed through one of those three white paths\\n(after the first blue path), but we don’t know which one, because we recorded only the color\\nof each marble. Finally, the third draw is\\n. Each of the remaining three paths in the middle\\nlayer sustain one blue path, leaving a total of three ways for the sequence\\nto appear,\\nassuming the bag contains [\\n]. Figure 2.3 shows the garden again, now with logically\\neliminated paths grayed out. We can’t be sure which of those three paths the actual data took.\\nBut as long as we’re considering only the possibility that the bag contains one blue and three\\nwhite marbles, we can be sure that the data took one of those three paths. Those are the only\\npaths consistent with both our knowledge of the bag’s contents (four marbles, white or blue)\\nand the data (\\n).\\nThis demonstrates that there are three (out of 64) ways for a bag containing [\\n]\\nto produce the data\\n. We have no way to decide among these three ways. The infer-\\nential power comes from comparing this count to the numbers of ways each of the other\\nconjectures of the bag’s contents could produce the same data. For example, consider the\\nconjecture [\\n]. There are zero ways for this conjecture to produce the observed data,\\nbecause even one\\nis logically incompatible with it. The conjecture [\\n] is likewise\\nlogically incompatible with the data. So we can eliminate these two conjectures, because\\nneither provides even a single path that is consistent with the data.\\nFigure 2.4 displays the full garden now, for the remaining three conjectures: [\\n],\\n[\\n], and [\\n]. The upper-left wedge displays the same garden as Figure 2.3.\\nThe upper-right shows the analogous garden for the conjecture that the bag contains three\\nblue marbles and one white marble. And the bottom wedge shows the garden for two blue\\n'},\n",
       " {'index': 41,\n",
       "  'number': 23,\n",
       "  'content': '2.1. THE GARDEN OF FORKING DATA\\n23\\nFigure 2.3. After eliminating paths inconsistent with the observed se-\\nquence, only 3 of the 64 paths remain.\\nand two white marbles. Now we count up all of the ways each conjecture could produce the\\nobserved data. For one blue and three white, there are three ways, as we counted already. For\\ntwo blue and two white, there are eight paths forking through the garden that are logically\\nconsistent with the observed sequence. For three blue and one white, there are nine paths\\nthat survive.\\nTo summarize, we’ve considered five different conjectures about the contents of the bag,\\nranging from zero blue marbles to four blue marbles. For each of these conjectures, we’ve\\ncounted up how many sequences, paths through the garden of forking data, could potentially\\nproduce the observed data,\\n:\\nConjecture\\nWays to produce\\n[\\n]\\n0 × 4 × 0 = 0\\n[\\n]\\n1 × 3 × 1 = 3\\n[\\n]\\n2 × 2 × 2 = 8\\n[\\n]\\n3 × 1 × 3 = 9\\n[\\n]\\n4 × 0 × 4 = 0\\nNotice that the number of ways to produce the data, for each conjecture, can be computed\\nby first counting the number of paths in each “ring” of the garden and then by multiplying\\nthese counts together. This is just a computational device. It tells us the same thing as Fig-\\nure 2.4, but without having to draw the garden. The fact that numbers are multiplied during\\ncalculation doesn’t change the fact that this is still just counting of logically possible paths.\\nThis point will come up again, when you meet a formal representation of Bayesian inference.\\nSo what good are these counts? By comparing these counts, we have part of a solution\\nfor a way to rate the relative plausibility of each conjectured bag composition. But it’s only a\\npart of a solution, because in order to compare these counts we first have to decide how many\\nways each conjecture could itself be realized. We might argue that when we have no reason\\nto assume otherwise, we can just consider each conjecture equally plausible and compare the\\ncounts directly. But often we do have reason to assume otherwise.\\n'},\n",
       " {'index': 42,\n",
       "  'number': 24,\n",
       "  'content': '24\\n2. SMALL WORLDS AND LARGE WORLDS\\nFigure 2.4. The garden of forking data, showing for each possible compo-\\nsition of the bag the forking paths that are logically compatible with the data.\\nRethinking: Justification. My justification for using paths through the garden as measures of relative\\nplausibility is humble: If we wish to reason about plausibility and remain consistent with ordinary\\nlogic—statements about true and false—then we should obey this procedure.41 There are other justi-\\nfications that lead to the same mathematical procedure. Regardless of how you choose to philosoph-\\nically justify it, notice that it actually works. Justifications and philosophy motivate procedures, but\\nit is the results that matter. The many successful real world applications of Bayesian inference may\\nbe all the justification you need. Twentieth century opponents of Bayesian data analysis argued that\\nBayesian inference was easy to justify, but hard to apply.42 That is luckily no longer true. Indeed, the\\nopposite is often true—scientists are switching to Bayesian approaches because it lets them use the\\nmodels they want. Just be careful not to assume that because Bayesian inference is justified that no\\nother approach can also be justified. Golems come in many types, and some of all types are useful.\\n'},\n",
       " {'index': 43,\n",
       "  'number': 25,\n",
       "  'content': '2.1. THE GARDEN OF FORKING DATA\\n25\\n2.1.2. Combining other information. We may have additional information about the rel-\\native plausibility of each conjecture. This information could arise from knowledge of how\\nthe contents of the bag were generated. It could also arise from previous data. Whatever the\\nsource, it would help to have a way to combine different sources of information to update\\nthe plausibilities. Luckily there is a natural solution: Just multiply the counts.\\nTo grasp this solution, suppose we’re willing to say each conjecture is equally plausible\\nat the start. So we just compare the counts of ways in which each conjecture is compatible\\nwith the observed data. This comparison suggests that [\\n] is slightly more plausible\\nthan [\\n], and both are about three times more plausible than [\\n]. Since these\\nare our initial counts, and we are going to update them next, let’s label them prior.\\nNow suppose we draw another marble from the bag to get another observation:\\n. Now\\nyou have two choices. You could start all over again, making a garden with four layers to trace\\nout the paths compatible with the data sequence\\n. Or you could take the previous\\ncounts—the prior counts—over conjectures (0, 3, 8, 9, 0) and just update them in light of the\\nnew observation. It turns out that these two methods are mathematically identical, as long\\nas the new observation is logically independent of the previous observations.\\nHere’s how to do it. First we count the numbers of ways each conjecture could produce\\nthe new observation,\\n. Then we multiply each of these new counts by the prior numbers\\nof ways for each conjecture. In table form:\\nWays to\\nPrior\\nConjecture\\nproduce\\ncounts\\nNew count\\n[\\n]\\n0\\n0\\n0 × 0 = 0\\n[\\n]\\n1\\n3\\n3 × 1 = 3\\n[\\n]\\n2\\n8\\n8 × 2 = 16\\n[\\n]\\n3\\n9\\n9 × 3 = 27\\n[\\n]\\n4\\n0\\n0 × 4 = 0\\nThe new counts in the right-hand column above summarize all the evidence for each conjec-\\nture. As new data arrive, and provided those data are independent of previous observations,\\nthen the number of logically possible ways for a conjecture to produce all the data up to that\\npoint can be computed just by multiplying the new count by the old count.\\nThis updating approach amounts to nothing more than asserting that (1) when we have\\nprevious information suggesting there are Wprior ways for a conjecture to produce a previous\\nobservation Dprior and (2) we acquire new observations Dnew that the same conjecture can\\nproduce in Wnew ways, then (3) the number of ways the conjecture can account for both\\nDprior as well as Dnew is just the product Wprior × Wnew. For example, in the table above the\\nconjecture [\\n] has Wprior = 8 ways to produce Dprior =\\n. It also has Wnew = 2\\nways to produce the new observation Dnew =\\n. So there are 8 × 2 = 16 ways for the\\nconjecture to produce both Dprior and Dnew. Why multiply? Multiplication is just a shortcut\\nto enumerating and counting up all of the paths through the garden that could produce all\\nthe observations.\\nIn this example, the prior data and new data are of the same type: marbles drawn from\\nthe bag. But in general, the prior data and new data can be of different types. Suppose for\\nexample that someone from the marble factory tells you that blue marbles are rare. So for\\nevery bag containing [\\n], they made two bags containing [\\n] and three bags\\ncontaining [\\n]. They also ensured that every bag contained at least one blue and one\\nwhite marble. We can update our counts again:\\n'},\n",
       " {'index': 44,\n",
       "  'number': 26,\n",
       "  'content': '26\\n2. SMALL WORLDS AND LARGE WORLDS\\nFactory\\nConjecture\\nPrior count\\ncount\\nNew count\\n[\\n]\\n0\\n0\\n0 × 0 = 0\\n[\\n]\\n3\\n3\\n3 × 3 = 9\\n[\\n]\\n16\\n2\\n16 × 2 = 32\\n[\\n]\\n27\\n1\\n27 × 1 = 27\\n[\\n]\\n0\\n0\\n0 × 0 = 0\\nNow the conjecture [\\n] is most plausible, but barely better than [\\n]. Is there a\\nthreshold difference in these counts at which we can safely decide that one of the conjectures\\nis the correct one? You’ll spend the next chapter exploring that question.\\nRethinking: Original ignorance. Which assumption should we use, when there is no previous infor-\\nmation about the conjectures? The most common solution is to assign an equal number of ways that\\neach conjecture could be correct, before seeing any data. This is sometimes known as the principle\\nof indifference: When there is no reason to say that one conjecture is more plausible than another,\\nweigh all of the conjectures equally. This book does not use nor endorse “ignorance” priors. As we’ll\\nsee in later chapters, the structure of the model and the scientific context always provide information\\nthat allows us to do better than ignorance.\\nFor the sort of problems we examine in this book, the principle of indifference results in infer-\\nences very comparable to mainstream non-Bayesian approaches, most of which contain implicit equal\\nweighting of possibilities. For example a typical non-Bayesian confidence interval weighs equally all\\nof the possible values a parameter could take, regardless of how implausible some of them are. In\\naddition, many non-Bayesian procedures have moved away from equal weighting, through the use of\\npenalized likelihood and other methods. We’ll discuss this in Chapter 7.\\n2.1.3. From counts to probability. It is helpful to think of this strategy as adhering to a\\nprinciple of honest ignorance: When we don’t know what caused the data, potential causes\\nthat may produce the data in more ways are more plausible. This leads us to count paths\\nthrough the garden of forking data. We’re counting the implications of assumptions.\\nIt’s hard to use these counts though, so we almost always standardize them in a way that\\ntransforms them into probabilities. Why is it hard to work with the counts? First, since\\nrelative value is all that matters, the size of the counts 3, 8, and 9 contain no information of\\nvalue. They could just as easily be 30, 80, and 90. The meaning would be the same. It’s just\\nthe relative values that matter. Second, as the amount of data grows, the counts will very\\nquickly grow very large and become difficult to manipulate. By the time we have 10 data\\npoints, there are already more than one million possible sequences. We’ll want to analyze\\ndata sets with thousands of observations, so explicitly counting these things isn’t practical.\\nLuckily, there’s a mathematical way to compress all of this. Specifically, we define the\\nupdated plausibility of each possible composition of the bag, after seeing the data, as:\\nplausibility of [\\n] after seeing\\n∝\\nways [\\n] can produce\\n×\\nprior plausibility [\\n]\\nThat little ∝means proportional to. We want to compare the plausibility of each possible bag\\ncomposition. So it’ll be helpful to define p as the proportion of marbles that are blue. For\\n'},\n",
       " {'index': 45,\n",
       "  'number': 27,\n",
       "  'content': '2.1. THE GARDEN OF FORKING DATA\\n27\\n[\\n], p = 1/4 = 0.25. Also let Dnew =\\n. And now we can write:\\nplausibility of p after Dnew ∝ways p can produce Dnew × prior plausibility of p\\nThe above just means that for any value p can take, we judge the plausibility of that value p\\nas proportional to the number of ways it can get through the garden of forking data. This\\nexpression just summarizes the calculations you did in the tables of the previous section.\\nFinally, we construct probabilities by standardizing the plausibility so that the sum of\\nthe plausibilities for all possible conjectures will be one. All you need to do in order to stan-\\ndardize is to add up all of the products, one for each value p can take, and then divide each\\nproduct by the sum of products:\\nplausibility of p after Dnew = ways p can produce Dnew × prior plausibility p\\nsum of products\\nA worked example is needed for this to really make sense. So consider again the table from\\nbefore, now updated using our definitions of p and “plausibility”:\\nWays to\\nPossible composition\\np\\nproduce data\\nPlausibility\\n[\\n]\\n0\\n0\\n0\\n[\\n]\\n0.25\\n3\\n0.15\\n[\\n]\\n0.5\\n8\\n0.40\\n[\\n]\\n0.75\\n9\\n0.45\\n[\\n]\\n1\\n0\\n0\\nYou can quickly compute these plausibilities in R:\\nR code\\n2.1\\nways <- c( 0 , 3 , 8 , 9 , 0 )\\nways/sum(ways)\\n[1] 0.00 0.15 0.40 0.45 0.00\\nThe values in ways are the products mentioned before. And sum(ways) is the denominator\\n“sum of products” in the expression near the top of the page.\\nThese plausibilities are also probabilities—they are non-negative (zero or positive) real\\nnumbers that sum to one. And all of the mathematical things you can do with probabilities\\nyou can also do with these values. Specifically, each piece of the calculation has a direct\\npartner in applied probability theory. These partners have stereotyped names, so it’s worth\\nlearning them, as you’ll see them again and again.\\n• A conjectured proportion of blue marbles, p, is usually called a parameter value.\\nIt’s just a way of indexing possible explanations of the data.\\n• The relative number of ways that a value p can produce the data is usually called\\na likelihood. It is derived by enumerating all the possible data sequences that\\ncould have happened and then eliminating those sequences inconsistent with the\\ndata.\\n• The prior plausibility of any specific p is usually called the prior probability.\\n• The new, updated plausibility of any specific p is usually called the posterior\\nprobability.\\nIn the next major section, you’ll meet the more formal notation for these objects and see how\\nthey compose a simple statistical model.\\n'},\n",
       " {'index': 46,\n",
       "  'number': 28,\n",
       "  'content': '28\\n2. SMALL WORLDS AND LARGE WORLDS\\nRethinking: Randomization. When you shuffle a deck of cards or assign subjects to treatments by\\nflipping a coin, it is common to say that the resulting deck and treatment assignments are randomized.\\nWhat does it mean to randomize something? It just means that we have processed the thing so that we\\nknow almost nothing about its arrangement. Shuffling a deck of cards changes our state of knowledge,\\nso that we no longer have any specific information about the ordering of cards. However, the bonus\\nthat arises from this is that, if we really have shuffled enough to erase any prior knowledge of the\\nordering, then the order the cards end up in is very likely to be one of the many orderings with high\\ninformation entropy. The concept of information entropy will be increasingly important as we\\nprogress, and will be unpacked in Chapters 7 and 10.\\n2.2. Building a model\\nBy working with probabilities instead of raw counts, Bayesian inference is made much\\neasier, but it looks much harder. So in this section, we follow up on the garden of forking\\ndata by presenting the conventional form of a Bayesian statistical model. The toy example\\nwe’ll use here has the anatomy of a typical statistical analysis, so it’s the style that you’ll grow\\naccustomed to. But every piece of it can be mapped onto the garden of forking data. The\\nlogic is the same.\\nSuppose you have a globe representing our planet, the Earth. This version of the world\\nis small enough to hold in your hands. You are curious how much of the surface is covered\\nin water. You adopt the following strategy: You will toss the globe up in the air. When you\\ncatch it, you will record whether or not the surface under your right index finger is water or\\nland. Then you toss the globe up in the air again and repeat the procedure.43 This strategy\\ngenerates a sequence of samples from the globe. The first nine samples might look like:\\nW L W W W L W L W\\nwhere W indicates water and L indicates land. So in this example you observe six W (water)\\nobservations and three L (land) observations. Call this sequence of observations the data.\\nTo get the logic moving, we need to make assumptions, and these assumptions constitute\\nthe model. Designing a simple Bayesian model benefits from a design loop with three steps.\\n(1) Data story: Motivate the model by narrating how the data might arise.\\n(2) Update: Educate your model by feeding it the data.\\n(3) Evaluate: All statistical models require supervision, leading to model revision.\\nThe next sections walk through these steps, in the context of the globe tossing evidence.\\n2.2.1. A data story. Bayesian data analysis usually means producing a story for how the\\ndata came to be. This story may be descriptive, specifying associations that can be used to\\npredict outcomes, given observations. Or it may be causal, a theory of how some events\\nproduce other events. Typically, any story you intend to be causal may also be descriptive.\\nBut many descriptive stories are hard to interpret causally. But all data stories are complete,\\nin the sense that they are sufficient for specifying an algorithm for simulating new data. In\\nthe next chapter, you’ll see examples of doing just that, as simulating new data is useful not\\nonly for model criticism but also for model construction.\\nYou can motivate your data story by trying to explain how each piece of data is born. This\\nusually means describing aspects of the underlying reality as well as the sampling process.\\nThe data story in this case is simply a restatement of the sampling process:\\n(1) The true proportion of water covering the globe is p.\\n'},\n",
       " {'index': 47,\n",
       "  'number': 29,\n",
       "  'content': '2.2. BUILDING A MODEL\\n29\\n(2) A single toss of the globe has a probability p of producing a water (W) observation.\\nIt has a probability 1 −p of producing a land (L) observation.\\n(3) Each toss of the globe is independent of the others.\\nThe data story is then translated into a formal probability model. This probability model is\\neasy to build, because the construction process can be usefully broken down into a series of\\ncomponent decisions. Before meeting these components, however, it’ll be useful to visualize\\nhow a Bayesian model behaves. After you’ve become acquainted with how such a model\\nlearns from data, we’ll pop the machine open and investigate its engineering.\\nRethinking: The value of storytelling. The data story has value, even if you quickly abandon it and\\nnever use it to build a model or simulate new observations. Indeed, it is important to eventually\\ndiscard the story, because many different stories correspond to the same model. As a result, showing\\nthat a model does a good job does not in turn uniquely support our data story. Still, the story has value\\nbecause in trying to outline the story, often one realizes that additional questions must be answered.\\nMost data stories are much more specific than are the verbal hypotheses that inspire data collection.\\nHypotheses can be vague, such as “it’s more likely to rain on warm days.” When you are forced to\\nconsider sampling and measurement and make a precise statement of how temperature predicts rain,\\nmany stories and resulting models will be consistent with the same vague hypothesis. Resolving that\\nambiguity often leads to important realizations and model revisions, before any model is fit to data.\\n2.2.2. Bayesian updating. Our problem is one of using the evidence—the sequence of globe\\ntosses—to decide among different possible proportions of water on the globe. These propor-\\ntions are like the conjectured marbles inside the bag, from earlier in the chapter. Each possi-\\nble proportion may be more or less plausible, given the evidence. A Bayesian model begins\\nwith one set of plausibilities assigned to each of these possibilities. These are the prior plau-\\nsibilities. Then it updates them in light of the data, to produce the posterior plausibilities.\\nThis updating process is a kind of learning, called Bayesian updating. The details of this\\nupdating—how it is mechanically achieved—can wait until later in the chapter. For now,\\nlet’s look only at how such a machine behaves.\\nFor the sake of the example only, let’s program our Bayesian machine to initially assign\\nthe same plausibility to every proportion of water, every value of p. We’ll do better than this\\nlater. Now look at the top-left plot in Figure 2.5. The dashed horizontal line represents this\\ninitial plausibility of each possible value of p. After seeing the first toss, which is a “W,” the\\nmodel updates the plausibilities to the solid line. The plausibility of p = 0 has now fallen\\nto exactly zero—the equivalent of “impossible.” Why? Because we observed at least one\\nspeck of water on the globe, so now we know there is some water. The model executes this\\nlogic automatically. You don’t have it instruct it to account for this consequence. Probability\\ntheory takes care of it for you, because it is essentially counting paths through the garden of\\nforking data, as in the previous section.\\nLikewise, the plausibility of p > 0.5 has increased. This is because there is not yet any\\nevidence that there is land on the globe, so the initial plausibilities are modified to be consis-\\ntent with this. Note however that the relative plausibilities are what matter, and there isn’t\\nyet much evidence. So the differences in plausibility are not yet very large. In this way, the\\namount of evidence seen so far is embodied in the plausibilities of each value of p.\\nIn the remaining plots in Figure 2.5, the additional samples from the globe are intro-\\nduced to the model, one at a time. Each dashed curve is just the solid curve from the previous\\n'},\n",
       " {'index': 48,\n",
       "  'number': 30,\n",
       "  'content': '30\\n2. SMALL WORLDS AND LARGE WORLDS\\nprobability of water\\n0\\n0.5\\n1\\nn = 1\\nW L W W W L W L W\\nconfidence\\nW L W\\nprobability of water\\n0\\n0.5\\n1\\nn = 2\\nW L W W W L W L W\\nW L W\\nprobability of water\\n0\\n0.5\\n1\\nn = 3\\nW L W W W L W L W\\n0\\n0.5\\nprobability of water\\n0\\n0.5\\n1\\nn = 4\\nW L W W W L W L W\\nconfidence\\n0.5\\n1\\n0\\nW L W\\nprobability of water\\n0\\n0.5\\n1\\nn = 5\\nW L W W W L W L W\\n0.5\\n1\\n1\\n0\\nW L W\\nprobability of water\\n0\\n0.5\\n1\\nn = 6\\nW L W W W L W L W\\n0\\n0.5\\nprobability of water\\n0\\n0.5\\n1\\nn = 7\\nW L W W W L W L W\\nconfidence\\n0.5\\n1\\n0\\n1\\nW L W\\nprobability of water\\n0\\n0.5\\n1\\nn = 8\\nW L W W W L W L W\\n0.5\\n1\\n1\\n0\\n1\\nW L W\\nprobability of water\\n0\\n0.5\\n1\\nn = 9\\nW L W W W L W L W\\nplausibility\\nplausibility\\nplausibility\\nproportion water\\nproportion water\\nproportion water\\nFigure 2.5. How a Bayesian model learns. Each toss of the globe produces\\nan observation of water (W) or land (L). The model’s estimate of the pro-\\nportion of water on the globe is a plausibility for every possible value. The\\nlines and curves in this figure are these collections of plausibilities. In each\\nplot, previous plausibilities (dashed curve) are updated in light of the latest\\nobservation to produce a new set of plausibilities (solid curve).\\nplot, moving left to right and top to bottom. Every time a “W” is seen, the peak of the plausi-\\nbility curve moves to the right, towards larger values of p. Every time an “L” is seen, it moves\\nthe other direction. The maximum height of the curve increases with each sample, meaning\\nthat fewer values of p amass more plausibility as the amount of evidence increases. As each\\nnew observation is added, the curve is updated consistent with all previous observations.\\n'},\n",
       " {'index': 49,\n",
       "  'number': 31,\n",
       "  'content': '2.2. BUILDING A MODEL\\n31\\nNotice that every updated set of plausibilities becomes the initial plausibilities for the\\nnext observation. Every conclusion is the starting point for future inference. However, this\\nupdating process works backwards, as well as forwards. Given the final set of plausibilities\\nin the bottom-right plot of Figure 2.5, and knowing the final observation (W), it is possible\\nto mathematically divide out the observation, to infer the previous plausibility curve. So the\\ndata could be presented to your model in any order, or all at once even. In most cases, you\\nwill present the data all at once, for the sake of convenience. But it’s important to realize that\\nthis merely represents abbreviation of an iterated learning process.\\nRethinking: Sample size and reliable inference. It is common to hear that there is a minimum num-\\nber of observations for a useful statistical estimate. For example, there is a widespread superstition\\nthat 30 observations are needed before one can use a Gaussian distribution. Why? In non-Bayesian\\nstatistical inference, procedures are often justified by the method’s behavior at very large sample sizes,\\nso-called asymptotic behavior. As a result, performance at small samples sizes is questionable.\\nIn contrast, Bayesian estimates are valid for any sample size. This does not mean that more data\\nisn’t helpful—it certainly is. Rather, the estimates have a clear and valid interpretation, no matter the\\nsample size. But the price for this power is dependency upon the initial plausibilities, the prior. If\\nthe prior is a bad one, then the resulting inference will be misleading. There’s no free lunch,44 when\\nit comes to learning about the world. A Bayesian golem must choose an initial plausibility, and a\\nnon-Bayesian golem must choose an estimator. Both golems pay for lunch with their assumptions.\\n2.2.3. Evaluate. The Bayesian model learns in a way that is demonstrably optimal, provided\\nthat it accurately describes the real, large world. This is to say that your Bayesian machine\\nguarantees perfect inference within the small world. No other way of using the available\\ninformation, beginning with the same state of information, could do better.\\nDon’t get too excited about this logical virtue, however. The calculations may malfunc-\\ntion, so results always have to be checked. And if there are important differences between\\nthe model and reality, then there is no logical guarantee of large world performance. And\\neven if the two worlds did match, any particular sample of data could still be misleading. So\\nit’s worth keeping in mind at least two cautious principles.\\nFirst, the model’s certainty is no guarantee that the model is a good one. As the amount\\nof data increases, the globe tossing model will grow increasingly sure of the proportion of\\nwater. This means that the curves in Figure 2.5 will become increasingly narrow and tall,\\nrestricting plausible values within a very narrow range. But models of all sorts—Bayesian or\\nnot—can be very confident about an inference, even when the model is seriously misleading.\\nThis is because the inferences are conditional on the model. What your model is telling you\\nis that, given a commitment to this particular model, it can be very sure that the plausible\\nvalues are in a narrow range. Under a different model, things might look differently. There\\nwill be examples in later chapters.\\nSecond, it is important to supervise and critique your model’s work. Consider again the\\nfact that the updating in the previous section works in any order of data arrival. We could\\nshuffle the order of the observations, as long as six W’s and three L’s remain, and still end up\\nwith the same final plausibility curve. That is only true, however, because the model assumes\\nthat order is irrelevant to inference. When something is irrelevant to the machine, it won’t\\naffect the inference directly. But it may affect it indirectly, because the data will depend upon\\norder. So it is important to check the model’s inferences in light of aspects of the data it does\\n'},\n",
       " {'index': 50,\n",
       "  'number': 32,\n",
       "  'content': '32\\n2. SMALL WORLDS AND LARGE WORLDS\\nnot know about. Such checks are an inherently creative enterprise, left to the analyst and the\\nscientific community. Golems are very bad at it.\\nIn Chapter 3, you’ll see some examples of such checks. For now, note that the goal is\\nnot to test the truth value of the model’s assumptions. We know the model’s assumptions\\nare never exactly right, in the sense of matching the true data generating process. Therefore\\nthere’s no point in checking if the model is true. Failure to conclude that a model is false\\nmust be a failure of our imagination, not a success of the model. Moreover, models do not\\nneed to be exactly true in order to produce highly precise and useful inferences. All manner\\nof small world assumptions about error distributions and the like can be violated in the large\\nworld, but a model may still produce a perfectly useful estimate. This is because models\\nare essentially information processing machines, and there are some surprising aspects of\\ninformation that cannot be easily captured by framing the problem in terms of the truth of\\nassumptions.45\\nInstead, the objective is to check the model’s adequacy for some purpose. This usually\\nmeans asking and answering additional questions, beyond those that originally constructed\\nthe model. Both the questions and answers will depend upon the scientific context. So it’s\\nhard to provide general advice. There will be many examples, throughout the book, and\\nof course the scientific literature is replete with evaluations of the suitability of models for\\ndifferent jobs—prediction, comprehension, measurement, and persuasion.\\nRethinking: Deflationary statistics. It may be that Bayesian inference is the best general purpose\\nmethod of inference known. However, Bayesian inference is much less powerful than we’d like it\\nto be. There is no approach to inference that provides universal guarantees. No branch of applied\\nmathematics has unfettered access to reality, because math is not discovered, like the proton. Instead\\nit is invented, like the shovel.46\\n2.3. Components of the model\\nNow that you’ve seen how the Bayesian model behaves, it’s time to open up the machine\\nand learn how it works. Consider three different things that we counted in the previous\\nsections.\\n(1) The number of ways each conjecture could produce an observation\\n(2) The accumulated number of ways each conjecture could produce the entire data\\n(3) The initial plausibility of each conjectured cause of the data\\nEach of these things has a direct analog in conventional probability theory. And so the usual\\nway we build a statistical model involves choosing distributions and devices for each that\\nrepresent the relative numbers of ways things can happen.\\nIn this section, you’ll meet these components in some detail and see how each relates to\\nthe counting you did earlier in the chapter. The job in front of us is really nothing more than\\nnaming all of the variables and defining each. We’ll take these tasks in turn.\\n2.3.1. Variables. Variables are just symbols that can take on different values. In a scientific\\ncontext, variables include things we wish to infer, such as proportions and rates, as well as\\nthings we might observe, the data. In the globe tossing model, there are three variables.\\nThe first variable is our target of inference, p, the proportion of water on the globe. This\\nvariable cannot be observed. Unobserved variables are usually called parameters. But\\nwhile p itself is unobserved, we can infer it from the other variables.\\n'},\n",
       " {'index': 51,\n",
       "  'number': 33,\n",
       "  'content': '2.3. COMPONENTS OF THE MODEL\\n33\\nThe other variables are the observed variables, the counts of water and land. Call the\\ncount of water W and the count of land L. The sum of these two variables is the number of\\nglobe tosses: N = W + L.\\n2.3.2. Definitions. Once we have the variables listed, we then have to define each of them.\\nIn defining each, we build a model that relates the variables to one another. Remember, the\\ngoal is to count all the ways the data could arise, given the assumptions. This means, as in\\nthe globe tossing model, that for each possible value of the unobserved variables, such as\\np, we need to define the relative number of ways—the probability—that the values of each\\nobserved variable could arise. And then for each unobserved variable, we need to define the\\nprior plausibility of each value it could take. I appreciate that this is all a bit abstract. So here\\nare the specifics, for the globe.\\n2.3.2.1. Observed variables. For the count of water W and land L, we define how plau-\\nsible any combination of W and L would be, for a specific value of p. This is very much like\\nthe marble counting we did earlier in the chapter. Each specific value of p corresponds to a\\nspecific plausibility of the data, as in Figure 2.5.\\nSo that we don’t have to literally count, we can use a mathematical function that tells\\nus the right plausibility. In conventional statistics, a distribution function assigned to an\\nobserved variable is usually called a likelihood. That term has special meaning in non-\\nBayesian statistics, however.47 We will be able to do things with our distributions that non-\\nBayesian models forbid. So I will sometimes avoid the term likelihood and just talk about\\ndistributions of variables. But when someone says, “likelihood,” they will usually mean a\\ndistribution function assigned to an observed variable.\\nIn the case of the globe tossing model, the function we need can be derived directly from\\nthe data story. Begin by nominating all of the possible events. There are two: water (W) and\\nland (L). There are no other events. The globe never gets stuck to the ceiling, for example.\\nWhen we observe a sample of W’s and L’s of length N (nine in the actual sample), we need\\nto say how likely that exact sample is, out of the universe of potential samples of the same\\nlength. That might sound challenging, but it’s the kind of thing you get good at very quickly,\\nonce you start practicing.\\nIn this case, once we add our assumptions that (1) every toss is independent of the other\\ntosses and (2) the probability of W is the same on every toss, probability theory provides\\na unique answer, known as the binomial distribution. This is the common “coin tossing”\\ndistribution. And so the probability of observing W waters and L lands, with a probability p\\nof water on each toss, is:\\nPr(W, L|p) = (W + L)!\\nW!L!\\npW(1 −p)L\\nRead the above as:\\nThe counts of “water” W and “land’ L are distributed binomially, with prob-\\nability p of “water” on each toss.\\nAnd the binomial distribution formula is built into R, so you can easily compute the likeli-\\nhood of the data—six W’s in nine tosses—under any value of p with:\\nR code\\n2.2\\ndbinom( 6 , size=9 , prob=0.5 )\\n[1] 0.1640625\\n'},\n",
       " {'index': 52,\n",
       "  'number': 34,\n",
       "  'content': '34\\n2. SMALL WORLDS AND LARGE WORLDS\\nThat number is the relative number of ways to get six water, holding p at 0.5 and N = W + L\\nat nine. So it does the job of counting relative number of paths through the garden. Change\\nthe 0.5 to any other value, to see how the value changes.\\nMuch later in the book, in Chapter 10, we’ll see that the binomial distribution is rather\\nspecial, because it represents the maximum entropy way to count binary events. “Maxi-\\nmum entropy” might sound like a bad thing. Isn’t entropy disorder? Doesn’t “maximum\\nentropy” mean the death of the universe? Actually it means that the distribution contains\\nno additional information other than: There are two events, and the probabilities of each\\nin each trial are p and 1 −p. Chapter 10 explains this in more detail, and the details can\\ncertainly wait.\\nOverthinking: Names and probability distributions. The “d” in dbinom stands for density. Func-\\ntions named in this way almost always have corresponding partners that begin with “r” for random\\nsamples and that begin with “p” for cumulative probabilities. See for example the help ?dbinom.\\nRethinking: A central role for likelihood. A great deal of ink has been spilled focusing on how\\nBayesian and non-Bayesian data analyses differ. Focusing on differences is useful, but sometimes\\nit distracts us from fundamental similarities. Notably, the most influential assumptions in both\\nBayesian and many non-Bayesian models are the distributions assigned to data, the likelihood func-\\ntions. The likelihoods influence inference for every piece of data, and as sample size increases, the\\nlikelihood matters more and more. This helps to explain why Bayesian and non-Bayesian inferences\\nare often so similar. If we had to explain Bayesian inference using only one aspect of it, we should\\ndescribe likelihood, not priors.\\n2.3.2.2. Unobserved variables. The distributions we assign to the observed variables typ-\\nically have their own variables. In the binomial above, there is p, the probability of sampling\\nwater. Since p is not observed, we usually call it a parameter. Even though we cannot\\nobserve p, we still have to define it.\\nIn future chapters, there will be more parameters in your models. In statistical modeling,\\nmany of the most common questions we ask about data are answered directly by parameters:\\n• What is the average difference between treatment groups?\\n• How strong is the association between a treatment and an outcome?\\n• Does the effect of the treatment depend upon a covariate?\\n• How much variation is there among groups?\\nYou’ll see how these questions become extra parameters inside the distribution function we\\nassign to the data.\\nFor every parameter you intend your Bayesian machine to consider, you must provide a\\ndistribution of prior plausibility, its prior. A Bayesian machine must have an initial plausi-\\nbility assignment for each possible value of the parameter, and these initial assignments do\\nuseful work. When you have a previous estimate to provide to the machine, that can become\\nthe prior, as in the steps in Figure 2.5. Back in Figure 2.5, the machine did its learning one\\npiece of data at a time. As a result, each estimate becomes the prior for the next step. But this\\ndoesn’t resolve the problem of providing a prior, because at the dawn of time, when N = 0,\\nthe machine still had an initial state of information for the parameter p: a flat line specifying\\nequal plausibility for every possible value.\\n'},\n",
       " {'index': 53,\n",
       "  'number': 35,\n",
       "  'content': '2.3. COMPONENTS OF THE MODEL\\n35\\nSo where do priors come from? They are both engineering assumptions, chosen to help\\nthe machine learn, and scientific assumptions, chosen to reflect what we know about a phe-\\nnomenon. The flat prior in Figure 2.5 is very common, but it is hardly ever the best prior.\\nLater chapters will focus on prior choice a lot more.\\nThere is a school of Bayesian inference that emphasizes choosing priors based upon the\\npersonal beliefs of the analyst.48 While this subjective Bayesian approach thrives in some\\nstatistics and philosophy and economics programs, it is rare in the sciences. Within Bayesian\\ndata analysis in the natural and social sciences, the prior is considered to be just part of\\nthe model. As such it should be chosen, evaluated, and revised just like all of the other\\ncomponents of the model. In practice, the subjectivist and the non-subjectivist will often\\nanalyze data in nearly the same way.\\nNone of this should be understood to mean that any statistical analysis is not inherently\\nsubjective, because of course it is—lots of little subjective decisions are involved in all parts\\nof science. It’s just that priors and Bayesian data analysis are no more inherently subjective\\nthan are likelihoods and the repeat sampling assumptions required for significance testing.49\\nAnyone who has visited a statistics help desk at a university has probably experienced this\\nsubjectivity—statisticians do not in general exactly agree on how to analyze anything but the\\nsimplest of problems. The fact that statistical inference uses mathematics does not imply that\\nthere is only one reasonable or useful way to conduct an analysis. Engineering uses math as\\nwell, but there are many ways to build a bridge.\\nBeyond all of the above, there’s no law mandating we use only one prior. If you don’t\\nhave a strong argument for any particular prior, then try different ones. Because the prior is\\nan assumption, it should be interrogated like other assumptions: by altering it and checking\\nhow sensitive inference is to the assumption. No one is required to swear an oath to the\\nassumptions of a model, and no set of assumptions deserves our obedience.\\nOverthinking: Prior as probability distribution. You could write the prior in the example here as:\\nPr(p) =\\n1\\n1 −0 = 1.\\nThe prior is a probability distribution for the parameter. In general, for a uniform prior from a to b, the\\nprobability of any point in the interval is 1/(b −a). If you’re bothered by the fact that the probability\\nof every value of p is 1, remember that every probability distribution must sum (integrate) to 1. The\\nexpression 1/(b −a) ensures that the area under the flat line from a to b is equal to 1. There will be\\nmore to say about this in Chapter 4.\\nRethinking: Datum or parameter? It is typical to conceive of data and parameters as completely\\ndifferent kinds of entities. Data are measured and known; parameters are unknown and must be\\nestimated from data. Usefully, in the Bayesian framework the distinction between a datum and a\\nparameter is not so fundamental. Sometimes we observe a variable, but sometimes we do not. In that\\ncase, the same distribution function applies, even though we didn’t observe the variable. As a result,\\nthe same assumption can look like a “likelihood” or a “prior,” depending upon context, without any\\nchange to the model. Much later in the book (Chapter 15), you’ll see how to exploit this deep identity\\nbetween certainty (data) and uncertainty (parameters) to incorporate measurement error and missing\\ndata into your modeling.\\n'},\n",
       " {'index': 54,\n",
       "  'number': 36,\n",
       "  'content': '36\\n2. SMALL WORLDS AND LARGE WORLDS\\nRethinking: Prior, prior pants on fire. Historically, some opponents of Bayesian inference objected\\nto the arbitrariness of priors. It’s true that priors are very flexible, being able to encode many different\\nstates of information. If the prior can be anything, isn’t it possible to get any answer you want? Indeed\\nit is. Regardless, after a couple hundred years of Bayesian calculation, it hasn’t turned out that people\\nuse priors to lie. If your goal is to lie with statistics, you’d be a fool to do it with priors, because such a\\nlie would be easily uncovered. Better to use the more opaque machinery of the likelihood. Or better\\nyet—don’t actually take this advice!—massage the data, drop some “outliers,” and otherwise engage\\nin motivated data transformation.\\nIt is true though that choice of the likelihood is much more conventionalized than choice of prior.\\nBut conventional choices are often poor ones, smuggling in influences that can be hard to discover.\\nIn this regard, both Bayesian and non-Bayesian models are equally harried, because both traditions\\ndepend heavily upon likelihood functions and conventionalized model forms. And the fact that the\\nnon-Bayesian procedure doesn’t have to make an assumption about the prior is of little comfort. This\\nis because non-Bayesian procedures need to make choices that Bayesian ones do not, such as choice of\\nestimator or likelihood penalty. Often, such choices can be shown to be equivalent to some Bayesian\\nchoice of prior or rather choice of loss function. (You’ll meet loss functions later in Chapter 3.)\\n2.3.3. A model is born. With all the above work, we can now summarize our model. The\\nobserved variables W and L are given relative counts through the binomial distribution. So\\nwe can write, as a shortcut:\\nW ∼Binomial(N, p)\\nwhere N = W + L. The above is just a convention for communicating the assumption that\\nthe relative counts of ways to realize W in N trials with probability p on each trial comes from\\nthe binomial distribution. And the unobserved parameter p similarly gets:\\np ∼Uniform(0, 1)\\nThis means that p has a uniform—flat—prior over its entire possible range, from zero to one.\\nAs I mentioned earlier, this is obviously not the best we could do, since we know the Earth\\nhas more water than land, even if we do not know the exact proportion yet.\\nNext, let’s see how to use these assumptions to generate inference.\\n2.4. Making the model go\\nOnce you have named all the variables and chosen definitions for each, a Bayesian model\\ncan update all of the prior distributions to their purely logical consequences: the posterior\\ndistribution. For every unique combination of data, likelihood, parameters, and prior,\\nthere is a unique posterior distribution. This distribution contains the relative plausibility\\nof different parameter values, conditional on the data and model. The posterior distribution\\ntakes the form of the probability of the parameters, conditional on the data. In this case, it\\nwould be Pr(p|W, L), the probability of each possible value of p, conditional on the specific\\nW and L that we observed.\\n2.4.1. Bayes’theorem. The mathematical definition of the posterior distribution arises from\\nBayes’ theorem. This is the theorem that gives Bayesian data analysis its name. But the\\ntheorem itself is a trivial implication of probability theory. Here’s a quick derivation of it,\\nin the context of the globe tossing example. Really this will just be a re-expression of the\\ngarden of forking data derivation from earlier in the chapter. What makes it look different\\n'},\n",
       " {'index': 55,\n",
       "  'number': 37,\n",
       "  'content': '2.4. MAKING THE MODEL GO\\n37\\nis that it will use the rules of probability theory to coax out the updating rule. But it is still\\njust counting.\\nThe joint probability of the data W and L and any particular value of p is:\\nPr(W, L, p) = Pr(W, L|p) Pr(p)\\nThis just says that the probability of W, L and p is the product of Pr(W, L|p) and the prior\\nprobability Pr(p). This is like saying that the probability of rain and cold on the same day is\\nequal to the probability of rain, when it’s cold, times the probability that it’s cold. This much\\nis just definition. But it’s just as true that:\\nPr(W, L, p) = Pr(p|W, L) Pr(W, L)\\nAll I’ve done is reverse which probability is conditional, on the right-hand side. It is still a\\ntrue definition. It’s like saying that the probability of rain and cold on the same day is equal\\nto the probability that it’s cold, when it’s raining, times the probability of rain. Compare this\\nstatement to the one in the previous paragraph.\\nNow since both right-hand sides above are equal to the same thing, Pr(W, L, p), they are\\nalso equal to one another:\\nPr(W, L|p) Pr(p) = Pr(p|W, L) Pr(W, L)\\nSo we can now solve for the thing that we want, Pr(p|W, L):\\nPr(p|W, L) = Pr(W, L|p) Pr(p)\\nPr(W, L)\\nAnd this is Bayes’ theorem. It says that the probability of any particular value of p, consid-\\nering the data, is equal to the product of the relative plausibility of the data, conditional on\\np, and the prior plausibility of p, divided by this thing Pr(W, L), which I’ll call the average\\nprobability of the data. In word form:\\nPosterior = Probability of the data × Prior\\nAverage probability of the data\\nThe average probability of the data, Pr(W, L), can be confusing. It is commonly called\\nthe “evidence” or the “average likelihood,” neither of which is a transparent name. The prob-\\nability Pr(W, L) is literally the average probability of the data. Averaged over what? Averaged\\nover the prior. It’s job is just to standardize the posterior, to ensure it sums (integrates) to\\none. In mathematical form:\\nPr(W, L) = E\\n\\x00Pr(W, L|p)\\n\\x01\\n=\\nZ\\nPr(W, L|p) Pr(p)dp\\nThe operator E means to take an expectation. Such averages are commonly called marginals\\nin mathematical statistics, and so you may also see this same probability called a marginal\\nlikelihood. And the integral above just defines the proper way to compute the average over a\\ncontinuous distribution of values, like the infinite possible values of p.\\nThe key lesson is that the posterior is proportional to the product of the prior and the\\nprobability of the data. Why? Because for each specific value of p, the number of paths\\nthrough the garden of forking data is the product of the prior number of paths and the new\\nnumber of paths. Multiplication is just compressed counting. The average probability on\\nthe bottom just standardizes the counts so they sum to one. So while Bayes’ theorem looks\\ncomplicated, because the relationship with counting paths is obscured, it just expresses the\\ncounting that logic demands.\\n'},\n",
       " {'index': 56,\n",
       "  'number': 38,\n",
       "  'content': '38\\n2. SMALL WORLDS AND LARGE WORLDS\\nposterior\\n0\\n0.5\\n1\\nlikelihood\\n0\\n0.5\\n1\\nprior\\n0\\n0.5\\n1\\n⇥\\n/\\nposterior\\n0\\n0.5\\n1\\nprior\\n0\\n0.5\\n1\\n⇥\\n/\\n⇥\\n/\\n1\\n⇥\\n/\\nlikelihood\\n0\\n0.5\\n1\\nprior\\n0\\n0.5\\n1\\n⇥\\n1\\nlikelihood\\n0\\n0.5\\n1\\n/\\n1\\nposterior\\n0\\n0.5\\n1\\nFigure 2.6. The posterior distribution as a product of the prior distribu-\\ntion and likelihood. Top: A flat prior constructs a posterior that is simply\\nproportional to the likelihood. Middle: A step prior, assigning zero proba-\\nbility to all values less than 0.5, results in a truncated posterior. Bottom: A\\npeaked prior that shifts and skews the posterior, relative to the likelihood.\\nFigure 2.6 illustrates the multiplicative interaction of a prior and a probability of data.\\nOn each row, a prior on the left is multiplied by the probability of data in the middle to\\nproduce a posterior on the right. The probability of data in each case is the same. The priors\\nhowever vary. As a result, the posterior distributions vary.\\nRethinking: Bayesian data analysis isn’t about Bayes’ theorem. A common notion about Bayesian\\ndata analysis, and Bayesian inference more generally, is that it is distinguished by the use of Bayes’\\ntheorem. This is a mistake. Inference under any probability concept will eventually make use of Bayes’\\ntheorem. Common introductory examples of “Bayesian” analysis using HIV and DNA testing are not\\n'},\n",
       " {'index': 57,\n",
       "  'number': 39,\n",
       "  'content': '2.4. MAKING THE MODEL GO\\n39\\nuniquely Bayesian. Since all of the elements of the calculation are frequencies of observations, a non-\\nBayesian analysis would do exactly the same thing. Instead, Bayesian approaches get to use Bayes’\\ntheorem more generally, to quantify uncertainty about theoretical entities that cannot be observed,\\nlike parameters and models. Powerful inferences can be produced under both Bayesian and non-\\nBayesian probability concepts, but different justifications and sacrifices are necessary.\\n2.4.2. Motors. Recall that your Bayesian model is a machine, a figurative golem. It has built-\\nin definitions for the likelihood, the parameters, and the prior. And then at its heart lies a\\nmotor that processes data, producing a posterior distribution. The action of this motor can\\nbe thought of as conditioning the prior on the data. As explained in the previous section, this\\nconditioning is governed by the rules of probability theory, which defines a uniquely logical\\nposterior for set of assumptions and observations.\\nHowever, knowing the mathematical rule is often of little help, because many of the in-\\nteresting models in contemporary science cannot be conditioned formally, no matter your\\nskill in mathematics. And while some broadly useful models like linear regression can be\\nconditioned formally, this is only possible if you constrain your choice of prior to special\\nforms that are easy to do mathematics with. We’d like to avoid forced modeling choices of\\nthis kind, instead favoring conditioning engines that can accommodate whichever prior is\\nmost useful for inference.\\nWhat this means is that various numerical techniques are needed to approximate the\\nmathematics that follows from the definition of Bayes’ theorem. In this book, you’ll meet\\nthree different conditioning engines, numerical techniques for computing posterior distri-\\nbutions:\\n(1) Grid approximation\\n(2) Quadratic approximation\\n(3) Markov chain Monte Carlo (MCMC)\\nThere are many other engines, and new ones are being invented all the time. But the three\\nyou’ll get to know here are common and widely useful. In addition, as you learn them, you’ll\\nalso learn principles that will help you understand other techniques.\\nRethinking: How you fit the model is part of the model. Earlier in this chapter, I implicitly defined\\nthe model as a composite of a prior and a likelihood. That definition is typical. But in practical terms,\\nwe should also consider how the model is fit to data as part of the model. In very simple problems,\\nlike the globe tossing example that consumes this chapter, calculation of the posterior density is trivial\\nand foolproof. In even moderately complex problems, however, the details of fitting the model to\\ndata force us to recognize that our numerical technique influences our inferences. This is because\\ndifferent mistakes and compromises arise under different techniques. The same model fit to the same\\ndata using different techniques may produce different answers. When something goes wrong, every\\npiece of the machine may be suspect. And so our golems carry with them their updating engines, as\\nmuch slaves to their engineering as they are to the priors and likelihoods we program into them.\\n2.4.3. Grid approximation. One of the simplest conditioning techniques is grid approxi-\\nmation. While most parameters are continuous, capable of taking on an infinite number of\\nvalues, it turns out that we can achieve an excellent approximation of the continuous pos-\\nterior distribution by considering only a finite grid of parameter values. At any particular\\n'},\n",
       " {'index': 58,\n",
       "  'number': 40,\n",
       "  'content': '40\\n2. SMALL WORLDS AND LARGE WORLDS\\nvalue of a parameter, p′, it’s a simple matter to compute the posterior probability: just mul-\\ntiply the prior probability of p′ by the likelihood at p′. Repeating this procedure for each\\nvalue in the grid generates an approximate picture of the exact posterior distribution. This\\nprocedure is called grid approximation. In this section, you’ll see how to perform a grid\\napproximation, using simple bits of R code.\\nGrid approximation will mainly be useful as a pedagogical tool, as learning it forces the\\nuser to really understand the nature of Bayesian updating. But in most of your real modeling,\\ngrid approximation isn’t practical. The reason is that it scales very poorly, as the number of\\nparameters increases. So in later chapters, grid approximation will fade away, to be replaced\\nby other, more efficient techniques. Still, the conceptual value of this exercise will carry\\nforward, as you graduate to other techniques.\\nIn the context of the globe tossing problem, grid approximation works extremely well.\\nSo let’s build a grid approximation for the model we’ve constructed so far. Here is the recipe:\\n(1) Define the grid. This means you decide how many points to use in estimating the\\nposterior, and then you make a list of the parameter values on the grid.\\n(2) Compute the value of the prior at each parameter value on the grid.\\n(3) Compute the likelihood at each parameter value.\\n(4) Compute the unstandardized posterior at each parameter value, by multiplying the\\nprior by the likelihood.\\n(5) Finally, standardize the posterior, by dividing each value by the sum of all values.\\nIn the globe tossing context, here’s the code to complete all five of these steps:\\nR code\\n2.3\\n# define grid\\np_grid <- seq( from=0 , to=1 , length.out=20 )\\n# define prior\\nprior <- rep( 1 , 20 )\\n# compute likelihood at each value in grid\\nlikelihood <- dbinom( 6 , size=9 , prob=p_grid )\\n# compute product of likelihood and prior\\nunstd.posterior <- likelihood * prior\\n# standardize the posterior, so it sums to 1\\nposterior <- unstd.posterior / sum(unstd.posterior)\\nThe above code makes a grid of only 20 points. To display the posterior distribution now:\\nR code\\n2.4\\nplot( p_grid , posterior , type=\"b\" ,\\nxlab=\"probability of water\" , ylab=\"posterior probability\" )\\nmtext( \"20 points\" )\\nYou’ll get the right-hand plot in Figure 2.7. Try sparser grids (5 points) and denser grids\\n(100 or 1000 points). The correct density for your grid is determined by how accurate you\\nwant your approximation to be. More points means more precision. In this simple example,\\nyou can go crazy and use 100,000 points, but there won’t be much change in inference after\\nthe first 100.\\n'},\n",
       " {'index': 59,\n",
       "  'number': 41,\n",
       "  'content': '2.4. MAKING THE MODEL GO\\n41\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nprobability of water\\nposterior probability\\n5 points\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.00\\n0.04\\n0.08\\n0.12\\nprobability of water\\nposterior probability\\n20 points\\nFigure 2.7. Computing posterior distribution by grid approximation. In\\neach plot, the posterior distribution for the globe toss data and model is\\napproximated with a finite number of evenly spaced points. With only 5\\npoints (left), the approximation is terrible. But with 20 points (right), the\\napproximation is already quite good. Compare to the analytically solved,\\nexact posterior distribution in Figure 2.5 (page 30).\\nNow to replicate the different priors in Figure 2.5, try these lines of code—one at a\\ntime—for the prior grid:\\nR code\\n2.5\\nprior <- ifelse( p_grid < 0.5 , 0 , 1 )\\nprior <- exp( -5*abs( p_grid - 0.5 ) )\\nThe rest of the code remains the same.\\nOverthinking: Vectorization. One of R’s useful features is that it makes working with lists of numbers\\nalmost as easy as working with single values. So even though both lines of code above say nothing\\nabout how dense your grid is, whatever length you chose for the vector p_grid will determine the\\nlength of the vector prior. In R jargon, the calculations above are vectorized, because they work on\\nlists of values, vectors. In a vectorized calculation, the calculation is performed on each element of\\nthe input vector—p_grid in this case—and the resulting output therefore has the same length. In\\nother computing environments, the same calculation would require a loop. R can also use loops, but\\nvectorized calculations are typically faster. They can however be much harder to read, when you are\\nstarting out with R. Be patient, and you’ll soon grow accustomed to vectorized calculations.\\n2.4.4. Quadratic approximation. We’ll stick with the grid approximation to the globe toss-\\ning posterior, for the rest of this chapter and the next. But before long you’ll have to resort to\\nanother approximation, one that makes stronger assumptions. The reason is that the num-\\nber of unique values to consider in the grid grows rapidly as the number of parameters in\\nyour model increases. For the single-parameter globe tossing model, it’s no problem to com-\\npute a grid of 100 or 1000 values. But for two parameters approximated by 100 values each,\\nthat’s already 1002 = 10,000 values to compute. For 10 parameters, the grid becomes many\\n'},\n",
       " {'index': 60,\n",
       "  'number': 42,\n",
       "  'content': '42\\n2. SMALL WORLDS AND LARGE WORLDS\\nbillions of values. These days, it’s routine to have models with hundreds or thousands of pa-\\nrameters. The grid approximation strategy scales very poorly with model complexity, so it\\nwon’t get us very far.\\nA useful approach is quadratic approximation. Under quite general conditions, the\\nregion near the peak of the posterior distribution will be nearly Gaussian—or “normal”—in\\nshape. This means the posterior distribution can be usefully approximated by a Gaussian\\ndistribution. A Gaussian distribution is convenient, because it can be completely described\\nby only two numbers: the location of its center (mean) and its spread (variance).\\nA Gaussian approximation is called “quadratic approximation” because the logarithm of\\na Gaussian distribution forms a parabola. And a parabola is a quadratic function. So this\\napproximation essentially represents any log-posterior with a parabola.\\nWe’ll use quadratic approximation for much of the first half of this book. For many of the\\nmost common procedures in applied statistics—linear regression, for example—the approx-\\nimation works very well. Often, it is even exactly correct, not actually an approximation at\\nall. Computationally, quadratic approximation is very inexpensive, at least compared to grid\\napproximation and MCMC (discussed next). The procedure, which R will happily conduct\\nat your command, contains two steps.\\n(1) Find the posterior mode. This is usually accomplished by some optimization algo-\\nrithm, a procedure that virtually “climbs” the posterior distribution, as if it were a\\nmountain. The golem doesn’t know where the peak is, but it does know the slope\\nunder its feet. There are many well-developed optimization procedures, most of\\nthem more clever than simple hill climbing. But all of them try to find peaks.\\n(2) Once you find the peak of the posterior, you must estimate the curvature near the\\npeak. This curvature is sufficient to compute a quadratic approximation of the\\nentire posterior distribution. In some cases, these calculations can be done analyt-\\nically, but usually your computer uses some numerical technique instead.\\nTo compute the quadratic approximation for the globe tossing data, we’ll use a tool in\\nthe rethinking package: quap. We’re going to be using quap a lot in the first half of this\\nbook. It’s a flexible model fitting tool that will allow us to specify a large number of different\\n“regression” models. So it’ll be worth trying it out right now. You’ll get a more thorough\\nunderstanding of it later.\\nTo compute the quadratic approximation to the globe tossing data:\\nR code\\n2.6\\nlibrary(rethinking)\\nglobe.qa <- quap(\\nalist(\\nW ~ dbinom( W+L ,p) ,\\n# binomial likelihood\\np ~ dunif(0,1)\\n# uniform prior\\n) ,\\ndata=list(W=6,L=3) )\\n# display summary of quadratic approximation\\nprecis( globe.qa )\\nTo use quap, you provide a formula, a list of data. The formula defines the probability of the\\ndata and the prior. I’ll say much more about these formulas in Chapter 4. Now let’s see the\\noutput:\\nMean StdDev 5.5% 94.5%\\n'},\n",
       " {'index': 61,\n",
       "  'number': 43,\n",
       "  'content': '2.4. MAKING THE MODEL GO\\n43\\n0.0\\n0.5\\n1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nproportion water\\nDensity\\nn = 9\\n0.0\\n0.5\\n1.0\\n0\\n1\\n2\\n3\\nproportion water\\nn = 18\\n0.0\\n0.5\\n1.0\\n0\\n1\\n2\\n3\\n4\\n5\\nproportion water\\nn = 36\\nFigure 2.8. Accuracy of the quadratic approximation. In each plot, the\\nexact posterior distribution is plotted in blue, and the quadratic approxima-\\ntion is plotted as the black curve. Left: The globe tossing data with n = 9\\ntosses and w = 6 waters. Middle: Double the amount of data, with the\\nsame fraction of water, n = 18 and w = 12. Right: Four times as much\\ndata, n = 36 and w = 24.\\np 0.67\\n0.16 0.42\\n0.92\\nThe function precis presents a brief summary of the quadratic approximation. In this case,\\nit shows the posterior mean value of p = 0.67, which it calls the “Mean.” The curvature is\\nlabeled “StdDev” This stands for standard deviation. This value is the standard deviation of\\nthe posterior distribution, while the mean value is its peak. Finally, the last two values in the\\nprecis output show the 89% percentile interval, which you’ll learn more about in the next\\nchapter. You can read this kind of approximation like: Assuming the posterior is Gaussian, it\\nis maximized at 0.67, and its standard deviation is 0.16.\\nSince we already know the posterior, let’s compare to see how good the approximation is.\\nI’ll use the analytical approach here, which uses dbeta. I won’t explain this calculation, but\\nit ensures that we have exactly the right answer. You can find an explanation and derivation\\nof it in just about any mathematical textbook on Bayesian inference.\\nR code\\n2.7\\n# analytical calculation\\nW <- 6\\nL <- 3\\ncurve( dbeta( x , W+1 , L+1 ) , from=0 , to=1 )\\n# quadratic approximation\\ncurve( dnorm( x , 0.67 , 0.16 ) , lty=2 , add=TRUE )\\nYou can see this plot (with a little extra formatting) on the left in Figure 2.8. The blue curve is\\nthe analytical posterior and the black curve is the quadratic approximation. The black curve\\ndoes alright on its left side, but looks pretty bad on its right side. It even assigns positive\\nprobability to p = 1, which we know is impossible, since we saw at least one land sample.\\nAs the amount of data increases, however, the quadratic approximation gets better. In the\\nmiddle of Figure 2.8, the sample size is doubled to n = 18 tosses, but with the same fraction\\n'},\n",
       " {'index': 62,\n",
       "  'number': 44,\n",
       "  'content': '44\\n2. SMALL WORLDS AND LARGE WORLDS\\nof water, so that the mode of the posterior is in the same place. The quadratic approximation\\nlooks better now, although still not great. At quadruple the data, on the right side of the\\nfigure, the two curves are nearly the same now.\\nThis phenomenon, where the quadratic approximation improves with the amount of\\ndata, is very common. It’s one of the reasons that so many classical statistical procedures\\nare nervous about small samples: Those procedures use quadratic (or other) approximations\\nthat are only known to be safe with infinite data. Often, these approximations are useful\\nwith less than infinite data, obviously. But the rate of improvement as sample size increases\\nvaries greatly depending upon the details. In some models, the quadratic approximation can\\nremain terrible even with thousands of samples.\\nUsing the quadratic approximation in a Bayesian context brings with it all the same con-\\ncerns. But you can always lean on some algorithm other than quadratic approximation, if\\nyou have doubts. Indeed, grid approximation works very well with small samples, because\\nin such cases the model must be simple and the computations will be quite fast. You can also\\nuse MCMC, which is introduced next.\\nRethinking: Maximum likelihood estimation. The quadratic approximation, either with a uniform\\nprior or with a lot of data, is often equivalent to a maximum likelihood estimate (MLE) and its\\nstandard error. The MLE is a very common non-Bayesian parameter estimate. This correspon-\\ndence between a Bayesian approximation and a common non-Bayesian estimator is both a blessing\\nand a curse. It is a blessing, because it allows us to re-interpret a wide range of published non-Bayesian\\nmodel fits in Bayesian terms. It is a curse, because maximum likelihood estimates have some curious\\ndrawbacks, and the quadratic approximation can share them. We’ll explore these drawbacks in later\\nchapters, and they are one of the reasons we’ll turn to Markov chain Monte Carlo for the second half\\nof the book.\\nOverthinking: The Hessians are coming. Sometimes it helps to know more about how the quadratic\\napproximation is computed. In particular, the approximation sometimes fails. When it does, chances\\nare you’ll get a confusing error message that says something about the “Hessian.” Students of world\\nhistory may know that the Hessians were German mercenaries hired by the British in the eighteenth\\ncentury to do various things, including fight against the American revolutionary George Washington.\\nThese mercenaries are named after a region of what is now central Germany, Hesse.\\nThe Hessian that concerns us here has little to do with mercenaries. It is named after mathe-\\nmatician Ludwig Otto Hesse (1811–1874). A Hessian is a square matrix of second derivatives. It is\\nused for many purposes in mathematics, but in the quadratic approximation it is second derivatives\\nof the log of posterior probability with respect to the parameters. It turns out that these derivatives\\nare sufficient to describe a Gaussian distribution, because the logarithm of a Gaussian distribution\\nis just a parabola. Parabolas have no derivatives beyond the second, so once we know the center of\\nthe parabola (the posterior mode) and its second derivative, we know everything about it. And in-\\ndeed the second derivative (with respect to the outcome) of the logarithm of a Gaussian distribution\\nis proportional to its inverse squared standard deviation (its “precision”: page 76). So knowing the\\nstandard deviation tells us everything about its shape.\\nThe standard deviation is typically computed from the Hessian, so computing the Hessian is\\nnearly always a necessary step. But sometimes the computation goes wrong, and your golem will\\nchoke while trying to compute the Hessian. In those cases, you have several options. Not all hope is\\nlost. But for now it’s enough to recognize the term and associate it with an attempt to find the standard\\ndeviation for a quadratic approximation.\\n'},\n",
       " {'index': 63,\n",
       "  'number': 45,\n",
       "  'content': '2.4. MAKING THE MODEL GO\\n45\\n2.4.5. Markov chain Monte Carlo. There are lots of important model types, like multilevel\\n(mixed-effects) models, for which neither grid approximation nor quadratic approximation\\nis always satisfactory. Such models may have hundreds or thousands or tens-of-thousands\\nof parameters. Grid approximation routinely fails here, because it just takes too long—the\\nSun will go dark before your computer finishes the grid. Special forms of quadratic approx-\\nimation might work, if everything is just right. But commonly, something is not just right.\\nFurthermore, multilevel models do not always allow us to write down a single, unified func-\\ntion for the posterior distribution. This means that the function to maximize (when finding\\nthe MAP) is not known, but must be computed in pieces.\\nAs a result, various counterintuitive model fitting techniques have arisen. The most pop-\\nular of these is Markov chain Monte Carlo (MCMC), which is a family of conditioning\\nengines capable of handling highly complex models. It is fair to say that MCMC is largely re-\\nsponsible for the insurgence of Bayesian data analysis that began in the 1990s. While MCMC\\nis older than the 1990s, affordable computer power is not, so we must also thank the en-\\ngineers. Much later in the book (Chapter 9), you’ll meet simple and precise examples of\\nMCMC model fitting, aimed at helping you understand the technique.\\nThe conceptual challenge with MCMC lies in its highly non-obvious strategy. Instead of\\nattempting to compute or approximate the posterior distribution directly, MCMC techniques\\nmerely draw samples from the posterior. You end up with a collection of parameter values,\\nand the frequencies of these values correspond to the posterior plausibilities. You can then\\nbuild a picture of the posterior from the histogram of these samples.\\nWe nearly always work directly with these samples, rather than first constructing some\\nmathematical estimate from them. And the samples are in many ways more convenient than\\nhaving the posterior, because they are easier to think with. And so that’s where we turn in\\nthe next chapter, to thinking with samples.\\nOverthinking: Monte Carlo globe tossing. If you are eager to see MCMC in action, a working\\nMarkov chain for the globe tossing model does not require much code. The following R code is\\nsufficient for a MCMC estimate of the posterior:\\nR code\\n2.8\\nn_samples <- 1000\\np <- rep( NA , n_samples )\\np[1] <- 0.5\\nW <- 6\\nL <- 3\\nfor ( i in 2:n_samples ) {\\np_new <- rnorm( 1 , p[i-1] , 0.1 )\\nif ( p_new < 0 ) p_new <- abs( p_new )\\nif ( p_new > 1 ) p_new <- 2 - p_new\\nq0 <- dbinom( W , W+L , p[i-1] )\\nq1 <- dbinom( W , W+L , p_new )\\np[i] <- ifelse( runif(1) < q1/q0 , p_new , p[i-1] )\\n}\\nThe values in p are samples from the posterior distribution. To compare to the analytical posterior:\\nR code\\n2.9\\ndens( p , xlim=c(0,1) )\\ncurve( dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE )\\nIt’s weird. But it works. I’ll explain this algorithm, the Metropolis algorithm, in Chapter 9.\\n'},\n",
       " {'index': 64,\n",
       "  'number': 46,\n",
       "  'content': '46\\n2. SMALL WORLDS AND LARGE WORLDS\\n2.5. Summary\\nThis chapter introduced the conceptual mechanics of Bayesian data analysis. The target\\nof inference in Bayesian inference is a posterior probability distribution. Posterior probabil-\\nities state the relative numbers of ways each conjectured cause of the data could have pro-\\nduced the data. These relative numbers indicate plausibilities of the different conjectures.\\nThese plausibilities are updated in light of observations through Bayesian updating.\\nMore mechanically, a Bayesian model is a composite of variables and distributional def-\\ninitions for these variables. The probability of the data, often called the likelihood, provides\\nthe plausibility of an observation (data), given a fixed value for the parameters. The prior\\nprovides the plausibility of each possible value of the parameters, before accounting for the\\ndata. The rules of probability tell us that the logical way to compute the plausibilities, after\\naccounting for the data, is to use Bayes’ theorem. This results in the posterior distribution.\\nIn practice, Bayesian models are fit to data using numerical techniques, like grid approx-\\nimation, quadratic approximation, and Markov chain Monte Carlo. Each method imposes\\ndifferent trade-offs.\\n2.6. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n2E1. Which of the expressions below correspond to the statement: the probability of rain on Monday?\\n(1) Pr(rain)\\n(2) Pr(rain|Monday)\\n(3) Pr(Monday|rain)\\n(4) Pr(rain, Monday)/ Pr(Monday)\\n2E2. Which of the following statements corresponds to the expression: Pr(Monday|rain)?\\n(1) The probability of rain on Monday.\\n(2) The probability of rain, given that it is Monday.\\n(3) The probability that it is Monday, given that it is raining.\\n(4) The probability that it is Monday and that it is raining.\\n2E3. Which of the expressions below correspond to the statement: the probability that it is Monday,\\ngiven that it is raining?\\n(1) Pr(Monday|rain)\\n(2) Pr(rain|Monday)\\n(3) Pr(rain|Monday) Pr(Monday)\\n(4) Pr(rain|Monday) Pr(Monday)/ Pr(rain)\\n(5) Pr(Monday|rain) Pr(rain)/ Pr(Monday)\\n2E4. The Bayesian statistician Bruno de Finetti (1906–1985) began his 1973 book on probability the-\\nory with the declaration: “PROBABILITY DOES NOT EXIST.” The capitals appeared in the original,\\nso I imagine de Finetti wanted us to shout this statement. What he meant is that probability is a de-\\nvice for describing uncertainty from the perspective of an observer with limited knowledge; it has no\\nobjective reality. Discuss the globe tossing example from the chapter, in light of this statement. What\\ndoes it mean to say “the probability of water is 0.7”?\\n'},\n",
       " {'index': 65,\n",
       "  'number': 47,\n",
       "  'content': '2.6. PRACTICE\\n47\\n2M1. Recall the globe tossing model from the chapter. Compute and plot the grid approximate\\nposterior distribution for each of the following sets of observations. In each case, assume a uniform\\nprior for p.\\n(1) W, W, W\\n(2) W, W, W, L\\n(3) L, W, W, L, W, W, W\\n2M2. Now assume a prior for p that is equal to zero when p < 0.5 and is a positive constant when\\np ≥0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of\\nobservations in the problem just above.\\n2M3. Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered\\nin water. The Mars globe is 100% land. Further suppose that one of these globes—you don’t know\\nwhich—was tossed in the air and produced a “land” observation. Assume that each globe was equally\\nlikely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on\\nseeing “land” (Pr(Earth|land)), is 0.23.\\n2M4. Suppose you have a deck with only three cards. Each card has two sides, and each side is either\\nblack or white. One card has two black sides. The second card has one black and one white side. The\\nthird card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone\\nreaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up,\\nbut you don’t know the color of the side facing down. Show that the probability that the other side is\\nalso black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This\\nmeans counting up the ways that each card could produce the observed data (a black side facing up\\non the table).\\n2M5. Now suppose there are four cards: B/B, B/W, W/W, and another B/B. Again suppose a card is\\ndrawn from the bag and a black side appears face up. Again calculate the probability that the other\\nside is black.\\n2M6. Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white\\nsides. As a result, it’s less likely that a card with black sides is pulled from the bag. So again assume\\nthere are three cards: B/B, B/W, and W/W. After experimenting a number of times, you conclude that\\nfor every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to\\npull the W/W card. Again suppose that a card is pulled and a black side appears face up. Show that\\nthe probability the other side is black is now 0.5. Use the counting method, as before.\\n2M7. Assume again the original card problem, with a single card showing a black side face up. Before\\nlooking at the other side, we draw another card from the bag and lay it face up on the table. The face\\nthat is shown on the new card is white. Show that the probability that the first card, the one showing\\na black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat\\nthis like the sequence of globe tosses, counting all the ways to see each observation, for each possible\\nfirst card.\\n2H1. Suppose there are two species of panda bear. Both are equally common in the wild and live\\nin the same places. They look exactly alike and eat the same food, and there is yet no genetic assay\\ncapable of telling them apart. They differ however in their family sizes. Species A gives birth to twins\\n10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise\\nbirthing singleton infants. Assume these numbers are known with certainty, from many years of field\\nresearch.\\nNow suppose you are managing a captive panda breeding program. You have a new female panda\\nof unknown species, and she has just given birth to twins. What is the probability that her next birth\\nwill also be twins?\\n'},\n",
       " {'index': 66,\n",
       "  'number': 48,\n",
       "  'content': '48\\n2. SMALL WORLDS AND LARGE WORLDS\\n2H2. Recall all the facts from the problem above. Now compute the probability that the panda we\\nhave is from species A, assuming we have observed only the first birth and that it was twins.\\n2H3. Continuing on from the previous problem, suppose the same panda mother has a second birth\\nand that it is not twins, but a singleton infant. Compute the posterior probability that this panda is\\nspecies A.\\n2H4. A common boast of Bayesian statisticians is that Bayesian inference makes it easy to use all of\\nthe data, even if the data are of different types.\\nSo suppose now that a veterinarian comes along who has a new genetic test that she claims can\\nidentify the species of our mother panda. But the test, like all tests, is imperfect. This is the informa-\\ntion you have about the test:\\n• The probability it correctly identifies a species A panda is 0.8.\\n• The probability it correctly identifies a species B panda is 0.65.\\nThe vet administers the test to your panda and tells you that the test is positive for species A. First\\nignore your previous information from the births and compute the posterior probability that your\\npanda is species A. Then redo your calculation, now using the birth data as well.\\n'},\n",
       " {'index': 67,\n",
       "  'number': 49,\n",
       "  'content': '3 Sampling the Imaginary\\nLots of books on Bayesian statistics introduce posterior inference by using a medical test-\\ning scenario. To repeat the structure of common examples, suppose there is a blood test that\\ncorrectly detects vampirism 95% of the time. In more precise and mathematical notation,\\nPr(positive test result|vampire) = 0.95. It’s a very accurate test, nearly always catching real\\nvampires. It also make mistakes, though, in the form of false positives. One percent of the\\ntime, it incorrectly diagnoses normal people as vampires, Pr(positive test result|mortal) =\\n0.01. The final bit of information we are told is that vampires are rather rare, being only 0.1%\\nof the population, implying Pr(vampire) = 0.001. Suppose now that someone tests positive\\nfor vampirism. What’s the probability that he or she is a bloodsucking immortal?\\nThe correct approach is just to use Bayes’ theorem to invert the probability, to compute\\nPr(vampire|positive). The calculation can be presented as:\\nPr(vampire|positive) = Pr(positive|vampire) Pr(vampire)\\nPr(positive)\\nwhere Pr(positive) is the average probability of a positive test result, that is,\\nPr(positive) = Pr(positive|vampire) Pr(vampire)\\n+ Pr(positive|mortal)\\n\\x001 −Pr(vampire)\\n\\x01\\nPerforming the calculation in R:\\nR code\\n3.1\\nPr_Positive_Vampire <- 0.95\\nPr_Positive_Mortal <- 0.01\\nPr_Vampire <- 0.001\\nPr_Positive <- Pr_Positive_Vampire * Pr_Vampire +\\nPr_Positive_Mortal * ( 1 - Pr_Vampire )\\n( Pr_Vampire_Positive <- Pr_Positive_Vampire*Pr_Vampire / Pr_Positive )\\n[1] 0.08683729\\nThat corresponds to an 8.7% chance that the suspect is actually a vampire.\\nMost people find this result counterintuitive. And it’s a very important result, because\\nit mimics the structure of many realistic testing contexts, such as HIV and DNA testing,\\ncriminal profiling, and even statistical significance testing (see the Rethinking box at the end\\nof this section). Whenever the condition of interest is very rare, having a test that finds all\\nthe true cases is still no guarantee that a positive result carries much information at all. The\\nreason is that most positive results are false positives, even when all the true positives are\\ndetected correctly.\\n49\\n'},\n",
       " {'index': 68,\n",
       "  'number': 50,\n",
       "  'content': '50\\n3. SAMPLING THE IMAGINARY\\nBut I don’t like these examples, for two reasons. First, there’s nothing uniquely “Bayesian”\\nabout them. Remember: Bayesian inference is distinguished by a broad view of probability,\\nnot by the use of Bayes’ theorem. Since all of the probabilities I provided above reference\\nfrequencies of events, rather than theoretical parameters, all major statistical philosophies\\nwould agree to use Bayes’ theorem in this case. Second, and more important to our work\\nin this chapter, these examples make Bayesian inference seem much harder than it has to\\nbe. Few people find it easy to remember which number goes where, probably because they\\nnever grasp the logic of the procedure. It’s just a formula that descends from the sky. If you\\nare confused, it is only because you are trying to understand.\\nThere is a way to present the same problem that does make it more intuitive, however.\\nSuppose that instead of reporting probabilities, as before, I tell you the following:\\n(1) In a population of 100,000 people, 100 of them are vampires.\\n(2) Of the 100 who are vampires, 95 of them will test positive for vampirism.\\n(3) Of the 99,900 mortals, 999 of them will test positive for vampirism.\\nNow tell me, if we test all 100,000 people, what proportion of those who test positive for\\nvampirism actually are vampires? Many people, although certainly not all people, find this\\npresentation a lot easier.50 Now we can just count up the number of people who test positive:\\n95 + 999 = 1094. Out of these 1094 positive tests, 95 of them are real vampires, so that\\nimplies:\\nPr(vampire|positive) =\\n95\\n1094 ≈0.087\\nIt’s exactly the same answer as before, but without a seemingly arbitrary rule.\\nThe second presentation of the problem, using counts rather than probabilities, is often\\ncalled the frequency format or natural frequencies. Why a frequency format helps people in-\\ntuit the correct approach remains contentious. Some people think that human psychology\\nnaturally works better when it receives information in the form a person in a natural envi-\\nronment would receive it. In the real world, we encounter counts only. No one has ever seen\\na probability, the thinking goes. But everyone sees counts (“frequencies”) in their daily lives.\\nRegardless of the explanation for this phenomenon, we can exploit it. And in this chap-\\nter we exploit it by taking the probability distributions from the previous chapter and sam-\\npling from them to produce counts. The posterior distribution is a probability distribution.\\nAnd like all probability distributions, we can imagine drawing samples from it. The sampled\\nevents in this case are parameter values. Most parameters have no exact empirical realiza-\\ntion. The Bayesian formalism treats parameter distributions as relative plausibility, not as\\nany physical random process. In any event, randomness is always a property of informa-\\ntion, never of the real world. But inside the computer, parameters are just as empirical as\\nthe outcome of a coin flip or a die toss or an agricultural experiment. The posterior defines\\nthe expected frequency that different parameter values will appear, once we start plucking\\nparameters out of it.\\nRethinking: The natural frequency phenomenon is not unique. Changing the representation of\\na problem often makes it easier to address or inspires new ideas that were not available in an old\\nrepresentation.51 In physics, switching between Newtonian and Lagrangian mechanics can make\\nproblems much easier. In evolutionary biology, switching between inclusive fitness and multilevel\\nselection sheds new light on old models. And in statistics, switching between Bayesian and non-\\nBayesian representations often teaches us new things about both approaches.\\n'},\n",
       " {'index': 69,\n",
       "  'number': 51,\n",
       "  'content': '3. SAMPLING THE IMAGINARY\\n51\\nThis chapter teaches you basic skills for working with samples from the posterior dis-\\ntribution. It will seem a little silly to work with samples at this point, because the posterior\\ndistribution for the globe tossing model is very simple. It’s so simple that it’s no problem to\\nwork directly with the grid approximation or even the exact mathematical form.52 But there\\nare two reasons to adopt the sampling approach early on, before it’s really necessary.\\nFirst, many scientists are uncomfortable with integral calculus, even though they have\\nstrong and valid intuitions about how to summarize data. Working with samples transforms\\na problem in calculus into a problem in data summary, into a frequency format problem.\\nAn integral in a typical Bayesian context is just the total probability in some interval. That\\ncan be a challenging calculus problem. But once you have samples from the probability\\ndistribution, it’s just a matter of counting values in the interval. An empirical attack on the\\nposterior allows the scientist to ask and answer more questions about the model, without\\nrelying upon a captive mathematician. For this reason, it is easier and more intuitive to work\\nwith samples from the posterior, than to work with probabilities and integrals directly.\\nSecond, some of the most capable methods of computing the posterior produce nothing\\nbut samples. Many of these methods are variants of Markov chain Monte Carlo techniques\\n(MCMC, Chapter 9). So if you learn early on how to conceptualize and process samples from\\nthe posterior, when you inevitably must fit a model to data using MCMC, you will already\\nknow how to make sense of the output. Beginning with Chapter 9 of this book, you will\\nuse MCMC to open up the types and complexity of models you can practically fit to data.\\nMCMC is no longer a technique only for experts, but rather part of the standard toolkit of\\nquantitative science. So it’s worth planning ahead.\\nSo in this chapter we’ll begin to use samples to summarize and simulate model output.\\nThe skills you learn here will apply to every problem in the remainder of the book, even\\nthough the details of the models and how the samples are produced will vary.\\nRethinking: Why statistics can’t save bad science. The vampirism example at the start of this chapter\\nhas the same logical structure as many different signal detection problems: (1) There is some binary\\nstate that is hidden from us; (2) we observe an imperfect cue of the hidden state; (3) we (should) use\\nBayes’ theorem to logically deduce the impact of the cue on our uncertainty.\\nScientific inference is sometimes framed in similar terms: (1) An hypothesis is either true or false;\\n(2) we get a statistical cue of the hypothesis’ falsity; (3) we (should) use Bayes’ theorem to logically\\ndeduce the impact of the cue on the status of the hypothesis. It’s the third step that is hardly ever done.\\nI’m not really a fan of this framing. But let’s consider a toy example, so you can see the implications.\\nSuppose the probability of a positive finding, when an hypothesis is true, is Pr(sig|true) = 0.95. That’s\\nthe power of the test. Suppose that the probability of a positive finding, when an hypothesis is false,\\nis Pr(sig|false) = 0.05. That’s the false-positive rate, like the 5% of conventional significance testing.\\nFinally, we have to state the base rate at which hypotheses are true. Suppose for example that 1 in\\nevery 100 hypotheses turns out to be true. Then Pr(true) = 0.01. No one knows this value, but the\\nhistory of science suggests it’s small. See Chapter 17 for more discussion. Now compute the posterior:\\nPr(true|pos) = Pr(pos|true) Pr(true)\\nPr(pos)\\n=\\nPr(pos|true) Pr(true)\\nPr(pos|true) Pr(true) + Pr(pos|false) Pr(false)\\nPlug in the appropriate values, and the answer is approximately Pr(true|pos) = 0.16. So a positive\\nfinding corresponds to a 16% chance that the hypothesis is true. This is the same low base-rate phe-\\nnomenon that applies in medical (and vampire) testing. You can shrink the false-positive rate to 1%\\nand get this posterior probability up to 0.5, only as good as a coin flip. The most important thing to\\ndo is to improve the base rate, Pr(true), and that requires thinking, not testing.53\\n'},\n",
       " {'index': 70,\n",
       "  'number': 52,\n",
       "  'content': '52\\n3. SAMPLING THE IMAGINARY\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nproportion water (p)\\nDensity\\nFigure 3.1. Sampling parameter values from the posterior distribution.\\nLeft: 10,000 samples from the posterior implied by the globe tossing data\\nand model. Right: The density of samples (vertical) at each parameter value\\n(horizontal).\\n3.1. Sampling from a grid-approximate posterior\\nBefore beginning to work with samples, we need to generate them. Here’s a reminder\\nfor how to compute the posterior for the globe tossing model, using grid approximation.\\nRemember, the posterior here means the probability of p conditional on the data.\\nR code\\n3.2\\np_grid <- seq( from=0 , to=1 , length.out=1000 )\\nprob_p <- rep( 1 , 1000 )\\nprob_data <- dbinom( 6 , size=9 , prob=p_grid )\\nposterior <- prob_data * prob_p\\nposterior <- posterior / sum(posterior)\\nNow we wish to draw 10,000 samples from this posterior. Imagine the posterior is a bucket\\nfull of parameter values, numbers such as 0.1, 0.7, 0.5, 1, etc. Within the bucket, each value\\nexists in proportion to its posterior probability, such that values near the peak are much more\\ncommon than those in the tails. We’re going to scoop out 10,000 values from the bucket.\\nProvided the bucket is well mixed, the resulting samples will have the same proportions as\\nthe exact posterior density. Therefore the individual values of p will appear in our samples\\nin proportion to the posterior plausibility of each value.\\nHere’s how you can do this in R, with one line of code:\\nR code\\n3.3\\nsamples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )\\nThe workhorse here is sample, which randomly pulls values from a vector. The vector in\\nthis case is p_grid, the grid of parameter values. The probability of each value is given by\\nposterior, which you computed just above.\\n'},\n",
       " {'index': 71,\n",
       "  'number': 53,\n",
       "  'content': '3.2. SAMPLING TO SUMMARIZE\\n53\\nThe resulting samples are displayed in Figure 3.1. On the left, all 10,000 (1e4) random\\nsamples are shown sequentially.\\nR code\\n3.4\\nplot( samples )\\nIn this plot, it’s as if you are flying over the posterior distribution, looking down on it. There\\nare many more samples from the dense region near 0.6 and very few samples below 0.25. On\\nthe right, the plot shows the density estimate computed from these samples.\\nR code\\n3.5\\nlibrary(rethinking)\\ndens( samples )\\nYou can see that the estimated density is very similar to ideal posterior you computed via\\ngrid approximation. If you draw even more samples, maybe 1e5 or 1e6, the density estimate\\nwill get more and more similar to the ideal.\\nAll you’ve done so far is crudely replicate the posterior density you had already com-\\nputed. That isn’t of much value. But next it is time to use these samples to describe and\\nunderstand the posterior. That is of great value.\\n3.2. Sampling to summarize\\nOnce your model produces a posterior distribution, the model’s work is done. But your\\nwork has just begun. It is necessary to summarize and interpret the posterior distribution.\\nExactly how it is summarized depends upon your purpose. But common questions include:\\n• How much posterior probability lies below some parameter value?\\n• How much posterior probability lies between two parameter values?\\n• Which parameter value marks the lower 5% of the posterior probability?\\n• Which range of parameter values contains 90% of the posterior probability?\\n• Which parameter value has highest posterior probability?\\nThese simple questions can be usefully divided into questions about (1) intervals of defined\\nboundaries, (2) questions about intervals of defined probability mass, and (3) questions about\\npoint estimates. We’ll see how to approach these questions using samples from the posterior.\\n3.2.1. Intervals of defined boundaries. Suppose I ask you for the posterior probability that\\nthe proportion of water is less than 0.5. Using the grid-approximate posterior, you can just\\nadd up all of the probabilities, where the corresponding parameter value is less than 0.5:\\nR code\\n3.6\\n# add up posterior probability where p < 0.5\\nsum( posterior[ p_grid < 0.5 ] )\\n[1] 0.1718746\\nSo about 17% of the posterior probability is below 0.5. Couldn’t be easier. But since grid ap-\\nproximation isn’t practical in general, it won’t always be so easy. Once there is more than one\\nparameter in the posterior distribution (wait until the next chapter for that complication),\\neven this simple sum is no longer very simple.\\nSo let’s see how to perform the same calculation, using samples from the posterior. This\\napproach does generalize to complex models with many parameters, and so you can use\\nit everywhere. All you have to do is similarly add up all of the samples below 0.5, but also\\n'},\n",
       " {'index': 72,\n",
       "  'number': 54,\n",
       "  'content': '54\\n3. SAMPLING THE IMAGINARY\\ndivide the resulting count by the total number of samples. In other words, find the frequency\\nof parameter values below 0.5:\\nR code\\n3.7\\nsum( samples < 0.5 ) / 1e4\\n[1] 0.1726\\nAnd that’s nearly the same answer as the grid approximation provided, although your answer\\nwill not be exactly the same, because the exact samples you drew from the posterior will be\\ndifferent. This region is shown in the upper-left plot in Figure 3.2. Using the same approach,\\nyou can ask how much posterior probability lies between 0.5 and 0.75:\\nR code\\n3.8\\nsum( samples > 0.5 & samples < 0.75 ) / 1e4\\n[1] 0.6059\\nSo about 61% of the posterior probability lies between 0.5 and 0.75. This region is shown in\\nthe upper-right plot of Figure 3.2.\\nOverthinking: Counting with sum. In the R code examples just above, I used the function sum\\nto effectively count up how many samples fulfill a logical criterion. Why does this work? It works\\nbecause R internally converts a logical expression, like samples < 0.5, to a vector of TRUE and FALSE\\nresults, one for each element of samples, saying whether or not each element matches the criterion.\\nGo ahead and enter samples < 0.5 on the R prompt, to see this for yourself. Then when you sum\\nthis vector of TRUE and FALSE, R counts each TRUE as 1 and each FALSE as 0. So it ends up counting\\nhow many TRUE values are in the vector, which is the same as the number of elements in samples\\nthat match the logical criterion.\\n3.2.2. Intervals of defined mass. It is more common to see scientific journals reporting\\nan interval of defined mass, usually known as a confidence interval. An interval of\\nposterior probability, such as the ones we are working with, may instead be called a credible\\ninterval. We’re going to call it a compatibility interval instead, in order to avoid the\\nunwarranted implications of “confidence” and “credibility.”54 What the interval indicates\\nis a range of parameter values compatible with the model and data. The model and data\\nthemselves may not inspire confidence, in which case the interval will not either.\\nThese posterior intervals report two parameter values that contain between them a spec-\\nified amount of posterior probability, a probability mass. For this type of interval, it is easier\\nto find the answer by using samples from the posterior than by using a grid approximation.\\nSuppose for example you want to know the boundaries of the lower 80% posterior probabil-\\nity. You know this interval starts at p = 0. To find out where it stops, think of the samples\\nas data and ask where the 80th percentile lies:\\nR code\\n3.9\\nquantile( samples , 0.8 )\\n80%\\n0.7607608\\nThis region is shown in the bottom-left plot in Figure 3.2. Similarly, the middle 80% interval\\nlies between the 10th percentile and the 90th percentile. These boundaries are found using\\nthe same approach:\\n'},\n",
       " {'index': 73,\n",
       "  'number': 55,\n",
       "  'content': '3.2. SAMPLING TO SUMMARIZE\\n55\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n0.0000\\n0.0010\\n0.0020\\nproportion water (p)\\nDensity\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n0.0000\\n0.0010\\n0.0020\\nproportion water (p)\\nDensity\\np\\np\\n(p)\\nproportion water (p)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n0.0000\\n0.0010\\n0.0020\\nproportion water (p)\\nDensity\\nlower 80%\\nproportion water (p)\\nproportion water (\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n0.0000\\n0.0010\\n0.0020\\nproportion water (p)\\nDensity\\nmiddle 80%\\nFigure 3.2. Two kinds of posterior interval. Top row: Intervals of defined\\nboundaries. Top-left: The blue area is the posterior probability below a pa-\\nrameter value of 0.5. Top-right: The posterior probability between 0.5 and\\n0.75. Bottom row: Intervals of defined mass. Bottom-left: Lower 80% poste-\\nrior probability exists below a parameter value of about 0.75. Bottom-right:\\nMiddle 80% posterior probability lies between the 10% and 90% quantiles.\\nR code\\n3.10\\nquantile( samples , c( 0.1 , 0.9 ) )\\n10%\\n90%\\n0.4464464 0.8118118\\nThis region is shown in the bottom-right plot in Figure 3.2.\\nIntervals of this sort, which assign equal probability mass to each tail, are very common\\nin the scientific literature. We’ll call them percentile intervals (PI). These intervals do\\na good job of communicating the shape of a distribution, as long as the distribution isn’t too\\nasymmetrical. But in terms of supporting inferences about which parameters are consistent\\nwith the data, they are not perfect. Consider the posterior distribution and different intervals\\n'},\n",
       " {'index': 74,\n",
       "  'number': 56,\n",
       "  'content': '56\\n3. SAMPLING THE IMAGINARY\\nin Figure 3.3. This posterior is consistent with observing three waters in three tosses and a\\nuniform (flat) prior. It is highly skewed, having its maximum value at the boundary, p = 1.\\nYou can compute it, via grid approximation, with:\\nR code\\n3.11\\np_grid <- seq( from=0 , to=1 , length.out=1000 )\\nprior <- rep(1,1000)\\nlikelihood <- dbinom( 3 , size=3 , prob=p_grid )\\nposterior <- likelihood * prior\\nposterior <- posterior / sum(posterior)\\nsamples <- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )\\nThis code also goes ahead to sample from the posterior. Now, on the left of Figure 3.3, the\\n50% percentile compatibility interval is shaded. You can conveniently compute this from the\\nsamples with PI (part of rethinking):\\nR code\\n3.12\\nPI( samples , prob=0.5 )\\n25%\\n75%\\n0.7037037 0.9329329\\nThis interval assigns 25% of the probability mass above and below the interval. So it pro-\\nvides the central 50% probability. But in this example, it ends up excluding the most prob-\\nable parameter values, near p = 1. So in terms of describing the shape of the posterior\\ndistribution—which is really all these intervals are asked to do—the percentile interval can\\nbe misleading.\\nIn contrast, the right-hand plot in Figure 3.3 displays the 50% highest posterior\\ndensity interval (HPDI).57 The HPDI is the narrowest interval containing the specified\\nprobability mass. If you think about it, there must be an infinite number of posterior intervals\\nRethinking: Why 95%? The most common interval mass in the natural and social sciences is the\\n95% interval. This interval leaves 5% of the probability outside, corresponding to a 5% chance of the\\nparameter not lying within the interval (although see below). This customary interval also reflects\\nthe customary threshold for statistical significance, which is 5% or p < 0.05. It is not easy to defend\\nthe choice of 95% (5%), outside of pleas to convention. Ronald Fisher is sometimes blamed for this\\nchoice, but his widely cited 1925 invocation of it was not enthusiastic:\\n“The [number of standard deviations] for which P = .05, or 1 in 20, is 1.96 or\\nnearly 2; it is convenient to take this point as a limit in judging whether a devia-\\ntion is to be considered significant or not.”55\\nMost people don’t think of convenience as a serious criterion. Later in his career, Fisher actively\\nadvised against always using the same threshold for significance.56\\nSo what are you supposed to do then? There is no consensus, but thinking is always a good idea.\\nIf you are trying to say that an interval doesn’t include some value, then you might use the widest\\ninterval that excludes the value. Often, all compatibility intervals do is communicate the shape of a\\ndistribution. In that case, a series of nested intervals may be more useful than any one interval. For\\nexample, why not present 67%, 89%, and 97% intervals, along with the median? Why these values?\\nNo reason. They are prime numbers, which makes them easy to remember. But all that matters is\\nthey be spaced enough to illustrate the shape of the posterior. And these values avoid 95%, since\\nconventional 95% intervals encourage many readers to conduct unconscious hypothesis tests.\\n'},\n",
       " {'index': 75,\n",
       "  'number': 57,\n",
       "  'content': '3.2. SAMPLING TO SUMMARIZE\\n57\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n0.000\\n0.001\\n0.002\\n0.003\\n0.004\\nproportion water (p)\\nDensity\\n50% Percentile Interval\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n0.000\\n0.001\\n0.002\\n0.003\\n0.004\\nproportion water (p)\\nDensity\\n50% HPDI\\nFigure 3.3. The difference between percentile and highest posterior den-\\nsity compatibility intervals. The posterior density here corresponds to a flat\\nprior and observing three water samples in three total tosses of the globe.\\nLeft: 50% percentile interval. This interval assigns equal mass (25%) to both\\nthe left and right tail. As a result, it omits the most probable parameter value,\\np = 1. Right: 50% highest posterior density interval, HPDI. This interval\\nfinds the narrowest region with 50% of the posterior probability. Such a\\nregion always includes the most probable parameter value.\\nwith the same mass. But if you want an interval that best represents the parameter values\\nmost consistent with the data, then you want the densest of these intervals. That’s what the\\nHPDI is. Compute it from the samples with HPDI (also part of rethinking):\\nR code\\n3.13\\nHPDI( samples , prob=0.5 )\\n|0.5\\n0.5|\\n0.8408408 1.0000000\\nThis interval captures the parameters with highest posterior probability, as well as being no-\\nticeably narrower: 0.16 in width rather than 0.23 for the percentile interval.\\nSo the HPDI has some advantages over the PI. But in most cases, these two types of\\ninterval are very similar.58 They only look so different in this case because the posterior\\ndistribution is highly skewed. If we instead used samples from the posterior distribution for\\nsix waters in nine tosses, these intervals would be nearly identical. Try it for yourself, using\\ndifferent probability masses, such as prob=0.8 and prob=0.95. When the posterior is bell\\nshaped, it hardly matters which type of interval you use. Remember, we’re not launching\\nrockets or calibrating atom smashers, so fetishizing precision to the 5th decimal place will\\nnot improve your science.\\nThe HPDI also has some disadvantages. HPDI is more computationally intensive than PI\\nand suffers from greater simulation variance, which is a fancy way of saying that it is sensitive\\nto how many samples you draw from the posterior. It is also harder to understand and many\\nscientific audiences will not appreciate its features, while they will immediately understand a\\n'},\n",
       " {'index': 76,\n",
       "  'number': 58,\n",
       "  'content': '58\\n3. SAMPLING THE IMAGINARY\\npercentile interval, as ordinary non-Bayesian intervals are typically interpreted (incorrectly)\\nas percentile intervals (although see the Rethinking box below).\\nOverall, if the choice of interval type makes a big difference, then you shouldn’t be us-\\ning intervals to summarize the posterior. Remember, the entire posterior distribution is the\\nBayesian “estimate.” It summarizes the relative plausibilities of each possible value of the\\nparameter. Intervals of the distribution are just helpful for summarizing it. If choice of in-\\nterval leads to different inferences, then you’d be better off just plotting the entire posterior\\ndistribution.\\nRethinking: What do compatibility intervals mean? It is common to hear that a 95% “confidence”\\ninterval means that there is a probability 0.95 that the true parameter value lies within the interval. In\\nstrict non-Bayesian statistical inference, such a statement is never correct, because strict non-Bayesian\\ninference forbids using probability to measure uncertainty about parameters. Instead, one should say\\nthat if we repeated the study and analysis a very large number of times, then 95% of the computed in-\\ntervals would contain the true parameter value. If the distinction is not entirely clear to you, then you\\nare in good company. Most scientists find the definition of a confidence interval to be bewildering,\\nand many of them slip unconsciously into a Bayesian interpretation.\\nBut whether you use a Bayesian interpretation or not, a 95% interval does not contain the true\\nvalue 95% of the time. The history of science teaches us that confidence intervals exhibit chronic\\noverconfidence.59 The word true should set off alarms that something is wrong with a statement like\\n“contains the true value.” The 95% is a small world number (see the introduction to Chapter 2), only\\ntrue in the model’s logical world. So it will never apply exactly to the real or large world. It is what the\\ngolem believes, but you are free to believe something else. Regardless, the width of the interval, and\\nthe values it covers, can provide valuable advice.\\n3.2.3. Point estimates. The third and final common summary task for the posterior is to\\nproduce point estimates of some kind. Given the entire posterior distribution, what value\\nshould you report? This seems like an innocent question, but it is difficult to answer. The\\nBayesian parameter estimate is precisely the entire posterior distribution, which is not a sin-\\ngle number, but instead a function that maps each unique parameter value onto a plausibility\\nvalue. So really the most important thing to note is that you don’t have to choose a point es-\\ntimate. It’s hardly ever necessary and often harmful. It discards information.\\nBut if you must produce a single point to summarize the posterior, you’ll have to ask and\\nanswer more questions. Consider the following example. Suppose again the globe tossing\\nexperiment in which we observe 3 waters out of 3 tosses, as in Figure 3.3. Let’s consider three\\nalternative point estimates. First, it is very common for scientists to report the parameter\\nvalue with highest posterior probability, a maximum a posteriori (MAP) estimate. You can\\neasily compute the MAP in this example:\\nR code\\n3.14\\np_grid[ which.max(posterior) ]\\n[1] 1\\nOr if you instead have samples from the posterior, you can still approximate the same point:\\nR code\\n3.15\\nchainmode( samples , adj=0.01 )\\n[1] 0.9985486\\n'},\n",
       " {'index': 77,\n",
       "  'number': 59,\n",
       "  'content': '3.2. SAMPLING TO SUMMARIZE\\n59\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.000\\n0.001\\n0.002\\n0.003\\n0.004\\nproportion water (p)\\nDensity\\nmode\\nmedian\\nmean\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.2\\n0.4\\n0.6\\n0.8\\ndecision\\nexpected proportional loss\\nFigure 3.4. Point estimates and loss functions. Left: Posterior distribution\\n(blue) after observing 3 water in 3 tosses of the globe. Vertical lines show\\nthe locations of the mode, median, and mean. Each point implies a different\\nloss function. Right: Expected loss under the rule that loss is proportional\\nto absolute distance of decision (horizontal axis) from the true value. The\\npoint marks the value of p that minimizes the expected loss, the posterior\\nmedian.\\nBut why is this point, the mode, interesting? Why not report the posterior mean or median?\\nR code\\n3.16\\nmean( samples )\\nmedian( samples )\\n[1] 0.8005558\\n[1] 0.8408408\\nThese are also point estimates, and they also summarize the posterior. But all three—the\\nmode (MAP), mean, and median—are different in this case. How can we choose? Figure 3.4\\nshows this posterior distribution and the locations of these point summaries.\\nOne principled way to go beyond using the entire posterior as the estimate is to choose\\na loss function. A loss function is a rule that tells you the cost associated with using any\\nparticular point estimate. While statisticians and game theorists have long been interested\\nin loss functions, and how Bayesian inference supports them, scientists hardly ever use them\\nexplicitly. The key insight is that different loss functions imply different point estimates.\\nHere’s an example to help us work through the procedure. Suppose I offer you a bet. Tell\\nme which value of p, the proportion of water on the Earth, you think is correct. I will pay\\nyou $100, if you get it exactly right. But I will subtract money from your gain, proportional\\nto the distance of your decision from the correct value. Precisely, your loss is proportional\\nto the absolute value of d −p, where d is your decision and p is the correct answer. We could\\nchange the precise dollar values involved, without changing the important aspects of this\\n'},\n",
       " {'index': 78,\n",
       "  'number': 60,\n",
       "  'content': '60\\n3. SAMPLING THE IMAGINARY\\nproblem. What matters is that the loss is proportional to the distance of your decision from\\nthe true value.\\nNow once you have the posterior distribution in hand, how should you use it to maxi-\\nmize your expected winnings? It turns out that the parameter value that maximizes expected\\nwinnings (minimizes expected loss) is the median of the posterior distribution. Let’s calcu-\\nlate that fact, without using a mathematical proof. Those interested in the proof should follow\\nthe endnote.60\\nCalculating expected loss for any given decision means using the posterior to average\\nover our uncertainty in the true value. Of course we don’t know the true value, in most\\ncases. But if we are going to use our model’s information about the parameter, that means\\nusing the entire posterior distribution. So suppose we decide p = 0.5 will be our decision.\\nThen the expected loss will be:\\nR code\\n3.17\\nsum( posterior*abs( 0.5 - p_grid ) )\\n[1] 0.3128752\\nThe symbols posterior and p_grid are the same ones we’ve been using throughout this\\nchapter, containing the posterior probabilities and the parameter values, respectively. All\\nthe code above does is compute the weighted average loss, where each loss is weighted by its\\ncorresponding posterior probability. There’s a trick for repeating this calculation for every\\npossible decision, using the function sapply.\\nR code\\n3.18\\nloss <- sapply( p_grid , function(d) sum( posterior*abs( d - p_grid ) ) )\\nNow the symbol loss contains a list of loss values, one for each possible decision, corre-\\nsponding the values in p_grid. From here, it’s easy to find the parameter value that mini-\\nmizes the loss:\\nR code\\n3.19\\np_grid[ which.min(loss) ]\\n[1] 0.8408408\\nAnd this is actually the posterior median, the parameter value that splits the posterior density\\nsuch that half of the mass is above it and half below it. Try median(samples) for compari-\\nson. It may not be exactly the same value, due to sampling variation, but it will be close.\\nSo what are we to learn from all of this? In order to decide upon a point estimate, a\\nsingle-value summary of the posterior distribution, we need to pick a loss function. Different\\nloss functions nominate different point estimates. The two most common examples are the\\nabsolute loss as above, which leads to the median as the point estimate, and the quadratic\\nloss (d −p)2, which leads to the posterior mean (mean(samples)) as the point estimate.\\nWhen the posterior distribution is symmetrical and normal-looking, then the median and\\nmean converge to the same point, which relaxes some anxiety we might have about choosing\\na loss function. For the original globe tossing data (6 waters in 9 tosses), for example, the\\nmean and median are barely different.\\nIn principle, though, the details of the applied context may demand a rather unique loss\\nfunction. Consider a practical example like deciding whether or not to order an evacuation,\\nbased upon an estimate of hurricane wind speed. Damage to life and property increases\\nvery rapidly as wind speed increases. There are also costs to ordering an evacuation when\\n'},\n",
       " {'index': 79,\n",
       "  'number': 61,\n",
       "  'content': '3.3. SAMPLING TO SIMULATE PREDICTION\\n61\\nnone is needed, but these are much smaller. Therefore the implied loss function is highly\\nasymmetric, rising sharply as true wind speed exceeds our guess, but rising only slowly as\\ntrue wind speed falls below our guess. In this context, the optimal point estimate would tend\\nto be larger than posterior mean or median. Moreover, the real issue is whether or not to\\norder an evacuation. Producing a point estimate of wind speed may not be necessary at all.\\nUsually, research scientists don’t think about loss functions. And so any point estimate\\nlike the mean or MAP that they may report isn’t intended to support any particular decision,\\nbut rather to describe the shape of the posterior. You might argue that the decision to make\\nis whether or not to accept an hypothesis. But the challenge then is to say what the relevant\\ncosts and benefits would be, in terms of the knowledge gained or lost.61 Usually it’s better\\nto communicate as much as you can about the posterior distribution, as well as the data and\\nthe model itself, so that others can build upon your work. Premature decisions to accept or\\nreject hypotheses can cost lives.62\\nIt’s healthy to keep these issues in mind, if only because they remind us that many of\\nthe routine questions in statistical inference can only be answered under consideration of a\\nparticular empirical context and applied purpose. Statisticians can provide general outlines\\nand standard answers, but a motivated and attentive scientist will always be able to improve\\nupon such general advice.\\n3.3. Sampling to simulate prediction\\nAnother common job for samples is to ease simulation of the model’s implied obser-\\nvations. Generating implied observations from a model is useful for at least four reasons.\\n(1) Model design. We can sample not only from the posterior, but also from the prior.\\nSeeing what the model expects, before the data arrive, is the best way to understand\\nthe implications of the prior. We’ll do a lot of this in later chapters, where there will\\nbe multiple parameters and so their joint implications are not always very clear.\\n(2) Model checking. After a model is updated using data, it is worth simulating im-\\nplied observations, to check both whether the fit worked correctly and to investi-\\ngate model behavior.\\n(3) Software validation. In order to be sure that our model fitting software is working,\\nit helps to simulate observations under a known model and then attempt to recover\\nthe values of the parameters the data were simulated under.\\n(4) Research design. If you can simulate observations from your hypothesis, then you\\ncan evaluate whether the research design can be effective. In a narrow sense, this\\nmeans doing power analysis, but the possibilities are much broader.\\n(5) Forecasting. Estimates can be used to simulate new predictions, for new cases and\\nfuture observations. These forecasts can be useful as applied prediction, but also\\nfor model criticism and revision.\\nIn this final section of the chapter, we’ll look at how to produce simulated observations and\\nhow to perform some simple model checks.\\n3.3.1. Dummy data. Let’s summarize the globe tossing model that you’ve been working\\nwith for two chapters now. A fixed true proportion of water p exists, and that is the target of\\nour inference. Tossing the globe in the air and catching it produces observations of “water”\\nand “land” that appear in proportion to p and 1 −p, respectively.\\n'},\n",
       " {'index': 80,\n",
       "  'number': 62,\n",
       "  'content': '62\\n3. SAMPLING THE IMAGINARY\\nNow note that these assumptions not only allow us to infer the plausibility of each possi-\\nble value of p, after observation. That’s what you did in the previous chapter. These assump-\\ntions also allow us to simulate the observations that the model implies. They allow this,\\nbecause likelihood functions work in both directions. Given a realized observation, the like-\\nlihood function says how plausible the observation is. And given only the parameters, the\\nlikelihood defines a distribution of possible observations that we can sample from, to simu-\\nlate observation. In this way, Bayesian models are always generative, capable of simulating\\npredictions. Many non-Bayesian models are also generative, but many are not.\\nWe will call such simulated data dummy data, to indicate that it is a stand-in for actual\\ndata. With the globe tossing model, the dummy data arises from a binomial likelihood:\\nPr(W|N, p) =\\nN!\\nW!(N −W)!pW(1 −p)N−W\\nwhere W is an observed count of “water” and N is the number of tosses. Suppose N = 2, two\\ntosses of the globe. Then there are only three possible observations: 0 water, 1 water, 2 water.\\nYou can quickly compute the probability of each, for any given value of p. Let’s use p = 0.7,\\nwhich is just about the true proportion of water on the Earth:\\nR code\\n3.20\\ndbinom( 0:2 , size=2 , prob=0.7 )\\n[1] 0.09 0.42 0.49\\nThis means that there’s a 9% chance of observing w = 0, a 42% chance of w = 1, and a 49%\\nchance of w = 2. If you change the value of p, you’ll get a different distribution of implied\\nobservations.\\nNow we’re going to simulate observations, using these probabilities. This is done by\\nsampling from the distribution just described above. You could use sample to do this, but\\nR provides convenient sampling functions for all the ordinary probability distributions, like\\nthe binomial. So a single dummy data observation of W can be sampled with:\\nR code\\n3.21\\nrbinom( 1 , size=2 , prob=0.7 )\\n[1] 1\\nThat 1 means “1 water in 2 tosses.” The “r” in rbinom stands for “random.” It can also\\ngenerate more than one simulation at a time. A set of 10 simulations can be made by:\\nR code\\n3.22\\nrbinom( 10 , size=2 , prob=0.7 )\\n[1] 2 2 2 1 2 1 1 1 0 2\\nLet’s generate 100,000 dummy observations, just to verify that each value (0, 1, or 2) appears\\nin proportion to its likelihood:\\nR code\\n3.23\\ndummy_w <- rbinom( 1e5 , size=2 , prob=0.7 )\\ntable(dummy_w)/1e5\\ndummy_w\\n0\\n1\\n2\\n0.08904 0.41948 0.49148\\n'},\n",
       " {'index': 81,\n",
       "  'number': 63,\n",
       "  'content': '3.3. SAMPLING TO SIMULATE PREDICTION\\n63\\n0\\n2\\n4\\n6\\n8\\n0\\n5000\\n15000\\n25000\\ndummy water count\\nFrequency\\nFigure 3.5. Distribution of simulated sample\\nobservations from 9 tosses of the globe. These\\nsamples assume the proportion of water is 0.7.\\nAnd those values are very close to the analytically calculated likelihoods further up. You will\\nsee slightly different values, due to simulation variance. Execute the code above multiple\\ntimes, to see how the exact realized frequencies fluctuate from simulation to simulation.\\nOnly two tosses of the globe isn’t much of a sample, though. So now let’s simulate the\\nsame sample size as before, 9 tosses.\\nR code\\n3.24\\ndummy_w <- rbinom( 1e5 , size=9 , prob=0.7 )\\nsimplehist( dummy_w , xlab=\"dummy water count\" )\\nThe resulting plot is shown in Figure 3.5. Notice that most of the time the expected obser-\\nvation does not contain water in its true proportion, 0.7. That’s the nature of observation:\\nThere is a one-to-many relationship between data and data-generating processes. You should\\nexperiment with sample size, the size input in the code above, as well as the prob, to see\\nhow the distribution of simulated samples changes shape and location.\\nSo that’s how to perform a basic simulation of observations. What good is this? There\\nare many useful jobs for these samples. In this chapter, we’ll put them to use in examining\\nthe implied predictions of a model. But to do that, we’ll have to combine them with samples\\nfrom the posterior distribution. That’s next.\\nRethinking: Sampling distributions. Many readers will already have seen simulated observations.\\nSampling distributions are the foundation of common non-Bayesian statistical traditions. In\\nthose approaches, inference about parameters is made through the sampling distribution. In this\\nbook, inference about parameters is never done directly through a sampling distribution. The poste-\\nrior distribution is not sampled, but deduced logically. Then samples can be drawn from the poste-\\nrior, as earlier in this chapter, to aid in inference. In neither case is “sampling” a physical act. In both\\ncases, it’s just a mathematical device and produces only small world (Chapter 2) numbers.\\n3.3.2. Model checking. Model checking means (1) ensuring the model fitting worked\\ncorrectly and (2) evaluating the adequacy of a model for some purpose. Since Bayesian mod-\\nels are always generative, able to simulate observations as well as estimate parameters from\\nobservations, once you condition a model on data, you can simulate to examine the model’s\\nempirical expectations.\\n'},\n",
       " {'index': 82,\n",
       "  'number': 64,\n",
       "  'content': '64\\n3. SAMPLING THE IMAGINARY\\n3.3.2.1. Did the software work? In the simplest case, we can check whether the software\\nworked by checking for correspondence between implied predictions and the data used to\\nfit the model. You might also call these implied predictions retrodictions, as they ask how\\nwell the model reproduces the data used to educate it. An exact match is neither expected\\nnor desired. But when there is no correspondence at all, it probably means the software did\\nsomething wrong.\\nThere is no way to really be sure that software works correctly. Even when the retro-\\ndictions correspond to the observed data, there may be subtle mistakes. And when you start\\nworking with multilevel models, you’ll have to expect a certain pattern of lack of correspon-\\ndence between retrodictions and observations. Despite there being no perfect way to ensure\\nsoftware has worked, the simple check I’m encouraging here often catches silly mistakes,\\nmistakes of the kind everyone makes from time to time.\\nIn the case of the globe tossing analysis, the software implementation is simple enough\\nthat it can be checked against analytical results. So instead let’s move directly to considering\\nthe model’s adequacy.\\n3.3.2.2. Is the model adequate? After assessing whether the posterior distribution is the\\ncorrect one, because the software worked correctly, it’s useful to also look for aspects of the\\ndata that are not well described by the model’s expectations. The goal is not to test whether\\nthe model’s assumptions are “true,” because all models are false. Rather, the goal is to assess\\nexactly how the model fails to describe the data, as a path towards model comprehension,\\nrevision, and improvement.\\nAll models fail in some respect, so you have to use your judgment—as well as the judg-\\nments of your colleagues—to decide whether any particular failure is or is not important.\\nFew scientists want to produce models that do nothing more than re-describe existing sam-\\nples. So imperfect prediction (retrodiction) is not a bad thing. Typically we hope to either\\npredict future observations or understand enough that we might usefully tinker with the\\nworld. We’ll consider these problems in future chapters.\\nFor now, we need to learn how to combine sampling of simulated observations, as in the\\nprevious section, with sampling parameters from the posterior distribution. We expect to\\ndo better when we use the entire posterior distribution, not just some point estimate derived\\nfrom it. Why? Because there is a lot of information about uncertainty in the entire posterior\\ndistribution. We lose this information when we pluck out a single parameter value and then\\nperform calculations with it. This loss of information leads to overconfidence.\\nLet’s do some basic model checks, using simulated observations for the globe tossing\\nmodel. The observations in our example case are counts of water, over tosses of the globe.\\nThe implied predictions of the model are uncertain in two ways, and it’s important to be\\naware of both.\\nFirst, there is observation uncertainty. For any unique value of the parameter p, there\\nis a unique implied pattern of observations that the model expects. These patterns of ob-\\nservations are the same gardens of forking data that you explored in the previous chapter.\\nThese patterns are also what you sampled in the previous section. There is uncertainty in the\\npredicted observations, because even if you know p with certainty, you won’t know the next\\nglobe toss with certainty (unless p = 0 or p = 1).\\nSecond, there is uncertainty about p. The posterior distribution over p embodies this\\nuncertainty. And since there is uncertainty about p, there is uncertainty about everything\\n'},\n",
       " {'index': 83,\n",
       "  'number': 65,\n",
       "  'content': '3.3. SAMPLING TO SIMULATE PREDICTION\\n65\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nprobability of water\\n0\\n0.5\\n1\\nnumber of water samples\\n0\\n3\\n6\\n9\\nPosterior probability\\nSampling distributions\\nPosterior predictive \\ndistribution\\nFigure 3.6. Simulating predictions from the total posterior. Top: The fa-\\nmiliar posterior distribution for the globe tossing data. Ten example pa-\\nrameter values are marked by the vertical lines. Values with greater poste-\\nrior probability indicated by thicker lines. Middle row: Each of the ten pa-\\nrameter values implies a unique sampling distribution of predictions. Bot-\\ntom: Combining simulated observation distributions for all parameter val-\\nues (not just the ten shown), each weighted by its posterior probability, pro-\\nduces the posterior predictive distribution. This distribution propagates un-\\ncertainty about parameter to uncertainty about prediction.\\nthat depends upon p. The uncertainty in p will interact with the sampling variation, when\\nwe try to assess what the model tells us about outcomes.\\nWe’d like to propagate the parameter uncertainty—carry it forward—as we evaluate the\\nimplied predictions. All that is required is averaging over the posterior density for p, while\\ncomputing the predictions. For each possible value of the parameter p, there is an implied\\ndistribution of outcomes. So if you were to compute the sampling distribution of outcomes at\\neach value of p, then you could average all of these prediction distributions together, using the\\nposterior probabilities of each value of p, to get a posterior predictive distribution.\\nFigure 3.6 illustrates this averaging. At the top, the posterior distribution is shown,\\nwith 10 unique parameter values highlighted by the vertical lines. The implied distribution\\nof observations specific to each of these parameter values is shown in the middle row of\\nplots. Observations are never certain for any value of p, but they do shift around in response\\nto it. Finally, at the bottom, the sampling distributions for all values of p are combined,\\nusing the posterior probabilities to compute the weighted average frequency of each possible\\nobservation, zero to nine water samples.\\n'},\n",
       " {'index': 84,\n",
       "  'number': 66,\n",
       "  'content': '66\\n3. SAMPLING THE IMAGINARY\\nThe resulting distribution is for predictions, but it incorporates all of the uncertainty\\nembodied in the posterior distribution for the parameter p. As a result, it is honest. While\\nthe model does a good job of predicting the data—the most likely observation is indeed\\nthe observed data—predictions are still quite spread out. If instead you were to use only a\\nsingle parameter value to compute implied predictions, say the most probable value at the\\npeak of posterior distribution, you’d produce an overconfident distribution of predictions,\\nnarrower than the posterior predictive distribution in Figure 3.6 and more like the sampling\\ndistribution shown for p = 0.6 in the middle row. The usual effect of this overconfidence\\nwill be to lead you to believe that the model is more consistent with the data than it really is—\\nthe predictions will cluster around the observations more tightly. This illusion arises from\\ntossing away uncertainty about the parameters.\\nSo how do you actually do the calculations? To simulate predicted observations for a\\nsingle value of p, say p = 0.6, you can use rbinom to generate random binomial samples:\\nR code\\n3.25\\nw <- rbinom( 1e4 , size=9 , prob=0.6 )\\nThis generates 10,000 (1e4) simulated predictions of 9 globe tosses (size=9), assuming p =\\n0.6. The predictions are stored as counts of water, so the theoretical minimum is zero and the\\ntheoretical maximum is nine. You can use simplehist(w) (in the rethinking package) to\\nget a clean histogram of your simulated outcomes.\\nAll you need to propagate parameter uncertainty into these predictions is replace the\\nvalue 0.6 with samples from the posterior:\\nR code\\n3.26\\nw <- rbinom( 1e4 , size=9 , prob=samples )\\nThe symbol samples above is the same list of random samples from the posterior distribu-\\ntion that you’ve used in previous sections. For each sampled value, a random binomial obser-\\nvation is generated. Since the sampled values appear in proportion to their posterior proba-\\nbilities, the resulting simulated observations are averaged over the posterior. You can manip-\\nulate these simulated observations just like you manipulate samples from the posterior—you\\ncan compute intervals and point statistics using the same procedures. If you plot these sam-\\nples, you’ll see the distribution shown in the right-hand plot in Figure 3.6.\\nThe simulated model predictions are quite consistent with the observed data in this\\ncase—the actual count of 6 lies right in the middle of the simulated distribution. There is\\nquite a lot of spread to the predictions, but a lot of this spread arises from the binomial pro-\\ncess itself, not uncertainty about p. Still, it’d be premature to conclude that the model is\\nperfect. So far, we’ve only viewed the data just as the model views it: Each toss of the globe\\nis completely independent of the others. This assumption is questionable. Unless the person\\ntossing the globe is careful, it is easy to induce correlations and therefore patterns among\\nthe sequential tosses. Consider for example that about half of the globe (and planet) is cov-\\nered by the Pacific Ocean. As a result, water and land are not uniformly distributed on the\\nglobe, and therefore unless the globe spins and rotates enough while in the air, the position\\nwhen tossed could easily influence the sample once it lands. The same problem arises in coin\\ntosses, and indeed skilled individuals can influence the outcome of a coin toss, by exploiting\\nthe physics of it.63\\nSo with the goal of seeking out aspects of prediction in which the model fails, let’s look\\nat the data in two different ways. Recall that the sequence of nine tosses was W L W W W L\\n'},\n",
       " {'index': 85,\n",
       "  'number': 67,\n",
       "  'content': '3.3. SAMPLING TO SIMULATE PREDICTION\\n67\\n2\\n4\\n6\\n8\\n0\\n500\\n1500\\n2500\\nlongest run length\\nFrequency\\n0\\n2\\n4\\n6\\n8\\n0\\n500\\n1500\\n2500\\nnumber of switches\\nFrequency\\nFigure 3.7. Alternative views of the same posterior predictive distribution\\n(see Figure 3.6). Instead of considering the data as the model saw it, as\\na sum of water samples, now we view the data as both the length of the\\nmaximum run of water or land (left) and the number of switches between\\nwater and land samples (right). Observed values highlighted in blue. While\\nthe simulated predictions are consistent with the run length (3 water in a\\nrow), they are much less consistent with the frequent switches (6 switches\\nin 9 tosses).\\nW L W. First, consider the length of the longest run of either water or land. This will provide\\na crude measure of correlation between tosses. So in the observed data, the longest run is 3\\nW’s. Second, consider the number of times in the data that the sample switches from water\\nto land or from land to water. This is another measure of correlation between samples. In\\nthe observed data, the number of switches is 6. There is nothing special about these two new\\nways of describing the data. They just serve to inspect the data in new ways. In your own\\nmodeling, you’ll have to imagine aspects of the data that are relevant in your context, for\\nyour purposes.\\nFigure 3.7 shows the simulated predictions, viewed in these two new ways. On the\\nleft, the length of the longest run of water or land is plotted, with the observed value of 3\\nhighlighted by the bold line. Again, the true observation is the most common simulated ob-\\nservation, but with a lot of spread around it. On the right, the number of switches from water\\nto land and land to water is shown, with the observed value of 6 highlighted in bold. Now\\nthe simulated predictions appear less consistent with the data, as the majority of simulated\\nobservations have fewer switches than were observed in the actual sample. This is consis-\\ntent with lack of independence between tosses of the globe, in which each toss is negatively\\ncorrelated with the last.\\nDoes this mean that the model is bad? That depends. The model will always be wrong\\nin some sense, be mis-specified. But whether or not the mis-specification should lead us to\\ntry other models will depend upon our specific interests. In this case, if tosses do tend to\\nswitch from W to L and L to W, then each toss will provide less information about the true\\ncoverage of water on the globe. In the long run, even the wrong model we’ve used throughout\\n'},\n",
       " {'index': 86,\n",
       "  'number': 68,\n",
       "  'content': '68\\n3. SAMPLING THE IMAGINARY\\nthe chapter will converge on the correct proportion. But it will do so more slowly than the\\nposterior distribution may lead us to believe.\\nRethinking: What does more extreme mean? A common way of measuring deviation of observation\\nfrom model is to count up the tail area that includes the observed data and any more extreme data.\\nOrdinary p-values are an example of such a tail-area probability. When comparing observations to\\ndistributions of simulated predictions, as in Figure 3.6 and Figure 3.7, we might wonder how far\\nout in the tail the observed data must be before we conclude that the model is a poor one. Because\\nstatistical contexts vary so much, it’s impossible to give a universally useful answer.\\nBut more importantly, there are usually very many ways to view data and define “extreme.” Or-\\ndinary p-values view the data in just the way the model expects it, and so provide a very weak form of\\nmodel checking. For example, the far-right plot in Figure 3.6 evaluates model fit in the best way for\\nthe model. Alternative ways of defining “extreme” may provide a more serious challenge to a model.\\nThe different definitions of extreme in Figure 3.7 can more easily embarrass it.\\nModel fitting remains an objective procedure—everyone and every golem conducts Bayesian\\nupdating in a way that doesn’t depend upon personal preferences. But model checking is inherently\\nsubjective, and this actually allows it to be quite powerful, since subjective knowledge of an empirical\\ndomain provides expertise. Expertise in turn allows for imaginative checks of model performance.\\nSince golems have terrible imaginations, we need the freedom to engage our own imaginations. In\\nthis way, the objective and subjective work together.64\\n3.4. Summary\\nThis chapter introduced the basic procedures for manipulating posterior distributions.\\nOur fundamental tool is samples of parameter values drawn from the posterior distribution.\\nWorking with samples transforms a problem of integral calculus into a problem of data sum-\\nmary. These samples can be used to produce intervals, point estimates, posterior predictive\\nchecks, as well as other kinds of simulations.\\nPosterior predictive checks combine uncertainty about parameters, as described by the\\nposterior distribution, with uncertainty about outcomes, as described by the assumed like-\\nlihood function. These checks are useful for verifying that your software worked correctly.\\nThey are also useful for prospecting for ways in which your models are inadequate.\\nOnce models become more complex, posterior predictive simulations will be used for\\na broader range of applications. Even understanding a model often requires simulating im-\\nplied observations. We’ll keep working with samples from the posterior, to make these tasks\\nas easy and customizable as possible.\\n3.5. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\nEasy. The Easy problems use the samples from the posterior distribution for the globe tossing ex-\\nample. This code will give you a specific set of samples, so that you can check your answers exactly.\\nR code\\n3.27\\np_grid <- seq( from=0 , to=1 , length.out=1000 )\\nprior <- rep( 1 , 1000 )\\nlikelihood <- dbinom( 6 , size=9 , prob=p_grid )\\nposterior <- likelihood * prior\\nposterior <- posterior / sum(posterior)\\n'},\n",
       " {'index': 87,\n",
       "  'number': 69,\n",
       "  'content': '3.5. PRACTICE\\n69\\nset.seed(100)\\nsamples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )\\nUse the values in samples to answer the questions that follow.\\n3E1. How much posterior probability lies below p = 0.2?\\n3E2. How much posterior probability lies above p = 0.8?\\n3E3. How much posterior probability lies between p = 0.2 and p = 0.8?\\n3E4. 20% of the posterior probability lies below which value of p?\\n3E5. 20% of the posterior probability lies above which value of p?\\n3E6. Which values of p contain the narrowest interval equal to 66% of the posterior probability?\\n3E7. Which values of p contain 66% of the posterior probability, assuming equal posterior probabil-\\nity both below and above the interval?\\n3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the poste-\\nrior distribution, using grid approximation. Use the same flat prior as before.\\n3M2. Draw 10,000 samples from the grid approximation from above. Then use the samples to cal-\\nculate the 90% HPDI for p.\\n3M3. Construct a posterior predictive check for this model and data. This means simulate the distri-\\nbution of samples, averaging over the posterior uncertainty in p. What is the probability of observing\\n8 water in 15 tosses?\\n3M4. Using the posterior distribution constructed from the new (8/15) data, now calculate the prob-\\nability of observing 6 water in 9 tosses.\\n3M5. Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5.\\nThis corresponds to prior information that a majority of the Earth’s surface is water. Repeat each\\nproblem above and compare the inferences. What difference does the better prior make? If it helps,\\ncompare inferences (using both priors) to the true value p = 0.7.\\n3M6. Suppose you want to estimate the Earth’s proportion of water very precisely. Specifically, you\\nwant the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means\\nthe distance between the upper and lower bound of the interval should be 0.05. How many times will\\nyou have to toss the globe to do this?\\nHard. The Hard problems here all use the data below. These data indicate the gender (male=1, fe-\\nmale=0) of officially reported first and second born children in 100 two-child families.\\nR code\\n3.28\\nbirth1 <- c(1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0,\\n0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0,\\n1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0,\\n1,0,1,1,1,0,1,1,1,1)\\nbirth2 <- c(0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0,\\n1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,\\n1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1,\\n0,0,0,1,1,1,0,0,0,0)\\n'},\n",
       " {'index': 88,\n",
       "  'number': 70,\n",
       "  'content': '70\\n3. SAMPLING THE IMAGINARY\\nSo for example, the first family in the data reported a boy (1) and then a girl (0). The second family\\nreported a girl (0) and then a boy (1). The third family reported two girls. You can load these two\\nvectors into R’s memory by typing:\\nR code\\n3.29\\nlibrary(rethinking)\\ndata(homeworkch3)\\nUse these vectors as data. So for example to compute the total number of boys born across all of these\\nbirths, you could use:\\nR code\\n3.30\\nsum(birth1) + sum(birth2)\\n[1] 111\\n3H1. Using grid approximation, compute the posterior distribution for the probability of a birth\\nbeing a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior\\nprobability?\\n3H2. Using the sample function, draw 10,000 random parameter values from the posterior distri-\\nbution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior\\ndensity intervals.\\n3H3. Use rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 num-\\nbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers\\nof boys to the actual count in the data (111 boys out of 200 births). There are many good ways to\\nvisualize the simulations, but the dens command (part of the rethinking package) is probably the\\neasiest way in this case. Does it look like the model fits the data well? That is, does the distribution\\nof predictions include the actual observation as a central, likely outcome?\\n3H4. Now compare 10,000 counts of boys from 100 simulated first borns only to the number of boys\\nin the first births, birth1. How does the model look in this light?\\n3H5. The model assumes that sex of first and second births are independent. To check this assump-\\ntion, focus now on second births that followed female first borns. Compare 10,000 simulated counts\\nof boys to only those second births that followed girls. To do this correctly, you need to count the\\nnumber of first borns who were girls and simulate that many births, 10,000 times. Compare the\\ncounts of boys in your simulations to the actual observed count of boys following girls. How does the\\nmodel look in this light? Any guesses what is going on in these data?\\n'},\n",
       " {'index': 89,\n",
       "  'number': 71,\n",
       "  'content': '4 Geocentric Models\\nHistory has been unkind to Ptolemy. Claudius Ptolemy (born 90 CE, died 168 CE) was\\nan Egyptian mathematician and astronomer, famous for his geocentric model of the solar\\nsystem. These days, when scientists wish to mock someone, they might compare him to a\\nsupporter of the geocentric model. But Ptolemy was a genius. His mathematical model of\\nthe motions of the planets (Figure 4.1) was extremely accurate. To achieve its accuracy, it\\nemployed a device known as an epicycle, a circle on a circle. It is even possible to have epi-\\nepicycles, circles on circles on circles. With enough epicycles in the right places, Ptolemy’s\\nmodel could predict planetary motion with great accuracy. And so the model was utilized\\nfor over a thousand years. And Ptolemy and people like him worked it all out without the\\naid of a computer. Anyone should be flattered to be compared to Ptolemy.\\nThe trouble of course is that the geocentric model is wrong, in many respects. If you\\nused it to plot the path of your Mars probe, you’d miss the red planet by quite a distance.\\nBut for spotting Mars in the night sky, it remains an excellent model. It would have to be\\nre-calibrated every century or so, depending upon which heavenly body you wish to locate.\\nBut the geocentric model continues to make useful predictions, provided those predictions\\nremain within a narrow domain of questioning.\\nThe strategy of using epicycles might seem crazy, once you know the correct structure\\nof the solar system. But it turns out that the ancients had hit upon a generalized system of\\napproximation. Given enough circles embedded in enough places, the Ptolemaic strategy is\\nthe same as a Fourier series, a way of decomposing a periodic function (like an orbit) into\\na series of sine and cosine functions. So no matter the actual arrangement of planets and\\nmoons, a geocentric model can be built to describe their paths against the night sky.\\nLinear regression is the geocentric model of applied statistics. By “linear regression,”\\nwe will mean a family of simple statistical golems that attempt to learn about the mean and\\nvariance of some measurement, using an additive combination of other measurements. Like\\ngeocentrism, linear regression can usefully describe a very large variety of natural phenom-\\nena. Like geocentrism, linear regression is a descriptive model that corresponds to many\\ndifferent process models. If we read its structure too literally, we’re likely to make mistakes.\\nBut used wisely, these little linear golems continue to be useful.\\nThis chapter introduces linear regression as a Bayesian procedure. Under a probability\\ninterpretation, which is necessary for Bayesian work, linear regression uses a Gaussian (nor-\\nmal) distribution to describe our golem’s uncertainty about some measurement of interest.\\nThis type of model is simple, flexible, and commonplace. Like all statistical models, it is\\nnot universally useful. But linear regression has a strong claim to being foundational, in the\\nsense that once you learn to build and interpret linear regression models, you can more easily\\nmove on to other types of regression which are less normal.\\n71\\n'},\n",
       " {'index': 90,\n",
       "  'number': 72,\n",
       "  'content': '72\\n4. GEOCENTRIC MODELS\\nEarth\\nequant\\nplanet\\nepicycle\\ndeferent\\nFigure 4.1. The Ptolemaic Universe, in\\nwhich complex motion of the planets in\\nthe night sky was explained by orbits\\nwithin orbits, called epicycles. The model\\nis incredibly wrong, yet makes quite good\\npredictions.\\n4.1. Why normal distributions are normal\\nSuppose you and a thousand of your closest friends line up on the halfway line of a soccer\\nfield (football pitch). Each of you has a coin in your hand. At the sound of the whistle, you\\nbegin flipping the coins. Each time a coin comes up heads, that person moves one step\\ntowards the left-hand goal. Each time a coin comes up tails, that person moves one step\\ntowards the right-hand goal. Each person flips the coin 16 times, follows the implied moves,\\nand then stands still. Now we measure the distance of each person from the halfway line.\\nCan you predict what proportion of the thousand people who are standing on the halfway\\nline? How about the proportion 5 yards left of the line?\\nIt’s hard to say where any individual person will end up, but you can say with great con-\\nfidence what the collection of positions will be. The distances will be distributed in approxi-\\nmately normal, or Gaussian, fashion. This is true even though the underlying distribution is\\nbinomial. It does this because there are so many more possible ways to realize a sequence of\\nleft-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends\\nup one step left or right of zero, and so on, with the number of possible sequences declining\\nin the characteristic bell curve of the normal distribution.\\n4.1.1. Normal by addition. Let’s see this result, by simulating this experiment in R. To show\\nthat there’s nothing special about the underlying coin flip, assume instead that each step is\\ndifferent from all the others, a random distance between zero and one yard. Thus a coin is\\nflipped, a distance between zero and one yard is taken in the indicated direction, and the\\nprocess repeats. To simulate this, we generate for each person a list of 16 random numbers\\nbetween −1 and 1. These are the individual steps. Then we add these steps together to get\\nthe position after 16 steps. Then we need to replicate this procedure 1000 times. This is the\\nsort of task that would be harrowing in a point-and-click interface, but it is made trivial by\\nthe command line. Here’s a single line to do the whole thing:\\nR code\\n4.1\\npos <- replicate( 1000 , sum( runif(16,-1,1) ) )\\n'},\n",
       " {'index': 91,\n",
       "  'number': 73,\n",
       "  'content': '4.1. WHY NORMAL DISTRIBUTIONS ARE NORMAL\\n73\\nFigure 4.2. Random walks on the soccer field converge to a normal dis-\\ntribution. The more steps are taken, the closer the match between the real\\nempirical distribution of positions and the ideal normal distribution, super-\\nimposed in the last plot in the bottom panel.\\nYou can plot the distribution of final positions in a number of different ways, including\\nhist(pos) and plot(density(pos)). In Figure 4.2, I show the result of these random\\nwalks and how their distribution evolves as the number of steps increases. The top panel\\nplots 100 different, independent random walks, with one highlighted in black. The vertical\\ndashes indicate the locations corresponding to the distribution plots underneath, measured\\nafter 4, 8, and 16 steps. Although the distribution of positions starts off seemingly idiosyn-\\ncratic, after 16 steps, it has already taken on a familiar outline. The familiar “bell” curve of\\nthe Gaussian distribution is emerging from the randomness. Go ahead and experiment with\\neven larger numbers of steps to verify for yourself that the distribution of positions is stabi-\\nlizing on the Gaussian. You can square the step sizes and transform them in a number of\\narbitrary ways, without changing the result: Normality emerges. Where does it come from?\\nAny process that adds together random values from the same distribution converges to\\na normal. But it’s not easy to grasp why addition should result in a bell curve of sums.65\\nHere’s a conceptual way to think of the process. Whatever the average value of the source\\ndistribution, each sample from it can be thought of as a fluctuation from that average value.\\nWhen we begin to add these fluctuations together, they also begin to cancel one another out.\\nA large positive fluctuation will cancel a large negative one. The more terms in the sum, the\\nmore chances for each fluctuation to be canceled by another, or by a series of smaller ones\\nin the opposite direction. So eventually the most likely sum, in the sense that there are the\\nmost ways to realize it, will be a sum in which every fluctuation is canceled by another, a sum\\nof zero (relative to the mean).66\\n'},\n",
       " {'index': 92,\n",
       "  'number': 74,\n",
       "  'content': '74\\n4. GEOCENTRIC MODELS\\nIt doesn’t matter what shape the underlying distribution possesses. It could be uniform,\\nlike in our example above, or it could be (nearly) anything else.67 Depending upon the un-\\nderlying distribution, the convergence might be slow, but it will be inevitable. Often, as in\\nthis example, convergence is rapid.\\n4.1.2. Normal by multiplication. Here’s another way to get a normal distribution. Suppose\\nthe growth rate of an organism is influenced by a dozen loci, each with several alleles that\\ncode for more growth. Suppose also that all of these loci interact with one another, such that\\neach increase growth by a percentage. This means that their effects multiply, rather than add.\\nFor example, we can sample a random growth rate for this example with this line of code:\\nR code\\n4.2\\nprod( 1 + runif(12,0,0.1) )\\nThis code just samples 12 random numbers between 1.0 and 1.1, each representing a pro-\\nportional increase in growth. Thus 1.0 means no additional growth and 1.1 means a 10%\\nincrease. The product of all 12 is computed and returned as output. Now what distribution\\ndo you think these random products will take? Let’s generate 10,000 of them and see:\\nR code\\n4.3\\ngrowth <- replicate( 10000 , prod( 1 + runif(12,0,0.1) ) )\\ndens( growth , norm.comp=TRUE )\\nThe reader should execute this code in R and see that the distribution is approximately nor-\\nmal again. I said normal distributions arise from summing random fluctuations, which is\\ntrue. But the effect at each locus was multiplied by the effects at all the others, not added. So\\nwhat’s going on here?\\nWe again get convergence towards a normal distribution, because the effect at each lo-\\ncus is quite small. Multiplying small numbers is approximately the same as addition. For\\nexample, if there are two loci with alleles increasing growth by 10% each, the product is:\\n1.1 × 1.1 = 1.21\\nWe could also approximate this product by just adding the increases, and be off by only 0.01:\\n1.1 × 1.1 = (1 + 0.1)(1 + 0.1) = 1 + 0.2 + 0.01 ≈1.2\\nThe smaller the effect of each locus, the better this additive approximation will be. In this\\nway, small effects that multiply together are approximately additive, and so they also tend to\\nstabilize on Gaussian distributions. Verify this for yourself by comparing:\\nR code\\n4.4\\nbig <- replicate( 10000 , prod( 1 + runif(12,0,0.5) ) )\\nsmall <- replicate( 10000 , prod( 1 + runif(12,0,0.01) ) )\\nThe interacting growth deviations, as long as they are sufficiently small, converge to a Gauss-\\nian distribution. In this way, the range of causal forces that tend towards Gaussian distribu-\\ntions extends well beyond purely additive interactions.\\n4.1.3. Normal by log-multiplication. But wait, there’s more. Large deviates that are multi-\\nplied together do not produce Gaussian distributions, but they do tend to produce Gaussian\\ndistributions on the log scale. For example:\\n'},\n",
       " {'index': 93,\n",
       "  'number': 75,\n",
       "  'content': '4.1. WHY NORMAL DISTRIBUTIONS ARE NORMAL\\n75\\nR code\\n4.5\\nlog.big <- replicate( 10000 , log(prod(1 + runif(12,0,0.5))) )\\nYet another Gaussian distribution. We get the Gaussian distribution back, because adding\\nlogs is equivalent to multiplying the original numbers. So even multiplicative interactions\\nof large deviations can produce Gaussian distributions, once we measure the outcomes on\\nthe log scale. Since measurement scales are arbitrary, there’s nothing suspicious about this\\ntransformation. After all, it’s natural to measure sound and earthquakes and even informa-\\ntion (Chapter 7) on a log scale.\\n4.1.4. Using Gaussian distributions. We’re going to spend the rest of this chapter using the\\nGaussian distribution as a skeleton for our hypotheses, building up models of measurements\\nas aggregations of normal distributions. The justifications for using the Gaussian distribution\\nfall into two broad categories: (1) ontological and (2) epistemological.\\nBy the ontological justification, the world is full of Gaussian distributions, approximately.\\nWe’re never going to experience a perfect Gaussian distribution. But it is a widespread pat-\\ntern, appearing again and again at different scales and in different domains. Measurement\\nerrors, variations in growth, and the velocities of molecules all tend towards Gaussian distri-\\nbutions. These processes do this because at their heart, these processes add together fluctu-\\nations. And repeatedly adding finite fluctuations results in a distribution of sums that have\\nshed all information about the underlying process, aside from mean and spread.\\nOne consequence of this is that statistical models based on Gaussian distributions can-\\nnot reliably identify micro-process. This recalls the modeling philosophy from Chapter 1\\n(page 6). But it also means that these models can do useful work, even when they cannot\\nidentify process. If we had to know the development biology of height before we could build\\na statistical model of height, human biology would be sunk.\\nThere are many other patterns in nature, so make no mistake in assuming that the Gauss-\\nian pattern is universal. In later chapters, we’ll see how other useful and common patterns,\\nlike the exponential and gamma and Poisson, also arise from natural processes. The Gauss-\\nian is a member of a family of fundamental natural distributions known as the exponential\\nfamily. All of the members of this family are important for working science, because they\\npopulate our world.\\nBut the natural occurrence of the Gaussian distribution is only one reason to build mod-\\nels around it. By the epistemological justification, the Gaussian represents a particular state\\nof ignorance. When all we know or are willing to say about a distribution of measures (mea-\\nsures are continuous values on the real number line) is their mean and variance, then the\\nGaussian distribution arises as the most consistent with our assumptions.\\nThat is to say that the Gaussian distribution is the most natural expression of our state\\nof ignorance, because if all we are willing to assume is that a measure has finite variance,\\nthe Gaussian distribution is the shape that can be realized in the largest number of ways\\nand does not introduce any new assumptions. It is the least surprising and least informative\\nassumption to make. In this way, the Gaussian is the distribution most consistent with our\\nassumptions. Or rather, it is the most consistent with our golem’s assumptions. If you don’t\\nthink the distribution should be Gaussian, then that implies that you know something else\\nthat you should tell your golem about, something that would improve inference.\\n'},\n",
       " {'index': 94,\n",
       "  'number': 76,\n",
       "  'content': '76\\n4. GEOCENTRIC MODELS\\nThis epistemological justification is premised on information theory and maximum\\nentropy. We’ll dwell on information theory in Chapter 7 and maximum entropy in Chap-\\nter 10. Then in later chapters, other common and useful distributions will be used to build\\ngeneralized linear models (GLMs). When these other distributions are introduced, you’ll\\nlearn the constraints that make them the uniquely most appropriate distributions.\\nFor now, let’s take the ontological and epistemological justifications of just the Gaussian\\ndistribution as reasons to start building models of measures around it. Throughout all of this\\nmodeling, keep in mind that using a model is not equivalent to swearing an oath to it. The\\ngolem is your servant, not the other way around.\\nRethinking: Heavy tails. The Gaussian distribution is common in nature and has some nice proper-\\nties. But there are some risks in using it as a default data model. The extreme ends of a distribution are\\nknown as its tails. And the Gaussian distribution has some very thin tails—there is very little prob-\\nability in them. Instead most of the mass in the Gaussian lies within one standard deviation of the\\nmean. Many natural (and unnatural) processes have much heavier tails. These processes have much\\nhigher probabilities of producing extreme events. A real and important example is financial time\\nseries—the ups and downs of a stock market can look Gaussian in the short term, but over medium\\nand long periods, extreme shocks make the Gaussian model (and anyone who uses it) look foolish.68\\nHistorical time series may behave similarly, and any inference for example of trends in warfare is\\nprone to heavy-tailed surprises.69 We’ll consider alternatives to the Gaussian later.\\nOverthinking: Gaussian distribution. You don’t have to memorize the Gaussian probability distri-\\nbution. You’re computer already knows it. But some knowledge of its form can help demystify it. The\\nprobability density (see below) of some value y, given a Gaussian (normal) distribution with mean µ\\nand standard deviation σ, is:\\np(y|µ, σ) =\\n1\\n√\\n2πσ2 exp\\n\\x12\\n−(y −µ)2\\n2σ2\\n\\x13\\nThis looks monstrous. The important bit is just the (y −µ)2 bit. This is the part that gives the normal\\ndistribution its fundamental quadratic shape. Once you exponentiate the quadratic shape, you get\\nthe classic bell curve. The rest of it just scales and standardizes the distribution.\\nThe Gaussian is a continuous distribution, unlike the discrete distributions of earlier chapters.\\nProbability distributions with only discrete outcomes, like the binomial, are called probability mass\\nfunctions and denoted Pr. Continuous ones like the Gaussian are called probability density functions,\\ndenoted with p or just plain old f, depending upon author and tradition. For mathematical reasons,\\nprobability densities can be greater than 1. Try dnorm(0,0,0.1), for example, which is the way to\\nmake R calculate p(0|0, 0.1). The answer, about 4, is no mistake. Probability density is the rate of\\nchange in cumulative probability. So where cumulative probability is increasing rapidly, density can\\neasily exceed 1. But if we calculate the area under the density function, it will never exceed 1. Such\\nareas are also called probability mass. You can usually ignore these density/mass details while doing\\ncomputational work. But it’s good to be aware of the distinction. Sometimes the difference matters.\\nThe Gaussian distribution is routinely seen without σ but with another parameter, τ. The param-\\neter τ in this context is usually called precision and defined as τ = 1/σ2. When σ is large, τ is small.\\nThis change of parameters gives us the equivalent formula (just substitute σ = 1/√τ):\\np(y|µ, τ) =\\nr τ\\n2π exp\\n\\x00−1\\n2τ(y −µ)2\\x01\\nThis form is common in Bayesian data analysis, and Bayesian model fitting software, such as BUGS\\nor JAGS, sometimes requires using τ rather than σ.\\n'},\n",
       " {'index': 95,\n",
       "  'number': 77,\n",
       "  'content': '4.2. A LANGUAGE FOR DESCRIBING MODELS\\n77\\n4.2. A language for describing models\\nThis book adopts a standard language for describing and coding statistical models. You\\nfind this language in many statistical texts and in nearly all statistical journals, as it is general\\nto both Bayesian and non-Bayesian modeling. Scientists increasingly use this same language\\nto describe their statistical methods, as well. So learning this language is an investment, no\\nmatter where you are headed next.\\nHere’s the approach, in abstract. There will be many examples later, but it is important\\nto get the general recipe before seeing these.\\n(1) First, we recognize a set of variables to work with. Some of these variables are ob-\\nservable. We call these data. Others are unobservable things like rates and averages.\\nWe call these parameters.\\n(2) We define each variable either in terms of the other variables or in terms of a prob-\\nability distribution.\\n(3) The combination of variables and their probability distributions defines a joint gen-\\nerative model that can be used both to simulate hypothetical observations as well\\nas analyze real ones.\\nThis outline applies to models in every field, from astronomy to art history. The biggest\\ndifficulty usually lies in the subject matter—which variables matter and how does theory tell\\nus to connect them?—not in the mathematics.\\nAfter all these decisions are made—and most of them will come to seem automatic to\\nyou before long—we summarize the model with something mathy like:\\nyi ∼Normal(µi, σ)\\nµi = βxi\\nβ ∼Normal(0, 10)\\nσ ∼Exponential(1)\\nxi ∼Normal(0, 1)\\nIf that doesn’t make much sense, good. That indicates that you are holding the right textbook,\\nsince this book teaches you how to read and write these mathematical model descriptions.\\nWe won’t do any mathematical manipulation of them. Instead, they provide an unambigu-\\nous way to define and communicate our models. Once you get comfortable with their gram-\\nmar, when you start reading these mathematical descriptions in other books or in scientific\\njournals, you’ll find them less obtuse.\\nThe approach above surely isn’t the only way to describe statistical modeling, but it is a\\nwidespread and productive language. Once a scientist learns this language, it becomes easier\\nto communicate the assumptions of our models. We no longer have to remember seemingly\\narbitrary lists of bizarre conditions like homoscedasticity (constant variance), because we can\\njust read these conditions from the model definitions. We will also be able to see natural ways\\nto change these assumptions, instead of feeling trapped within some procrustean model type,\\nlike regression or multiple regression or ANOVA or ANCOVA or such. These are all the\\nsame kind of model, and that fact becomes obvious once we know how to talk about models\\nas mappings of one set of variables through a probability distribution onto another set of\\nvariables. Fundamentally, these models define the ways values of some variables can arise,\\ngiven values of other variables (Chapter 2).\\n'},\n",
       " {'index': 96,\n",
       "  'number': 78,\n",
       "  'content': '78\\n4. GEOCENTRIC MODELS\\n4.2.1. Re-describing the globe tossing model. It’s good to work with examples. Recall the\\nproportion of water problem from previous chapters. The model in that case was always:\\nW ∼Binomial(N, p)\\np ∼Uniform(0, 1)\\nwhere W was the observed count of water, N was the total number of tosses, and p was the\\nproportion of water on the globe. Read the above statement as:\\nThe count W is distributed binomially with sample size N and probability p.\\nThe prior for p is assumed to be uniform between zero and one.\\nOnce we know the model in this way, we automatically know all of its assumptions. We\\nknow the binomial distribution assumes that each sample (globe toss) is independent of the\\nothers, and so we also know that the model assumes that sample points are independent of\\none another.\\nFor now, we’ll focus on simple models like the above. In these models, the first line de-\\nfines the likelihood function used in Bayes’ theorem. The other lines define priors. Both of\\nthe lines in this model are stochastic, as indicated by the ∼symbol. A stochastic relation-\\nship is just a mapping of a variable or parameter onto a distribution. It is stochastic because\\nno single instance of the variable on the left is known with certainty. Instead, the mapping is\\nprobabilistic: Some values are more plausible than others, but very many different values are\\nplausible under any model. Later, we’ll have models with deterministic definitions in them.\\nOverthinking: From model definition to Bayes’ theorem. To relate the mathematical format above\\nto Bayes’ theorem, you could use the model definition to define the posterior distribution:\\nPr(p|w, n) =\\nBinomial(w|n, p)Uniform(p|0, 1)\\nR\\nBinomial(w|n, p)Uniform(p|0, 1)dp\\nThat monstrous denominator is just the average likelihood again. It standardizes the posterior to sum\\nto 1. The action is in the numerator, where the posterior probability of any particular value of p is\\nseen again to be proportional to the product of the likelihood and prior. In R code form, this is the\\nsame grid approximation calculation you’ve been using all along. In a form recognizable as the above\\nexpression:\\nR code\\n4.6\\nw <- 6; n <- 9;\\np_grid <- seq(from=0,to=1,length.out=100)\\nposterior <- dbinom(w,n,p_grid)*dunif(p_grid,0,1)\\nposterior <- posterior/sum(posterior)\\nCompare to the calculations in earlier chapters.\\n4.3. Gaussian model of height\\nLet’s build a linear regression model now. Well, it’ll be a “regression” once we have a\\npredictor variable in it. For now, we’ll get the scaffold in place and construct the predictor\\nvariable in the next section. For the moment, we want a single measurement variable to\\nmodel as a Gaussian distribution. There will be two parameters describing the distribution’s\\nshape, the mean µ and the standard deviation σ. Bayesian updating will allow us to consider\\nevery possible combination of values for µ and σ and to score each combination by its relative\\n'},\n",
       " {'index': 97,\n",
       "  'number': 79,\n",
       "  'content': \"4.3. GAUSSIAN MODEL OF HEIGHT\\n79\\nplausibility, in light of the data. These relative plausibilities are the posterior probabilities of\\neach combination of values µ, σ.\\nAnother way to say the above is this. There are an infinite number of possible Gaussian\\ndistributions. Some have small means. Others have large means. Some are wide, with a large\\nσ. Others are narrow. We want our Bayesian machine to consider every possible distribution,\\neach defined by a combination of µ and σ, and rank them by posterior plausibility. Posterior\\nplausibility provides a measure of the logical compatibility of each possible distribution with\\nthe data and model.\\nIn practice we’ll use approximations to the formal analysis. So we won’t really consider\\nevery possible value of µ and σ. But that won’t cost us anything in most cases. Instead the\\nthing to worry about is keeping in mind that the “estimate” here will be the entire posterior\\ndistribution, not any point within it. And as a result, the posterior distribution will be a\\ndistribution of Gaussian distributions. Yes, a distribution of distributions. If that doesn’t\\nmake sense yet, then that just means you are being honest with yourself. Hold on, work\\nhard, and it will make plenty of sense before long.\\n4.3.1. The data. The data contained in data(Howell1) are partial census data for the Dobe\\narea !Kung San, compiled from interviews conducted by Nancy Howell in the late 1960s.70\\nFor the non-anthropologists reading along, the !Kung San are the most famous foraging\\npopulation of the twentieth century, largely because of detailed quantitative studies by people\\nlike Howell. Load the data and place them into a convenient object with:\\nR code\\n4.7\\nlibrary(rethinking)\\ndata(Howell1)\\nd <- Howell1\\nWhat you have now is a data frame named simply d. I use the name d over and over again\\nin this book to refer to the data frame we are working with at the moment. I keep its name\\nshort to save you typing. A data frame is a special kind of object in R. It is a table with\\nnamed columns, corresponding to variables, and numbered rows, corresponding to individ-\\nual cases. In this example, the cases are individuals. Inspect the structure of the data frame,\\nthe same way you can inspect the structure of any symbol in R:\\nR code\\n4.8\\nstr( d )\\n'data.frame': 544 obs. of\\n4 variables:\\n$ height: num\\n152 140 137 157 145 ...\\n$ weight: num\\n47.8 36.5 31.9 53 41.3 ...\\n$ age\\n: num\\n63 63 65 41 51 35 32 27 19 54 ...\\n$ male\\n: int\\n1 0 0 1 0 1 0 1 0 1 ...\\nWe can also use rethinking’s precis summary function, which we’ll also use to summarize\\nposterior distributions later on:\\nR code\\n4.9\\nprecis( d )\\n'data.frame': 544 obs. of 4 variables:\\nmean\\nsd\\n5.5%\\n94.5%\\nhistogram\\nheight 138.26 27.60 81.11 165.74 ▁▁▁▁▁▁▁▂▁▇▇▅▁\\nweight\\n35.61 14.72\\n9.36\\n54.50 ▁▂▃▂▂▂▂▅▇▇▃▂▁\\n\"},\n",
       " {'index': 98,\n",
       "  'number': 80,\n",
       "  'content': \"80\\n4. GEOCENTRIC MODELS\\nage\\n29.34 20.75\\n1.00\\n66.13\\n▇▅▅▃▅▂▂▁▁\\nmale\\n0.47\\n0.50\\n0.00\\n1.00\\n▇▁▁▁▁▁▁▁▁▇\\nIf you cannot see the histograms on your system, use instead precis(d,hist=FALSE). This\\ndata frame contains four columns. Each column has 544 entries, so there are 544 individuals\\nin these data. Each individual has a recorded height (centimeters), weight (kilograms), age\\n(years), and “maleness” (0 indicating female and 1 indicating male).\\nWe’re going to work with just the height column, for the moment. The column con-\\ntaining the heights is really just a regular old R vector, the kind of list we have been working\\nwith in many of the code examples. You can access this vector by using its name:\\nR code\\n4.10\\nd$height\\nRead the symbol $ as extract, as in extract the column named height from the data frame d.\\nAll we want for now are heights of adults in the sample. The reason to filter out non-\\nadults for now is that height is strongly correlated with age, before adulthood. Later in the\\nchapter, I’ll ask you to tackle the age problem. But for now, better to postpone it. You can\\nfilter the data frame down to individuals of age 18 or greater with:\\nR code\\n4.11\\nd2 <- d[ d$age >= 18 , ]\\nWe’ll be working with the data frame d2 now. It should have 352 rows (individuals) in it.\\nOverthinking: Data frames and indexes. The square bracket notation used in the code above is index\\nnotation. It is very powerful, but also quite compact and confusing. The data frame d is a matrix, a\\nrectangular grid of values. You can access any value in the matrix with d[row,col], replacing row\\nand col with row and column numbers. If row or col are lists of numbers, then you get more than\\none row or column. If you leave the spot for row or col blank, then you get all of whatever you leave\\nblank. For example, d[ 3 , ] gives all columns at row 3. Typing d[,] just gives you the entire\\nmatrix, because it returns all rows and all columns.\\nSo what d[ d$age >= 18 , ] does is give you all of the rows in which d$age is greater-than-\\nor-equal-to 18. It also gives you all of the columns, because the spot after the comma is blank. The\\nresult is stored in d2, the new data frame containing only adults. With a little practice, you can use\\nthis square bracket index notion to perform custom searches of your data, much like performing a\\ndatabase query.\\nIt might seem like this whole data frame thing is unnecessary. If we’re working with only one\\ncolumn here, why bother with this d thing at all? You don’t have to use a data frame, as you can just\\npass raw vectors to every command we’ll use in this book. But keeping related variables in the same\\ndata frame is a convenience. Once we have more than one variable, and we wish to model one as a\\nfunction of the others, you’ll better see the value of the data frame. You won’t have to wait long. More\\ntechnically, a data frame is a special kind of list in R. So you access the individual variables with the\\nusual list “double bracket” notation, like d[[1]] for the first variable or d[['x']] for the variable\\nnamed x. Unlike regular lists, however, data frames force all variables to have the same length. That\\nisn’t always a good thing. In the second half of the book, we’ll start using ordinary list collections\\ninstead of data frames.\\n4.3.2. The model. Our goal is to model these values using a Gaussian distribution. First, go\\nahead and plot the distribution of heights, with dens(d2$height). These data look rather\\nGaussian in shape, as is typical of height data. This may be because height is a sum of many\\nsmall growth factors. As you saw at the start of the chapter, a distribution of sums tends\\n\"},\n",
       " {'index': 99,\n",
       "  'number': 81,\n",
       "  'content': '4.3. GAUSSIAN MODEL OF HEIGHT\\n81\\nto converge to a Gaussian distribution. Whatever the reason, adult heights from a single\\npopulation are nearly always approximately normal.\\nSo it’s reasonable for the moment to adopt the stance that the model should use a Gauss-\\nian distribution for the probability distribution of the data. But be careful about choosing\\nthe Gaussian distribution only when the plotted outcome variable looks Gaussian to you.\\nGawking at the raw data, to try to decide how to model them, is usually not a good idea. The\\ndata could be a mixture of different Gaussian distributions, for example, and in that case\\nyou won’t be able to detect the underlying normality just by eyeballing the outcome distribu-\\ntion. Furthermore, as mentioned earlier in this chapter, the empirical distribution needn’t\\nbe actually Gaussian in order to justify using a Gaussian probability distribution.\\nSo which Gaussian distribution? There are an infinite number of them, with an infinite\\nnumber of different means and standard deviations. We’re ready to write down the general\\nmodel and compute the plausibility of each combination of µ and σ. To define the heights\\nas normally distributed with a mean µ and standard deviation σ, we write:\\nhi ∼Normal(µ, σ)\\nIn many books you’ll see the same model written as hi ∼N(µ, σ), which means the same\\nthing. The symbol h refers to the list of heights, and the subscript i means each individual\\nelement of this list. It is conventional to use i because it stands for index. The index i takes on\\nrow numbers, and so in this example can take any value from 1 to 352 (the number of heights\\nin d2$height). As such, the model above is saying that all the golem knows about each\\nheight measurement is defined by the same normal distribution, with mean µ and standard\\ndeviation σ. Before long, those little i’s are going to show up on the right-hand side of the\\nmodel definition, and you’ll be able to see why we must bother with them. So don’t ignore\\nthe i, even if it seems like useless ornamentation right now.\\nRethinking: Independent and identically distributed. The short model above assumes that the val-\\nues hi are independent and identically distributed, abbreviated i.i.d., iid, or IID. You might even see\\nthe same model written:\\nhi\\niid∼Normal(µ, σ).\\n“iid” indicates that each value hi has the same probability function, independent of the other h values\\nand using the same parameters. A moment’s reflection tells us that this is often untrue. For example,\\nheights within families are correlated because of alleles shared through recent shared ancestry.\\nThe i.i.d. assumption doesn’t have to seem awkward, as long as you remember that probability is\\ninside the golem, not outside in the world. The i.i.d. assumption is about how the golem represents its\\nuncertainty. It is an epistemological assumption. It is not a physical assumption about the world, an\\nontological one. E. T. Jaynes (1922–1998) called this the mind projection fallacy, the mistake of con-\\nfusing epistemological claims with ontological claims.71 The point isn’t that epistemology trumps\\nreality, but that in ignorance of such correlations the best distribution may be i.i.d.72 This issue will\\nreturn in Chapter 10. Furthermore, there is a mathematical result known as de Finetti’s theorem that\\nsays values which are exchangeable can be approximated by mixtures of i.i.d. distributions. Col-\\nloquially, exchangeable values can be reordered. The practical impact is that “i.i.d.” cannot be read\\nliterally. There are also types of correlation that do little to the overall shape of a distribution, only\\naffecting the sequence in which values appear. For example, pairs of sisters have highly correlated\\nheights. But the overall distribution of female height remains normal. Markov chain Monte Carlo\\n(Chapter 9) exploits this, using highly correlated sequential samples to estimate most any distribution\\nwe like.\\n'},\n",
       " {'index': 100,\n",
       "  'number': 82,\n",
       "  'content': '82\\n4. GEOCENTRIC MODELS\\nTo complete the model, we’re going to need some priors. The parameters to be estimated\\nare both µ and σ, so we need a prior Pr(µ, σ), the joint prior probability for all parameters.\\nIn most cases, priors are specified independently for each parameter, which amounts to as-\\nsuming Pr(µ, σ) = Pr(µ) Pr(σ). Then we can write:\\nhi ∼Normal(µ, σ)\\n[likelihood]\\nµ ∼Normal(178, 20)\\n[µ prior]\\nσ ∼Uniform(0, 50)\\n[σ prior]\\nThe labels on the right are not part of the model, but instead just notes to help you keep track\\nof the purpose of each line. The prior for µ is a broad Gaussian prior, centered on 178 cm,\\nwith 95% of probability between 178 ± 40 cm.\\nWhy 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218 cm encom-\\npasses a huge range of plausible mean heights for human populations. So domain-specific\\ninformation has gone into this prior. Everyone knows something about human height and\\ncan set a reasonable and vague prior of this kind. But in many regression problems, as you’ll\\nsee later, using prior information is more subtle, because parameters don’t always have such\\nclear physical meaning.\\nWhatever the prior, it’s a very good idea to plot your priors, so you have a sense of the\\nassumption they build into the model. In this case:\\nR code\\n4.12\\ncurve( dnorm( x , 178 , 20 ) , from=100 , to=250 )\\nExecute that code yourself, to see that the golem is assuming that the average height (not\\neach individual height) is almost certainly between 140 cm and 220 cm. So this prior carries\\na little information, but not a lot. The σ prior is a truly flat prior, a uniform one, that functions\\njust to constrain σ to have positive probability between zero and 50 cm. View it with:\\nR code\\n4.13\\ncurve( dunif( x , 0 , 50 ) , from=-10 , to=60 )\\nA standard deviation like σ must be positive, so bounding it at zero makes sense. How should\\nwe pick the upper bound? In this case, a standard deviation of 50 cm would imply that 95%\\nof individual heights lie within 100 cm of the average height. That’s a very large range.\\nAll this talk is nice. But it’ll help to see what these priors imply about the distribution of\\nindividual heights. The prior predictive simulation is an essential part of your modeling.\\nOnce you’ve chosen priors for h, µ, and σ, these imply a joint prior distribution of individual\\nheights. By simulating from this distribution, you can see what your choices imply about\\nobservable height. This helps you diagnose bad choices. Lots of conventional choices are\\nindeed bad ones, and we’ll be able to see this through prior predictive simulations.\\nOkay, so how to do this? You can quickly simulate heights by sampling from the prior,\\nlike you sampled from the posterior back in Chapter 3. Remember, every posterior is also\\npotentially a prior for a subsequent analysis, so you can process priors just like posteriors.\\nR code\\n4.14\\nsample_mu <- rnorm( 1e4 , 178 , 20 )\\nsample_sigma <- runif( 1e4 , 0 , 50 )\\nprior_h <- rnorm( 1e4 , sample_mu , sample_sigma )\\ndens( prior_h )\\n'},\n",
       " {'index': 101,\n",
       "  'number': 83,\n",
       "  'content': '4.3. GAUSSIAN MODEL OF HEIGHT\\n83\\n0.000\\n0.010\\n0.020\\nmu\\nDensity\\nmu ~ dnorm( 178 , 20 )\\n100\\n178\\n250\\n0.000\\n0.010\\n0.020\\nsigma\\nDensity\\nsigma ~ dunif( 0 , 50 )\\n0\\n50\\n0.000\\n0.004\\n0.008\\n0.012\\nheight\\nDensity\\nh ~ dnorm(mu,sigma)\\n0\\n73\\n178\\n283\\n0.000\\n0.002\\n0.004\\nheight\\nDensity\\nh ~ dnorm(mu,sigma)\\n mu ~ dnorm(178,100)\\n-128\\n0\\n178\\n484\\nFigure 4.3. Prior predictive simulation for the height model. Top row:\\nPrior distributions for µ and σ. Bottom left: The prior predictive simulation\\nfor height, using the priors in the top row. Values at 3 standard deviations\\nshown on horizontal axis. Bottom right: Prior predictive simulation using\\nµ ∼Normal(178, 100).\\nThis density, as well as the individual densities for µ and σ, is shown in Figure 4.3. It dis-\\nplays a vaguely bell-shaped density with thick tails. It is the expected distribution of heights,\\naveraged over the prior. Notice that the prior probability distribution of height is not itself\\nGaussian. This is okay. The distribution you see is not an empirical expectation, but rather\\nthe distribution of relative plausibilities of different heights, before seeing the data.\\nPrior predictive simulation is very useful for assigning sensible priors, because it can be\\nquite hard to anticipate how priors influence the observable variables. As an example, con-\\nsider a much flatter and less informative prior for µ, like µ ∼Normal(178, 100). Priors with\\nsuch large standard deviations are quite common in Bayesian models, but they are hardly\\never sensible. Let’s use simulation again to see the implied heights:\\nR code\\n4.15\\nsample_mu <- rnorm( 1e4 , 178 , 100 )\\nprior_h <- rnorm( 1e4 , sample_mu , sample_sigma )\\ndens( prior_h )\\n'},\n",
       " {'index': 102,\n",
       "  'number': 84,\n",
       "  'content': '84\\n4. GEOCENTRIC MODELS\\nThe result is displayed in the lower right of Figure 4.3. Now the model, before seeing the\\ndata, expects 4% of people, those left of the dashed line, to have negative height. It also\\nexpects some giants. One of the tallest people in recorded history, Robert Pershing Wadlow\\n(1918–1940) stood 272 cm tall. In our prior predictive simulation, 18% of people (right of\\nsolid line) are taller than this.\\nDoes this matter? In this case, we have so much data that the silly prior is harmless. But\\nthat won’t always be the case. There are plenty of inference problems for which the data alone\\nare not sufficient, no matter how numerous. Bayes lets us proceed in these cases. But only\\nif we use our scientific knowledge to construct sensible priors. Using scientific knowledge\\nto build priors is not cheating. The important thing is that your prior not be based on the\\nvalues in the data, but only on what you know about the data before you see it.\\nRethinking: A farewell to epsilon. Some readers will have already met an alternative notation for a\\nGaussian linear model:\\nhi = µ + ϵi\\nϵi ∼Normal(0, σ)\\nThis is equivalent to the hi ∼Normal(µ, σ) form, with the ϵ standing in for the Gaussian density. But\\nthis ϵ form is poor form. The reason is that it does not usually generalize to other types of models.\\nThis means it won’t be possible to express non-Gaussian models using tricks like ϵ. Better to learn\\none system that does generalize.\\nOverthinking: Model definition to Bayes’ theorem again. It can help to see how the model definition\\non the previous page allows us to build up the posterior distribution. The height model, with its priors\\nfor µ and σ, defines this posterior distribution:\\nPr(µ, σ|h) =\\nQ\\ni Normal(hi|µ, σ)Normal(µ|178, 20)Uniform(σ|0, 50)\\nR R Q\\ni Normal(hi|µ, σ)Normal(µ|178, 20)Uniform(σ|0, 50)dµdσ\\nThis looks monstrous, but it’s the same creature as before. There are two new things that make it seem\\ncomplicated. The first is that there is more than one observation in h, so to get the joint likelihood\\nacross all the data, we have to compute the probability for each hi and then multiply all these likeli-\\nhoods together. The product on the right-hand side takes care of that. The second complication is\\nthe two priors, one for µ and one for σ. But these just stack up. In the grid approximation code in\\nthe section to follow, you’ll see the implications of this definition in the R code. Everything will be\\ncalculated on the log scale, so multiplication will become addition. But otherwise it’s just a matter of\\nexecuting Bayes’ theorem.\\n4.3.3. Grid approximation of the posterior distribution. Since this is the first Gaussian\\nmodel in the book, and indeed the first model with more than one parameter, it’s worth\\nquickly mapping out the posterior distribution through brute force calculations. This isn’t\\nthe approach I encourage in any other place, because it is laborious and computationally ex-\\npensive. Indeed, it is usually so impractical as to be essentially impossible. But as always, it\\nis worth knowing what the target actually looks like, before you start accepting approxima-\\ntions of it. A little later in this chapter, you’ll use quadratic approximation to estimate the\\nposterior distribution, and that’s the approach you’ll use for several chapters more. Once you\\nhave the samples you’ll produce in this subsection, you can compare them to the quadratic\\napproximation in the next.\\n'},\n",
       " {'index': 103,\n",
       "  'number': 85,\n",
       "  'content': '4.3. GAUSSIAN MODEL OF HEIGHT\\n85\\nUnfortunately, doing the calculations here requires some technical tricks that add little,\\nif any, conceptual insight. So I’m going to present the code here without explanation. You can\\nexecute it and keep going for now, but later return and follow the endnote for an explanation\\nof the algorithm.73 For now, here are the guts of the golem:\\nR code\\n4.16\\nmu.list <- seq( from=150, to=160 , length.out=100 )\\nsigma.list <- seq( from=7 , to=9 , length.out=100 )\\npost <- expand.grid( mu=mu.list , sigma=sigma.list )\\npost$LL <- sapply( 1:nrow(post) , function(i) sum(\\ndnorm( d2$height , post$mu[i] , post$sigma[i] , log=TRUE ) ) )\\npost$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +\\ndunif( post$sigma , 0 , 50 , TRUE )\\npost$prob <- exp( post$prod - max(post$prod) )\\nYou can inspect this posterior distribution, now residing in post$prob, using a variety of\\nplotting commands. You can get a simple contour plot with:\\nR code\\n4.17\\ncontour_xyz( post$mu , post$sigma , post$prob )\\nOr you can plot a simple heat map with:\\nR code\\n4.18\\nimage_xyz( post$mu , post$sigma , post$prob )\\nThe functions contour_xyz and image_xyz are both in the rethinking package.\\n4.3.4. Sampling from the posterior. To study this posterior distribution in more detail,\\nagain I’ll push the flexible approach of sampling parameter values from it. This works just\\nlike it did in Chapter 3, when you sampled values of p from the posterior distribution for\\nthe globe tossing example. The only new trick is that since there are two parameters, and\\nwe want to sample combinations of them, we first randomly sample row numbers in post\\nin proportion to the values in post$prob. Then we pull out the parameter values on those\\nrandomly sampled rows. This code will do it:\\nR code\\n4.19\\nsample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE ,\\nprob=post$prob )\\nsample.mu <- post$mu[ sample.rows ]\\nsample.sigma <- post$sigma[ sample.rows ]\\nYou end up with 10,000 samples, with replacement, from the posterior for the height data.\\nTake a look at these samples:\\nR code\\n4.20\\nplot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )\\nI reproduce this plot in Figure 4.4. Note that the function col.alpha is part of the rethink-\\ning R package. All it does is make colors transparent, which helps the plot in Figure 4.4\\nmore easily show density, where samples overlap. Adjust the plot to your tastes by playing\\naround with cex (character expansion, the size of the points), pch (plot character), and the\\n0.1 transparency value.\\n'},\n",
       " {'index': 104,\n",
       "  'number': 86,\n",
       "  'content': '86\\n4. GEOCENTRIC MODELS\\nFigure 4.4. Samples from the posterior dis-\\ntribution for the heights data.\\nThe density\\nof points is highest in the center, reflecting\\nthe most plausible combinations of µ and σ.\\nThere are many more ways for these parame-\\nter values to produce the data, conditional on\\nthe model.\\nNow that you have these samples, you can describe the distribution of confidence in each\\ncombination of µ and σ by summarizing the samples. Think of them like data and describe\\nthem, just like in Chapter 3. For example, to characterize the shapes of the marginal posterior\\ndensities of µ and σ, all we need to do is:\\nR code\\n4.21\\ndens( sample.mu )\\ndens( sample.sigma )\\nThe jargon “marginal” here means “averaging over the other parameters.” Execute the above\\ncode and inspect the plots. These densities are very close to being normal distributions.\\nAnd this is quite typical. As sample size increases, posterior densities approach the normal\\ndistribution. If you look closely, though, you’ll notice that the density for σ has a longer\\nright-hand tail. I’ll exaggerate this tendency a bit later, to show you that this condition is\\nvery common for standard deviation parameters.\\nTo summarize the widths of these densities with posterior compatibility intervals:\\nR code\\n4.22\\nPI( sample.mu )\\nPI( sample.sigma )\\nSince these samples are just vectors of numbers, you can compute any statistic from them\\nthat you could from ordinary data: mean, median, or quantile, for example.\\nOverthinking: Sample size and the normality of σ’s posterior. Before moving on to using quadratic\\napproximation (quap) as shortcut to all of this inference, it is worth repeating the analysis of the height\\ndata above, but now with only a fraction of the original data. The reason to do this is to demonstrate\\nthat, in principle, the posterior is not always so Gaussian in shape. There’s no trouble with the mean,\\nµ. For a Gaussian likelihood and a Gaussian prior on µ, the posterior distribution is always Gaussian\\nas well, regardless of sample size. It is the standard deviation σ that causes problems. So if you care\\nabout σ—often people do not—you do need to be careful of abusing the quadratic approximation.\\nThe deep reasons for the posterior of σ tending to have a long right-hand tail are complex. But\\na useful way to conceive of the problem is that variances must be positive. As a result, there must be\\nmore uncertainty about how big the variance (or standard deviation) is than about how small it is.\\n'},\n",
       " {'index': 105,\n",
       "  'number': 87,\n",
       "  'content': '4.3. GAUSSIAN MODEL OF HEIGHT\\n87\\nFor example, if the variance is estimated to be near zero, then you know for sure that it can’t be much\\nsmaller. But it could be a lot bigger.\\nLet’s quickly analyze only 20 of the heights from the height data to reveal this issue. To sample\\n20 random heights from the original list:\\nR code\\n4.23\\nd3 <- sample( d2$height , size=20 )\\nNow I’ll repeat all the code from the previous subsection, modified to focus on the 20 heights in d3\\nrather than the original data. I’ll compress all of the code together here.\\nR code\\n4.24\\nmu.list <- seq( from=150, to=170 , length.out=200 )\\nsigma.list <- seq( from=4 , to=20 , length.out=200 )\\npost2 <- expand.grid( mu=mu.list , sigma=sigma.list )\\npost2$LL <- sapply( 1:nrow(post2) , function(i)\\nsum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] ,\\nlog=TRUE ) ) )\\npost2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +\\ndunif( post2$sigma , 0 , 50 , TRUE )\\npost2$prob <- exp( post2$prod - max(post2$prod) )\\nsample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE ,\\nprob=post2$prob )\\nsample2.mu <- post2$mu[ sample2.rows ]\\nsample2.sigma <- post2$sigma[ sample2.rows ]\\nplot( sample2.mu , sample2.sigma , cex=0.5 ,\\ncol=col.alpha(rangi2,0.1) ,\\nxlab=\"mu\" , ylab=\"sigma\" , pch=16 )\\nAfter executing the code above, you’ll see another scatter plot of the samples from the posterior den-\\nsity, but this time you’ll notice a distinctly longer tail at the top of the cloud of points. You should\\nalso inspect the marginal posterior density for σ, averaging over µ, produced with:\\nR code\\n4.25\\ndens( sample2.sigma , norm.comp=TRUE )\\nThis code will also show a normal approximation with the same mean and variance. Now you can\\nsee that the posterior for σ is not Gaussian, but rather has a long tail towards higher values.\\n4.3.5. Finding the posterior distribution with quap. Now we leave grid approximation be-\\nhind and move on to one of the great engines of applied statistics, the quadratic approxi-\\nmation. Our interest in quadratic approximation, recall, is as a handy way to quickly make\\ninferences about the shape of the posterior. The posterior’s peak will lie at the maximum a\\nposteriori estimate (MAP), and we can get a useful image of the posterior’s shape by using\\nthe quadratic approximation of the posterior distribution at this peak.\\nTo build the quadratic approximation, we’ll use quap, a command in the rethinking\\npackage. The quap function works by using the model definition you were introduced to ear-\\nlier in this chapter. Each line in the definition has a corresponding definition in the form of\\nR code. The engine inside quap then uses these definitions to define the posterior probability\\nat each combination of parameter values. Then it can climb the posterior distribution and\\nfind the peak, its MAP. Finally, it estimates the quadratic curvature at the MAP to produce\\nan approximation of the posterior distribution. Remember: This procedure is very similar\\nto what many non-Bayesian procedures do, just without any priors.\\nLet’s begin by repeating the code to load the data and select out the adults:\\n'},\n",
       " {'index': 106,\n",
       "  'number': 88,\n",
       "  'content': '88\\n4. GEOCENTRIC MODELS\\nR code\\n4.26\\nlibrary(rethinking)\\ndata(Howell1)\\nd <- Howell1\\nd2 <- d[ d$age >= 18 , ]\\nNow we’re ready to define the model, using R’s formula syntax. The model definition in this\\ncase is just as before, but now we’ll repeat it with each corresponding line of R code shown\\non the right-hand margin:\\nhi ∼Normal(µ, σ)\\nheight ~ dnorm(mu,sigma)\\nµ ∼Normal(178, 20)\\nmu ~ dnorm(178,20)\\nσ ∼Uniform(0, 50)\\nsigma ~ dunif(0,50)\\nNow place the R code equivalents into an alist. Here’s an alist of the formulas above:\\nR code\\n4.27\\nflist <- alist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu ~ dnorm( 178 , 20 ) ,\\nsigma ~ dunif( 0 , 50 )\\n)\\nNote the commas at the end of each line, except the last. These commas separate each line\\nof the model definition.\\nFit the model to the data in the data frame d2 with:\\nR code\\n4.28\\nm4.1 <- quap( flist , data=d2 )\\nAfter executing this code, you’ll have a fit model stored in the symbol m4.1. Now take a look\\nat the posterior distribution:\\nR code\\n4.29\\nprecis( m4.1 )\\nmean\\nsd\\n5.5%\\n94.5%\\nmu\\n154.61 0.41 153.95 155.27\\nsigma\\n7.73 0.29\\n7.27\\n8.20\\nThese numbers provide Gaussian approximations for each parameter’s marginal distribution.\\nThis means the plausibility of each value of µ, after averaging over the plausibilities of each\\nvalue of σ, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4.\\nThe 5.5% and 94.5% quantiles are percentile interval boundaries, corresponding to an\\n89% compatibility interval. Why 89%? It’s just the default. It displays a quite wide interval,\\nso it shows a high-probability range of parameter values. If you want another interval, such\\nas the conventional and mindless 95%, you can use precis(m4.1,prob=0.95). But I don’t\\nrecommend 95% intervals, because readers will have a hard time not viewing them as signif-\\nicance tests. 89 is also a prime number, so if someone asks you to justify it, you can stare at\\nthem meaningfully and incant, “Because it is prime.” That’s no worse justification than the\\nconventional justification for 95%.\\n'},\n",
       " {'index': 107,\n",
       "  'number': 89,\n",
       "  'content': '4.3. GAUSSIAN MODEL OF HEIGHT\\n89\\nI encourage you to compare these 89% boundaries to the compatibility intervals from\\nthe grid approximation earlier. You’ll find that they are almost identical. When the posterior\\nis approximately Gaussian, then this is what you should expect.\\nOverthinking: Start values for quap. quap estimates the posterior by climbing it like a hill. To do\\nthis, it has to start climbing someplace, at some combination of parameter values. Unless you tell it\\notherwise, quap starts at random values sampled from the prior. But it’s also possible to specify a\\nstarting value for any parameter in the model. In the example in the previous section, that means the\\nparameters µ and σ. Here’s a good list of starting values in this case:\\nR code\\n4.30\\nstart <- list(\\nmu=mean(d2$height),\\nsigma=sd(d2$height)\\n)\\nm4.1 <- quap( flist , data=d2 , start=start )\\nThese start values are good guesses of the rough location of the MAP values.\\nNote that the list of start values is a regular list, not an alist like the formula list is. The two\\nfunctions alist and list do the same basic thing: allow you to make a collection of arbitrary R\\nobjects. They differ in one important respect: list evaluates the code you embed inside it, while\\nalist does not. So when you define a list of formulas, you should use alist, so the code isn’t ex-\\necuted. But when you define a list of start values for parameters, you should use list, so that code\\nlike mean(d2$height) will be evaluated to a numeric value.\\nThe priors we used before are very weak, both because they are nearly flat and because\\nthere is so much data. So I’ll splice in a more informative prior for µ, so you can see the\\neffect. All I’m going to do is change the standard deviation of the prior to 0.1, so it’s a very\\nnarrow prior. I’ll also build the formula right into the call to quap this time.\\nR code\\n4.31\\nm4.2 <- quap(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu ~ dnorm( 178 , 0.1 ) ,\\nsigma ~ dunif( 0 , 50 )\\n) , data=d2 )\\nprecis( m4.2 )\\nmean\\nsd\\n5.5%\\n94.5%\\nmu\\n177.86 0.10 177.70 178.02\\nsigma\\n24.52 0.93\\n23.03\\n26.00\\nNotice that the estimate for µ has hardly moved off the prior. The prior was very concentrated\\naround 178. So this is not surprising. But also notice that the estimate for σ has changed quite\\na lot, even though we didn’t change its prior at all. Once the golem is certain that the mean\\nis near 178—as the prior insists—then the golem has to estimate σ conditional on that fact.\\nThis results in a different posterior for σ, even though all we changed is prior information\\nabout the other parameter.\\n4.3.6. Sampling from a quap. The above explains how to get a quadratic approximation of\\nthe posterior, using quap. But how do you then get samples from the quadratic approxi-\\nmate posterior distribution? The answer is rather simple, but non-obvious, and it requires\\n'},\n",
       " {'index': 108,\n",
       "  'number': 90,\n",
       "  'content': '90\\n4. GEOCENTRIC MODELS\\nrecognizing that a quadratic approximation to a posterior distribution with more than one\\nparameter dimension—µ and σ each contribute one dimension—is just a multi-dimensional\\nGaussian distribution.\\nAs a consequence, when R constructs a quadratic approximation, it calculates not only\\nstandard deviations for all parameters, but also the covariances among all pairs of param-\\neters. Just like a mean and standard deviation (or its square, a variance) are sufficient to\\ndescribe a one-dimensional Gaussian distribution, a list of means and a matrix of variances\\nand covariances are sufficient to describe a multi-dimensional Gaussian distribution. To see\\nthis matrix of variances and covariances, for model m4.1, use:\\nR code\\n4.32\\nvcov( m4.1 )\\nmu\\nsigma\\nmu\\n0.1697395865 0.0002180593\\nsigma 0.0002180593 0.0849057933\\nThe above is a variance-covariance matrix. It is the multi-dimensional glue of a qua-\\ndratic approximation, because it tells us how each parameter relates to every other param-\\neter in the posterior distribution. A variance-covariance matrix can be factored into two\\nelements: (1) a vector of variances for the parameters and (2) a correlation matrix that tells\\nus how changes in any parameter lead to correlated changes in the others. This decomposi-\\ntion is usually easier to understand. So let’s do that now:\\nR code\\n4.33\\ndiag( vcov( m4.1 ) )\\ncov2cor( vcov( m4.1 ) )\\nmu\\nsigma\\n0.16973959 0.08490579\\nmu\\nsigma\\nmu\\n1.000000000 0.001816412\\nsigma 0.001816412 1.000000000\\nThe two-element vector in the output is the list of variances. If you take the square root of this\\nvector, you get the standard deviations that are shown in precis output. The two-by-two\\nmatrix in the output is the correlation matrix. Each entry shows the correlation, bounded\\nbetween −1 and +1, for each pair of parameters. The 1’s indicate a parameter’s correlation\\nwith itself. If these values were anything except 1, we would be worried. The other entries\\nare typically closer to zero, and they are very close to zero in this example. This indicates\\nthat learning µ tells us nothing about σ and likewise that learning σ tells us nothing about\\nµ. This is typical of simple Gaussian models of this kind. But it is quite rare more generally,\\nas you’ll see in later chapters.\\nOkay, so how do we get samples from this multi-dimensional posterior? Now instead\\nof sampling single values from a simple Gaussian distribution, we sample vectors of values\\nfrom a multi-dimensional Gaussian distribution. The rethinking package provides a con-\\nvenience function to do exactly that:\\nR code\\n4.34\\nlibrary(rethinking)\\npost <- extract.samples( m4.1 , n=1e4 )\\n'},\n",
       " {'index': 109,\n",
       "  'number': 91,\n",
       "  'content': '4.4. LINEAR PREDICTION\\n91\\nhead(post)\\nmu\\nsigma\\n1 155.0031 7.443893\\n2 154.0347 7.771255\\n3 154.9157 7.822178\\n4 154.4252 7.530331\\n5 154.5307 7.655490\\n6 155.1772 7.974603\\nYou end up with a data frame, post, with 10,000 (1e4) rows and two columns, one column\\nfor µ and one for σ. Each value is a sample from the posterior, so the mean and standard\\ndeviation of each column will be very close to the MAP values from before. You can confirm\\nthis by summarizing the samples:\\nR code\\n4.35\\nprecis(post)\\nquap posterior: 10000 samples from m4.1\\nmean\\nsd\\n5.5%\\n94.5%\\nhistogram\\nmu\\n154.61 0.41 153.95 155.27\\n▁▁▁▅▇▂▁▁\\nsigma\\n7.72 0.29\\n7.26\\n8.18 ▁▁▁▂▅▇▇▃▁▁▁▁\\nCompare these values to the output from precis(m4.1). And you can use plot(post)\\nto see how much they resemble the samples from the grid approximation in Figure 4.4\\n(page 86). These samples also preserve the covariance between µ and σ. This hardly matters\\nright now, because µ and σ don’t covary at all in this model. But once you add a predictor\\nvariable to your model, covariance will matter a lot.\\nOverthinking: Under the hood with multivariate sampling. The function extract.samples is\\nfor convenience. It is just running a simple simulation of the sort you conducted near the end of\\nChapter 3. Here’s a peak at the motor. The work is done by a multi-dimensional version of rnorm,\\nmvrnorm. The function rnorm simulates random Gaussian values, while mvrnorm simulates random\\nvectors of multivariate Gaussian values. Here’s how to use it to do what extract.samples does:\\nR code\\n4.36\\nlibrary(MASS)\\npost <- mvrnorm( n=1e4 , mu=coef(m4.1) , Sigma=vcov(m4.1) )\\nYou don’t usually need to use mvrnorm directly like this, but sometimes you want to simulate multi-\\nvariate Gaussian outcomes. In that case, you’ll need to access mvrnorm directly. And of course it’s\\nalways good to know a little about how the machine operates. Later on, we’ll work with posterior\\ndistributions that cannot be correctly approximated this way.\\n4.4. Linear prediction\\nWhat we’ve done above is a Gaussian model of height in a population of adults. But it\\ndoesn’t really have the usual feel of “regression” to it. Typically, we are interested in modeling\\nhow an outcome is related to some other variable, a predictor variable. If the predictor\\nvariable has any statistical association with the outcome variable, then we can use it to predict\\n'},\n",
       " {'index': 110,\n",
       "  'number': 92,\n",
       "  'content': '92\\n4. GEOCENTRIC MODELS\\nthe outcome. When the predictor variable is built inside the model in a particular way, we’ll\\nhave linear regression.\\nSo now let’s look at how height in these Kalahari foragers (the outcome variable) covaries\\nwith weight (the predictor variable). This isn’t the most thrilling scientific question, I know.\\nBut it is an easy relationship to start with, and if it seems dull, it’s because you don’t have\\na theory about growth and life history in mind. If you did, it would be thrilling. We’ll try\\nlater on to add some of that thrill, when we reconsider this example from a more causal per-\\nspective. Right now, I ask only that you focus on the mechanics of estimating an association\\nbetween two variables.\\nGo ahead and plot adult height and weight against one another:\\nR code\\n4.37\\nlibrary(rethinking)\\ndata(Howell1); d <- Howell1; d2 <- d[ d$age >= 18 , ]\\nplot( d2$height ~ d2$weight )\\nThe resulting plot is not shown here. You really should do it yourself. Once you can see\\nthe plot, you’ll see that there’s obviously a relationship: Knowing a person’s weight helps you\\npredict height.\\nTo make this vague observation into a more precise quantitative model that relates values\\nof weight to plausible values of height, we need some more technology. How do we take\\nour Gaussian model from the previous section and incorporate predictor variables?\\nRethinking: What is “regression”? Many diverse types of models are called “regression.” The term\\nhas come to mean using one or more predictor variables to model the distribution of one or more\\noutcome variables. The original use of term, however, arose from anthropologist Francis Galton’s\\n(1822–1911) observation that the sons of tall and short men tended to be more similar to the popula-\\ntion mean, hence regression to the mean.74\\nThe causal reasons for regression to the mean are diverse. In the case of height, the causal expla-\\nnation is a key piece of the foundation of population genetics. But this phenomenon arises statistically\\nwhenever individual measurements are assigned a common distribution, leading to shrinkage as each\\nmeasurement informs the others. In the context of Galton’s height data, attempting to predict each\\nson’s height on the basis of only his father’s height is folly. Better to use the population of fathers.\\nThis leads to a prediction for each son which is similar to each father but “shrunk” towards the over-\\nall mean. Such predictions are routinely better. This same regression/shrinkage phenomenon applies\\nat higher levels of abstraction and forms one basis of multilevel modeling (Chapter 13).\\n4.4.1. The linear model strategy. The strategy is to make the parameter for the mean of\\na Gaussian distribution, µ, into a linear function of the predictor variable and other, new\\nparameters that we invent. This strategy is often simply called the linear model. The linear\\nmodel strategy instructs the golem to assume that the predictor variable has a constant and\\nadditive relationship to the mean of the outcome. The golem then computes the posterior\\ndistribution of this constant relationship.\\nWhat this means, recall, is that the machine considers every possible combination of the\\nparameter values. With a linear model, some of the parameters now stand for the strength\\nof association between the mean of the outcome, µ, and the value of some other variable.\\nFor each combination of values, the machine computes the posterior probability, which is\\na measure of relative plausibility, given the model and data. So the posterior distribution\\nranks the infinite possible combinations of parameter values by their logical plausibility. As\\n'},\n",
       " {'index': 111,\n",
       "  'number': 93,\n",
       "  'content': '4.4. LINEAR PREDICTION\\n93\\na result, the posterior distribution provides relative plausibilities of the different possible\\nstrengths of association, given the assumptions you programmed into the model. We ask\\nthe golem: “Consider all the lines that relate one variable to the other. Rank all of these lines\\nby plausibility, given these data.” The golem answers with a posterior distribution.\\nHere’s how it works, in the simplest case of only one predictor variable. We’ll wait until\\nthe next chapter to confront more than one predictor. Recall the basic Gaussian model:\\nhi ∼Normal(µ, σ)\\n[likelihood]\\nµ ∼Normal(178, 20)\\n[µ prior]\\nσ ∼Uniform(0, 50)\\n[σ prior]\\nNow how do we get weight into a Gaussian model of height? Let x be the name for the\\ncolumn of weight measurements, d2$weight. Let the average of the x values be ¯x, “ex bar”.\\nNow we have a predictor variable x, which is a list of measures of the same length as h. To get\\nweight into the model, we define the mean µ as a function of the values in x. This is what it\\nlooks like, with explanation to follow:\\nhi ∼Normal(µi, σ)\\n[likelihood]\\nµi = α + β(xi −¯x)\\n[linear model]\\nα ∼Normal(178, 20)\\n[α prior]\\nβ ∼Normal(0, 10)\\n[β prior]\\nσ ∼Uniform(0, 50)\\n[σ prior]\\nAgain, I’ve labeled each line on the right-hand side by the type of definition it encodes. We’ll\\ndiscuss each in turn.\\n4.4.1.1. Probability of the data. Let’s begin with just the probability of the observed\\nheight, the first line of the model. This is nearly identical to before, except now there is a\\nlittle index i on the µ as well as the h. You can read hi as “each h” and µi as “each µ.” The\\nmean µ now depends upon unique values on each row i. So the little i on µi indicates that\\nthe mean depends upon the row.\\n4.4.1.2. Linear model. The mean µ is no longer a parameter to be estimated. Rather, as\\nseen in the second line of the model, µi is constructed from other parameters, α and β, and\\nthe observed variable x. This line is not a stochastic relationship—there is no ∼in it, but\\nrather an = in it—because the definition of µi is deterministic. That is to say that, once we\\nknow α and β and xi, we know µi with certainty.\\nThe value xi is just the weight value on row i. It refers to the same individual as the\\nheight value, hi, on the same row. The parameters α and β are more mysterious. Where did\\nthey come from? We made them up. The parameters µ and σ are necessary and sufficient to\\ndescribe a Gaussian distribution. But α and β are instead devices we invent for manipulating\\nµ, allowing it to vary systematically across cases in the data.\\nYou’ll be making up all manner of parameters as your skills improve. One way to under-\\nstand these made-up parameters is to think of them as targets of learning. Each parameter is\\nsomething that must be described in the posterior distribution. So when you want to know\\nsomething about the data, you ask your golem by inventing a parameter for it. This will make\\nmore and more sense as you progress. Here’s how it works in this context. The second line\\n'},\n",
       " {'index': 112,\n",
       "  'number': 94,\n",
       "  'content': '94\\n4. GEOCENTRIC MODELS\\nof the model definition is just:\\nµi = α + β(xi −¯x)\\nWhat this tells the regression golem is that you are asking two questions about the mean of\\nthe outcome.\\n(1) What is the expected height when xi = ¯x? The parameter α answers this question,\\nbecause when xi = ¯x, µi = α. For this reason, α is often called the intercept. But we\\nshould think not in terms of some abstract line, but rather in terms of the meaning\\nwith respect to the observable variables.\\n(2) What is the change in expected height, when xi changes by 1 unit? The parameter\\nβ answers this question. It is often called a “slope,” again because of the abstract\\nline. Better to think of it as a rate of change in expectation.\\nJointly these two parameters ask the golem to find a line that relates x to h, a line that passes\\nthrough α when xi = ¯x and has slope β. That is a task that golems are very good at. It’s up\\nto you, though, to be sure it’s a good question.\\nRethinking: Nothing special or natural about linear models. Note that there’s nothing special about\\nthe linear model, really. You can choose a different relationship between α and β and µ. For example,\\nthe following is a perfectly legitimate definition for µi:\\nµi = α exp(−βxi)\\nThis does not define a linear regression, but it does define a regression model. The linear relationship\\nwe are using instead is conventional, but nothing requires that you use it. It is very common in some\\nfields, like ecology and demography, to use functional forms for µ that come from theory, rather than\\nthe geocentrism of linear models. Models built out of substantive theory can dramatically outperform\\nlinear models of the same phenomena.75 We’ll revisit this point later in the book.\\nOverthinking: Units and regression models. Readers who had a traditional training in physical\\nsciences will know how to carry units through equations of this kind. For their benefit, here’s the\\nmodel again (omitting priors for brevity), now with units of each symbol added.\\nhicm ∼Normal(µicm, σcm)\\nµicm = αcm + β cm\\nkg (xikg −¯xkg)\\nSo you can see that β must have units of cm/kg in order for the mean µi to have units of cm. One of\\nthe facts that labeling with units clears up is that a parameter like β is a kind of rate—centimeters per\\nkilogram. There’s also a tradition called dimensionless analysis that advocates constructing variables\\nso that they are unit-less ratios. In this context, for example, we might divide height by a reference\\nheight, removing its units. Measurement scales are arbitrary human constructions, and sometimes\\nthe unit-less analysis is more natural and general.\\n4.4.1.3. Priors. The remaining lines in the model define distributions for the unobserved\\nvariables. These variables are commonly known as parameters, and their distributions as pri-\\nors. There are three parameters: α, β, and σ. You’ve seen priors for α and σ before, although\\nα was called µ back then.\\nThe prior for β deserves explanation. Why have a Gaussian prior with mean zero? This\\nprior places just as much probability below zero as it does above zero, and when β = 0,\\n'},\n",
       " {'index': 113,\n",
       "  'number': 95,\n",
       "  'content': '4.4. LINEAR PREDICTION\\n95\\nFigure 4.5. Prior predictive simulation for the height and weight model.\\nLeft: Simulation using the β ∼Normal(0, 10) prior. Right: A more sensible\\nlog(β) ∼Normal(0, 1) prior.\\nweight has no relationship to height. To figure out what this prior implies, we have to simulate\\nthe prior predictive distribution. There is no other reliable way to understand.\\nThe goal is to simulate heights from the model, using only the priors. First, let’s consider\\na range of weight values to simulate over. The range of observed weights will do fine. Then\\nwe need to simulate a bunch of lines, the lines implied by the priors for α and β. Here’s how\\nto do it, setting a seed so you can reproduce it exactly:\\nR code\\n4.38\\nset.seed(2971)\\nN <- 100\\n# 100 lines\\na <- rnorm( N , 178 , 20 )\\nb <- rnorm( N , 0 , 10 )\\nNow we have 100 pairs of α and β values. Now to plot the lines:\\nR code\\n4.39\\nplot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) ,\\nxlab=\"weight\" , ylab=\"height\" )\\nabline( h=0 , lty=2 )\\nabline( h=272 , lty=1 , lwd=0.5 )\\nmtext( \"b ~ dnorm(0,10)\" )\\nxbar <- mean(d2$weight)\\nfor ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,\\nfrom=min(d2$weight) , to=max(d2$weight) , add=TRUE ,\\ncol=col.alpha(\"black\",0.2) )\\nThe result is displayed in Figure 4.5. For reference, I’ve added a dashed line at zero—no one\\nis shorter than zero—and the “Wadlow” line at 272 cm for the world’s tallest person. The\\npattern doesn’t look like any human population at all. It essentially says that the relationship\\n'},\n",
       " {'index': 114,\n",
       "  'number': 96,\n",
       "  'content': '96\\n4. GEOCENTRIC MODELS\\nbetween weight and height could be absurdly positive or negative. Before we’ve even seen\\nthe data, this is a bad model. Can we do better?\\nWe can do better immediately. We know that average height increases with average\\nweight, at least up to a point. Let’s try restricting it to positive values. The easiest way to do\\nthis is to define the prior as Log-Normal instead. If you aren’t accustomed to playing with\\nlogarithms, that’s okay. There’s more detail in the box at the end of this section.\\nDefining β as Log-Normal(0,1) means to claim that the logarithm of β has a Normal(0,1)\\ndistribution. Plainly:\\nβ ∼Log-Normal(0, 1)\\nR provides the dlnorm and rlnorm densities for working with log-normal distributions. You\\ncan simulate this relationship to see what this means for β:\\nR code\\n4.40\\nb <- rlnorm( 1e4 , 0 , 1 )\\ndens( b , xlim=c(0,5) , adj=0.1 )\\nIf the logarithm of β is normal, then β itself is strictly positive. The reason is that exp(x)\\nis greater than zero for any real number x. This is the reason that Log-Normal priors are\\ncommonplace. They are an easy way to enforce positive relationships. So what does this\\nearn us? Do the prior predictive simulation again, now with the Log-Normal prior:\\nR code\\n4.41\\nset.seed(2971)\\nN <- 100\\n# 100 lines\\na <- rnorm( N , 178 , 20 )\\nb <- rlnorm( N , 0 , 1 )\\nPlotting as before produces the right-hand plot in Figure 4.5. This is much more sensible.\\nThere is still a rare impossible relationship. But nearly all lines in the joint prior for α and β\\nare now within human reason.\\nWe’re fussing about this prior, even though as you’ll see in the next section there is so\\nmuch data in this example that the priors end up not mattering. We fuss for two reasons.\\nFirst, there are many analyses in which no amount of data makes the prior irrelevant. In such\\ncases, non-Bayesian procedures are no better off. They also depend upon structural features\\nof the model. Paying careful attention to those features is essential. Second, thinking about\\nthe priors helps us develop better models, maybe even eventually going beyond geocentrism.\\nRethinking: What’s the correct prior? People commonly ask what the correct prior is for a given\\nanalysis. The question sometimes implies that for any given set of data, there is a uniquely correct\\nprior that must be used, or else the analysis will be invalid. This is a mistake. There is no more a\\nuniquely correct prior than there is a uniquely correct likelihood. Statistical models are machines for\\ninference. Many machines will work, but some work better than others. Priors can be wrong, but\\nonly in the same sense that a kind of hammer can be wrong for building a table.\\nIn choosing priors, there are simple guidelines to get you started. Priors encode states of infor-\\nmation before seeing data. So priors allow us to explore the consequences of beginning with different\\ninformation. In cases in which we have good prior information that discounts the plausibility of some\\nparameter values, like negative associations between height and weight, we can encode that informa-\\ntion directly into priors. When we don’t have such information, we still usually know enough about\\nthe plausible range of values. And you can vary the priors and repeat the analysis in order to study\\n'},\n",
       " {'index': 115,\n",
       "  'number': 97,\n",
       "  'content': \"4.4. LINEAR PREDICTION\\n97\\nhow different states of initial information influence inference. Frequently, there are many reasonable\\nchoices for a prior, and all of them produce the same inference. And conventional Bayesian priors\\nare conservative, relative to conventional non-Bayesian approaches. We’ll see how this conservatism\\narises in Chapter 7.\\nMaking choices tends to make novices nervous. There’s an illusion sometimes that default pro-\\ncedures are more objective than procedures that require user choice, such as choosing priors. If that’s\\ntrue, then all “objective” means is that everyone does the same thing. It carries no guarantees of\\nrealism or accuracy.\\nRethinking: Prior predictive simulation and p-hacking A serious problem in contemporary applied\\nstatistics is “p-hacking,” the practice of adjusting the model and the data to achieve a desired result.\\nThe desired result is usually a p-value less then 5%. The problem is that when the model is adjusted in\\nlight of the observed data, then p-values no longer retain their original meaning. False results are to\\nbe expected. We don’t pay any attention to p-values in this book. But the danger remains, if we choose\\nour priors conditional on the observed sample, just to get some desired result. The procedure we’ve\\nperformed in this chapter is to choose priors conditional on pre-data knowledge of the variables—\\ntheir constraints, ranges, and theoretical relationships. This is why the actual data are not shown in\\nthe earlier section. We are judging our priors against general facts, not the sample. We’ll look at how\\nthe model performs against the real data next.\\n4.4.2. Finding the posterior distribution. The code needed to approximate the posterior is\\na straightforward modification of the kind of code you’ve already seen. All we have to do\\nis incorporate our new model for the mean into the model specification inside quap and be\\nsure to add a prior for the new parameter, β. Let’s repeat the model definition, now with the\\ncorresponding R code on the right-hand side:\\nhi ∼Normal(µi, σ)\\nheight ~ dnorm(mu,sigma)\\nµi = α + β(xi −¯x)\\nmu <- a + b*(weight-xbar)\\nα ∼Normal(178, 20)\\na ~ dnorm(178,20)\\nβ ∼Log-Normal(0, 1)\\nb ~ dlnorm(0,1)\\nσ ∼Uniform(0, 50)\\nsigma ~ dunif(0,50)\\nNotice that the linear model, in the R code on the right-hand side, uses the R assignment\\noperator, <-, even though the mathematical definition uses the symbol =. This is a code\\nconvention shared by several Bayesian model fitting engines, so it’s worth getting used to the\\nswitch. You just have to remember to use <- instead of = when defining a linear model.\\nThat’s it. The above allows us to build the posterior approximation:\\nR code\\n4.42\\n# load data again, since it's a long way back\\nlibrary(rethinking)\\ndata(Howell1); d <- Howell1; d2 <- d[ d$age >= 18 , ]\\n# define the average weight, x-bar\\nxbar <- mean(d2$weight)\\n# fit model\\n\"},\n",
       " {'index': 116,\n",
       "  'number': 98,\n",
       "  'content': '98\\n4. GEOCENTRIC MODELS\\nm4.3 <- quap(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a + b*( weight - xbar ) ,\\na ~ dnorm( 178 , 20 ) ,\\nb ~ dlnorm( 0 , 1 ) ,\\nsigma ~ dunif( 0 , 50 )\\n) , data=d2 )\\nRethinking: Everything that depends upon parameters has a posterior distribution. In the model\\nabove, the parameter µ is no longer a parameter, since it has become a function of the parameters α\\nand β. But since the parameters α and β have a joint posterior, so too does µ. Later in the chapter,\\nyou’ll work directly with the posterior distribution of µ, even though it’s not a parameter anymore.\\nSince parameters are uncertain, everything that depends upon them is also uncertain. This includes\\nstatistics like µ, as well as model-based predictions, measures of fit, and everything else that uses pa-\\nrameters. By working with samples from the posterior, all you have to do to account for posterior\\nuncertainty in any quantity is to compute that quantity for each sample from the posterior. The result-\\ning quantities, one for each posterior sample, will approximate the quantity’s posterior distribution.\\nOverthinking: Logs and exps, oh my. My experience is that many natural and social scientists have\\nnaturally forgotten whatever they once knew about logarithms. Logarithms appear all the time in\\napplied statistics. You can usefully think of y = log(x) as assigning to y the order of magnitude of x.\\nThe function x = exp(y) is the reverse, turning a magnitude into a value. These definitions will make\\na mathematician shriek. But much of our computational work relies only on these intuitions.\\nThese definitions allow the Log-Normal prior for β to be coded another way. Instead of defining a\\nparameter β, we define a parameter that is the logarithm of β and then assign it a normal distribution.\\nThen we can reverse the logarithm inside the linear model. It looks like this:\\nR code\\n4.43\\nm4.3b <- quap(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a + exp(log_b)*( weight - xbar ),\\na ~ dnorm( 178 , 20 ) ,\\nlog_b ~ dnorm( 0 , 1 ) ,\\nsigma ~ dunif( 0 , 50 )\\n) , data=d2 )\\nNote the exp(log_b) in the definition of mu. This is the same model as m4.3. It will make the same\\npredictions. But instead of β in the posterior distribution, you get log(β). It is easy to translate\\nbetween the two, because β = exp(log(β)). In code form: b <- exp(log_b).\\n4.4.3. Interpreting the posterior distribution. One trouble with statistical models is that\\nthey are hard to understand. Once you’ve fit the model, it can only report posterior distribu-\\ntion. This is the right answer to the question you asked. But it’s your responsibility to process\\nthe answer and make sense of it.\\nThere are two broad categories of processing: (1) reading tables and (2) plotting simu-\\nlations. For some simple questions, it’s possible to learn a lot just from tables of marginal\\n'},\n",
       " {'index': 117,\n",
       "  'number': 99,\n",
       "  'content': '4.4. LINEAR PREDICTION\\n99\\nvalues. But most models are very hard to understand from tables of numbers alone. A major\\ndifficulty with tables alone is their apparent simplicity compared to the complexity of the\\nmodel and data that generated them. Once you have more than a couple of parameters in a\\nmodel, it is very hard to figure out from numbers alone how all of them act to influence pre-\\ndiction. This is also the reason we simulate from priors. Once you begin adding interaction\\nterms (Chapter 8) or polynomials (later in this chapter), it may not even be possible to guess\\nthe direction of influence a predictor variable has on an outcome.\\nSo throughout this book, I emphasize plotting posterior distributions and posterior pre-\\ndictions, instead of attempting to understand a table. Plotting the implications of your mod-\\nels will allow you to inquire about things that are hard to read from tables:\\n(1) Whether or not the model fitting procedure worked correctly\\n(2) The absolute magnitude, rather than merely relative magnitude, of a relationship\\nbetween outcome and predictor\\n(3) The uncertainty surrounding an average relationship\\n(4) The uncertainty surrounding the implied predictions of the model, as these are\\ndistinct from mere parameter uncertainty\\nIn addition, once you get the hang of processing posterior distributions into plots, you can\\nask any question you can think of, for any model type. And readers of your results will\\nappreciate a figure much more than they will a table of estimates.\\nSo in the remainder of this section, I first spend a little time talking about tables of esti-\\nmates. Then I move on to show how to plot estimates that always incorporate information\\nfrom the full posterior distribution, including correlations among parameters.\\nRethinking: What do parameters mean? A basic issue with interpreting model-based estimates is\\nin knowing the meaning of parameters. There is no consensus about what a parameter means, how-\\never, because different people take different philosophical stances towards models, probability, and\\nprediction. The perspective in this book is a common Bayesian perspective: Posterior probabilities\\nof parameter values describe the relative compatibility of different states of the world with the data, ac-\\ncording to the model. These are small world (Chapter 2) numbers. So reasonable people may disagree\\nabout the large world meaning, and the details of those disagreements depend strongly upon context.\\nSuch disagreements are productive, because they lead to model criticism and revision, something that\\ngolems cannot do for themselves. In later chapters, you’ll see that parameters can refer to observable\\nquantities—data—as well as unobservable values. This makes parameters even more useful and their\\ninterpretation even more context dependent.\\n4.4.3.1. Tables of marginal distributions. With the new linear regression trained on the\\nKalahari data, we inspect the marginal posterior distributions of the parameters:\\nR code\\n4.44\\nprecis( m4.3 )\\nmean\\nsd\\n5.5%\\n94.5%\\na\\n154.60 0.27 154.17 155.03\\nb\\n0.90 0.04\\n0.84\\n0.97\\nsigma\\n5.07 0.19\\n4.77\\n5.38\\nThe first row gives the quadratic approximation for α, the second the approximation for β,\\nand the third approximation for σ. Let’s try to make some sense of them.\\n'},\n",
       " {'index': 118,\n",
       "  'number': 100,\n",
       "  'content': '100\\n4. GEOCENTRIC MODELS\\nLet’s focus on b (β), because it’s the new parameter. Since β is a slope, the value 0.90\\ncan be read as a person 1 kg heavier is expected to be 0.90 cm taller. 89% of the posterior\\nprobability lies between 0.84 and 0.97. That suggests that β values close to zero or greatly\\nabove one are highly incompatible with these data and this model. It is most certainly not\\nevidence that the relationship between weight and height is linear, because the model only\\nconsidered lines. It just says that, if you are committed to a line, then lines with a slope\\naround 0.9 are plausible ones.\\nRemember, the numbers in the default precis output aren’t sufficient to describe the\\nquadratic posterior completely. For that, we also require the variance-covariance matrix.\\nYou can see the covariances among the parameters with vcov:\\nR code\\n4.45\\nround( vcov( m4.3 ) , 3 )\\na\\nb sigma\\na\\n0.073 0.000 0.000\\nb\\n0.000 0.002 0.000\\nsigma 0.000 0.000 0.037\\nVery little covariation among the parameters in this case. Using pairs(m4.3) shows both\\nthe marginal posteriors and the covariance. In the practice problems at the end of the chapter,\\nyou’ll see that the lack of covariance among the parameters results from centering.\\n4.4.3.2. Plotting posterior inference against the data. It’s almost always much more use-\\nful to plot the posterior inference against the data. Not only does plotting help in interpret-\\ning the posterior, but it also provides an informal check on model assumptions. When the\\nmodel’s predictions don’t come close to key observations or patterns in the plotted data,\\nthen you might suspect the model either did not fit correctly or is rather badly specified. But\\neven if you only treat plots as a way to help in interpreting the posterior, they are invaluable.\\nFor simple models like this one, it is possible (but not always easy) to just read the table of\\nnumbers and understand what the model says. But for even slightly more complex models,\\nespecially those that include interaction effects (Chapter 8), interpreting posterior distribu-\\ntions is hard. Combine with this the problem of incorporating the information in vcov into\\nyour interpretations, and the plots are irreplaceable.\\nWe’re going to start with a simple version of that task, superimposing just the posterior\\nmean values over the height and weight data. Then we’ll slowly add more and more infor-\\nmation to the prediction plots, until we’ve used the entire posterior distribution.\\nWe’ll start with just the raw data and a single line. The code below plots the raw data,\\ncomputes the posterior mean values for a and b, then draws the implied line:\\nR code\\n4.46\\nplot( height ~ weight , data=d2 , col=rangi2 )\\npost <- extract.samples( m4.3 )\\na_map <- mean(post$a)\\nb_map <- mean(post$b)\\ncurve( a_map + b_map*(x - xbar) , add=TRUE )\\nYou can see the resulting plot in Figure 4.6. Each point in this plot is a single individual.\\nThe black line is defined by the mean slope β and mean intercept α. This is not a bad line.\\nIt certainly looks highly plausible. But there are an infinite number of other highly plausible\\nlines near it. Let’s draw those too.\\n'},\n",
       " {'index': 119,\n",
       "  'number': 101,\n",
       "  'content': '4.4. LINEAR PREDICTION\\n101\\nFigure 4.6. Height in centimeters (vertical)\\nplotted against weight in kilograms (horizon-\\ntal), with the line at the posterior mean plotted\\nin black.\\n4.4.3.3. Adding uncertainty around the mean. The posterior mean line is just the poste-\\nrior mean, the most plausible line in the infinite universe of lines the posterior distribution\\nhas considered. Plots of the average line, like Figure 4.6, are useful for getting an impres-\\nsion of the magnitude of the estimated influence of a variable. But they do a poor job of\\ncommunicating uncertainty. Remember, the posterior distribution considers every possible\\nregression line connecting height to weight. It assigns a relative plausibility to each. This\\nmeans that each combination of α and β has a posterior probability. It could be that there\\nare many lines with nearly the same posterior probability as the average line. Or it could be\\ninstead that the posterior distribution is rather narrow near the average line.\\nSo how can we get that uncertainty onto the plot? Together, a combination of α and\\nβ define a line. And so we could sample a bunch of lines from the posterior distribution.\\nThen we could display those lines on the plot, to visualize the uncertainty in the regression\\nrelationship.\\nTo better appreciate how the posterior distribution contains lines, we work with all of\\nthe samples from the model. Let’s take a closer look at the samples now:\\nR code\\n4.47\\npost <- extract.samples( m4.3 )\\npost[1:5,]\\na\\nb\\nsigma\\n1 154.5505 0.9222372 5.188631\\n2 154.4965 0.9286227 5.278370\\n3 154.4794 0.9490329 4.937513\\n4 155.2289 0.9252048 4.869807\\n5 154.9545 0.8192535 5.063672\\nEach row is a correlated random sample from the joint posterior of all three parameters, using\\nthe covariances provided by vcov(m4.3). The paired values of a and b on each row define a\\nline. The average of very many of these lines is the posterior mean line. But the scatter around\\nthat average is meaningful, because it alters our confidence in the relationship between the\\npredictor and the outcome.\\nSo now let’s display a bunch of these lines, so you can see the scatter. This lesson will be\\neasier to appreciate, if we use only some of the data to begin. Then you can see how adding\\n'},\n",
       " {'index': 120,\n",
       "  'number': 102,\n",
       "  'content': '102\\n4. GEOCENTRIC MODELS\\nin more data changes the scatter of the lines. So we’ll begin with just the first 10 cases in d2.\\nThe following code extracts the first 10 cases and re-estimates the model:\\nR code\\n4.48\\nN <- 10\\ndN <- d2[ 1:N , ]\\nmN <- quap(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a + b*( weight - mean(weight) ) ,\\na ~ dnorm( 178 , 20 ) ,\\nb ~ dlnorm( 0 , 1 ) ,\\nsigma ~ dunif( 0 , 50 )\\n) , data=dN )\\nNow let’s plot 20 of these lines, to see what the uncertainty looks like.\\nR code\\n4.49\\n# extract 20 samples from the posterior\\npost <- extract.samples( mN , n=20 )\\n# display raw data and sample size\\nplot( dN$weight , dN$height ,\\nxlim=range(d2$weight) , ylim=range(d2$height) ,\\ncol=rangi2 , xlab=\"weight\" , ylab=\"height\" )\\nmtext(concat(\"N = \",N))\\n# plot the lines, with transparency\\nfor ( i in 1:20 )\\ncurve( post$a[i] + post$b[i]*(x-mean(dN$weight)) ,\\ncol=col.alpha(\"black\",0.3) , add=TRUE )\\nThe last line loops over all 20 lines, using curve to display each.\\nThe result is shown in the upper-left plot in Figure 4.7. By plotting multiple regression\\nlines, sampled from the posterior, it is easy to see both the highly confident aspects of the\\nrelationship and the less confident aspects. The cloud of regression lines displays greater\\nuncertainty at extreme values for weight.\\nThe other plots in Figure 4.7 show the same relationships, but for increasing amounts\\nof data. Just re-use the code from before, but change N <- 10 to some other value. Notice\\nthat the cloud of regression lines grows more compact as the sample size increases. This is a\\nresult of the model growing more confident about the location of the mean.\\n4.4.3.4. Plotting regression intervals and contours. The cloud of regression lines in Fig-\\nure 4.7 is an appealing display, because it communicates uncertainty about the relationship\\nin a way that many people find intuitive. But it’s more common, and often much clearer, to\\nsee the uncertainty displayed by plotting an interval or contour around the average regres-\\nsion line. In this section, I’ll walk you through how to compute any arbitrary interval you\\nlike, using the underlying cloud of regression lines embodied in the posterior distribution.\\nFocus for the moment on a single weight value, say 50 kilograms. You can quickly make\\na list of 10,000 values of µ for an individual who weighs 50 kilograms, by using your samples\\nfrom the posterior:\\n'},\n",
       " {'index': 121,\n",
       "  'number': 103,\n",
       "  'content': '4.4. LINEAR PREDICTION\\n103\\nFigure 4.7. Samples from the quadratic approximate posterior distribution\\nfor the height/weight model, m4.3, with increasing amounts of data. In each\\nplot, 20 lines sampled from the posterior distribution, showing the uncer-\\ntainty in the regression relationship.\\nR code\\n4.50\\npost <- extract.samples( m4.3 )\\nmu_at_50 <- post$a + post$b * ( 50 - xbar )\\nThe code to the right of the <- above takes its form from the equation for µi:\\nµi = α + β(xi −¯x)\\nThe value of xi in this case is 50. Go ahead and take a look inside the result, mu_at_50. It’s a\\nvector of predicted means, one for each random sample from the posterior. Since joint a and\\nb went into computing each, the variation across those means incorporates the uncertainty\\nin and correlation between both parameters. It might be helpful at this point to actually plot\\nthe density for this vector of means:\\n'},\n",
       " {'index': 122,\n",
       "  'number': 104,\n",
       "  'content': '104\\n4. GEOCENTRIC MODELS\\n158.0\\n158.5\\n159.0\\n159.5\\n160.0\\n160.5\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nmu|weight=50\\nDensity\\nFigure 4.8. The quadratic approximate poste-\\nrior distribution of the mean height, µ, when\\nweight is 50 kg. This distribution represents\\nthe relative plausibility of different values of\\nthe mean.\\nR code\\n4.51\\ndens( mu_at_50 , col=rangi2 , lwd=2 , xlab=\"mu|weight=50\" )\\nI reproduce this plot in Figure 4.8. Since the components of µ have distributions, so too\\ndoes µ. And since the distributions of α and β are Gaussian, so too is the distribution of µ\\n(adding Gaussian distributions always produces a Gaussian distribution).\\nSince the posterior for µ is a distribution, you can find intervals for it, just like for any\\nposterior distribution. To find the 89% compatibility interval of µ at 50 kg, just use the PI\\ncommand as usual:\\nR code\\n4.52\\nPI( mu_at_50 , prob=0.89 )\\n5%\\n94%\\n158.5860 159.6706\\nWhat these numbers mean is that the central 89% of the ways for the model to produce the\\ndata place the average height between about 159 cm and 160 cm (conditional on the model\\nand data), assuming the weight is 50 kg.\\nThat’s good so far, but we need to repeat the above calculation for every weight value\\non the horizontal axis, not just when it is 50 kg. We want to draw 89% intervals around the\\naverage slope in Figure 4.6.\\nThis is made simple by strategic use of the link function, a part of the rethinking\\npackage. What link will do is take your quap approximation, sample from the posterior\\ndistribution, and then compute µ for each case in the data and sample from the posterior\\ndistribution. Here’s what it looks like for the data you used to fit the model:\\nR code\\n4.53\\nmu <- link( m4.3 )\\nstr(mu)\\nnum [1:1000, 1:352] 157 157 158 157 157 ...\\nYou end up with a big matrix of values of µ. Each row is a sample from the posterior distribu-\\ntion. The default is 1000 samples, but you can use as many or as few as you like. Each column\\n'},\n",
       " {'index': 123,\n",
       "  'number': 105,\n",
       "  'content': '4.4. LINEAR PREDICTION\\n105\\nis a case (row) in the data. There are 352 rows in d2, corresponding to 352 individuals. So\\nthere are 352 columns in the matrix mu above.\\nNow what can we do with this big matrix? Lots of things. The function link provides\\na posterior distribution of µ for each case we feed it. So above we have a distribution of\\nµ for each individual in the original data. We actually want something slightly different: a\\ndistribution of µ for each unique weight value on the horizontal axis. It’s only slightly harder\\nto compute that, by just passing link some new data:\\nR code\\n4.54\\n# define sequence of weights to compute predictions for\\n# these values will be on the horizontal axis\\nweight.seq <- seq( from=25 , to=70 , by=1 )\\n# use link to compute mu\\n# for each sample from posterior\\n# and for each weight in weight.seq\\nmu <- link( m4.3 , data=data.frame(weight=weight.seq) )\\nstr(mu)\\nnum [1:1000, 1:46] 136 136 138 136 137 ...\\nAnd now there are only 46 columns in mu, because we fed it 46 different values for weight.\\nTo visualize what you’ve got here, let’s plot the distribution of µ values at each height.\\nR code\\n4.55\\n# use type=\"n\" to hide raw data\\nplot( height ~ weight , d2 , type=\"n\" )\\n# loop over samples and plot each mu value\\nfor ( i in 1:100 )\\npoints( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )\\nThe result is shown on the left-hand side of Figure 4.9. At each weight value in weight.seq,\\na pile of computed µ values are shown. Each of these piles is a Gaussian distribution, like\\nthat in Figure 4.8. You can see now that the amount of uncertainty in µ depends upon the\\nvalue of weight. And this is the same fact you saw in Figure 4.7.\\nThe final step is to summarize the distribution for each weight value. We’ll use apply,\\nwhich applies a function of your choice to a matrix.\\nR code\\n4.56\\n# summarize the distribution of mu\\nmu.mean <- apply( mu , 2 , mean )\\nmu.PI <- apply( mu , 2 , PI , prob=0.89 )\\nRead apply(mu,2,mean) as compute the mean of each column (dimension “2”) of the matrix\\nmu. Now mu.mean contains the average µ at each weight value, and mu.PI contains 89% lower\\nand upper bounds for each weight value. Be sure to take a look inside mu.mean and mu.PI,\\nto demystify them. They are just different kinds of summaries of the distributions in mu, with\\neach column being for a different weight value. These summaries are only summaries. The\\n“estimate” is the entire distribution.\\nYou can plot these summaries on top of the data with a few lines of R code:\\n'},\n",
       " {'index': 124,\n",
       "  'number': 106,\n",
       "  'content': '106\\n4. GEOCENTRIC MODELS\\nFigure 4.9. Left: The first 100 values in the distribution of µ at each weight\\nvalue. Right: The !Kung height data again, now with 89% compatibility in-\\nterval of the mean indicated by the shaded region. Compare this region to\\nthe distributions of blue points on the left.\\nR code\\n4.57\\n# plot raw data\\n# fading out points to make line and interval more visible\\nplot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )\\n# plot the MAP line, aka the mean mu for each weight\\nlines( weight.seq , mu.mean )\\n# plot a shaded region for 89% PI\\nshade( mu.PI , weight.seq )\\nYou can see the results in the right-hand plot in Figure 4.9.\\nUsing this approach, you can derive and plot posterior prediction means and intervals\\nfor quite complicated models, for any data you choose. It’s true that it is possible to use\\nanalytical formulas to compute intervals like this. I have tried teaching such an analytical\\napproach before, and it has always been disaster. Part of the reason is probably my own failure\\nas a teacher, but another part is that most social and natural scientists have never had much\\ntraining in probability theory and tend to get very nervous around\\nR\\n’s. I’m sure with enough\\neffort, every one of them could learn to do the mathematics. But all of them can quickly\\nlearn to generate and summarize samples derived from the posterior distribution. So while\\nthe mathematics would be a more elegant approach, and there is some additional insight\\nthat comes from knowing the mathematics, the pseudo-empirical approach presented here\\nis very flexible and allows a much broader audience of scientists to pull insight from their\\nstatistical modeling. And again, when you start estimating models with MCMC (Chapter 9),\\nthis is really the only approach available. So it’s worth learning now.\\nTo summarize, here’s the recipe for generating predictions and intervals from the poste-\\nrior of a fit model.\\n'},\n",
       " {'index': 125,\n",
       "  'number': 107,\n",
       "  'content': '4.4. LINEAR PREDICTION\\n107\\n(1) Use link to generate distributions of posterior values for µ. The default behavior\\nof link is to use the original data, so you have to pass it a list of new horizontal axis\\nvalues you want to plot posterior predictions across.\\n(2) Use summary functions like mean or PI to find averages and lower and upper\\nbounds of µ for each value of the predictor variable.\\n(3) Finally, use plotting functions like lines and shade to draw the lines and intervals.\\nOr you might plot the distributions of the predictions, or do further numerical\\ncalculations with them. It’s really up to you.\\nThis recipe works for every model we fit in the book. As long as you know the structure of\\nthe model—how parameters relate to the data—you can use samples from the posterior to\\ndescribe any aspect of the model’s behavior.\\nRethinking: Overconfident intervals. The compatibility interval for the regression line in Figure 4.9\\nclings tightly to the MAP line. Thus there is very little uncertainty about the average height as a\\nfunction of average weight. But you have to keep in mind that these inferences are always conditional\\non the model. Even a very bad model can have very tight compatibility intervals. It may help if you\\nthink of the regression line in Figure 4.9 as saying: Conditional on the assumption that height and\\nweight are related by a straight line, then this is the most plausible line, and these are its plausible bounds.\\nOverthinking: How link works. The function link is not really very sophisticated. All it is doing\\nis using the formula you provided when you fit the model to compute the value of the linear model.\\nIt does this for each sample from the posterior distribution, for each case in the data. You could\\naccomplish the same thing for any model, fit by any means, by performing these steps yourself. This\\nis how it’d look for m4.3.\\nR code\\n4.58\\npost <- extract.samples(m4.3)\\nmu.link <- function(weight) post$a + post$b*( weight - xbar )\\nweight.seq <- seq( from=25 , to=70 , by=1 )\\nmu <- sapply( weight.seq , mu.link )\\nmu.mean <- apply( mu , 2 , mean )\\nmu.CI <- apply( mu , 2 , PI , prob=0.89 )\\nAnd the values in mu.mean and mu.CI should be very similar (allowing for simulation variance) to\\nwhat you got the automated way, using link.\\nKnowing this manual method is useful both for (1) understanding and (2) sheer power. What-\\never the model you find yourself with, this approach can be used to generate posterior predictions for\\nany component of it. Automated tools like link save effort, but they are never as flexible as the code\\nyou can write yourself.\\n4.4.3.5. Prediction intervals. Now let’s walk through generating an 89% prediction in-\\nterval for actual heights, not just the average height, µ. This means we’ll incorporate the\\nstandard deviation σ and its uncertainty as well. Remember, the first line of the statistical\\nmodel here is:\\nhi ∼Normal(µi, σ)\\nWhat you’ve done so far is just use samples from the posterior to visualize the uncertainty\\nin µi, the linear model of the mean. But actual predictions of heights depend also upon the\\ndistribution in the first line. The Gaussian distribution on the first line tells us that the model\\n'},\n",
       " {'index': 126,\n",
       "  'number': 108,\n",
       "  'content': '108\\n4. GEOCENTRIC MODELS\\nexpects observed heights to be distributed around µ, not right on top of it. And the spread\\naround µ is governed by σ. All of this suggests we need to incorporate σ in the predictions\\nsomehow.\\nHere’s how you do it. Imagine simulating heights. For any unique weight value, you sam-\\nple from a Gaussian distribution with the correct mean µ for that weight, using the correct\\nvalue of σ sampled from the same posterior distribution. If you do this for every sample\\nfrom the posterior, for every weight value of interest, you end up with a collection of simu-\\nlated heights that embody the uncertainty in the posterior as well as the uncertainty in the\\nGaussian distribution of heights. There is a tool called sim which does this:\\nR code\\n4.59\\nsim.height <- sim( m4.3 , data=list(weight=weight.seq) )\\nstr(sim.height)\\nnum [1:1000, 1:46] 140 131 136 137 142 ...\\nThis matrix is much like the earlier one, mu, but it contains simulated heights, not distribu-\\ntions of plausible average height, µ.\\nWe can summarize these simulated heights in the same way we summarized the distri-\\nbutions of µ, by using apply:\\nR code\\n4.60\\nheight.PI <- apply( sim.height , 2 , PI , prob=0.89 )\\nNow height.PI contains the 89% posterior prediction interval of observable (according to\\nthe model) heights, across the values of weight in weight.seq.\\nLet’s plot everything we’ve built up: (1) the average line, (2) the shaded region of 89%\\nplausible µ, and (3) the boundaries of the simulated heights the model expects.\\nR code\\n4.61\\n# plot raw data\\nplot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) )\\n# draw MAP line\\nlines( weight.seq , mu.mean )\\n# draw HPDI region for line\\nshade( mu.HPDI , weight.seq )\\n# draw PI region for simulated heights\\nshade( height.PI , weight.seq )\\nThe code above uses some objects computed in previous sections, so go back and execute\\nthat code, if you need to.\\nIn Figure 4.10, I plot the result. The wide shaded region in the figure represents the\\narea within which the model expects to find 89% of actual heights in the population, at each\\nweight. There is nothing special about the value 89% here. You could plot the boundary for\\nother percents, such as 67% and 97% (also both primes), and add those to the plot. Doing so\\nwould help you see more of the shape of the predicted distribution of heights. I leave that as\\nan exercise for the reader. Just go back to the code above and add prob=0.67, for example,\\nto the call to PI. That will give you 67% intervals, instead of 89% ones.\\n'},\n",
       " {'index': 127,\n",
       "  'number': 109,\n",
       "  'content': '4.4. LINEAR PREDICTION\\n109\\nFigure 4.10. 89% prediction interval for\\nheight, as a function of weight. The solid line\\nis the average line for the mean height at each\\nweight. The two shaded regions show different\\n89% plausible regions. The narrow shaded in-\\nterval around the line is the distribution of µ.\\nThe wider shaded region represents the region\\nwithin which the model expects to find 89%\\nof actual heights in the population, at each\\nweight.\\nNotice that the outline for the wide shaded interval is a little rough. This is the simulation\\nvariance in the tails of the sampled Gaussian values. If it really bothers you, increase the\\nnumber of samples you take from the posterior distribution. The optional n parameter for\\nsim.height controls how many samples are used. Try for example:\\nR code\\n4.62\\nsim.height <- sim( m4.3 , data=list(weight=weight.seq) , n=1e4 )\\nheight.PI <- apply( sim.height , 2 , PI , prob=0.89 )\\nRun the plotting code again, and you’ll see the shaded boundary smooth out some. With\\nextreme percentiles, it can be very hard to get out all of the roughness. Luckily, it hardly\\nmatters, except for aesthetics. Moreover, it serves to remind us that all statistical inference\\nis approximate. The fact that we can compute an expected value to the 10th decimal place\\ndoes not imply that our inferences are precise to the 10th decimal place.\\nRethinking: Two kinds of uncertainty. In the procedure above, we encountered both uncertainty\\nin parameter values and uncertainty in a sampling process. These are distinct concepts, even though\\nthey are processed much the same way and end up blended together in the posterior predictive simu-\\nlation. The posterior distribution is a ranking of the relative plausibilities of every possible combina-\\ntion of parameter values. The distribution of simulated outcomes, like height, is instead a distribution\\nthat includes sampling variation from some process that generates Gaussian random variables. This\\nsampling variation is still a model assumption. It’s no more or less objective than the posterior distri-\\nbution. Both kinds of uncertainty matter, at least sometimes. But it’s important to keep them straight,\\nbecause they depend upon different model assumptions. Furthermore, it’s possible to view the Gauss-\\nian likelihood as a purely epistemological assumption (a device for estimating the mean and variance\\nof a variable), rather than an ontological assumption about what future data will look like. In that\\ncase, it may not make complete sense to simulate outcomes.\\nOverthinking: Rolling your own sim. Just like with link, it’s useful to know a little about how\\nsim operates. For every distribution like dnorm, there is a companion simulation function. For the\\n'},\n",
       " {'index': 128,\n",
       "  'number': 110,\n",
       "  'content': '110\\n4. GEOCENTRIC MODELS\\nGaussian distribution, the companion is rnorm, and it simulates sampling from a Gaussian distribu-\\ntion. What we want R to do is simulate a height for each set of samples, and to do this for each value\\nof weight. The following will do it:\\nR code\\n4.63\\npost <- extract.samples(m4.3)\\nweight.seq <- 25:70\\nsim.height <- sapply( weight.seq , function(weight)\\nrnorm(\\nn=nrow(post) ,\\nmean=post$a + post$b*( weight - xbar ) ,\\nsd=post$sigma ) )\\nheight.PI <- apply( sim.height , 2 , PI , prob=0.89 )\\nThe values in height.PI will be practically identical to the ones computed in the main text and\\ndisplayed in Figure 4.10.\\n4.5. Curves from lines\\nIn the next chapter, you’ll see how to use linear models to build regressions with more\\nthan one predictor variable. But before then, it helps to see how to model the outcome as a\\ncurved function of a predictor. The models so far all assume that a straight line describes the\\nrelationship. But there’s nothing special about straight lines, aside from their simplicity.\\nWe’ll consider two commonplace methods that use linear regression to build curves.\\nThe first is polynomial regression. The second is b-splines. Both approaches work by\\ntransforming a single predictor variable into several synthetic variables. But splines have\\nsome clear advantages. Neither approach aims to do more than describe the function that\\nrelates one variable to another. Causal inference, which we’ll consider much more beginning\\nin the next chapter, wants more.\\n4.5.1. Polynomial regression. Polynomial regression uses powers of a variable—squares\\nand cubes—as extra predictors. This is an easy way to build curved associations. Polyno-\\nmial regressions are very common, and understanding how they work will help scaffold later\\nmodels. To understand how polynomial regression works, let’s work through an example,\\nusing the full !Kung data, not just the adults:\\nR code\\n4.64\\nlibrary(rethinking)\\ndata(Howell1)\\nd <- Howell1\\nGo ahead and plot( height ~ weight , d ). The relationship is visibly curved, now\\nthat we’ve included the non-adult individuals.\\nThe most common polynomial regression is a parabolic model of the mean. Let x be\\nstandardized body weight. Then the parabolic equation for the mean height is:\\nµi = α + β1xi + β2x2\\ni\\nThe above is a parabolic (second order) polynomial. The α + β1xi part is the same linear\\nfunction of x in a linear regression, just with a little “1” subscript added to the parameter\\nname, so we can tell it apart from the new parameter. The additional term uses the square\\nof xi to construct a parabola, rather than a perfectly straight line. The new parameter β2\\nmeasures the curvature of the relationship.\\n'},\n",
       " {'index': 129,\n",
       "  'number': 111,\n",
       "  'content': '4.5. CURVES FROM LINES\\n111\\nFitting these models to data is easy. Interpreting them can be hard. We’ll begin with\\nthe easy part, fitting a parabolic model of height on weight. The first thing to do is to stan-\\ndardize the predictor variable. We’ve done this in previous examples. But this is especially\\nhelpful for working with polynomial models. When predictor variables have very large val-\\nues in them, there are sometimes numerical glitches. Even well-known statistical software\\ncan suffer from these glitches, leading to mistaken estimates. These problems are very com-\\nmon for polynomial regression, because the square or cube of a large number can be truly\\nmassive. Standardizing largely resolves this issue. It should be your default behavior.\\nTo define the parabolic model, just modify the definition of µi. Here’s the model:\\nhi ∼Normal(µi, σ)\\nheight ~ dnorm(mu,sigma)\\nµi = α + β1xi + β2x2\\ni\\nmu <- a + b1*weight.s + b2*weight.s^2\\nα ∼Normal(178, 20)\\na ~ dnorm(178,20)\\nβ1 ∼Log-Normal(0, 1)\\nb1 ~ dlnorm(0,1)\\nβ2 ∼Normal(0, 1)\\nb2 ~ dnorm(0,1)\\nσ ∼Uniform(0, 50)\\nsigma ~ dunif(0,50)\\nThe confusing issue here is assigning a prior for β2, the parameter on the squared value\\nof x. Unlike β1, we don’t want a positive constraint. In the practice problems at the end\\nof the chapter, you’ll use prior predictive simulation to understand why. These polynomial\\nparameters are in general very difficult to understand. But prior predictive simulation does\\nhelp a lot.\\nApproximating the posterior is straightforward. Just modify the definition of mu so that\\nit contains both the linear and quadratic terms. But in general it is better to pre-process any\\nvariable transformations—you don’t need the computer to recalculate the transformations\\non every iteration of the fitting procedure. So I’ll also build the square of weight_s as a\\nseparate variable:\\nR code\\n4.65\\nd$weight_s <- ( d$weight - mean(d$weight) )/sd(d$weight)\\nd$weight_s2 <- d$weight_s^2\\nm4.5 <- quap(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a + b1*weight_s + b2*weight_s2 ,\\na ~ dnorm( 178 , 20 ) ,\\nb1 ~ dlnorm( 0 , 1 ) ,\\nb2 ~ dnorm( 0 , 1 ) ,\\nsigma ~ dunif( 0 , 50 )\\n) , data=d )\\nNow, since the relationship between the outcome height and the predictor weight depends\\nupon two slopes, b1 and b2, it isn’t so easy to read the relationship off a table of coefficients:\\nR code\\n4.66\\nprecis( m4.5 )\\nmean\\nsd\\n5.5%\\n94.5%\\na\\n146.06 0.37 145.47 146.65\\nb1\\n21.73 0.29\\n21.27\\n22.19\\n'},\n",
       " {'index': 130,\n",
       "  'number': 112,\n",
       "  'content': '112\\n4. GEOCENTRIC MODELS\\nFigure 4.11. Polynomial regressions of height on weight (standardized),\\nfor the full !Kung data. In each plot, the raw data are shown by the circles.\\nThe solid curves show the path of µ in each model, and the shaded regions\\nshow the 89% interval of the mean (close to the solid curve) and the 89%\\ninterval of predictions (wider). Left: Linear regression. Middle: A second\\norder polynomial, a parabolic or quadratic regression. Right: A third order\\npolynomial, a cubic regression.\\nb2\\n-7.80 0.27\\n-8.24\\n-7.37\\nsigma\\n5.77 0.18\\n5.49\\n6.06\\nThe parameter α (a) is still the intercept, so it tells us the expected value of height when\\nweight is at its mean value. But it is no longer equal to the mean height in the sample, since\\nthere is no guarantee it should in a polynomial regression.76 And those β1 and β2 parameters\\nare the linear and square components of the curve. But that doesn’t make them transparent.\\nYou have to plot these model fits to understand what they are saying. So let’s do that.\\nWe’ll calculate the mean relationship and the 89% intervals of the mean and the predictions,\\nlike in the previous section. Here’s the working code:\\nR code\\n4.67\\nweight.seq <- seq( from=-2.2 , to=2 , length.out=30 )\\npred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2 )\\nmu <- link( m4.5 , data=pred_dat )\\nmu.mean <- apply( mu , 2 , mean )\\nmu.PI <- apply( mu , 2 , PI , prob=0.89 )\\nsim.height <- sim( m4.5 , data=pred_dat )\\nheight.PI <- apply( sim.height , 2 , PI , prob=0.89 )\\nPlotting all of this is straightforward:\\nR code\\n4.68\\nplot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) )\\nlines( weight.seq , mu.mean )\\nshade( mu.PI , weight.seq )\\nshade( height.PI , weight.seq )\\n'},\n",
       " {'index': 131,\n",
       "  'number': 113,\n",
       "  'content': '4.5. CURVES FROM LINES\\n113\\nThe results are shown in Figure 4.11. The left panel of the figure shows the familiar linear\\nregression from earlier in the chapter, but now with the standardized predictor and full data\\nwith both adults and non-adults. The linear model makes some spectacularly poor predic-\\ntions, at both very low and middle weights. Compare this to the middle panel, our new\\nquadratic regression. The curve does a better job of finding a central path through the data.\\nThe right panel in Figure 4.11 shows a higher-order polynomial regression, a cubic\\nregression on weight. The model is:\\nhi ∼Normal(µi, σ)\\nµi = α + β1xi + β2x2\\ni + β3x3\\ni\\nα ∼Normal(178, 20)\\na ~ dnorm(178,20)\\nβ1 ∼Log-Normal(0, 1)\\nb1 ~ dlnorm(0,1)\\nβ2 ∼Normal(0, 1)\\nb2 ~ dnorm(0,1)\\nβ3 ∼Normal(0, 1)\\nb3 ~ dnorm(0,1)\\nσ ∼Uniform(0, 50)\\nsigma ~ dunif(0,50)\\nFit the model with a slight modification of the parabolic model’s code:\\nR code\\n4.69\\nd$weight_s3 <- d$weight_s^3\\nm4.6 <- quap(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a + b1*weight_s + b2*weight_s2 + b3*weight_s3 ,\\na ~ dnorm( 178 , 20 ) ,\\nb1 ~ dlnorm( 0 , 1 ) ,\\nb2 ~ dnorm( 0 , 10 ) ,\\nb3 ~ dnorm( 0 , 10 ) ,\\nsigma ~ dunif( 0 , 50 )\\n) , data=d )\\nComputing the curve and intervals is similarly a small modification of the previous code.\\nThis cubic curve is even more flexible than the parabola, so it fits the data even better.\\nBut it’s not clear that any of these models make a lot of sense. They are good geocentric\\ndescriptions of the sample, yes. But there are two problems. First, a better fit to the sample\\nmight not actually be a better model. That’s the subject of Chapter 7. Second, the model con-\\ntains no biological information. We aren’t learning any causal relationship between height\\nand weight. We’ll deal with this second problem much later, in Chapter 16.\\nRethinking: Linear, additive, funky. The parabolic model of µi above is still a “linear model” of\\nthe mean, even though the equation is clearly not of a straight line. Unfortunately, the word “linear”\\nmeans different things in different contexts, and different people use it differently in the same context.\\nWhat “linear” means in this context is that µi is a linear function of any single parameter. Such models\\nhave the advantage of being easier to fit to data. They are also often easier to interpret, because they\\nassume that parameters act independently on the mean. They have the disadvantage of being used\\nthoughtlessly. When you have expert knowledge, it is often easy to do better than a linear model.\\nThese models are geocentric devices for describing partial correlations. We should feel embarrassed\\nto use them, just so we don’t become satisfied with the phenomenological explanations they provide.\\n'},\n",
       " {'index': 132,\n",
       "  'number': 114,\n",
       "  'content': '114\\n4. GEOCENTRIC MODELS\\nOverthinking: Converting back to natural scale. The plots in Figure 4.11 have standard units on\\nthe horizontal axis. These units are sometimes called z-scores. But suppose you fit the model using\\nstandardized variables, but want to plot the estimates on the original scale. All that’s really needed is\\nfirst to turn off the horizontal axis when you plot the raw data:\\nR code\\n4.70\\nplot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) , xaxt=\"n\" )\\nThe xaxt at the end there turns off the horizontal axis. Then you explicitly construct the axis, using\\nthe axis function.\\nR code\\n4.71\\nat <- c(-2,-1,0,1,2)\\nlabels <- at*sd(d$weight) + mean(d$weight)\\naxis( side=1 , at=at , labels=round(labels,1) )\\nThe first line above defines the location of the labels, in standardized units. The second line then takes\\nthose units and converts them back to the original scale. The third line draws the axis. Take a look at\\nthe help ?axis for more details.\\n4.5.2. Splines. The second way to introduce a curve is to construct something known as\\na spline. The word spline originally referred to a long, thin piece of wood or metal that\\ncould be anchored in a few places in order to aid drafters or designers in drawing curves.\\nIn statistics, a spline is a smooth function built out of smaller, component functions. There\\nare actually many types of splines. The b-spline we’ll look at here is commonplace. The “B”\\nstands for “basis,” which here just means “component.” B-splines build up wiggly functions\\nfrom simpler less-wiggly components. Those components are called basis functions. While\\nthere are fancier splines, we want to start B-splines because they force you to make a number\\nof choices that other types of splines automate. You’ll need to understand B-splines before\\nyou can understand fancier splines.\\nTo see how B-splines work, we’ll need an example that is much wigglier—that’s a scien-\\ntific term—than the !Kung stature data. Cherry trees blossom all over Japan in the spring\\neach year, and the tradition of flower viewing (Hanami 花見) follows. The timing of the\\nblossoms can vary a lot by year and century. Let’s load a thousand years of blossom dates:\\nR code\\n4.72\\nlibrary(rethinking)\\ndata(cherry_blossoms)\\nd <- cherry_blossoms\\nprecis(d)\\n\\'data.frame\\': 1215 obs. of 5 variables:\\nmean\\nsd\\n5.5%\\n94.5%\\nhistogram\\nyear\\n1408.00 350.88 867.77 1948.23\\n▇▇▇▇▇▇▇▇▇▇▇▇▁\\ndoy\\n104.54\\n6.41\\n94.43\\n115.00\\n▁▂▅▇▇▃▁▁\\ntemp\\n6.14\\n0.66\\n5.15\\n7.29\\n▁▃▅▇▃▂▁▁\\ntemp_upper\\n7.19\\n0.99\\n5.90\\n8.90 ▁▂▅▇▇▅▂▂▁▁▁▁▁▁▁\\ntemp_lower\\n5.10\\n0.85\\n3.79\\n6.37 ▁▁▁▁▁▁▁▃▅▇▃▂▁▁▁\\nSee ?cherry_blossoms for details and sources. We’re going to work with the historical\\nrecord of first day of blossom, doy, for now. It ranges from 86 (late March) to 124 (early\\nMay). The years with recorded blossom dates run from 812 CE to 2015 CE. You should go\\n'},\n",
       " {'index': 133,\n",
       "  'number': 115,\n",
       "  'content': '4.5. CURVES FROM LINES\\n115\\nahead and plot doy against year to see (also see the figure on the next page). There might\\nbe some wiggly trend in that cloud. It’s hard to tell.\\nLet’s try extracting a trend with a B-spline. The short explanation of B-splines is that\\nthey divide the full range of some predictor variable, like year, into parts. Then they assign\\na parameter to each part. These parameters are gradually turned on and off in a way that\\nmakes their sum into a fancy, wiggly curve. The long explanation contains lots more details.\\nBut all of those details just exist to achieve this goal of building up a big, curvy function from\\nindividually less curvy local functions.\\nHere’s a longer explanation, with visual examples. Our goal is to approximate the blos-\\nsom trend with a wiggly function. With B-splines, just like with polynomial regression, we\\ndo this by generating new predictor variables and using those in the linear model, µi. Un-\\nlike polynomial regression, B-splines do not directly transform the predictor by squaring or\\ncubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of\\nthese synthetic variables exists only to gradually turn a specific parameter on and off within\\na specific range of the real predictor variable. Each of the synthetic variables is called a basis\\nfunction. The linear model ends up looking very familiar:\\nµi = α + w1Bi,1 + w2Bi,2 + w3Bi,3 + ...\\nwhere Bi,n is the n-th basis function’s value on row i, and the w parameters are correspond-\\ning weights for each. The parameters act like slopes, adjusting the influence of each basis\\nfunction on the mean µi. So really this is just another linear regression, but with some fancy,\\nsynthetic predictor variables. These synthetic variables do some really elegant descriptive\\n(geocentric) work for us.\\nHow do we construct these basis variables B? I display the simplest case in Figure 4.12,\\nin which I approximate the blossom date data with a combination of linear approximations.\\nFirst, I divide the full range of the horizontal axis into four parts, using pivot points called\\nknots. The knots are shown by the + symbols in the top plot. I’ve placed the knots at even\\nquantiles of the blossom data. In the blossom data, there are fewer recorded blossom dates\\ndeep in the past. So using even quantiles does not produce evenly spaced knots. This is why\\nthe second knot is so far from the first knot. Don’t worry right now about the code to make\\nthese knots. You’ll see it later.\\nFocus for now just on the picture. The knots act as pivots for five different basis functions,\\nour B variables. These synthetic variables are used to gently transition from one region of\\nthe horizontal axis to the next. Essentially, these variables tell you which knot you are close\\nto. Beginning on the left of the top plot, basis function 1 has value 1 and all of the others\\nare set to zero. As we move rightwards towards the second knot, basis 1 declines and basis 2\\nincreases. At knot 2, basis 2 has value 1, and all of the others are set to zero.\\nThe nice feature of these basis functions is that they make the influence of each parameter\\nquite local. At any point on the horizontal axis in Figure 4.12, only two basis functions have\\nnon-zero values. For example, the dashed blue line in the top plot shows the year 1200. Basis\\nfunctions 1 and 2 are non-zero for that year. So the parameters for basis functions 1 and 2 are\\nthe only parameters influencing prediction for the year 1200. This is quite unlike polynomial\\nregression, where parameters influence the entire shape of the curve.\\nIn the middle plot in Figure 4.12, I show each basis function multiplied by its corre-\\nsponding weight parameter. I got these weights by fitting the model to the data. I’ll show\\nyou how to do that in a moment. Again focus on the figure for now. Weight parameters can\\nbe positive or negative. So for example basis function 5 ends up below the zero line. It has\\n'},\n",
       " {'index': 134,\n",
       "  'number': 116,\n",
       "  'content': '116\\n4. GEOCENTRIC MODELS\\n800\\n1000\\n1200\\n1400\\n1600\\n1800\\n2000\\nyear\\nbasis value\\n0\\n1\\n1\\n2\\n3\\n4\\n5\\n1200\\n800\\n1000\\n1200\\n1400\\n1600\\n1800\\n2000\\nyear\\nbasis * weight\\n0\\nFigure 4.12. Using B-splines to make local, linear approximations. Top:\\nEach basis function is a variable that turns on specific ranges of the predic-\\ntor variable. At any given value on the horizontal axis, e.g. 1200, only two\\nhave non-zero values. Middle: Parameters called weights multiply the basis\\nfunctions. The spline at any given point is the sum of these weighted basis\\nfunctions. Bottom: The resulting B-spline shown against the data. Each\\nweight parameter determines the slope in a specific range of the predictor\\nvariable.\\nnegative weight. To construct a prediction for any given year, say for example 1200 again, we\\njust add up these weighted basis functions at that year. In the year 1200, only basis functions\\n1 and 2 influence prediction. Their sum is slightly above the zero (the mean).\\nFinally, in the bottom plot of Figure 4.12, I display the spline, as a 97% posterior interval\\nfor µ, over the raw blossom date data. All the spline seems to pick up is a change in trend\\naround 1800. You can probably guess which global climate trend this reflects. But there\\nis more going on in the data, before 1800. To see it, we can do two things. First, we can\\nuse more knots. The more knots, the more flexible the spline. Second, instead of linear\\napproximations, we can use higher-degree polynomials.\\n'},\n",
       " {'index': 135,\n",
       "  'number': 117,\n",
       "  'content': '4.5. CURVES FROM LINES\\n117\\nLet’s build up the code that will let you reproduce the plots in Figure 4.12, but also let\\nyou change the knots and degree to anything you like. First, we choose the knots. Remem-\\nber, the knots are just values of year that serve as pivots for our spline. Where should the\\nknots go? There are different ways to answer this question.77 You can, in principle, put the\\nknots wherever you like. Their locations are part of the model, and you are responsible for\\nthem. Let’s do what we did in the simple example above, place the knots at different evenly-\\nspaced quantiles of the predictor variable. This gives you more knots where there are more\\nobservations. We used only 5 knots in the first example. Now let’s go for 15:\\nR code\\n4.73\\nd2 <- d[ complete.cases(d$doy) , ] # complete cases on doy\\nnum_knots <- 15\\nknot_list <- quantile( d2$year , probs=seq(0,1,length.out=num_knots) )\\nGo ahead and inspect knot_list to see that it contains 15 dates.\\nThe next choice is polynomial degree. This determines how basis functions combine,\\nwhich determines how the parameters interact to produce the spline. For degree 1, as in\\nFigure 4.12, two basis functions combine at each point. For degree 2, three functions com-\\nbine at each point. For degree 3, four combine. R already has a nice function that will build\\nbasis functions for any list of knots and degree. This code will construct the necessary basis\\nfunctions for a degree 3 (cubic) spline:\\nR code\\n4.74\\nlibrary(splines)\\nB <- bs(d2$year,\\nknots=knot_list[-c(1,num_knots)] ,\\ndegree=3 , intercept=TRUE )\\nThe matrix B should have 827 rows and 17 columns. Each row is a year, corresponding to the\\nrows in the d2 data frame. Each column is a basis function, one of our synthetic variables\\ndefining a span of years within which a corresponding parameter will influence prediction.\\nTo display the basis functions, just plot each column against year:\\nR code\\n4.75\\nplot( NULL , xlim=range(d2$year) , ylim=c(0,1) , xlab=\"year\" , ylab=\"basis\" )\\nfor ( i in 1:ncol(B) ) lines( d2$year , B[,i] )\\nI show these cubic basis functions in the top plot of Figure 4.13.\\nNow to get the parameter weights for each basis function, we need to actually define the\\nmodel and make it run. The model is just a linear regression. The synthetic basis functions\\ndo all the work. We’ll use each column of the matrix B as a variable. We’ll also have an\\nintercept to capture the average blossom day. This will make it easier to define priors on the\\nbasis weights, because then we can just conceive of each as a deviation from the intercept.\\nIn mathematical form, we start with the probability of the data and the linear model:\\nDi ∼Normal(µi, σ)\\nµi = α +\\nK\\nX\\nk=1\\nwkBk,i\\n'},\n",
       " {'index': 136,\n",
       "  'number': 118,\n",
       "  'content': '118\\n4. GEOCENTRIC MODELS\\n800\\n1000\\n1200\\n1400\\n1600\\n1800\\n2000\\nyear\\nbasis value\\n0\\n1\\n800\\n1000\\n1200\\n1400\\n1600\\n1800\\n2000\\nyear\\nbasis * weight\\n0\\n800\\n1000\\n1200\\n1400\\n1600\\n1800\\n2000\\n90\\n100\\n110\\n120\\nyear\\nDay in year\\nFigure 4.13. A cubic spline with 15 knots. The top plot is, just like in the\\nprevious figure, the basis functions. However now more of these overlap.\\nThe middle plot is again each basis weighted by its corresponding parameter.\\nAnd the sum of these weighted basis functions, at each point, produces the\\nspline shown at the bottom, displayed as a 97% posterior interval of µ.\\nAnd then the priors:\\nα ∼Normal(100, 10)\\nwj ∼Normal(0, 10)\\nσ ∼Exponential(1)\\nThat linear model might look weird. But all it is doing is multiplying each basis value by\\na corresponding parameter wk and then adding up all K of those products. This is just a\\ncompact way of writing a linear model. The rest should be familiar. Although I will ask you\\nto simulate from those priors in the practice problems at the end of the chapter. You might\\nguess already that the w priors influence how wiggly the spline can be.\\nThis is also the first time we’ve used an exponential distribution as a prior. Expo-\\nnential distributions are useful priors for scale parameters, parameters that must be positive.\\nThe prior for σ is exponential with a rate of 1. The way to read an exponential distribution\\nis to think of it as containing no more information than an average deviation. That average\\n'},\n",
       " {'index': 137,\n",
       "  'number': 119,\n",
       "  'content': '4.5. CURVES FROM LINES\\n119\\nis the inverse of the rate. So in this case it is 1/1 = 1. If the rate were 0.5, the mean would be\\n1/0.5 = 2. We’ll use exponential priors for the rest of the book, in place of uniform priors.\\nIt is much more common to have a sense of the average deviation than of the maximum.\\nTo build this model in quap, we just need a way to do that sum. The easiest way is to\\nuse matrix multiplication. If you aren’t familiar with linear algebra in this context, that’s fine.\\nThere is an Overthinking box at the end with some more detail about why this works. The\\nonly other trick is to use a start list for the weights to tell quap how many there are.\\nR code\\n4.76\\nm4.7 <- quap(\\nalist(\\nD ~ dnorm( mu , sigma ) ,\\nmu <- a + B %*% w ,\\na ~ dnorm(100,10),\\nw ~ dnorm(0,10),\\nsigma ~ dexp(1)\\n), data=list( D=d2$doy , B=B ) ,\\nstart=list( w=rep( 0 , ncol(B) ) ) )\\nYou can look at the posterior means if you like with precis(m4.7,depth=2). But it won’t\\nreveal much. You should see 17 w parameters. But you can’t tell what the model thinks from\\nthe parameter summaries. Instead we need to plot the posterior predictions. First, here are\\nthe weighted basis functions:\\nR code\\n4.77\\npost <- extract.samples( m4.7 )\\nw <- apply( post$w , 2 , mean )\\nplot( NULL , xlim=range(d2$year) , ylim=c(-6,6) ,\\nxlab=\"year\" , ylab=\"basis * weight\" )\\nfor ( i in 1:ncol(B) ) lines( d2$year , w[i]*B[,i] )\\nThis plot, with the knots added for reference, is displayed in the middle row of Figure 4.13.\\nAnd finally the 97% posterior interval for µ, at each year:\\nR code\\n4.78\\nmu <- link( m4.7 )\\nmu_PI <- apply(mu,2,PI,0.97)\\nplot( d2$year , d2$doy , col=col.alpha(rangi2,0.3) , pch=16 )\\nshade( mu_PI , d2$year , col=col.alpha(\"black\",0.5) )\\nThis is shown in the bottom of the figure. The spline is much wigglier now. Something\\nhappened around 1500, for example. If you add more knots, you can make this even wigglier.\\nYou might wonder how many knots is correct. We’ll be ready to address that question in a few\\nmore chapters. Really we’ll answer it by changing the question. So hang on to the question,\\nand we’ll turn to it later.\\nDistilling the trend across years provides a lot of information. But year is not really a\\ncausal variable, only a proxy for features of each year. In the practice problems below, you’ll\\ncompare this trend to the temperature record, in an attempt to explain those wiggles.\\nOverthinking: Matrix multiplication in the spline model. Matrix algebra is a stressful topic for many\\nscientists. If you have had a course in it, it’s obvious what it does. But if you haven’t, it is mysterious.\\n'},\n",
       " {'index': 138,\n",
       "  'number': 120,\n",
       "  'content': '120\\n4. GEOCENTRIC MODELS\\nMatrix algebra is just a new way to represent ordinary algebra. It is often much more compact. So\\nto make model m4.7 easier to program, we used a matrix multiplication of the basis matrix B by the\\nvector of parameters w: B %*% w. This notation is just linear algebra shorthand for (1) multiplying\\neach element of the vector w by each value in the corresponding row of B and then (2) summing up\\neach result. You could also fit the same model with the following less-elegant code:\\nR code\\n4.79\\nm4.7alt <- quap(\\nalist(\\nD ~ dnorm( mu , sigma ) ,\\nmu <- a + sapply( 1:827 , function(i) sum( B[i,]*w ) ) ,\\na ~ dnorm(100,1),\\nw ~ dnorm(0,10),\\nsigma ~ dexp(1)\\n),\\ndata=list( D=d2$doy , B=B ) ,\\nstart=list( w=rep( 0 , ncol(B) ) ) )\\nSo you end up with exactly what you need: A sum linear predictor for each year (row). If you haven’t\\nworked with much linear algebra, matrix notation can be intimidating. It is useful to remember that it\\nis nothing more than the mathematics you already know, but expressed in a highly compressed form\\nthat is convenient when working with repeated calculations on lists of numbers.\\n4.5.3. Smooth functions for a rough world. The splines in the previous section are just the\\nbeginning. A entire class of models, generalized additive models (GAMs), focuses on\\npredicting an outcome variable using smooth functions of some predictor variables. The\\ntopic is deep enough to deserve its own book.78\\n4.6. Summary\\nThis chapter introduced the simple linear regression model, a framework for estimating\\nthe association between a predictor variable and an outcome variable. The Gaussian distri-\\nbution comprises the likelihood in such models, because it counts up the relative numbers of\\nways different combinations of means and standard deviations can produce an observation.\\nTo fit these models to data, the chapter introduced quadratic approximation of the posterior\\ndistribution and the tool quap. It also introduced new procedures for visualizing prior and\\nposterior distributions.\\nThe next chapter expands on these concepts by introducing regression models with more\\nthan one predictor variable. The basic techniques from this chapter are the foundation of\\nmost of the examples in future chapters. So if much of the material was new to you, it might\\nbe worth reviewing this chapter now, before pressing onwards.\\n4.7. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n4E1. In the model definition below, which line is the likelihood?\\nyi ∼Normal(µ, σ)\\nµ ∼Normal(0, 10)\\nσ ∼Exponential(1)\\n4E2. In the model definition just above, how many parameters are in the posterior distribution?\\n'},\n",
       " {'index': 139,\n",
       "  'number': 121,\n",
       "  'content': '4.7. PRACTICE\\n121\\n4E3. Using the model definition above, write down the appropriate form of Bayes’ theorem that\\nincludes the proper likelihood and priors.\\n4E4. In the model definition below, which line is the linear model?\\nyi ∼Normal(µ, σ)\\nµi = α + βxi\\nα ∼Normal(0, 10)\\nβ ∼Normal(0, 1)\\nσ ∼Exponential(2)\\n4E5. In the model definition just above, how many parameters are in the posterior distribution?\\n4M1. For the model definition below, simulate observed y values from the prior (not the posterior).\\nyi ∼Normal(µ, σ)\\nµ ∼Normal(0, 10)\\nσ ∼Exponential(1)\\n4M2. Translate the model just above into a quap formula.\\n4M3. Translate the quap model formula below into a mathematical model definition.\\ny ~ dnorm( mu , sigma ),\\nmu <- a + b*x,\\na ~ dnorm( 0 , 10 ),\\nb ~ dunif( 0 , 1 ),\\nsigma ~ dexp( 1 )\\n4M4. A sample of students is measured for height each year for 3 years. After the third year, you want\\nto fit a linear regression predicting height using year as a predictor. Write down the mathematical\\nmodel definition for this regression, using any variable names and priors you choose. Be prepared to\\ndefend your choice of priors.\\n4M5. Now suppose I remind you that every student got taller each year. Does this information lead\\nyou to change your choice of priors? How?\\n4M6. Now suppose I tell you that the variance among heights for students of the same age is never\\nmore than 64cm. How does this lead you to revise your priors?\\n4M7. Refit model m4.3 from the chapter, but omit the mean weight xbar this time. Compare the\\nnew model’s posterior to that of the original model. In particular, look at the covariance among the\\nparameters. What is different? Then compare the posterior predictions of both models.\\n4M8. In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots\\nand observe what happens to the resulting spline. Then adjust also the width of the prior on the\\nweights—change the standard deviation of the prior and watch what happens. What do you think\\nthe combination of knot number and the prior on the weights controls?\\n'},\n",
       " {'index': 140,\n",
       "  'number': 122,\n",
       "  'content': '122\\n4. GEOCENTRIC MODELS\\n4H1. The weights listed below were recorded in the !Kung census, but heights were not recorded for\\nthese individuals. Provide predicted heights and 89% intervals for each of these individuals. That is,\\nfill in the table below, using model-based predictions.\\nIndividual\\nweight\\nexpected height\\n89% interval\\n1\\n46.95\\n2\\n43.72\\n3\\n64.78\\n4\\n32.59\\n5\\n54.63\\n4H2. Select out all the rows in the Howell1 data with ages below 18 years of age. If you do it right,\\nyou should end up with a new data frame with 192 rows in it.\\n(a) Fit a linear regression to these data, using quap. Present and interpret the estimates. For\\nevery 10 units of increase in weight, how much taller does the model predict a child gets?\\n(b) Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Super-\\nimpose the MAP regression line and 89% interval for the mean. Also superimpose the 89% interval\\nfor predicted heights.\\n(c) What aspects of the model fit concern you? Describe the kinds of assumptions you would\\nchange, if any, to improve the model. You don’t have to write any new code. Just explain what the\\nmodel appears to be doing a bad job of, and what you hypothesize would be a better model.\\n4H3. Suppose a colleague of yours, who works on allometry, glances at the practice problems just\\nabove. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body\\nweight that scales with height!” Let’s take your colleague’s advice and see what happens.\\n(a) Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use\\nthe entire Howell1 data frame, all 544 rows, adults and non-adults. Can you interpret the resulting\\nestimates?\\n(b) Begin with this plot: plot( height ~ weight , data=Howell1 ). Then use samples\\nfrom the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the\\npredicted mean height as a function of weight, (2) the 97% interval for the mean, and (3) the 97%\\ninterval for predicted heights.\\n4H4. Plot the prior predictive distribution for the parabolic polynomial regression model in the\\nchapter. You can modify the code that plots the linear regression prior predictive distribution. Can\\nyou modify the prior distributions of α, β1, and β2 so that the prior predictions stay within the bio-\\nlogically reasonable outcome space? That is to say: Do not try to fit the data by hand. But do try to\\nkeep the curves consistent with what you know about height and weight, before seeing these exact\\ndata.\\n4H5. Return to data(cherry_blossoms) and model the association between blossom date (doy)\\nand March temperature (temp). Note that there are many missing values in both variables. You may\\nconsider a linear model, a polynomial, or a spline on temperature. How well does temperature trend\\npredict the blossom trend?\\n4H6. Simulate the prior predictive distribution for the cherry blossom spline in the chapter. Adjust\\nthe prior on the weights and observe what happens. What do you think the prior on the weights is\\ndoing?\\n4H8. The cherry blossom spline in the chapter used an intercept α, but technically it doesn’t require\\none. The first basis functions could substitute for the intercept. Try refitting the cherry blossom spline\\nwithout the intercept. What else about the model do you need to change to make this work?\\n'},\n",
       " {'index': 141,\n",
       "  'number': 123,\n",
       "  'content': '5 The Many Variables & The Spurious Waffles\\nOne of the most reliable sources of waffles in North America, if not the entire world, is\\na Waffle House diner. Waffle House is nearly always open, even just after a hurricane. Most\\ndiners invest in disaster preparedness, including having their own electrical generators. As a\\nconsequence, the United States’ disaster relief agency (FEMA) informally uses Waffle House\\nas an index of disaster severity.79 If the Waffle House is closed, that’s a serious event.\\nIt is ironic then that steadfast Waffle House is associated with the nation’s highest divorce\\nrates (Figure 5.1). States with many Waffle Houses per person, like Georgia and Alabama,\\nalso have some of the highest divorce rates in the United States. The lowest divorce rates are\\nfound where there are zero Waffle Houses. Could always-available waffles and hash brown\\npotatoes put marriage at risk?\\nProbably not. This is an example of a misleading correlation. No one thinks there is any\\nplausible mechanism by which Waffle House diners make divorce more likely. Instead, when\\nwe see a correlation of this kind, we immediately start asking about other variables that are\\nreally driving the relationship between waffles and divorce. In this case, Waffle House began\\nin Georgia in the year 1955. Over time, the diners spread across the Southern United States,\\nremaining largely within it. So Waffle House is associated with the South. Divorce is not a\\nuniquely Southern institution, but the Southern United States has some of the highest divorce\\nrates in the nation. So it’s probably just an accident of history that Waffle House and high\\ndivorce rates both occur in the South.\\nSuch accidents are commonplace. It is not surprising that Waffle House is correlated\\nwith divorce, because correlation in general is not surprising. In large data sets, every pair\\nof variables has a statistically discernible non-zero correlation.80 But since most correlations\\ndo not indicate causal relationships, we need tools for distinguishing mere association from\\nevidence of causation. This is why so much effort is devoted to multiple regression, using\\nmore than one predictor variable to simultaneously model an outcome. Reasons given for\\nmultiple regression models include:\\n(1) Statistical “control” for confounds. A confound is something that misleads us about\\na causal influence—there will be a more precise definition in the next chapter. The\\nspurious waffles and divorce correlation is one type of confound, where southern-\\nness makes a variable with no real importance (Waffle House density) appear to\\nbe important. But confounds are diverse. They can hide important effects just as\\neasily as they can produce false ones.\\n(2) Multiple and complex causation. A phenomenon may arise from multiple simul-\\ntaneous causes, and causes can cascade in complex ways. And since one cause can\\nhide another, they must be measured simultaneously.\\n123\\n'},\n",
       " {'index': 142,\n",
       "  'number': 124,\n",
       "  'content': '124\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\n0\\n10\\n20\\n30\\n40\\n50\\n6\\n8\\n10\\n12\\n14\\nWaffle Houses per million\\nDivorce rate\\nAL\\nAR\\nGA\\nME\\nNJ\\nOK\\nSC\\nFigure 5.1. The number of Waffle House\\ndiners per million people is associated with\\ndivorce rate (in the year 2009) within the\\nUnited States. Each point is a State. “South-\\nern” (former Confederate) States shown in\\nblue.\\nShaded region is 89% percentile in-\\nterval of the mean.\\nThese data are in\\ndata(WaffleDivorce) in the rethinking\\npackage.\\n(3) Interactions. The importance of one variable may depend upon another. For ex-\\nample, plants benefit from both light and water. But in the absence of either, the\\nother is no benefit at all. Such interactions occur very often. Effective inference\\nabout one variable will often depend upon consideration of others.\\nIn this chapter, we begin to deal with the first of these two, using multiple regression to\\ndeal with simple confounds and to take multiple measurements of association. You’ll see how\\nto include any arbitrary number of main effects in your linear model of the Gaussian mean.\\nThese main effects are additive combinations of variables, the simplest type of multiple vari-\\nable model. We’ll focus on two valuable things these models can help us with: (1) revealing\\nspurious correlations like the Waffle House correlation with divorce and (2) revealing impor-\\ntant correlations that may be masked by unrevealed correlations with other variables. Along\\nthe way, you’ll meet categorical variables, which require special handling compared\\nto continuous variables.\\nHowever, multiple regression can be worse than useless, if we don’t know how to use\\nit. Just adding variables to a model can do a lot of damage. In this chapter, we’ll begin to\\nthink formally about causal inference and introduce graphical causal models as a way to\\ndesign and interpret regression models. The next chapter continues on this theme, describing\\nsome serious and common dangers of adding predictor variables, ending with a unifying\\nframework for understanding the examples in both this chapter and the next.\\nRethinking: Causal inference. Despite its central importance, there is no unified approach to causal\\ninference yet in the sciences. There are even people who argue that cause does not really exist; it’s just\\na psychological illusion.81 And in complex dynamical systems, everything seems to cause everything\\nelse. “Cause” loses intuitive value. About one thing, however, there is general agreement: Causal\\ninference always depends upon unverifiable assumptions. Another way to say this is that it’s always\\npossible to imagine some way in which your inference about cause is mistaken, no matter how careful\\nthe design or analysis. A lot can be accomplished, despite this barrier.82\\n'},\n",
       " {'index': 143,\n",
       "  'number': 125,\n",
       "  'content': '5.1. SPURIOUS ASSOCIATION\\n125\\nMarriage rate\\nDivorce rate\\n13\\n20\\n30\\n6\\n10\\n13\\nMedian age marriage\\nDivorce rate\\n23\\n26\\n29\\n6\\n10\\n13\\nFigure 5.2. Divorce rate is associated with both marriage rate (left) and\\nmedian age at marriage (right). Both predictor variables are standardized in\\nthis example. The average marriage rate across States is 20 per 1000 adults,\\nand the average median age at marriage is 26 years.\\n5.1. Spurious association\\nLet’s leave waffles behind, at least for the moment. An example that is easier to under-\\nstand is the correlation between divorce rate and marriage rate (Figure 5.2). The rate at\\nwhich adults marry is a great predictor of divorce rate, as seen in the left-hand plot in the\\nfigure. But does marriage cause divorce? In a trivial sense it obviously does: One cannot get\\na divorce without first getting married. But there’s no reason high marriage rate must cause\\nmore divorce. It’s easy to imagine high marriage rate indicating high cultural valuation of\\nmarriage and therefore being associated with low divorce rate.\\nAnother predictor associated with divorce is the median age at marriage, displayed in\\nthe right-hand plot in Figure 5.2. Age at marriage is also a good predictor of divorce rate—\\nhigher age at marriage predicts less divorce. But there is no reason this has to be causal,\\neither, unless age at marriage is very late and the spouses do not live long enough to get a\\ndivorce.\\nLet’s load these data and standardize the variables of interest:\\nR code\\n5.1\\n# load data and copy\\nlibrary(rethinking)\\ndata(WaffleDivorce)\\nd <- WaffleDivorce\\n# standardize variables\\nd$D <- standardize( d$Divorce )\\nd$M <- standardize( d$Marriage )\\nd$A <- standardize( d$MedianAgeMarriage )\\n'},\n",
       " {'index': 144,\n",
       "  'number': 126,\n",
       "  'content': '126\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nYou can replicate the right-hand plot in the figure using a linear regression model:\\nDi ∼Normal(µi, σ)\\nµi = α + βAAi\\nα ∼Normal(0, 0.2)\\nβA ∼Normal(0, 0.5)\\nσ ∼Exponential(1)\\nDi is the standardized (zero centered, standard deviation one) divorce rate for State i, and\\nAi is State i’s standardized median age at marriage. The linear model structure should be\\nfamiliar from the previous chapter.\\nWhat about those priors? Since the outcome and the predictor are both standardized, the\\nintercept α should end up very close to zero. What does the prior slope βA imply? If βA = 1,\\nthat would imply that a change of one standard deviation in age at marriage is associated\\nlikewise with a change of one standard deviation in divorce. To know whether or not that is\\na strong relationship, you need to know how big a standard deviation of age at marriage is:\\nR code\\n5.2\\nsd( d$MedianAgeMarriage )\\n[1] 1.24363\\nSo when βA = 1, a change of 1.2 years in median age at marriage is associated with a full\\nstandard deviation change in the outcome variable. That seems like an insanely strong rela-\\ntionship. The prior above thinks that only 5% of plausible slopes are more extreme than 1.\\nWe’ll simulate from these priors in a moment, so you can see how they look in the outcome\\nspace.\\nTo compute the approximate posterior, there are no new code tricks or techniques here.\\nBut I’ll add comments to help explain the mass of code to follow.\\nR code\\n5.3\\nm5.1 <- quap(\\nalist(\\nD ~ dnorm( mu , sigma ) ,\\nmu <- a + bA * A ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbA ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data = d )\\nTo simulate from the priors, we can use extract.prior and link as in the previous chapter.\\nI’ll plot the lines over the range of 2 standard deviations for both the outcome and predictor.\\nThat’ll cover most of the possible range of both variables.\\nR code\\n5.4\\nset.seed(10)\\nprior <- extract.prior( m5.1 )\\nmu <- link( m5.1 , post=prior , data=list( A=c(-2,2) ) )\\nplot( NULL , xlim=c(-2,2) , ylim=c(-2,2) )\\nfor ( i in 1:50 ) lines( c(-2,2) , mu[i,] , col=col.alpha(\"black\",0.4) )\\n'},\n",
       " {'index': 145,\n",
       "  'number': 127,\n",
       "  'content': '5.1. SPURIOUS ASSOCIATION\\n127\\n-2\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\nMedian age marriage (std)\\nDivorce rate (std)\\nFigure 5.3. Plausible regression lines implied\\nby the priors in m5.1. These are weakly infor-\\nmative priors in that they allow some implusi-\\nbly strong relationships but generally bound\\nthe lines to possible ranges of the variables.\\nFigure 5.3 displays the result. You may wish to try some vaguer, flatter priors and see how\\nquickly the prior regression lines become ridiculous.\\nNow for the posterior predictions. The procedure is exactly like the examples from the\\nprevious chapter: link, then summarize with mean and PI, and then plot.\\nR code\\n5.5\\n# compute percentile interval of mean\\nA_seq <- seq( from=-3 , to=3.2 , length.out=30 )\\nmu <- link( m5.1 , data=list(A=A_seq) )\\nmu.mean <- apply( mu , 2, mean )\\nmu.PI <- apply( mu , 2 , PI )\\n# plot it all\\nplot( D ~ A , data=d , col=rangi2 )\\nlines( A_seq , mu.mean , lwd=2 )\\nshade( mu.PI , A_seq )\\nIf you inspect the precis output, you’ll see that posterior for βA is reliably negative, as seen\\nin Figure 5.2.\\nYou can fit a similar regression for the relationship in the left-hand plot:\\nR code\\n5.6\\nm5.2 <- quap(\\nalist(\\nD ~ dnorm( mu , sigma ) ,\\nmu <- a + bM * M ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbM ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data = d )\\nAs you can see in the figure, this relationship isn’t as strong as the previous one.\\nBut merely comparing parameter means between different bivariate regressions is no\\nway to decide which predictor is better. Both of these predictors could provide independent\\nvalue, or they could be redundant, or one could eliminate the value of the other.\\n'},\n",
       " {'index': 146,\n",
       "  'number': 128,\n",
       "  'content': '128\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nTo make sense of this, we’re going to have to think causally. And then, only after we’ve\\ndone some thinking, a bigger regression model that includes both age at marriage and mar-\\nriage rate will help us.\\n5.1.1. Think before you regress. There are three observed variables in play: divorce rate\\n(D), marriage rate (M), and the median age at marriage (A) in each State. The pattern we\\nsee in the previous two models and illustrated in Figure 5.2 is symptomatic of a situation in\\nwhich only one of the predictor variables, A in this case, has a causal impact on the outcome,\\nD, even though both predictor variables are strongly associated with the outcome.\\nTo understand this better, it is helpful to introduce a particular type of causal graph\\nknown as a DAG, short for directed acyclic graph. Graph means it is nodes and con-\\nnections. Directed means the connections have arrows that indicate directions of causal in-\\nfluence. And acyclic means that causes do not eventually flow back on themselves. A DAG\\nis a way of describing qualitative causal relationships among variables. It isn’t as detailed as\\na full model description, but it contains information that a purely statistical model does not.\\nUnlike a statistical model, a DAG will tell you the consequences of intervening to change a\\nvariable. But only if the DAG is correct. There is no inference without assumption.\\nThe full framework for using DAGs to design and critique statistical models is compli-\\ncated. So instead of smothering you in the whole framework right now, I’ll build it up one\\nexample at a time. By the end of the next chapter, you’ll have a set of simple rules that let\\nyou accomplish quite a lot of criticism. And then other applications will be introduced in\\nlater chapters.\\nLet’s start with the basics. Here is a possible DAG for our divorce rate example:\\nA\\nD\\nM\\nIf you want to see the code to draw this, see the Overthinking box at the end of this section. It\\nmay not look like much, but this type of diagram does a lot of work. It represents a heuristic\\ncausal model. Like other models, it is an analytical assumption. The symbols A, M, and D\\nare our observed variables. The arrows show directions of influence. What this DAG says is:\\n(1) A directly influences D\\n(2) M directly influences D\\n(3) A directly influences M\\nThese statements can then have further implications. In this case, age of marriage influences\\ndivorce in two ways. First it has a direct effect, A →D. Perhaps a direct effect would arise\\nbecause younger people change faster than older people and are therefore more likely to grow\\nincompatible with a partner. Second, it has an indirect effect by influencing the marriage rate,\\nwhich then influences divorce, A →M →D. If people get married earlier, then the marriage\\nrate may rise, because there are more young people. Consider for example if an evil dictator\\nforced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than\\nto 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has\\nany direct effect on divorce, maybe by making marriage more or less normative, then some\\nof that direct effect could be the indirect effect of age at marriage.\\n'},\n",
       " {'index': 147,\n",
       "  'number': 129,\n",
       "  'content': '5.1. SPURIOUS ASSOCIATION\\n129\\nTo infer the strength of these different arrows, we need more than one statistical model.\\nModel m5.1, the regression of D on A, tells us only that the total influence of age at marriage is\\nstrongly negative with divorce rate. The “total” here means we have to account for every path\\nfrom A to D. There are two such paths in this graph: A →D, a direct path, and A →M →D,\\nan indirect path. In general, it is possible that a variable like A has no direct effect at all on an\\noutcome like D. It could still be associated with D entirely through the indirect path. That\\ntype of relationship is known as mediation, and we’ll have another example later.\\nAs you’ll see however, the indirect path does almost no work in this case. How can we\\nshow that? We know from m5.2 that marriage rate is positively associated with divorce rate.\\nBut that isn’t enough to tell us that the path M →D is positive. It could be that the association\\nbetween M and D arises entirely from A’s influence on both M and D. Like this:\\nA\\nD\\nM\\nThis DAG is also consistent with the posterior distributions of models m5.1 and m5.2. Why?\\nBecause both M and D “listen” to A. They have information from A. So when you inspect\\nthe association between D and M, you pick up that common information that they both got\\nfrom listening to A. You’ll see a more formal way to deduce this, in the next chapter.\\nSo which is it? Is there a direct effect of marriage rate, or rather is age at marriage just\\ndriving both, creating a spurious correlation between marriage rate and divorce rate? To find\\nout, we need to consider carefully what each DAG implies. That’s what’s next.\\nRethinking: What’s a cause? Questions of causation can become bogged down in philosophical\\ndebates. These debates are worth having. But they don’t usually intersect with statistical concerns.\\nKnowing a cause in statistics means being able to correctly predict the consequences of an interven-\\ntion. There are contexts in which even this is complicated. For example, it isn’t possible to directly\\nchange someone’s body weight. Changing someone’s body weight would mean intervening on an-\\nother variable, like diet, and that variable would have other causal effects in addition. But being\\nunderweight can still be a legitimate cause of disease, even when we can’t intervene on it directly.\\nOverthinking: Drawing a DAG. There are several packages for drawing and analyzing DAGs. In this\\nbook, we’ll use dagitty. It is both an R package and something you can use in your internet browser:\\nhttp://www.dagitty.net/. To draw the simple DAG you saw earlier in this section:\\nR code\\n5.7\\nlibrary(dagitty)\\ndag5.1 <- dagitty( \"dag{ A -> D; A -> M; M -> D }\" )\\ncoordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )\\ndrawdag( dag5.1 )\\nThe -> arrows in the DAG definition indicate directions of influence. The coordinates function lets\\nyou arrange the plot as you like.\\n'},\n",
       " {'index': 148,\n",
       "  'number': 130,\n",
       "  'content': '130\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\n5.1.2. Testable implications. How do we use data to compare multiple, plausible causal\\nmodels? The first thing to consider is the testable implications of each model. Con-\\nsider the two DAGs we have so far considered:\\nA\\nD\\nM\\nA\\nD\\nM\\nAny DAG may imply that some variables are independent of others under certain condi-\\ntions. These are the model’s testable implications, its conditional independencies. Con-\\nditional independencies come in two forms. First, they are statements of which variables\\nshould be associated with one another (or not) in the data. Second, they are statements of\\nwhich variables become dis-associated when we condition on some other set of variables.\\nWhat does “conditioning” mean? Informally, conditioning on a variable Z means learn-\\ning its value and then asking if X adds any additional information about Y. If learning X\\ndoesn’t give you any more information about Y, then we might say that Y is independent of\\nX conditional on Z. This conditioning statement is sometimes written as: Y ⊥⊥X|Z. This\\nis very weird notation and any feelings of annoyance on your part are justified. We’ll work\\nwith this concept a lot, so don’t worry if it doesn’t entirely make sense right now. You’ll see\\nexamples very soon.\\nLet’s consider conditional independence in the context of the divorce example. What are\\nthe conditional independencies of the DAGs at the top of the page? How do we derive these\\nconditional independencies? Finding conditional independencies is not hard, but also not\\nat all obvious. With a little practice, it becomes very easy. The more general rules can wait\\nuntil the next chapter. For now, let’s consider each DAG in turn and inspect the possibilities.\\nFor the DAG on the left above, the one with three arrows, first note that every pair of\\nvariables is correlated. This is because there is a causal arrow between every pair. These\\narrows create correlations. So before we condition on anything, everything is associated\\nwith everything else. This is already a testable implication. We could write it:\\nD ̸⊥⊥A\\nD ̸⊥⊥M\\nA ̸⊥⊥M\\nThat ̸⊥⊥thing means “not independent of.” If we now look in the data and find that any pair of\\nvariables are not associated, then something is wrong with the DAG (assuming the data are\\ncorrect). In these data, all three pairs are in fact strongly associated. Check for yourself. You\\ncan use cor to measure simple correlations. Correlations are sometimes terrible measures of\\nassociation—many different patterns of association with different implications can produce\\nthe same correlation. But they do honest work in this case.\\nAre there any other testable implications for the first DAG above? No. It will be easier to\\nsee why, if we slide over to consider the second DAG, the one in which M has no influence\\non D. In this DAG, it is still true that all three variables are associated with one another. A is\\nassociated with D and M because it influences them both. And D and M are associated with\\none another, because M influences them both. They share a cause, and this leads them to be\\ncorrelated with one another through that cause. But suppose we condition on A. All of the\\ninformation in M that is relevant to predicting D is in A. So once we’ve conditioned on A,\\nM tells us nothing more about D. So in the second DAG, a testable implication is that D is\\nindependent of M, conditional on A. In other words, D ⊥⊥M|A. The same thing does not\\n'},\n",
       " {'index': 149,\n",
       "  'number': 131,\n",
       "  'content': \"5.1. SPURIOUS ASSOCIATION\\n131\\nhappen with the first DAG. Conditioning on A does not make D independent of M, because\\nM really influences D all by itself in this model.\\nIn the next chapter, I’ll show you the general rules for deducing these implications. For\\nnow, the dagitty package has the rules built in and can find the implications for you. Here’s\\nthe code to define the second DAG and display the implied conditional independencies.\\nR code\\n5.8\\nDMA_dag2 <- dagitty('dag{ D <- A -> M }')\\nimpliedConditionalIndependencies( DMA_dag2 )\\nD _||_ M | A\\nThe first DAG has no conditional independencies. You can define it and check with this:\\nR code\\n5.9\\nDMA_dag1 <- dagitty('dag{ D <- A -> M -> D }')\\nimpliedConditionalIndependencies( DMA_dag1 )\\nThere are no conditional independencies, so there is no output to display.\\nLet’s try to summarize. The testable implications of the first DAG are that all pairs of vari-\\nables should be associated, whatever we condition on. The testable implications of the second\\nDAG are that all pairs of variables should be associated, before conditioning on anything, but\\nthat D and M should be independent after conditioning on A. So the only implication that\\ndiffers between these DAGs is the last one: D ⊥⊥M|A.\\nTo test this implication, we need a statistical model that conditions on A, so we can see\\nwhether that renders D independent of M. And that is what multiple regression helps with.\\nIt can address a useful descriptive question:\\nIs there any additional value in knowing a variable, once I already know all of\\nthe other predictor variables?\\nSo for example once you fit a multiple regression to predict divorce using both marriage rate\\nand age at marriage, the model addresses the questions:\\n(1) After I already know marriage rate, what additional value is there in also knowing\\nage at marriage?\\n(2) After I already know age at marriage, what additional value is there in also knowing\\nmarriage rate?\\nThe parameter estimates corresponding to each predictor are the (often opaque) answers to\\nthese questions. The questions above are descriptive, and the answers are also descriptive. It\\nis only the derivation of the testable implications above that gives these descriptive results a\\ncausal meaning. But that meaning is still dependent upon believing the DAG.\\nRethinking: “Control” is out of control. Very often, the question just above is spoken of as “statisti-\\ncal control,” as in controlling for the effect of one variable while estimating the effect of another. But\\nthis is sloppy language, as it implies too much. Statistical control is quite different from experimental\\ncontrol, as we’ll explore more in the next chapter. The point here isn’t to police language. Instead, the\\npoint is to observe the distinction between small world and large world interpretations. Since most\\npeople who use statistics are not statisticians, sloppy language like “control” can promote a sloppy\\nculture of interpretation. Such cultures tend to overestimate the power of statistical methods, so re-\\nsisting them can be difficult. Disciplining your own language may be enough. Disciplining another’s\\nlanguage is hard to do, without seeming like a fastidious scold, as this very box must seem.\\n\"},\n",
       " {'index': 150,\n",
       "  'number': 132,\n",
       "  'content': '132\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\n5.1.3. Multiple regression notation. Multiple regression formulas look a lot like the poly-\\nnomial models at the end of the previous chapter—they add more parameters and variables\\nto the definition of µi. The strategy is straightforward:\\n(1) Nominate the predictor variables you want in the linear model of the mean.\\n(2) For each predictor, make a parameter that will measure its conditional association\\nwith the outcome.\\n(3) Multiply the parameter by the variable and add that term to the linear model.\\nExamples are always necessary, so here is the model that predicts divorce rate, using both\\nmarriage rate and age at marriage.\\nDi ∼Normal(µi, σ)\\n[probability of data]\\nµi = α + βMMi + βAAi\\n[linear model]\\nα ∼Normal(0, 0.2)\\n[prior for α]\\nβM ∼Normal(0, 0.5)\\n[prior for βM]\\nβA ∼Normal(0, 0.5)\\n[prior for βA]\\nσ ∼Exponential(1)\\n[prior for σ]\\nYou can use whatever symbols you like for the parameters and variables, but here I’ve chosen\\nR for marriage rate and A for age at marriage, reusing these symbols as subscripts for the\\ncorresponding parameters. But feel free to use whichever symbols reduce the load on your\\nown memory.\\nSo what does it mean to assume µi = α+βMMi+βAAi? Mechanically, it means that the\\nexpected outcome for any State with marriage rate Mi and median age at marriage Ai is the\\nsum of three independent terms. If you are like most people, this is still pretty mysterious.\\nThe mechanical meaning of the equation doesn’t map onto a unique causal meaning. Let’s\\ntake care of the mechanical bits first, before returning to interpretation.\\nOverthinking: Compact notation and the design matrix. Often, linear models are written using a\\ncompact form like:\\nµi = α +\\nn\\nX\\nj=1\\nβjxji\\nwhere j is an index over predictor variables and n is the number of predictor variables. This may be\\nread as the mean is modeled as the sum of an intercept and an additive combination of the products of\\nparameters and predictors. Even more compactly, using matrix notation:\\nm = Xb\\nwhere m is a vector of predicted means, one for each row in the data, b is a (column) vector of param-\\neters, one for each predictor variable, and X is a matrix. This matrix is called a design matrix. It has\\nas many rows as the data, and as many columns as there are predictors plus one. So X is basically a\\ndata frame, but with an extra first column. The extra column is filled with 1s. These 1s are multiplied\\nby the first parameter, which is the intercept, and so return the unmodified intercept. When X is\\nmatrix-multiplied by b, you get the predicted means. In R notation, this operation is X %*% b.\\nWe’re not going to use the design matrix approach. But it’s good to recognize it, and sometimes\\nit can save you a lot of work. For example, for linear regressions, there is a nice matrix formula for\\nthe maximum likelihood (or least squares) estimates. Most statistical software exploits that formula.\\n'},\n",
       " {'index': 151,\n",
       "  'number': 133,\n",
       "  'content': '5.1. SPURIOUS ASSOCIATION\\n133\\n5.1.4. Approximating the posterior. To fit this model to the divorce data, we just expand\\nthe linear model. Here’s the model definition again, with the code on the right-hand side:\\nDi ∼Normal(µi, σ)\\nD ~ dnorm(mu,sigma)\\nµi = α + βMMi + βAAi\\nmu <- a + bM*M + bA*A\\nα ∼Normal(0, 0.2)\\na ~ dnorm(0,0.2)\\nβM ∼Normal(0, 0.5)\\nbM ~ dnorm(0,0.5)\\nβA ∼Normal(0, 0.5)\\nbA ~ dnorm(0,0.5)\\nσ ∼Exponential(1)\\nsigma ~ dexp(1)\\nAnd here is the quap code to approximate the posterior distribution:\\nR code\\n5.10\\nm5.3 <- quap(\\nalist(\\nD ~ dnorm( mu , sigma ) ,\\nmu <- a + bM*M + bA*A ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbM ~ dnorm( 0 , 0.5 ) ,\\nbA ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data = d )\\nprecis( m5.3 )\\nmean\\nsd\\n5.5% 94.5%\\na\\n0.00 0.10 -0.16\\n0.16\\nbM\\n-0.07 0.15 -0.31\\n0.18\\nbA\\n-0.61 0.15 -0.85 -0.37\\nsigma\\n0.79 0.08\\n0.66\\n0.91\\nThe posterior mean for marriage rate, bM, is now close to zero, with plenty of probability\\nof both sides of zero. The posterior mean for age at marriage, bA, is essentially unchanged.\\nIt will help to visualize the posterior distributions for all three models, focusing just on the\\nslope parameters βA and βM:\\nR code\\n5.11\\nplot( coeftab(m5.1,m5.2,m5.3), par=c(\"bA\",\"bM\") )\\nm5.1\\nm5.2\\nm5.3\\nm5.1\\nm5.2\\nm5.3\\nbA\\nbM\\n-0.5\\n0.0\\n0.5\\nEstimate\\nThe posterior means are shown by the points and the 89% compatibility intervals by the\\nsolid horizontal lines. Notice how bA doesn’t move, only grows a bit more uncertain, while\\n'},\n",
       " {'index': 152,\n",
       "  'number': 134,\n",
       "  'content': '134\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nbM is only associated with divorce when age at marriage is missing from the model. You can\\ninterpret these distributions as saying:\\nOnce we know median age at marriage for a State, there is little or no addi-\\ntional predictive power in also knowing the rate of marriage in that State.\\nIn that weird notation, D ⊥⊥M|A. This tests the implication of the second DAG from earlier.\\nSince the first DAG did not imply this result, it is out.\\nNote that this does not mean that there is no value in knowing marriage rate. Consistent\\nwith the earlier DAG, if you didn’t have access to age-at-marriage data, then you’d definitely\\nfind value in knowing the marriage rate. M is predictive but not causal. Assuming there\\nare no other causal variables missing from the model (more on that in the next chapter),\\nthis implies there is no important direct causal path from marriage rate to divorce rate. The\\nassociation between marriage rate and divorce rate is spurious, caused by the influence of age\\nof marriage on both marriage rate and divorce rate. I’ll leave it to the reader to investigate\\nthe relationship between age at marriage, A, and marriage rate, M, to complete the picture.\\nBut how did model m5.3 achieve the inference that marriage rate adds no additional\\ninformation, once we know age at marriage? Let’s draw some pictures.\\nOverthinking: Simulating the divorce example. The divorce data are real data. See the sources in\\n?WaffleDivorce. But it is useful to simulate the kind of causal relationships shown in the previous\\nDAG: M ←A →D. Every DAG implies a simulation, and such simulations can help us design\\nmodels to correctly infer relationships among variables. In this case, you just need to simulate each\\nof the three variables:\\nR code\\n5.12\\nN <- 50 # number of simulated States\\nage <- rnorm( N )\\n# sim A\\nmar <- rnorm( N , -age )\\n# sim A -> M\\ndiv <- rnorm( N , age )\\n# sim A -> D\\nNow if you use these variables in models m5.1, m5.2, and m5.3, you’ll see the same pattern of posterior\\ninferences. It is also possible to simulate that both A and M influence D: div <- rnorm(N, age +\\nmar ). In that case, a naive regression of D on A will overestimate the influence of A, just like a\\nnaive regression of D on M will overestimate the importance of M. The multiple regression will help\\nsort things out for you in this situation as well. But interpreting the parameter estimates will always\\ndepend upon what you believe about the causal model, because typically several (or very many) causal\\nmodels are consistent with any one set of parameter estimates. We’ll discuss this later in the chapter\\nas Markov equivalence.\\n5.1.5. Plotting multivariate posteriors. Let’s pause for a moment, before moving on. There\\nare a lot of moving parts here: three variables, some strange DAGs, and three models. If you\\nfeel at all confused, it is only because you are paying attention.\\nIt will help to visualize the model’s inferences. Visualizing the posterior distribution\\nin simple bivariate regressions, like those in the previous chapter, is easy. There’s only one\\npredictor variable, so a single scatterplot can convey a lot of information. And so in the\\nprevious chapter we used scatters of the data. Then we overlaid regression lines and intervals\\nto both (1) visualize the size of the association between the predictor and outcome and (2)\\nto get a crude sense of the ability of the model to predict the individual observations.\\nWith multivariate regression, you’ll need more plots. There is a huge literature detail-\\ning a variety of plotting techniques that all attempt to help one understand multiple linear\\n'},\n",
       " {'index': 153,\n",
       "  'number': 135,\n",
       "  'content': '5.1. SPURIOUS ASSOCIATION\\n135\\nregression. None of these techniques is suitable for all jobs, and most do not generalize be-\\nyond linear regression. So the approach I take here is to instead help you compute whatever\\nyou need from the model. I offer three examples of interpretive plots:\\n(1) Predictor residual plots. These plots show the outcome against residual predictor\\nvalues. They are useful for understanding the statistical model, but not much else.\\n(2) Posterior prediction plots. These show model-based predictions against raw data,\\nor otherwise display the error in prediction. They are tools for checking fit and\\nassessing predictions. They are not causal tools.\\n(3) Counterfactual plots. These show the implied predictions for imaginary experi-\\nments. These plots allow you to explore the causal implications of manipulating\\none or more variables.\\nEach of these plot types has its advantages and deficiencies, depending upon the context and\\nthe question of interest. In the rest of this section, I show you how to manufacture each of\\nthese in the context of the divorce data.\\n5.1.5.1. Predictor residual plots. A predictor residual is the average prediction error when\\nwe use all of the other predictor variables to model a predictor of interest. That’s a compli-\\ncated concept, so we’ll go straight to the example, where it will make sense. The benefit of\\ncomputing these things is that, once plotted against the outcome, we have a bivariate regres-\\nsion that has already conditioned on all of the other predictor variables. It leaves the variation\\nthat is not expected by the model of the mean, µ, as a function of the other predictors.\\nIn our model of divorce rate, we have two predictors: (1) marriage rate M and (2) median\\nage at marriage A. To compute predictor residuals for either, we just use the other predictor\\nto model it. So for marriage rate, this is the model we need:\\nMi ∼Normal(µi, σ)\\nµi = α + βAi\\nα ∼Normal(0, 0.2)\\nβ ∼Normal(0, 0.5)\\nσ ∼Exponential(1)\\nAs before, M is marriage rate and A is median age at marriage. Note that since we standard-\\nized both variables, we already expect the mean α to be around zero, as before. So I’m reusing\\nthe same priors as earlier. This code will approximate the posterior:\\nR code\\n5.13\\nm5.4 <- quap(\\nalist(\\nM ~ dnorm( mu , sigma ) ,\\nmu <- a + bAM * A ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbAM ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data = d )\\nAnd then we compute the residuals by subtracting the observed marriage rate in each State\\nfrom the predicted rate, based upon the model above:\\n'},\n",
       " {'index': 154,\n",
       "  'number': 136,\n",
       "  'content': '136\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\n-2\\n-1\\n0\\n1\\n2\\n3\\n-1\\n0\\n1\\n2\\nAge at marriage (std)\\nMarriage rate (std)\\nDC\\nHI\\nME\\nND\\nWY\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\n3\\nMarriage rate (std)\\nAge at marriage (std)\\nDC\\nHI\\nID\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-2\\n-1\\n0\\n1\\n2\\nMarriage rate residuals\\nDivorce rate (std)\\nDC\\nHI\\nME\\nND\\nWY\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\nAge at marriage residuals\\nDivorce rate (std)\\nDC\\nHI\\nID\\nFigure 5.4. Understanding multiple regression through residuals. The top\\nrow shows each predictor regressed on the other predictor. The lengths of\\nthe line segments connecting the model’s expected value of the outcome,\\nthe regression line, and the actual value are the residuals. In the bottom\\nrow, divorce rate is regressed on the residuals from the top row. Bottom left:\\nResidual variation in marriage rate shows little association with divorce rate.\\nBottom right: Divorce rate on age at marriage residuals, showing remaining\\nvariation, and this variation is associated with divorce rate.\\nR code\\n5.14\\nmu <- link(m5.4)\\nmu_mean <- apply( mu , 2 , mean )\\nmu_resid <- d$M - mu_mean\\nWhen a residual is positive, that means that the observed rate was in excess of what the model\\nexpects, given the median age at marriage in that State. When a residual is negative, that\\nmeans the observed rate was below what the model expects. In simpler terms, States with\\n'},\n",
       " {'index': 155,\n",
       "  'number': 137,\n",
       "  'content': '5.1. SPURIOUS ASSOCIATION\\n137\\npositive residuals have high marriage rates for their median age of marriage, while States\\nwith negative residuals have low rates for their median age of marriage. It’ll help to plot\\nthe relationship between these two variables, and show the residuals as well. In Figure 5.4,\\nupper left, I show m5.4 along with line segments for each residual. Notice that the residuals\\nare variation in marriage rate that is left over, after taking out the purely linear relationship\\nbetween the two variables.\\nNow to use these residuals, let’s put them on a horizontal axis and plot them against the\\nactual outcome of interest, divorce rate. In Figure 5.4 also (lower left), I plot these residuals\\nagainst divorce rate, overlaying the linear regression of the two variables. You can think of\\nthis plot as displaying the linear relationship between divorce and marriage rates, having\\nconditioned already on median age of marriage. The vertical dashed line indicates marriage\\nrate that exactly matches the expectation from median age at marriage. So States to the right\\nof the line have higher marriage rates than expected. States to the left of the line have lower\\nrates. Average divorce rate on both sides of the line is about the same, and so the regression\\nline demonstrates little relationship between divorce and marriage rates.\\nThe same procedure works for the other predictor. The top right plot in Figure 5.4\\nshows the regression of A on M and the residuals. In the lower right, these residuals are used\\nto predict divorce rate. States to the right of the vertical dashed line have older-than-expected\\nmedian age at marriage, while those to the left have younger-than-expected median age at\\nmarriage. Now we find that the average divorce rate on the right is lower than the rate on the\\nleft, as indicated by the regression line. States in which people marry older than expected for\\na given rate of marriage tend to have less divorce.\\nSo what’s the point of all of this? There’s conceptual value in seeing the model-based\\npredictions displayed against the outcome, after subtracting out the influence of other pre-\\ndictors. The plots in Figure 5.4 do this. But this procedure also brings home the message\\nthat regression models measure the remaining association of each predictor with the out-\\ncome, after already knowing the other predictors. In computing the predictor residual plots,\\nyou had to perform those calculations yourself. In the unified multivariate model, it all hap-\\npens automatically. Nevertheless, it is useful to keep this fact in mind, because regressions\\ncan behave in surprising ways as a result. We’ll have an example soon.\\nLinear regression models do all of this simultaneous measurement with a very specific\\nadditive model of how the variables relate to one another. But predictor variables can be\\nrelated to one another in non-additive ways. The basic logic of statistical conditioning does\\nnot change in those cases, but the details definitely do, and these residual plots cease to be\\nuseful. Luckily there are other ways to understand a model. That’s where we turn next.\\nRethinking: Residuals are parameters, not data. There is a tradition, especially in parts of biology,\\nof using residuals from one model as data in another model. For example, a biologist might regress\\nbrain size on body size and then use the brain size residuals as data in another model. This procedure\\nis always a mistake. Residuals are not known. They are parameters, variables with unobserved values.\\nTreating them as known values throws away uncertainty. The right way to adjust for body size is to\\ninclude it in the same model,83 preferably a model designed in light of an explicit causal model.\\n5.1.5.2. Posterior prediction plots. It’s important to check the model’s implied predic-\\ntions against the observed data. This is what you did in Chapter 3, when you simulated\\nglobe tosses, averaging over the posterior, and comparing the simulated results to the ob-\\nserved. These kinds of checks are useful in many ways. For now, we’ll focus on two uses.\\n'},\n",
       " {'index': 156,\n",
       "  'number': 138,\n",
       "  'content': '138\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\n-2\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\nObserved divorce\\nPredicted divorce\\nID\\nME\\nRI\\nUT\\nFigure 5.5. Posterior predictive plot for the\\nmultivariate divorce model, m5.3. The hori-\\nzontal axis is the observed divorce rate in each\\nState. The vertical axis is the model’s posterior\\npredicted divorce rate, given each State’s me-\\ndian age at marriage and marriage rate. The\\nblue line segments are 89% compatibility inter-\\nvals. The diagonal line shows where posterior\\npredictions exactly match the sample.\\n(1) Did the model correctly approximate the posterior distribution? Golems do make\\nmistakes, as do golem engineers. Errors can be more easily diagnosed by compar-\\ning implied predictions to the raw data. Some caution is required, because not all\\nmodels try to exactly match the sample. But even then, you’ll know what to expect\\nfrom a successful approximation. You’ll see some examples later (Chapter 13).\\n(2) How does the model fail? Models are useful fictions. So they always fail in some\\nway. Sometimes, a model fits correctly but is still so poor for our purposes that it\\nmust be discarded. More often, a model predicts well in some respects, but not in\\nothers. By inspecting the individual cases where the model makes poor predictions,\\nyou might get an idea of how to improve it. The difficulty is that this process is\\nessentially creative and relies upon the analyst’s domain expertise. No robot can\\n(yet) do it for you. It also risks chasing noise, a topic we’ll focus on in later chapters.\\nHow could we produce a simple posterior predictive check in the divorce example? Let’s\\nbegin by simulating predictions, averaging over the posterior.\\nR code\\n5.15\\n# call link without specifying new data\\n# so it uses original data\\nmu <- link( m5.3 )\\n# summarize samples across cases\\nmu_mean <- apply( mu , 2 , mean )\\nmu_PI <- apply( mu , 2 , PI )\\n# simulate observations\\n# again no new data, so uses original data\\nD_sim <- sim( m5.3 , n=1e4 )\\nD_PI <- apply( D_sim , 2 , PI )\\nThis code is similar to what you’ve seen before, but now using the original observed data.\\nFor multivariate models, there are many different ways to display these simulations. The\\nsimplest is to just plot predictions against observed. This code will do that, and then add a line\\nto show perfect prediction and line segments for the confidence interval of each prediction:\\n'},\n",
       " {'index': 157,\n",
       "  'number': 139,\n",
       "  'content': '5.1. SPURIOUS ASSOCIATION\\n139\\nR code\\n5.16\\nplot( mu_mean ~ d$D , col=rangi2 , ylim=range(mu_PI) ,\\nxlab=\"Observed divorce\" , ylab=\"Predicted divorce\" )\\nabline( a=0 , b=1 , lty=2 )\\nfor ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_PI[,i] , col=rangi2 )\\nThe resulting plot appears in Figure 5.5. It’s easy to see from this arrangement of the sim-\\nulations that the model under-predicts for States with very high divorce rates while it over-\\npredicts for States with very low divorce rates. That’s normal. This is what regression does—it\\nis skeptical of extreme values, so it expects regression towards the mean. But beyond this gen-\\neral regression to the mean, some States are very frustrating to the model, lying very far from\\nthe diagonal. I’ve labeled some points like this, including Idaho (ID) and Utah (UT), both of\\nwhich have much lower divorce rates than the model expects them to have. The easiest way\\nto label a few select points is to use identify:\\nR code\\n5.17\\nidentify( x=d$D , y=mu_mean , labels=d$Loc )\\nAfter executing the line of code above, R will wait for you to click near a point in the active\\nplot window. It’ll then place a label near that point, on the side you choose. When you are\\ndone labeling points, press your right mouse button (or press esc, on some platforms).\\nWhat is unusual about Idaho and Utah? Both of these States have large proportions of\\nmembers of the Church of Jesus Christ of Latter-day Saints. Members of this church have\\nlow rates of divorce, wherever they live. This suggests that having a finer view on the demo-\\ngraphic composition of each State, beyond just median age at marriage, would help.\\nRethinking: Stats, huh, yeah what is it good for? Often people want statistical modeling to do things\\nthat statistical modeling cannot do. For example, we’d like to know whether an effect is “real” or rather\\nspurious. Unfortunately, modeling merely quantifies uncertainty in the precise way that the model\\nunderstands the problem. Usually answers to large world questions about truth and causation depend\\nupon information not included in the model. For example, any observed correlation between an out-\\ncome and predictor could be eliminated or reversed once another predictor is added to the model.\\nBut if we cannot think of the right variable, we might never notice. Therefore all statistical models are\\nvulnerable to and demand critique, regardless of the precision of their estimates and apparent accu-\\nracy of their predictions. Rounds of model criticism and revision embody the real tests of scientific\\nhypotheses. A true hypothesis will pass and fail many statistical “tests” on its way to acceptance.\\nOverthinking: Simulating spurious association. One way that spurious associations between a pre-\\ndictor and outcome can arise is when a truly causal predictor, call it xreal, influences both the outcome,\\ny, and a spurious predictor, xspur. This can be confusing, however, so it may help to simulate this sce-\\nnario and see both how the spurious data arise and prove to yourself that multiple regression can\\nreliably indicate the right predictor, xreal. So here’s a very basic simulation:\\nR code\\n5.18\\nN <- 100\\n# number of cases\\nx_real <- rnorm( N )\\n# x_real as Gaussian with mean 0 and stddev 1\\nx_spur <- rnorm( N , x_real )\\n# x_spur as Gaussian with mean=x_real\\ny <- rnorm( N , x_real )\\n# y as Gaussian with mean=x_real\\nd <- data.frame(y,x_real,x_spur) # bind all together in data frame\\n'},\n",
       " {'index': 158,\n",
       "  'number': 140,\n",
       "  'content': '140\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nNow the data frame d has 100 simulated cases. Because x_real influences both y and x_spur, you\\ncan think of x_spur as another outcome of x_real, but one which we mistake as a potential predictor\\nof y. As a result, both xreal and xspur are correlated with y. You can see this in the scatterplots from\\npairs(d). But when you include both x variables in a linear regression predicting y, the posterior\\nmean for the association between y and xspur will be close to zero.\\n5.1.5.3. Counterfactual plots. A second sort of inferential plot displays the causal impli-\\ncations of the model. I call these plots counterfactual, because they can be produced for\\nany values of the predictor variables you like, even unobserved combinations like very high\\nmedian age of marriage and very high marriage rate. There are no States with this combi-\\nnation, but in a counterfactual plot, you can ask the model for a prediction for such a State,\\nasking questions like “What would Utah’s divorce rate be, if it’s median age at marriage were\\nhigher?” Used with clarity of purpose, counterfactual plots help you understand the model,\\nas well as generate predictions for imaginary interventions and compute how much some\\nobserved outcome could be attributed to some cause.\\nNote that the term “counterfactual” is highly overloaded in statistics and philosophy. It\\nhardly ever means the same thing when used by different authors. Here, I use it to indicate\\nsome computation that makes use of the structural causal model, going beyond the posterior\\ndistribution. But it could refer to questions about both the past and the future.\\nThe simplest use of a counterfactual plot is to see how the outcome would change as you\\nchange one predictor at a time. If some predictor X took on a new value for one or more cases\\nin our data, how would the outcome Y have changed? Changing just one predictor X might\\nalso change other predictors, depending upon the causal model. Suppose for example that\\nyou pay young couples to postpone marriage until they are 35 years old. Surely this will also\\ndecrease the number of couples who ever get married—some people will die before turning\\n35, among other reasons—decreasing the overall marriage rate. An extraordinary and evil\\ndegree of control over people would be necessary to really hold marriage rate constant while\\nforcing everyone to marry at a later age.\\nSo let’s see how to generate plots of model predictions that take the causal structure into\\naccount. The basic recipe is:\\n(1) Pick a variable to manipulate, the intervention variable.\\n(2) Define the range of values to set the intervention variable to.\\n(3) For each value of the intervention variable, and for each sample in posterior, use\\nthe causal model to simulate the values of other variables, including the outcome.\\nIn the end, you end up with a posterior distribution of counterfactual outcomes that you can\\nplot and summarize in various ways, depending upon your goal.\\nLet’s see how to do this for the divorce model. Again we take this DAG as given:\\nA\\nD\\nM\\nTo simulate from this, we need more than the DAG. We also need a set of functions that tell\\nus how each variable is generated. For simplicity, we’ll use Gaussian distributions for each\\nvariable, just like in model m5.3. But model m5.3 ignored the assumption that A influences\\n'},\n",
       " {'index': 159,\n",
       "  'number': 141,\n",
       "  'content': '5.1. SPURIOUS ASSOCIATION\\n141\\nM. We didn’t need that to estimate A →D. But we do need it to predict the consequences\\nof manipulating A, because some of the effect of A acts through M.\\nTo estimate the influence of A on M, all we need is to regress A on M. There are no\\nother variables in the DAG creating an association between A and M. We can just add this\\nregression to the quap model, running two regressions at the same time:\\nR code\\n5.19\\ndata(WaffleDivorce)\\nd <- list()\\nd$A <- standardize( WaffleDivorce$MedianAgeMarriage )\\nd$D <- standardize( WaffleDivorce$Divorce )\\nd$M <- standardize( WaffleDivorce$Marriage )\\nm5.3_A <- quap(\\nalist(\\n## A -> D <- M\\nD ~ dnorm( mu , sigma ) ,\\nmu <- a + bM*M + bA*A ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbM ~ dnorm( 0 , 0.5 ) ,\\nbA ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 ),\\n## A -> M\\nM ~ dnorm( mu_M , sigma_M ),\\nmu_M <- aM + bAM*A,\\naM ~ dnorm( 0 , 0.2 ),\\nbAM ~ dnorm( 0 , 0.5 ),\\nsigma_M ~ dexp( 1 )\\n) , data = d )\\nLook at the precis(5.3_A) summary. You’ll see that M and A are strongly negatively asso-\\nciated. If we interpret this causally, it indicates that manipulating A reduces M.\\nThe goal is to simulate what would happen, if we manipulate A. So next we define a\\nrange of values for A.\\nR code\\n5.20\\nA_seq <- seq( from=-2 , to=2 , length.out=30 )\\nThis defines a list of 30 imaginary interventions, ranging from 2 standard deviations below\\nand 2 above the mean. Now we can use sim, which you met in the previous chapter, to\\nsimulate observations from model m5.3_A. But this time we’ll tell it to simulate both M and\\nD, in that order. Why in that order? Because we have to simulate the influence of A on M\\nbefore we simulate the joint influence of A and M on D. The vars argument to sim tells it\\nboth which observables to simulate and in which order.\\nR code\\n5.21\\n# prep data\\nsim_dat <- data.frame( A=A_seq )\\n# simulate M and then D, using A_seq\\ns <- sim( m5.3_A , data=sim_dat , vars=c(\"M\",\"D\") )\\n'},\n",
       " {'index': 160,\n",
       "  'number': 142,\n",
       "  'content': '142\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\n-2\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\nmanipulated A\\ncounterfactual D\\nTotal counterfactual effect of A on D\\n-2\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\nmanipulated A\\ncounterfactual M\\nCounterfactual effect A -> M\\nFigure 5.6. Counterfactual plots for the multivariate divorce model, m5.3.\\nThese plots visualize the predicted effect of manipulating age at marriage A\\non divorce rate D. Left: Total causal effect of manipulating A (horizontal)\\non D. This plot contains both paths, A →D and A →M →D. Right:\\nSimulated values of M show the estimated influence A →M.\\nThat’s all there is to it. But do at least glance at the Overthinking box at the end of this\\nsection, where I show you the individual steps, so you can perform this kind of counterfactual\\nsimulation for any model fit with any software. Now to plot the predictions:\\nR code\\n5.22\\nplot( sim_dat$A , colMeans(s$D) , ylim=c(-2,2) , type=\"l\" ,\\nxlab=\"manipulated A\" , ylab=\"counterfactual D\"\\n)\\nshade( apply(s$D,2,PI) , sim_dat$A )\\nmtext( \"Total counterfactual effect of A on D\" )\\nThe resulting plot is shown in Figure 5.6 (left side). This predicted trend in D includes both\\npaths: A →D and A →M →D. We found previously that M →D is very small, so the\\nsecond path doesn’t contribute much to the trend. But if M were to strongly influence D, the\\ncode above would include the effect. The counterfactual simulation also generated values for\\nM. These are shown on the right in Figure 5.6. The object s from the code above includes\\nthese simulated M values. Try to reproduce the figure yourself.\\nOf course these calculations also permit numerical summaries. For example, the ex-\\npected causal effect of increasing median age at marriage from 20 to 30 is:\\nR code\\n5.23\\n# new data frame, standardized to mean 26.1 and std dev 1.24\\nsim2_dat <- data.frame( A=(c(20,30)-26.1)/1.24 )\\ns2 <- sim( m5.3_A , data=sim2_dat , vars=c(\"M\",\"D\") )\\nmean( s2$D[,2] - s2$D[,1] )\\n[1] -4.591425\\nThis is a huge effect of four and one half standard deviations, probably impossibly large.\\n'},\n",
       " {'index': 161,\n",
       "  'number': 143,\n",
       "  'content': '5.1. SPURIOUS ASSOCIATION\\n143\\n-2\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\nmanipulated M\\ncounterfactual D\\nTotal counterfactual effect of M on D\\nFigure 5.7. The counterfactual effect of ma-\\nnipulating marriage rate M on divorce rate D.\\nSince M →D was estimated to be very small,\\nthere is no strong trend here. By manipulating\\nM, we break the influence of A on M, and this\\nremoves the association between M and D.\\nThe trick with simulating counterfactuals is to realize that when we manipulate some\\nvariable X, we break the causal influence of other variables on X. This is the same as saying\\nwe modify the DAG so that no arrows enter X. Suppose for example that we now simulate\\nthe effect of manipulating M. This implies the DAG:\\nA\\nD\\nM\\nThe arrow A →M is deleted, because if we control the values of M, then A no longer influ-\\nences it. It’s like a perfectly controlled experiment. Now we can modify the code above to\\nsimulate the counterfactual result of manipulating M. We’ll simulate a counterfactual for an\\naverage state, with A = 0, and see what changing M does.\\nR code\\n5.24\\nsim_dat <- data.frame( M=seq(from=-2,to=2,length.out=30) , A=0 )\\ns <- sim( m5.3_A , data=sim_dat , vars=\"D\" )\\nplot( sim_dat$M , colMeans(s) , ylim=c(-2,2) , type=\"l\" ,\\nxlab=\"manipulated M\" , ylab=\"counterfactual D\"\\n)\\nshade( apply(s,2,PI) , sim_dat$M )\\nmtext( \"Total counterfactual effect of M on D\" )\\nWe only simulate D now—note the vars argument to sim() in the code above. We don’t\\nsimulate A, because M doesn’t influence it. I show this plot in Figure 5.7. This trend is less\\nstrong, because there is no evidence for a strong influence of M on D.\\nIn more complex models with many potential paths, the same strategy will compute\\ncounterfactuals for an exposure of interest. But as you’ll see in later examples, often it is\\nsimply not possible to estimate a plausible, un-confounded causal effect of some exposure\\nX on some outcome Y. But even in those cases, there are still important counterfactuals to\\nconsider. So we’ll return to this theme in future chapters.\\n'},\n",
       " {'index': 162,\n",
       "  'number': 144,\n",
       "  'content': '144\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nOverthinking: Simulating counterfactuals. The example in this section used sim() to hide the de-\\ntails. But simulating counterfactuals on your own is not hard. It just uses the model definition.\\nAssume we’ve already fit model m5.3_A, the model that includes both causal paths A →D and\\nA →M →D. We define a range of values that we want to assign to A:\\nR code\\n5.25\\nA_seq <- seq( from=-2 , to=2 , length.out=30 )\\nNext we need to extract the posterior samples, because we’ll simulate observations for each set of\\nsamples. Then it really is just a matter of using the model definition with the samples, as in previous\\nexamples. The model defines the distribution of M. We just convert that definition to the correspond-\\ning simulation function, which is rnorm in this case:\\nR code\\n5.26\\npost <- extract.samples( m5.3_A )\\nM_sim <- with( post , sapply( 1:30 ,\\nfunction(i) rnorm( 1e3 , aM + bAM*A_seq[i] , sigma_M ) ) )\\nI used the with function, which saves us having to type post$ in front of every parameter name. The\\nlinear model inside rnorm comes right out of the model definition. This produces a matrix of values,\\nwith samples in rows and cases corresponding to the values in A_seq in the columns. Now that we\\nhave simulated values for M, we can simulate D too:\\nR code\\n5.27\\nD_sim <- with( post , sapply( 1:30 ,\\nfunction(i) rnorm( 1e3 , a + bA*A_seq[i] + bM*M_sim[,i] , sigma ) ) )\\nIf you plot A_seq against the column means of D_sim, you’ll see the same result as before. In complex\\nmodels, there might be many more variables to simulate. But the basic procedure is the same.\\n5.2. Masked relationship\\nThe divorce rate example demonstrates that multiple predictor variables are useful for\\nknocking out spurious association. A second reason to use more than one predictor variable\\nis to measure the direct influences of multiple factors on an outcome, when none of those\\ninfluences is apparent from bivariate relationships. This kind of problem tends to arise when\\nthere are two predictor variables that are correlated with one another. However, one of these\\nis positively correlated with the outcome and the other is negatively correlated with it.\\nYou’ll consider this kind of problem in a new data context, information about the com-\\nposition of milk across primate species, as well as some facts about those species, like body\\nmass and brain size.84 Milk is a huge investment, being much more expensive than gestation.\\nSuch an expensive resource is likely adjusted in subtle ways, depending upon the physiolog-\\nical and development details of each mammal species. Let’s load the data into R first:\\nR code\\n5.28\\nlibrary(rethinking)\\ndata(milk)\\nd <- milk\\nstr(d)\\nYou should see in the structure of the data frame that you have 29 rows for 8 variables. The\\nvariables we’ll consider for now are kcal.per.g (kilocalories of energy per gram of milk),\\n'},\n",
       " {'index': 163,\n",
       "  'number': 145,\n",
       "  'content': \"5.2. MASKED RELATIONSHIP\\n145\\nmass (average female body mass, in kilograms), and neocortex.perc (percent of total brain\\nmass that is neocortex mass).\\nA popular hypothesis has it that primates with larger brains produce more energetic\\nmilk, so that brains can grow quickly. Answering questions of this sort consumes a lot of\\neffort in evolutionary biology, because there are many subtle statistical issues that arise when\\ncomparing species. It doesn’t help that many biologists have no reference model other than\\na series of regressions, and so the output of the regressions is not really interpretable. The\\ncausal meaning of statistical estimates always depends upon information outside the data.\\nWe won’t solve these problems here. But we will explore a useful example. The question\\nhere is to what extent energy content of milk, measured here by kilocalories, is related to the\\npercent of the brain mass that is neocortex. Neocortex is the gray, outer part of the brain\\nthat is especially elaborate in some primates. We’ll end up needing female body mass as\\nwell, to see the masking that hides the relationships among the variables. Let’s standardize\\nthese three variables. As in previous examples, standardizing helps us both get a reliable\\napproximation of the posterior as well as build reasonable priors.\\nR code\\n5.29\\nd$K <- standardize( d$kcal.per.g )\\nd$N <- standardize( d$neocortex.perc )\\nd$M <- standardize( log(d$mass) )\\nThe first model to consider is the simple bivariate regression between kilocalories and\\nneocortex percent. You already know how to set up this regression. In mathematical form:\\nKi ∼Normal(µi, σ)\\nµi = α + βNNi\\nwhere K is standardized kilocalories and N is standardized neocortex percent. We still need\\nto consider the priors. But first let’s just try to run this as a quap model with some vague\\npriors, because there is another key modeling issue to address first.\\nR code\\n5.30\\nm5.5_draft <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ) ,\\nmu <- a + bN*N ,\\na ~ dnorm( 0 , 1 ) ,\\nbN ~ dnorm( 0 , 1 ) ,\\nsigma ~ dexp( 1 )\\n) , data=d )\\nWhen you execute this code, you’ll get a confusing error message:\\nError in quap(alist(K ~ dnorm(mu, sigma), mu <- a + bN * N, a ~ dnorm(0,\\n:\\ninitial value in 'vmmin' is not finite\\nThe start values for the parameters were invalid. This could be caused by\\nmissing values (NA) in the data or by start values outside the parameter\\nconstraints. If there are no NAs, try using explicit start values.\\nWhat has gone wrong here? This particular error message means that the model didn’t return\\na valid probability for even the starting parameter values. In this case, the culprit is the\\nmissing values in the N variable. Take a look inside the original variable and see for yourself:\\n\"},\n",
       " {'index': 164,\n",
       "  'number': 146,\n",
       "  'content': '146\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nR code\\n5.31\\nd$neocortex.perc\\n[1] 55.16\\nNA\\nNA\\nNA\\nNA 64.54 64.54 67.64\\nNA 68.85 58.85 61.69\\n[13] 60.32\\nNA\\nNA 69.97\\nNA 70.41\\nNA 73.40\\nNA 67.53\\nNA 71.26\\n[25] 72.60\\nNA 70.24 76.30 75.49\\nEach NA in the output is a missing value. If you pass a vector like this to a likelihood func-\\ntion like dnorm, it doesn’t know what to do. After all, what’s the probability of a missing\\nvalue? Whatever the answer, it isn’t a number, and so dnorm returns a NaN. Unable to even\\nget started, quap (or rather optim, which does the real work) gives up and barks about some\\nweird thing called vmmin not being finite. This kind of opaque error message is unfortunately\\nthe norm in R. The additional part of the message suggesting NA values might be responsible\\nis just quap taking a guess.\\nThis is easy to fix. What you need to do here is manually drop all the cases with missing\\nvalues. This is known as a complete case analysis. More automated model fitting com-\\nmands, like lm and glm, will silently drop such cases for you. But this isn’t always a good\\nthing. First, it’s validity depends upon the process that caused these particular values to go\\nmissing. In Chapter 15, you’ll explore this in much more depth. Second, once you start com-\\nparing models, you must compare models fit to the same data. If some variables have missing\\nvalues that others do not, automated tools will silently produce misleading comparisons.\\nLet’s march forward for now, dropping any cases with missing values. It’s worth learning\\nhow to do this yourself. To make a new data frame with only complete cases, use:\\nR code\\n5.32\\ndcc <- d[ complete.cases(d$K,d$N,d$M) , ]\\nThis makes a new data frame, dcc, that consists of the 17 rows from d that have no missing\\nvalues in any of the variables listed inside complete.cases. Now let’s work with the new\\ndata frame. All that is new in the code is using dcc instead of d:\\nR code\\n5.33\\nm5.5_draft <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ) ,\\nmu <- a + bN*N ,\\na ~ dnorm( 0 , 1 ) ,\\nbN ~ dnorm( 0 , 1 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dcc )\\nBefore considering the posterior predictions, let’s consider those priors. As in many simple\\nlinear regression problems, these priors are harmless. But are they reasonable? It is impor-\\ntant to build reasonable priors, because as the model becomes less simple, the priors can\\nbe very helpful, but only if they are scientifically reasonable. To simulate and plot 50 prior\\nregression lines:\\nR code\\n5.34\\nprior <- extract.prior( m5.5_draft )\\nxseq <- c(-2,2)\\nmu <- link( m5.5_draft , post=prior , data=list(N=xseq) )\\n'},\n",
       " {'index': 165,\n",
       "  'number': 147,\n",
       "  'content': '5.2. MASKED RELATIONSHIP\\n147\\n-2\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\nneocortex percent (std)\\nkilocal per g (std)\\na ~ dnorm(0, 1)\\nbN ~ dnorm(0, 1)\\n-2\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\nneocortex percent (std)\\nkilocal per g (std)\\na ~ dnorm(0, 0.2)\\nbN ~ dnorm(0, 0.5)\\nFigure 5.8. Prior predictive distributions for the first primate milk model,\\nm5.5. Each plot shows a range of 2 standard deviations for each variable.\\nLeft: The vague first guess. These priors are clearly silly. Right: Slightly less\\nsilly priors that at least stay within the potential space of observations.\\nplot( NULL , xlim=xseq , ylim=xseq )\\nfor ( i in 1:50 ) lines( xseq , mu[i,] , col=col.alpha(\"black\",0.3) )\\nThe result is displayed on the left side of Figure 5.8. I’ve shown a range of 2 standard de-\\nviations for both variables. So that is most of the outcome space. These lines are crazy. As\\nin previous examples, we can do better by both tightening the α prior so that it sticks closer\\nto zero. With two standardized variables, when predictor is zero, the expected value of the\\noutcome should also be zero. And the slope βN needs to be a bit tighter as well, so that it\\ndoesn’t regularly produce impossibly strong relationships. Here’s an attempt:\\nR code\\n5.35\\nm5.5 <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ) ,\\nmu <- a + bN*N ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbN ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dcc )\\nIf you plot these priors, you’ll get what is shown on the right side of Figure 5.8. These are\\nstill very vague priors, but at least the lines stay within the high probability region of the\\nobservable data.\\nNow let’s look at the posterior:\\n'},\n",
       " {'index': 166,\n",
       "  'number': 148,\n",
       "  'content': '148\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nR code\\n5.36\\nprecis( m5.5 )\\nmean\\nsd\\n5.5% 94.5%\\na\\n0.04 0.15 -0.21\\n0.29\\nbN\\n0.13 0.22 -0.22\\n0.49\\nsigma 1.00 0.16\\n0.74\\n1.26\\nFrom this summary, you can possibly see that this is neither a strong nor very precise asso-\\nciation. The standard deviation is almost twice the posterior mean. But as always, it’s much\\neasier to see this if we draw a picture. Tables of numbers are golem speak, and we are not\\ngolems. We can plot the predicted mean and 89% compatibility interval for the mean to see\\nthis more easily. The code below contains no surprises. But if have extended the range of N\\nvalues to consider, in xseq, so that the plot looks nicer.\\nR code\\n5.37\\nxseq <- seq( from=min(dcc$N)-0.15 , to=max(dcc$N)+0.15 , length.out=30 )\\nmu <- link( m5.5 , data=list(N=xseq) )\\nmu_mean <- apply(mu,2,mean)\\nmu_PI <- apply(mu,2,PI)\\nplot( K ~ N , data=dcc )\\nlines( xseq , mu_mean , lwd=2 )\\nshade( mu_PI , xseq )\\nI display this plot in the upper-left of Figure 5.9. The posterior mean line is weakly positive,\\nbut it is highly imprecise. A lot of mildly positive and negative slopes are plausible, given this\\nmodel and these data.\\nNow consider another predictor variable, adult female body mass, mass in the data\\nframe. Let’s use the logarithm of mass, log(mass), as a predictor as well. Why the logarithm\\nof mass instead of the raw mass in kilograms? It is often true that scaling measurements like\\nbody mass are related by magnitudes to other variables. Taking the log of a measure trans-\\nlates the measure into magnitudes. So by using the logarithm of body mass here, we’re saying\\nthat we suspect that the magnitude of a mother’s body mass is related to milk energy, in a\\nlinear fashion. Much later, in Chapter 16, you’ll see why these logarithmic relationships are\\nalmost inevitable results of the physics of organisms.\\nNow we construct a similar model, but consider the bivariate relationship between kilo-\\ncalories and body mass. Since body mass is also standardized, we can use the same priors\\nand stay within possible outcome values. But if you were a domain expert in growth, you\\ncould surely do better than this.\\nR code\\n5.38\\nm5.6 <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ) ,\\nmu <- a + bM*M ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbM ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dcc )\\nprecis(m5.6)\\n'},\n",
       " {'index': 167,\n",
       "  'number': 149,\n",
       "  'content': '5.2. MASKED RELATIONSHIP\\n149\\n-2.0\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nneocortex percent (std)\\nkilocal per g (std)\\n-2\\n-1\\n0\\n1\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nlog body mass (std)\\nkilocal per g (std)\\n-2.0\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nneocortex percent (std)\\nkilocal per g (std)\\nCounterfactual holding M = 0\\n-2\\n-1\\n0\\n1\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nlog body mass (std)\\nkilocal per g (std)\\nCounterfactual holding N = 0\\nFigure 5.9. Milk energy and neocortex among primates. In the top two\\nplots, simple bivariate regressions of kilocalories per gram of milk (K) on\\n(left) neocortex percent (N) and (right) log female body mass (M) show\\nweak associations. In the bottom row, a model with both neocortex percent\\n(N) and log body mass (M) shows stronger associations.\\nmean\\nsd\\n5.5% 94.5%\\na\\n0.05 0.15 -0.20\\n0.29\\nbM\\n-0.28 0.19 -0.59\\n0.03\\nsigma\\n0.95 0.16\\n0.70\\n1.20\\nLog-mass is negatively associated with kilocalories. This association does seem stronger than\\nthat of neocortex percent, although in the opposite direction. It is quite uncertain though,\\nwith a wide compatibility interval that is consistent with a wide range of both weak and\\nstronger relationships. This regression is shown in the upper-right of Figure 5.9. You should\\nmodify the code that plotted the upper-left plot in the same figure, to be sure you understand\\nhow to do this.\\n'},\n",
       " {'index': 168,\n",
       "  'number': 150,\n",
       "  'content': '150\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nNow let’s see what happens when we add both predictor variables at the same time to the\\nregression. This is the multivariate model, in math form:\\nKi ∼Normal(µi, σ)\\nµi = α + βNNi + βMMi\\nα ∼Normal(0, 0.2)\\nβN ∼Normal(0, 0.5)\\nβM ∼Normal(0, 0.5)\\nσ ∼Exponential(1)\\nApproximating the posterior requires no new tricks:\\nR code\\n5.39\\nm5.7 <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ) ,\\nmu <- a + bN*N + bM*M ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbN ~ dnorm( 0 , 0.5 ) ,\\nbM ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dcc )\\nprecis(m5.7)\\nmean\\nsd\\n5.5% 94.5%\\na\\n0.07 0.13 -0.15\\n0.28\\nbN\\n0.68 0.25\\n0.28\\n1.07\\nbM\\n-0.70 0.22 -1.06 -0.35\\nsigma\\n0.74 0.13\\n0.53\\n0.95\\nBy incorporating both predictor variables in the regression, the posterior association of both\\nwith the outcome has increased. Visually comparing this posterior to those of the previous\\ntwo models helps to see the pattern of change:\\nR code\\n5.40\\nplot( coeftab( m5.5 , m5.6 , m5.7 ) , pars=c(\"bM\",\"bN\") )\\nm5.5\\nm5.6\\nm5.7\\nm5.5\\nm5.6\\nm5.7\\nbM\\nbN\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\nEstimate\\nThe posterior means for neocortex percent and log-mass have both moved away from zero.\\nAdding both predictors to the model seems to have made their estimates move apart.\\n'},\n",
       " {'index': 169,\n",
       "  'number': 151,\n",
       "  'content': '5.2. MASKED RELATIONSHIP\\n151\\nWhat happened here? Why did adding neocortex and body mass to the same model\\nlead to stronger associations for both? This is a context in which there are two variables\\ncorrelated with the outcome, but one is positively correlated with it and the other is negatively\\ncorrelated with it. In addition, both of the explanatory variables are positively correlated with\\none another. Try a simple pairs( ~K + M + N , dcc ) plot to appreciate this pattern of\\ncorrelation. The result of this pattern is that the variables tend to cancel one another out.\\nThis is another case in which multiple regression automatically finds the most revealing\\ncases and uses them to produce inferences. What the regression model does is ask if species\\nthat have high neocortex percent for their body mass have higher milk energy. Likewise, the\\nmodel asks if species with high body mass for their neocortex percent have higher milk energy.\\nBigger species, like apes, have milk with less energy. But species with more neocortex tend\\nto have richer milk. The fact that these two variables, body size and neocortex, are correlated\\nacross species makes it hard to see these relationships, unless we account for both.\\nSome DAGs will help. There are at least three graphs consistent with these data.\\nK\\nM\\nN\\nK\\nM\\nN\\nK\\nM\\nN\\nU\\nBeginning on the left, the first possibility is that body mass (M) influences neocortex percent\\n(N). Both then influence kilocalories in milk (K). Second, in the middle, neocortex could\\ninstead influence body mass. The two variables still end up correlated in the sample. Finally,\\non the right, there could be an unobserved variable U that influences both M and N, produc-\\ning a correlation between them. In this book, I’ll circle variables that are unobserved. One\\nof the threats to causal inference is that there are potentially many unobserved variables that\\ninfluence an outcome or the predictors. We’ll consider this more in the next chapter.\\nWhich of these graphs is right? We can’t tell from the data alone, because these graphs\\nimply the same set of conditional independencies. In this case, there are no conditional\\nindependencies—each DAG above implies that all pairs of variables are associated, regardless\\nof what we condition on. A set of DAGs with the same conditional independencies is known\\nas a Markov equivalence set. In the Overthinking box on the next page, I’ll show you\\nhow to simulate observations consistent with each of these DAGs, how each can produce\\nthe masking phenomenon, and how to use the dagitty package to compute the complete\\nset of Markov equivalent DAGs. Remember that while the data alone can never tell you\\nwhich causal model is correct, your scientific knowledge of the variables will eliminate a\\nlarge number of silly, but Markov equivalent, DAGs.\\nThe final thing we’d like to do with these models is to finish Figure 5.9. Let’s make\\ncounterfactual plots again. Suppose the third DAG above is the right one. Then imagine ma-\\nnipulating M and N, breaking the influence of U on each. In the real world, such experiments\\nare impossible. If we change an animal’s body size, natural selection would then change the\\nother features to match it. But these counterfactual plots do help us see how the model views\\nthe association between each predictor and the outcome. Here is the code to produce the\\nlower-left plot in Figure 5.9 (page 149).\\n'},\n",
       " {'index': 170,\n",
       "  'number': 152,\n",
       "  'content': '152\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nR code\\n5.41\\nxseq <- seq( from=min(dcc$M)-0.15 , to=max(dcc$M)+0.15 , length.out=30 )\\nmu <- link( m5.7 , data=data.frame( M=xseq , N=0 ) )\\nmu_mean <- apply(mu,2,mean)\\nmu_PI <- apply(mu,2,PI)\\nplot( NULL , xlim=range(dcc$M) , ylim=range(dcc$K) )\\nlines( xseq , mu_mean , lwd=2 )\\nshade( mu_PI , xseq )\\nYou should try to reproduce the lower-right plot by modifying this code. In the practice\\nproblems, I’ll ask you to consider what would happen, if you chose one of the other DAGs at\\nthe top of the page.\\nOverthinking: Simulating a masking relationship. Just as with understanding spurious association\\n(page 139), it may help to simulate data in which two meaningful predictors act to mask one another.\\nIn the previous section, I showed three DAGs consistent with this. To simulate data consistent with\\nthe first DAG:\\nR code\\n5.42\\n# M -> K <- N\\n# M -> N\\nn <- 100\\nM <- rnorm( n )\\nN <- rnorm( n , M )\\nK <- rnorm( n , N - M )\\nd_sim <- data.frame(K=K,N=N,M=M)\\nYou can quickly see the masking pattern of inferences by replacing dcc with d_sim in models m5.5,\\nm5.6, and m5.7. Look at the precis summaries and you’ll see the same masking pattern where the\\nslopes become more extreme in m5.7. The other two DAGs can be simulated like this:\\nR code\\n5.43\\n# M -> K <- N\\n# N -> M\\nn <- 100\\nN <- rnorm( n )\\nM <- rnorm( n , N )\\nK <- rnorm( n , N - M )\\nd_sim2 <- data.frame(K=K,N=N,M=M)\\n# M -> K <- N\\n# M <- U -> N\\nn <- 100\\nU <- rnorm( n )\\nN <- rnorm( n , U )\\nM <- rnorm( n , U )\\nK <- rnorm( n , N - M )\\nd_sim3 <- data.frame(K=K,N=N,M=M)\\nIn the primate milk example, it may be that the positive association between large body size and\\nneocortex percent arises from a tradeoff between lifespan and learning. Large animals tend to live\\na long time. And in such animals, an investment in learning may be a better investment, because\\nlearning can be amortized over a longer lifespan. Both large body size and large neocortex then\\ninfluence milk composition, but in different directions, for different reasons. This story implies that\\nthe DAG with an arrow from M to N, the first one, is the right one. But with the evidence at hand,\\n'},\n",
       " {'index': 171,\n",
       "  'number': 153,\n",
       "  'content': '5.3. CATEGORICAL VARIABLES\\n153\\nwe cannot easily see which is right. To compute the Markov equivalence set, let’s define the first\\nDAG and ask dagitty to do the hard work:\\nR code\\n5.44\\ndag5.7 <- dagitty( \"dag{\\nM -> K <- N\\nM -> N }\" )\\ncoordinates(dag5.7) <- list( x=c(M=0,K=1,N=2) , y=c(M=0.5,K=1,N=0.5) )\\nMElist <- equivalentDAGs(dag5.7)\\nNow MElist should contain six different DAGs. To plot them all, you can use drawdag(MElist).\\nWhich of these do you think you could eliminate, based upon scientific knowledge of the variables?\\n5.3. Categorical variables\\nA common question for statistical methods is to what extent an outcome changes as a\\nresult of presence or absence of a category. A category here means discrete and unordered.\\nFor example, consider the different species in the milk energy data again. Some of them are\\napes, while others are New World monkeys. We might want to ask how predictions should\\nvary when the species is an ape instead of a monkey. Taxonomic group is a categorical\\nvariable, because no species can be half-ape and half-monkey (discreteness), and there\\nis no sense in which one is larger or smaller than the other (unordered). Other common\\nexamples of categorical variables include:\\n• Sex: male, female\\n• Developmental status: infant, juvenile, adult\\n• Geographic region: Africa, Europe, Melanesia\\nMany readers will already know that variables like this, routinely called factors, can\\neasily be included in linear models. But what is not widely understood is how these variables\\nare represented in a model. The computer does all of the work for us, hiding the machinery.\\nBut there are some subtleties that make it worth exposing the machinery. Knowing how the\\nmachine (golem) works both helps you interpret the posterior distribution and gives you\\nadditional power in building the model.\\nRethinking: Continuous countries. With automated software and lack of attention, categorical vari-\\nables can be dangerous. In 2015, a high-impact journal published a study of 1170 children from\\nsix countries, finding a strong negative association between religiosity and generosity.85 The paper\\ncaused a small stir among religion researchers, because it disagreed with the existing literature. Upon\\nreanalysis, it was found that the country variable, which is categorical, was entered as a continuous\\nvariable instead. This made Canada (value 2) twice as much “country” as the United States (value 1).\\nAfter reanalysis with country as a categorical variable, the result vanished and the original paper has\\nbeen retracted. This is a happy ending, because the authors shared their data. How many cases like\\nthis exist, undiscovered because the data have never been shared and are possible lost forever?\\n5.3.1. Binary categories. In the simplest case, the variable of interest has only two cate-\\ngories, like male and female. Let’s rewind to the Kalahari data you met in Chapter 4. Back\\nthen, we ignored sex when predicting height, but obviously we expect males and females to\\nhave different averages. Take a look at the variables available:\\n'},\n",
       " {'index': 172,\n",
       "  'number': 154,\n",
       "  'content': \"154\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nR code\\n5.45\\ndata(Howell1)\\nd <- Howell1\\nstr(d)\\n'data.frame': 544 obs. of\\n4 variables:\\n$ height: num\\n152 140 137 157 145 ...\\n$ weight: num\\n47.8 36.5 31.9 53 41.3 ...\\n$ age\\n: num\\n63 63 65 41 51 35 32 27 19 54 ...\\n$ male\\n: int\\n1 0 0 1 0 1 0 1 0 1 ...\\nThe male variable is our new predictor, an example of a indicator variable. Indicator\\nvariables—sometimes also called “dummy” variables—are devices for encoding unordered\\ncategories into quantitative models. There is no sense here in which “male” is one more than\\n“female.” The purpose of the male variable is to indicate when a person in the sample is\\n“male.” So it takes the value 1 whenever the person is male, but it takes the value 0 when the\\nperson belongs to any other category. It doesn’t matter which category is indicated by the 1.\\nThe model won’t care. But correctly interpreting the model demands that you remember, so\\nit’s a good idea to name the variable after the category assigned the 1 value.\\nThere are two ways to make a model with this information. The first is to use the indicator\\nvariable directly inside the linear model, as if it were a typical predictor variable. The effect of\\nan indicator variable is to turn a parameter on for those cases in the category. Simultaneously,\\nthe variable turns the same parameter off for those cases in another category. This will make\\nmore sense, once you see it in the mathematical definition of the model. Consider again a\\nlinear model of height, as in Chapter 4. Now we’ll ignore weight and the other variables and\\nfocus only on sex.\\nhi ∼Normal(µi, σ)\\nµi = α + βmmi\\nα ∼Normal(178, 20)\\nβm ∼Normal(0, 10)\\nσ ∼Uniform(0, 50)\\nwhere h is height and m is the dummy variable indicating a male individual. The parameter\\nβm influences prediction only for those cases where mi = 1. When mi = 0, it has no effect\\non prediction, because it is multiplied by zero inside the linear model, α+βmmi, canceling it\\nout, whatever its value. This is just to say that, when mi = 1, the linear model is µi = α+βm.\\nAnd when mi = 0, the linear model is simply µi = α.\\nUsing this approach means that βm represents the expected difference between males\\nand females in height. The parameter α is used to predict both female and male heights. But\\nmale height gets an extra βm. This also means that α is no longer the average height in the\\nsample, but rather just the average female height. This can make assigning sensible priors a\\nlittle harder. If you don’t have a sense of the expected difference in height—what would be\\nreasonable before seeing the data?—then this approach can be a bother. Of course you could\\nget away with a vague prior in this case—there is a lot of data.\\nAnother consequence of having to assign a prior to the difference is that this approach\\nnecessarily assumes there is more uncertainty about one of the categories—“male” in this\\ncase—than the other. Why? Because a prediction for a male includes two parameters and\\n\"},\n",
       " {'index': 173,\n",
       "  'number': 155,\n",
       "  'content': \"5.3. CATEGORICAL VARIABLES\\n155\\ntherefore two priors. We can simulate this directly from the priors. The prior distributions\\nfor µ for females and males are:\\nR code\\n5.46\\nmu_female <- rnorm(1e4,178,20)\\nmu_male <- rnorm(1e4,178,20) + rnorm(1e4,0,10)\\nprecis( data.frame( mu_female , mu_male ) )\\n'data.frame': 10000 obs. of 2 variables:\\nmean\\nsd\\n5.5%\\n94.5% histogram\\nmu_female 178.41 20.04 146.30 209.94\\n▁▁▃▇▇▂▁▁\\nmu_male\\n177.97 22.40 142.39 214.82 ▁▁▁▃▇▇▂▁▁\\nThe prior for males is wider, because it uses both parameters. While in a regression this\\nsimple, these priors will wash out very quickly, in general we should be careful. We aren’t\\nactually more unsure about male height than female height, a priori. Is there another way?\\nAnother approach available to us is an index variable. An index variable contains\\nintegers that correspond to different categories. The integers are just names, but they also let\\nus reference a list of corresponding parameters, one for each category. In this case, we can\\nconstruct our index like this:\\nR code\\n5.47\\nd$sex <- ifelse( d$male==1 , 2 , 1 )\\nstr( d$sex )\\nnum [1:544] 2 1 1 2 1 2 1 2 1 2 ...\\nNow “1” means female and “2” means male. No order is implied. These are just labels. And\\nthe mathematical version of the model becomes:\\nhi ∼Normal(µi, σ)\\nµi = αsex[i]\\nαj ∼Normal(178, 20)\\nfor j = 1..2\\nσ ∼Uniform(0, 50)\\nWhat this does is create a list of α parameters, one for each unique value in the index variable.\\nSo in this case we end up with two α parameters, named α1 and α2. The numbers correspond\\nto the values in the index variable sex. I know this seems overly complicated, but it solves\\nour problem with the priors. Now the same prior can be assigned to each, corresponding to\\nthe notion that all the categories are the same, prior to the data. Neither category has more\\nprior uncertainty than the other. And as you’ll see in a bit, this approach extends effortlessly\\nto contexts with more than two categories.\\nLet’s approximate the posterior for the above model, the one using an index variable.\\nR code\\n5.48\\nm5.8 <- quap(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a[sex] ,\\na[sex] ~ dnorm( 178 , 20 ) ,\\nsigma ~ dunif( 0 , 50 )\\n) , data=d )\\nprecis( m5.8 , depth=2 )\\n\"},\n",
       " {'index': 174,\n",
       "  'number': 156,\n",
       "  'content': '156\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nmean\\nsd\\n5.5%\\n94.5%\\na[1]\\n134.91 1.61 132.34 137.48\\na[2]\\n142.58 1.70 139.86 145.29\\nsigma\\n27.31 0.83\\n25.98\\n28.63\\nNote the depth=2 that I added to precis. This tells it to show any vector parameters, like\\nour new a vector. Vector (and matrix) parameters are hidden by precies by default, because\\nsometimes there are lots of these and you don’t want to inspect their individual values. You’ll\\nsee what I mean in later chapters.\\nInterpreting these parameters is easy enough—they are the expected heights in each cat-\\negory. But often we are interested in differences between categories. In this case, what is\\nthe expected difference between females and males? We can compute this using samples\\nfrom the posterior. In fact, I’ll extract posterior samples into a data frame and insert our\\ncalculation directly into the same frame:\\nR code\\n5.49\\npost <- extract.samples(m5.8)\\npost$diff_fm <- post$a[,1] - post$a[,2]\\nprecis( post , depth=2 )\\nquap posterior: 10000 samples from m5.8\\nmean\\nsd\\n5.5%\\n94.5%\\nhistogram\\nsigma\\n27.29 0.84\\n25.95\\n28.63 ▁▁▁▁▃▇▇▇▃▂▁▁▁\\na[1]\\n134.91 1.59 132.37 137.42 ▁▁▁▂▅▇▇▅▂▁▁▁▁\\na[2]\\n142.60 1.71 139.90 145.35\\n▁▁▁▅▇▃▁▁▁\\ndiff_fm\\n-7.70 2.33 -11.41\\n-3.97\\n▁▁▁▁▃▇▇▃▁▁▁\\nOur calculation appears at the bottom, as a new parameter in the posterior. This is the ex-\\npected difference between a female and male in the sample. This kind of calculation is called\\na contrast. No matter how many categories you have, you can use samples from the pos-\\nterior to compute the contrast between any two.\\n5.3.2. Many categories. Binary categories are easy, whether you use an indicator variable\\nor instead an index variable. But when there are more than two categories, the indicator\\nvariable approach explodes. You’ll need a new indicator variable for each new category. If\\nyou have k unique categories, you need k −1 indicator variables. Automated tools like R’s\\nlm do in fact go this route, constructing k−1 indicator variables for you and returning k−1\\nparameters (in addition to the intercept).\\nBut we’ll instead stick with the index variable approach. It does not change at all when\\nyou add more categories. You do get more parameters, of course, just as many as in the\\nindicator variable approach. But the model specification looks just like it does in the bi-\\nnary case. And the priors continue to be easier, unless you really do have prior information\\nabout contrasts. It is also important to get used to index variables, because multilevel models\\n(Chapter 13) depend upon them.\\nLet’s explore an example using the primate milk data again. We’re interested now in the\\nclade variable, which encodes the broad taxonomic membership of each species:\\nR code\\n5.50\\ndata(milk)\\nd <- milk\\nlevels(d$clade)\\n'},\n",
       " {'index': 175,\n",
       "  'number': 157,\n",
       "  'content': '5.3. CATEGORICAL VARIABLES\\n157\\n[1] \"Ape\" \"New World Monkey\" \"Old World Monkey\" \"Strepsirrhine\"\\nWe want an index value for each of these four categories. You could do this by hand, but just\\ncoercing the factor to an integer will do the job:\\nR code\\n5.51\\nd$clade_id <- as.integer( d$clade )\\nLet’s use a model to measure the average milk energy in each clade. In math form:\\nKi ∼Normal(µi, σ)\\nµi = αclade[i]\\nαj ∼Normal(0, 0.5)\\nfor j = 1..4\\nσ ∼Exponential(1)\\nRemember, K is the standardized kilocalories. I widened the prior on α a little, to allow\\nthe different clades to disperse, if the data wants them to. But I encourage you to play with\\nthat prior and repeatedly re-approximate the posterior so you can see how the posterior\\ndifferences among the categories depend upon it. Firing up quap now:\\nR code\\n5.52\\nd$K <- standardize( d$kcal.per.g )\\nm5.9 <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ),\\nmu <- a[clade_id],\\na[clade_id] ~ dnorm( 0 , 0.5 ),\\nsigma ~ dexp( 1 )\\n) , data=d )\\nlabels <- paste( \"a[\" , 1:4 , \"]:\" , levels(d$clade) , sep=\"\" )\\nplot( precis( m5.9 , depth=2 , pars=\"a\" ) , labels=labels ,\\nxlab=\"expected kcal (std)\" )\\na[4]:Strepsirrhine\\na[3]:Old World Monkey\\na[2]:New World Monkey\\na[1]:Ape\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\nexpected kcal (std)\\nI used the optional labels argument to augment the parameter names a[1] through a[4]\\nwith the clade names from the original variable. In practice, you have to be very careful to\\nkeep track of which index values go with which categories. Don’t trust R’s factor variable\\ntype to necessarily do things right.\\nIf you have another kind of categorical variable that you’d like to add to the model, the\\napproach is just the same. For example, let’s randomly assign these primates to some made\\nup categories: [1] Gryffindor, [2] Hufflepuff, [3] Ravenclaw, and [4] Slytherin.\\nR code\\n5.53\\nset.seed(63)\\nd$house <- sample( rep(1:4,each=8) , size=nrow(d) )\\n'},\n",
       " {'index': 176,\n",
       "  'number': 158,\n",
       "  'content': '158\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\nNow we can include these categories as another predictor in the model:\\nR code\\n5.54\\nm5.10 <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ),\\nmu <- a[clade_id] + h[house],\\na[clade_id] ~ dnorm( 0 , 0.5 ),\\nh[house] ~ dnorm( 0 , 0.5 ),\\nsigma ~ dexp( 1 )\\n) , data=d )\\nIf you inspect the posterior, you’ll see that Slytherin stands out.\\nRethinking: Differences and statistical significance. A common error in interpretation of parameter\\nestimates is to suppose that because one parameter is sufficiently far from zero—is “significant”—and\\nanother parameter is not—is “not significant”—that the difference between the parameters is also\\nsignificant. This is not necessarily so.86 This isn’t just an issue for non-Bayesian analysis: If you want\\nto know the distribution of a difference, then you must compute that difference, a contrast. It\\nisn’t enough to just observe, for example, that a slope among males overlaps a lot with zero while the\\nsame slope among females is reliably above zero. You must compute the posterior distribution of the\\ndifference in slope between males and females. For example, suppose you have posterior distributions\\nfor two parameters, βf and βm. βf’s mean and standard deviation is 0.15±0.02, and βm’s is 0.02±0.10.\\nSo while βf is reliably different from zero (“significant”) and βm is not, the difference between the two\\n(assuming they are uncorrelated) is (0.15 −0.02) ±\\n√\\n0.022 + 0.12 ≈0.13 ± 0.10. The distribution\\nof the difference overlaps a lot with zero. In other words, you can be confident that βf is far from zero,\\nbut you cannot be sure that the difference between βf and βm is far from zero.\\nIn the context of non-Bayesian significance testing, this phenomenon arises from the fact that\\nstatistical significance is inferentially powerful in one way: difference from the null. When βm over-\\nlaps with zero, it may also overlap with values very far from zero. Its value is uncertain. So when you\\nthen compare βm to βf, that comparison is also uncertain, manifesting in the width of the posterior\\ndistribution of the difference βf −βm. Lurking underneath this example is a more fundamental mis-\\ntake in interpreting statistical significance: The mistake of accepting the null hypothesis. Whenever\\nan article or book says something like “we found no difference” or “no effect,” this usually means\\nthat some parameter was not significantly different from zero, and so the authors adopted zero as the\\nestimate. This is both illogical and extremely common.\\n5.4. Summary\\nThis chapter introduced multiple regression, a way of constructing descriptive models\\nfor how the mean of a measurement is associated with more than one predictor variable. The\\ndefining question of multiple regression is: What is the value of knowing each predictor, once\\nwe already know the other predictors? The answer to this question does not by itself provide\\nany causal information. Causal inference requires additional assumptions. Simple directed\\nacyclic graph (DAG) models of causation are one way to represent those assumptions. In\\nthe next chapter we’ll continue building the DAG framework and see how adding predictor\\nvariables can create as many problems as it can solve.\\n'},\n",
       " {'index': 177,\n",
       "  'number': 159,\n",
       "  'content': '5.5. PRACTICE\\n159\\n5.5. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n5E1. Which of the linear models below are multiple linear regressions?\\n(1) µi = α + βxi\\n(2) µi = βxxi + βzzi\\n(3) µi = α + β(xi −zi)\\n(4) µi = α + βxxi + βzzi\\n5E2. Write down a multiple regression to evaluate the claim: Animal diversity is linearly related to\\nlatitude, but only after controlling for plant diversity. You just need to write down the model definition.\\n5E3. Write down a multiple regression to evaluate the claim: Neither amount of funding nor size\\nof laboratory is by itself a good predictor of time to PhD degree; but together these variables are both\\npositively associated with time to degree. Write down the model definition and indicate which side of\\nzero each slope parameter should be on.\\n5E4. Suppose you have a single categorical predictor with 4 levels (unique values), labeled A, B, C\\nand D. Let Ai be an indicator variable that is 1 where case i is in category A. Also suppose Bi, Ci,\\nand Di for the other categories. Now which of the following linear models are inferentially equivalent\\nways to include the categorical variable in a regression? Models are inferentially equivalent when it’s\\npossible to compute one posterior distribution from the posterior distribution of another model.\\n(1) µi = α + βAAi + βBBi + βDDi\\n(2) µi = α + βAAi + βBBi + βCCi + βDDi\\n(3) µi = α + βBBi + βCCi + βDDi\\n(4) µi = αAAi + αBBi + αCCi + αDDi\\n(5) µi = αA(1 −Bi −Ci −Di) + αBBi + αCCi + αDDi\\n5M1. Invent your own example of a spurious correlation. An outcome variable should be correlated\\nwith both predictor variables. But when both predictors are entered in the same model, the correlation\\nbetween the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).\\n5M2. Invent your own example of a masked relationship. An outcome variable should be correlated\\nwith both predictor variables, but in opposite directions. And the two predictor variables should be\\ncorrelated with one another.\\n5M3. It is sometimes observed that the best predictor of fire risk is the presence of firefighters—\\nStates and localities with many firefighters also have more fires. Presumably firefighters do not cause\\nfires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the\\nsame reversal of causal inference in the context of the divorce and marriage data. How might a high\\ndivorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using\\nmultiple regression?\\n5M4. In the divorce data, States with high numbers of members of the Church of Jesus Christ of\\nLatter-day Saints (LDS) have much lower divorce rates than the regression models expected. Find a\\nlist of LDS population by State and use those numbers as a predictor variable, predicting divorce rate\\nusing marriage rate, median age at marriage, and percent LDS population (possibly standardized).\\nYou may want to consider transformations of the raw percent LDS variable.\\n'},\n",
       " {'index': 178,\n",
       "  'number': 160,\n",
       "  'content': '160\\n5. THE MANY VARIABLES & THE SPURIOUS WAFFLES\\n5M5. One way to reason through multiple causation hypotheses is to imagine detailed mechanisms\\nthrough which predictor variables may influence outcomes. For example, it is sometimes argued that\\nthe price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome\\nvariable). However, there are at least two important mechanisms by which the price of gas could\\nreduce obesity. First, it could lead to less driving and therefore more exercise. Second, it could lead to\\nless driving, which leads to less eating out, which leads to less consumption of huge restaurant meals.\\nCan you outline one or more multiple regressions that address these two mechanisms? Assume you\\ncan have any predictor data you need.\\n5H1. In the divorce example, suppose the DAG is: M →A →D. What are the implied conditional\\nindependencies of the graph? Are the data consistent with it?\\n5H2. Assuming that the DAG for the divorce example is indeed M →A →D, fit a new model and\\nuse it to estimate the counterfactual effect of halving a State’s marriage rate M. Use the counterfactual\\nexample from the chapter (starting on page 140) as a template.\\n5H3. Return to the milk energy model, m5.7. Suppose that the true causal relationship among the\\nvariables is:\\nK\\nM\\nN\\nNow compute the counterfactual effect on K of doubling M. You will need to account for both the\\ndirect and indirect paths of causation. Use the counterfactual example from the chapter (starting on\\npage 140) as a template.\\n5H4. Here is an open practice problem to engage your imagination. In the divorce date, States in\\nthe southern United States have many of the highest divorce rates. Add the South indicator variable\\nto the analysis. First, draw one or more DAGs that represent your ideas for how Southern American\\nculture might influence any of the other three variables (D, M or A). Then list the testable implications\\nof your DAGs, if there are any, and fit one or more models to evaluate the implications. What do you\\nthink the influence of “Southerness” is?\\n'},\n",
       " {'index': 179,\n",
       "  'number': 161,\n",
       "  'content': '6 The Haunted DAG & The Causal Terror\\nIt seems like the most newsworthy scientific studies are the least trustworthy. The more\\nlikely it is to kill you, if true, the less likely it is to be true. The more boring the topic, the\\nmore rigorous the results. How could this widely believed negative correlation exist? There\\ndoesn’t seem to be any reason for studies of topics that people care about to produce less\\nreliable results. Maybe popular topics attract more and worse researchers, like flies drawn to\\nthe smell of honey?\\nActually all that is necessary for such a negative correlation to arise is that peer reviewers\\ncare about both newsworthiness and trustworthiness. Whether it is grant review or journal\\nreview, if editors and reviewers care about both, then the act of selection itself is enough to\\nmake the most newsworthy studies the least trustworthy. In fact, it’s hard to imagine how\\nscientific peer review could avoid creating this negative correlation. And, dear reader, this\\nfact will help us understand the perils of multiple regression.\\nHere’s a simple simulation to illustrate the point.87 Suppose a grant review panel receives\\n200 research proposals. Among these proposals, there is no correlation at all between trust-\\nworthiness (rigor, scholarship, plausibility of success) and newsworthiness (social welfare\\nvalue, public interest). The panel weighs trustworthiness and newsworthiness equally. Then\\nthey rank the proposals by their combined scores and select the top 10% for funding.\\nAt the end of this section, I show the code to simulate this thought experiment. Fig-\\nure 6.1 displays the full sample of simulated proposals, with those selected in blue. I’ve\\ndrawn a simple linear regression line through the selected proposals. There’s the negative\\ncorrelation, −0.77 in this example. Strong selection induces a negative correlation among\\nthe criteria used in selection. Why? If the only way to cross the threshold is to score high, it is\\nmore common to score high on one item than on both. Therefore among funded proposals,\\nthe most newsworthy studies can actually have less than average trustworthiness (less than 0\\nin the figure). Similarly the most trustworthy studies can be less newsworthy than average.\\nThis general phenomenon has been recognized for a long time. It is sometimes called\\nBerkson’s paradox.88 But it is easier to remember if we call it the selection-distortion\\neffect. Once you appreciate this effect, you’ll see it everywhere. Why do so many restaurants\\nin good locations have bad food? The only way a restaurant with less-than-good food can\\nsurvive is if it is in a nice location. Similarly, restaurants with excellent food can survive even\\nin bad locations. Selection-distortion ruins your city.\\nWhat does this have to do with multiple regression? Unfortunately, everything. The\\nprevious chapter demonstrated some amazing powers of multiple regression. It can smoke\\nout spurious correlations and clear up masking effects. This may encourage the view that,\\nwhen in doubt, just add everything to the model and let the oracle of regression sort it out.\\n161\\n'},\n",
       " {'index': 180,\n",
       "  'number': 162,\n",
       "  'content': '162\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\n-2\\n-1\\n0\\n1\\n2\\n3\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\nnewsworthiness\\ntrustworthiness\\nselected\\nrejected\\nFigure 6.1. Why the most newsworthy stud-\\nies might be the least trustworthy.\\n200 re-\\nsearch proposals are ranked by combined\\ntrustworthiness and newsworthiness. The top\\n10% are selected for funding. While there is\\nno correlation before selection, the two crite-\\nria are strongly negatively correlated after se-\\nlection. The correlation here is −0.77.\\nRegression will not sort it out. Regression is indeed an oracle, but a cruel one. It speaks\\nin riddles and delights in punishing us for asking bad questions. The selection-distortion\\neffect can happen inside of a multiple regression, because the act of adding a predictor in-\\nduces statistical selection within the model, a phenomenon that goes by the unhelpful name\\ncollider bias. This can mislead us into believing, for example, that there is a negative as-\\nsociation between newsworthiness and trustworthiness in general, when in fact it is just a\\nconsequence of conditioning on some variable. This is both a deeply confusing fact and one\\nthat is important to understand in order to regress responsibly.\\nThis chapter and the next are both about terrible things that can happen when we simply\\nadd variables to a regression, without a clear idea of a causal model. In this chapter, we’ll ex-\\nplore three different hazards: multicollinearity, post-treatment bias, and collider bias. We’ll\\nend by tying all of these examples together in a framework that can tell us which variables we\\nmust and must not add to a model in order to arrive at valid inferences. But this framework\\ndoes not do the most important step for us: It will not give us a valid model.\\nOverthinking: Simulated science distortion. Simulations like this one are easy to do in R, or in any\\nother scripting language, once you have seen a few examples. In this simulation, we just draw some\\nrandom Gaussian criteria for a sample of proposals and then select the top 10% combined scores.\\nR code\\n6.1\\nset.seed(1914)\\nN <- 200 # num grant proposals\\np <- 0.1 # proportion to select\\n# uncorrelated newsworthiness and trustworthiness\\nnw <- rnorm(N)\\ntw <- rnorm(N)\\n# select top 10% of combined scores\\ns <- nw + tw\\n# total score\\nq <- quantile( s , 1-p ) # top 10% threshold\\nselected <- ifelse( s >= q , TRUE , FALSE )\\ncor( tw[selected] , nw[selected] )\\nI chose a specific seed so you can replicate the result in Figure 6.1, but if you rerun the simulation\\nwithout the set.seed line, you’ll see there is nothing special about the seed I used.\\n'},\n",
       " {'index': 181,\n",
       "  'number': 163,\n",
       "  'content': '6.1. MULTICOLLINEARITY\\n163\\n6.1. Multicollinearity\\nIt is commonly true that there are many potential predictor variables to add to a regres-\\nsion model. In the case of the primate milk data, for example, there are 7 variables available\\nto predict any column we choose as an outcome. Why not just build a model that includes\\nall 7? There are several hazards.\\nLet’s begin with the least of your worries: multicollinearity. Multicollinearity means\\na very strong association between two or more predictor variables. The raw correlation isn’t\\nwhat matters. Rather what matters is the association, conditional on the other variables in\\nthe model. The consequence of multicollinearity is that the posterior distribution will seem\\nto suggest that none of the variables is reliably associated with the outcome, even if all of the\\nvariables are in reality strongly associated with the outcome.\\nThis frustrating phenomenon arises from the details of how multiple regression works.\\nIn fact, there is nothing wrong with multicollinearity. The model will work fine for predic-\\ntion. You will just be frustrated trying to understand it. The hope is that once you understand\\nmulticollinearity, you will better understand regression models in general.\\nLet’s begin with a simple simulation. Then we’ll turn to the primate milk data again and\\nsee multicollinearity in a real data set.\\n6.1.1. Multicollinear legs. Imagine trying to predict an individual’s height using the length\\nof his or her legs as predictor variables. Surely height is positively associated with leg length,\\nor at least our simulation will assume it is. Nevertheless, once you put both legs (right and\\nleft) into the model, something vexing will happen.\\nThe code below will simulate the heights and leg lengths of 100 individuals. For each,\\nfirst a height is simulated from a Gaussian distribution. Then each individual gets a simulated\\nproportion of height for their legs, ranging from 0.4 to 0.5. Finally, each leg is salted with a\\nlittle measurement or developmental error, so the left and right legs are not exactly the same\\nlength, as is typical in real populations. At the end, the code puts height and the two leg\\nlengths into a common data frame.\\nR code\\n6.2\\nN <- 100\\n# number of individuals\\nset.seed(909)\\nheight <- rnorm(N,10,2)\\n# sim total height of each\\nleg_prop <- runif(N,0.4,0.5)\\n# leg as proportion of height\\nleg_left <- leg_prop*height +\\n# sim left leg as proportion + error\\nrnorm( N , 0 , 0.02 )\\nleg_right <- leg_prop*height +\\n# sim right leg as proportion + error\\nrnorm( N , 0 , 0.02 )\\n# combine into data frame\\nd <- data.frame(height,leg_left,leg_right)\\nNow let’s analyze these data, predicting the outcome height with both predictors, leg_left\\nand leg_right. Before approximating the posterior, however, consider what we expect. On\\naverage, an individual’s legs are 45% of their height (in these simulated data). So we should\\nexpect the beta coefficient that measures the association of a leg with height to end up around\\nthe average height (10) divided by 45% of the average height (4.5). This is 10/4.5 ≈2.2. Now\\nlet’s see what happens instead. I’ll use very vague, bad priors here, just so we can be sure that\\nthe priors aren’t responsible for what is about to happen.\\n'},\n",
       " {'index': 182,\n",
       "  'number': 164,\n",
       "  'content': '164\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\nR code\\n6.3\\nm6.1 <- quap(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a + bl*leg_left + br*leg_right ,\\na ~ dnorm( 10 , 100 ) ,\\nbl ~ dnorm( 2 , 10 ) ,\\nbr ~ dnorm( 2 , 10 ) ,\\nsigma ~ dexp( 1 )\\n) , data=d )\\nprecis(m6.1)\\nmean\\nsd\\n5.5% 94.5%\\na\\n0.98 0.28\\n0.53\\n1.44\\nbl\\n0.21 2.53 -3.83\\n4.25\\nbr\\n1.78 2.53 -2.26\\n5.83\\nsigma 0.62 0.04\\n0.55\\n0.69\\nThose posterior means and standard deviations look crazy. This is a case in which a graphical\\nview of the precis output is more useful, because it displays the posterior means and 89%\\nintervals in a way that allows us with a glance to see that something has gone wrong here:\\nR code\\n6.4\\nplot(precis(m6.1))\\nsigma\\nbr\\nbl\\na\\n-4\\n-2\\n0\\n2\\n4\\n6\\nValue\\nGo ahead and try the simulation a few more times, omitting the set.seed line. If both legs\\nhave almost identical lengths, and height is so strongly associated with leg length, then why\\nis this posterior distribution so weird? Did the posterior approximation work correctly?\\nIt did work correctly, and the posterior distribution here is the right answer to the ques-\\ntion we asked. The problem is the question. Recall that a multiple linear regression answers\\nthe question: What is the value of knowing each predictor, after already knowing all of the\\nother predictors? So in this case, the question becomes: What is the value of knowing each\\nleg’s length, after already knowing the other leg’s length?\\nThe answer to this weird question is equally weird, but perfectly logical. The posterior\\ndistribution is the answer to this question, considering every possible combination of the\\nparameters and assigning relative plausibilities to every combination, conditional on this\\nmodel and these data. It might help to look at the joint posterior distribution for bl and br:\\nR code\\n6.5\\npost <- extract.samples(m6.1)\\nplot( bl ~ br , post , col=col.alpha(rangi2,0.1) , pch=16 )\\nThe resulting plot is shown on the left of Figure 6.2. The posterior distribution for these\\ntwo parameters is very highly correlated, with all of the plausible values of bl and br lying\\n'},\n",
       " {'index': 183,\n",
       "  'number': 165,\n",
       "  'content': '6.1. MULTICOLLINEARITY\\n165\\n1.8\\n1.9\\n2.0\\n2.1\\n2.2\\n2.3\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\nsum of bl and br\\nDensity\\nFigure 6.2. Left: Posterior distribution of the association of each leg with\\nheight, from model m6.1. Since both variables contain almost identical in-\\nformation, the posterior is a narrow ridge of negatively correlated values.\\nRight: The posterior distribution of the sum of the two parameters is cen-\\ntered on the proper association of either leg with height.\\nalong a narrow ridge. When bl is large, then br must be small. What has happened here\\nis that since both leg variables contain almost exactly the same information, if you insist on\\nincluding both in a model, then there will be a practically infinite number of combinations\\nof bl and br that produce the same predictions.\\nOne way to think of this phenomenon is that you have approximated this model:\\nyi ∼Normal(µi, σ)\\nµi = α + β1xi + β2xi\\nThe variable y is the outcome, like height in the example, and x is a single predictor, like the\\nleg lengths in the example. Here x is used twice, which is a perfect example of the problem\\ncaused by using both leg lengths. From the golem’s perspective, the model for µi is:\\nµi = α + (β1 + β2)xi\\nAll I’ve done is factor xi out of each term. The parameters β1 and β2 cannot be pulled apart,\\nbecause they never separately influence the mean µ. Only their sum, β1+β2, influences µ. So\\nthis means the posterior distribution ends up reporting the very large range of combinations\\nof β1 and β2 that make their sum close to the actual association of x with y.\\nAnd the posterior distribution in this simulated example has done exactly that: It has\\nproduced a good estimate of the sum of bl and br. Here’s how you can compute the posterior\\ndistribution of their sum, and then plot it:\\nR code\\n6.6\\nsum_blbr <- post$bl + post$br\\ndens( sum_blbr , col=rangi2 , lwd=2 , xlab=\"sum of bl and br\" )\\nAnd the resulting density plot is shown on the right-hand side of Figure 6.2. The posterior\\nmean is in the right neighborhood, a little over 2, and the standard deviation is much smaller\\n'},\n",
       " {'index': 184,\n",
       "  'number': 166,\n",
       "  'content': '166\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\nthan it is for either component of the sum, bl or br. If you fit a regression with only one of\\nthe leg length variables, you’ll get approximately the same posterior mean:\\nR code\\n6.7\\nm6.2 <- quap(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a + bl*leg_left,\\na ~ dnorm( 10 , 100 ) ,\\nbl ~ dnorm( 2 , 10 ) ,\\nsigma ~ dexp( 1 )\\n) , data=d )\\nprecis(m6.2)\\nmean\\nsd 5.5% 94.5%\\na\\n1.00 0.28 0.54\\n1.45\\nbl\\n1.99 0.06 1.89\\n2.09\\nsigma 0.62 0.04 0.55\\n0.69\\nThat 1.99 is almost identical to the mean value of sum_blbr.\\nThe basic lesson is only this: When two predictor variables are very strongly correlated\\n(conditional on other variables in the model), including both in a model may lead to confu-\\nsion. The posterior distribution isn’t wrong, in such cases. It’s telling you that the question\\nyou asked cannot be answered with these data. And that’s a great thing for a model to say,\\nthat it cannot answer your question. And if you are just interested in prediction, you’ll find\\nthat this leg model makes fine predictions. It just doesn’t make any claims about which leg\\nis more important.\\nThis leg example is clear and cute. But it is also purely statistical. We aren’t asking any\\nserious causal questions here. Let’s try a more causally interesting example next.\\n6.1.2. Multicollinear milk. In the leg length example, it’s easy to see that including both legs\\nin the model is a little silly. But the problem that arises in real data sets is that we may not\\nanticipate a clash between highly correlated predictors. And therefore we may mistakenly\\nread the posterior distribution to say that neither predictor is important. In this section, we\\nlook at an example of this issue with real data.\\nLet’s return to the primate milk data from earlier in the chapter:\\nR code\\n6.8\\nlibrary(rethinking)\\ndata(milk)\\nd <- milk\\nd$K <- standardize( d$kcal.per.g )\\nd$F <- standardize( d$perc.fat )\\nd$L <- standardize( d$perc.lactose )\\nIn this example, we are concerned with the perc.fat (percent fat) and perc.lactose (per-\\ncent lactose) variables. We’ll use these to model the total energy content, kcal.per.g. The\\ncode above has already standardized these three variables. You’re going to use these three\\nvariables to explore a natural case of multicollinearity. Note that there are no missing values,\\nNA, in these columns, so there’s no need here to extract complete cases. But you can rest\\nassured that quap, unlike reckless functions like lm, would never silently drop cases.\\n'},\n",
       " {'index': 185,\n",
       "  'number': 167,\n",
       "  'content': '6.1. MULTICOLLINEARITY\\n167\\nStart by modeling kcal.per.g as a function of perc.fat and perc.lactose, but in\\ntwo bivariate regressions. Look back in Chapter 5 (page 147), for a discussion of these priors.\\nR code\\n6.9\\n# kcal.per.g regressed on perc.fat\\nm6.3 <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ) ,\\nmu <- a + bF*F ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbF ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data=d )\\n# kcal.per.g regressed on perc.lactose\\nm6.4 <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ) ,\\nmu <- a + bL*L ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbL ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data=d )\\nprecis( m6.3 )\\nprecis( m6.4 )\\nmean\\nsd\\n5.5% 94.5%\\na\\n0.00 0.08 -0.12\\n0.12\\nbF\\n0.86 0.08\\n0.73\\n1.00\\nsigma 0.45 0.06\\n0.36\\n0.54\\nmean\\nsd\\n5.5% 94.5%\\na\\n0.00 0.07 -0.11\\n0.11\\nbL\\n-0.90 0.07 -1.02 -0.79\\nsigma\\n0.38 0.05\\n0.30\\n0.46\\nThe posterior distributions for bF and bL are essentially mirror images of one another. The\\nposterior mean of bF is as positive as the mean of bL is negative. Both are narrow posterior\\ndistributions that lie almost entirely on one side or the other of zero. Given the strong associ-\\nation of each predictor with the outcome, we might conclude that both variables are reliable\\npredictors of total energy in milk, across species. The more fat, the more kilocalories in the\\nmilk. The more lactose, the fewer kilocalories in milk. But watch what happens when we\\nplace both predictor variables in the same regression model:\\nR code\\n6.10\\nm6.5 <- quap(\\nalist(\\nK ~ dnorm( mu , sigma ) ,\\nmu <- a + bF*F + bL*L ,\\na ~ dnorm( 0 , 0.2 ) ,\\n'},\n",
       " {'index': 186,\n",
       "  'number': 168,\n",
       "  'content': '168\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\nkcal.per.g\\n10\\n30\\n50\\n0.5\\n0.7\\n0.9\\n10\\n30\\n50\\nperc.fat\\n0.5\\n0.7\\n0.9\\n30\\n50\\n70\\n30\\n50\\n70\\nperc.lactose\\nFigure 6.3. A pairs plot of the total en-\\nergy, percent fat, and percent lactose vari-\\nables from the primate milk data. Percent\\nfat and percent lactose are strongly nega-\\ntively correlated with one another, provid-\\ning mostly the same information.\\nbF ~ dnorm( 0 , 0.5 ) ,\\nbL ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) ,\\ndata=d )\\nprecis( m6.5 )\\nmean\\nsd\\n5.5% 94.5%\\na\\n0.00 0.07 -0.11\\n0.11\\nbF\\n0.24 0.18 -0.05\\n0.54\\nbL\\n-0.68 0.18 -0.97 -0.38\\nsigma\\n0.38 0.05\\n0.30\\n0.46\\nNow the posterior means of both bF and bL are closer to zero. And the standard deviations\\nfor both parameters are twice as large as in the bivariate models (m6.3 and m6.4).\\nThis is the same statistical phenomenon as in the leg length example. What has happened\\nis that the variables perc.fat and perc.lactose contain much of the same information.\\nThey are almost substitutes for one another. As a result, when you include both in a regres-\\nsion, the posterior distribution ends up describing a long ridge of combinations of bF and\\nbL that are equally plausible. In the case of the fat and lactose, these two variables form\\nessentially a single axis of variation. The easiest way to see this is to use a pairs plot:\\nR code\\n6.11\\npairs( ~ kcal.per.g + perc.fat + perc.lactose , data=d , col=rangi2 )\\nI display this plot in Figure 6.3. Along the diagonal, the variables are labeled. In each scat-\\nterplot off the diagonal, the vertical axis variable is the variable labeled on the same row and\\nthe horizontal axis variable is the variable labeled in the same column. For example, the\\ntwo scatterplots in the first row in Figure 6.3 are kcal.per.g (vertical) against perc.fat\\n(horizontal) and then kcal.per.g (vertical) against perc.lactose (horizontal). Notice\\n'},\n",
       " {'index': 187,\n",
       "  'number': 169,\n",
       "  'content': '6.1. MULTICOLLINEARITY\\n169\\nthat percent fat is positively correlated with the outcome, while percent lactose is negatively\\ncorrelated with it. Now look at the right-most scatterplot in the middle row. This plot is the\\nscatter of percent fat (vertical) against percent lactose (horizontal). Notice that the points\\nline up almost entirely along a straight line. These two variables are negatively correlated,\\nand so strongly so that they are nearly redundant. Either helps in predicting kcal.per.g,\\nbut neither helps as much once you already know the other.\\nIn the scientific literature, you might encounter a variety of dodgy ways of coping with\\nmulticollinearity. Few of them take a causal perspective. Some fields actually teach students\\nto inspect pairwise correlations before fitting a model, to identify and drop highly correlated\\npredictors. This is a mistake. Pairwise correlations are not the problem. It is the conditional\\nassociations—not correlations—that matter. And even then, the right thing to do will de-\\npend upon what is causing the collinearity. The associations within the data alone are not\\nenough to decide what to do.\\nWhat is likely going on in the milk example is that there is a core tradeoff in milk com-\\nposition that mammal mothers must obey. If a species nurses often, then the milk tends to\\nbe watery and low in energy. Such milk is high in sugar (lactose). If instead a species nurses\\nrarely, in short bouts, then the milk needs to be higher in energy. Such milk is very high in\\nfat. This implies a causal model something like this:\\nD\\nF\\nK\\nL\\nThe central tradeoff decides how dense, D, the milk needs to be. We haven’t observed this\\nvariable, so it’s shown circled. Then fat, F, and lactose, L, are determined. Finally, the com-\\nposition of F and L determines the kilocalories, K. If we could measure D, or had an evolu-\\ntionary and economic model to predict it based upon other aspects of a species, that would\\nbe better than stumbling through regressions.\\nThe problem of multicollinearity is a member of a family of problems with fitting models,\\na family sometimes known as non-identifiability. When a parameter is non-identifiable,\\nit means that the structure of the data and model do not make it possible to estimate the\\nparameter’s value. Sometimes this problem arises from mistakes in coding a model, but\\nmany important types of models present non-identifiable or weakly identifiable parameters,\\neven when coded completely correctly. Nature does not owe us easy inference, even when\\nthe model is correct.\\nIn general, there’s no guarantee that the available data contain much information about\\na parameter of interest. When that’s true, your Bayesian machine will return a posterior\\ndistribution very similar to the prior. Comparing the posterior to the prior can therefore\\nbe a good idea, a way of seeing how much information the model extracted from the data.\\nWhen the posterior and prior are similar, it doesn’t mean the calculations are wrong—you\\ngot the right answer to the question you asked. But it might lead you to ask a better question.\\nRethinking: Identification guaranteed; comprehension up to you. Technically speaking, identifia-\\nbility is not a concern for Bayesian models. The reason is that as long as the posterior distribution is\\nproper—which just means that it integrates to 1—then all of the parameters are identified. But this\\n'},\n",
       " {'index': 188,\n",
       "  'number': 170,\n",
       "  'content': '170\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\ntechnical fact doesn’t also mean that you can make sense of the posterior distribution. So it’s probably\\nbetter to speak of weakly identified parameters in a Bayesian context. But the difference may be only\\ntechnical. The truth is that even when a DAG says a causal effect should be identifiable, it may not be\\nstatistically identifiable. We have to work just as hard at the statistics as we do at the design.\\nOverthinking: Simulating collinearity. To see how imprecise of the posterior increases with associ-\\nation between two predictors, let’s use a simulation. The code below makes a function that generates\\ncorrelated predictors, fits a model, and returns the standard deviation of the posterior distribution\\nfor the slope relating perc.fat to kcal.per.g. Then the code repeatedly calls this function, with\\ndifferent degrees of correlation as input, and collects the results.\\nR code\\n6.12\\nlibrary(rethinking)\\ndata(milk)\\nd <- milk\\nsim.coll <- function( r=0.9 ) {\\nd$x <- rnorm( nrow(d) , mean=r*d$perc.fat ,\\nsd=sqrt( (1-r^2)*var(d$perc.fat) ) )\\nm <- lm( kcal.per.g ~ perc.fat + x , data=d )\\nsqrt( diag( vcov(m) ) )[2] # stddev of parameter\\n}\\nrep.sim.coll <- function( r=0.9 , n=100 ) {\\nstddev <- replicate( n , sim.coll(r) )\\nmean(stddev)\\n}\\nr.seq <- seq(from=0,to=0.99,by=0.01)\\nstddev <- sapply( r.seq , function(z) rep.sim.coll(r=z,n=100) )\\nplot( stddev ~ r.seq , type=\"l\" , col=rangi2, lwd=2 , xlab=\"correlation\" )\\nSo for each correlation value in r.seq, the code generates 100 regressions and returns the average\\nstandard deviation from them. This code uses implicit flat priors, which are bad priors. So it does\\nexaggerate the effect of collinear variables. When you use informative priors, the inflation in standard\\ndeviation can be much slower.\\n6.2. Post-treatment bias\\nIt is routine to worry about mistaken inferences that arise from omitting predictor vari-\\nables. Such mistakes are often called omitted variable bias, and the examples from the\\nprevious chapter illustrate it. It is much less routine to worry about mistaken inferences\\narising from including variables. But included variable bias is real. Carefully random-\\nized experiments can be ruined just as easily as uncontrolled observational studies. Blindly\\ntossing variables into the causal salad is never a good idea.\\nIncluded variable bias takes several forms. The first is post-treatment bias.89 Post-\\ntreatment bias is a risk in all types of studies. The language “post-treatment” comes in fact\\nfrom thinking about experimental designs. Suppose for example that you are growing some\\nplants in a greenhouse. You want to know the difference in growth under different anti-\\nfungal soil treatments, because fungus on the plants tends to reduce their growth. Plants are\\ninitially seeded and sprout. Their heights are measured. Then different soil treatments are\\napplied. Final measures are the height of the plant and the presence of fungus. There are\\nfour variables of interest here: initial height, final height, treatment, and presence of fungus.\\n'},\n",
       " {'index': 189,\n",
       "  'number': 171,\n",
       "  'content': '6.2. POST-TREATMENT BIAS\\n171\\nFinal height is the outcome of interest. But which of the other variables should be in the\\nmodel? If your goal is to make a causal inference about the treatment, you shouldn’t include\\nthe fungus, because it is a post-treatment effect.\\nLet’s simulate some data, to make the example more transparent and see what exactly\\ngoes wrong when we include a post-treatment variable.\\nR code\\n6.13\\nset.seed(71)\\n# number of plants\\nN <- 100\\n# simulate initial heights\\nh0 <- rnorm(N,10,2)\\n# assign treatments and simulate fungus and growth\\ntreatment <- rep( 0:1 , each=N/2 )\\nfungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 )\\nh1 <- h0 + rnorm(N, 5 - 3*fungus)\\n# compose a clean data frame\\nd <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )\\nprecis(d)\\nmean\\nsd\\n5.5% 94.5%\\nhistogram\\nh0\\n9.96 2.10\\n6.57 13.08 ▁▂▂▂▇▃▂▃▁▁▁▁\\nh1\\n14.40 2.69 10.62 17.93\\n▁▁▃▇▇▇▁▁\\ntreatment\\n0.50 0.50\\n0.00\\n1.00\\n▇▁▁▁▁▁▁▁▁▇\\nfungus\\n0.23 0.42\\n0.00\\n1.00\\n▇▁▁▁▁▁▁▁▁▂\\nNow you should have a data frame d with the simulated plant experiment data.\\nRethinking: Causal inference heuristics. The danger of post-treatment bias has been known for a\\nlong time. So many scientists have been taught the heuristic that while it is risky to condition on post-\\ntreatment variables, pre-treatment variables are safe. This heuristic may lead to sensible estimates in\\nmany cases. But it is not principled. Pre-treatment variables can also create bias, as you’ll see later\\nin this chapter. There is nothing wrong, in principle, with heuristics. They are safe in the context for\\nwhich they were developed. But we still need principles to know when to deploy them.\\n6.2.1. A prior is born. When designing the model, it helps to pretend you don’t have the\\ndata generating process just above. In real research, you will not know the real data gener-\\nating process. But you will have a lot of scientific information to guide model construction.\\nSo let’s spend some time taking this mock analysis seriously.\\nWe know that the plants at time t = 1 should be taller than at time t = 0, whatever scale\\nthey are measured on. So if we put the parameters on a scale of proportion of height at time\\nt = 0, rather than on the absolute scale of the data, we can set the priors more easily. To\\nmake this simpler, let’s focus right now only on the height variables, ignoring the predictor\\nvariables. We might have a linear model like:\\nh1,i ∼Normal(µi, σ)\\nµi = h0,i × p\\n'},\n",
       " {'index': 190,\n",
       "  'number': 172,\n",
       "  'content': \"172\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\nwhere h0,i is plant i’s height at time t = 0, h1,i is its height at time t = 1, and p is a parameter\\nmeasuring the proportion of h0,i that h1,i is. More precisely, p = h1,i/h0,i. If p = 1, the plant\\nhasn’t changed at all from time t = 0 to time t = 1. If p = 2, it has doubled in height. So if\\nwe center our prior for p on 1, that implies an expectation of no change in height. That is less\\nthan we know. But we should allow p to be less than 1, in case the experiment goes horribly\\nwrong and we kill all the plants. We also have to ensure that p > 0, because it is a proportion.\\nBack in Chapter 4 (page 96), we used a Log-Normal distribution, because it is always positive.\\nLet’s use one again. If we use p ∼Log-Normal(0, 0.25), the prior distribution looks like:\\nR code\\n6.14\\nsim_p <- rlnorm( 1e4 , 0 , 0.25 )\\nprecis( data.frame(sim_p) )\\n'data.frame': 10000 obs. of 1 variables:\\nmean\\nsd 5.5% 94.5%\\nhistogram\\nsim_p 1.03 0.26 0.67\\n1.48 ▁▃▇▇▃▁▁▁▁▁▁\\nSo this prior expects anything from 40% shrinkage up to 50% growth. Let’s fit this model, so\\nyou can see how it just measures the average growth in the experiment.\\nR code\\n6.15\\nm6.6 <- quap(\\nalist(\\nh1 ~ dnorm( mu , sigma ),\\nmu <- h0*p,\\np ~ dlnorm( 0 , 0.25 ),\\nsigma ~ dexp( 1 )\\n), data=d )\\nprecis(m6.6)\\nmean\\nsd 5.5% 94.5%\\np\\n1.43 0.02 1.40\\n1.45\\nsigma 1.79 0.13 1.59\\n1.99\\nAbout 40% growth, on average. Now to include the treatment and fungus variables. We’ll\\ninclude both of them, following the notion that we’d like to measure the impact of both the\\ntreatment and the fungus itself. The parameters for these variables will also be on the pro-\\nportion scale. They will be changes in proportion growth. So we’re going to make a linear\\nmodel of p now.\\nh1,i ∼Normal(µi, σ)\\nµi = h0,i × p\\np = α + βTTi + βFFi\\nα ∼Log-Normal(0, 0.25)\\nβT ∼Normal(0, 0.5)\\nβF ∼Normal(0, 0.5)\\nσ ∼Exponential(1)\\nThe proportion of growth p is now a function of the predictor variables. It looks like any\\nother linear model. The priors on the slopes are almost certainly too flat. They place 95% of\\nthe prior mass between −1 (100% reduction) and +1 (100% increase) and two-thirds of the\\n\"},\n",
       " {'index': 191,\n",
       "  'number': 173,\n",
       "  'content': '6.2. POST-TREATMENT BIAS\\n173\\nprior mass between −0.5 and +0.5. After we finish this section, you may want to loop back\\nand try simulating from these priors. Here’s the code to approximate the posterior:\\nR code\\n6.16\\nm6.7 <- quap(\\nalist(\\nh1 ~ dnorm( mu , sigma ),\\nmu <- h0 * p,\\np <- a + bt*treatment + bf*fungus,\\na ~ dlnorm( 0 , 0.2 ) ,\\nbt ~ dnorm( 0 , 0.5 ),\\nbf ~ dnorm( 0 , 0.5 ),\\nsigma ~ dexp( 1 )\\n), data=d )\\nprecis(m6.7)\\nmean\\nsd\\n5.5% 94.5%\\na\\n1.48 0.02\\n1.44\\n1.52\\nbt\\n0.00 0.03 -0.05\\n0.05\\nbf\\n-0.27 0.04 -0.33 -0.21\\nsigma\\n1.41 0.10\\n1.25\\n1.57\\nThat a parameter is the same as p before. And it has nearly the same posterior. The marginal\\nposterior for bt, the effect of treatment, is solidly zero, with a tight interval. The treatment is\\nnot associated with growth. The fungus seems to have hurt growth, however. Given that we\\nknow the treatment matters, because we built the simulation that way, what happened here?\\n6.2.2. Blocked by consequence. The problem is that fungus is mostly a consequence of\\ntreatment. This is to say that fungus is a post-treatment variable. So when we control\\nfor fungus, the model is implicitly answering the question: Once we already know whether\\nor not a plant developed fungus, does soil treatment matter? The answer is “no,” because soil\\ntreatment has its effects on growth through reducing fungus. But we actually want to know,\\nbased on the design of the experiment, is the impact of treatment on growth. To measure\\nthis properly, we should omit the post-treatment variable fungus. Here’s what the inference\\nlooks like in that case:\\nR code\\n6.17\\nm6.8 <- quap(\\nalist(\\nh1 ~ dnorm( mu , sigma ),\\nmu <- h0 * p,\\np <- a + bt*treatment,\\na ~ dlnorm( 0 , 0.2 ),\\nbt ~ dnorm( 0 , 0.5 ),\\nsigma ~ dexp( 1 )\\n), data=d )\\nprecis(m6.8)\\nmean\\nsd 5.5% 94.5%\\na\\n1.38 0.03 1.34\\n1.42\\nbt\\n0.08 0.03 0.03\\n0.14\\nsigma 1.75 0.12 1.55\\n1.94\\n'},\n",
       " {'index': 192,\n",
       "  'number': 174,\n",
       "  'content': '174\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\nNow the impact of treatment is clearly positive, as it should be. It makes sense to control\\nfor pre-treatment differences, like the initial height h0, that might mask the causal influence\\nof treatment. But including post-treatment variables can actually mask the treatment itself.\\nThis doesn’t mean you don’t want the model that includes both treatment and fungus. The\\nfact that including fungus zeros the coefficient for treatment suggests that the treatment\\nworks for exactly the anticipated reasons. It tells us about mechanism. But a correct inference\\nabout the treatment still depends upon omitting the post-treatment variable.\\n6.2.3. Fungus and d-separation. It helps to look at this problem in terms of a DAG. In this\\ncase, I’ll show you how to draw it using the dagitty R package, because we are going to use\\nthat package now to do some graph analysis.\\nR code\\n6.18\\nlibrary(dagitty)\\nplant_dag <- dagitty( \"dag {\\nH_0 -> H_1\\nF -> H_1\\nT -> F\\n}\")\\ncoordinates( plant_dag ) <- list( x=c(H_0=0,T=2,F=1.5,H_1=1) ,\\ny=c(H_0=0,T=0,F=0,H_1=0) )\\ndrawdag( plant_dag )\\nF\\nH0\\nH1\\nT\\nSo the treatment T influences the presence of fungus F which influences plant height at time\\n1, H1. Plant height at time 1 is also influenced by plant height at time 0, H0. That’s our\\nDAG. When we include F, the post-treatment effect, in the model, we end up blocking the\\npath from the treatment to the outcome. This is the DAG way of saying that learning the\\ntreatment tells us nothing about the outcome, once we know the fungus status.\\nAn even more DAG way to say this is that conditioning on F induces d-separation.\\nThe “d” stands for directional.90 D-separation means that some variables on a directed graph\\nare independent of others. There is no path connecting them. In this case, H1 is d-separated\\nfrom T, but only when we condition on F. Conditioning on F effectively blocks the directed\\npath T →F →H1, making T and H1 independent (d-separated). In the previous chapter,\\nyou saw the notation H1 ⊥⊥T|F for this kind of statement, when we discussed implied con-\\nditional independencies. Why does this happen? There is no information in T about\\nH1 that is not also in F. So once we know F, learning T provides no additional information\\nabout H1. You can query the implied conditional independencies for this DAG:\\nR code\\n6.19\\nimpliedConditionalIndependencies(plant_dag)\\nF _||_ H0\\nH0 _||_ T\\nH1 _||_ T | F\\nThere are three. The third one is the focus of our discussion. But the other two implications\\nprovide ways to test the DAG. What F ⊥⊥H0 and H0 ⊥⊥T say is that the original plant\\n'},\n",
       " {'index': 193,\n",
       "  'number': 175,\n",
       "  'content': '6.2. POST-TREATMENT BIAS\\n175\\nheight, H0, should not be associated with the treatment T or fungus F, provided we do not\\ncondition on anything.\\nObviously the problem of post-treatment variables applies just as well to observational\\nstudies as it does to experiments. But in experiments, it can be easier to tell which variables\\nare pre-treatment, like h0, and which are post-treatment, like fungus. In observational stud-\\nies, it is harder to know. But there are many traps in experiments as well.91 For example,\\nconditioning on a post-treatment variable can not only fool you into thinking the treatment\\ndoesn’t work. It can also fool you into thinking it does work. Consider the DAG below:\\nF\\nH0\\nH1\\nM\\nT\\nIn this graph, the treatment T influences fungus F, but fungus doesn’t influence plant growth.\\nMaybe the plant species just isn’t bothered by this particular fungus. The new variable M is\\nmoisture. It influences both H1 and F. M is circled to indicate that it is unobserved. Any\\nunobserved common cause of H1 and F will do—it doesn’t have to be moisture of course.\\nA regression of H1 on T will show no association between the treatment and plant growth.\\nBut if we include F in the model, suddenly there will be an association. Let’s try it. I’ll just\\nmodify the plant growth simulation so that fungus has no influence on growth, but moisture\\nM influences both H1 and F:\\nR code\\n6.20\\nset.seed(71)\\nN <- 1000\\nh0 <- rnorm(N,10,2)\\ntreatment <- rep( 0:1 , each=N/2 )\\nM <- rbern(N)\\nfungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 + 0.4*M )\\nh1 <- h0 + rnorm( N , 5 + 3*M )\\nd2 <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )\\nRerun the models from earlier, models m6.7 and m6.8, using the data in d2 now. You’ll see\\nthat including fungus again confounds inference about the treatment, this time by making\\nit seem like it helped the plants, even though it had no effect.\\nThis result is rather mysterious. Why should M have this effect? The next section is all\\nabout effects like this.\\nRethinking: Model selection doesn’t help. In the next chapter, you’ll learn about model selection\\nusing information criteria. Like other model comparison and selection schemes, these criteria help in\\ncontrasting and choosing model structure. But such approaches are no help in the example presented\\njust above, since the model that includes fungus both fits the sample better and would make better\\nout-of-sample predictions. Model m6.7 misleads because it asks the wrong question, not because it\\nwould make poor predictions. As argued in Chapter 1, prediction and causal inference are just not\\nthe same task. No statistical procedure can substitute for scientific knowledge and attention to it. We\\nneed multiple models because they help us understand causal paths, not just so we can choose one or\\nanother for prediction.\\n'},\n",
       " {'index': 194,\n",
       "  'number': 176,\n",
       "  'content': '176\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\n6.3. Collider bias\\nAt the start of the chapter, I argued that all that is necessary for scientific studies to\\nshow a negative association between trustworthiness and newsworthiness is that selection\\nprocesses—grant and journal review—care about both. Now I want to explain how this same\\nselection phenomenon can happen inside a statistical model. When it does, it can seriously\\ndistort our inferences, a phenomenon known as collider bias.\\nLet’s consider a DAG for this example. The model is that trustworthiness (T) and news-\\nworthiness (N) are not associated in the population of research proposals submitted to grant\\nreview panels. But both of them influence selection (S) for funding. This is the graph:\\nN\\nS\\nT\\nThe fact that two arrows enter S means it is a collider. The core concept is easy to under-\\nstand: When you condition on a collider, it creates statistical—but not necessarily causal—\\nassociations among its causes. In this case, once you learn that a proposal has been selected\\n(S), then learning its trustworthiness (T) also provides information about its newsworthiness\\n(N). Why? Because if, for example, a selected proposal has low trustworthiness, then it must\\nhave high newsworthiness. Otherwise it wouldn’t have been funded. The same works in re-\\nverse: If a proposal has low newsworthiness, we’d infer that it must have higher than average\\ntrustworthiness. Otherwise it would not have been selected for funding.\\nThis is the informational phenomenon that generates the negative association between T\\nand N in the population of selected proposals. And it means we have to pay attention to pro-\\ncesses that select our sample of observations and may distort associations among variables.\\nBut the same phenomenon will also generate a misleading association inside a statistical\\nmodel, when you include the collider as a predictor variable. If you are not careful, you can\\nmake an erroneous causal inference. Let’s consider an extended example.\\n6.3.1. Collider of false sorrow. Consider the question of how aging influences happiness. If\\nwe have a large survey of people rating how happy they are, is age associated with happiness?\\nIf so, is that association causal? Here, I want to show you how controlling for a plausible\\nconfound of happiness can actually bias inference about the influence of age.92\\nSuppose, just to be provocative, that an individual’s average happiness is a trait that is\\ndetermined at birth and does not change with age. However, happiness does influence events\\nin one’s life. One of those events is marriage. Happier people are more likely to get married.\\nAnother variable that causally influences marriage is age: The more years you are alive, the\\nmore likely you are to eventually get married. Putting these three variables together, this is\\nthe causal model:\\nA\\nH\\nM\\nHappiness (H) and age (A) both cause marriage (M). Marriage is therefore a collider. Even\\nthough there is no causal association between happiness and age, if we condition on marriage—\\nwhich means here, if we include it as a predictor in a regression—then it will induce a statis-\\ntical association between age and happiness. And this can mislead us to think that happiness\\nchanges with age, when in fact it is constant.\\n'},\n",
       " {'index': 195,\n",
       "  'number': 177,\n",
       "  'content': \"6.3. COLLIDER BIAS\\n177\\nTo convince you of this, let’s do another simulation. Simulations are useful in these ex-\\namples, because these are the only times when we know the true causal model. If a procedure\\ncannot figure out the truth in a simulated example, we shouldn’t trust it in a real one. We’re\\ngoing to do a fancier simulation this time, using an agent-based model of aging and marriage\\nto produce a simulated data set to use in a regression. Here is the simulation design:\\n(1) Each year, 20 people are born with uniformly distributed happiness values.\\n(2) Each year, each person ages one year. Happiness does not change.\\n(3) At age 18, individuals can become married. The odds of marriage each year are\\nproportional to an individual’s happiness.\\n(4) Once married, an individual remains married.\\n(5) After age 65, individuals leave the sample. (They move to Spain.)\\nI’ve written this algorithm into the rethinking package. You can run it out for 1000 years\\nand collect the resulting data:\\nR code\\n6.21\\nlibrary(rethinking)\\nd <- sim_happiness( seed=1977 , N_years=1000 )\\nprecis(d)\\n'data.frame': 1300 obs. of 3 variables:\\nmean\\nsd\\n5.5% 94.5%\\nhistogram\\nage\\n33.0 18.77\\n4.00 62.00 ▇▇▇▇▇▇▇▇▇▇▇▇▇\\nmarried\\n0.3\\n0.46\\n0.00\\n1.00\\n▇▁▁▁▁▁▁▁▁▃\\nhappiness\\n0.0\\n1.21 -1.79\\n1.79\\n▇▅▇▅▅▇▅▇\\nThese data comprise 1300 people of all ages from birth to 65 years old. The variables corre-\\nspond to the variables in the DAG above, and the simulation itself obeys the DAG.\\nI’ve plotted these data in Figure 6.4, showing each individual as a point. Filled points\\nare married individuals. Age is on the horizontal, and happiness the vertical, with the hap-\\npiest individuals at the top. At age 18, they become able to marry, and then gradually more\\nindividuals are married each year. So at older ages, more individuals are married. But at all\\nages, the happiest individuals are more likely to be married.\\nSuppose you come across these data and want to ask whether age is related to happiness.\\nYou don’t know the true causal model. But you reason, reasonably, that marriage status\\nmight be a confound. If married people are more or less happy, on average, then you need to\\ncondition on marriage status in order to infer the relationship between age and happiness.\\nSo let’s consider a multiple regression model aimed at inferring the influence of age on\\nhappiness, while controlling for marriage status. This is just a plain multiple regression, like\\nthe others in this and the previous chapter. The linear model is this:\\nµi = αmid[i] + βAAi\\nwhere mid[i] is an index for the marriage status of individual i, with 1 meaning single and\\n2 meaning married. This is just the categorical variable strategy from Chapter 4. It’s easier\\nto make priors, when we use multiple intercepts, one for each category, than when we use\\nindicator variables.\\nNow we should do our duty and think about the priors. Let’s consider the slope βA first,\\nbecause how we scale the predictor A will determine the meaning of the intercept. We’ll\\nfocus only on the adult sample, those 18 or over. Imagine a very strong relationship between\\n\"},\n",
       " {'index': 196,\n",
       "  'number': 178,\n",
       "  'content': '178\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n-2\\n-1\\n0\\n1\\n2\\nage\\nhappiness\\nmarried\\nunmarried\\nFigure 6.4. Simulated data, assuming that happiness is uniformly dis-\\ntributed and never changes. Each point is a person. Married individuals\\nare shown with filled blue points. At each age after 18, the happiest individ-\\nuals are more likely to be married. At later ages, more individuals tend to be\\nmarried. Marriage status is a collider of age and happiness: A →M ←H.\\nIf we condition on marriage in a regression, it will mislead us to believe that\\nhappiness declines with age.\\nage and happiness, such that happiness is at its maximum at age 18 and its minimum at age\\n65. It’ll be easier if we rescale age so that the range from 18 to 65 is one unit. This will do it:\\nR code\\n6.22\\nd2 <- d[ d$age>17 , ] # only adults\\nd2$A <- ( d2$age - 18 ) / ( 65 - 18 )\\nNow this new variable A ranges from 0 to 1, where 0 is age 18 and 1 is age 65. Happiness\\nis on an arbitrary scale, in these data, from −2 to +2. So our imaginary strongest rela-\\ntionship, taking happiness from maximum to minimum, has a slope with rise over run of\\n(2 −(−2))/1 = 4. Remember that 95% of the mass of a normal distribution is contained\\nwithin 2 standard deviations. So if we set the standard deviation of the prior to half of 4, we\\nare saying that we expect 95% of plausible slopes to be less than maximally strong. That isn’t\\na very strong prior, but again, it at least helps bound inference to realistic ranges. Now for\\nthe intercepts. Each α is the value of µi when Ai = 0. In this case, that means at age 18. So\\nwe need to allow α to cover the full range of happiness scores. Normal(0, 1) will put 95% of\\nthe mass in the −2 to +2 interval.\\nFinally, let’s approximate the posterior. We need to construct the marriage status index\\nvariable, as well. I’ll do that, and then immediately present the quap code.\\nR code\\n6.23\\nd2$mid <- d2$married + 1\\nm6.9 <- quap(\\nalist(\\nhappiness ~ dnorm( mu , sigma ),\\n'},\n",
       " {'index': 197,\n",
       "  'number': 179,\n",
       "  'content': '6.3. COLLIDER BIAS\\n179\\nmu <- a[mid] + bA*A,\\na[mid] ~ dnorm( 0 , 1 ),\\nbA ~ dnorm( 0 , 2 ),\\nsigma ~ dexp(1)\\n) , data=d2 )\\nprecis(m6.9,depth=2)\\nmean\\nsd\\n5.5% 94.5%\\na[1]\\n-0.23 0.06 -0.34 -0.13\\na[2]\\n1.26 0.08\\n1.12\\n1.40\\nbA\\n-0.75 0.11 -0.93 -0.57\\nsigma\\n0.99 0.02\\n0.95\\n1.03\\nThe model is quite sure that age is negatively associated with happiness. We’d like to compare\\nthe inferences from this model to a model that omits marriage status. Here it is, followed by\\na comparison of the marginal posterior distributions:\\nR code\\n6.24\\nm6.10 <- quap(\\nalist(\\nhappiness ~ dnorm( mu , sigma ),\\nmu <- a + bA*A,\\na ~ dnorm( 0 , 1 ),\\nbA ~ dnorm( 0 , 2 ),\\nsigma ~ dexp(1)\\n) , data=d2 )\\nprecis(m6.10)\\nmean\\nsd\\n5.5% 94.5%\\na\\n0.00 0.08 -0.12\\n0.12\\nbA\\n0.00 0.13 -0.21\\n0.21\\nsigma 1.21 0.03\\n1.17\\n1.26\\nThis model, in contrast, finds no association between age and happiness.\\nThe pattern above is exactly what we should expect when we condition on a collider. The\\ncollider is marriage status. It is a common consequence of age and happiness. As a result,\\nwhen we condition on it, we induce a spurious association between the two causes. So it\\nlooks like, to model m6.9, that age is negatively associated with happiness. But this is just a\\nstatistical association, not a causal association. Once we know whether someone is married\\nor not, then their age does provide information about how happy they are.\\nYou can see this in Figure 6.4. Consider only the blue points, the married people.\\nAmong only the blue points, older individuals have lower average happiness. This is because\\nmore people get married as time goes on, so the mean happiness among married people ap-\\nproaches the population average of zero. Now consider only the open points, the unmarried\\npeople. Here it is also true that mean happiness declines with age. This is because happier\\nindividuals migrate over time into the married sub-population. So in both the married and\\nunmarried sub-populations, there is a negative relationship between age and happiness. But\\nin neither sub-population does this accurately reflect causation.\\nIt’s easy to plead with this example. Shouldn’t marriage also influence happiness? What\\nif happiness does change with age? But this misses the point. If you don’t have a causal\\n'},\n",
       " {'index': 198,\n",
       "  'number': 180,\n",
       "  'content': '180\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\nmodel, you can’t make inferences from a multiple regression. And the regression itself does\\nnot provide the evidence you need to justify a causal model. Instead, you need some science.\\n6.3.2. ThehauntedDAG. Collider bias arises from conditioning on a common consequence,\\nas in the previous example. If we can just get our graph sorted, we can avoid it. But it isn’t\\nalways so easy to see a potential collider, because there may be unmeasured causes. Unmea-\\nsured causes can still induce collider bias. So I’m sorry to say that we also have to consider\\nthe possibility that our DAG may be haunted.\\nSuppose for example that we want to infer the direct influence of both parents (P) and\\ngrandparents (G) on the educational achievement of children (C).93 Since grandparents also\\npresumably influence their own children’s education, there is an arrow G →P. This sounds\\npretty easy, so far. It’s similar in structure to our divorce rate example from the last chapter:\\nC\\nG\\nP\\nBut suppose there are unmeasured, common influences on parents and their children, such\\nas neighborhoods, that are not shared by grandparents (who live on the south coast of Spain\\nnow). Then our DAG becomes haunted by the unobserved U:\\nC\\nG\\nP\\nU\\nNow P is a common consequence of G and U, so if we condition on P, it will bias inference\\nabout G →C, even if we never get to measure U. I don’t expect that fact to be immediately\\nobvious. So let’s crawl through a quantitative example.\\nFirst, let’s simulate 200 triads of grandparents, parents, and children. This simulation\\nwill be simple. We’ll just project our DAG as a series of implied functional relationships. The\\nDAG above implies that:\\n(1) P is some function of G and U\\n(2) C is some function of G, P, and U\\n(3) G and U are not functions of any other known variables\\nWe can make these implications into a simple simulation, using rnorm to generate simulated\\nobservations. But to do this, we need to be a bit more precise than “some function of.” So I’ll\\ninvent some strength of association:\\nR code\\n6.25\\nN <- 200\\n# number of grandparent-parent-child triads\\nb_GP <- 1 # direct effect of G on P\\nb_GC <- 0 # direct effect of G on C\\nb_PC <- 1 # direct effect of P on C\\nb_U <- 2\\n# direct effect of U on P and C\\n'},\n",
       " {'index': 199,\n",
       "  'number': 181,\n",
       "  'content': '6.3. COLLIDER BIAS\\n181\\nThese parameters are like slopes in a regression model. Notice that I’ve assumed that grand-\\nparents G have zero effect on their grandkids C. The example doesn’t depend upon that\\neffect being exactly zero, but it will make the lesson clearer. Now we use these slopes to draw\\nrandom observations:\\nR code\\n6.26\\nset.seed(1)\\nU <- 2*rbern( N , 0.5 ) - 1\\nG <- rnorm( N )\\nP <- rnorm( N , b_GP*G + b_U*U )\\nC <- rnorm( N , b_PC*P + b_GC*G + b_U*U )\\nd <- data.frame( C=C , P=P , G=G , U=U )\\nI’ve made the neighborhood effect, U, binary. This will make the example easier to under-\\nstand. But the example doesn’t depend upon that assumption. The other lines are just linear\\nmodels embedded in rnorm.\\nNow what happens when we try to infer the influence of grandparents? Since some of\\nthe total effect of grandparents passes through parents, we realize we need to control for\\nparents. Here is a simple regression of C on P and G. Normally I would advise standardizing\\nthe variables, because it makes establishing sensible priors a lot easier. But I’m going to keep\\nthe simulated data on its original scale, so you can see what happens to inference about the\\nslopes above. If we changed the scale, we shouldn’t expect to get those values back. But if\\nwe leave the scale alone, we should be able to recover something close to those values. So I\\napologize for using vague priors here, just to push forward in the example.\\nR code\\n6.27\\nm6.11 <- quap(\\nalist(\\nC ~ dnorm( mu , sigma ),\\nmu <- a + b_PC*P + b_GC*G,\\na ~ dnorm( 0 , 1 ),\\nc(b_PC,b_GC) ~ dnorm( 0 , 1 ),\\nsigma ~ dexp( 1 )\\n), data=d )\\nprecis(m6.11)\\nmean\\nsd\\n5.5% 94.5%\\na\\n-0.12 0.10 -0.28\\n0.04\\nb_PC\\n1.79 0.04\\n1.72\\n1.86\\nb_GC\\n-0.84 0.11 -1.01 -0.67\\nsigma\\n1.41 0.07\\n1.30\\n1.52\\nThe inferred effect of parents looks too big, almost twice as large as it should be. That isn’t\\nsurprising. Some of the correlation between P and C is due to U, and the model doesn’t know\\nabout U. That’s a simple confound. More surprising is that the model is confident that the\\ndirect effect of grandparents is to hurt their grandkids. The regression is not wrong. But a\\ncausal interpretation of that association would be.\\nHow does collider bias arise in this case? Consider Figure 6.5. Note that I did stan-\\ndardize the variables to make this plot. So the units on the axes are standard deviations. The\\nhorizontal axis is grandparent education. The vertical is grandchild education. There are\\ntwo clouds of points. The blue cloud comprises children who live in good neighborhoods\\n'},\n",
       " {'index': 200,\n",
       "  'number': 182,\n",
       "  'content': '182\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\ngrandparent education (G)\\ngrandchild education (C)\\ngood neighborhoods\\nbad neighborhoods\\nParents in 45th to 60th centiles\\nFigure 6.5. Unobserved confounds and col-\\nlider bias. In this example, grandparents influ-\\nence grandkids only indirectly, through par-\\nents. However, unobserved neighborhood ef-\\nfects on parents and their children create the\\nillusion that grandparents harm their grand-\\nkids education. Parental education is a col-\\nlider: Once we condition on it, grandparental\\neducation becomes negatively associated with\\ngrandchild education.\\n(U = 1). The black cloud comprises children who live in bad neighborhoods (U = −1). No-\\ntice that both clouds of points show positive associations between G and C. More educated\\ngrandparents have more educated grandkids, but this effect arises entirely through parents.\\nWhy? Because we assumed it is so. The direct effect of G in the simulation is zero.\\nSo how does the negative association arise, when we condition on parents? Conditioning\\non parents is like looking within sub-populations of parents with similar education. So let’s\\ntry that. In Figure 6.5, I’ve highlighted in filled points those parents between the 45th and\\n60th centiles of education. There is nothing special of this range. It just makes the phenom-\\nenon easier to see. Now if we draw a regression line through only these points, regressing\\nC on G, the slope is negative. There is the negative association that our multiple regression\\nfinds. But why does it exist?\\nIt exists because, once we know P, learning G invisibly tells us about the neighborhood\\nU, and U is associated with the outcome C. I know this is confusing. As I keep saying, if you\\nare confused, it is only because you are paying attention. So consider two different parents\\nwith the same education level, say for example at the median 50th centile. One of these\\nparents has a highly educated grandparent. The other has a poorly educated grandparent.\\nThe only probable way, in this example, for these parents to have the same education is if\\nthey live in different types of neighborhoods. We can’t see these neighborhood effects—we\\nhaven’t measured them, recall—but the influence of neighborhood is still transmitted to the\\nchildren C. So for our mythical two parents with the same education, the one with the highly\\neducated grandparent ends up with a less well educated child. The one with the less educated\\ngrandparent ends up with the better educated child. G predicts lower C.\\nThe unmeasured U makes P a collider, and conditioning on P produces collider bias. So\\nwhat can we do about this? You have to measure U. Here’s the regression that conditions\\nalso on U:\\nR code\\n6.28\\nm6.12 <- quap(\\nalist(\\nC ~ dnorm( mu , sigma ),\\nmu <- a + b_PC*P + b_GC*G + b_U*U,\\na ~ dnorm( 0 , 1 ),\\n'},\n",
       " {'index': 201,\n",
       "  'number': 183,\n",
       "  'content': '6.4. CONFRONTING CONFOUNDING\\n183\\nc(b_PC,b_GC,b_U) ~ dnorm( 0 , 1 ),\\nsigma ~ dexp( 1 )\\n), data=d )\\nprecis(m6.12)\\nmean\\nsd\\n5.5% 94.5%\\na\\n-0.12 0.07 -0.24 -0.01\\nb_PC\\n1.01 0.07\\n0.91\\n1.12\\nb_GC\\n-0.04 0.10 -0.20\\n0.11\\nb_U\\n2.00 0.15\\n1.76\\n2.23\\nsigma\\n1.02 0.05\\n0.94\\n1.10\\nAnd those are the slopes we simulated with.\\nRethinking: Statistical paradoxes and causal explanations. The grandparents example serves as\\nan example of Simpson’s paradox: Including another predictor (P in this case) can reverse the\\ndirection of association between some other predictor (G) and the outcome (C). Usually, Simpson’s\\nparadox is presented in cases where adding the new predictor helps us. But in this case, it misleads\\nus. Simpson’s paradox is a statistical phenomenon. To know whether the reversal of the association\\ncorrectly reflects causation, we need something more than just a statistical model.94\\n6.4. Confronting confounding\\nIn this chapter and in the previous one, there have been several examples of how we can\\nuse multiple regression to deal with confounding. But we have also seen how multiple regres-\\nsion can cause confounding—controlling for the wrong variables ruins inference. Hopefully\\nI have succeeded in scaring you away from just adding everything to a model and hoping re-\\ngression will sort it out, as well as inspired you to believe that effective inference is possible,\\nif we are careful enough and knowledgable enough.\\nBut which principles explain why sometimes leaving out variables and sometimes adding\\nthem can produce the same phenomenon? Are there other causal monsters lurking out there,\\nhaunting our graphs? We need some principles to pull these examples together.\\nLet’s define confounding as any context in which the association between an outcome\\nY and a predictor of interest X is not the same as it would be, if we had experimentally deter-\\nmined the values of X.95 For example, suppose we are interested in the association between\\neducation E and wages W. The problem is that in a typical population there are many un-\\nobserved variables U that influence both E and W. Examples include where a person lives,\\nwho their parents are, and who their friends are. This is what the DAG looks like:\\nE\\nU\\nW\\nIf we regress W on E, the estimate of the causal effect will be confounded by U. It is con-\\nfounded, because there are two paths connecting E and W: (1) E →W and (2) E ←U →W.\\nA “path” here just means any series of variables you could walk through to get from one vari-\\nable to another, ignoring the directions of the arrows. Both of these paths create a statistical\\nassociation between E and W. But only the first path is causal. The second path is non-causal.\\n'},\n",
       " {'index': 202,\n",
       "  'number': 184,\n",
       "  'content': '184\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\nWhy? Because if only the second path existed, and we changed E, it would not change W.\\nAny causal influence of E on W operates only on the first path.\\nHow can we isolate the causal path? The most famous solution is to run an experiment.\\nIf we could assign education levels at random, it changes the graph:\\nE\\nU\\nW\\nManipulation removes the influence of U on E. The unobserved variables do not influence\\neducation when we ourselves determine education. With the influence of U removed from\\nE, this then removes the path E ←U →W. It blocks the second path. Once the path is\\nblocked, there is only one way for information to go between E and W, and then measuring\\nthe association between E and W would yield a useful measure of causal influence. Manipu-\\nlation removes the confounding, because it blocks the other path between E and W.\\nLuckily, there are statistical ways to achieve the same result, without actually manipulat-\\ning E. How? The most obvious is to add U to the model, to condition on U. Why does this\\nalso remove the confounding? Because it also blocks the flow of information between E and\\nW through U. It blocks the second path.\\nTo understand why conditioning on U blocks the path E ←U →W, think of this path in\\nisolation, as a complete model. Once you learn U, also learning E will give you no additional\\ninformation about W. Suppose for example that U is the average wealth in a region. Regions\\nwith high wealth have better schools, resulting in more education E, as well as better paying\\njobs, resulting in higher wages W. If you don’t know the region a person lives in, learning\\nthe person’s education E will provide information about their wages W, because E and W are\\ncorrelated across regions. But after you learn which region a person lives in, assuming there\\nis no other path between E and W, then learning E tells you nothing more about W. This\\nis the sense in which conditioning on U blocks the path—it makes E and W independent,\\nconditional on U.\\n6.4.1. Shutting the backdoor. Blocking confounding paths between some predictor X and\\nsome outcome Y is known as shutting the backdoor. We don’t want any spurious associ-\\nation sneaking in through a non-causal path that enters the back of the predictor X. In the\\nexample above, the path E ←U →W is a backdoor path, because it enters E with an arrow\\nand also connects E to W. This path is non-causal—intervening on E will not cause a change\\nin W through this path—but it still produces an association between E and W.\\nNow for some good news. Given a causal DAG, it is always possible to say which, if any,\\nvariables one must control for in order to shut all the backdoor paths. It is also possible to say\\nwhich variables one must not control for, in order to avoid making new confounds. And—\\nsome more good news—there are only four types of variable relations that combine to form\\nall possible paths. So you really only need to understand four things and how information\\nflows in each of them. I’ll define the four types of relations. Then we’ll work some examples.\\nFigure 6.6 shows DAGs for each elemental relation. Every DAG, no matter how big and\\ncomplicated, is built out of these four relations. Let’s consider each, going left to right.\\n(1) The first type of relation is the one we worked with just above, a fork: X ←Z →Y.\\nThis is the classic confounder. In a fork, some variable Z is a common cause of X\\n'},\n",
       " {'index': 203,\n",
       "  'number': 185,\n",
       "  'content': '6.4. CONFRONTING CONFOUNDING\\n185\\nThe Fork\\nThe Pipe\\nThe Collider\\nThe Descendant\\nX\\nY\\nZ\\nX\\nY\\nZ\\nX\\nY\\nZ\\nD\\nX\\nY\\nZ\\nFigure 6.6. The four elemental confounds. Any directed acyclic graph is\\nbuilt from these elementary relationships. From left to right: X ⊥⊥Y|Z in\\nboth the Fork and the Pipe, X ̸⊥⊥Y|Z in the Collider, and conditioning on\\nthe Descendent D is like conditioning on its parent Z.\\nand Y, generating a correlation between them. If we condition on Z, then learning\\nX tells us nothing about Y. X and Y are independent, conditional on Z.\\n(2) The second type of relation is a pipe: X →Z →Y. We saw this when we discussed\\nthe plant growth example and post-treatment bias: The treatment X influences fun-\\ngus Z which influences growth Y. If we condition on Z now, we also block the path\\nfrom X to Y. So in both a fork and a pipe, conditioning of the middle variable\\nblocks the path.\\n(3) The third type of relation is a collider: X →Z ←Y. You met colliders earlier\\nin this chapter. Unlike the other two types of relations, in a collider there is no\\nassociation between X and Y unless you condition on Z. Conditioning on Z, the\\ncollider variable, opens the path. Once the path is open, information flows between\\nX and Y. However neither X nor Y has any causal influence on the other.\\n(4) The fourth relation is the descendent. A descendent is a variable influenced by\\nanother variable. Conditioning on a descendent partly conditions on its parent. In\\nthe far right DAG in Figure 6.6, conditioning on D will also condition, to a lesser\\nextent, on Z. The reason is that D has some information about Z. In this example,\\nthis will partially open the path from X to Y, because Z is a collider. But in general\\nthe consequence of conditioning on a descendent depends upon the nature of its\\nparent. Descendants are common, because often we cannot measure a variable\\ndirectly and instead have only some proxy for it.\\nNo matter how complicated a causal DAG appears, it is always built out of these four\\ntypes of relations. And since you know how to open and close each, you (or your computer)\\ncan figure out which variables you need to include or not include. Here’s the recipe:\\n(1) List all of the paths connecting X (the potential cause of interest) and Y (the out-\\ncome).\\n(2) Classify each path by whether it is open or closed. A path is open unless it contains\\na collider.\\n(3) Classify each path by whether it is a backdoor path. A backdoor path has an arrow\\nentering X.\\n(4) If there are any open backdoor paths, decide which variable(s) to condition on to\\nclose it (if possible).\\nLet’s consider some examples.\\n'},\n",
       " {'index': 204,\n",
       "  'number': 186,\n",
       "  'content': '186\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\n6.4.2. Two roads. The DAG below contains an exposure of interest X, an outcome of interest\\nY, an unobserved variable U, and three observed covariates (A, B, and C).\\nA\\nB\\nC\\nU\\nX\\nY\\nWe are interested in the X →Y path, the causal effect of X on Y. Which of the observed\\ncovariates do we need to add to the model, in order to correctly infer it? To figure this out,\\nlook for backdoor paths. Aside from the direct path, there are two paths from X to Y:\\n(1) X ←U ←A →C →Y\\n(2) X ←U →B ←C →Y\\nThese are both backdoor paths that could confound inference. Now ask which of these paths\\nis open. If a backdoor path is open, then we must close it. If a backdoor path is closed already,\\nthen we must not accidentally open it and create a confound.\\nConsider the first path, passing through A. This path is open, because there is no collider\\nwithin it. There is just a fork at the top and two pipes, one on each side. Information will\\nflow through this path, confounding X →Y. It is a backdoor. To shut this backdoor, we\\nneed to condition on one of its variables. We can’t condition on U, since it is unobserved.\\nThat leaves A or C. Either will shut the backdoor. You can ask your computer to reproduce\\nthis analysis, to analyze the graph and find the necessary variables to control for in order to\\nblock the backdoor. The dagitty R package provides adjustmentSets for this purpose:\\nR code\\n6.29\\nlibrary(dagitty)\\ndag_6.1 <- dagitty( \"dag {\\nU [unobserved]\\nX -> Y\\nX <- U <- A -> C -> Y\\nU -> B <- C\\n}\")\\nadjustmentSets( dag_6.1 , exposure=\"X\" , outcome=\"Y\" )\\n{ C } { A }\\nConditioning on either C or A would suffice. Conditioning on C is the better idea, from\\nthe perspective of efficiency, since it could also help with the precision of the estimate of\\nX →Y. Notice that conditioning on U would also work. But since we told dagitty that U\\nis unobserved (see the code above), it didn’t suggest it in the adjustment sets.\\nNow consider the second path, passing through B. This path does contain a collider,\\nU →B ←C. It is therefore already closed. That is why adjustmentSets above did not\\nmention B. In fact, if we do condition on B, it will open the path, creating a confound. Then\\nour inference about X →Y will change, but without the DAG, we won’t know whether that\\nchange is helping us or rather misleading us. The fact that including a variable changes the\\nX →Y coefficient does not always mean that the coefficient is better now. You could have\\njust conditioned on a collider.\\n'},\n",
       " {'index': 205,\n",
       "  'number': 187,\n",
       "  'content': '6.4. CONFRONTING CONFOUNDING\\n187\\n6.4.3. Backdoor waffles. As a final example, let’s return to the Waffle House and divorce rate\\ncorrelation from the introduction to Chapter 5. We’ll make a DAG, use it to find a minimal\\nset of covariates, and use it as well to derive the testable implications of the DAG. This is\\nimportant, because sometimes you really can test whether your DAG is consistent with the\\nevidence. The data alone can never tell us when a DAG is right. But the data can tell us when\\na DAG is wrong.\\nWe’re interested in the total causal effect of the number of Waffle Houses on divorce rate\\nin each State. Presumably, the naive correlation between these two variables is spurious.\\nWhat is the minimal adjustment set that will block backdoor paths from Waffle House to\\ndivorce? Let’s make a graph:\\nA\\nD\\nM\\nS\\nW\\nIn this graph, S is whether or not a State is in the southern United States, A is median age\\nat marriage, M is marriage rate, W is number of Waffle Houses, and D is divorce rate. This\\ngraph assumes that southern States have lower ages of marriage (S →A), higher rates of\\nmarriage both directly (S →M) and mediated through age of marriage (S →A →M), as\\nwell as more waffles (S →W). Age of marriage and marriage rate both influence divorce.\\nThere are three open backdoor paths between W and D. Just trace backwards, starting at\\nW and ending up at D. But notice that all of them pass first through S. So we can close them\\nall by conditioning on S. That’s all there is to it. Your computer can confirm this answer:\\nR code\\n6.30\\nlibrary(dagitty)\\ndag_6.2 <- dagitty( \"dag {\\nA -> D\\nA -> M -> D\\nA <- S -> M\\nS -> W -> D\\n}\")\\nadjustmentSets( dag_6.2 , exposure=\"W\" , outcome=\"D\" )\\n{ A, M } { S }\\nWe could control for either A and M or for S alone.\\nThis DAG is obviously not satisfactory—it assumes there are no unobserved confounds,\\nwhich is very unlikely for this sort of data. But we can still learn something by analyzing\\nit. While the data cannot tell us whether a graph is correct, it can sometimes suggest how a\\ngraph is wrong. Earlier, we discussed conditional independencies, which are some of a\\nmodel’s testable implications. Conditional independencies are pairs of variables that are not\\nassociated, once we condition on some set of other variables. By inspecting these implied\\nconditional independencies, we can at least test some of the features of a graph.\\nNow that you know the elemental confounds, you are ready to derive any DAG’s con-\\nditional independencies on your own. You can find conditional independencies using the\\nsame path logic you learned for finding and closing backdoors. You just have to focus on a\\n'},\n",
       " {'index': 206,\n",
       "  'number': 188,\n",
       "  'content': '188\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\npair of variables, find all paths connecting them, and figure out if there is any set of variables\\nyou could condition on to close them all. In a large graph, this is quite a chore, because there\\nare many pairs of variables and possibly many paths. But your computer is good at such\\nchores. In this case, there are three implied conditional independencies:\\nR code\\n6.31\\nimpliedConditionalIndependencies( dag_6.2 )\\nA _||_ W | S\\nD _||_ S | A, M, W\\nM _||_ W | S\\nRead the first as “median age of marriage should be independent of (_||_) Waffle Houses,\\nconditioning on (|) a State being in the south.” In the second, divorce and being in the\\nsouth should be independent when we simultaneously condition on all of median age of\\nmarriage, marriage rate, and Waffle Houses. Finally, marriage rate and Waffle Houses should\\nbe independent, conditioning on being in the south.\\nIn the practice problems at the end of this chapter, I’ll ask you to evaluate these implica-\\ntions, as well as try to assess the causal influence of Waffle Houses on divorce.\\nRethinking: DAGs are not enough. If you don’t have a real, mechanistic model of your system, DAGs\\nare fantastic tools. They make assumptions transparent and easier to critique. And if nothing else,\\nthey highlight the danger of using multiple regression as a substitute for theory. But DAGs are not a\\ndestination. Once you have a dynamical model of your system, you don’t need a DAG. In fact, many\\ndynamical systems have complex behavior that is sensitive to initial conditions, and so cannot be use-\\nfully represented by DAGs.96 But these models can still be analyzed and causal interventions designed\\nfrom them. In fact, domain specific structural causal models can make causal inference possible even\\nwhen a DAG with the same structure cannot decide how to proceed. Additional assumptions, when\\naccurate, give us power.\\nThe fact that DAGs are not useful for everything is no argument against them. All theory tools\\nhave limitations. I have yet to see a better tool than DAGs for teaching the foundations of and obstacles\\nto causal inference. And general tools like DAGs have added value in abstracting away from specific\\ndetails and teaching us general principles. For example, DAGs clarify why experiments work and\\nhighlight threats to experiments like differential measurement error (Chapter 15).\\nOverthinking: A smooth operator. To define confounding with precise notation, we need to adopt\\nsomething called the do-operator.97 Confounding occurs when:\\nPr(Y|X) ̸= Pr(Y|do(X))\\nThat do(X) means to cut all of the backdoor paths into X, as if we did a manipulative experiment. The\\ndo-operator changes the graph, closing the backdoors. The do-operator defines a causal relationship,\\nbecause Pr(Y|do(X)) tells us the expected result of manipulating X on Y, given a causal graph. We\\nmight say that some variable X is a cause of Y when Pr(Y|do(X)) ̸= Pr(Y|do(not-X)). The ordinary\\nconditional probability comparison, Pr(Y|X) ̸= Pr(Y|not-X), is not the same. It does not close the\\nbackdoor. Note that what the do-operator gives you is not just the direct causal effect. It is the total\\ncausal effect through all forward paths. To get a direct causal effect, you might have to close more\\ndoors. The do-operator can also be used to derive causal inference strategies even when some back\\ndoors cannot be closed. We’ll look at one example in a later chapter.\\n'},\n",
       " {'index': 207,\n",
       "  'number': 189,\n",
       "  'content': '6.6. PRACTICE\\n189\\n6.5. Summary\\nMultiple regression is no oracle, but only a golem. It is logical, but the relationships it de-\\nscribes are conditional associations, not causal influences. Therefore additional information,\\nfrom outside the model, is needed to make sense of it. This chapter presented introductory\\nexamples of some common frustrations: multicollinearity, post-treatment bias, and collider\\nbias. Solutions to these frustrations can be organized under a coherent framework in which\\nhypothetical causal relations among variables are analyzed to cope with confounding. In all\\ncases, causal models exist outside the statistical model and can be difficult to test. However,\\nit is possible to reach valid causal inferences in the absence of experiments. This is good\\nnews, because we often cannot perform experiments, both for practical and ethical reasons.\\n6.6. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n6E1. List three mechanisms by which multiple regression can produce false inferences about causal\\neffects.\\n6E2. For one of the mechanisms in the previous problem, provide an example of your choice, perhaps\\nfrom your own research.\\n6E3. List the four elemental confounds. Can you explain the conditional dependencies of each?\\n6E4. How is a biased sample like conditioning on a collider? Think of the example at the open of the\\nchapter.\\n6M1. Modify the DAG on page 186 to include the variable V, an unobserved cause of C and Y:\\nC ←V →Y. Reanalyze the DAG. How many paths connect X to Y? Which must be closed? Which\\nvariables should you condition on now?\\n6M2. Sometimes, in order to avoid multicollinearity, people inspect pairwise correlations among\\npredictors before including them in a model. This is a bad procedure, because what matters is the\\nconditional association, not the association before the variables are included in the model. To high-\\nlight this, consider the DAG X →Z →Y. Simulate data from this DAG so that the correlation\\nbetween X and Z is very large. Then include both in a model prediction Y. Do you observe any\\nmulticollinearity? Why or why not? What is different from the legs example in the chapter?\\n6M3. Learning to analyze DAGs requires practice. For each of the four DAGs below, state which\\nvariables, if any, you must adjust for (condition on) to estimate the total causal influence of X on Y.\\nA\\nX\\nY\\nZ\\nA\\nX\\nY\\nZ\\nA\\nX\\nY\\nZ\\nA\\nX\\nY\\nZ\\n6H1. Use the Waffle House data, data(WaffleDivorce), to find the total causal influence of num-\\nber of Waffle Houses on divorce rate. Justify your model or models with a causal graph.\\n'},\n",
       " {'index': 208,\n",
       "  'number': 190,\n",
       "  'content': '190\\n6. THE HAUNTED DAG & THE CAUSAL TERROR\\n6H2. Build a series of models to test the implied conditional independencies of the causal graph\\nyou used in the previous problem. If any of the tests fail, how do you think the graph needs to be\\namended? Does the graph need more or fewer arrows? Feel free to nominate variables that aren’t in\\nthe data.\\nAll three problems below are based on the same data. The data in data(foxes) are 116 foxes from\\n30 different urban groups in England. These foxes are like street gangs. Group size varies from 2 to\\n8 individuals. Each group maintains its own urban territory. Some territories are larger than others.\\nThe area variable encodes this information. Some territories also have more avgfood than others.\\nWe want to model the weight of each fox. For the problems below, assume the following DAG:\\narea\\navgfood\\ngroupsize\\nweight\\n6H3. Use a model to infer the total causal influence of area on weight. Would increasing the area\\navailable to each fox make it heavier (healthier)? You might want to standardize the variables. Re-\\ngardless, use prior predictive simulation to show that your model’s prior predictions stay within the\\npossible outcome range.\\n6H4. Now infer the causal impact of adding food to a territory. Would this make foxes heavier?\\nWhich covariates do you need to adjust for to estimate the total causal influence of food?\\n6H5. Now infer the causal impact of group size. Which covariates do you need to adjust for? Looking\\nat the posterior distribution of the resulting model, what do you think explains these data? That is,\\ncan you explain the estimates for all three problems? How do they go together?\\n6H6. Consider your own research question. Draw a DAG to represent it. What are the testable\\nimplications of your DAG? Are there any variables you could condition on to close all backdoor\\npaths? Are there unobserved variables that you have omitted? Would a reasonable colleague imagine\\nadditional threats to causal inference that you have ignored?\\n6H7. For the DAG you made in the previous problem, can you write a data generating simulation for\\nit? Can you design one or more statistical models to produce causal estimates? If so, try to calculate\\ninteresting counterfactuals. If not, use the simulation to estimate the size of the bias you might expect.\\nUnder what conditions would you, for example, infer the opposite of a true causal effect?\\n'},\n",
       " {'index': 209,\n",
       "  'number': 191,\n",
       "  'content': '7 Ulysses’ Compass\\nMikołaj Kopernik (also known as Nicolaus Copernicus, 1473–1543): Polish astronomer,\\necclesiastical lawyer, and blasphemer. Famous for his heliocentric model of the solar sys-\\ntem, Kopernik argued for replacing the geocentric model, because the heliocentric model\\nwas more “harmonious.” This position eventually lead (decades later) to Galileo’s famous\\ndisharmony with, and trial by, the Church.\\nThis story has become a fable of science’s triumph over ideology and superstition. But\\nKopernik’s justification looks poor to us now, ideology aside. There are two problems: The\\nmodel was neither particularly harmonious nor more accurate than the geocentric model.\\nThe Copernican model was very complicated. In fact, it had similar epicycle clutter as the\\nPtolemaic model (Figure 7.1). Kopernik had moved the Sun to the center, but since he still\\nused perfect circles for orbits, he still needed epicycles. And so “harmony” doesn’t quite\\ndescribe the model’s appearance. Just like the Ptolemaic model, the Kopernikan model was\\neffectively a Fourier series, a means of approximating periodic functions. This leads to the\\nsecond problem: The heliocentric model made exactly the same predictions as the geocentric\\nmodel. Equivalent approximations can be constructed whether the Earth is stationary or\\nrather moving. So there was no reason to prefer it on the basis of accuracy alone.\\nKopernik didn’t appeal just to some vague harmony, though. He also argued for the\\nsuperiority of his model on the basis of needing fewer causes: “We thus follow Nature, who\\nproducing nothing in vain or superfluous often prefers to endow one cause with many ef-\\nfects.”98 And it was true that a heliocentric model required fewer circles and epicycles to\\nmake the same predictions as a geocentric model. In this sense, it was simpler.\\nScholars often prefer simpler theories. This preference is sometimes vague—a kind of\\naesthetic preference. Other times we retreat to pragmatism, preferring simpler theories be-\\ncause their simpler models are easier to work with. Frequently, scientists cite a loose princi-\\nple known as Ockham’s razor: Models with fewer assumptions are to be preferred. In the\\ncase of Kopernik and Ptolemy, the razor makes a clear recommendation. It cannot guarantee\\nthat Kopernik was right (he wasn’t, after all), but since the heliocentric and geocentric mod-\\nels make the same predictions, at least the razor offers a clear resolution to the dilemma. But\\nthe razor can be hard to use more generally, because usually we must choose among models\\nthat differ in both their accuracy and their simplicity. How are we to trade these different\\ncriteria against one another? The razor offers no guidance.\\nThis chapter describes some of the most commonly used tools for coping with this trade-\\noff. Some notion of simplicity usually features in all of these tools, and so each is commonly\\ncompared to Ockham’s razor. But each tool is equally about improving predictive accuracy.\\nSo they are not like the razor, because they explicitly trade-off accuracy and simplicity.\\n191\\n'},\n",
       " {'index': 210,\n",
       "  'number': 192,\n",
       "  'content': '192\\n7. ULYSSES’ COMPASS\\nPtolemaic Model\\nCopernican Model\\nEarth\\nEarth\\nSun\\nSun\\nFigure 7.1. Ptolemaic (left) and Copernican (right) models of the solar\\nsystem. Both models use epicycles (circles on circles), and both models\\nproduce exactly the same predictions. However, the Copernican model re-\\nquires fewer circles. (Not all Ptolemaic epicycles are visible in the figure.)\\nSo instead of Ockham’s razor, think of Ulysses’ compass. Ulysses was the hero of Homer’s\\nOdyssey. During his voyage, Ulysses had to navigate a narrow straight between the many-\\nheaded beast Scylla—who attacked from a cliff face and gobbled up sailors—and the sea\\nmonster Charybdis—who pulled boats and men down to a watery grave. Passing too close\\nto either meant disaster. In the context of scientific models, you can think of these monsters\\nas representing two fundamental kinds of statistical error:\\n(1) The many-headed beast of overfitting, which leads to poor prediction by learn-\\ning too much from the data\\n(2) The whirlpool of underfitting, which leads to poor prediction by learning too\\nlittle from the data\\nThere is a third monster, the one you met in previous chapters—confounding. In this\\nchapter you’ll see that confounded models can in fact produce better predictions than models\\nthat correctly measure a causal relationship. The consequence of this is that, when we design\\nany particular statistical model, we must decide whether we want to understand causes or\\nrather just predict. These are not the same goal, and different models are needed for each.\\nHowever, to accurately measure a causal influence, we still have to deal with overfitting. The\\nmonsters of overfitting and underfitting are always lurking, no matter the goal.\\nOur job is to carefully navigate among these monsters. There are two common families\\nof approaches. The first approach is to use a regularizing prior to tell the model not to\\nget too excited by the data. This is the same device that non-Bayesian methods refer to as\\n“penalized likelihood.” The second approach is to use some scoring device, like informa-\\ntion criteria or cross-validation, to model the prediction task and estimate predictive\\naccuracy. Both families of approaches are routinely used in the natural and social sciences.\\nFurthermore, they can be—maybe should be—used in combination. So it’s worth under-\\nstanding both, as you’re going to need both at some point.\\n'},\n",
       " {'index': 211,\n",
       "  'number': 193,\n",
       "  'content': '7.1. THE PROBLEM WITH PARAMETERS\\n193\\nIn order to introduce information criteria, this chapter must also introduce informa-\\ntion theory. If this is your first encounter with information theory, it’ll probably seem\\nstrange. But some understanding of it is needed. Once you start using information criteria—\\nthis chapter describes AIC, DIC, WAIC, and PSIS—you’ll find that implementing them is\\nmuch easier than understanding them. This is their curse. So most of this chapter aims to\\nfight the curse, focusing on their conceptual foundations, with applications to follow.\\nIt’s worth noting, before getting started, that this material is hard. If you find yourself\\nconfused at any point, you are normal. Any sense of confusion you feel is just your brain cor-\\nrectly calibrating to the subject matter. Over time, confusion is replaced by comprehension\\nfor how overfitting, regularization, and information criteria behave in familiar contexts.\\nRethinking: Stargazing. The most common form of model selection among practicing scientists is\\nto search for a model in which every coefficient is statistically significant. Statisticians sometimes call\\nthis stargazing, as it is embodied by scanning for asterisks (**) trailing after estimates. A colleague\\nof mine once called this approach the “Space Odyssey,” in honor of A. C. Clarke’s novel and film. The\\nmodel that is full of stars, the thinking goes, is best.\\nBut such a model is not best. Whatever you think about null hypothesis significance testing in\\ngeneral, using it to select among structurally different models is a mistake—p-values are not designed\\nto help you navigate between underfitting and overfitting. As you’ll see once you start using AIC and\\nrelated measures, predictor variables that improve prediction are not always statistically significant. It\\nis also possible for variables that are statistically significant to do nothing useful for prediction. Since\\nthe conventional 5% threshold is purely conventional, we shouldn’t expect it to optimize anything.\\nRethinking: Is AIC Bayesian? AIC is not usually thought of as a Bayesian tool. There are both his-\\ntorical and statistical reasons for this. Historically, AIC was originally derived without reference to\\nBayesian probability. Statistically, AIC uses MAP estimates instead of the entire posterior, and it re-\\nquires flat priors. So it doesn’t look particularly Bayesian. Reinforcing this impression is the existence\\nof another model comparison metric, the Bayesian information criterion (BIC). However, BIC\\nalso requires flat priors and MAP estimates, although it’s not actually an “information criterion.”\\nRegardless, AIC has a clear and pragmatic interpretation under Bayesian probability, and Akaike\\nand others have long argued for alternative Bayesian justifications of the procedure.99 And as you’ll\\nsee later in the book, more obviously Bayesian information criteria like WAIC provide almost exactly\\nthe same results as AIC, when AIC’s assumptions are met. In this light, we can fairly regard AIC as\\na special limit of a Bayesian criterion like WAIC, even if that isn’t how AIC was originally derived.\\nAll of this is an example of a common feature of statistical procedures: The same procedure can be\\nderived and justified from multiple, sometimes philosophically incompatible, perspectives.\\n7.1. The problem with parameters\\nIn the previous chapters, we saw how adding variables and parameters to a model can\\nhelp to reveal hidden effects and improve estimates. You also saw that adding variables can\\nhurt, in particular when we lack a trusted causal model. Colliders are real. But sometimes we\\ndon’t care about causal inference. Maybe we just want to make good predictions. Consider\\nfor example the grandparent-parent-child example from the previous chapter. Just adding\\nall the variables to the model will give us a good predictive model in that case. That we don’t\\nunderstand what is going on is irrelevant. So is just adding everything to the model okay?\\nThe answer is “no.” There are two related problems with just adding variables. The first\\nis that adding parameters—making the model more complex—nearly always improves the\\n'},\n",
       " {'index': 212,\n",
       "  'number': 194,\n",
       "  'content': '194\\n7. ULYSSES’ COMPASS\\nfit of a model to the data.100 By “fit” I mean a measure of how well the model can retrodict\\nthe data used to fit the model. There are many such measures, each with its own foibles. In\\nthe context of linear Gaussian models, R2 is the most common measure of this kind. Often\\ndescribed as “variance explained,” R2 is defined as:\\nR2 = var(outcome) −var(residuals)\\nvar(outcome)\\n= 1 −var(residuals)\\nvar(outcome)\\nBeing easy to compute, R2 is popular. Like other measures of fit to sample, R2 increases as\\nmore predictor variables are added. This is true even when the variables you add to a model\\nare just random numbers, with no relation to the outcome. So it’s no good to choose among\\nmodels using only fit to the data.\\nSecond, while more complex models fit the data better, they often predict new data\\nworse. Models that have many parameters tend to overfit more than simpler models. This\\nmeans that a complex model will be very sensitive to the exact sample used to fit it, leading\\nto potentially large mistakes when future data is not exactly like the past data. But simple\\nmodels, with too few parameters, tend instead to underfit, systematically over-predicting or\\nunder-predicting the data, regardless of how well future data resemble past data. So we can’t\\nalways favor either simple models or complex models.\\nLet’s examine both of these issues in the context of a simple example.\\n7.1.1. More parameters (almost) always improve fit. Overfitting occurs when a model\\nlearns too much from the sample. What this means is that there are both regular and irregular\\nfeatures in every sample. The regular features are the targets of our learning, because they\\ngeneralize well or answer a question of interest. Regular features are useful, given an objective\\nof our choice. The irregular features are instead aspects of the data that do not generalize and\\nso may mislead us.\\nOverfitting happens automatically, unfortunately. In the kind of statistical models we’ve\\nseen so far in this book, adding additional parameters will always improve the fit of a model\\nto the sample. Later in the book, beginning with Chapter 13, you’ll meet models for which\\nadding parameters does not necessarily improve fit to the sample, but may well improve\\npredictive accuracy.\\nHere’s an example of overfitting. The data displayed in Figure 7.2 are average brain\\nvolumes and body masses for seven hominin species.101 Let’s get these data into R, so you\\ncan work with them. I’m going to build these data from direct input, rather than loading a\\npre-made data frame, just so you see an example of how to build a data frame from scratch.\\nR code\\n7.1\\nsppnames <- c( \"afarensis\",\"africanus\",\"habilis\",\"boisei\",\\n\"rudolfensis\",\"ergaster\",\"sapiens\")\\nbrainvolcc <- c( 438 , 452 , 612, 521, 752, 871, 1350 )\\nmasskg <- c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )\\nd <- data.frame( species=sppnames , brain=brainvolcc , mass=masskg )\\nNow you have a data frame, d, containing the brain size and body size values. It’s not un-\\nusual for data like this to be highly correlated—brain size is correlated with body size, across\\nspecies. A standing question, however, is to what extent particular species have brains that\\nare larger than we’d expect, after taking body size into account. A common solution is to fit a\\nlinear regression that models brain size as a linear function of body size. Then the remaining\\n'},\n",
       " {'index': 213,\n",
       "  'number': 195,\n",
       "  'content': '7.1. THE PROBLEM WITH PARAMETERS\\n195\\n30\\n40\\n50\\n60\\n70\\n600\\n800\\n1000\\n1200\\nbody mass (kg)\\nbrain volume (cc)\\nafarensis\\nafricanus\\nhabilis\\nboisei\\nrudolfensis\\nergaster\\nsapiens\\nFigure 7.2. Average brain volume in cubic\\ncentimeters against body mass in kilograms,\\nfor six hominin species. What model best de-\\nscribes the relationship between brain size and\\nbody size?\\nvariation in brain size can be modeled as a function of other variables, like ecology or diet.\\nThis is the same “statistical control” strategy explained in previous chapters.\\nControlling for body size, however, depends upon having a good functional mapping\\nof the association between body size and brain size. We’ve just used linear functions so far.\\nBut why use a line to relate body size to brain size? It’s not clear why nature demands that\\nthe relationship among species be a straight line. Why not consider a curved model, like a\\nparabola? Indeed, why not a cubic function of body size, or even a spline? There’s no reason\\nto suppose a priori that brain size scales only linearly with body size. Indeed, many readers\\nwill prefer to model a linear relationship between log brain volume and log body mass (an\\nexponential relationship). But that’s not the direction I’m headed with this example. The\\nlesson here will arise, no matter how we transform the data.\\nLet’s fit a series of increasingly complex model families and see which function fits the\\ndata best. We’ll use polynomial regressions, so review Section 4.5 (page 110) if necessary.\\nImportantly, recall that polynomial regressions are common, but usually a bad idea. In this\\nexample, I will show you that they can be a very bad idea when used blindly. But the splines\\nfrom Chapter 4 will suffer the same basic problem. In the practice problems at the end of the\\nchapter, you will return to this example and try it with splines.\\nThe simplest model that relates brain size to body size is the linear one. It will be the\\nfirst model we consider. Before writing out the model, let’s rescale the variables. Recall from\\nearlier chapters that rescaling predictor and outcome variables is often helpful in getting the\\nmodel to fit and in specifying and understanding the priors. In this case, we want to stan-\\ndardize body mass—give it mean zero and standard deviation one—and rescale the outcome,\\nbrain volume, so that the largest observed value is 1. Why not standardize brain volume as\\nwell? Because we want to preserve zero as a reference point: No brain at all. You can’t have\\nnegative brain. I don’t think.\\nR code\\n7.2\\nd$mass_std <- (d$mass - mean(d$mass))/sd(d$mass)\\nd$brain_std <- d$brain / max(d$brain)\\n'},\n",
       " {'index': 214,\n",
       "  'number': 196,\n",
       "  'content': '196\\n7. ULYSSES’ COMPASS\\nNow here’s the mathematical version of the first linear model. The only trick to note is\\nthe log-normal prior on σ. This will make it easier to keep σ positive, as it should be.\\nbi ∼Normal(µi, σ)\\nµi = α + βmi\\nα ∼Normal(0.5, 1)\\nβ ∼Normal(0, 10)\\nσ ∼Log-Normal(0, 1)\\nThis simply says that the average brain volume bi of species i is a linear function of its body\\nmass mi. Now consider what the priors imply. The prior for α is just centered on the mean\\nbrain volume (rescaled) in the data. So it says that the average species with an average body\\nmass has a brain volume with an 89% credible interval from about −1 to 2. That is ridicu-\\nlously wide and includes impossible (negative) values. The prior for β is very flat and cen-\\ntered on zero. It allows for absurdly large positive and negative relationships. These priors\\nallow for absurd inferences, especially as the model gets more complex. And that’s part of\\nthe lesson, so let’s continue to fit the model now:\\nR code\\n7.3\\nm7.1 <- quap(\\nalist(\\nbrain_std ~ dnorm( mu , exp(log_sigma) ),\\nmu <- a + b*mass_std,\\na ~ dnorm( 0.5 , 1 ),\\nb ~ dnorm( 0 , 10 ),\\nlog_sigma ~ dnorm( 0 , 1 )\\n), data=d )\\nI’ve used exp(log_sigma) in the likelihood, so that the result is always greater than zero.\\nRethinking: OLS and Bayesian anti-essentialism. It would be possible to use ordinary least\\nsquares (OLS) to get posterior distributions for these brain size models. For example, you could use\\nR’s simple lm function to get the posterior distribution for m6.1. You won’t get a posterior for sigma\\nhowever.\\nR code\\n7.4\\nm7.1_OLS <- lm( brain_std ~ mass_std , data=d )\\npost <- extract.samples( m7.1_OLS )\\nOLS is not considered a Bayesian algorithm. But as long as the priors are vague, minimizing the sum\\nof squared deviations to the regression line is equivalent to finding the posterior mean. In fact, Carl\\nFriedrich Gauss originally derived the OLS procedure in a Bayesian framework.102 Back then, nearly\\nall probability was Bayesian, although the term “Bayesian” wouldn’t be used much until the twentieth\\ncentury. In most cases, a non-Bayesian procedure will have an approximate Bayesian interpretation.\\nThis fact is powerful in both directions. The Bayesian interpretation of a non-Bayesian procedure\\nrecasts assumptions in terms of information, and this can be very useful for understanding why a\\nprocedure works. Likewise, a Bayesian model can be embodied in an efficient, but approximate,\\n“non-Bayesian” procedure. Bayesian inference means approximating the posterior distribution. It\\ndoes not specify how that approximation is done.\\n'},\n",
       " {'index': 215,\n",
       "  'number': 197,\n",
       "  'content': '7.1. THE PROBLEM WITH PARAMETERS\\n197\\nBefore pausing to plot the posterior distribution, like we did in previous chapters, let’s\\nfocus on the R2, the proportion of variance “explained” by the model. What is really meant\\nhere is that the linear model retrodicts some proportion of the total variation in the outcome\\ndata it was fit to. The remaining variation is just the variation of the residuals (page 135).\\nThe point of this example is not to praise R2 but to bury it. But we still need to compute\\nit before burial. This is thankfully easy. We just compute the posterior predictive distribu-\\ntion for each observation—you did this in earlier chapters with sim. Then we subtract each\\nobservation from its prediction to get a residual. Then we need the variance of both these\\nresiduals and the outcome variable. This means the actual empirical variance, not the vari-\\nance that R returns with the var function, which is a frequentist estimator and therefore\\nhas the wrong denominator. So we’ll compute variance the old fashioned way: the average\\nsquared deviation from the mean. The rethinking package includes a function var2 for\\nthis purpose. In principle, the Bayesian approach mandates that we do this for each sample\\nfrom the posterior. But R2 is traditionally computed only at the mean prediction. So we’ll\\ndo that as well here. Later in the chapter you’ll learn a properly Bayesian score that uses the\\nentire posterior distribution.\\nR code\\n7.5\\nset.seed(12)\\ns <- sim( m7.1 )\\nr <- apply(s,2,mean) - d$brain_std\\nresid_var <- var2(r)\\noutcome_var <- var2( d$brain_std )\\n1 - resid_var/outcome_var\\n[1] 0.4774589\\nWe’ll want to do this for the next several models, so let’s write a function to make it repeatable.\\nIf you find yourself writing code more than once, it is usually saner to write a function and\\ncall the function more than once instead.\\nR code\\n7.6\\nR2_is_bad <- function( quap_fit ) {\\ns <- sim( quap_fit , refresh=0 )\\nr <- apply(s,2,mean) - d$brain_std\\n1 - var2(r)/var2(d$brain_std)\\n}\\nNow for some other models to compare to m7.1. We’ll consider five more models, each\\nmore complex than the last. Each of these models will just be a polynomial of higher degree.\\nFor example, a second-degree polynomial that relates body size to brain size is a parabola.\\nIn math form, it is:\\nbi ∼Normal(µi, σ)\\nµi = α + β1mi + β2m2\\ni\\nα ∼Normal(0.5, 1)\\nβj ∼Normal(0, 10)\\nfor j = 1..2\\nσ ∼Log-Normal(0, 1)\\n'},\n",
       " {'index': 216,\n",
       "  'number': 198,\n",
       "  'content': '198\\n7. ULYSSES’ COMPASS\\nThis model family adds one more parameter, β2, but uses all of the same data as m7.1. To do\\nthis model in quap, we can define β as a vector. The only trick required is to tell quap how\\nlong that vector is by using a start list:\\nR code\\n7.7\\nm7.2 <- quap(\\nalist(\\nbrain_std ~ dnorm( mu , exp(log_sigma) ),\\nmu <- a + b[1]*mass_std + b[2]*mass_std^2,\\na ~ dnorm( 0.5 , 1 ),\\nb ~ dnorm( 0 , 10 ),\\nlog_sigma ~ dnorm( 0 , 1 )\\n), data=d , start=list(b=rep(0,2)) )\\nThe next four models are constructed in similar fashion. The models m7.3 through m7.6 are\\njust third-degree, fourth-degree, fifth-degree, and sixth-degree polynomials.\\nR code\\n7.8\\nm7.3 <- quap(\\nalist(\\nbrain_std ~ dnorm( mu , exp(log_sigma) ),\\nmu <- a + b[1]*mass_std + b[2]*mass_std^2 +\\nb[3]*mass_std^3,\\na ~ dnorm( 0.5 , 1 ),\\nb ~ dnorm( 0 , 10 ),\\nlog_sigma ~ dnorm( 0 , 1 )\\n), data=d , start=list(b=rep(0,3)) )\\nm7.4 <- quap(\\nalist(\\nbrain_std ~ dnorm( mu , exp(log_sigma) ),\\nmu <- a + b[1]*mass_std + b[2]*mass_std^2 +\\nb[3]*mass_std^3 + b[4]*mass_std^4,\\na ~ dnorm( 0.5 , 1 ),\\nb ~ dnorm( 0 , 10 ),\\nlog_sigma ~ dnorm( 0 , 1 )\\n), data=d , start=list(b=rep(0,4)) )\\nm7.5 <- quap(\\nalist(\\nbrain_std ~ dnorm( mu , exp(log_sigma) ),\\nmu <- a + b[1]*mass_std + b[2]*mass_std^2 +\\nb[3]*mass_std^3 + b[4]*mass_std^4 +\\nb[5]*mass_std^5,\\na ~ dnorm( 0.5 , 1 ),\\nb ~ dnorm( 0 , 10 ),\\nlog_sigma ~ dnorm( 0 , 1 )\\n), data=d , start=list(b=rep(0,5)) )\\nThat last model, m7.6, has one trick in it. The standard deviation is replaced with a constant\\nvalue 0.001. The model will not work otherwise, for a very important reason that will become\\nclear as we plot these monsters. Here’s the last model:\\n'},\n",
       " {'index': 217,\n",
       "  'number': 199,\n",
       "  'content': '7.1. THE PROBLEM WITH PARAMETERS\\n199\\nR code\\n7.9\\nm7.6 <- quap(\\nalist(\\nbrain_std ~ dnorm( mu , 0.001 ),\\nmu <- a + b[1]*mass_std + b[2]*mass_std^2 +\\nb[3]*mass_std^3 + b[4]*mass_std^4 +\\nb[5]*mass_std^5 + b[6]*mass_std^6,\\na ~ dnorm( 0.5 , 1 ),\\nb ~ dnorm( 0 , 10 )\\n), data=d , start=list(b=rep(0,6)) )\\nNow to plot each model. We’ll follow the steps from earlier chapters: extract samples\\nfrom the posterior, compute the posterior predictive distribution at each of several locations\\non the horizontal axis, summarize, and plot. For m7.1:\\nR code\\n7.10\\npost <- extract.samples(m7.1)\\nmass_seq <- seq( from=min(d$mass_std) , to=max(d$mass_std) , length.out=100 )\\nl <- link( m7.1 , data=list( mass_std=mass_seq ) )\\nmu <- apply( l , 2 , mean )\\nci <- apply( l , 2 , PI )\\nplot( brain_std ~ mass_std , data=d )\\nlines( mass_seq , mu )\\nshade( ci , mass_seq )\\nI show this plot and all the others, with some cosmetic improvements (see brain_plot for\\nthe code), in Figure 7.3. Each plot also displays R2. As the degree of the polynomial defining\\nthe mean increases, the R2 always improves, indicating better retrodiction of the data. The\\nfifth-degree polynomial has an R2 value of 0.99. It almost passes exactly through each point.\\nThe sixth-degree polynomial actually does pass through every point, and it has no residual\\nvariance. It’s a perfect fit, R2 = 1. That is why we had to fix the sigma value—if it were\\nestimated, it would shrink to zero, because the residual variance is zero when the line passes\\nright through the center of each point.\\nHowever, you can see from looking at the paths of the predicted means that the higher-\\ndegree polynomials are increasingly absurd. This absurdity is seen most easily in Figure 7.3,\\nm7.6, the most complex model. The fit is perfect, but the model is ridiculous. Notice that\\nthere is a gap in the body mass data, because there are no fossil hominins with body mass\\nbetween 55 kg and about 60 kg. In this region, the predicted mean brain size from the high-\\ndegree polynomial models has nothing to predict, and so the models pay no price for swing-\\ning around wildly in this interval. The swing is so extreme that I had to extend the range of\\nthe vertical axis to display the depth at which the predicted mean finally turns back around.\\nAt around 58 kg, the model predicts a negative brain size! The model pays no price (yet) for\\nthis absurdity, because there are no cases in the data with body mass near 58 kg.\\nWhy does the sixth-degree polynomial fit perfectly? Because it has enough parameters\\nto assign one to each point of data. The model’s equation for the mean has 7 parameters:\\nµi = α + β1mi + β2m2\\ni + β3m3\\ni + β4m4\\ni + β5m5\\ni + β6m6\\ni\\nand there are 7 species to predict brain sizes for. So effectively, this model assigns a unique\\nparameter to reiterate each observed brain size. This is a general phenomenon: If you adopt\\n'},\n",
       " {'index': 218,\n",
       "  'number': 200,\n",
       "  'content': '200\\n7. ULYSSES’ COMPASS\\nbody mass (kg)\\nbrain volume (cc)\\n35\\n47\\n60\\n450\\n900\\n1300\\nm7.1: R^2 = 0.51\\nbody mass (kg)\\nbrain volume (cc)\\n35\\n47\\n60\\n450\\n900\\n1300\\nm7.2: R^2 = 0.54\\nbody mass (kg)\\nbrain volume (cc)\\n35\\n47\\n60\\n450\\n900\\n1300\\nm7.3: R^2 = 0.69\\nbody mass (kg)\\nbrain volume (cc)\\n35\\n47\\n60\\n450\\n900\\n1300\\nm7.4: R^2 = 0.82\\nbody mass (kg)\\nbrain volume (cc)\\n35\\n47\\n60\\n450\\n900\\n1300\\nm7.5: R^2 = 0.99\\nbody mass (kg)\\nbrain volume (cc)\\n35\\n47\\n60\\n0\\n450\\n1300\\nm7.6: R^2 = 1\\nFigure 7.3. Polynomial linear models of increasing degree for the hominin\\ndata. Each plot shows the posterior mean in black, with 89% interval of\\nthe mean shaded. R2 is displayed above each plot. In order from top-left:\\nFirst-degree polynomial, second-degree, third-degree, fourth-degree, fifth-\\ndegree, and sixth-degree.\\n'},\n",
       " {'index': 219,\n",
       "  'number': 201,\n",
       "  'content': '7.1. THE PROBLEM WITH PARAMETERS\\n201\\na model family with enough parameters, you can fit the data exactly. But such a model will\\nmake rather absurd predictions for yet-to-be-observed cases.\\nRethinking: Model fitting as compression. Another perspective on the absurd model just above is to\\nconsider that model fitting can be considered a form of data compression. Parameters summarize\\nrelationships among the data. These summaries compress the data into a simpler form, although\\nwith loss of information (“lossy” compression) about the sample. The parameters can then be used\\nto generate new data, effectively decompressing the data.\\nWhen a model has a parameter to correspond to each datum, such as m7.6, then there is actually\\nno compression. The model just encodes the raw data in a different form, using parameters instead.\\nAs a result, we learn nothing about the data from such a model. Learning about the data requires using\\na simpler model that achieves some compression, but not too much. This view of model selection is\\noften known as Minimum Description Length (MDL).103\\n7.1.2. Too few parameters hurts, too. The overfit polynomial models fit the data extremely\\nwell, but they suffer for this within-sample accuracy by making nonsensical out-of-sample\\npredictions. In contrast, underfitting produces models that are inaccurate both within\\nand out of sample. They learn too little, failing to recover regular features of the sample.\\nAnother way to conceptualize an underfit model is to notice that it is insensitive to the\\nsample. We could remove any one point from the sample and get almost the same regression\\nline. In contrast, the most complex model, m7.6, is very sensitive to the sample. If we re-\\nmoved any one point, the mean would change a lot. You can see this sensitivity in Figure 7.4.\\nIn both plots what I’ve done is drop each row of the data, one at a time, and re-derive the\\nposterior distribution. On the left, each line is a first-degree polynomial, m7.1, fit to one of\\nthe seven possible sets of data constructed from dropping one row. The curves on the right\\nare instead different fourth-order polynomials, m7.4. Notice that the straight lines hardly\\nvary, while the curves fly about wildly. This is a general contrast between underfit and overfit\\nmodels: sensitivity to the exact composition of the sample used to fit the model.\\nOverthinking: Dropping rows. The calculations needed to produce Figure 7.4 are made easy by a\\ntrick of R’s index notation. To drop a row i from a data frame d, just use:\\nR code\\n7.11\\nd_minus_i <- d[ -i , ]\\nThis means drop the i-th row and keep all of the columns. Repeating the regression is then just a matter\\nof looping over the rows. Look inside the function brain_loo_plot in the rethinking package to\\nsee how the figure was drawn and explore other models.\\nRethinking: Bias and variance. The underfitting/overfitting dichotomy is often described as the\\nbias-variance trade-off.104 While not exactly the same distinction, the bias-variance trade-off\\naddresses the same problem. “Bias” is related to underfitting, while “variance” is related to overfitting.\\nThese terms are confusing, because they are used in many different ways in different contexts, even\\nwithin statistics. The term “bias” also sounds like a bad thing, even though increasing bias often leads\\nto better predictions.\\n'},\n",
       " {'index': 220,\n",
       "  'number': 202,\n",
       "  'content': '202\\n7. ULYSSES’ COMPASS\\nbody mass (kg)\\nbrain volume (cc)\\n35\\n47\\n60\\n450\\n900\\n1300\\nm7.1\\nbody mass (kg)\\nbrain volume (cc)\\n35\\n47\\n60\\n0\\n900\\n2000\\nm7.4\\nFigure 7.4. Underfitting and overfitting as under-sensitivity and over-\\nsensitivity to sample. In both plots, a regression is fit to the seven sets of\\ndata made by dropping one row from the original data. Left: An underfit\\nmodel is insensitive to the sample, changing little as individual points are\\ndropped. Right: An overfit model is sensitive to the sample, changing dra-\\nmatically as points are dropped.\\n7.2. Entropy and accuracy\\nSo how do we navigate between the hydra of overfitting and the vortex of underfitting?\\nWhether you end up using regularization or information criteria or both, the first thing you\\nmust do is pick a criterion of model performance. What do you want the model to do well\\nat? We’ll call this criterion the target, and in this section you’ll see how information theory\\nprovides a common and useful target.\\nThe path to out-of-sample deviance is twisty, however. Here are the steps ahead. First,\\nwe need to establish a measurement scale for distance from perfect accuracy. This will re-\\nquire a little information theory, as it will provide a natural measurement scale for the dis-\\ntance between two probability distributions. Second, we need to establish deviance as an\\napproximation of relative distance from perfect accuracy. Finally, we must establish that\\nit is only deviance out-of-sample that is of interest. Once you have deviance in hand as a\\nmeasure of model performance, in the sections to follow you’ll see how both regularizing\\npriors and information criteria help you improve and estimate the out-of-sample deviance\\nof a model.\\nThis material is complicated. You don’t have to understand everything on the first pass.\\n7.2.1. Firing the weatherperson. Accuracy depends upon the definition of the target, and\\nthere is no universally best target. In defining a target, there are two major dimensions to\\nworry about:\\n(1) Cost-benefit analysis. How much does it cost when we’re wrong? How much do we\\nwin when we’re right? Most scientists never ask these questions in any formal way,\\nbut applied scientists must routinely answer them.\\n'},\n",
       " {'index': 221,\n",
       "  'number': 203,\n",
       "  'content': '7.2. ENTROPY AND ACCURACY\\n203\\n(2) Accuracy in context. Some prediction tasks are inherently easier than others. So\\neven if we ignore costs and benefits, we still need a way to judge “accuracy” that\\naccounts for how much a model could possibly improve prediction.\\nIt will help to explore these two dimensions in an example. Suppose in a certain city,\\na certain weatherperson issues uncertain predictions for rain or shine on each day of the\\nyear.105 The predictions are in the form of probabilities of rain. The currently employed\\nweatherperson predicted these chances of rain over a 10-day sequence, with the actual out-\\ncomes shown below each prediction:\\nDay\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nPrediction\\n1\\n1\\n1\\n0.6\\n0.6\\n0.6\\n0.6\\n0.6\\n0.6\\n0.6\\nObserved\\nA newcomer rolls into town and boasts that he can best the current weatherperson by always\\npredicting sunshine. Over the same 10-day period, the newcomer’s record would be:\\nDay\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nPrediction\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\nObserved\\n“So by rate of correct prediction alone,” the newcomer announces, “I’m the best person for\\nthe job.”\\nThe newcomer is right. Define hit rate as the average chance of a correct prediction. So\\nfor the current weatherperson, she gets 3 × 1 + 7 × 0.4 = 5.8 hits in 10 days, for a rate of\\n5.8/10 = 0.58 correct predictions per day. In contrast, the newcomer gets 3×0+7×1 = 7,\\nfor 7/10 = 0.7 hits per day. The newcomer wins.\\n7.2.1.1. Costs and benefits. But it’s not hard to find another criterion, other than rate of\\ncorrect prediction, that makes the newcomer look foolish. Any consideration of costs and\\nbenefits will suffice. Suppose for example that you hate getting caught in the rain, but you also\\nhate carrying an umbrella. Let’s define the cost of getting wet as −5 points of happiness and\\nthe cost of carrying an umbrella as −1 point of happiness. Suppose your chance of carrying\\nan umbrella is equal to the forecast probability of rain. Your job is now to maximize your\\nhappiness by choosing a weatherperson. Here are your points, following either the current\\nweatherperson or the newcomer:\\nDay\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\nObserved\\nPoints\\nCurrent\\n−1\\n−1\\n−1\\n−0.6\\n−0.6\\n−0.6\\n−0.6\\n−0.6\\n−0.6\\n−0.6\\nNewcomer\\n−5\\n−5\\n−5\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\nSo the current weatherperson nets you 3 × (−1) + 7 × (−0.6) = −7.2 happiness, while the\\nnewcomer nets you −15 happiness. So the newcomer doesn’t look so clever now. You can\\nplay around with the costs and the decision rule, but since the newcomer always gets you\\ncaught unprepared in the rain, it’s not hard to beat his forecast.\\n'},\n",
       " {'index': 222,\n",
       "  'number': 204,\n",
       "  'content': '204\\n7. ULYSSES’ COMPASS\\n7.2.1.2. Measuring accuracy. But even if we ignore costs and benefits of any actual deci-\\nsion based upon the forecasts, there’s still ambiguity about which measure of “accuracy” to\\nadopt. There’s nothing special about “hit rate.” The question to focus on is: Which definition\\nof “accuracy” is maximized by knowing the true model generating the data? Surely we can’t\\ndo better than that.\\nConsider computing the probability of predicting the exact sequence of days. This means\\ncomputing the probability of a correct prediction for each day. Then multiply all of these\\nprobabilities together to get the joint probability of correctly predicting the observed se-\\nquence. This is the same thing as the joint likelihood, which you’ve been using up to this\\npoint to fit models with Bayes’ theorem. This is the definition of accuracy that is maximized\\nby the correct model.\\nIn this light, the newcomer looks even worse. The probability for the current weather-\\nperson is 13 × 0.47 ≈0.005. For the newcomer, it’s 03 × 17 = 0. So the newcomer has zero\\nprobability of getting the sequence correct. This is because the newcomer’s predictions never\\nexpect rain. So even though the newcomer has a high average probability of being correct\\n(hit rate), he has a terrible joint probability of being correct.\\nAnd the joint probability is the measure we want. Why? Because it appears in Bayes’ the-\\norem as the likelihood. It’s the unique measure that correctly counts up the relative number\\nof ways each event (sequence of rain and shine) could happen. Another way to think of this\\nis to consider what happens when we maximize average probability or joint probability. The\\ntrue data-generating model will not have the highest hit rate. You saw this already with the\\nweatherperson: Assigning zero probability to rain improves hit rate, but it is clearly wrong.\\nIn contrast, the true model will have the highest joint probability.\\nIn the statistics literature, you will sometimes see this measure of accuracy called the\\nlog scoring rule, because typically we compute the logarithm of the joint probability and\\nreport that. If you see an analysis using something else, either it is a special case of the log\\nscoring rule or it is possibly much worse.\\nRethinking: Calibration is overrated. It’s common for models to be judged by their calibration.\\nIf a model predicts a 40% chance of rain, then it is said to be “calibrated” if it actually rains on 40%\\nof such predictions. The problem is that calibrated predictions do not have to be good. For example,\\nif it rains on 40% of days, then a model that just predicts a 40% chance of rain on every day will\\nbe perfectly calibrated. But it will also be terribly inaccurate. Nor do good predictions have to be\\ncalibrated. Suppose a forecaster always has 100% confidence in each forecast and correctly predicts\\nthe weather on 80% of days. The forecaster is accurate, but he is not calibrated. He is overconfident.\\nHere’s a real example. The forecasting website www.fivethirtyeight.com makes many predictions.\\nTheir calibration for sporting events is almost perfect.106 But their accuracy is often barely better than\\nguessing. In contrast, their political predictions are less calibrated, but more accurate on average.\\nTerms like “calibration” have various meanings. So it’s good to provide and ask for contextual\\ndefinitions.107 The posterior predictive checks endorsed in this book, for example, are sometimes\\ncalled “calibration checks.”\\n7.2.2. Information and uncertainty. So we want to use the log probability of the data to\\nscore the accuracy of competing models. The next problem is how to measure distance from\\nperfect prediction. A perfect prediction would just report the true probabilities of rain on\\neach day. So when either weatherperson provides a prediction that differs from the target,\\nwe can measure the distance of the prediction from the target. But what kind of distance\\n'},\n",
       " {'index': 223,\n",
       "  'number': 205,\n",
       "  'content': '7.2. ENTROPY AND ACCURACY\\n205\\nshould we adopt? It’s not obvious how to go about answering this question. But there turns\\nout to be a unique and optimal answer.\\nGetting to the answer depends upon appreciating what an accuracy metric needs to do.\\nIt should appreciate that some targets are just easier to hit than other targets. For example,\\nsuppose we extend the weather forecast into the winter. Now there are three types of days:\\nrain, sun, and snow. Now there are three ways to be wrong, instead of just two. This has to be\\nreflected in any reasonable measure of distance from the target, because by adding another\\ntype of event, the target has gotten harder to hit.\\nIt’s like taking a two-dimensional archery bullseye and forcing the archer to hit the tar-\\nget at the right time—a third dimension—as well. Now the possible distance between the\\nbest archer and the worst archer has grown, because there’s another way to miss. And with\\nanother way to miss, one might also say that there is another way for an archer to impress.\\nAs the potential distance between the target and the shot increases, so too does the potential\\nimprovement and ability of a talented archer to impress us.\\nThe solution to the problem of how to measure distance of a model’s accuracy from a\\ntarget was provided in the late 1940s.108 Originally applied to problems in communication\\nof messages, such as telegraph, the field of information theory is now important across\\nthe basic and applied sciences, and it has deep connections to Bayesian inference. And like\\nmany successful fields, information theory has spawned many bogus applications, as well.109\\nThe basic insight is to ask: How much is our uncertainty reduced by learning an outcome?\\nConsider the weather forecasts again. Forecasts are issued in advance and the weather is\\nuncertain. When the actual day arrives, the weather is no longer uncertain. The reduction\\nin uncertainty is then a natural measure of how much we have learned, how much “infor-\\nmation” we derive from observing the outcome. So if we can develop a precise definition of\\n“uncertainty,” we can provide a baseline measure of how hard it is to predict, as well as how\\nmuch improvement is possible. The measured decrease in uncertainty is the definition of\\ninformation in this context.\\nInformation: The reduction in uncertainty when we learn an outcome.\\nTo use this definition, what we need is a principled way to quantify the uncertainty in-\\nherent in a probability distribution. So suppose again that there are two possible weather\\nevents on any particular day: Either it is sunny or it is rainy. Each of these events occurs\\nwith some probability, and these probabilities add up to one. What we want is a function\\nthat uses the probabilities of shine and rain and produces a measure of uncertainty.\\nThere are many possible ways to measure uncertainty. The most common way begins\\nby naming some properties a measure of uncertainty should possess. These are the three\\nintuitive desiderata:\\n(1) The measure of uncertainty should be continuous. If it were not, then an arbitrarily\\nsmall change in any of the probabilities, for example the probability of rain, would\\nresult in a massive change in uncertainty.\\n(2) The measure of uncertainty should increase as the number of possible events in-\\ncreases. For example, suppose there are two cities that need weather forecasts. In\\nthe first city, it rains on half of the days in the year and is sunny on the others. In\\nthe second, it rains, shines, and hails, each on 1 out of every 3 days in the year. We’d\\nlike our measure of uncertainty to be larger in the second city, where there is one\\nmore kind of event to predict.\\n'},\n",
       " {'index': 224,\n",
       "  'number': 206,\n",
       "  'content': '206\\n7. ULYSSES’ COMPASS\\n(3) The measure of uncertainty should be additive. What this means is that if we first\\nmeasure the uncertainty about rain or shine (2 possible events) and then the uncer-\\ntainty about hot or cold (2 different possible events), the uncertainty over the four\\ncombinations of these events—rain/hot, rain/cold, shine/hot, shine/cold—should\\nbe the sum of the separate uncertainties.\\nThere is only one function that satisfies these desiderata. This function is usually known as\\ninformation entropy, and has a surprisingly simple definition. If there are n different\\npossible events and each event i has probability pi, and we call the list of probabilities p, then\\nthe unique measure of uncertainty we seek is:\\nH(p) = −E log(pi) = −\\nn\\nX\\ni=1\\npi log(pi)\\n(7.1)\\nIn plainer words:\\nThe uncertainty contained in a probability distribution is the average log-probability\\nof an event.\\n“Event” here might refer to a type of weather, like rain or shine, or a particular species of bird\\nor even a particular nucleotide in a DNA sequence.\\nWhile it’s not worth going into the details of the derivation of H, it is worth pointing\\nout that nothing about this function is arbitrary. Every part of it derives from the three\\nrequirements above. Still, we accept H(p) as a useful measure of uncertainty not because of\\nthe premises that lead to it, but rather because it has turned out to be so useful and productive.\\nAn example will help to demystify the function H(p). To compute the information en-\\ntropy for the weather, suppose the true probabilities of rain and shine are p1 = 0.3 and\\np2 = 0.7, respectively. Then:\\nH(p) = −\\n\\x00p1 log(p1) + p2 log(p2)\\n\\x01\\n≈0.61\\nAs an R calculation:\\nR code\\n7.12\\np <- c( 0.3 , 0.7 )\\n-sum( p*log(p) )\\n[1] 0.6108643\\nSuppose instead we live in Abu Dhabi. Then the probabilities of rain and shine might be more\\nlike p1 = 0.01 and p2 = 0.99. Now the entropy would be approximately 0.06. Why has the\\nuncertainty decreased? Because in Abu Dhabi it hardly ever rains. Therefore there’s much\\nless uncertainty about any given day, compared to a place in which it rains 30% of the time.\\nIt’s in this way that information entropy measures the uncertainty inherent in a distribution\\nof events. Similarly, if we add another kind of event to the distribution—forecasting into\\nwinter, so also predicting snow—entropy tends to increase, due to the added dimensionality\\nof the prediction problem. For example, suppose probabilities of sun, rain, and snow are\\np1 = 0.7, p2 = 0.15, and p3 = 0.15, respectively. Then entropy is about 0.82.\\nThese entropy values by themselves don’t mean much to us, though. Instead we can use\\nthem to build a measure of accuracy. That comes next.\\nOverthinking: More on entropy. Above I said that information entropy is the average log-probability.\\nBut there’s also a −1 in the definition. Multiplying the average log-probability by −1 just makes the\\nentropy H increase from zero, rather than decrease from zero. It’s conventional, but not functional.\\n'},\n",
       " {'index': 225,\n",
       "  'number': 207,\n",
       "  'content': '7.2. ENTROPY AND ACCURACY\\n207\\nThe logarithms above are natural logs (base e), but changing the base rescales without any effect on\\ninference. Binary logarithms, base 2, are just as common. As long as all of the entropies you compare\\nuse the same base, you’ll be fine.\\nThe only trick in computing H is to deal with the inevitable question of what to do when pi = 0.\\nThe log(0) = −∞, which won’t do. However, L’Hôpital’s rule tells us that limpi→0 pi log(pi) = 0. So\\njust assume that 0 log(0) = 0, when you compute H. In other words, events that never happen drop\\nout. Just remember that when an event never happens, there’s no point in keeping it in the model.\\nRethinking: The benefits of maximizing uncertainty. Information theory has many applications.\\nA particularly important application is maximum entropy, also known as maxent. Maximum\\nentropy is a family of techniques for finding probability distributions that are most consistent with\\nstates of knowledge. In other words, given what we know, what is the least surprising distribution?\\nIt turns out that one answer to this question maximizes the information entropy, using the prior\\nknowledge as constraint.110 If you do this, you actually end up with the posterior distribution. So\\nBayesian updating is entropy maximization. Maximum entropy features prominently in Chapter 10,\\nwhere it will help us build generalized linear models (GLMs).\\n7.2.3. From entropy to accuracy. It’s nice to have a way to quantify uncertainty. H provides\\nthis. So we can now say, in a precise way, how hard it is to hit the target. But how can we use\\ninformation entropy to say how far a model is from the target? The key lies in divergence:\\nDivergence: The additional uncertainty induced by using probabilities from\\none distribution to describe another distribution.\\nThis is often known as Kullback-Leibler divergence or simply KL divergence, named after the\\npeople who introduced it for this purpose.111\\nSuppose for example that the true distribution of events is p1 = 0.3, p2 = 0.7. If we\\nbelieve instead that these events happen with probabilities q1 = 0.25, q2 = 0.75, how much\\nadditional uncertainty have we introduced, as a consequence of using q = {q1, q2} to ap-\\nproximate p = {p1, p2}? The formal answer to this question is based upon H, and has a\\nsimilarly simple formula:\\nDKL(p, q) =\\nX\\ni\\npi\\n\\x00log(pi) −log(qi)\\n\\x01\\n=\\nX\\ni\\npi log\\n\\x12pi\\nqi\\n\\x13\\nIn plainer language, the divergence is the average difference in log probability between the\\ntarget (p) and model (q). This divergence is just the difference between two entropies: The\\nentropy of the target distribution p and the cross entropy arising from using q to predict p\\n(see the Overthinking box on the next page for some more detail). When p = q, we know\\nthe actual probabilities of the events. In that case:\\nDKL(p, q) = DKL(p, p) =\\nX\\ni\\npi\\n\\x00log(pi) −log(pi)\\n\\x01\\n= 0\\nThere is no additional uncertainty induced when we use a probability distribution to repre-\\nsent itself. That’s somehow a comforting thought.\\nBut more importantly, as q grows more different from p, the divergence DKL also grows.\\nFigure 7.5 displays an example. Suppose the true target distribution is p = {0.3, 0.7}.\\nSuppose the approximating distribution q can be anything from q = {0.01, 0.99} to q =\\n{0.99, 0.01}. The first of these probabilities, q1, is displayed on the horizontal axis, and the\\n'},\n",
       " {'index': 226,\n",
       "  'number': 208,\n",
       "  'content': '208\\n7. ULYSSES’ COMPASS\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nq[1]\\nDivergence of q from p\\nq = p\\nFigure 7.5. Information divergence of an ap-\\nproximating distribution q from a true dis-\\ntribution p. Divergence can only equal zero\\nwhen q = p (dashed line). Otherwise, the di-\\nvergence is positive and grows as q becomes\\nmore dissimilar from p. When we have more\\nthan one candidate approximation q, the q\\nwith the smallest divergence is the most ac-\\ncurate approximation, in the sense that it in-\\nduces the least additional uncertainty.\\nvertical displays the divergence DKL(p, q). Only exactly where q = p, at q1 = 0.3, does the\\ndivergence achieve a value of zero. Everyplace else, it grows.\\nWhat divergence can do for us now is help us contrast different approximations to p. As\\nan approximating function q becomes more accurate, DKL(p, q) will shrink. So if we have\\na pair of candidate distributions, then the candidate that minimizes the divergence will be\\nclosest to the target. Since predictive models specify probabilities of events (observations),\\nwe can use divergence to compare the accuracy of models.\\nOverthinking: Cross entropy and divergence. Deriving divergence is easier than you might think.\\nThe insight is in realizing that when we use a probability distribution q to predict events from another\\ndistribution p, this defines something known as cross entropy: H(p, q) = −P\\ni pi log(qi). The notion\\nis that events arise according the the p’s, but they are expected according to the q’s, so the entropy is\\ninflated, depending upon how different p and q are. Divergence is defined as the additional entropy\\ninduced by using q. So it’s just the difference between H(p), the actual entropy of events, and H(p, q):\\nDKL(p, q) = H(p, q) −H(p)\\n= −\\nX\\ni\\npi log(qi) −\\n\\x00−\\nX\\ni\\npi log(pi)\\n\\x01\\n= −\\nX\\ni\\npi\\n\\x00log(qi) −log(pi)\\n\\x01\\nSo divergence really is measuring how far q is from the target p, in units of entropy. Notice that\\nwhich is the target matters: H(p, q) does not in general equal H(q, p). For more on that fact, see the\\nRethinking box that follows.\\nRethinking: Divergence depends upon direction. In general, H(p, q) is not equal to H(q, p). The\\ndirection matters, when computing divergence. Understanding why this is true is of some value, so\\nhere’s a contrived teaching example.\\nSuppose we get in a rocket and head to Mars. But we have no control over our landing spot,\\nonce we reach Mars. Let’s try to predict whether we land in water or on dry land, using the Earth to\\nprovide a probability distribution q to approximate the actual distribution on Mars, p. For the Earth,\\nq = {0.7, 0.3}, for probability of water and land, respectively. Mars is very dry, but let’s say for the\\nsake of the example that there is 1% surface water, so p = {0.01, 0.99}. If we count the ice caps,\\nthat’s not too big a lie. Now compute the divergence going from Earth to Mars. It turns out to be\\n'},\n",
       " {'index': 227,\n",
       "  'number': 209,\n",
       "  'content': '7.2. ENTROPY AND ACCURACY\\n209\\nDE→M = DKL(p, q) = 1.14. That’s the additional uncertainty induced by using the Earth to predict\\nthe Martian landing spot. Now consider going back the other direction. The numbers in p and q stay\\nthe same, but we swap their roles, and now DM→E = DKL(q, p) = 2.62. The divergence is more than\\ndouble in this direction. This result seems to defy comprehension. How can the distance from Earth\\nto Mars be shorter than the distance from Mars to Earth?\\nDivergence behaves this way as a feature, not a bug. There really is more additional uncertainty\\ninduced by using Mars to predict Earth than by using Earth to predict Mars. The reason is that, going\\nfrom Mars to Earth, Mars has so little water on its surface that we will be very very surprised when\\nwe most likely land in water on Earth. In contrast, Earth has good amounts of both water and dry\\nland. So when we use the Earth to predict Mars, we expect both water and land, to some extent, even\\nthough we do expect more water than land. So we won’t be nearly as surprised when we inevitably\\narrive on Martian dry land, because 30% of Earth is dry land.\\nAn important practical consequence of this asymmetry, in a model fitting context, is that if we\\nuse a distribution with high entropy to approximate an unknown true distribution of events, we will\\nreduce the distance to the truth and therefore the error. This fact will help us build generalized linear\\nmodels, later on in Chapter 10.\\n7.2.4. Estimating divergence. At this point in the chapter, dear reader, you may be won-\\ndering where the chapter is headed. At the start, the goal was to deal with overfitting and\\nunderfitting. But now we’ve spent pages and pages on entropy and other fantasies. It’s as if\\nI promised you a day at the beach, but now you find yourself at a dark cabin in the woods,\\nwondering if this is a necessary detour or rather a sinister plot.\\nIt is a necessary detour. The point of all the preceding material about information theory\\nand divergence is to establish both:\\n(1) How to measure the distance of a model from our target. Information theory gives\\nus the distance measure we need, the KL divergence.\\n(2) How to estimate the divergence. Having identified the right measure of distance,\\nwe now need a way to estimate it in real statistical modeling tasks.\\nItem (1) is accomplished. Item (2) remains for last. You’re going to see now that the diver-\\ngence leads to using a measure of model fit known as deviance.\\nTo use DKL to compare models, it seems like we would have to know p, the target proba-\\nbility distribution. In all of the examples so far, I’ve just assumed that p is known. But when\\nwe want to find a model q that is the best approximation to p, the “truth,” there is usually no\\nway to access p directly. We wouldn’t be doing statistical inference, if we already knew p.\\nBut there’s an amazing way out of this predicament. It helps that we are only interested\\nin comparing the divergences of different candidates, say q and r. In that case, most of p just\\nsubtracts out, because there is a E log(pi) term in the divergence of both q and r. This term\\nhas no effect on the distance of q and r from one another. So while we don’t know where p is,\\nwe can estimate how far apart q and r are, and which is closer to the target. It’s as if we can’t\\ntell how far any particular archer is from hitting the target, but we can tell which archer gets\\ncloser and by how much.\\nAll of this also means that all we need to know is a model’s average log-probability:\\nE log(qi) for q and E log(ri) for r. These expressions look a lot like log-probabilities of out-\\ncomes you’ve been using already to simulate implied predictions of a fit model. Indeed, just\\nsumming the log-probabilities of each observed case provides an approximation of E log(qi).\\nWe don’t have to know the p inside the expectation.\\n'},\n",
       " {'index': 228,\n",
       "  'number': 210,\n",
       "  'content': '210\\n7. ULYSSES’ COMPASS\\nSo we can compare the average log-probability from each model to get an estimate of the\\nrelative distance of each model from the target. This also means that the absolute magnitude\\nof these values will not be interpretable—neither E log(qi) nor E log(ri) by itself suggests a\\ngood or bad model. Only the difference E log(qi)−E log(ri) informs us about the divergence\\nof each model from the target p.\\nTo put all this into practice, it is conventional to sum over all the observations i, yielding\\na total score for a model q:\\nS(q) =\\nX\\ni\\nlog(qi)\\nThis kind of score is a log-probability score, and it is the gold standard way to compare the\\npredictive accuracy of different models. It is an estimate of E log(qi), just without the final\\nstep of dividing by the number of observations.\\nTo compute this score for a Bayesian model, we have to use the entire posterior distribu-\\ntion. Otherwise, vengeful angels will descend upon you. Why will they be angry? If we don’t\\nuse the entire posterior, we are throwing away information. Because the parameters have dis-\\ntributions, the predictions also have a distribution. How can we use the entire distribution of\\npredictions? We need to find the log of the average probability for each observation i, where\\nthe average is taken over the posterior distribution. Doing this calculation correctly requires\\na little subtlety. The rethinking package has a function called lppd—log-pointwise-\\npredictive-density—to do this calculation for quap models. If you are interested in the\\nsubtle details, however, see the box at the end of this section. To compute lppd for the first\\nmodel we fit in this chapter:\\nR code\\n7.13\\nset.seed(1)\\nlppd( m7.1 , n=1e4 )\\n[1]\\n0.6098668\\n0.6483438\\n0.5496093\\n0.6234934\\n0.4648143\\n0.4347605 -0.8444633\\nEach of these values is the log-probability score for a specific observation. Recall that there\\nwere only 7 observations in those data. If you sum these values, you’ll have the total log-\\nprobability score for the model and data. What do these values mean? Larger values are\\nbetter, because that indicates larger average accuracy. It is also quite common to see some-\\nthing called the deviance, which is like a lppd score, but multiplied by −2 so that smaller\\nvalues are better. The 2 is there for historical reasons.112\\nOverthinking: Computing the lppd. The Bayesian version of the log-probability score is called the\\nlog-pointwise-predictive-density. For some data y and posterior distribution Θ:\\nlppd(y, Θ) =\\nX\\ni\\nlog 1\\nS\\nX\\ns\\np(yi|Θs)\\nwhere S is the number of samples and Θs is the s-th set of sampled parameter values in the posterior\\ndistribution. While in principle this is easy—you just need to compute the probability (density) of\\neach observation i for each sample s, take the average, and then the logarithm—in practice it is not so\\neasy. The reason is that doing arithmetic in a computer often requires some tricks to retain precision.\\nIn probability calculations, it is usually safest to do everything on the log-probability scale. Here’s the\\ncode we need, to repeat the calculation in the previous section:\\n'},\n",
       " {'index': 229,\n",
       "  'number': 211,\n",
       "  'content': '7.2. ENTROPY AND ACCURACY\\n211\\nR code\\n7.14\\nset.seed(1)\\nlogprob <- sim( m7.1 , ll=TRUE , n=1e4 )\\nn <- ncol(logprob)\\nns <- nrow(logprob)\\nf <- function( i ) log_sum_exp( logprob[,i] ) - log(ns)\\n( lppd <- sapply( 1:n , f ) )\\nYou should see the same values as before. The code first calculates the log-probability of each obser-\\nvation, using sim. You used sim in Chapter 4 to simulate observations from the posterior. It can also\\njust return the log-probability, using ll=TRUE. It returns a matrix with a row for each sample and a\\ncolumn for each observation. Then the function f does the hard work. log_sum_exp computes the\\nlog of the sum of exponentiated values. So it takes all the log-probabilities for a given observation,\\nexponentiates each, sums them, then takes the log. But it does this in a way that is numerically stable.\\nThen the function subtracts the log of the number of samples, which is the same as dividing the sum\\nby the number of samples.\\n7.2.5. Scoring the right data. The log-probability score is a principled way to measure dis-\\ntance from the target. But the score as computed in the previous section has the same flaw\\nas R2: It always improves as the model gets more complex, at least for the types of models\\nwe have considered so far. Just like R2, log-probability on training data is a measure of retro-\\ndictive accuracy, not predictive accuracy. Let’s compute the log-score for each of the models\\nfrom earlier in this chapter:\\nR code\\n7.15\\nset.seed(1)\\nsapply( list(m7.1,m7.2,m7.3,m7.4,m7.5,m7.6) , function(m) sum(lppd(m)) )\\n[1]\\n2.490390\\n2.565982\\n3.695910\\n5.380871 14.089261 39.445390\\nThe more complex models have larger scores! But we already know that they are absurd. We\\nsimply cannot score models by their performance on training data. That way lies the monster\\nScylla, devourer of naive data scientists.\\nIt is really the score on new data that interests us. So before looking at tools for improving\\nand measuring out-of-sample score, let’s bring the problem into sharper focus by simulating\\nthe score both in and out of sample. When we usually have data and use it to fit a statistical\\nmodel, the data comprise a training sample. Parameters are estimated from it, and then\\nwe can imagine using those estimates to predict outcomes in a new sample, called the test\\nsample. R is going to do all of this for you. But here’s the full procedure, in outline:\\n(1) Suppose there’s a training sample of size N.\\n(2) Compute the posterior distribution of a model for the training sample, and com-\\npute the score on the training sample. Call this score Dtrain.\\n(3) Suppose another sample of size N from the same process. This is the test sample.\\n(4) Compute the score on the test sample, using the posterior trained on the training\\nsample. Call this new score Dtest.\\nThe above is a thought experiment. It allows us to explore the distinction between accuracy\\nmeasured in and out of sample, using a simple prediction scenario.\\nTo visualize the results of the thought experiment, what we’ll do now is conduct the\\nabove thought experiment 10,000 times, for each of five different linear regression models.\\n'},\n",
       " {'index': 230,\n",
       "  'number': 212,\n",
       "  'content': '212\\n7. ULYSSES’ COMPASS\\n1\\n2\\n3\\n4\\n5\\n45\\n50\\n55\\n60\\n65\\nnumber of parameters\\ndeviance\\nN = 20\\nin\\nout\\n+1SD\\n–1SD\\n1\\n2\\n3\\n4\\n5\\n250\\n260\\n270\\n280\\n290\\n300\\nnumber of parameters\\ndeviance\\nN = 100\\nin\\nout\\nFigure 7.6. Deviance in and out of sample. In each plot, models with dif-\\nferent numbers of predictor variables are shown on the horizontal axis. De-\\nviance across 10,000 simulations is shown on the vertical. Blue shows de-\\nviance in-sample, the training data. Black shows deviance out-of-sample,\\nthe test data. Points show means, and the line segments show ±1 standard\\ndeviation.\\nThe model that generates the data is:\\nyi ∼Normal(µi, 1)\\nµi = (0.15)x1,i −(0.4)x2,i\\nThis corresponds to a Gaussian outcome y for which the intercept is α = 0 and the slopes\\nfor each of two predictors are β1 = 0.15 and β2 = −0.4. The models for analyzing the\\ndata are linear regressions with between 1 and 5 free parameters. The first model, with 1 free\\nparameter to estimate, is just a linear regression with an unknown mean and fixed σ = 1.\\nEach parameter added to the model adds a predictor variable and its beta-coefficient. Since\\nthe “true” model has non-zero coefficients for only the first two predictors, we can say that\\nthe true model has 3 parameters. By fitting all five models, with between 1 and 5 parameters,\\nto training samples from the same processes, we can get an impression for how the score\\nbehaves, both inside and outside the training sample.\\nFigure 7.6 shows the results of 10,000 simulations for each model type, at two differ-\\nent sample sizes. The function that conducts the simulations is sim_train_test in the\\nrethinking package. If you want to conduct more simulations of this sort, see the Over-\\nthinking box on the next page for the full code. The vertical axis is scaled as −2 × lppd,\\n“deviance,” so that larger values are worse. In the left-hand plot in Figure 7.6, both training\\nand test samples contain 20 cases. Blue points and line segments show the mean plus-and-\\nminus one standard deviation of the deviance calculated on the training data. Moving left\\nto right with increasing numbers of parameters, the average deviance declines. A smaller\\ndeviance means a better fit. So this decline with increasing model complexity is the same\\nphenomenon you saw earlier in the chapter with R2.\\n'},\n",
       " {'index': 231,\n",
       "  'number': 213,\n",
       "  'content': '7.2. ENTROPY AND ACCURACY\\n213\\nBut now inspect the open points and black line segments. These display the distribu-\\ntion of out-of-sample deviance at each number of parameters. While the training deviance\\nalways gets better with an additional parameter, the test deviance is smallest on average for\\n3 parameters, which is the data-generating model in this case. The deviance out-of-sample\\ngets worse (increases) with the addition of each parameter after the third. These additional\\nparameters fit the noise in the additional predictors. So while deviance keeps improving (de-\\nclining) in the training sample, it gets worse on average in the test sample. The right-hand\\nplot shows the same relationships for larger samples of N = 100 cases.\\nThe size of the standard deviation bars may surprise you. While it is always true on\\naverage that deviance out-of-sample is worse than deviance in-sample, any individual pair\\nof train and test samples may reverse the expectation. The reason is that any given training\\nsample may be highly misleading. And any given testing sample may be unrepresentative.\\nKeep this fact in mind as we develop devices for comparing models, because this fact should\\nprevent you from placing too much confidence in analysis of any particular sample. Like all\\nof statistical inference, there are no guarantees here.\\nOn that note, there is also no guarantee that the “true” data-generating model will have\\nthe smallest average out-of-sample deviance. You can see a symptom of this fact in the de-\\nviance for the 2 parameter model. That model does worse in prediction than the model with\\nonly 1 parameter, even though the true model does include the additional predictor. This is\\nbecause with only N = 20 cases, the imprecision of the estimate for the first predictor pro-\\nduces more error than just ignoring it. In the right-hand plot, in contrast, there is enough\\ndata to precisely estimate the association between the first predictor and the outcome. Now\\nthe deviance for the 2 parameter model is better than that of the 1 parameter model.\\nDeviance is an assessment of predictive accuracy, not of truth. The true model, in terms\\nof which predictors are included, is not guaranteed to produce the best predictions. Likewise\\na false model, in terms of which predictors are included, is not guaranteed to produce poor\\npredictions.\\nThe point of this thought experiment is to demonstrate how deviance behaves, in the-\\nory. While deviance on training data always improves with additional predictor variables,\\ndeviance on future data may or may not, depending upon both the true data-generating pro-\\ncess and how much data is available to precisely estimate the parameters. These facts form\\nthe basis for understanding both regularizing priors and information criteria.\\nOverthinking: Simulated training and testing. To reproduce Figure 7.6, sim.train.test is run\\n10,000 (1e4) times for each of the 5 models. This code is sufficient to run all of the simulations:\\nR code\\n7.16\\nN <- 20\\nkseq <- 1:5\\ndev <- sapply( kseq , function(k) {\\nprint(k);\\nr <- replicate( 1e4 , sim_train_test( N=N, k=k ) );\\nc( mean(r[1,]) , mean(r[2,]) , sd(r[1,]) , sd(r[2,]) )\\n} )\\nIf you use Mac OS or Linux, you can parallelize the simulations by replacing the replicate line with:\\nR code\\n7.17\\nr <- mcreplicate( 1e4 , sim_train_test( N=N, k=k ) , mc.cores=4 )\\n'},\n",
       " {'index': 232,\n",
       "  'number': 214,\n",
       "  'content': '214\\n7. ULYSSES’ COMPASS\\nSet mc.cores to the number of processor cores you want to use for the simulations. Once the sim-\\nulations complete, dev will be a 4-by-5 matrix of means and standard deviations. To reproduce the\\nplot:\\nR code\\n7.18\\nplot( 1:5 , dev[1,] , ylim=c( min(dev[1:2,])-5 , max(dev[1:2,])+10 ) ,\\nxlim=c(1,5.1) , xlab=\"number of parameters\" , ylab=\"deviance\" ,\\npch=16 , col=rangi2 )\\nmtext( concat( \"N = \",N ) )\\npoints( (1:5)+0.1 , dev[2,] )\\nfor ( i in kseq ) {\\npts_in <- dev[1,i] + c(-1,+1)*dev[3,i]\\npts_out <- dev[2,i] + c(-1,+1)*dev[4,i]\\nlines( c(i,i) , pts_in , col=rangi2 )\\nlines( c(i,i)+0.1 , pts_out )\\n}\\nBy altering this code, you can simulate many different train-test scenarios. See ?sim_train_test\\nfor additional options.\\n7.3. Golem taming: regularization\\nWhat if I told you that one way to produce better predictions is to make the model worse\\nat fitting the sample? Would you believe it? In this section, we’ll demonstrate it.\\nThe root of overfitting is a model’s tendency to get overexcited by the training sample.\\nWhen the priors are flat or nearly flat, the machine interprets this to mean that every parame-\\nter value is equally plausible. As a result, the model returns a posterior that encodes as much\\nof the training sample—as represented by the likelihood function—as possible.\\nOne way to prevent a model from getting too excited by the training sample is to use a\\nskeptical prior. By “skeptical,” I mean a prior that slows the rate of learning from the sample.\\nThe most common skeptical prior is a regularizing prior. Such a prior, when tuned\\nproperly, reduces overfitting while still allowing the model to learn the regular features of a\\nsample. If the prior is too skeptical, however, then regular features will be missed, resulting\\nin underfitting. So the problem is really one of tuning. But as you’ll see, even mild skepticism\\ncan help a model do better, and doing better is all we can really hope for in the large world,\\nwhere no model nor prior is optimal.\\nIn previous chapters, I forced us to revise the priors until the prior predictive distribution\\nproduced only reasonable outcomes. As a consequence, those priors regularized inference.\\nIn very small samples, they would be a big help. Here I want to show you why, using some\\nmore simulations. Consider this Gaussian model:\\nyi ∼Normal(µi, σ)\\nµi = α + βxi\\nα ∼Normal(0, 100)\\nβ ∼Normal(0, 1)\\nσ ∼Exponential(1)\\nAssume, as is good practice, that the predictor x is standardized so that its standard deviation\\nis 1 and its mean is zero. Then the prior on α is a nearly flat prior that has no practical effect\\non inference, as you’ve seen in earlier chapters.\\n'},\n",
       " {'index': 233,\n",
       "  'number': 215,\n",
       "  'content': '7.3. GOLEM TAMING: REGULARIZATION\\n215\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nparameter value\\nDensity\\nFigure 7.7. Regularizing priors, weak and\\nstrong. Three Gaussian priors of varying stan-\\ndard deviation. These priors reduce overfit-\\nting, but with different strength.\\nDashed:\\nNormal(0, 1).\\nThin solid: Normal(0, 0.5).\\nThick solid: Normal(0, 0.2).\\nBut the prior on β is narrower and is meant to regularize. The prior β ∼Normal(0, 1)\\nsays that, before seeing the data, the machine should be very skeptical of values above 2 and\\nbelow −2, as a Gaussian prior with a standard deviation of 1 assigns only 5% plausibility to\\nvalues above and below 2 standard deviations. Because the predictor variable x is standard-\\nized, you can interpret this as meaning that a change of 1 standard deviation in x is very\\nunlikely to produce 2 units of change in the outcome.\\nYou can visualize this prior in Figure 7.7 as the dashed curve. Since more probability\\nis massed up around zero, estimates are shrunk towards zero—they are conservative. The\\nother curves are narrower priors that are even more skeptical of parameter values far from\\nzero. The thin solid curve is a stronger Gaussian prior with a standard deviation of 0.5. The\\nthick solid curve is even stronger, with a standard deviation of only 0.2.\\nHow strong or weak these skeptical priors will be in practice depends upon the data\\nand model. So let’s explore a train-test example, similar to what you saw in the previous\\nsection (Figure 7.6). This time we’ll use the regularizing priors pictured in Figure 7.7,\\ninstead of flat priors. For each of five different models, we simulate 10,000 times for each of\\nthe three regularizing priors above. Figure 7.8 shows the results. The points are the same\\nflat-prior deviances as in the previous section: blue for training deviance and black for test\\ndeviance. The lines show the train and test deviances for the different priors. The blue lines\\nare training deviance and the black lines test deviance. The style of the lines correspond to\\nthose in Figure 7.7.\\nFocus on the left-hand plot, where the sample size is N = 20, for the moment. The\\ntraining deviance always increases—gets worse—with tighter priors. The thick blue trend is\\nsubstantially larger than the others, and this is because the skeptical prior prevents the model\\nfrom adapting completely to the sample. But the test deviances, out-of-sample, improve (get\\nsmaller) with the tighter priors. The model with three parameters is still the best model\\nout-of-sample, and the regularizing priors have little impact on its deviance.\\nBut also notice that as the prior gets more skeptical, the harm done by an overly complex\\nmodel is greatly reduced. For the Normal(0, 0.2) prior (thick line), the models with 4 and 5\\nparameters are barely worse than the correct model with 3 parameters. If you can tune the\\nregularizing prior right, then overfitting can be greatly reduced.\\n'},\n",
       " {'index': 234,\n",
       "  'number': 216,\n",
       "  'content': '216\\n7. ULYSSES’ COMPASS\\n1\\n2\\n3\\n4\\n5\\n48\\n50\\n52\\n54\\n56\\n58\\n60\\nnumber of parameters\\ndeviance\\nN = 20\\nN(0,1)\\nN(0,0.5)\\nN(0,0.2)\\n1\\n2\\n3\\n4\\n5\\n260\\n265\\n270\\n275\\n280\\n285\\nnumber of parameters\\ndeviance\\nN = 100\\nFigure 7.8. Regularizing priors and out-of-sample deviance. The points in\\nboth plots are the same as in Figure 7.6. The lines show training (blue)\\nand testing (black) deviance for the three regularizing priors in Figure 7.7.\\nDashed: Each beta-coefficient is given a Normal(0, 1) prior. Thin solid:\\nNormal(0, 0.5). Thick solid: Normal(0, 0.2).\\nNow focus on the right-hand plot, where sample size is N = 100. The priors have much\\nless of an effect here, because there is so much more evidence. The priors do help. But\\noverfitting was less of a concern to begin with, and there is enough information in the data\\nto overwhelm even the Normal(0, 0.2) prior (thick line).\\nRegularizing priors are great, because they reduce overfitting. But if they are too skep-\\ntical, they prevent the model from learning from the data. When you encounter multilevel\\nmodels in Chapter 13, you’ll see that their central device is to learn the strength of the prior\\nfrom the data itself. So you can think of multilevel models as adaptive regularization, where\\nthe model itself tries to learn how skeptical it should be.\\nRethinking: Ridge regression. Linear models in which the slope parameters use Gaussian priors,\\ncentered at zero, are sometimes known as ridge regression. Ridge regression typically takes as\\ninput a precision λ that essentially describes the narrowness of the prior. λ > 0 results in less over-\\nfitting. However, just as with the Bayesian version, if λ is too large, we risk underfitting. While not\\noriginally developed as Bayesian, ridge regression is another example of how a statistical procedure\\ncan be understood from both Bayesian and non-Bayesian perspectives. Ridge regression does not\\ncompute a posterior distribution. Instead it uses a modification of OLS that stitches λ into the usual\\nmatrix algebra formula for the estimates. The function lm.ridge, built into R’s MASS library, will fit\\nlinear models this way.\\nDespite how easy it is to use regularization, most traditional statistical methods use no regular-\\nization at all. Statisticians often make fun of machine learning for reinventing statistics under new\\nnames. But regularization is one area where machine learning is more mature. Introductory machine\\nlearning courses usually describe regularization. Most introductory statistics courses do not.\\n'},\n",
       " {'index': 235,\n",
       "  'number': 217,\n",
       "  'content': '7.4. PREDICTING PREDICTIVE ACCURACY\\n217\\n7.4. Predicting predictive accuracy\\nAll of the preceding suggests one way to navigate overfitting and underfitting: Evaluate\\nour models out-of-sample. But we do not have the out-of-sample, by definition, so how can\\nwe evaluate our models on it? There are two families of strategies: cross-validation and\\ninformation criteria. These strategies try to guess how well models will perform, on\\naverage, in predicting new data. We’ll consider both approaches in more detail. Despite\\nsubtle differences in their mathematics, they produce extremely similar approximations.\\n7.4.1. Cross-validation. A popular strategy for estimating predictive accuracy is to actually\\ntest the model’s predictive accuracy on another sample. This is known as cross-validation,\\nleaving out a small chunk of observations from our sample and evaluating the model on the\\nobservations that were left out. Of course we don’t want to leave out data. So what is usually\\ndone is to divide the sample in a number of chunks, called “folds.” The model is asked to\\npredict each fold, after training on all the others. We then average over the score for each\\nfold to get an estimate of out-of-sample accuracy. The minimum number of folds is 2. At\\nthe other extreme, you could make each point observation a fold and fit as many models as\\nyou have individual observations. You can perform cross-validation on quap models using\\nthe cv_quap function in the rethinking package.\\nHow many folds should you use? This is an understudied question. A lot of advice states\\nthat both too few and too many folds produce less reliable approximations of out-of-sample\\nperformance. But simulation studies do not reliably find that this is the case.113 It is ex-\\ntremely common to use the maximum number of folds, resulting in leaving out one unique\\nobservation in each fold. This is called leave-one-out cross-validation (often abbrevi-\\nated as LOOCV). Leave-one-out cross-validation is what we’ll consider in this chapter, and\\nit is the default in cv_quap.\\nThe key trouble with leave-one-out cross-validation is that, if we have 1000 observations,\\nthat means computing 1000 posterior distributions. That can be time consuming. Luckily,\\nthere are clever ways to approximate the cross-validation score without actually running the\\nmodel over and over again. One approach is to use the “importance” of each observation to\\nthe posterior distribution. What “importance” means here is that some observations have\\na larger impact on the posterior distribution—if we remove an important observation, the\\nposterior changes more. Other observations have less impact. It is a benign aspect of the uni-\\nverse that this importance can be estimated without refitting the model.114 The key intuition\\nis that an observation that is relatively unlikely is more important than one that is relatively\\nexpected. When your expectations are violated, you should change your expectation more.\\nBayesian inference works the same way. This importance is often called a weight, and these\\nweights can be used to estimate a model’s out-of-sample accuracy.\\nSmuggling a bunch of mathematical details under the carpet, this strategy results in a\\nuseful approximation of the cross-validation score. The approximation goes by the awkward\\nname of Pareto-smoothed importance sampling cross-validation.115 We’ll call\\nit PSIS for short, and the PSIS function will compute it. PSIS uses importance sampling,\\nwhich just means that it uses the importance weights approach described in the previous\\nparagraph. The Pareto-smoothing is a technique for making the importance weights more\\nreliable. Pareto is the name of a small town in northern Italy. But it is also the name of\\nan Italian scientist, Vilfredo Pareto (1848–1923), who made many important contributions.\\nOne of these is known as the Pareto distribution. PSIS uses this distribution to derive\\n'},\n",
       " {'index': 236,\n",
       "  'number': 218,\n",
       "  'content': '218\\n7. ULYSSES’ COMPASS\\nmore reliable cross-validation score, without actually doing any cross-validation. If you want\\na little more detail, see the Overthinking box below.\\nThe best feature of PSIS is that it provides feedback about its own reliability. It does this\\nby noting particular observations with very high weights that could make the PSIS score\\ninaccurate. We’ll look at this in much more detail both later in this chapter and in several\\nexamples in the remainder of the book.\\nAnother nice feature of cross-validation and PSIS as an approximation is that it is com-\\nputed point by point. This pointwise nature provides an approximate—sometimes very\\napproximate—estimate of the standard error of our estimate of out-of-sample deviance. To\\ncompute this standard error, we calculate the CV or PSIS score for each observation and then\\nexploit the central limit theorem to provide a measure of the standard error:\\nspsis =\\nq\\nN var(psisi)\\nwhere N is the number of observations and psisi is the PSIS estimate for observation i. If this\\ndoesn’t quite make sense, be sure to look at the code box at the end of this section (page 222).\\nOverthinking: Pareto-smoothed cross-validation. Cross-validation estimates the out-of-sample\\nlog-pointwise-predictive-density (lppd, page 210). If you have N observations and fit the\\nmodel N times, dropping a single observation yi each time, then the out-of-sample lppd is the sum\\nof the average accuracy for each omitted yi.\\nlppdCV =\\nN\\nX\\ni=1\\n1\\nS\\nS\\nX\\ns=1\\nlog Pr(yi|θ−i,s)\\nwhere s indexes samples from a Markov chain and θ−i,s is the s-th sample from the posterior distri-\\nbution computed for observations omitting yi.\\nImportance sampling replaces the computation of N posterior distributions by using an estimate\\nof the importance of each i to the posterior distribution. We draw samples from the full posterior dis-\\ntribution p(θ|y), but we want samples from the reduced leave-one-out posterior distribution p(θ|y−i).\\nSo we re-weight each sample s by the inverse of the probability of the omitted observation:116\\nr(θs) =\\n1\\np(yi|θs)\\nThis weight is only relative, but it is normalized inside the calculation like this:\\nlppdIS =\\nN\\nX\\ni=1\\nlog\\nPS\\ns=1 r(θs)p(yi|θs)\\nPS\\ns=1 r(θs)\\nAnd that is the importance sampling estimate of out-of-sample lppd.\\nWe haven’t done any Pareto smoothing yet, however. The reason we need to is that the weights\\nr(θs) can be unreliable. In particular, if any r(θs) is too relatively large, it can ruin the estimate of lppd\\nby dominating it. One strategy is to truncate the weights so that none are larger than a theoretically\\nderived limit. This helps, but it also biases the estimate. What PSIS does is more clever. It exploits the\\nfact that the distribution of weights should have a particular shape, under some regular conditions.\\nThe largest weights should follow a generalized Pareto distribution:\\np(r|u, σ, k) = σ−1\\x001 + k(r −u)σ−1\\x01−1\\nk −1\\nwhere u is the location parameter, σ is the scale, and k is the shape. For each observation yi, the largest\\nweights are used to estimate a Pareto distribution and then smoothed using that Pareto distribution.\\nThis works quite well, both in theory and practice.117 The best thing about the approach however\\nis that the estimates of k provide information about the reliability of the approximation. There will\\nbe one k value for each yi. Larger k values indicate more influential points, and if k > 0.5, then the\\n'},\n",
       " {'index': 237,\n",
       "  'number': 219,\n",
       "  'content': '7.4. PREDICTING PREDICTIVE ACCURACY\\n219\\nPareto distribution has infinite variance. A distribution with infinite variance has a very thick tail.\\nSince we are trying to smooth the importance weights with the distribution’s tail, an infinite variance\\nmakes the weights harder to trust. Still, both theory and simulation suggest PSIS’s weights perform\\nwell as long as k < 0.7. When we start using PSIS, you’ll see warnings about large k values. These are\\nvery useful for identifying influential observations.\\n7.4.2. Information criteria. The second approach is the use of information criteria\\nto compute an expected score out of sample. Information criteria construct a theoretical\\nestimate of the relative out-of-sample KL divergence.\\nIf you look back at Figure 7.8, there is a curious pattern in the distance between the\\npoints (showing the train-test pairs with flat priors): The difference is approximately twice\\nthe number of parameters in each model. The difference between training deviance and\\ntesting deviance is almost exactly 2 for the first model (with 1 parameter) and about 10 for\\nthe last (with 5 parameters). This is not a coincidence but rather one of the coolest results in\\nmachine learning: For ordinary linear regressions with flat priors, the expected overfitting\\npenalty is about twice the number of parameters.\\nThis is the phenomenon behind information criteria. The best known information\\ncriterion is the Akaike information criterion, abbreviated AIC.118 AIC provides a sur-\\nprisingly simple estimate of the average out-of-sample deviance:\\nAIC = Dtrain + 2p = −2lppd + 2p\\nwhere p is the number of free parameters in the posterior distribution. As the 2 is just there\\nfor scaling, what AIC tells us is that the dimensionality of the posterior distribution is a\\nnatural measure of the model’s overfitting tendency. More complex models tend to overfit\\nmore, directly in proportion to the number of parameters.\\nAIC is of mainly historical interest now. Newer and more general approximations exist\\nthat dominate AIC in every context. But Akaike deserves tremendous credit for the initial\\ninspiration. See the box further down for more details. AIC is an approximation that is\\nreliable only when:\\n(1) The priors are flat or overwhelmed by the likelihood.\\n(2) The posterior distribution is approximately multivariate Gaussian.\\n(3) The sample size N is much greater119 than the number of parameters k.\\nSince flat priors are hardly ever the best priors, we’ll want something more general. And when\\nyou get to multilevel models, the priors are never flat by definition. There is a more general\\ncriterion, the Deviance Information Criterion (DIC). DIC is okay with informative\\npriors, but still assumes that the posterior is multivariate Gaussian and that N ≫k.120\\nOverthinking: The Akaike inspiration criterion. The Akaike Information Criterion is a truly ele-\\ngant result. Hirotugu Akaike (赤池弘次, 1927–2009) explained how the insight came to him: “On\\nthe morning of March 16, 1971, while taking a seat in a commuter train, I suddenly realized that the\\nparameters of the factor analysis model were estimated by maximizing the likelihood and that the\\nmean value of the logarithmus of the likelihood was connected with the Kullback-Leibler information\\nnumber.”121 Must have been some train. What was at the heart of Akaike’s realization? Mechanically,\\nderiving AIC means writing down the goal, which is the expected KL divergence, and then making\\napproximations. The expected bias turns out to be proportional to the number of parameters, pro-\\nvided a number of assumptions are approximately correct.\\n'},\n",
       " {'index': 238,\n",
       "  'number': 220,\n",
       "  'content': '220\\n7. ULYSSES’ COMPASS\\nWe’ll focus on a criterion that is more general than both AIC and DIC. Sumio Watanabe’s\\n(渡辺澄夫) Widely Applicable Information Criterion (WAIC) makes no assump-\\ntion about the shape of the posterior.122 It provides an approximation of the out-of-sample\\ndeviance that converges to the cross-validation approximation in a large sample. But in a\\nfinite sample, it can disagree. It can disagree because it has a different target—it isn’t trying\\nto approximate the cross-validation score, but rather guess the out-of-sample KL divergence.\\nIn the large-sample limit, these tend to be the same.\\nHow do we compute WAIC? Unfortunately, it’s generality comes at the expense of a more\\ncomplicated formula. But really it just has two pieces, and you can compute both directly\\nfrom samples from the posterior distribution. WAIC is just the log-posterior-predictive-\\ndensity (lppd, page 210) that we calculated earlier plus a penalty proportional to the variance\\nin the posterior predictions:\\nWAIC(y, Θ) = −2\\n\\x00lppd −\\nX\\ni\\nvarθ log p(yi|θ)\\n|\\n{z\\n}\\npenalty term\\n\\x01\\nwhere y is the observations and Θ is the posterior distribution. The penalty term means,\\n“compute the variance in log-probabilities for each observation i, and then sum up these\\nvariances to get the total penalty.” So you can think of each observation as having its own\\npersonal penalty score. And since these scores measure overfitting risk, you can also assess\\noverfitting risk at the level of each observation.\\nBecause of the analogy to Akaike’s original criterion, the penalty term in WAIC is some-\\ntimes called the effective number of parameters, labeled pwaic. This label makes histor-\\nical sense, but it doesn’t make much mathematical sense. As we’ll see as the book progresses,\\nthe overfitting risk of a model has less to do with the number of parameters than with how\\nthe parameters are related to one another. When we get to multilevel models, adding param-\\neters to the model can actually reduce the “effective number of parameters.” Like English\\nlanguage spelling, the field of statistics is full of historical baggage that impedes learning.\\nNo one chose this situation. It’s just cultural evolution. I’ll try to call the penalty term “the\\noverfitting penalty.” But if you see it called the effective number of parameters elsewhere,\\nyou’ll know it is the same thing.\\nThe function WAIC in the rethinking package will compute WAIC for a model fit with\\nquap or ulam or rstan (which we’ll use later in the book). If you want to see a didactic\\nimplementation of computing lppd and the penalty term, see the Overthinking box at the\\nend of this section. Seeing the mathematical formula above as computer code may be what\\nyou need to understand it.\\nLike PSIS, WAIC is pointwise. Prediction is considered case-by-case, or point-by-point,\\nin the data. Several things arise from this. First, WAIC also has an approximate standard\\nerror (see calculation in the Overthinking box on page 222). Second, since some observa-\\ntions have stronger influence on the posterior distribution, WAIC notes this in its pointwise\\npenalty terms. Third, just like cross-validation and PSIS, because WAIC allows splitting up\\nthe data into independent observations, it is sometimes hard to define. Consider for example\\na model in which each prediction depends upon a previous observation. This happens, for\\nexample, in a time series. In a time series, a previous observation becomes a predictor vari-\\nable for the next observation. So it’s not easy to think of each observation as independent or\\nexchangeable. In such a case, you can of course compute WAIC as if each observation were\\nindependent of the others, but it’s not clear what the resulting value means.\\n'},\n",
       " {'index': 239,\n",
       "  'number': 221,\n",
       "  'content': '7.4. PREDICTING PREDICTIVE ACCURACY\\n221\\nThis caution raises a more general issue with all strategies to guess out-of-sample accu-\\nracy: Their validity depends upon the predictive task you have in mind. And not all predic-\\ntion can reasonably take the form that we’ve been assuming for the train-test simulations in\\nthis chapter. When we consider multilevel models, this issue will arise again.\\nRethinking: Information criteria and consistency. As mentioned previously, information criteria\\nlike AIC and WAIC do not always assign the best expected Dtest to the “true” model. In statisti-\\ncal jargon, information criteria are not consistent for model identification. These criteria aim to\\nnominate the model that will produce the best predictions, as judged by out-of-sample deviance, so\\nit shouldn’t surprise us that they do not also do something that they aren’t designed to do. Other\\nmetrics for model comparison are however consistent. So are information criteria broken?\\nThey are not broken, if you care about prediction.123 Issues like consistency are nearly always\\nevaluated asymptotically. This means that we imagine the sample size N approaching infinity. Then\\nwe ask how a procedure behaves in this large-data limit. With practically infinite data, AIC and\\nWAIC and cross-validation will often select a more complex model, so they are sometimes accused\\nof “overfitting.” But at the large-data limit, the most complex model will make predictions identical\\nto the true model (assuming it exists in the model set). The reason is that with so much data every\\nparameter can be very precisely estimated. And so using an overly complex model will not hurt\\nprediction. For example, as sample size N →∞the model with 5 parameters in Figure 7.8 will tell\\nyou that the coefficients for predictors after the second are almost exactly zero. Therefore failing to\\nidentify the “correct” model does not hurt us, at least not in this sense. Furthermore, in the natural\\nand social sciences the models under consideration are almost never the data-generating models. It\\nmakes little sense to attempt to identify a “true” model.\\nRethinking: What about BIC and Bayes factors? The Bayesian information criterion, abbre-\\nviated BIC and also known as the Schwarz criterion,124 is more commonly juxtaposed with AIC. The\\nchoice between BIC or AIC (or neither!) is not about being Bayesian or not. There are both Bayesian\\nand non-Bayesian ways to motivate both, and depending upon how strict one wishes to be, neither\\nis Bayesian. BIC is related to the logarithm of the average likelihood of a linear model. The average\\nlikelihood is the denominator in Bayes’ theorem, the likelihood averaged over the prior. There is a\\nvenerable tradition in Bayesian inference of comparing average likelihoods as a means to comparing\\nmodels. A ratio of average likelihoods is called a Bayes factor. On the log scale, these ratios are\\ndifferences, and so comparing differences in average likelihoods resembles comparing differences in\\ninformation criteria. Since average likelihood is averaged over the prior, more parameters induce a\\nnatural penalty on complexity. This helps guard against overfitting, even though the exact penalty is\\nnot the same as with information criteria.\\nMany Bayesian statisticians dislike the Bayes factor approach,125 and all admit that there are\\ntechnical obstacles to its use. One problem is that computing average likelihood is hard. Even when\\nyou can compute the posterior, you may not be able to estimate the average likelihood. Another\\nproblem is that, even when priors are weak and have little influence on posterior distributions within\\nmodels, priors can have a huge impact on comparisons between models.\\nIt’s important to realize, though, that the choice of Bayesian or not does not also decide between\\ninformation criteria or Bayes factors. Moreover, there’s no need to choose, really. We can always\\nuse both and learn from the ways they agree and disagree. And both information criteria and Bayes\\nfactors are purely predictive criteria that will happily select confounded models. They know nothing\\nabout causation.\\n'},\n",
       " {'index': 240,\n",
       "  'number': 222,\n",
       "  'content': '222\\n7. ULYSSES’ COMPASS\\nOverthinking: WAIC calculations. To see how the WAIC calculations actually work, consider a\\nsimple regression fit with quap:\\nR code\\n7.19\\ndata(cars)\\nm <- quap(\\nalist(\\ndist ~ dnorm(mu,sigma),\\nmu <- a + b*speed,\\na ~ dnorm(0,100),\\nb ~ dnorm(0,10),\\nsigma ~ dexp(1)\\n) , data=cars )\\nset.seed(94)\\npost <- extract.samples(m,n=1000)\\nWe’ll need the log-likelihood of each observation i at each sample s from the posterior:\\nR code\\n7.20\\nn_samples <- 1000\\nlogprob <- sapply( 1:n_samples ,\\nfunction(s) {\\nmu <- post$a[s] + post$b[s]*cars$speed\\ndnorm( cars$dist , mu , post$sigma[s] , log=TRUE )\\n} )\\nYou end up with a 50-by-1000 matrix of log-likelihoods, with observations in rows and samples in\\ncolumns. Now to compute lppd, the Bayesian deviance, we average the samples in each row, take\\nthe log, and add all of the logs together. However, to do this with precision, we need to do all of the\\naveraging on the log scale. This is made easy with a function log_sum_exp, which computes the log\\nof a sum of exponentiated terms. Then we can just subtract the log of the number of samples. This\\ncomputes the log of the average.\\nR code\\n7.21\\nn_cases <- nrow(cars)\\nlppd <- sapply( 1:n_cases , function(i) log_sum_exp(logprob[i,]) - log(n_samples) )\\nTyping sum(lppd) will give you lppd, as defined in the main text. Now for the penalty term, pWAIC.\\nThis is more straightforward, as we just compute the variance across samples for each observation,\\nthen add these together:\\nR code\\n7.22\\npWAIC <- sapply( 1:n_cases , function(i) var(logprob[i,]) )\\nAnd sum(pWAIC) returns pWAIC, as defined in the main text. To compute WAIC:\\nR code\\n7.23\\n-2*( sum(lppd) - sum(pWAIC) )\\n[1] 423.3154\\nCompare to the output of the WAIC function. There will be simulation variance, because of how the\\nsamples are drawn from the quap fit. But that variance remains much smaller than the standard error\\nof WAIC itself. You can compute the standard error by computing the square root of number of cases\\nmultiplied by the variance over the individual observation terms in WAIC:\\nR code\\n7.24\\nwaic_vec <- -2*( lppd - pWAIC )\\nsqrt( n_cases*var(waic_vec) )\\n'},\n",
       " {'index': 241,\n",
       "  'number': 223,\n",
       "  'content': '7.4. PREDICTING PREDICTIVE ACCURACY\\n223\\n[1] 17.81628\\nAs models get more complicated, all that usually changes is how the log-probabilities, logprob, are\\ncomputed.\\nNote that each individual observation has its own penalty term in the pWAIC vector we calculated\\nabove. This provides an interesting opportunity to study how different observations contribute to\\noverfitting. You can get the same vectorized pointwise output from the WAIC function by using the\\npointwise=TRUE argument.\\n7.4.3. Comparing CV, PSIS, and WAIC. With definitions of cross-validation, PSIS, and\\nWAIC in hand, let’s conduct another simulation exercise. This will let us visualize the esti-\\nmates of out-of-sample deviance that these criteria provide, in the same familiar context as\\nearlier sections. Our interest for now is in seeing how well the criteria approximate out-of-\\nsample accuracy. Can they guess the overfitting risk?\\nFigure 7.9 shows the results of 1000 simulations each for the five familiar models with\\nbetween 1 and 5 parameters, simulated under two different sets of priors and two different\\nsample sizes. The plot is complicated. But taking it one piece at a time, all the parts are\\nalready familiar. Focus for now just on the top-left plot, where N = 20. The vertical axis is\\nthe out-of-sample deviance (−2 × lppd). The open points show the average out-of-sample\\ndeviance for models fit with flat priors. The filled points show the average out-of-sample\\ndeviance for models fit with regularizing priors with a standard deviation of 0.5. Notice that\\nthe regularizing priors overfit less, just as you saw in the previous section about regularizing\\npriors. So that isn’t new.\\nWe are interested now in how well CV, PSIS, and WAIC approximate these points. Still\\nfocusing on the top-left plot in Figure 7.9, there are trend lines for each criterion. Solid\\nblack trends show WAIC. Solid blue trends show full cross-validation, computed by fitting\\nthe model N times. The dashed blue trends are PSIS. Notice that all three criteria do a good\\njob of guessing the average out-of-sample score, whether the models used flat (upper trends)\\nor regularizing (lower trends) priors. Provided the process generating data remains the same,\\nit really is possible to use a single sample to guess the accuracy of our predictions.\\nWhile all three criteria get the expected out-of-sample deviance approximately correct,\\nit is also true that in any particular sample they usually miss it by some amount. So we\\nshould look at the average error as well. The upper-right plot makes the average error of each\\nmeasure easier to see. Now the vertical axis is the average absolute difference between the\\nout-of-sample deviance and each criterion. WAIC (black trend) is slightly better on average.\\nThe bottom row repeats these plots for a larger sample size, N = 100. With a sample this\\nlarge, in a family of models this simple, all three criteria become identical.\\nPSIS and WAIC perform very similarly in the context of ordinary linear models.126 If\\nthere are important differences, they lie in other model types, where the posterior distribu-\\ntion is not approximately Gaussian or in the presence of observations that strongly influence\\nthe posterior. CV and PSIS have higher variance as estimators of the KL divergence, while\\nWAIC has greater bias. So we should expect each to be slightly better in different contexts.127\\nHowever, in practice any advantage may be much smaller than the expected error. Watan-\\nabe recommends computing both WAIC and PSIS and contrasting them. If there are large\\ndifferences, this implies one or both criteria are unreliable.\\nEstimation aside, PSIS has a distinct advantage in warning the user about when it is\\nunreliable. The k values that PSIS computes for each observation indicate when the PSIS\\n'},\n",
       " {'index': 242,\n",
       "  'number': 224,\n",
       "  'content': '224\\n7. ULYSSES’ COMPASS\\n1\\n2\\n3\\n4\\n5\\n56.0\\n57.0\\n58.0\\n59.0\\nnumber of parameters\\naverage deviance\\nWAIC\\nPSIS\\nCV\\ntest\\nN = 20\\n1\\n2\\n3\\n4\\n5\\n6.0\\n6.5\\n7.0\\nnumber of parameters\\naverage error (test deviance)\\nN = 20\\nflat\\nprior\\nsigma =\\n0.5\\n1\\n2\\n3\\n4\\n5\\n270\\n275\\n280\\n285\\nnumber of parameters\\naverage deviance\\nN = 100\\n1\\n2\\n3\\n4\\n5\\n13.0\\n14.0\\n15.0\\nnumber of parameters\\naverage error (test deviance)\\nN = 100\\nflat\\nprior\\nsigma =\\n0.5\\nFigure 7.9. WAIC and cross-validation as estimates of the out-of-sample\\ndeviance. The top row displays 1000 train-test simulations with N = 20.\\nThe bottom row shows 1000 simulations with N = 1000. In each plot, there\\nare two sets of trends. The open points are unregularized. The filled points\\nare for regularizing σ = 0.5 priors. Left: The vertical axis is absolute de-\\nviance. Points are the average test deviance. The black line is the average\\nWAIC estimate. Blue is the leave-one-out cross-validation (CV) score, and\\ndashed blue is the PSIS approximation of the cross-validation score. Right:\\nThe same data, but now shown on the scale of average error in approximat-\\ning the test deviance.\\nscore may be unreliable, as well as identify which observations are at fault. We’ll see later\\nhow useful this can be.\\nRethinking: Diverse prediction frameworks. The train-test gambit we’ve been using in this chapter\\nentails predicting a test sample of the same size and nature as the training sample. This most certainly\\ndoes not mean that information criteria can only be used when we plan to predict a sample of the\\nsame size as training. The same size just scales the out-of-sample deviance similarly. It is the distance\\nbetween the models that is useful, not the absolute value of the deviance. Nor do cross-validation and\\ninformation criteria require that the data generating model be one of the models being considered.\\nThat was true in our simulations. But it isn’t a requirement for them to help in identifying good\\nmodels for prediction.\\n'},\n",
       " {'index': 243,\n",
       "  'number': 225,\n",
       "  'content': '7.5. MODEL COMPARISON\\n225\\nBut the train-test prediction task is not representative of everything we might wish to do with\\nmodels. For example, some statisticians prefer to evaluate predictions using a prequential frame-\\nwork, in which models are judged on their accumulated learning error over the training sample.128\\nAnd once you start using multilevel models, “prediction” is no longer uniquely defined, because the\\ntest sample can differ from the training sample in ways that forbid use of some the parameter esti-\\nmates. We’ll worry about that issue in Chapter 13.\\nPerhaps a larger concern is that our train-test thought experiment pulls the test sample from\\nexactly the same process as the training sample. This is a kind of uniformitarian assumption, in which\\nfuture data are expected to come from the same process as past data and have the same rough range of\\nvalues. This can cause problems. For example, suppose we fit a regression that predicts height using\\nbody weight. The training sample comes from a poor town, in which most people are pretty thin.\\nThe relationship between height and weight turns out to be positive and strong. Now also suppose\\nour prediction goal is to guess the heights in another, much wealthier, town. Plugging the weights\\nfrom the wealthy individuals into the model fit to the poor individuals will predict outrageously tall\\npeople. The reason is that, once weight becomes large enough, it has essentially no relationship with\\nheight. WAIC will not automatically recognize nor solve this problem. Nor will any other isolated\\nprocedure. But over repeated rounds of model fitting, attempts at prediction, and model criticism, it\\nis possible to overcome this kind of limitation. As always, statistics is no substitute for science.\\n7.5. Model comparison\\nLet’s review the original problem and the road so far. When there are several plausi-\\nble (and hopefully un-confounded) models for the same set of observations, how should we\\ncompare the accuracy of these models? Following the fit to the sample is no good, because\\nfit will always favor more complex models. Information divergence is the right measure of\\nmodel accuracy, but even it will just lead us to choose more and more complex and wrong\\nmodels. We need to somehow evaluate models out-of-sample. How can we do that? A meta-\\nmodel of forecasting tells us two important things. First, flat priors produce bad predictions.\\nRegularizing priors—priors which are skeptical of extreme parameter values—reduce fit to\\nsample but tend to improve predictive accuracy. Second, we can get a useful guess of predic-\\ntive accuracy with the criteria CV, PSIS, and WAIC. Regularizing priors and CV/PSIS/WAIC\\nare complementary. Regularization reduces overfitting, and predictive criteria measure it.\\nThat’s the road so far, the conceptual journey. And that’s the hardest part. Using tools\\nlike PSIS and WAIC is much easier than understanding them. Which makes them quite\\ndangerous. That is why this chapter has spent so much time on foundations, without doing\\nany actual data analysis.\\nNow let’s do some analysis. How do we use regularizing priors and CV/PSIS/WAIC?\\nA very common use of cross-validation and information criteria is to perform model se-\\nlection, which means choosing the model with the lowest criterion value and then dis-\\ncarding the others. But you should never do this. This kind of selection procedure dis-\\ncards the information about relative model accuracy contained in the differences among the\\nCV/PSIS/WAIC values. Why are the differences useful? Because sometimes the differences\\nare large and sometimes they are small. Just as relative posterior probability provides ad-\\nvice about how confident we might be about parameters (conditional on the model), relative\\nmodel accuracy provides advice about how confident we might be about models (conditional\\non the set of models compared).\\n'},\n",
       " {'index': 244,\n",
       "  'number': 226,\n",
       "  'content': '226\\n7. ULYSSES’ COMPASS\\nAnother reason to never select models based upon WAIC/CV/PSIS alone is that we\\nmight care about causal inference. Maximizing expected predictive accuracy is not the same\\nas inferring causation. Highly confounded models can still make good predictions, at least\\nin the short term. They won’t tell us the consequences of an intervention, but they might\\nhelp us forecast. So we need to be clear about our goals and not just toss variables into the\\ncausal salad and let WAIC select our meal.\\nSo what good are these criteria then? They measure expected predictive value of a vari-\\nable on the right scale, accounting for overfitting. This helps in testing model implications,\\ngiven a set of causal models. They also provide a way to measure the overfitting tendency\\nof a model, and that helps us both design models and understand how statistical inference\\nworks. Finally, minimizing a criterion like WAIC can help in designing models, especially\\nin tuning parameters in multilevel models.\\nSo instead of model selection, we’ll focus on model comparison. This is a more general\\napproach that uses multiple models to understand both how different variables influence\\npredictions and, in combination with a causal model, implied conditional independencies\\namong variables help us infer causal relationships.\\nWe’ll work through two examples. The first emphasizes the distinction between compar-\\ning models for predictive performance versus comparing them in order to infer causation.\\nThe second emphasizes the pointwise nature of model comparison and what inspecting in-\\ndividual points can reveal about model performance and mis-specification. This second ex-\\nample also introduces a more robust alternative to Gaussian regression.\\n7.5.1. Model mis-selection. We must keep in mind the lessons of the previous chapters: In-\\nferring cause and making predictions are different tasks. Cross-validation and WAIC aim\\nto find models that make good predictions. They don’t solve any causal inference problem.\\nIf you select a model based only on expected predictive accuracy, you could easily be con-\\nfounded. The reason is that backdoor paths do give us valid information about statistical\\nassociations in the data. So they can improve prediction, as long as we don’t intervene in the\\nsystem and the future is like the past. But recall that our working definition of knowing a\\ncause is that we can predict the consequences of an intervention. So a good PSIS or WAIC\\nscore does not in general indicate a good causal model.\\nFor example, recall the plant growth example from the previous chapter. The model that\\nconditions on fungus will make better predictions than the model that omits it. If you return\\nto that section (page 171) and run models m6.6, m6.7, and m6.8 again, we can compare\\ntheir WAIC values. To remind you, m6.6 is the model with just an intercept, m6.7 is the\\nmodel that includes both treatment and fungus (the post-treatment variable), and m6.8 is\\nthe model that includes treatment but omits fungus. It’s m6.8 that allows us to correctly infer\\nthe causal influence of treatment.\\nTo begin, let’s use the WAIC convenience function to calculate WAIC for m6.7:\\nR code\\n7.25\\nset.seed(11)\\nWAIC( m6.7 )\\nWAIC\\nlppd penalty\\nstd_err\\n1 361.4511 -177.1724\\n3.5532 14.17035\\nThe first value is the guess for the out-of-sample deviance. The other values are (in order):\\nlppd, the effective number of parameters penalty, and the standard error of the WAIC value.\\nThe Overthinking box in the previous section shows how to calculate these numbers from\\n'},\n",
       " {'index': 245,\n",
       "  'number': 227,\n",
       "  'content': '7.5. MODEL COMPARISON\\n227\\nscratch. To make it easier to compare multiple models, the rethinking package provides a\\nconvenience function, compare:\\nR code\\n7.26\\nset.seed(77)\\ncompare( m6.6 , m6.7 , m6.8 , func=WAIC )\\nWAIC\\nSE dWAIC\\ndSE pWAIC weight\\nm6.7 361.9 14.26\\n0.0\\nNA\\n3.8\\n1\\nm6.8 402.8 11.28\\n40.9 10.48\\n2.6\\n0\\nm6.6 405.9 11.66\\n44.0 12.23\\n1.6\\n0\\nPSIS will give you almost identical values. You can add func=PSIS to the compare call to\\ncheck. What do all of these numbers mean? Each row is a model. Columns from left to right\\nare: WAIC, standard error (SE) of WAIC, difference of each WAIC from the best model,\\nstandard error (dSE) of this difference, prediction penalty (pWAIC), and finally the Akaike\\nweight. Each of these needs a lot more explanation.\\nThe first column contains the WAIC values. Smaller values are better, and the models are\\nordered by WAIC, from best to worst. The model that includes the fungus variable has the\\nsmallest WAIC, as promised. The pWAIC column is the penalty term of WAIC. These values\\nare close to, but slightly below, the number of dimensions in the posterior of each model,\\nwhich is to be expected in linear regressions with regularizing priors. These penalties are\\nmore interesting later on in the book.\\nThe dWAIC column is the difference between each model’s WAIC and the best WAIC in\\nthe set. So it’s zero for the best model and then the differences with the other models tell\\nyou how far apart each is from the top model. So m6.7 is about 40 units of deviance smaller\\nthan both other models. The intercept model, m6.6, is 3 units worse than m6.8. Are these\\nbig differences or small differences? One way to answer that is to ask a clearer question:\\nAre the models easily distinguished by their expected out-of-sample accuracy? To answer\\nthat question, we need to consider the error in the WAIC estimates. Since we don’t have the\\ntarget sample, these are just guesses, and we know from the simulations that there is a lot of\\nvariation in WAIC’s error.\\nThat is what the two standard error columns, SE and dSE, are there to help us with. SE\\nis the approximate standard error of each WAIC. In a very approximate sense, we expect\\nthe uncertainty in out-of-sample accuracy to be normally distributed with mean equal to\\nthe reported WAIC value and a standard deviation equal to the standard error. When the\\nsample is small, this approximation tends to dramatically underestimate the uncertainty. But\\nit is still better than older criteria like AIC, which provide no way to gauge their uncertainty.\\nNow to judge whether two models are easy to distinguish, we don’t use their standard\\nerrors but rather the standard error of their difference. What does that mean? Just like\\neach WAIC value, each difference in WAIC values also has a standard error. To compute the\\nstandard error of the difference between models m6.7 and m6.8, we just need the pointwise\\nbreakdown of the WAIC values:\\nR code\\n7.27\\nset.seed(91)\\nwaic_m6.7 <- WAIC( m6.7 , pointwise=TRUE )$WAIC\\nwaic_m6.8 <- WAIC( m6.8 , pointwise=TRUE )$WAIC\\nn <- length(waic_m6.7)\\ndiff_m6.7_m6.8 <- waic_m6.7 - waic_m6.8\\n'},\n",
       " {'index': 246,\n",
       "  'number': 228,\n",
       "  'content': '228\\n7. ULYSSES’ COMPASS\\nsqrt( n*var( diff_m6.7_m6.8 ) )\\n[1] 10.35785\\nThis is the value in the second row of the compare table. It’s slightly different, only because\\nof simulation variance. The difference between the models is 40.9 and the standard error is\\nabout 10.4. If we imagine the 99% (corresponding to a z-score of about 2.6) interval of the\\ndifference, it’ll be about:\\nR code\\n7.28\\n40.0 + c(-1,1)*10.4*2.6\\n[1] 12.96 67.04\\nSo yes, these models are very easy to distinguish by expected out-of-sample accuracy. Model\\nm6.7 is a lot better. You might be able to see all of this better, if we plot the compare table:\\nR code\\n7.29\\nplot( compare( m6.6 , m6.7 , m6.8 ) )\\nm6.6\\nm6.8\\nm6.7\\n350\\n360\\n370\\n380\\n390\\n400\\n410\\n420\\ndeviance\\nWAIC\\nThe filled points are the in-sample deviance values. The open points are the WAIC values.\\nNotice that naturally each model does better in-sample than it is expected to do out-of-\\nsample. The line segments show the standard error of each WAIC. These are the values in\\nthe column labeled SE in the table above. So you can probably see how much better m6.7\\nis than m6.8. What we really want however is the standard error of the difference in WAIC\\nbetween the two models. That is shown by the lighter line segment with the triangle on it,\\nbetween m6.7 and m6.8.\\nWhat does all of this mean? It means that WAIC cannot be used to infer causation.\\nWe know, because we simulated these data, that the treatment matters. But because fungus\\nmediates treatment—it is on a pipe between treatment and the outcome—once we condition\\non fungus, treatment provides no additional information. And since fungus is more highly\\ncorrelated with the outcome, a model using it is likely to predict better. WAIC did its job. Its\\njob is not to infer causation. Its job is to guess predictive accuracy.\\nThat doesn’t mean that WAIC (or CV or PSIS) is useless here. It does provide a useful\\nmeasure of the expected improvement in prediction that comes from conditioning on the\\nfungus. Although the treatment works, it isn’t 100% effective, and so knowing the treatment\\nis no substitute for knowing whether fungus is present.\\nSimilarly, we can ask about the difference between models m6.8, the model with treat-\\nment only, and model m6.6, the intercept model. Model m6.8 provides pretty good evidence\\nthat the treatment works. You can inspect the posterior again, if you have forgotten. But\\nWAIC thinks these two models are quite similar. Their difference is only 3 units of deviance.\\nLet’s calculate the standard error of the difference, to highlight the issue:\\n'},\n",
       " {'index': 247,\n",
       "  'number': 229,\n",
       "  'content': '7.5. MODEL COMPARISON\\n229\\nR code\\n7.30\\nset.seed(92)\\nwaic_m6.6 <- WAIC( m6.6 , pointwise=TRUE )$WAIC\\ndiff_m6.6_m6.8 <- waic_m6.6 - waic_m6.8\\nsqrt( n*var( diff_m6.6_m6.8 ) )\\n[1] 4.858914\\nThe compare table doesn’t show this value, but it did calculate it. To see it, you need the dSE\\nslot of the return:\\nR code\\n7.31\\nset.seed(93)\\ncompare( m6.6 , m6.7 , m6.8 )@dSE\\nm6.6\\nm6.7\\nm6.8\\nm6.6\\nNA 12.20638\\n4.934353\\nm6.7 12.206380\\nNA 10.426576\\nm6.8\\n4.934353 10.42658\\nNA\\nThis matrix contains all of the pairwise difference standard errors for the models you com-\\npared. Notice that the standard error of the difference for m6.6 and m6.8 is bigger than the\\ndifference itself. We really cannot easily distinguish these models on the basis of WAIC. Note\\nthat these contrasts are possibly less reliable than the standard errors on each model. There\\nisn’t much analytical work on these contrasts yet, but before long there should be.129\\nDoes this mean that the treatment doesn’t work? Of course not. We know that it works.\\nWe simulated the data. And the posterior distribution of the treatment effect, bt in m6.8,\\nis reliably positive. But it isn’t especially large. So it doesn’t do much alone to improve pre-\\ndiction of plant height. There are just too many other sources of variation. This result just\\nechoes the core fact about WAIC (and CV and PSIS): It guesses predictive accuracy, not\\ncausal truth. A variable can be causally related to an outcome, but have little relative im-\\npact on it, and WAIC will tell you that. That is what is happening in this case. We can use\\nWAIC/CV/PSIS to measure how big a difference some variable makes in prediction. But we\\ncannot use these criteria to decide whether or not some effect exists. We need the posterior\\ndistributions of multiple models, maybe examining the implied conditional independencies\\nof a relevant causal graph, to do that.\\nThe last element of the compare table is the column we skipped over, weight. These\\nvalues are a traditional way to summarize relative support for each model. They always sum\\nto 1, within a set of compared models. The weight of a model i is computed as:\\nwi =\\nexp(−0.5∆i)\\nP\\nj exp(−0.5∆j)\\nwhere ∆i is the difference between model i’s WAIC value and the best WAIC in the set.\\nThese are the dWAIC values in the table. These weights can be a quick way to see how big the\\ndifferences are among models. But you still have to inspect the standard errors. Since the\\nweights don’t reflect the standard errors, they are simply not sufficient for model comparison.\\nWeights are also used in model averaging. Model averaging is a family of methods for\\ncombining the predictions of multiple models. For the sake of space, we won’t cover it in\\nthis book. But see the endnote for some places to start.130\\n'},\n",
       " {'index': 248,\n",
       "  'number': 230,\n",
       "  'content': '230\\n7. ULYSSES’ COMPASS\\nRethinking: WAIC metaphors. Here are two metaphors to help explain the concepts behind using\\nWAIC (or another information criterion) to compare models.\\nThink of models as race horses. In any particular race, the best horse may not win. But it’s more\\nlikely to win than is the worst horse. And when the winning horse finishes in half the time of the\\nsecond-place horse, you can be pretty sure the winning horse is also the best. But if instead it’s a photo-\\nfinish, with a near tie between first and second place, then it is much harder to be confident about\\nwhich is the best horse. WAIC values are analogous to these race times—smaller values are better,\\nand the distances between the horses/models are informative. Akaike weights transform differences\\nin finishing time into probabilities of being the best model/horse on future data/races. But if the track\\nconditions or jockey changes, these probabilities may mislead. Forecasting future racing/prediction\\nbased upon a single race/fit carries no guarantees.\\nThink of models as stones thrown to skip on a pond. No stone will ever reach the other side\\n(perfect prediction), but some sorts of stones make it farther than others, on average (make better\\ntest predictions). But on any individual throw, lots of unique conditions avail—the wind might pick\\nup or change direction, a duck could surface to intercept the stone, or the thrower’s grip might slip. So\\nwhich stone will go farthest is not certain. Still, the relative distances reached by each stone therefore\\nprovide information about which stone will do best on average. But we can’t be too confident about\\nany individual stone, unless the distances between stones is very large.\\nOf course neither metaphor is perfect. Metaphors never are. But many people find these to be\\nhelpful in interpreting information criteria.\\n7.5.2. Outliers and other illusions. In the divorce example from Chapter 5, we saw in the\\nposterior predictions that a few States were very hard for the model to retrodict. The State\\nof Idaho in particular was something of an outlier (page 5.5). Individual points like Idaho\\ntend to be very influential in ordinary regression models. Let’s see how PSIS and WAIC\\nrepresent that importance. Begin by refitting the three divorce models from Chapter 5.\\nR code\\n7.32\\nlibrary(rethinking)\\ndata(WaffleDivorce)\\nd <- WaffleDivorce\\nd$A <- standardize( d$MedianAgeMarriage )\\nd$D <- standardize( d$Divorce )\\nd$M <- standardize( d$Marriage )\\nm5.1 <- quap(\\nalist(\\nD ~ dnorm( mu , sigma ) ,\\nmu <- a + bA * A ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbA ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data = d )\\nm5.2 <- quap(\\nalist(\\nD ~ dnorm( mu , sigma ) ,\\nmu <- a + bM * M ,\\na ~ dnorm( 0 , 0.2 ) ,\\n'},\n",
       " {'index': 249,\n",
       "  'number': 231,\n",
       "  'content': '7.5. MODEL COMPARISON\\n231\\nbM ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data = d )\\nm5.3 <- quap(\\nalist(\\nD ~ dnorm( mu , sigma ) ,\\nmu <- a + bM*M + bA*A ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbM ~ dnorm( 0 , 0.5 ) ,\\nbA ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data = d )\\nLook at the posterior summaries, just to remind yourself that marriage rate (M) has little\\nassociation with divorce rate (D), once age at marriage (A) is included in m5.3. Now let’s\\ncompare these models using PSIS:\\nR code\\n7.33\\nset.seed(24071847)\\ncompare( m5.1 , m5.2 , m5.3 , func=PSIS )\\nPSIS\\nSE dPSIS\\ndSE pPSIS weight\\nm5.1 127.6 14.69\\n0.0\\nNA\\n4.7\\n0.71\\nm5.3 129.4 15.10\\n1.8\\n0.90\\n5.9\\n0.29\\nm5.2 140.6 11.21\\n13.1 10.82\\n3.8\\n0.00\\nThere are two important things to consider here. First note that the model that omits mar-\\nriage rate, m5.1, lands on top. This is because marriage rate has very little association with\\nthe outcome. So the model that omits it has slightly better expected out-of-sample perfor-\\nmance, even though it actually fits the sample slightly worse than m5.3, the model with both\\npredictors. The difference between the top two models is only 1.8, with a standard error of\\n0.9, so the models make very similar predictions. This is the typical pattern, whenever some\\npredictor has a very small association with the outcome.\\nSecond, in addition to the table above, you should also receive a message:\\nSome Pareto k values are very high (>1).\\nThis means that the smoothing approximation that PSIS uses is unreliable for some points.\\nRecall from the section on PSIS that when a point’s Pareto k value is above 0.5, the impor-\\ntance weight can be unreliable. Furthermore, these points tend to be outliers with unlikely\\nvalues, according to the model. As a result, they are highly influential and make it difficult\\nto estimate out-of-sample predictive accuracy. Why? Because any new sample is unlikely to\\ncontain these same outliers, and since these outliers were highly influential, they could make\\nout-of-sample predictions worse than expected. WAIC is vulnerable to outliers as well. It\\ndoesn’t have an automatic warning. But it does have a way to measure this risk, through the\\nestimate of the overfitting penalty.\\nLet’s look at the individual States, to see which are causing the problem. We can do this by\\nadding pointwise=TRUE to PSIS. When you do this, you get a matrix with each observation\\non a row and the PSIS information, including individual Pareto k values, in columns. I’ll also\\n'},\n",
       " {'index': 250,\n",
       "  'number': 232,\n",
       "  'content': '232\\n7. ULYSSES’ COMPASS\\n0.0\\n0.5\\n1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nPSIS Pareto k\\nWAIC penalty\\nID\\nME\\nGaussian model (m5.3)\\nFigure 7.10. Highly influential points and\\nout-of-sample prediction. The horizontal axis\\nis Pareto k from PSIS. The vertical axis is\\nWAIC’s penalty term. The State of Idaho (ID)\\nhas an extremely unlikely value, according to\\nthe model. As a result it has both a very high\\nPareto k and a large WAIC penalty. Points\\nlike these are highly influential and potentially\\nhurt prediction.\\nplot the individual “penalty” values from WAIC, to show the relationship between Pareto k\\nand the information theoretic prediction penalty.\\nR code\\n7.34\\nset.seed(24071847)\\nPSIS_m5.3 <- PSIS(m5.3,pointwise=TRUE)\\nset.seed(24071847)\\nWAIC_m5.3 <- WAIC(m5.3,pointwise=TRUE)\\nplot( PSIS_m5.3$k , WAIC_m5.3$penalty , xlab=\"PSIS Pareto k\" ,\\nylab=\"WAIC penalty\" , col=rangi2 , lwd=2 )\\nThis plot is shown in Figure 7.10. Individual points are individual States, with Pareto k on\\nthe horizontal axis and WAIC’s penalty term. The State of Idaho (ID, upper-right corner)\\nhas both a very high Pareto k value (above 1) and a large penalty term (over 2). As you saw\\nback in Chapter 5, Idaho has a very low divorce rate for its age at marriage. As a result, it\\nis highly influential—it exerts more influence on the posterior distribution than other States\\ndo. The Pareto k value is double the theoretical point at which the variance becomes infinite\\n(shown by the dashed line). Likewise, WAIC assigns Idaho a penalty over 2. This penalty\\nterm is sometimes called the “effective number of parameters,” because in ordinary linear\\nregressions the sum of all penalty terms from all points tends to be equal to the number of\\nfree parameters in the model. But in this case there are 4 parameters and the total penalty is\\ncloser to 6—check WAIC(m5.3). The outlier Idaho is causing this additional overfitting risk.\\nWhat can be done about this? There is a tradition of dropping outliers. People some-\\ntimes drop outliers even before a model is fit, based only on standard deviations from the\\nmean outcome value. You should never do that—a point can only be unexpected and highly\\ninfluential in light of a model. After you fit a model, the picture changes. If there are only a\\nfew outliers, and you are sure to report results both with and without them, dropping outliers\\nmight be okay. But if there are several outliers and we really need to model them, what then?\\nA basic problem here is that the Gaussian error model is easily surprised. Gaussian\\ndistributions (introduced at the start of Chapter 4) have very thin tails. This means that very\\nlittle probability mass is given to observations far from the mean. Many natural phenomena\\ndo have very thin tails like this. Human height is a good example. But many phenomena do\\n'},\n",
       " {'index': 251,\n",
       "  'number': 233,\n",
       "  'content': '7.5. MODEL COMPARISON\\n233\\n-4\\n-2\\n0\\n2\\n4\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nvalue\\nDensity\\nGaussian\\nStudent-t\\n-6\\n-4\\n-2\\n0\\n2\\n4\\n6\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\nvalue\\nminus log Density\\nGaussian\\nStudent-t\\nFigure 7.11. Thin tails and influential observations. The Gaussian distribu-\\ntion (blue) assigns very little probability to extreme observations. It has thin\\ntails. The Student-t distribution with shape ν = 2 (black) assigns more prob-\\nability to extreme events. These distributions are compared on the proba-\\nbility (left) and log-probability (right) scales.\\nnot. Instead many phenomena have thicker tails with rare, extreme observations. These are\\nnot measurement errors, but real events containing information about natural process.\\nOne way to both use these extreme observations and reduce their influence is to employ\\nsome kind of robust regression. A “robust regression” can mean many different things,\\nbut usually it indicates a linear model in which the influence of extreme observations is re-\\nduced. A common and useful kind of robust regression is to replace the Gaussian model\\nwith a thicker-tailed distribution like Student’s t (or “Student-t”) distribution.131 This dis-\\ntribution has nothing to do with students. The Student-t distribution arises from a mixture\\nof Gaussian distributions with different variances.132 If the variances are diverse, then the\\ntails can be quite thick.\\nThe generalized Student-t distribution has the same mean µ and scale σ parameters as\\nthe Gaussian, but it also has an extra shape parameter ν that controls how thick the tails\\nare. The rethinking package provides Student-t as dstudent. When ν is large, the tails\\nare thin, converging in the limit ν = ∞to a Gaussian distribution. But as ν approaches 1,\\nthe tails get thicker and rare extreme observations occur more often. Figure 7.11 compares\\na Gaussian distribution (in blue) to a corresponding Student-t distribution (in black) with\\nν = 2. The Student-t distribution has thicker tails, and this is most obvious on the log\\nscale (right), where the Gaussian tails shrink quadratically—a normal distribution is just an\\nexponentiated parabola remember—while the Student-t tails shrink much more slowly.\\nIf you have a very large data set with such events, you could estimate ν. Financial time\\nseries, taken over very long periods, are one example. But when using robust regression, we\\ndon’t usually try to estimate ν, because there aren’t enough extreme observations to do so.\\nInstead we assume ν is small (thick tails) in order to reduce the influence of outliers. For\\nexample, if we use the severity of wars since 1950 to estimate a trend, the estimate is likely\\nbiased by the fact that big conflicts like the first and second World Wars are rare. They reside\\n'},\n",
       " {'index': 252,\n",
       "  'number': 234,\n",
       "  'content': '234\\n7. ULYSSES’ COMPASS\\nin the thick tail of war casualties.133 A reasonable estimate depends upon either a longer\\ntime series or judicious use of a thick tailed distribution.\\nLet’s re-estimate the divorce model using a Student-t distribution with ν = 2.\\nR code\\n7.35\\nm5.3t <- quap(\\nalist(\\nD ~ dstudent( 2 , mu , sigma ) ,\\nmu <- a + bM*M + bA*A ,\\na ~ dnorm( 0 , 0.2 ) ,\\nbM ~ dnorm( 0 , 0.5 ) ,\\nbA ~ dnorm( 0 , 0.5 ) ,\\nsigma ~ dexp( 1 )\\n) , data = d )\\nWhen you compute PSIS now, PSIS(m5.3t), you won’t get any warnings about Pareto k\\nvalues. The relative influence of Idaho has been much reduced. How does this impact the\\nposterior distribution of the association between age at marriage and divorce? If you com-\\npare models m5.3t and m5.3, you’ll see that the coefficient bA has gotten farther from zero\\nwhen we introduce the Student-t distribution. This is because Idaho has a low divorce rate\\nand a low median age at marriage. When it was influential, it reduced the association be-\\ntween age at marriage and divorce. Now it is less influential, so the association is estimated\\nto be slightly larger. But the consequence of using robust regression is not always to increase\\nan association. It depends upon the details.\\nAnother thing that thick-tailed distributions make possible is control over how conflict\\nbetween prior and data is handled. We’ll revisit this point in a later chapter, once you have\\nstarted using Markov chains and can derive non-Gaussian posterior distributions.\\nRethinking: The Curse of Tippecanoe. One concern with model comparison is, if we try enough\\ncombinations and transformations of predictors, we might eventually find a model that fits any sample\\nvery well. But this fit will be badly overfit, unlikely to generalize. And WAIC and similar metrics\\nwill be fooled. Consider by analogy the Curse of Tippecanoe.134 From the year 1840 until 1960, every\\nUnited States president who was elected in a year ending in the digit 0 (which happens every 20 years)\\nhas died in office. William Henry Harrison was the first, elected in 1840 and died of pneumonia the\\nnext year. John F. Kennedy was the last, elected in 1960 and assassinated in 1963. Seven American\\npresidents died in sequence in this pattern. Ronald Reagan was elected in 1980, but despite at least\\none attempt on his life, he managed to live long after his term, breaking the curse. Given enough time\\nand data, a pattern like this can be found for almost any body of data. If we search hard enough, we\\nare bound to find a Curse of Tippecanoe.\\nFiddling with and constructing many predictor variables is a great way to find coincidences, but\\nnot necessarily a great way to evaluate hypotheses. However, fitting many possible models isn’t always\\na dangerous idea, provided some judgment is exercised in weeding down the list of variables at the\\nstart. There are two scenarios in which this strategy appears defensible. First, sometimes all one wants\\nto do is explore a set of data, because there are no clear hypotheses to evaluate. This is rightly labeled\\npejoratively as data dredging, when one does not admit to it. But when used together with model\\naveraging, and freely admitted, it can be a way to stimulate future investigation. Second, sometimes\\nwe need to convince an audience that we have tried all of the combinations of predictors, because\\nnone of the variables seem to help much in prediction.\\n'},\n",
       " {'index': 253,\n",
       "  'number': 235,\n",
       "  'content': '7.7. PRACTICE\\n235\\n7.6. Summary\\nThis chapter has been a marathon. It began with the problem of overfitting, a univer-\\nsal phenomenon by which models with more parameters fit a sample better, even when the\\nadditional parameters are meaningless. Two common tools were introduced to address over-\\nfitting: regularizing priors and estimates of out-of-sample accuracy (WAIC and PSIS). Reg-\\nularizing priors reduce overfitting during estimation, and WAIC and PSIS help estimate the\\ndegree of overfitting. Practical functions compare in the rethinking package were intro-\\nduced to help analyze collections of models fit to the same data. If you are after causal esti-\\nmates, then these tools will mislead you. So models must be designed through some other\\nmethod, not selected on the basis of out-of-sample predictive accuracy. But any causal esti-\\nmate will still overfit the sample. So you always have to worry about overfitting, measuring\\nit with WAIC/PSIS and reducing it with regularization.\\n7.7. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n7E1. State the three motivating criteria that define information entropy. Try to express each in your\\nown words.\\n7E2. Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads\\n70% of the time. What is the entropy of this coin?\\n7E3. Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2”\\n25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die?\\n7E4. Suppose another four-sided die is loaded such that it never shows “4”. The other three sides\\nshow equally often. What is the entropy of this die?\\n7M1. Write down and compare the definitions of AIC and WAIC. Which of these criteria is most\\ngeneral? Which assumptions are required to transform the more general criterion into a less general\\none?\\n7M2. Explain the difference between model selection and model comparison. What information is\\nlost under model selection?\\n7M3. When comparing models with an information criterion, why must all models be fit to exactly\\nthe same observations? What would happen to the information criterion values, if the models were\\nfit to different numbers of observations? Perform some experiments, if you are not sure.\\n7M4. What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior\\nbecomes more concentrated? Why? Perform some experiments, if you are not sure.\\n7M5. Provide an informal explanation of why informative priors reduce overfitting.\\n'},\n",
       " {'index': 254,\n",
       "  'number': 236,\n",
       "  'content': '236\\n7. ULYSSES’ COMPASS\\n7M6. Provide an informal explanation of why overly informative priors result in underfitting.\\n7H1. In 2007, The Wall Street Journal published an editorial (“We’re Num-\\nber One, Alas”) with a graph of corporate tax rates in 29 countries plot-\\nted against tax revenue. A badly fit curve was drawn in (reconstructed\\nat right), seemingly by hand, to make the argument that the relationship\\nbetween tax rate and tax revenue increases and then declines, such that\\nhigher tax rates can actually produce less tax revenue. I want you to actu-\\nally fit a curve to these data, found in data(Laffer). Consider models\\nthat use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a\\nstraight-line model to any curved models you like. What do you conclude\\nabout the relationship between tax rate and tax revenue?\\n0\\n10\\n20\\n30\\n0\\n5\\n10\\n7H2. In the Laffer data, there is one country with a high tax revenue that is an outlier. Use PSIS\\nand WAIC to measure the importance of this outlier in the models you fit in the previous problem.\\nThen use robust regression with a Student’s t distribution to revisit the curve fitting problem. How\\nmuch does a curved relationship depend upon the outlier point?\\n7H3. Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by\\nthe king with surveying the bird population. They have each found the following proportions of 5\\nimportant bird species:\\nSpecies A\\nSpecies B\\nSpecies C\\nSpecies D\\nSpecies E\\nIsland 1\\n0.2\\n0.2\\n0.2\\n0.2\\n0.2\\nIsland 2\\n0.8\\n0.1\\n0.05\\n0.025\\n0.025\\nIsland 3\\n0.05\\n0.15\\n0.7\\n0.05\\n0.05\\nNotice that each row sums to 1, all the birds. This problem has two parts. It is not computationally\\ncomplicated. But it is conceptually tricky. First, compute the entropy of each island’s bird distribution.\\nInterpret these entropy values. Second, use each island’s bird distribution to predict the other two.\\nThis means to compute the KL divergence of each island from the others, treating each island as if it\\nwere a statistical model of the other islands. You should end up with 6 different KL divergence values.\\nWhich island predicts the others best? Why?\\n7H4. Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models\\nm6.9 and m6.10 again (page 178). Compare these two models using WAIC (or PSIS, they will produce\\nidentical results). Which model is expected to make better predictions? Which model provides the\\ncorrect causal inference about the influence of age on happiness? Can you explain why the answers\\nto these two questions disagree?\\n7H5. Revisit the urban fox data, data(foxes), from the previous chapter’s practice problems. Use\\nWAIC or PSIS based model comparison on five different models, each using weight as the outcome,\\nand containing these sets of predictor variables:\\n(1) avgfood + groupsize + area\\n(2) avgfood + groupsize\\n(3) groupsize + area\\n(4) avgfood\\n(5) area\\nCan you explain the relative differences in WAIC scores, using the fox DAG from the previous chap-\\nter? Be sure to pay attention to the standard error of the score differences (dSE).\\n'},\n",
       " {'index': 255,\n",
       "  'number': 237,\n",
       "  'content': '8 Conditional Manatees\\nThe manatee (Trichechus manatus) is a slow-moving, aquatic mammal that lives in warm,\\nshallow water. Manatees have no natural predators, but they do share their waters with motor\\nboats. And motor boats have propellers. While manatees are related to elephants and have\\nvery thick skins, propeller blades can and do kill them. A majority of adult manatees bear\\nsome kind of scar earned in a collision with a boat (Figure 8.1, top).135\\nThe Armstrong Whitworth A.W.38 Whitley was a frontline Royal Air Force bomber.\\nDuring the second World War, the A.W.38 carried bombs and pamphlets into German ter-\\nritory. Unlike the manatee, the A.W.38 has fierce natural enemies: artillery and interceptor\\nfire. Many planes never returned from their missions. And those that survived had the scars\\nto prove it (Figure 8.1, bottom).\\nHow is a manatee like an A.W.38 bomber? In both cases—manatee propeller scars and\\nbomber bullet holes—we’d like to do something to improve the odds, to help manatees and\\nbombers survive. Most observers intuit that helping manatees or bombers means reducing\\nthe kind of damage we see on them. For manatees, this might mean requiring propeller\\nguards (on the boats, not the manatees). For bombers, it’d mean adding armor to the parts\\nof the plane that show the most damage.\\nBut in both cases, the evidence misleads us. Propellers do not cause most of the injury\\nand death caused to manatees. Rather autopsies confirm that collisions with blunt parts of\\nthe boat, like the keel, do far more damage. Similarly, up-armoring the damaged portions of\\nreturning bombers did little good. Instead, improving the A.W.38 bomber meant armoring\\nthe undamaged sections.136 The evidence from surviving manatees and bombers is mislead-\\ning, because it is conditional on survival. Manatees and bombers that perished look different.\\nA manatee struck by a keel is less likely to live than another grazed by a propeller. So among\\nthe survivors, propeller scars are common. Similarly, bombers that returned home conspic-\\nuously lacked damage to the cockpit and engines. They got lucky. Bombers that never re-\\nturned home were less so. To get the right answer, in either context, we have to realize that\\nthe kind of damage seen is conditional on survival.\\nConditioning is one of the most important principles of statistical inference. Data,\\nlike the manatee scars and bomber damage, are conditional on how they get into our sample.\\nPosterior distributions are conditional on the data. All model-based inference is conditional\\non the model. Every inference is conditional on something, whether we notice it or not.\\nAnd a large part of the power of statistical modeling comes from creating devices that\\nallow probability to be conditional of aspects of each case. The linear models you’ve grown to\\nlove are just crude devices that allow each outcome yi to be conditional on a set of predictors\\nfor each case i. Like the epicycles of the Ptolemaic and Kopernikan models (Chapters 4 and\\n7), linear models give us a way to describe conditionality.\\n237\\n'},\n",
       " {'index': 256,\n",
       "  'number': 238,\n",
       "  'content': '238\\n8. CONDITIONAL MANATEES\\nFigure 8.1. top: Dorsal scars for 5 adult Florida manatees. Rows of short\\nscars, for example on the individuals Africa and Flash, are indicative of pro-\\npeller laceration. bottom: Three exemplars of damage on A.W.38 bombers\\nreturning from missions.\\nSimple linear models frequently fail to provide enough conditioning, however. Every\\nmodel so far in this book has assumed that each predictor has an independent association\\nwith the mean of the outcome. What if we want to allow the association to be conditional?\\nFor example, in the primate milk data from the previous chapters, suppose the relationship\\nbetween milk energy and brain size varies by taxonomic group (ape, monkey, prosimian).\\nThis is the same as suggesting that the influence of brain size on milk energy is conditional\\non taxonomic group. The linear models of previous chapters cannot address this question.\\nTo model deeper conditionality—where the importance of one predictor depends upon\\nanother predictor—we need interaction (also known as moderation). Interaction is\\na kind of conditioning, a way of allowing parameters (really their posterior distributions)\\nto be conditional on further aspects of the data. The simplest kind of interaction, a linear\\ninteraction, is built by extending the linear modeling strategy to parameters within the lin-\\near model. So it is akin to placing epicycles on epicycles in the Ptolemaic and Kopernikan\\nmodels. It is descriptive, but very powerful.\\nMore generally, interactions are central to most statistical models beyond the cozy world\\nof Gaussian outcomes and linear models of the mean. In generalized linear models (GLMs,\\nChapter 10 and onwards), even when one does not explicitly define variables as interacting,\\nthey will always interact to some degree. Multilevel models induce similar effects. Common\\nsorts of multilevel models are essentially massive interaction models, in which estimates (in-\\ntercepts and slopes) are conditional on clusters (person, genus, village, city, galaxy) in the\\ndata. Multilevel interaction effects are complex. They’re not just allowing the impact of a\\n'},\n",
       " {'index': 257,\n",
       "  'number': 239,\n",
       "  'content': '8.1. BUILDING AN INTERACTION\\n239\\npredictor variable to change depending upon some other variable, but they are also estimat-\\ning aspects of the distribution of those changes. This may sound like genius, or madness, or\\nboth. Regardless, you can’t have the power of multilevel modeling without it.\\nModels that allow for complex interactions are easy to fit to data. But they can be con-\\nsiderably harder to understand. And so I spend this chapter reviewing simple interaction\\neffects: how to specify them, how to interpret them, and how to plot them. The chapter\\nstarts with a case of an interaction between a single categorical (indicator) variable and a\\nsingle continuous variable. In this context, it is easy to appreciate the sort of hypothesis that\\nan interaction allows for. Then the chapter moves on to more complex interactions between\\nmultiple continuous predictor variables. These are harder. In every section of this chapter,\\nthe model predictions are visualized, averaging over uncertainty in parameters.\\nInteractions are common, but they are not easy. My hope is that this chapter lays a solid\\nfoundation for interpreting generalized linear and multilevel models in later chapters.\\nRethinking: Statistics all-star, Abraham Wald. The World War II bombers story is the work of Abra-\\nham Wald (1902–1950). Wald was born in what is now Romania, but immigrated to the United States\\nafter the Nazi invasion of Austria. Wald made many contributions over his short life. Perhaps most\\ngermane to the current material, Wald proved that for many types of rules for making statistical de-\\ncisions, there will exist a Bayesian rule that is at least as good as any non-Bayesian one. Wald proved\\nthis, remarkably, beginning with non-Bayesian premises, and so anti-Bayesians could not ignore it.\\nThis work was summarized in Wald’s 1950 book, published just before his death.137 Wald died much\\ntoo young, from a plane crash while touring India.\\n8.1. Building an interaction\\nAfrica is special. The second largest continent, it is the most culturally and genetically\\ndiverse. Africa has about 3 billion fewer people than Asia, but it has just as many living lan-\\nguages. Africa is so genetically diverse that most of the genetic variation outside of Africa\\nis just a subset of the variation within Africa. Africa is also geographically special, in a puz-\\nzling way: Bad geography tends to be related to bad economies outside of Africa, but African\\neconomies may actually benefit from bad geography.\\nTo appreciate the puzzle, look at regressions of terrain ruggedness—a particular kind of\\nbad geography—against economic performance (log GDP138 per capita in the year 2000),\\nboth inside and outside of Africa (Figure 8.2). The variable rugged is a Terrain Rugged-\\nness Index139 that quantifies the topographic heterogeneity of a landscape. The outcome\\nvariable here is the logarithm of real gross domestic product per capita, from the year 2000,\\nrgdppc_2000. We use the logarithm of it, because the logarithm of GDP is the magnitude\\nof GDP. Since wealth generates wealth, it tends to be exponentially related to anything that\\nincreases it. This is like saying that the absolute distances in wealth grow increasingly large,\\nas nations become wealthier. So when we work with logarithms instead, we can work on a\\nmore evenly spaced scale of magnitudes. Regardless, keep in mind that a log transform loses\\nno information. It just changes what the model assumes about the shape of the association\\nbetween variables. In this case, raw GDP is not linearly associated with anything, because of\\nits exponential pattern. But log GDP is linearly associated with lots of things.\\nWhat is going on in this figure? It makes sense that ruggedness is associated with poorer\\ncountries, in most of the world. Rugged terrain means transport is difficult. Which means\\nmarket access is hampered. Which means reduced gross domestic product. So the reversed\\n'},\n",
       " {'index': 258,\n",
       "  'number': 240,\n",
       "  'content': '240\\n8. CONDITIONAL MANATEES\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.8\\n0.9\\n1.0\\n1.1\\nruggedness (standardized)\\nlog GDP (as proportion of mean)\\nAfrican nations\\nLesotho\\nSeychelles\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2\\n1.3\\nruggedness (standardized)\\nlog GDP (as proportion of mean)\\nNon-African nations\\nSwitzerland\\nTajikistan\\nFigure 8.2. Separate linear regressions inside and outside of Africa, for log-\\nGDP against terrain ruggedness. The slope is positive inside Africa, but\\nnegative outside. How can we recover this reversal of the slope, using the\\ncombined data?\\nrelationship within Africa is puzzling. Why should difficult terrain be associated with higher\\nGDP per capita?\\nIf this relationship is at all causal, it may be because rugged regions of Africa were pro-\\ntected against the Atlantic and Indian Ocean slave trades. Slavers preferred to raid easily\\naccessed settlements, with easy routes to the sea. Those regions that suffered under the slave\\ntrade understandably continue to suffer economically, long after the decline of slave-trading\\nmarkets. However, an outcome like GDP has many influences, and is furthermore a strange\\nmeasure of economic activity. And ruggedness is correlated with other geographic features,\\nlike coastlines, that also influence the economy. So it is hard to be sure what’s going on here.\\nThe causal hypothesis, in DAG form, might be (but see the Overthinking box at the end\\nof this section):\\nC\\nG\\nR\\nU\\nwhere R is terrain ruggedness, G is GDP, C is continent, and U is some set of unobserved\\nconfounds (like distance to coast). Let’s ignore U for now. You’ll consider some confounds\\nin the practice problems at the end. Focus instead on the implication that R and C both\\ninfluence G. This could mean that they are independent influences or rather that they interact\\n(one moderates the influence of the other). The DAG does not display an interaction. That’s\\nbecause DAGs do not specify how variables combine to influence other variables. The DAG\\nabove implies only that there is some function that uses R and C to generate G. In typical\\nnotation, G = f(R, C).\\nSo we need a statistical approach to judge different propositions for f(R, C). How do we\\nmake a model that produces the conditionality in Figure 8.2? We could cheat by splitting\\n'},\n",
       " {'index': 259,\n",
       "  'number': 241,\n",
       "  'content': '8.1. BUILDING AN INTERACTION\\n241\\nthe data into two data frames, one for Africa and one for all the other continents. But it’s not\\na good idea to split the data in this way. Here are four reasons.\\nFirst, there are usually some parameters, such as σ, that the model says do not depend\\nin any way upon continent. By splitting the data table, you are hurting the accuracy of the es-\\ntimates for these parameters, because you are essentially making two less-accurate estimates\\ninstead of pooling all of the evidence into one estimate. In effect, you have accidentally as-\\nsumed that variance differs between African and non-African nations. Now, there’s nothing\\nwrong with that sort of assumption. But you want to avoid accidental assumptions.\\nSecond, in order to acquire probability statements about the variable you used to split the\\ndata, cont_africa in this case, you need to include it in the model. Otherwise, you have a\\nweak statistical argument. Isn’t there uncertainty about the predictive value of distinguishing\\nbetween African and non-African nations? Of course there is. Unless you analyze all of the\\ndata in a single model, you can’t easily quantify that uncertainty. If you just let the posterior\\ndistribution do the work for you, you’ll have a useful measure of that uncertainty.\\nThird, we may want to use information criteria or another method to compare models.\\nIn order to compare a model that treats all continents the same way to a model that allows\\ndifferent slopes in different continents, we need models that use all of the same data (as\\nexplained in Chapter 7). This means we can’t split the data for two separate models. We have\\nto let a single model internally split the data.\\nFourth, once you begin using multilevel models (Chapter 13), you’ll see that there are\\nadvantages to borrowing information across categories like “Africa” and “not Africa.” This is\\nespecially true when sample sizes vary across categories, such that overfitting risk is higher\\nwithin some categories. In other words, what we learn about ruggedness outside of Africa\\nshould have some effect on our estimate within Africa, and visa versa. Multilevel models\\n(Chapter 13) borrow information in this way, in order to improve estimates in all categories.\\nWhen we split the data, this borrowing is impossible.\\nOverthinking: Not so simple causation. The terrain ruggedness DAG in the preceding section is\\nsimple. But the truth isn’t so simple. Continent isn’t really the cause of interest. Rather there are\\nhypothetical historical exposures to colonialism and the slave trade that have persistent influences\\non economic performance. Terrain features, like ruggedness, that causally reduced those historical\\nfactors may indirectly influence economy. Like this:\\nC\\nG\\nH\\nR\\nU\\nH stands for historical factors like exposure to slave trade. The total causal influence of R contains\\nboth a direct path R →G (this is presumably always negative) and an indirect path R →H →G. The\\nsecond path is the one that covaries with continent C, because H is strongly associated with C. Note\\nthat the confounds U could influence any of these variables (except for C). If for example distance to\\ncoast is really what influenced H in the past, not terrain ruggedness, then the association of terrain\\nruggedness with GDP is non-causal. The data contain a large number of potential confounds that\\nyou might consider. Natural systems like this are terrifyingly complex.\\n'},\n",
       " {'index': 260,\n",
       "  'number': 242,\n",
       "  'content': '242\\n8. CONDITIONAL MANATEES\\n8.1.1. Making a rugged model. Let’s see how to recover the reversal of slope, within a single\\nmodel. We’ll begin by fitting a single model to all the data, ignoring continent. This will let\\nus think through the model structure and priors before facing the devil of interaction. To\\nget started, load the data and preform some pre-processing:\\nR code\\n8.1\\nlibrary(rethinking)\\ndata(rugged)\\nd <- rugged\\n# make log version of outcome\\nd$log_gdp <- log( d$rgdppc_2000 )\\n# extract countries with GDP data\\ndd <- d[ complete.cases(d$rgdppc_2000) , ]\\n# rescale variables\\ndd$log_gdp_std <- dd$log_gdp / mean(dd$log_gdp)\\ndd$rugged_std <- dd$rugged / max(dd$rugged)\\nEach row in these data is a country, and the various columns are economic, geographic, and\\nhistorical features.140 Raw magnitudes of GDP and terrain ruggedness aren’t meaningful\\nto humans. So I’ve scaled the variables to make the units easier to work with. The usual\\nstandardization is to subtract the mean and divide by the standard deviation. This makes a\\nvariable into z-scores. We don’t want to do that here, because zero ruggedness is meaningful.\\nSo instead terrain ruggedness is divided by the maximum value observed. This means it ends\\nup scaled from totally flat (zero) to the maximum in the sample at 1 (Lesotho, a very rugged\\nand beautiful place). Similarly, log GDP is divided by the average value. So it is rescaled as a\\nproportion of the international average. 1 means average, 0.8 means 80% of the average, and\\n1.1 means 10% more than average.\\nTo build a Bayesian model for this relationship, we’ll again use our geocentric skeleton:\\nlog(yi) ∼Normal(µi, σ)\\nµi = α + β(ri −¯r)\\nwhere yi is GDP for nation i, ri is terrain ruggedness for nation i, and¯r is the average rugged-\\nness in the whole sample. Its value is 0.215—most nations aren’t that rugged. Remember\\nthat using ¯r just makes it easier to assign a prior to the intercept α.\\nThe hard thinking here comes when we specify priors. If you are like me, you don’t\\nhave much scientific information about plausible associations between log GDP and terrain\\nruggedness. But even when we don’t know much about the context, the measurements them-\\nselves constrain the priors in useful ways. The scaled outcome and predictor will make this\\neasier. Consider first the intercept, α, defined as the log GDP when ruggedness is at the\\nsample mean. So it must be close to 1, because we scaled the outcome so that the mean is 1.\\nLet’s start with a guess at:\\nα ∼Normal(1, 1)\\nNow for β, the slope. If we center it on zero, that indicates no bias for positive or negative,\\nwhich makes sense. But what about the standard deviation? Let’s start with a guess at 1:\\nβ ∼Normal(0, 1)\\n'},\n",
       " {'index': 261,\n",
       "  'number': 243,\n",
       "  'content': '8.1. BUILDING AN INTERACTION\\n243\\nWe’ll evaluate this guess by simulating prior predictive distributions. The last thing we need\\nis a prior for σ. Let’s assign something very broad, σ ∼Exponential(1). In the problems at\\nthe end of the chapter, I’ll ask you to confront this prior as well. But we’ll ignore it for the\\nrest of this example.\\nAll together, we have our first candidate model for the terrain ruggedness data:\\nR code\\n8.2\\nm8.1 <- quap(\\nalist(\\nlog_gdp_std ~ dnorm( mu , sigma ) ,\\nmu <- a + b*( rugged_std - 0.215 ) ,\\na ~ dnorm( 1 , 1 ) ,\\nb ~ dnorm( 0 , 1 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dd )\\nWe’re not going to look at the posterior predictions yet, but rather at the prior predictions.\\nLet’s extract the prior and plot the implied lines. We’ll do this using link.\\nR code\\n8.3\\nset.seed(7)\\nprior <- extract.prior( m8.1 )\\n# set up the plot dimensions\\nplot( NULL , xlim=c(0,1) , ylim=c(0.5,1.5) ,\\nxlab=\"ruggedness\" , ylab=\"log GDP\" )\\nabline( h=min(dd$log_gdp_std) , lty=2 )\\nabline( h=max(dd$log_gdp_std) , lty=2 )\\n# draw 50 lines from the prior\\nrugged_seq <- seq( from=-0.1 , to=1.1 , length.out=30 )\\nmu <- link( m8.1 , post=prior , data=data.frame(rugged_std=rugged_seq) )\\nfor ( i in 1:50 ) lines( rugged_seq , mu[i,] , col=col.alpha(\"black\",0.3) )\\nThe result is displayed on the left side of Figure 8.3. The horizontal dashed lines show the\\nmaximum and minimum observed log GDP values. The regression lines trend both positive\\nand negative, as they should, but many of these lines are in impossible territory. Considering\\nonly the measurement scales, the lines have to pass closer to the point where ruggedness is\\naverage (0.215 on the horizontal axis) and proportional log GDP is 1. Instead there are lots\\nof lines that expect average GDP outside observed ranges. So we need a tighter standard\\ndeviation on the α prior. Something like α ∼Normal(0, 0.1) will put most of the plausibility\\nwithin the observed GDP values. Remember: 95% of the Gaussian mass is within 2 standard\\ndeviations. So a Normal(0, 0.1) prior assigns 95% of the plausibility between 0.8 and 1.2.\\nThat is still very vague, but at least it isn’t ridiculous.\\nAt the same time, the slopes are too variable. It is not plausible that terrain ruggedness\\nexplains most of the observed variation in log GDP. An implausibly strong association would\\nbe, for example, a line that goes from minimum ruggedness and extreme GDP on one end to\\nmaximum ruggedness and the opposite extreme of GDP on the other end. I’ve highlighted\\nsuch a line in blue. The slope of such a line must be about 1.3 −0.7 = 0.6, the difference\\nbetween the maximum and minimum observed proportional log GDP. But very many lines\\n'},\n",
       " {'index': 262,\n",
       "  'number': 244,\n",
       "  'content': '244\\n8. CONDITIONAL MANATEES\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nruggedness\\nlog GDP (prop of mean)\\na ~ dnorm(1, 1)\\nb ~ dnorm(0, 1)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nruggedness\\nlog GDP (prop of mean)\\na ~ dnorm(1, 0.1)\\nb ~ dnorm(0, 0.3)\\nFigure 8.3. Simulating in search of reasonable priors for the terrain rugged-\\nness example. The dashed horizontal lines indicate the minimum and max-\\nimum observed GDP values. Left: The first guess with very vague priors.\\nRight: The improved model with much more plausible priors.\\nin the prior have much more extreme slopes than this. Under the β ∼Normal(0, 1) prior,\\nmore than half of all slopes will have absolute value greater than 0.6.\\nR code\\n8.4\\nsum( abs(prior$b) > 0.6 ) / length(prior$b)\\n[1] 0.545\\nLet’s try instead β ∼Normal(0, 0.3). This prior makes a slope of 0.6 two standard deviations\\nout. That is still a bit too plausible, but it’s a lot better than before.\\nWith these two changes, now the model is:\\nR code\\n8.5\\nm8.1 <- quap(\\nalist(\\nlog_gdp_std ~ dnorm( mu , sigma ) ,\\nmu <- a + b*( rugged_std - 0.215 ) ,\\na ~ dnorm( 1 , 0.1 ) ,\\nb ~ dnorm( 0 , 0.3 ) ,\\nsigma ~ dexp(1)\\n) , data=dd )\\nYou can extract the prior and plot the implied lines using the same code as before. The result\\nis shown on the right side of Figure 8.3. Some of these slopes are still implausibly strong.\\nBut in the main, this is a much better set of priors. Let’s look at the posterior now:\\nR code\\n8.6\\nprecis( m8.1 )\\nmean\\nsd\\n5.5% 94.5%\\na\\n1.00 0.01\\n0.98\\n1.02\\n'},\n",
       " {'index': 263,\n",
       "  'number': 245,\n",
       "  'content': '8.1. BUILDING AN INTERACTION\\n245\\nb\\n0.00 0.05 -0.09\\n0.09\\nsigma 0.14 0.01\\n0.12\\n0.15\\nReally no overall association between terrain ruggedness and log GDP. Next we’ll see how to\\nsplit apart the continents.\\nRethinking: Practicing for when it matters. The exercise in Figure 8.3 is really not necessary in this\\nexample, because there is enough data, and the model is simple enough, that even awful priors get\\nwashed out. You could even use completely flat priors (don’t!), and it would all be fine. But we practice\\ndoing things right not because it always matters. Rather, we practice doing things right so that we\\nare ready when it matters. No one would say that wearing a seat belt was a mistake, just because you\\ndidn’t get into an accident.\\n8.1.2. Adding an indicator variable isn’t enough. The first thing to realize is that just in-\\ncluding an indicator variable for African nations, cont_africa here, won’t reveal the re-\\nversed slope. It’s worth fitting this model to prove it to yourself, though. I’m going to walk\\nthrough this as a simple model comparison exercise, just so you begin to get some applied\\nexamples of concepts you’ve accumulated from earlier chapters. Note that model compari-\\nson here is not about selecting a model. Scientific considerations already select the relevant\\nmodel. Instead it is about measuring the impact of model differences while accounting for\\noverfitting risk.\\nTo build a model that allows nations inside and outside Africa to have different inter-\\ncepts, we need to modify the model for µi so that the mean is conditional on continent. The\\nconventional way to do this would be to just add another term to the linear model:\\nµi = α + β(ri −¯r) + γAi\\nwhere Ai is cont_africa, a 0/1 indicator variable. But let’s not follow this convention. In\\nfact, this convention is often a bad idea. It took me years to figure this out, and I’m trying to\\nsave you from the horrors I’ve seen. The problem here, and in general, is that we need a prior\\nfor γ. Okay, we can do priors. But what that prior will necessarily do is tell the model that\\nµi for a nation in Africa is more uncertain, before seeing the data, than µi outside Africa.\\nAnd that makes no sense. This is the same issue we confronted back in Chapter 4, when I\\nintroduced categorical variables.\\nThere is a simple solution: Nations in Africa will get one intercept and those outside\\nAfrica another. This is what µi looks like now:\\nµi = αcid[i] + β(ri −¯r)\\nwhere cid is an index variable, continent ID. It takes the value 1 for African nations and 2 for\\nall other nations. This means there are two parameters, α1 and α2, one for each unique index\\nvalue. The notation cid[i] just means the value of cid on row i. I use the bracket notation\\nwith index variables, because it is easier to read than adding a second level of subscript, αcidi.\\nWe can build this index ourselves:\\nR code\\n8.7\\n# make variable to index Africa (1) or not (2)\\ndd$cid <- ifelse( dd$cont_africa==1 , 1 , 2 )\\nUsing this approach, instead of the conventional approach of adding another term with the\\n0/1 indicator variable, doesn’t force us to say that the mean for Africa is inherently less certain\\nthan the mean for all other continents. We can just reuse the same prior as before. After all,\\n'},\n",
       " {'index': 264,\n",
       "  'number': 246,\n",
       "  'content': '246\\n8. CONDITIONAL MANATEES\\nwhatever Africa’s average log GDP, it is surely within plus-or-minus 0.2 of 1. But keep in\\nmind that this is structurally the same model you’d get in the conventional approach. It is\\njust much easier this way to assign sensible priors. You could easily assign different priors to\\nthe different continents, if you thought that was the right thing to do.\\nTo define the model in quap, we add brackets in the linear model and the prior:\\nR code\\n8.8\\nm8.2 <- quap(\\nalist(\\nlog_gdp_std ~ dnorm( mu , sigma ) ,\\nmu <- a[cid] + b*( rugged_std - 0.215 ) ,\\na[cid] ~ dnorm( 1 , 0.1 ) ,\\nb ~ dnorm( 0 , 0.3 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dd )\\nNow to compare these models, using WAIC:\\nR code\\n8.9\\ncompare( m8.1 , m8.2 )\\nWAIC\\nSE dWAIC\\ndSE pWAIC weight\\nm8.2 -252.4 15.38\\n0.0\\nNA\\n4.2\\n1\\nm8.1 -188.6 13.20\\n63.9 15.13\\n2.8\\n0\\nm8.2 gets all the model weight. And while the standard error of the difference in WAIC is 15,\\nthe difference itself is 64. So the continent variable seems to be picking up some important\\nassociation in the sample. The precis output gives a good hint. Note that we need to use\\ndepth=2 to display the vector parameter a. With only two parameters in a, it wouldn’t be\\nbad to display it by default. But often a vector like this has hundreds of values, and you don’t\\nwant to see each one in a table.\\nR code\\n8.10\\nprecis( m8.2 , depth=2 )\\nmean\\nsd\\n5.5% 94.5%\\na[1]\\n0.88 0.02\\n0.85\\n0.91\\na[2]\\n1.05 0.01\\n1.03\\n1.07\\nb\\n-0.05 0.05 -0.12\\n0.03\\nsigma\\n0.11 0.01\\n0.10\\n0.12\\nThe parameter a[1] is the intercept for African nations. It seems reliably lower than a[2].\\nThe posterior contrast between the two intercepts is:\\nR code\\n8.11\\npost <- extract.samples(m8.2)\\ndiff_a1_a2 <- post$a[,1] - post$a[,2]\\nPI( diff_a1_a2 )\\n5%\\n94%\\n-0.1990056 -0.1378378\\nThe difference is reliably below zero. Let’s plot the posterior predictions for m8.2, so you\\ncan see how, despite its predictive superiority to m8.1, it still doesn’t manage different slopes\\n'},\n",
       " {'index': 265,\n",
       "  'number': 247,\n",
       "  'content': '8.1. BUILDING AN INTERACTION\\n247\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2\\n1.3\\nruggedness (standardized)\\nlog GDP (as proportion of mean)\\nm8.4\\nAfrica\\nNot Africa\\nFigure\\n8.4. Including\\nan\\nindicator\\nfor\\nAfrican nations has no effect on the slope.\\nAfrican nations are shown in blue.\\nNon-\\nAfrican nations are shown in black. Regres-\\nsion means for each subset of nations are\\nshown in corresponding colors, along with\\n97% intervals shown by shading.\\ninside and outside of Africa. To sample from the posterior and compute the predicted means\\nand intervals for both African and non-African nations:\\nR code\\n8.12\\nrugged.seq <- seq( from=-0.1 , to=1.1 , length.out=30 )\\n# compute mu over samples, fixing cid=2 and then cid=1\\nmu.NotAfrica <- link( m8.2 ,\\ndata=data.frame( cid=2 , rugged_std=rugged.seq ) )\\nmu.Africa <- link( m8.2 ,\\ndata=data.frame( cid=1 , rugged_std=rugged.seq ) )\\n# summarize to means and intervals\\nmu.NotAfrica_mu <- apply( mu.NotAfrica , 2 , mean )\\nmu.NotAfrica_ci <- apply( mu.NotAfrica , 2 , PI , prob=0.97 )\\nmu.Africa_mu <- apply( mu.Africa , 2 , mean )\\nmu.Africa_ci <- apply( mu.Africa , 2 , PI , prob=0.97 )\\nI show these posterior predictions (retrodictions) in Figure 8.4. African nations are shown\\nin blue, while nations outside Africa are shown in gray. What you’ve ended up with here\\nis a rather weak negative relationship between economic development and ruggedness. The\\nAfrican nations do have lower overall economic development, and so the blue regression line\\nis below, but parallel to, the black line. All including a dummy variable for African nations\\nhas done is allow the model to predict a lower mean for African nations. It can’t do anything\\nto the slope of the line. The fact that WAIC tells you that the model with the dummy variable\\nis hugely better only indicates that African nations on average do have lower GDP.\\nRethinking: Why 97%? In the code block just above, and therefore also in Figure 8.4, I used 97%\\nintervals of the expected mean. This is a rather non-standard percentile interval. So why use 97%? In\\nthis book, I use non-standard percents to constantly remind the reader that conventions like 95% and\\n5% are arbitrary. Furthermore, boundaries are meaningless. There is continuous change in probabil-\\nity as we move away from the expected value. So one side of the boundary is almost equally probable\\nas the other side. Also, 97 is a prime number. That doesn’t mean it is a better choice than any other\\nnumber here, but it’s no less silly than using a multiple of 5, just because we have five digits on each\\nhand. Resist the tyranny of the Tetrapoda.\\n'},\n",
       " {'index': 266,\n",
       "  'number': 248,\n",
       "  'content': '248\\n8. CONDITIONAL MANATEES\\n8.1.3. Adding an interaction does work. How can you recover the change in slope you saw\\nat the start of this section? You need a proper interaction effect. This just means we also\\nmake the slope conditional on continent. The definition of µi in the model you just plotted,\\nin math form, is:\\nµi = αcid[i] + β(ri −¯r)\\nAnd now we’ll double-down on our indexing to make the slope conditional as well:\\nµi = αcid[i] + βcid[i](ri −¯r)\\nAnd again, there is a conventional approach to specifying an interaction that uses an indica-\\ntor variable and a new interaction parameter. It would look like this:\\nµi = αcid[i] + (β + γAi)(ri −¯r)\\nwhere Ai is a 0/1 indicator for African nations. This is equivalent to our index approach,\\nbut it is much harder to state sensible priors. Any prior we put on γ makes the slope inside\\nAfrica more uncertain than the slope outside Africa. And again that makes no sense. But\\nin the indexing approach, we can easily assign the same prior to the slope, no matter which\\ncontinent.\\nTo approximate the posterior of this new model, you can just use quap as before. Here’s\\nthe code that includes an interaction between ruggedness and being in Africa:\\nR code\\n8.13\\nm8.3 <- quap(\\nalist(\\nlog_gdp_std ~ dnorm( mu , sigma ) ,\\nmu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,\\na[cid] ~ dnorm( 1 , 0.1 ) ,\\nb[cid] ~ dnorm( 0 , 0.3 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dd )\\nLet’s inspect the marginal posterior distributions:\\nR code\\n8.14\\nprecis( m8.5 , depth=2 )\\nmean\\nsd\\n5.5% 94.5%\\na[1]\\n0.89 0.02\\n0.86\\n0.91\\na[2]\\n1.05 0.01\\n1.03\\n1.07\\nb[1]\\n0.13 0.07\\n0.01\\n0.25\\nb[2]\\n-0.14 0.05 -0.23 -0.06\\nsigma\\n0.11 0.01\\n0.10\\n0.12\\nThe slope is essentially reversed inside Africa, 0.13 instead of −0.14.\\nHow much does allowing the slope to vary improve expected prediction? Let’s use PSIS\\nto compare this new model to the previous two. You could use WAIC here as well. It’ll give\\nalmost identical results. But it won’t give us a sweet Pareto k warning.\\nR code\\n8.15\\ncompare( m8.1 , m8.2 , m8.3 , func=PSIS )\\nSome Pareto k values are high (>0.5).\\nPSIS\\nSE dPSIS\\ndSE pPSIS weight\\n'},\n",
       " {'index': 267,\n",
       "  'number': 249,\n",
       "  'content': '8.1. BUILDING AN INTERACTION\\n249\\nm8.3 -258.7 15.33\\n0.0\\nNA\\n5.3\\n0.97\\nm8.2 -251.8 15.43\\n6.9\\n6.81\\n4.5\\n0.03\\nm8.1 -188.7 13.31\\n70.0 15.52\\n2.7\\n0.00\\nModel family m8.3 has more than 95% of the weight. That’s very strong support for including\\nthe interaction effect, if prediction is our goal. But the modicum of weight given to m8.2\\nsuggests that the posterior means for the slopes in m8.3 are a little overfit. And the standard\\nerror of the difference in PSIS between the top two models is almost the same as the difference\\nitself. If you plot PSIS Pareto k values for m8.3, you’ll notice some influential countries.\\nR code\\n8.16\\nplot( PSIS( m8.3 , pointwise=TRUE )$k )\\nYou’ll explore this in the practice problems at the end of the chapter. This is possibly a good\\ncontext for robust regression, like the Student-t regression we did in Chapter 7.\\nRemember that these comparisons are not reliable guides to causal inference. They just\\nsuggest how important features are for prediction. Real causal effects may not be impor-\\ntant for overall prediction in any given sample. Prediction and inference are just different\\nquestions. Still, overfitting always happens. So anticipating and measuring it matters for\\ninference as well.\\n8.1.4. Plotting the interaction. Plotting this model doesn’t really require any new tricks.\\nThe goal is to make two plots. In the first, we’ll display nations in Africa and overlay the\\nposterior mean regression line and the 97% interval of that line. In the second, we’ll display\\nnations outside of Africa instead.\\nR code\\n8.17\\n# plot Africa - cid=1\\nd.A1 <- dd[ dd$cid==1 , ]\\nplot( d.A1$rugged_std , d.A1$log_gdp_std , pch=16 , col=rangi2 ,\\nxlab=\"ruggedness (standardized)\" , ylab=\"log GDP (as proportion of mean)\" ,\\nxlim=c(0,1) )\\nmu <- link( m8.3 , data=data.frame( cid=1 , rugged_std=rugged_seq ) )\\nmu_mean <- apply( mu , 2 , mean )\\nmu_ci <- apply( mu , 2 , PI , prob=0.97 )\\nlines( rugged_seq , mu_mean , lwd=2 )\\nshade( mu_ci , rugged_seq , col=col.alpha(rangi2,0.3) )\\nRethinking: All Greek to me. We use these Greek symbols α and β because it is conventional. They\\ndon’t have special meanings. If you prefer some other Greek symbol like ω—why should α get all\\nthe attention?—feel free to use that instead. It is conventional to use Greek letters for unobserved\\nvariables (parameters) and Roman letters for observed variables (data). That convention does have\\nsome value, because it helps others read your models. But breaking the convention is not an error,\\nand sometimes it is better to use a familiar Roman symbol than an unfamiliar Greek one like ξ or ζ.\\nIf your readers cannot say the symbol’s name, it could make understanding the model harder.\\nA core problem with the convention of using Greek for unobserved and Roman for observed\\nvariables is that in many models the same variable can be both observed and unobserved. This hap-\\npens, for example, when data are missing for some cases. It also happens in “occupancy” detection\\nmodels, where specific values of the outcome (usually zero) cannot be trusted. We will deal with these\\nissues explicitly in Chapter 15.\\n'},\n",
       " {'index': 268,\n",
       "  'number': 250,\n",
       "  'content': '250\\n8. CONDITIONAL MANATEES\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.8\\n0.9\\n1.0\\n1.1\\nruggedness (standardized)\\nlog GDP (as proportion of mean)\\nAfrican nations\\nBurundi\\nEquatorial Guinea\\nLesotho\\nRwanda\\nSwaziland\\nSeychelles\\nSouth Africa\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2\\n1.3\\nruggedness (standardized)\\nlog GDP (as proportion of mean)\\nNon-African nations\\nSwitzerland\\nGreece\\nLebanon\\nLuxembourg\\nNepal\\nTajikistan\\nYemen\\nFigure 8.5. Posterior predictions for the terrain ruggedness model, includ-\\ning the interaction between Africa and ruggedness. Shaded regions are 97%\\nposterior intervals of the mean.\\nmtext(\"African nations\")\\n# plot non-Africa - cid=2\\nd.A0 <- dd[ dd$cid==2 , ]\\nplot( d.A0$rugged_std , d.A0$log_gdp_std , pch=1 , col=\"black\" ,\\nxlab=\"ruggedness (standardized)\" , ylab=\"log GDP (as proportion of mean)\" ,\\nxlim=c(0,1) )\\nmu <- link( m8.3 , data=data.frame( cid=2 , rugged_std=rugged_seq ) )\\nmu_mean <- apply( mu , 2 , mean )\\nmu_ci <- apply( mu , 2 , PI , prob=0.97 )\\nlines( rugged_seq , mu_mean , lwd=2 )\\nshade( mu_ci , rugged_seq )\\nmtext(\"Non-African nations\")\\nAnd the result is shown in Figure 8.5. Finally, the slope reverses direction inside and outside\\nof Africa. And because we achieved this inside a single model, we could statistically evaluate\\nthe value of this reversal.\\n8.2. Symmetry of interactions\\nBuridan’s ass is a toy philosophical problem in which an ass who always moves towards\\nthe closest pile of food will starve to death when he finds himself equidistant between two\\nidentical piles. The basic problem is one of symmetry: How can the ass decide between two\\nidentical options? Like many toy problems, you can’t take this one too seriously. Of course\\nthe ass will not starve. But thinking about how the symmetry is broken can be productive.\\nInteractions are like Buridan’s ass. Like the two piles of identical food, a simple inter-\\naction model contains two symmetrical interpretations. Absent some other information,\\noutside the model, there’s no logical basis for preferring one over the other. Consider for\\n'},\n",
       " {'index': 269,\n",
       "  'number': 251,\n",
       "  'content': '8.2. SYMMETRY OF INTERACTIONS\\n251\\nexample the GDP and terrain ruggedness problem. The interaction there has two equally\\nvalid phrasings.\\n(1) How much does the association between ruggedness and log GDP depend upon\\nwhether the nation is in Africa?\\n(2) How much does the association of Africa with log GDP depend upon ruggedness?\\nWhile these two possibilities sound different to most humans, your golem thinks they are\\nidentical. In this section, we’ll examine this fact, first mathematically. Then we’ll plot the\\nruggedness and GDP example again, but with the reverse phrasing—the association between\\nAfrica and GDP depends upon ruggedness.\\nConsider yet again the model for µi:\\nµi = αcid[i] + βcid[i](ri −¯r)\\nThe interpretation previously has been that the slope is conditional on continent. But it’s also\\nfine to say that the intercept is conditional on ruggedness. It’s easier to see this if we write\\nthe above expression another way:\\nµi = (2 −cidi)(α1 + β1(ri −¯r))\\n|\\n{z\\n}\\ncid[i]=1\\n+ (cidi −1)(α2 + β2(ri −¯r))\\n|\\n{z\\n}\\ncid[i]=2\\nThis looks weird, but it’s the same model. When cidi = 1, only the first term, the Africa\\nparameters, remains. The second term vanishes to zero. When instead cidi = 2, the first\\nterm vanishes to zero and only the second term remains. Now if we imagine switching a\\nnation to Africa, in order to know what this does for the prediction, we have to know the\\nruggedness (unless we are exactly at the average ruggedness, ¯r).\\nIt’ll be helpful to plot the reverse interpretation: The association of being in Africa with\\nlog GDP depends upon terrain ruggedness. What we’ll do is compute the difference between\\na nation in Africa and outside Africa, holding its ruggedness constant. To do this, you can\\njust run link twice and then subtract the second result from the first:\\nR code\\n8.18\\nrugged_seq <- seq(from=-0.2,to=1.2,length.out=30)\\nmuA <- link( m8.3 , data=data.frame(cid=1,rugged_std=rugged_seq) )\\nmuN <- link( m8.3 , data=data.frame(cid=2,rugged_std=rugged_seq) )\\ndelta <- muA - muN\\nThen you can summarize and plot the difference in expected log GDP contained in delta.\\nThe result is shown in Figure 8.6. This plot is counter-factual. There is no raw data here.\\nInstead we are seeing through the model’s eyes and imagining comparisons between iden-\\ntical nations inside and outside Africa, as if we could independently manipulate continent\\nand also terrain ruggedness. Below the horizontal dashed line, African nations have lower\\nexpected GDP. This is the case for most terrain ruggedness values. But at the highest rugged-\\nness values, a nation is possibly better off inside Africa than outside it. Really it is hard to\\nfind any reliable difference inside and outside Africa, at high ruggedness values. It is only in\\nsmooth nations that being in Africa is a liability for the economy.\\nThis perspective on the GDP and terrain ruggedness is completely consistent with the\\nprevious perspective. It’s simultaneously true in these data (and with this model) that (1) the\\ninfluence of ruggedness depends upon continent and (2) the influence of continent depends\\nupon ruggedness. Indeed, something is gained by looking at the data in this symmetrical\\n'},\n",
       " {'index': 270,\n",
       "  'number': 252,\n",
       "  'content': '252\\n8. CONDITIONAL MANATEES\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n-0.3\\n-0.2\\n-0.1\\n0.0\\n0.1\\n0.2\\nruggedness\\nexpected difference log GDP\\nAfrica higher GDP\\nAfrica lower GDP\\nFigure 8.6. The other side of the interaction\\nbetween ruggedness and continent. The ver-\\ntical axis is the difference in expected propor-\\ntional log GDP for a nation in Africa and one\\noutside Africa. At low ruggedness, we expect\\n“moving” a nation to Africa to hurt its econ-\\nomy. But at high ruggedness, the opposite is\\ntrue. The association between continent and\\neconomy depends upon ruggedness, just as\\nmuch as the association between ruggedness\\nand economy depends upon continent.\\nperspective. Just inspecting the first view of the interaction, back on page 250, it’s not obvi-\\nous that African nations are on average nearly always worse off. It’s just at very high values\\nof rugged that nations inside and outside of Africa have the same expected log GDP. This\\nsecond way of plotting the interaction makes this clearer.\\nSimple interactions are symmetric, just like the choice facing Buridan’s ass. Within the\\nmodel, there’s no basis to prefer one interpretation over the other, because in fact they are the\\nsame interpretation. But when we reason causally about models, our minds tend to prefer\\none interpretation over the other, because it’s usually easier to imagine manipulating one of\\nthe predictor variables instead of the other. In this case, it’s hard to imagine manipulating\\nwhich continent a nation is on. But it’s easy to imagine manipulating terrain ruggedness,\\nby flattening hills or blasting tunnels through mountains.141 If in fact the explanation for\\nAfrica’s unusually positive relationship with terrain ruggedness is due to historical causes,\\nnot contemporary terrain, then tunnels might improve economies in the present. At the\\nsame time, continent is not really a cause of economic activity. Rather there are historical\\nand political factors associated with continents, and we use the continent variable as a proxy\\nfor those factors. It is manipulation of those other factors that would matter.\\n8.3. Continuous interactions\\nI want to convince the reader that interaction effects are difficult to interpret. They are\\nnearly impossible to interpret, using only posterior means and standard deviations. Once in-\\nteractions exist, multiple parameters are in play at the same time. It is hard enough with the\\nsimple, categorical interactions from the terrain ruggedness example. Once we start mod-\\neling interactions among continuous variables, it gets much harder. It’s one thing to make\\na slope conditional upon a category. In such a context, the model reduces to estimating a\\ndifferent slope for each category. But it’s quite a lot harder to understand that a slope varies\\nin a continuous fashion with a continuous variable. Interpretation is much harder in this\\ncase, even though the mathematics of the model are essentially the same.\\nIn pursuit of clarifying the construction and interpretation of continuous interac-\\ntions among two or more continuous predictor variables, in this section I develop a simple\\nregression example and show you a way to plot the two-way interaction between two contin-\\nuous variables. The method I present for plotting this interaction is a triptych plot, a panel of\\n'},\n",
       " {'index': 271,\n",
       "  'number': 253,\n",
       "  'content': '8.3. CONTINUOUS INTERACTIONS\\n253\\nthree complementary figures that comprise a whole picture of the regression results. There’s\\nnothing magic about having three figures—in other cases you might want more or less. In-\\nstead, the utility lies in making multiple figures that allow one to see how the interaction\\nalters a slope, across changes in a chosen variable.\\n8.3.1. A winter flower. The data in this example are sizes of blooms from beds of tulips\\ngrown in greenhouses, under different soil and light conditions.142 Load the data with:\\nR code\\n8.19\\nlibrary(rethinking)\\ndata(tulips)\\nd <- tulips\\nstr(d)\\n\\'data.frame\\': 27 obs. of\\n4 variables:\\n$ bed\\n: Factor w/ 3 levels \"a\",\"b\",\"c\": 1 1 1 1 1 1 1 1 1 2 ...\\n$ water : int\\n1 1 1 2 2 2 3 3 3 1 ...\\n$ shade : int\\n1 2 3 1 2 3 1 2 3 1 ...\\n$ blooms: num\\n0 0 111 183.5 59.2 ...\\nThe blooms column will be our outcome—what we wish to predict. The water and shade\\ncolumns will be our predictor variables. water indicates one of three ordered levels of soil\\nmoisture, from low (1) to high (3). shade indicates one of three ordered levels of light ex-\\nposure, from high (1) to low (3). The last column, bed, indicates a cluster of plants from the\\nsame section of the greenhouse.\\nSince both light and water help plants grow and produce blooms, it stands to reason that\\nthe independent effect of each will be to produce bigger blooms. But we’ll also be interested\\nin the interaction between these two variables. In the absence of light, for example, it’s hard\\nto see how water will help a plant—photosynthesis depends upon both light and water. Like-\\nwise, in the absence of water, sunlight does a plant little good. One way to model such an\\ninterdependency is to use an interaction effect. In the absence of a good mechanistic model\\nof the interaction, one that uses a theory about the plant’s physiology to hypothesize the\\nfunctional relationship between light and water, then a simple linear two-way interaction is\\na good start. But ultimately it’s not close to the best that we could do.\\n8.3.2. The models. I’m going to focus on just two models: (1) the model with both water\\nand shade but no interaction and (2) the model that also contains the interaction of water\\nwith shade. You could also inspect models that contain only one of these variables, water\\nor shade, and I encourage the reader to try that at the end and make sure you understand\\nthe full ensemble of models.\\nThe causal scenario is simply that water (W) and shade (S) both influence blooms (B):\\nW →B ←S. As before, this DAG doesn’t tell us the function through which W and S jointly\\ninfluence B, B = f(W, S). In principle, every unique combination of W and S could have a\\ndifferent mean B. The convention is to do something much simpler. We’ll start simple.\\nThe first model, containing no interaction at all (only “main effects”), begins this way:\\nBi ∼Normal(µi, σ)\\nµi = α + βW(Wi −¯W) + βS(Si −¯S)\\n'},\n",
       " {'index': 272,\n",
       "  'number': 254,\n",
       "  'content': '254\\n8. CONDITIONAL MANATEES\\nwhere Bi is the value of blooms on row i, Wi is the value of water, and Si is the value of\\nshade. The symbols ¯W and ¯S are the means of water and shade, respectively. All together,\\nthis is just a linear regression with two predictors, each centered by subtracting its mean.\\nTo make estimation easier, let’s center W and S and scale B by its maximum:\\nR code\\n8.20\\nd$blooms_std <- d$blooms / max(d$blooms)\\nd$water_cent <- d$water - mean(d$water)\\nd$shade_cent <- d$shade - mean(d$shade)\\nNow blooms_std ranges from 0 to 1, and both water_cent and shade_cent range from\\n−1 to 1. I’ve scaled blooms by its maximum observed value, for three reasons. First, the\\nlarge values on the raw scale will make optimization difficult. Second, it will be easier to\\nassign a reasonable prior this way. Third, we don’t want to standardize blooms, because zero\\nis a meaningful boundary we want to preserve.\\nWhen rescaling variables, a good goal is to create focal points that you have prior infor-\\nmation about, prior to seeing the actual data. That way we can assign priors that are not\\nobviously crazy. And in thinking about those priors, we might realize that the model makes\\nno sense. But this is only possible if we think about the relationship between measurements\\nand parameters. The exercise of rescaling and assigning priors helps. Even when there are\\nenough data that choice of priors is not crucial, this thought exercise is useful.\\nThere are three parameters (aside from σ) in this model, so we need three priors. As a\\nfirst, vague guess:\\nα ∼Normal(0.5, 1)\\nβW ∼Normal(0, 1)\\nβS ∼Normal(0, 1)\\nCentering the prior for α at 0.5 implies that, when both water and shade are at their mean\\nvalues, the model expects blooms to be halfway to the observed maximum. The two slopes\\nare centered on zero, implying no prior information about direction. This is obviously less\\ninformation than we have—basic botany informs us that water should have a positive slope\\nand shade a negative slope. But these priors allow us to see which trend the sample shows,\\nwhile still bounding the slopes to reasonable values. In the practice problems at the end of\\nthe chapter, I’ll ask you to use your botany instead.\\nThe prior bounds on the parameters come from the prior standard deviations, all set to\\n1 here. These are surely too broad. The intercept α must be greater than zero and less than\\none, for example. But this prior assigns most of the probability outside that range:\\nR code\\n8.21\\na <- rnorm( 1e4 , 0.5 , 1 ); sum( a < 0 | a > 1 ) / length( a )\\n[1] 0.6126\\nIf it’s 0.5 units from the mean to zero, then a standard deviation of 0.25 should put only 5%\\nof the mass outside the valid internal. Let’s see:\\nR code\\n8.22\\na <- rnorm( 1e4 , 0.5 , 0.25 ); sum( a < 0 | a > 1 ) / length( a )\\n[1] 0.0486\\n'},\n",
       " {'index': 273,\n",
       "  'number': 255,\n",
       "  'content': '8.3. CONTINUOUS INTERACTIONS\\n255\\nMuch better. What about those slopes? What would a very strong effect of water and shade\\nlook like? How big could those slopes be in theory? The range of both water and shade is 2—\\nfrom −1 to 1 is 2 units. To take us from the theoretical minimum of zero blooms on one end\\nto the observed maximum of 1—a range of 1 unit—on the other would require a slope of 0.5\\nfrom either variable—0.5 × 2 = 1. So if we assign a standard deviation of 0.25 to each, then\\n95% of the prior slopes are from −0.5 to 0.5, so either variable could in principle account\\nfor the entire range, but it would be unlikely. Remember, the goals here are to assign weakly\\ninformative priors to discourage overfitting—impossibly large effects should be assigned low\\nprior probability—and also to force ourselves to think about what the model means.\\nAll together now, in code form:\\nR code\\n8.23\\nm8.4 <- quap(\\nalist(\\nblooms_std ~ dnorm( mu , sigma ) ,\\nmu <- a + bw*water_cent + bs*shade_cent ,\\na ~ dnorm( 0.5 , 0.25 ) ,\\nbw ~ dnorm( 0 , 0.25 ) ,\\nbs ~ dnorm( 0 , 0.25 ) ,\\nsigma ~ dexp( 1 )\\n) , data=d )\\nIt’s a good idea at this point to simulate lines from the prior. But before doing that, let’s\\ndefine the interaction model as well. Then we can talk about how to plot predictions from\\ninteractions and see both prior and posterior predictions together.\\nTo build an interaction between water and shade, we need to construct µ so that the\\nimpact of changing either water or shade depends upon the value of the other variable. For\\nexample, if water is low, then decreasing the shade can’t help as much as when water is high.\\nWe want the slope of water, βW, to be conditional on shade. Likewise for shade being condi-\\ntional on water (remember Buridan’s interaction, page 250). How can we do this?\\nIn the previous example, terrain ruggedness, we made a slope conditional on the value of\\na category. When there are, in principle, an infinite number of categories, then it’s harder. In\\nthis case, the “categories” of shade and water are, in principle, infinite and ordered. We only\\nobserved three levels of water, but the model should be able to make a prediction with a water\\nlevel intermediate between any two of the observed ones. With continuous interactions, the\\nproblem isn’t so much the infinite part but rather the ordered part. Even if we only cared\\nabout the three observed values, we’d still need to preserve the ordering, which is bigger\\nthan which. So what to do?\\nThe conventional answer is to reapply the original geocentrism that justifies a linear re-\\ngression. When we have two variable, an outcome and a predictor, and we wish to model\\nthe mean of the outcome such that it is conditional on the value of a continuous predictor x,\\nwe can use a linear model: µi = α + βxi. Now in order to make the slope β conditional on\\nyet another variable, we can just recursively apply the same trick.\\nFor brevity, let Wi and Si be the centered variables. Then if we define the slope βW with\\nits own linear model γW:\\nµi = α + γW,iWi + βSSi\\nγW,i = βW + βWSSi\\n'},\n",
       " {'index': 274,\n",
       "  'number': 256,\n",
       "  'content': '256\\n8. CONDITIONAL MANATEES\\nNow γW,i is the slope defining how quickly blooms change with water level. The parameter\\nβW is the rate of change, when shade is at its mean value. And βWS is the rate change in γW,i\\nas shade changes—the slope for shade on the slope of water. Remember, it’s turtles all the\\nway down. Note the i in γW,i—it depends upon the row i, because it has Si in it.\\nWe also want to allow the association with shade, βS, to depend upon water. Luckily,\\nbecause of the symmetry of simple interactions, we get this for free. There is just no way\\nto specify a simple, linear interaction in which you can say the effect of some variable x\\ndepends upon z but the effect of z does not depend upon x. I explain this in more detail in\\nthe Overthinking box at the end of this section. The impact of this is that it is conventional\\nto substitute γW,i into the equation for µi and just state:\\nµi = α + (βW + βWSSi)\\n|\\n{z\\n}\\nγW,i\\nWi + βSSi = α + βWWi + βSSi + βWSSiWi\\nI just distributed the Wi and then placed the SiWi term at the end. And that’s the conventional\\nform of a continuous interaction, with the extra term on the far right end holding the product\\nof the two variables.\\nLet’s put this to work on the tulips. The interaction model is:\\nBi ∼Normal(µi, σ)\\nµi = α + βWWi + βSSi + βWSWiSi\\nThe last thing we need is a prior for this new interaction parameter, βWS. This is hard, because\\nthese epicycle parameters don’t have clear natural meaning. Still, implied predictions help.\\nSuppose the strongest plausible interaction is one in which high enough shade makes water\\nhave zero effect. That implies:\\nγW,i = βw + βWSSi = 0\\nIf we set Si = 1 (the maximum in the sample), then this means the interaction needs to\\nbe the same magnitude as the main effect, but reversed: βWS = −βW. That is the largest\\nconceivable interaction. So if we set the prior for βWS to have the same standard deviation\\nas βW, maybe that isn’t ridiculous. All together now, in code form:\\nR code\\n8.24\\nm8.5 <- quap(\\nalist(\\nblooms_std ~ dnorm( mu , sigma ) ,\\nmu <- a + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent ,\\na ~ dnorm( 0.5 , 0.25 ) ,\\nbw ~ dnorm( 0 , 0.25 ) ,\\nbs ~ dnorm( 0 , 0.25 ) ,\\nbws ~ dnorm( 0 , 0.25 ) ,\\nsigma ~ dexp( 1 )\\n) , data=d )\\nAnd that’s the structure of a simple, continuous interaction. You can inspect the precis\\noutput. You’ll see that bws is negative. What does that imply, on the outcome scale? It’s really\\nnot easy to imagine from the parameters alone, especially since the values in the predictors\\nare both negative and positive.\\nSo next, let’s figure out how to plot these creatures.\\n'},\n",
       " {'index': 275,\n",
       "  'number': 257,\n",
       "  'content': '8.3. CONTINUOUS INTERACTIONS\\n257\\nOverthinking: How is interaction formed? As in the main text, if you substitute γW,i into µi above\\nand expand:\\nµi = α + (βW + βWSSi)Wi + βSSi = α + βWWi + βSSi + βWSSiWi\\nNow it’s possible to refactor this to construct a γS,i that makes the association of shade with blooms\\ndepend upon water:\\nµi = α + βWWi + γS,iSi\\nγS,i = βS + βSWWi\\nSo both interpretations are simultaneously true. You could even put both γ definitions into µ at the\\nsame time:\\nµi = α + γW,iWi + γS,iSi\\nγW,i = βW + βWSSi\\nγS,i = βS + βSWWi\\nNote that I defined two different interaction parameters: βWS and βSW. Now let’s substitute the γ\\ndefinitions into µ and start factoring:\\nµi = α + (βW + βWSSi)Wi + (βS + βSWWi)Si\\n= α + βWWi + βSSi + (βWS + βSW)WiSi\\nThe only thing we can identify in such a model is the sum βWS + βSW, so really the sum is a single\\nparameter (dimension in the posterior). It’s the same interaction model all over again. We just cannot\\ntell the difference between water depending upon shade and shade depending upon water.\\nA more principled way to construct µi is to start with the derivatives ∂µi/∂Wi = βW + βWSSi\\nand ∂µi/∂Si = βS + βWSWi. Finding a function µi that satisfies both yields the traditional model.\\nBy including boundary conditions and other prior knowledge, you can use the same strategy to find\\nfancier functions. But the derivation could be harder. So you might want to consult a friendly neigh-\\nborhood mathematician in that case.\\n8.3.3. Plotting posterior predictions. Golems (models) have awesome powers of reason,\\nbut terrible people skills. The golem provides a posterior distribution of plausibility for com-\\nbinations of parameter values. But for us humans to understand its implications, we need to\\ndecode the posterior into something else. Centered predictors or not, plotting posterior pre-\\ndictions always tells you what the golem is thinking, on the scale of the outcome. That’s why\\nwe’ve emphasized plotting so much. But in previous chapters, there were no interactions. As\\na result, when plotting model predictions as a function of any one predictor, you could hold\\nthe other predictors constant at any value you liked. So the choice of which values to set the\\nun-viewed predictor variables to hardly mattered.\\nNow that’ll be different. Once there are interactions in a model, the effect of changing\\na predictor depends upon the values of the other predictors. Maybe the simplest way to go\\nabout plotting such interdependency is to make a frame of multiple bivariate plots. In each\\nplot, you choose different values for the un-viewed variables. Then by comparing the plots\\nto one another, you can see how big of a difference the changes make.\\nThat’s what we did for the terrain ruggedness example. But there we needed only two\\nplots, one for Africa and one for everyplace else. Now we’ll need more. Here’s how you\\nmight accomplish this visualization, for the tulip data. I’m going to make three plots in a\\nsingle panel. Such a panel of three plots that are meant to be viewed together is a triptych,\\nand triptych plots are very handy for understanding the impact of interactions. Here’s the\\nstrategy. We want each plot to show the bivariate relationship between water and blooms,\\n'},\n",
       " {'index': 276,\n",
       "  'number': 258,\n",
       "  'content': '258\\n8. CONDITIONAL MANATEES\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.4 post: shade = -1\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.4 post: shade = 0\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.4 post: shade = 1\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.5 post: shade = -1\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.5 post: shade = 0\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.5 post: shade = 1\\nFigure 8.7. Triptych plots of posterior predicted blooms across water and\\nshade treatments. Top row: Without an interaction between water and\\nshade. Bottom row: With an interaction between water and shade. Each\\nplot shows 20 posterior lines for each level of shade.\\nas predicted by the model. Each plot will plot predictions for a different value of shade. For\\nthis example, it is easy to pick which three values of shade to use, because there are only\\nthree values: −1, 0, and 1. But more generally, you might use a representative low value, the\\nmedian, and a representative high value.\\nHere’s the code to draw posterior predictions for m8.4, the non-interaction model. This\\nwill loop over three values for shade, compute posterior predictions, then draw 20 lines from\\nthe posterior.\\nR code\\n8.25\\npar(mfrow=c(1,3)) # 3 plots in 1 row\\nfor ( s in -1:1 ) {\\nidx <- which( d$shade_cent==s )\\nplot( d$water_cent[idx] , d$blooms_std[idx] , xlim=c(-1,1) , ylim=c(0,1) ,\\nxlab=\"water\" , ylab=\"blooms\" , pch=16 , col=rangi2 )\\nmu <- link( m8.4 , data=data.frame( shade_cent=s , water_cent=-1:1 ) )\\nfor ( i in 1:20 ) lines( -1:1 , mu[i,] , col=col.alpha(\"black\",0.3) )\\n}\\nThe result is shown in Figure 8.7, along with the same type of plot for the interaction model,\\nm8.5. Notice that the top model believes that water helps—there is a positive slope in each\\nplot—and that shade hurts—the lines sink lower moving from left to right. But the slope\\n'},\n",
       " {'index': 277,\n",
       "  'number': 259,\n",
       "  'content': '8.3. CONTINUOUS INTERACTIONS\\n259\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.4 prior: shade = -1\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.4 prior: shade = 0\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.4 prior: shade = 1\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.5 prior: shade = -1\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.5 prior: shade = 0\\nwater\\nblooms\\n-1\\n0\\n1\\n0\\n0.5\\n1\\nm8.5 prior: shade = 1\\nFigure 8.8. Triptych plots of prior predicted blooms across water and\\nshade treatments. Top row: Without an interaction between water and\\nshade. Bottom row: With an interaction between water and shade. Each\\nplot shows 20 prior lines for each level of shade.\\nwith water doesn’t vary across shade levels. Without the interaction, it cannot vary. In the\\nbottom row, the interaction is turned on. Now the model believes that the effect of water\\ndecreases as shade increases. The lines get flat.\\nWhat is going on here? The likely explanation for these results is that tulips need both\\nwater and light to produce blooms. At low light levels, water can’t have much of an effect,\\nbecause the tulips don’t have enough light to produce blooms. At higher light levels, water\\ncan matter more, because the tulips have enough light to produce blooms. At very high light\\nlevels, light is no longer limiting the blooms, and so water can have a much more dramatic\\nimpact on the outcome. The same explanation works symmetrically for shade. If there isn’t\\nenough light, then more water hardly helps. You could remake Figure 8.7 with shade on the\\nhorizontal axes and water level varied from left to right, if you’d like to visualize the model\\npredictions that way.\\n8.3.4. Plotting prior predictions. And we can use the same technique to finally plot prior\\npredictive simulations as well. This will let us evaluate my guesses from earlier. To produce\\nthe prior predictions, all that’s need is to extract the prior:\\nR code\\n8.26\\nset.seed(7)\\nprior <- extract.prior(m8.5)\\n'},\n",
       " {'index': 278,\n",
       "  'number': 260,\n",
       "  'content': '260\\n8. CONDITIONAL MANATEES\\nAnd then add post=prior as an argument to the link call in the previous code. I’ve also\\nadjusted the vertical range of the prior plots, so we can see more easily the lines that fall\\noutside the valid outcome range.\\nThe result is displayed as Figure 8.8. Since the lines are so scattered in the prior—the\\nprior not very informative—it is hard to see that the lines from the same set of samples actu-\\nally go together in meaningful ways. So I’ve bolded three lines in the top and in the bottom\\nrows. The three bolded lines in the top row come from the same parameter values. Notice\\nthat all three have the same slope. This is what we expect from a model without an interac-\\ntion. So while the lines in the prior have lots of different slopes, the slopes for water don’t\\ndepend upon shade. In the bottom row, the three bolded lines again come from a single prior\\nsample. But now the interaction makes the slope systematically change as shade changes.\\nWhat can we say about these priors, overall? They are harmless, but only weakly realistic.\\nMost of the lines stay within the valid outcome space. But silly trends are not rare. We could\\ndo better. We could also do a lot worse, such as flat priors which would consider plausible\\nthat even a tiny increase in shade would kill all the tulips. If you displayed these priors to your\\ncolleagues, a reasonable summary might be, “These priors contain no bias towards positive or\\nnegative effects, and at the same time they very weakly bound the effects to realistic ranges.”\\n8.4. Summary\\nThis chapter introduced interactions, which allow for the association between a predictor\\nand an outcome to depend upon the value of another predictor. While you can’t see them in\\na DAG, interactions can be important for making accurate inferences. Interactions can be\\ndifficult to interpret, and so the chapter also introduced triptych plots that help in visualizing\\nthe effect of an interaction. No new coding skills were introduced, but the statistical models\\nconsidered were among the most complicated so far in the book. To go any further, we’re\\ngoing to need a more capable conditioning engine to fit our models to data. That’s the topic\\nof the next chapter.\\n8.5. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n8E1. For each of the causal relationships below, name a hypothetical third variable that would lead\\nto an interaction effect.\\n(1) Bread dough rises because of yeast.\\n(2) Education leads to higher income.\\n(3) Gasoline makes a car go.\\n8E2. Which of the following explanations invokes an interaction?\\n(1) Caramelizing onions requires cooking over low heat and making sure the onions do not\\ndry out.\\n(2) A car will go faster when it has more cylinders or when it has a better fuel injector.\\n(3) Most people acquire their political beliefs from their parents, unless they get them instead\\nfrom their friends.\\n(4) Intelligent animal species tend to be either highly social or have manipulative appendages\\n(hands, tentacles, etc.).\\n8E3. For each of the explanations in 8E2, write a linear model that expresses the stated relationship.\\n'},\n",
       " {'index': 279,\n",
       "  'number': 261,\n",
       "  'content': '8.5. PRACTICE\\n261\\n8M1. Recall the tulips example from the chapter. Suppose another set of treatments adjusted the\\ntemperature in the greenhouse over two levels: cold and hot. The data in the chapter were collected\\nat the cold temperature. You find none of the plants grown under the hot temperature developed\\nany blooms at all, regardless of the water and shade levels. Can you explain this result in terms of\\ninteractions between water, shade, and temperature?\\n8M2. Can you invent a regression equation that would make the bloom size zero, whenever the\\ntemperature is hot?\\n8M3. In parts of North America, ravens depend upon wolves for their food. This is because ravens\\nare carnivorous but cannot usually kill or open carcasses of prey. Wolves however can and do kill\\nand tear open animals, and they tolerate ravens co-feeding at their kills. This species relationship\\nis generally described as a “species interaction.” Can you invent a hypothetical set of data on raven\\npopulation size in which this relationship would manifest as a statistical interaction? Do you think\\nthe biological interaction could be linear? Why or why not?\\n8M4. Repeat the tulips analysis, but this time use priors that constrain the effect of water to be pos-\\nitive and the effect of shade to be negative. Use prior predictive simulation. What do these prior\\nassumptions mean for the interaction prior, if anything?\\n8H1. Return to the data(tulips) example in the chapter. Now include the bed variable as a pre-\\ndictor in the interaction model. Don’t interact bed with the other predictors; just include it as a main\\neffect. Note that bed is categorical. So to use it properly, you will need to either construct dummy\\nvariables or rather an index variable, as explained in Chapter 5.\\n8H2. Use WAIC to compare the model from 8H1 to a model that omits bed. What do you infer\\nfrom this comparison? Can you reconcile the WAIC results with the posterior distribution of the bed\\ncoefficients?\\n8H3. Consider again the data(rugged) data on economic development and terrain ruggedness,\\nexamined in this chapter. One of the African countries in that example, Seychelles, is far outside\\nthe cloud of other nations, being a rare country with both relatively high GDP and high ruggedness.\\nSeychelles is also unusual, in that it is a group of islands far from the coast of mainland Africa, and\\nits main economic activity is tourism.\\n(a) Focus on model m8.5 from the chapter. Use WAIC pointwise penalties and PSIS Pareto k\\nvalues to measure relative influence of each country. By these criteria, is Seychelles influencing the\\nresults? Are there other nations that are relatively influential? If so, can you explain why?\\n(b) Now use robust regression, as described in the previous chapter. Modify m8.5 to use a\\nStudent-t distribution with ν = 2. Does this change the results in a substantial way?\\n8H4. The values in data(nettle) are data on language diversity in 74 nations.143 The meaning of\\neach column is given below.\\n(1) country: Name of the country\\n(2) num.lang: Number of recognized languages spoken\\n(3) area: Area in square kilometers\\n(4) k.pop: Population, in thousands\\n(5) num.stations: Number of weather stations that provided data for the next two columns\\n(6) mean.growing.season: Average length of growing season, in months\\n(7) sd.growing.season: Standard deviation of length of growing season, in months\\nUse these data to evaluate the hypothesis that language diversity is partly a product of food secu-\\nrity. The notion is that, in productive ecologies, people don’t need large social networks to buffer them\\nagainst risk of food shortfalls. This means cultural groups can be smaller and more self-sufficient,\\nleading to more languages per capita. Use the number of languages per capita as the outcome:\\n'},\n",
       " {'index': 280,\n",
       "  'number': 262,\n",
       "  'content': '262\\n8. CONDITIONAL MANATEES\\nR code\\n8.27\\nd$lang.per.cap <- d$num.lang / d$k.pop\\nUse the logarithm of this new variable as your regression outcome. (A count model would be bet-\\nter here, but you’ll learn those later, in Chapter 11.) This problem is open ended, allowing you to\\ndecide how you address the hypotheses and the uncertain advice the modeling provides. If you\\nthink you need to use WAIC anyplace, please do. If you think you need certain priors, argue for\\nthem. If you think you need to plot predictions in a certain way, please do. Just try to honestly\\nevaluate the main effects of both mean.growing.season and sd.growing.season, as well as their\\ntwo-way interaction. Here are three parts to help. (a) Evaluate the hypothesis that language diversity,\\nas measured by log(lang.per.cap), is positively associated with the average length of the grow-\\ning season, mean.growing.season. Consider log(area) in your regression(s) as a covariate (not\\nan interaction). Interpret your results. (b) Now evaluate the hypothesis that language diversity is\\nnegatively associated with the standard deviation of length of growing season, sd.growing.season.\\nThis hypothesis follows from uncertainty in harvest favoring social insurance through larger social\\nnetworks and therefore fewer languages. Again, consider log(area) as a covariate (not an inter-\\naction). Interpret your results. (c) Finally, evaluate the hypothesis that mean.growing.season and\\nsd.growing.season interact to synergistically reduce language diversity. The idea is that, in nations\\nwith longer average growing seasons, high variance makes storage and redistribution even more im-\\nportant than it would be otherwise. That way, people can cooperate to preserve and protect windfalls\\nto be used during the droughts.\\n8H5. Consider the data(Wines2012) data table. These data are expert ratings of 20 different French\\nand American wines by 9 different French and American judges. Your goal is to model score, the\\nsubjective rating assigned by each judge to each wine. I recommend standardizing it. In this problem,\\nconsider only variation among judges and wines. Construct index variables of judge and wine and\\nthen use these index variables to construct a linear regression model. Justify your priors. You should\\nend up with 9 judge parameters and 20 wine parameters. How do you interpret the variation among\\nindividual judges and individual wines? Do you notice any patterns, just by plotting the differences?\\nWhich judges gave the highest/lowest ratings? Which wines were rated worst/best on average?\\n8H6. Now consider three features of the wines and judges:\\n(1) flight: Whether the wine is red or white.\\n(2) wine.amer: Indicator variable for American wines.\\n(3) judge.amer: Indicator variable for American judges.\\nUse indicator or index variables to model the influence of these features on the scores. Omit the\\nindividual judge and wine index variables from Problem 1. Do not include interaction effects yet.\\nAgain justify your priors. What do you conclude about the differences among the wines and judges?\\nTry to relate the results to the inferences in the previous problem.\\n8H7. Now consider two-way interactions among the three features. You should end up with three\\ndifferent interaction terms in your model. These will be easier to build, if you use indicator variables.\\nAgain justify your priors. Explain what each interaction means. Be sure to interpret the model’s\\npredictions on the outcome scale (mu, the expected score), not on the scale of individual parameters.\\nYou can use link to help with this, or just use your knowledge of the linear model instead. What do\\nyou conclude about the features and the scores? Can you relate the results of your model(s) to the\\nindividual judge and wine inferences from 8H5?\\n'},\n",
       " {'index': 281,\n",
       "  'number': 263,\n",
       "  'content': '9 Markov Chain Monte Carlo\\nIn the twentieth century, scientists and engineers began publishing books of random\\nnumbers (Figure9.1). For scientists from previous centuries, these books would have looked\\nlike madness. For most of Western history, chance has been a villain. In classical Rome,\\nchance was personified by Fortuna, goddess of cruel fate, with her spinning wheel of (mis)for-\\ntune. Opposed to her sat Minerva, goddess of wisdom and understanding. Only the desper-\\nate would pray to Fortuna, while everyone implored Minerva for aid. Certainly science was\\nthe domain of Minerva, a realm with no useful role for Fortuna to play.\\nBut by the twentieth century, Fortuna and Minerva had become collaborators. Now\\nfew of us are bewildered by the notion that an understanding of chance could help us ac-\\nquire wisdom. Everything from weather forecasting to finance to evolutionary biology is\\ndominated by the study of stochastic processes.144 Researchers rely upon random numbers\\nfor the proper design of experiments. And mathematicians routinely make use of random\\ninputs to compute specific outputs.\\nThis chapter introduces one commonplace example of Fortuna and Minerva’s coopera-\\ntion: the estimation of posterior probability distributions using a stochastic process known\\nas Markov chain Monte Carlo (MCMC). Unlike earlier chapters in this book, here we’ll\\nproduce samples from the joint posterior without maximizing anything. Instead of having\\nto lean on quadratic and other approximations of the shape of the posterior, now we’ll be\\nable to sample directly from the posterior without assuming a Gaussian, or any other, shape.\\nThe cost of this power is that it may take much longer for our estimation to complete,\\nand usually more work is required to specify the model as well. But the benefit is escaping the\\nawkwardness of assuming multivariate normality. Equally important is the ability to directly\\nestimate models, such as the generalized linear and multilevel models of later chapters. Such\\nmodels routinely produce non-Gaussian posterior distributions, and sometimes they cannot\\nbe estimated at all with the techniques of earlier chapters.\\nThe good news is that tools for building and inspecting MCMC estimates are getting\\nbetter all the time. In this chapter you’ll meet a convenient way to convert the quap formulas\\nyou’ve used so far into Markov chains. The engine that makes this possible is Stan (free and\\nonline at: mc-stan.org). Stan’s creators describe it as “a probabilistic programming language\\nimplementing statistical inference.” You won’t be working directly in Stan to begin with—\\nthe rethinking package provides tools that hide it from you for now. But as you move\\non to more advanced techniques, you’ll be able to generate Stan versions of the models you\\nalready understand. Then you can tinker with them and witness the power of a fully armed\\nand operational Stan.\\n263\\n'},\n",
       " {'index': 282,\n",
       "  'number': 264,\n",
       "  'content': '264\\n9. MARKOV CHAIN MONTE CARLO\\nFigure 9.1. A page from A Million Random Digits, a book consisting of\\nnothing but random numbers.\\nRethinking: Stan was a man. The Stan programming language is not an abbreviation or acronym.\\nRather, it is named after Stanisław Ulam (1909–1984). Ulam is credited as one of the inventors of\\nMarkov chain Monte Carlo. Together with Ed Teller, Ulam applied it to designing fusion bombs. But\\nhe and others soon applied the general Monte Carlo method to diverse problems of less monstrous\\nnature. Ulam made important contributions in pure mathematics, chaos theory, and molecular and\\ntheoretical biology, as well.\\n9.1. Good King Markov and his island kingdom\\nFor the moment, forget about posterior densities and MCMC. Consider instead the tale\\nof Good King Markov.145 King Markov was a benevolent autocrat of an island kingdom,\\na circular archipelago, with 10 islands. Each island was neighbored by two others, and the\\nentire archipelago formed a ring. The islands were of different sizes, and so had different\\nsized populations living on them. The second island was about twice as populous as the first,\\nthe third about three times as populous as the first, and so on, up to the largest island, which\\nwas 10 times as populous as the smallest.\\nThe Good King was an autocrat, but he did have a number of obligations to his people.\\nAmong these obligations, King Markov agreed to visit each island in his kingdom from time\\nto time. Since the people loved their king, each island preferred that he visit them more often.\\nAnd so everyone agreed that the king should visit each island in proportion to its population\\nsize, visiting the largest island 10 times as often as the smallest, for example.\\nThe Good King Markov, however, wasn’t one for schedules or bookkeeping, and so he\\nwanted a way to fulfill his obligation without planning his travels months ahead of time. Also,\\nsince the archipelago was a ring, the King insisted that he only move among adjacent islands,\\n'},\n",
       " {'index': 283,\n",
       "  'number': 265,\n",
       "  'content': '9.1. GOOD KING MARKOV AND HIS ISLAND KINGDOM\\n265\\nto minimize time spent on the water—like many citizens of his kingdom, the king believed\\nthere were sea monsters in the middle of the archipelago.\\nThe king’s advisor, a Mr Metropolis, engineered a clever solution to these demands. We’ll\\ncall this solution the Metropolis algorithm. Here’s how it works.\\n(1) Wherever the King is, each week he decides between staying put for another week\\nor moving to one of the two adjacent islands. To decide, he flips a coin.\\n(2) If the coin turns up heads, the King considers moving to the adjacent island clock-\\nwise around the archipelago. If the coin turns up tails, he considers instead moving\\ncounterclockwise. Call the island the coin nominates the proposal island.\\n(3) Now, to see whether or not he moves to the proposal island, King Markov counts\\nout a number of seashells equal to the relative population size of the proposal is-\\nland. So for example, if the proposal island is number 9, then he counts out 9\\nseashells. Then he also counts out a number of stones equal to the relative popula-\\ntion of the current island. So for example, if the current island is number 10, then\\nKing Markov ends up holding 10 stones, in addition to the 9 seashells.\\n(4) When there are more seashells than stones, King Markov always moves to the pro-\\nposal island. But if there are fewer shells than stones, he discards a number of stones\\nequal to the number of shells. So for example, if there are 4 shells and 6 stones, he\\nends up with 4 shells and 6 −4 = 2 stones. Then he places the shells and the re-\\nmaining stones in a bag. He reaches in and randomly pulls out one object. If it is a\\nshell, he moves to the proposal island. Otherwise, he stays put another week. As a\\nresult, the probability that he moves is equal to the number of shells divided by the\\noriginal number of stones.\\nThis procedure may seem baroque and, honestly, a bit crazy. But it does work. The king will\\nappear to move around the islands randomly, sometimes staying on one island for weeks,\\nother times bouncing around without apparent pattern. But in the long run, this procedure\\nguarantees that the king will be found on each island in proportion to its population size.\\nYou can prove this to yourself, by simulating King Markov’s journey. Here’s a short piece\\nof code to do this, storing the history of the king’s journey in the vector positions:\\nR code\\n9.1\\nnum_weeks <- 1e5\\npositions <- rep(0,num_weeks)\\ncurrent <- 10\\nfor ( i in 1:num_weeks ) {\\n## record current position\\npositions[i] <- current\\n## flip coin to generate proposal\\nproposal <- current + sample( c(-1,1) , size=1 )\\n## now make sure he loops around the archipelago\\nif ( proposal < 1 ) proposal <- 10\\nif ( proposal > 10 ) proposal <- 1\\n## move?\\nprob_move <- proposal/current\\ncurrent <- ifelse( runif(1) < prob_move , proposal , current )\\n}\\nI’ve added comments to this code, to help you decipher it. The first three lines just define\\nthe number of weeks to simulate, an empty history vector, and a starting island position (the\\n'},\n",
       " {'index': 284,\n",
       "  'number': 266,\n",
       "  'content': '266\\n9. MARKOV CHAIN MONTE CARLO\\n0\\n20\\n40\\n60\\n80\\n100\\n2\\n4\\n6\\n8\\n10\\nweek\\nisland\\n2\\n4\\n6\\n8\\n10\\n0\\n5000\\n10000\\n15000\\nisland\\nnumber of weeks\\nFigure 9.2. Results of the king following the Metropolis algorithm. The\\nleft-hand plot shows the king’s position (vertical axis) across weeks (hori-\\nzontal axis). In any particular week, it’s nearly impossible to say where the\\nking will be. The right-hand plot shows the long-run behavior of the algo-\\nrithm, as the time spent on each island turns out to be proportional to its\\npopulation size.\\nbiggest island, number 10). Then the for loop steps through the weeks. Each week, it records\\nthe king’s current position. Then it simulates a coin flip to nominate a proposal island. The\\nonly trick here lies in making sure that a proposal of “11” loops around to island 1 and a\\nproposal of “0” loops around to island 10. Finally, a random number between zero and one\\nis generated (runif(1)), and the king moves, if this random number is less than the ratio of\\nthe proposal island’s population to the current island’s population (proposal/current).\\nYou can see the results of this simulation in Figure 9.2. The left-hand plot shows the\\nking’s location across the first 100 weeks of his simulated travels.\\nR code\\n9.2\\nplot( 1:100 , positions[1:100] )\\nAs you move from the left to the right in this plot, the points show the king’s location through\\ntime. The king travels among islands, or sometimes stays in place for a few weeks. This plot\\ndemonstrates the seemingly pointless path the Metropolis algorithm sends the king on. The\\nright-hand plot shows that the path is far from pointless, however.\\nR code\\n9.3\\nplot( table( positions ) )\\nThe horizontal axis is now islands (and their relative populations), while the vertical is the\\nnumber of weeks the king is found on each. After the entire 100,000 weeks (almost 2000\\nyears) of the simulation, you can see that the proportion of time spent on each island con-\\nverges to be almost exactly proportional to the relative populations of the islands.\\nThe algorithm will still work in this way, even if we allow the king to be equally likely\\nto propose a move to any island from any island, not just among neighbors. As long as\\n'},\n",
       " {'index': 285,\n",
       "  'number': 267,\n",
       "  'content': '9.2. METROPOLIS ALGORITHMS\\n267\\nKing Markov still uses the ratio of the proposal island’s population to the current island’s\\npopulation as his probability of moving, in the long run, he will spend the right amount of\\ntime on each island. The algorithm would also work for any size archipelago, even if the king\\ndidn’t know how many islands were in it. All he needs to know at any point in time is the\\npopulation of the current island and the population of the proposal island. Then, without any\\nforward planning or backwards record keeping, King Markov can satisfy his royal obligation\\nto visit his people proportionally.\\n9.2. Metropolis algorithms\\nThe precise algorithm King Markov used is a special case of the general Metropolis\\nalgorithm from the real world.146 And this algorithm is an example of Markov chain\\nMonte Carlo. In real applications, the goal is of course not to help an autocrat schedule\\nhis journeys, but instead to draw samples from an unknown and usually complex target\\ndistribution, like a posterior probability distribution.\\n• The “islands” in our objective are parameter values, and they need not be discrete,\\nbut can instead take on a continuous range of values as usual.\\n• The “population sizes” in our objective are the posterior probabilities at each pa-\\nrameter value.\\n• The “weeks” in our objective are samples taken from the joint posterior of the pa-\\nrameters in the model.\\nProvided the way we choose our proposed parameter values at each step is symmetric—so\\nthat there is an equal chance of proposing from A to B and from B to A—then the Metropolis\\nalgorithm will eventually give us a collection of samples from the joint posterior. We can then\\nuse these samples just like all the samples you’ve already used in this book.\\nThe Metropolis algorithm is the grandparent of several different strategies for getting\\nsamples from unknown posterior distributions. In the remainder of this section, I briefly\\nexplain the concept behind Gibbs sampling. Gibbs sampling is much better than plain Me-\\ntropolis, and it continues to be common in applied Bayesian statistics. But it is rapidly being\\nreplaced by other algorithms.\\n9.2.1. Gibbssampling. The Metropolis algorithm works whenever the probability of propos-\\ning a jump to B from A is equal to the probability of proposing A from B, when the pro-\\nposal distribution is symmetric. There is a more general method, known as Metropolis-\\nHastings,147 that allows asymmetric proposals. This would mean, in the context of King\\nMarkov’s fable, that the King’s coin were biased to lead him clockwise on average.\\nWhy would we want an algorithm that allows asymmetric proposals? One reason is that\\nit makes it easier to handle parameters, like standard deviations, that have boundaries at\\nzero. A better reason, however, is that it allows us to generate savvy proposals that explore\\nthe posterior distribution more efficiently. By “more efficiently,” I mean that we can acquire\\nan equally good image of the posterior distribution in fewer steps.\\nThe most common way to generate savvy proposals is a technique known as Gibbs sam-\\npling.148 Gibbs sampling is a variant of the Metropolis-Hastings algorithm that uses clever\\nproposals and is therefore more efficient. By “efficient,” I mean that you can get a good\\nestimate of the posterior from Gibbs sampling with many fewer samples than a comparable\\nMetropolis approach. The improvement arises from adaptive proposals in which the distribu-\\ntion of proposed parameter values adjusts itself intelligently, depending upon the parameter\\nvalues at the moment.\\n'},\n",
       " {'index': 286,\n",
       "  'number': 268,\n",
       "  'content': '268\\n9. MARKOV CHAIN MONTE CARLO\\nHow Gibbs sampling computes these adaptive proposals depends upon using particu-\\nlar combinations of prior distributions and likelihoods known as conjugate pairs. Conjugate\\npairs have analytical solutions for the posterior distribution of an individual parameter. And\\nthese solutions are what allow Gibbs sampling to make smart jumps around the joint poste-\\nrior distribution of all parameters.\\nIn practice, Gibbs sampling can be very efficient, and it’s the basis of popular Bayesian\\nmodel fitting software like BUGS (Bayesian inference Using Gibbs Sampling) and JAGS (Just\\nAnother Gibbs Sampler). In these programs, you compose your statistical model using def-\\ninitions very similar to what you’ve been doing so far in this book. The software automates\\nthe rest, to the best of its ability.\\n9.2.2. High-dimensional problems. But there are some severe limitations to Gibbs sam-\\npling. First, maybe you don’t want to use conjugate priors. Some conjugate priors are ac-\\ntually pathological in shape, once you start building multilevel models and need priors for\\nentire covariance matrixes. This will be something to discuss once we reach Chapter 14.\\nSecond, as models become more complex and contain hundreds or thousands or tens\\nof thousands of parameters, both Metropolis and Gibbs sampling become shockingly ineffi-\\ncient. The reason is that they tend to get stuck in small regions of the posterior for potentially\\na long time. The high number of parameters isn’t the problem so much as the fact that mod-\\nels with many parameters nearly always have regions of high correlation in the posterior.\\nThis means that two or more parameters are highly correlated with one another in the pos-\\nterior samples. You’ve seen this before with, for example, the two legs example in Chapter 6.\\nWhy is this a problem? Because high correlation means a narrow ridge of high probability\\ncombinations, and both Metropolis and Gibbs make too many dumb proposals of where to\\ngo next. So they get stuck.\\nA picture will help to make this clearer. Figure 9.3 shows an ordinary Metropolis al-\\ngorithm trying to explore a 2-dimensional posterior with a strong negative correlation of\\n−0.9. The region of high-probability parameter values forms a narrow valley. Focus on the\\nleft-hand plot for now. The chain starts in the upper-left of the valley. Filled points are ac-\\ncepted proposals. Open points are rejected proposals. Proposals are generated by adding\\nrandom Gaussian noise to each parameter, using a standard deviation of 0.01, the step size.\\n50 proposals are shown. The acceptance rate is only 60%, because when the valley is narrow\\nlike this, proposals can easily fall outside it. But the chain does manage to move slowly down\\nthe valley. It moves slow, because even when a proposal is accepted, it is still close to the\\nprevious point.\\nWhat happens then if we increase the step size, for more distant proposals? Now look\\non the right in Figure 9.3. Only 30% of proposals are accepted now. A bigger step size\\nmeans more silly proposals outside the valley. The accepted proposals do move faster along\\nthe length of the valley, however. In practice, it is hard to win this tradeoff. Both Metropolis\\nand Gibbs get stuck like this, because their proposals don’t know enough about the global\\nshape of the posterior. They don’t know where they are going.\\nThe high correlation example illustrates the problem. But the actual problem is more\\nsevere and more interesting. Any Markov chain approach that samples individual parameters\\nin individual steps is going to get stuck, once the number of parameters grows sufficiently\\nlarge. The reason goes by the name concentration of measure. This is an awkward\\nname for the amazing fact that most of the probability mass of a high-dimension distribution\\nis always very far from the mode of the distribution. It is hard to visualize. We can’t see in 100\\n'},\n",
       " {'index': 287,\n",
       "  'number': 269,\n",
       "  'content': '9.2. METROPOLIS ALGORITHMS\\n269\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\na1\\na2\\nstep size 0.1, accept rate 0.62\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\na1\\na2\\nstep size 0.25, accept rate 0.34\\nFigure 9.3. Metropolis chains under high correlation. Filled points indi-\\ncate accepted proposals. Open points are rejected proposals. Both plots\\nshow 50 proposals under different proposal distribution step sizes. Left:\\nWith a small step size, the chain very slowly makes its way down the valley.\\nIt rejects 40% of the proposals in the process, because most of the proposals\\nare in silly places. Right: With a larger step size, the chain moves faster, but\\nit now rejects 70% of the proposals, because they tend to be even sillier. In\\nhigher dimensions, it is essentially impossible to tune Metropolis or Gibbs\\nto be efficient.\\ndimensions, on most days. But if we think about the 2D and 3D versions, we can understand\\nthe basic phenomenon. In two dimensions, a Gaussian distribution is a hill. The highest\\npoint is in the middle, at the mode. But if we imagine this hill is filled with dirt—what else\\nare hills filled with?—then we can ask: Where is most of the dirt? As we move away from the\\npeak in any direction, the altitude declines, so there is less dirt directly under our feet. But in\\nthe ring around the hill at the same distance, there is more dirt than there is at the peak. The\\narea increases as we move away from the peak, even though the height goes down. So the\\ntotal dirt, um probability, increases as we move away from the peak. Eventually the total dirt\\n(probability) declines again, as the hill slopes down to zero. So at some radial distance from\\nthe peak, dirt (probability mass) is maximized. In three dimensions, it isn’t a hill, but now a\\nfuzzy sphere. The sphere is densest at the core, its “peak.” But again the volume increases as\\nwe move away from the core. So there is more total sphere-stuff in a shell around the core.\\nBack to thinking of probability distributions, all of this means that the combination of\\nparameter values that maximizes posterior probability, the mode, is not actually in a region\\nof parameter values that are highly plausible. This means in turn that when we properly\\nsample from a high dimensional distribution, we won’t get any points near the mode. You\\ncan demonstrate this for yourself very easily. Just sample randomly from a high-dimension\\ndistribution—10 dimensions is enough—and plot the radial distances of the points. Here’s\\nsome code to do this:\\n'},\n",
       " {'index': 288,\n",
       "  'number': 270,\n",
       "  'content': '270\\n9. MARKOV CHAIN MONTE CARLO\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nRadial distance from mode\\nDensity\\n1\\n10\\n100\\n1000\\nFigure 9.4. Concentration of measure and the curse of high dimensions.\\nThe horizontal axis shows radial distance from the mode in parameter\\nspace. Each density is a random sample of 1000 points. The number above\\neach density is the number of dimensions. As the number of parameters\\nincreases, the mode is further away from the values we want to sample.\\nR code\\n9.4\\nD <- 10\\nT <- 1e3\\nY <- rmvnorm(T,rep(0,D),diag(D))\\nrad_dist <- function( Y ) sqrt( sum(Y^2) )\\nRd <- sapply( 1:T , function(i) rad_dist( Y[i,] ) )\\ndens( Rd )\\nI display this density, as well as the corresponding densities for distributions with 1, 100, and\\n1000 dimensions, in Figure 9.4. The horizontal axis here is radial distance of the point from\\nthe mode. So the value 0 is the peak of probability. You can see that an ordinary Gaussian\\ndistribution with only 1 dimension, on the left, samples most of its points right next to this\\npeak, as you’d expect. But with 10 dimensions, already there are no samples next to the\\npeak at zero. With 100 dimensions, we’ve moved very far from the peak. And with 1000\\ndimensions, even further. The sampled points are in a thin, high-dimensional shell very far\\nfrom the mode. This shell can create very hard paths for a sampler to follow.\\nThis is why we need MCMC algorithms that focus on the entire posterior at once, instead\\nof one or a few dimensions at a time like Metropolis and Gibbs. Otherwise we get stuck in a\\nnarrow, highly curving region of parameter space.\\n9.3. Hamiltonian Monte Carlo\\nIt appears to be a quite general principle that, whenever there is a random-\\nized way of doing something, then there is a nonrandomized way that deliv-\\ners better performance but requires more thought. —E. T. Jaynes\\nThe Metropolis algorithm and Gibbs sampling are highly random procedures. They try\\nout new parameter values—proposals—and see how good they are, compared to the current\\nvalues. Gibbs sampling gains efficiency by reducing the randomness of proposals by exploit-\\ning knowledge of the target distribution. This seems to fit Jaynes’ suggestion, quoted above,\\nthat when there is a random way of accomplishing some calculation, there is probably a less\\n'},\n",
       " {'index': 289,\n",
       "  'number': 271,\n",
       "  'content': '9.3. HAMILTONIAN MONTE CARLO\\n271\\nrandom way that is better.149 This less random way may require a lot more thought. The\\nGibbs strategy has limitations, but it gets its improvement over plain Metropolis by being\\nless random, not more.\\nHamiltonian Monte Carlo (or Hybrid Monte Carlo, HMC) pushes Jaynes’ principle\\nmuch further. HMC is more computationally costly than Metropolis or Gibbs sampling. But\\nits proposals are also much more efficient. As a result, HMC doesn’t need as many samples to\\ndescribe the posterior distribution. You need less computer time in total, even though each\\nsample needs more. And as models become more complex—thousands or tens of thousands\\nof parameters—HMC can really outshine other algorithms, because the other algorithms just\\nwon’t work. The Earth would be swallowed by the Sun before your chain produces a reliable\\napproximation of the posterior.\\nWe’re going to be using HMC on and off for the remainder of this book. You won’t\\nhave to implement it yourself. But understanding some of the concept behind it will help\\nyou grasp how it outperforms Metropolis and Gibbs sampling and also how it encounters its\\nown, unique problems.\\n9.3.1. Another parable. Suppose King Markov’s cousin Monty is King on the mainland.\\nMonty’s kingdom is not a discrete set of islands. Instead, it is a continuous territory stretched\\nout along a narrow valley, running north-south. But the King has a similar obligation: to\\nvisit his citizens in proportion to their local population density. Within the valley, people\\ndistribute themselves inversely proportional to elevation—most people live in the middle of\\nthe valley, fewer up the mountainside. How can King Monty fulfill his royal obligation?\\nLike Markov, Monty doesn’t wish to bother with schedules and calculations. Also like\\nMarkov, Monty has a highly educated and mathematically gifted advisor, named Hamilton.\\nHamilton designed an odd, but highly efficient, method. And this method solves one of\\nMetropolis’ flaws—the king hardly ever stays in the same place, but keeps moving on to visit\\nnew locations.\\nHere’s how it works. The king’s vehicle picks a random direction, either north or south,\\nand drives off at a random momentum. As the vehicle goes uphill, it slows down and turns\\naround when its declining momentum forces it to. Then it picks up speed again on the way\\ndown. After a fixed period of time, they stop the vehicle, get out, and start shaking hands\\nand kissing babies. Then they get back in the vehicle and begin again. Amazingly, Hamilton\\ncan prove mathematically that this procedure guarantees that, in the long run, the locations\\nvisited will be inversely proportional to their relative elevations, which are also inversely pro-\\nportional to the population densities. Not only does this keep the king moving, but it also\\nspaces the locations apart better—unlike the other king, Monty does not only visit neighbor-\\ning locations.\\nThis mad plan is illustrated, and simulated, in Figure 9.5. The horizontal axis is time.\\nThe vertical axis is location. The king’s journey starts on the far left, in the middle of the\\nvalley. The vehicle begins by heading south. The width of the curve indicates the momentum\\nat each time. The vehicle climbs uphill but slows and briefly turns around before stopping\\nat the first location. Then again and again new locations are chosen in the same way, but\\nwith different random directions and momentums, departing from the most recent location.\\nWhen the initial momentum is small, the vehicle starts to turn around earlier. But when the\\ninitial momentum is large, like in the big swing around time 300, the king can traverse the\\nentire valley before stopping.\\n'},\n",
       " {'index': 290,\n",
       "  'number': 272,\n",
       "  'content': '272\\n9. MARKOV CHAIN MONTE CARLO\\n0\\n100\\n200\\n300\\ntime\\nposition\\n0\\nsouth\\nnorth\\nFigure 9.5. King Monty’s Royal Drive. The journey begins at time 1 on\\nthe far left. The vehicle is given a random momentum and a random direc-\\ntion, either north (top) or south (bottom). The thickness of the path shows\\nmomentum at each moment. The vehicle travels, losing momentum uphill\\nor gaining it downhill. After a fixed amount of time, they stop and make\\na visit, as shown by the points. Then a new random direction and momen-\\ntum is chosen. In the long run, positions are visited in proportion to their\\npopulation density.\\nThe autocorrelation between locations visited is very low under this strategy. This\\nmeans that adjacent locations have a very low, almost zero correlation. The king can move\\nfrom one end of the valley to another. This stands in contrast to the highly autocorrelated\\nmovement under the Metropolis plan (Figure 9.2). King Markov of the Islands might wish\\nto adopt this Hamiltonian strategy, but he cannot: The islands are not continuous. Hamilton’s\\napproach only works when all the locations are connected by dry land, because it requires\\nthat the vehicle be capable of stopping at any point.\\nRethinking: Hamiltonians. The Hamilton who gives his name to Hamiltonian Monte Carlo had\\nnothing to do with the development of the method. Sir William Rowan Hamilton (1805–1865) was an\\nIrish mathematician, arguably the greatest mathematician of his generation. Hamilton accomplished\\ngreat things in pure mathematics, but he also dabbled in physics and reformulated Newton’s laws\\nof motion into a new system that we now call Hamiltonian mechanics (or dynamics). Hamiltonian\\nMonte Carlo was originally called Hybrid Monte Carlo, but is now usually referred to by the\\nHamiltonian differential equations that drive it.\\n9.3.2. Particles in space. This story of King Monty is analogous to how the actual Hamil-\\ntonian Monte Carlo algorithm works. In statistical applications, the royal vehicle is the cur-\\nrent vector of parameter values. Let’s consider the single parameter case, just to keep things\\nsimple. In that case, the log-posterior is like a bowl, with the point of highest posterior prob-\\nability at its nadir, in the center of the valley. Then we give the particle a random flick—give\\n'},\n",
       " {'index': 291,\n",
       "  'number': 273,\n",
       "  'content': '9.3. HAMILTONIAN MONTE CARLO\\n273\\nit some momentum—and simulate its path. It must obey the physics, gliding along until we\\nstop the clock and take a sample.\\nThis is not another metaphor. HMC really does run a physics simulation, pretending\\nthe vector of parameters gives the position of a little frictionless particle. The log-posterior\\nprovides a surface for this particle to glide across. When the log-posterior is very flat, because\\nthere isn’t much information in the likelihood and the priors are rather flat, then the particle\\ncan glide for a long time before the slope (gradient) makes it turn around. When instead the\\nlog-posterior is very steep, because either the likelihood or the priors are very concentrated,\\nthen the particle doesn’t get far before turning around.\\nIn principle, HMC will always accept every proposal, because it only makes intelligent\\nproposals. In practice, HMC uses a rejection criterion, because it is only approximating\\nthe smooth path of a particle. It isn’t unusual to see acceptance rates over 95% with HMC.\\nMaking smart proposals pays. What is the rejection criterion? Because HMC runs a physics\\nsimulation, certain things have to be conserved, like total energy of the system. When the\\ntotal energy changes during the simulation, that means the numerical approximation is bad.\\nWhen the approximation isn’t good, it might reject the proposal.\\nAll of this sounds, and is, complex. But what is gained from all of this complexity is very\\nefficient sampling of complex models. In cases where ordinary Metropolis or Gibbs sam-\\npling wander slowly through parameter space, Hamiltonian Monte Carlo remains efficient.\\nThis is especially true when working with multilevel models with hundreds or thousands of\\nparameters. A particle in 1000-dimension space sounds crazy, but it’s not harder for your\\ncompute to imagine than a particle in 3-dimensions.\\nTo take some of the magic out of this, let’s do a two-dimensional simulation, for a simple\\nposterior distribution with two parameters, the mean and standard deviation of a Gaussian.\\nI’m going to show just the most minimal mathematical details. You don’t need to grasp all\\nthe mathematics to make use of HMC. But having some intuition about how it works will\\nhelp you appreciate why it works so much better than other approaches, as well as why it\\nsometimes doesn’t work. If you want much more mathematical detail, follow the endnote.150\\nSuppose the data are 100 x and 100 y values, all sampled from Normal(0, 1). We’ll use\\nthis statistical model:\\nxi ∼Normal(µx, 1)\\nyi ∼Normal(µy, 1)\\nµx ∼Normal(0, 0.5)\\nµy ∼Normal(0, 0.5)\\nWhat HMC needs to drive are two functions and two settings. The first function computes\\nthe log-probability of the data and parameters. This is just the top part of Bayes’ formula,\\nand every MCMC strategy requires this. It tells the algorithm the “elevation” of any set of\\nparameter values. For the model above, it is just:\\nX\\ni\\nlog p(yi|µy, 1) +\\nX\\ni\\nlog p(xi|µx, 1) + log p(µy|0, 0.5) + log p(µx, 0, 0.5)\\nwhere p(x|a, b) here means the Gaussian density of x at mean a and standard deviation b. The\\nsecond thing HMC needs is the gradient, which just means the slope in all directions at\\nthe current position. In this case, that means just two derivatives. If you take the expression\\nabove and differentiate it with respect to µx and then µy, you have what you need. I’ve placed\\n'},\n",
       " {'index': 292,\n",
       "  'number': 274,\n",
       "  'content': '274\\n9. MARKOV CHAIN MONTE CARLO\\nthese derivatives, in code form, in the Overthinking box further down, where you’ll find a\\ncomplete R implementation of this example.\\nThe two settings that HMC needs are a choice of number of leapfrog steps and a\\nchoice of step size for each. This part is strange. And usually your machine will pick these\\nvalues for you. But having some idea of them will be useful for understanding some of the\\nnewer features of HMC algorithms. Each path in the simulation—each curve for example\\nbetween visits in Figure 9.5—is divided up into a number of leapfrog steps. If you choose\\nmany steps, the paths will be long. If you choose few, they will be short. The size of each step\\nis determined by, you guessed it, the step size. The step size determines how fine grained the\\nsimulation is. If the step size is small, then the particle can turn sharply. If the step size is\\nlarge, then each leap will be large and could even overshoot the point where the simulation\\nwould want to turn around.\\nLet’s put it all together in Figure 9.6. The code to reproduce this figure is in the Over-\\nthinking box below. The left simulation uses L = 11 leapfrog steps, each with a step size of\\nϵ = 0.03. The contours show the log-posterior. It’s a symmetric bowl in this example. Only\\n4 samples from the posterior distribution are shown. The chain begins at the ×. The first\\nsimulation gets flicked to the right and rolls downhill and then uphill again, stopping on the\\nother side and taking a sample at the point labeled 1. The width of the path shows the total\\nmomentum, the kinetic energy, at each point.151 Each leapfrog step is indicated by the white\\ndots along the path. The process repeats, with random direction and momentum in both di-\\nmensions each time. You could take 100 samples here and get an excellent approximation\\nwith very low autocorrelation.\\nHowever, that low autocorrelation is not automatic. The right-hand plot in Figure 9.6\\nshows the same code but with L = 28 leapfrog steps. Now because of the combination of\\nleapfrog steps and step size, the paths tend to land close to where they started. Instead of in-\\ndependent samples from the posterior, we get correlated samples, like in a Metropolis chain.\\nThis problem is called the U-turn problem—the simulations turn around and return to the\\nsame neighborhood. The U-turn problem looks especially bad in this example, because the\\nposterior is a perfect 2-dimensional Gaussian bowl. So the parabolic paths always loop back\\nonto themselves. In most models, this won’t be the case. But you’ll still get paths return-\\ning close to where they started. This just shows that the efficiency of HMC comes with the\\nexpense of having to tune the leapfrog steps and step size in each application.\\nFancy HMC samplers, like Stan and its rstan package, have two ways to deal with U-\\nturns. First, they will choose the leapfrog steps and step size for you. They can do this by\\nconducting a warmup phase in which they try to figure out which step size explores the\\nposterior efficiently. If you are familiar with older algorithms like Gibbs sampling, which use\\na burn-in phase, warmup is not like burn-in. Technically, burn-in samples are just samples.\\nThey are part of the posterior. But Stan’s warmup phase, for example, does not produce\\nuseful samples. It is just tuning the simulation. The warmup phase tends to be slower than\\nthe sampling phase. So when you start using Stan, and warmup seems to be slow, in most\\ncases it will speed up a lot as time goes on.\\nThe second thing fancy HMC samplers do is use a clever algorithm to adaptively set the\\nnumber of leapfrog steps. The type of algorithm is a no-U-turn sampler, or NUTS. A\\nno-U-turn sampler uses the shape of the posterior to infer when the path is turning around.\\nThen it stops the simulation. The details are both complicated and amazing.152 Stan currently\\n(since version 2.0) uses a second-generation NUTS2 sampler. See the Stan manual for more.\\n'},\n",
       " {'index': 293,\n",
       "  'number': 275,\n",
       "  'content': '9.3. HAMILTONIAN MONTE CARLO\\n275\\n-0.3\\n-0.2\\n-0.1\\n0.0\\n0.1\\n0.2\\n0.3\\n-0.3\\n-0.2\\n-0.1\\n0.0\\n0.1\\n0.2\\n0.3\\nmux\\nmuy\\n1\\n2\\n3\\n4\\n2D Gaussian, L = 11\\n-0.3\\n-0.2\\n-0.1\\n0.0\\n0.1\\n0.2\\n0.3\\n-0.3\\n-0.2\\n-0.1\\n0.0\\n0.1\\n0.2\\n0.3\\nmux\\nmuy\\n2D Gaussian, L = 28\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\na1\\na2\\n1\\n2\\n3\\n4\\nPosterior correlation -0.9\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\na1\\na2\\n50 trajectories\\nFigure 9.6. Hamiltonian Monte Carlo trajectories follow physical paths de-\\ntermined by the curvature of the posterior distribution. Top-left: With the\\nright combination of leapfrog steps and step size, the individual paths pro-\\nduce independent samples from the posterior. The simulation begins at the\\n× and then moves in order to the points labeled 1 though 4. Top-right:\\nWith the wrong combination, sequential samples can end up very close to\\none another. The chain in the top-right will still work. It’ll just be much\\nless efficient. Bottom-left: HMC really shines when the posterior contains\\nhigh correlations, as here. Bottom-right: 50 samples from the same high\\ncorrelation posterior, showing only one rejected sample (the open point).\\nA low rate of rejected proposals and lower autocorrelation between samples\\nmeans fewer samples are needed to approximate the posterior.\\n'},\n",
       " {'index': 294,\n",
       "  'number': 276,\n",
       "  'content': '276\\n9. MARKOV CHAIN MONTE CARLO\\nOverthinking: Hamiltonian Monte Carlo in the raw. The HMC algorithm needs five things to go:\\n(1) a function U that returns the negative log-probability of the data at the current position (parameter\\nvalues), (2) a function grad_U that returns the gradient of the negative log-probability at the current\\nposition, (3) a step size epsilon, (4) a count of leapfrog steps L, and (5) a starting position current_q.\\nKeep in mind that the position is a vector of parameter values and that the gradient also needs to\\nreturn a vector of the same length. So that these U and grad_U functions make more sense, let’s\\npresent them first, built custom for the 2D Gaussian example. The U function just expresses the log-\\nposterior, as stated before in the main text:\\nX\\ni\\nlog p(yi|µy, 1) +\\nX\\ni\\nlog p(xi|µx, 1) + log p(µy|0, 0.5) + log p(µx, 0, 0.5)\\nSo it’s just four calls to dnorm really:\\nR code\\n9.5\\n# U needs to return neg-log-probability\\nU <- function( q , a=0 , b=1 , k=0 , d=1 ) {\\nmuy <- q[1]\\nmux <- q[2]\\nU <- sum( dnorm(y,muy,1,log=TRUE) ) + sum( dnorm(x,mux,1,log=TRUE) ) +\\ndnorm(muy,a,b,log=TRUE) + dnorm(mux,k,d,log=TRUE)\\nreturn( -U )\\n}\\nNow the gradient function requires two partial derivatives. Luckily, Gaussian derivatives are very\\nclean. The derivative of the logarithm of any univariate Gaussian with mean a and standard deviation\\nb with respect to a is:\\n∂log N(y|a, b)\\n∂a\\n= y −a\\nb2\\nAnd since the derivative of a sum is a sum of derivatives, this is all we need to write the gradients:\\n∂U\\n∂µx\\n= ∂log N(x|µx, 1)\\n∂µx\\n+ ∂log N(µx|0, 0.5)\\n∂µx\\n=\\nX\\ni\\nxi −µx\\n12\\n+ 0 −µx\\n0.52\\nAnd the gradient for µy has the same form. Now in code form:\\nR code\\n9.6\\n# gradient function\\n# need vector of partial derivatives of U with respect to vector q\\nU_gradient <- function( q , a=0 , b=1 , k=0 , d=1 ) {\\nmuy <- q[1]\\nmux <- q[2]\\nG1 <- sum( y - muy ) + (a - muy)/b^2 #dU/dmuy\\nG2 <- sum( x - mux ) + (k - mux)/d^2 #dU/dmux\\nreturn( c( -G1 , -G2 ) ) # negative bc energy is neg-log-prob\\n}\\n# test data\\nset.seed(7)\\ny <- rnorm(50)\\nx <- rnorm(50)\\nx <- as.numeric(scale(x))\\ny <- as.numeric(scale(y))\\nThe gradient function above isn’t too bad for this model. But it can be terrifying for a reasonably\\ncomplex model. That is why tools like Stan build the gradients dynamically, using the model defini-\\ntion. Now we are ready to visit the heart. To understand some of the details here, you should read\\nRadford Neal’s chapter in the Handbook of Markov Chain Monte Carlo. Armed with the log-posterior\\nand gradient functions, here’s the code to produce Figure 9.6:\\n'},\n",
       " {'index': 295,\n",
       "  'number': 277,\n",
       "  'content': '9.3. HAMILTONIAN MONTE CARLO\\n277\\nR code\\n9.7\\nlibrary(shape) # for fancy arrows\\nQ <- list()\\nQ$q <- c(-0.1,0.2)\\npr <- 0.3\\nplot( NULL , ylab=\"muy\" , xlab=\"mux\" , xlim=c(-pr,pr) , ylim=c(-pr,pr) )\\nstep <- 0.03\\nL <- 11 # 0.03/28 for U-turns --- 11 for working example\\nn_samples <- 4\\npath_col <- col.alpha(\"black\",0.5)\\npoints( Q$q[1] , Q$q[2] , pch=4 , col=\"black\" )\\nfor ( i in 1:n_samples ) {\\nQ <- HMC2( U , U_gradient , step , L , Q$q )\\nif ( n_samples < 10 ) {\\nfor ( j in 1:L ) {\\nK0 <- sum(Q$ptraj[j,]^2)/2 # kinetic energy\\nlines( Q$traj[j:(j+1),1] , Q$traj[j:(j+1),2] , col=path_col , lwd=1+2*K0 )\\n}\\npoints( Q$traj[1:L+1,] , pch=16 , col=\"white\" , cex=0.35 )\\nArrows( Q$traj[L,1] , Q$traj[L,2] , Q$traj[L+1,1] , Q$traj[L+1,2] ,\\narr.length=0.35 , arr.adj = 0.7 )\\ntext( Q$traj[L+1,1] , Q$traj[L+1,2] , i , cex=0.8 , pos=4 , offset=0.4 )\\n}\\npoints( Q$traj[L+1,1] , Q$traj[L+1,2] , pch=ifelse( Q$accept==1 , 16 , 1 ) ,\\ncol=ifelse( abs(Q$dH)>0.1 , \"red\" , \"black\" ) )\\n}\\nThe function HMC2 is built into rethinking. It is based upon one of Radford Neal’s example scripts.153\\nIt isn’t actually too complicated. Let’s tour through it, one step at a time, to take the magic away. This\\nfunction runs a single trajectory, and so produces a single sample. You need to use it repeatedly to\\nbuild a chain. That’s what the loop above does. The first chunk of the function chooses random\\nmomentum—the flick of the particle—and initializes the trajectory.\\nR code\\n9.8\\nHMC2 <- function (U, grad_U, epsilon, L, current_q) {\\nq = current_q\\np = rnorm(length(q),0,1) # random flick - p is momentum.\\ncurrent_p = p\\n# Make a half step for momentum at the beginning\\np = p - epsilon * grad_U(q) / 2\\n# initialize bookkeeping - saves trajectory\\nqtraj <- matrix(NA,nrow=L+1,ncol=length(q))\\nptraj <- qtraj\\nqtraj[1,] <- current_q\\nptraj[1,] <- p\\nThen the action comes in a loop over leapfrog steps. L steps are taken, using the gradient to compute\\na linear approximation of the log-posterior surface at each point.\\nR code\\n9.9\\n# Alternate full steps for position and momentum\\nfor ( i in 1:L ) {\\nq = q + epsilon * p # Full step for the position\\n# Make a full step for the momentum, except at end of trajectory\\nif ( i!=L ) {\\np = p - epsilon * grad_U(q)\\nptraj[i+1,] <- p\\n}\\n'},\n",
       " {'index': 296,\n",
       "  'number': 278,\n",
       "  'content': '278\\n9. MARKOV CHAIN MONTE CARLO\\nqtraj[i+1,] <- q\\n}\\nNotice how the step size epsilon is added to the position and momentum vectors. It is in this way\\nthat the path is only an approximation, because it is a series of linear jumps, not an actual smooth\\ncurve. This can have important consequences, if the log-posterior bends sharply and the simulation\\njumps over a bend. All that remains is clean up: ensure the proposal is symmetric so the Markov\\nchain is valid and decide whether to accept or reject the proposal.\\nR code\\n9.10\\n# Make a half step for momentum at the end\\np = p - epsilon * grad_U(q) / 2\\nptraj[L+1,] <- p\\n# Negate momentum at end of trajectory to make the proposal symmetric\\np = -p\\n# Evaluate potential and kinetic energies at start and end of trajectory\\ncurrent_U = U(current_q)\\ncurrent_K = sum(current_p^2) / 2\\nproposed_U = U(q)\\nproposed_K = sum(p^2) / 2\\n# Accept or reject the state at end of trajectory, returning either\\n# the position at the end of the trajectory or the initial position\\naccept <- 0\\nif (runif(1) < exp(current_U-proposed_U+current_K-proposed_K)) {\\nnew_q <- q\\n# accept\\naccept <- 1\\n} else new_q <- current_q\\n# reject\\nreturn(list( q=new_q, traj=qtraj, ptraj=ptraj, accept=accept ))\\n}\\nThe accept/reject decision at the bottom uses the fact that in Hamiltonian dynamics, the total energy\\nof the system must be constant. So if the energy at the start of the trajectory differs substantially from\\nthe energy at the end, something has gone wrong. This is known as a divergent transition, and\\nwe’ll talk more about these in a later chapter.\\n9.3.3. Limitations. As always, there are some limitations. HMC requires continuous pa-\\nrameters. It can’t glide through a discrete parameter. In practice, this means that certain\\ntechniques, like the imputation of discrete missing data, have to be done differently with\\nHMC. HMC can certainly sample from such models, often much more efficiently than a\\nGibbs sampler could. But you have to change how you code them. There will be examples\\nin Chapter 15 and Chapter 16.\\nIt is also important to keep in mind that HMC is not magic. Some posterior distributions\\nare just very difficult to sample from, for any algorithm. We’ll see examples in later chapters.\\nIn these cases, HMC will encounter something called a divergent transition. We’ll talk\\na lot about these, what causes them, and how to fix them, later on.\\nRethinking: The MCMC horizon. While the ideas behind Markov chain Monte Carlo are not new,\\nwidespread use dates only to the last decade of the twentieth century.154 New variants of and improve-\\nments to MCMC algorithms arise all the time. We might anticipate that interesting advances are com-\\ning, and that the current crop of tools—Gibbs sampling and first-generation HMC for example—will\\nlook rather pedestrian in another 20 years. At least we can hope.\\n'},\n",
       " {'index': 297,\n",
       "  'number': 279,\n",
       "  'content': '9.4. EASY HMC: ULAM\\n279\\n9.4. Easy HMC: ulam\\nThe rethinking package provides a convenient interface, ulam, to compile lists of for-\\nmulas, like the lists you’ve been using so far to construct quap estimates, into Stan HMC\\ncode. A little more housekeeping is needed to use ulam: You should preprocess any vari-\\nable transformations, and you should construct a clean data list with only the variables you\\nwill use. But otherwise installing Stan on your computer is the hardest part. And once you\\nget comfortable with interpreting samples produced in this way, you go peek inside and see\\nexactly how the model formulas you already understand correspond to the code that drives\\nthe Markov chain. When you use ulam, you can also use the same helper functions as quap:\\nextract.samples, extract.prior, link, sim, and others.\\nThere are other R packages that make using Stan even easier, because they don’t require\\nthe full formulas that quap and ulam do. At the time of printing, the best are brms and\\nrstanarm for multilevel models and blavaan for structural equation models. For learning\\nabout Bayesian modeling, I recommend you stick with the full and explicit formulas of ulam\\nfor now. The reason is that an interface that hides the model structure makes it hard to learn\\nthe model structure. But there is nothing wrong with moving on to simplified interfaces\\nlater, once you gain experience.\\nTo see how ulam works, let’s revisit the terrain ruggedness example from Chapter 8. This\\ncode will load the data and reduce it down to cases (nations) that have the outcome variable\\nof interest:\\nR code\\n9.11\\nlibrary(rethinking)\\ndata(rugged)\\nd <- rugged\\nd$log_gdp <- log(d$rgdppc_2000)\\ndd <- d[ complete.cases(d$rgdppc_2000) , ]\\ndd$log_gdp_std <- dd$log_gdp / mean(dd$log_gdp)\\ndd$rugged_std <- dd$rugged / max(dd$rugged)\\ndd$cid <- ifelse( dd$cont_africa==1 , 1 , 2 )\\nSo you remember the old way, we’re going to repeat the procedure for fitting the interaction\\nmodel. This model aims to predict log GDP with terrain ruggedness, continent, and the\\ninteraction of the two. Here’s the way to do it with quap, just like before.\\nR code\\n9.12\\nm8.3 <- quap(\\nalist(\\nlog_gdp_std ~ dnorm( mu , sigma ) ,\\nmu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,\\na[cid] ~ dnorm( 1 , 0.1 ) ,\\nb[cid] ~ dnorm( 0 , 0.3 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dd )\\nprecis( m8.3 , depth=2 )\\nmean\\nsd\\n5.5% 94.5%\\na[1]\\n0.89 0.02\\n0.86\\n0.91\\na[2]\\n1.05 0.01\\n1.03\\n1.07\\nb[1]\\n0.13 0.07\\n0.01\\n0.25\\n'},\n",
       " {'index': 298,\n",
       "  'number': 280,\n",
       "  'content': '280\\n9. MARKOV CHAIN MONTE CARLO\\nb[2]\\n-0.14 0.05 -0.23 -0.06\\nsigma\\n0.11 0.01\\n0.10\\n0.12\\nJust as you saw in the previous chapter.\\n9.4.1. Preparation. But now we’ll also fit this model using Hamiltonian Monte Carlo. This\\nmeans there will be no more quadratic approximation—if the posterior distribution is non-\\nGaussian, then we’ll get whatever non-Gaussian shape it has. You can use exactly the same\\nformula list as before, but you should do two additional things.\\n(1) Preprocess all variable transformations. If the outcome is transformed somehow,\\nlike by taking the logarithm, then do this before fitting the model by constructing a\\nnew variable in the data frame. Likewise, if any predictor variables are transformed,\\nincluding squaring and cubing and such to build polynomial models, then compute\\nthese transformed values before fitting the model. It’s a waste of computing power\\nto do these transformations repeatedly in every step of the Markov chain.\\n(2) Once you’ve got all the variables ready, make a new trimmed down data frame that\\ncontains only the variables you will actually use to fit the model. Technically, you\\ndon’t have to do this. But doing so avoids common problems. For example, if any\\nof the unused variables have missing values, NA, then Stan will refuse to work.\\nWe’ve already pre-transformed all the variables. Now we need a slim list of the variables we\\nwill use:\\nR code\\n9.13\\ndat_slim <- list(\\nlog_gdp_std = dd$log_gdp_std,\\nrugged_std = dd$rugged_std,\\ncid = as.integer( dd$cid )\\n)\\nstr(dat_slim)\\nList of 3\\n$ log_gdp_std: num [1:170] 0.88 0.965 1.166 1.104 0.915 ...\\n$ rugged_std : num [1:170] 0.138 0.553 0.124 0.125 0.433 ...\\n$ cid\\n: int [1:170] 1 2 2 2 2 2 2 2 2 1 ...\\nIt is better to use a list than a data.frame, because the elements in a list can be any\\nlength. In a data.frame, all the elements must be the same length. With some models to\\ncome later, like multilevel models, it isn’t unusual to have variables of different lengths.\\n9.4.2. Sampling from the posterior. Now provided you have the rstan package installed\\n(mc-stan.org), you can get samples from the posterior distribution with this code:\\nR code\\n9.14\\nm9.1 <- ulam(\\nalist(\\nlog_gdp_std ~ dnorm( mu , sigma ) ,\\nmu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,\\na[cid] ~ dnorm( 1 , 0.1 ) ,\\nb[cid] ~ dnorm( 0 , 0.3 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dat_slim , chains=1 )\\n'},\n",
       " {'index': 299,\n",
       "  'number': 281,\n",
       "  'content': '9.4. EASY HMC: ULAM\\n281\\nAll that ulam does is translate the formula above into a Stan model, and then Stan defines\\nthe sampler and does the hard part. Stan models look very similar, but require some more\\nexplicit definitions. This also makes them much more flexible. If you’d rather start working\\ndirectly with Stan code, I’ll present this same model in raw Stan a bit later. You can always\\nextract the Stan code with stancode(m9.1).\\nAfter messages about compiling, and sampling, ulam returns an object that contains a\\nbunch of summary information, as well as samples from the posterior distribution. You can\\nsummarize just like a quap model:\\nR code\\n9.15\\nprecis( m9.1 , depth=2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat4\\na[1]\\n0.89 0.02\\n0.86\\n0.91\\n739\\n1\\na[2]\\n1.05 0.01\\n1.03\\n1.07\\n714\\n1\\nb[1]\\n0.13 0.08\\n0.01\\n0.26\\n793\\n1\\nb[2]\\n-0.14 0.05 -0.23 -0.06\\n799\\n1\\nsigma\\n0.11 0.01\\n0.10\\n0.12\\n785\\n1\\nThese estimates are very similar to the quadratic approximation. But note that there are two\\nnew columns, n_eff and Rhat4. These columns provide MCMC diagnostic criteria, to help\\nyou tell how well the sampling worked. We’ll discuss them in detail later in the chapter. For\\nnow, it’s enough to know that n_eff is a crude estimate of the number of independent sam-\\nples you managed to get. Rhat (ˆR) is an indicator of the convergence of the Markov chains to\\nthe target distribution. It should approach 1.00 from above, when all is well. There are sev-\\neral different ways to compute it. The “4” on the end indicates the fourth generation version\\nof Rhat, not the original 1992 version that you usually see cited in papers. In the future, this\\nwill increase to Rhat5, the 5th generation. See the details and citations in ?precis.\\n9.4.3. Sampling again, in parallel. The example so far is a very easy problem for MCMC. So\\neven the default 1000 samples is enough for accurate inference. In fact, as few as 200 effective\\nsamples is usually plenty for a good approximation of the posterior. But we also want to run\\nmultiple chains, for reasons we’ll discuss in more depth in the next sections. There will be\\nspecific advice in Section 9.5 (page 287).\\nFor now, it’s worth noting that you can easily parallelize those chains, as well. They\\ncan all run at the same time, instead of in sequence. So as long as your computer has four\\ncores (it probably does), it won’t take longer to run four chains than one chain. To run four\\nindependent Markov chains for the model above, and to distribute them across separate cores\\nin your computer, just increase the number of chains and add a cores argument:\\nR code\\n9.16\\nm9.1 <- ulam(\\nalist(\\nlog_gdp_std ~ dnorm( mu , sigma ) ,\\nmu <- a[cid] + b[cid]*( rugged_std - 0.215 ) ,\\na[cid] ~ dnorm( 1 , 0.1 ) ,\\nb[cid] ~ dnorm( 0 , 0.3 ) ,\\nsigma ~ dexp( 1 )\\n) , data=dat_slim , chains=4 , cores=4 )\\n'},\n",
       " {'index': 300,\n",
       "  'number': 282,\n",
       "  'content': '282\\n9. MARKOV CHAIN MONTE CARLO\\nThere are a bunch of optional arguments that allow us to tune and customize the process.\\nWe’ll bring them up as they are needed. For now, keep in mind that show will remind you of\\nthe model formula and also how long each chain took to run:\\nR code\\n9.17\\nshow( m9.1 )\\nHamiltonian Monte Carlo approximation\\n2000 samples from 4 chains\\nSampling durations (seconds):\\nwarmup sample total\\nchain:1\\n0.06\\n0.03\\n0.09\\nchain:2\\n0.05\\n0.03\\n0.09\\nchain:3\\n0.05\\n0.03\\n0.08\\nchain:4\\n0.05\\n0.04\\n0.10\\nFormula:\\nlog_gdp_std ~ dnorm(mu, sigma)\\nmu <- a[cid] + b[cid] * (rugged_std - 0.215)\\na[cid] ~ dnorm(1, 0.1)\\nb[cid] ~ dnorm(0, 0.3)\\nsigma ~ dexp(1)\\nThere were 2000 samples from all 4 chains, because each 1000 sample chain uses by default\\nthe first half of the samples to adapt. Something curious happens when we look at the sum-\\nmary:\\nR code\\n9.18\\nprecis( m9.1 , 2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat4\\na[1]\\n0.89 0.02\\n0.86\\n0.91\\n2490\\n1\\na[2]\\n1.05 0.01\\n1.03\\n1.07\\n3020\\n1\\nb[1]\\n0.13 0.08\\n0.01\\n0.25\\n2729\\n1\\nb[2]\\n-0.14 0.06 -0.23 -0.06\\n2867\\n1\\nsigma\\n0.11 0.01\\n0.10\\n0.12\\n2368\\n1\\nIf there were only 2000 samples in total, how can we have more than 2000 effective samples\\nfor each parameter? It’s no mistake. The adaptive sampler that Stan uses is so good, it can ac-\\ntually produce sequential samples that are better than uncorrelated. They are anti-correlated.\\nThis means it can explore the posterior distribution so efficiently that it can beat random. It’s\\nJaynes’ principle (page 270) in action.\\n9.4.4. Visualization. By plotting the samples, you can get a direct appreciation for how\\nGaussian (quadratic) the actual posterior density has turned out to be. Use pairs directly on\\nthe model object, so that R knows to display parameter names and parameter correlations:\\nR code\\n9.19\\npairs( m9.1 )\\nFigure 9.7 shows the resulting plot. This is a pairs plot, so it’s still a matrix of bivariate scatter\\nplots. But now along the diagonal the smoothed histogram of each parameter is shown, along\\n'},\n",
       " {'index': 301,\n",
       "  'number': 283,\n",
       "  'content': '9.4. EASY HMC: ULAM\\n283\\nFigure 9.7. Pairs plot of the samples produced by ulam. The diagonal\\nshows a density estimate for each parameter. Below the diagonal, corre-\\nlations between parameters are shown.\\nwith its name. And in the lower triangle of the matrix, the correlation between each pair of\\nparameters is shown, with stronger correlations indicated by relative size.\\nFor this model and these data, the resulting posterior distribution is quite nearly multi-\\nvariate Gaussian. The density for sigma is certainly skewed in the expected direction. But\\notherwise the quadratic approximation does almost as well as Hamiltonian Monte Carlo.\\nThis is a very simple kind of model structure of course, with Gaussian priors, so an approxi-\\nmately quadratic posterior should be no surprise. Later, we’ll see some more exotic posterior\\ndistributions.\\n9.4.5. Checking the chain. Provided the Markov chain is defined correctly, then it is guar-\\nanteed to converge in the long run to the answer we want, the posterior distribution. But\\nsome posterior distributions are hard to explore—there will be examples—and the time it\\nwould take for them to provide an unbiased approximation is very long indeed. Such prob-\\nlems are rarer for HMC than other algorithms, but they still exist. In fact, one of the virtues\\n'},\n",
       " {'index': 302,\n",
       "  'number': 284,\n",
       "  'content': '284\\n9. MARKOV CHAIN MONTE CARLO\\nof HMC is that it tells us when things are going wrong. Other algorithms, like Metropolis-\\nHastings, can remain silent about major problems. In the next major section, we’ll dwell on\\ncauses of and solutions to malfunction.\\nFor now, let’s look at two chain visualizations that can often, but not always, spot prob-\\nlems. The first is called a trace plot. A trace plot merely plots the samples in sequential\\norder, joined by a line. It’s King Markov’s path through the islands, in the metaphor at the\\nstart of the chapter. Looking at the trace plot of each parameter is often the best thing for diag-\\nnosing common problems. And once you come to recognize a healthy, functioning Markov\\nchain, quick checks of trace plots provide a lot of peace of mind. A trace plot isn’t the last\\nthing analysts do to inspect MCMC output. But it’s often the first.\\nIn the terrain ruggedness example, the trace plot shows a very healthy chain.\\nR code\\n9.20\\ntraceplot( m9.1 )\\nThe result is shown in Figure 9.8 (top). Actually, the figure shows the trace of just the first\\nchain. You can get this by adding chains=1 to the call. You can think of the zig-zagging trace\\nof each parameter as the path the chain took through each dimension of parameter space.\\nThe gray region in each plot, the first 500 samples, marks the adaptation samples. Dur-\\ning adaptation, the Markov chain is learning to more efficiently sample from the posterior\\ndistribution. So these samples are not reliable to use for inference. They are automatically\\ndiscarded by extract.samples, which returns only the samples shown in the white regions\\nof Figure 9.8.\\nNow, how is this chain a healthy one? Typically we look for three things in these trace\\nplots: (1) stationarity, (2) good mixing, and (3) convergence. Stationarity refers to the path\\nof each chain staying within the same high-probability portion of the posterior distribution.\\nNotice that these traces, for example, all stick around a very stable central tendency, the\\ncenter of gravity of each dimension of the posterior. Another way to think of this is that\\nthe mean value of the chain is quite stable from beginning to end. Good mixing means\\nthat the chain rapidly explores the full region. It doesn’t slowly wander, but rather rapidly\\nzig-zags around, as a good Hamiltonian chain should. Convergence means that multiple,\\nindependent chains stick around the same region of high probability.\\nTrace plots are a natural way to view a chain, but they are often hard to read, because once\\nyou start plotting lots of chains over one another, the plot can look very confusing and hide\\npathologies in some chains. A second way to visualize the chains is a plot of the distribution\\nof the ranked samples, a trace rank plot, or trank plot.155 What this means is to take\\nall the samples for each individual parameter and rank them. The lowest sample gets rank 1.\\nThe largest gets the maximum rank (the number of samples across all chains). Then we draw\\na histogram of these ranks for each individual chain. Why do this? Because if the chains\\nare exploring the same space efficiently, the histograms should be similar to one another and\\nlargely overlapping. The rethinking package provides a function to produce these:\\nR code\\n9.21\\ntrankplot( m9.1 )\\nThe result is reproduced in Figure 9.8 (bottom). The axes are not labeled in these plots,\\nto reduce clutter. But the horizontal is rank, from 1 to the number of samples across all\\nchains (2000 in this example). The vertical axis is the frequency of ranks in each bin of the\\n'},\n",
       " {'index': 303,\n",
       "  'number': 285,\n",
       "  'content': '9.4. EASY HMC: ULAM\\n285\\n200\\n400\\n600\\n800\\n1000\\n0.82\\n0.86\\n0.90\\n0.94\\nn_eff = 2671\\na[1]\\n200\\n400\\n600\\n800\\n1000\\n1.02\\n1.04\\n1.06\\n1.08\\nn_eff = 3109\\na[2]\\n200\\n400\\n600\\n800\\n1000\\n-0.1\\n0.1\\n0.3\\nn_eff = 2153\\nb[1]\\n200\\n400\\n600\\n800\\n1000\\n-0.3\\n-0.2\\n-0.1\\n0.0\\n0.1\\nn_eff = 2027\\nb[2]\\nn_eff = 2671\\na[1]\\nn_eff = 3109\\na[2]\\nn_eff = 2153\\nb[1]\\nn_eff = 2027\\nb[2]\\nFigure 9.8. Trace (top) and trank (bottom) plots of the Markov chain from\\nthe ruggedness model, m9.1. sigma not shown. This is a healthy Markov\\nchain, both stationary and well-mixing. Top: Gray region is warmup.\\nhistogram. This trank plot is what we hope for: Histograms that overlap and stay within the\\nsame range.\\nTo really understand the value of these plots, you’ll have to see some trace and trank\\nplots for unhealthy chains. That’s the project of the next section.\\nOverthinking: Raw Stan model code. All ulam does is translate a list of formulas into Stan’s modeling\\nlanguage. Then Stan does the rest. Learning how to write Stan code is not necessary for most of the\\nmodels in this book. But other models do require some direct interaction with Stan, because it is\\ncapable of much more than ulam allows you to express. And even for simple models, you’ll gain\\nadditional comprehension and control, if you peek into the machine. You can always access the raw\\nStan code that ulam produces by using the function stancode. For example, stancode(m9.1) prints\\nout the Stan code for the ruggedness model. Before you’re familiar with Stan’s language, it’ll look long\\n'},\n",
       " {'index': 304,\n",
       "  'number': 286,\n",
       "  'content': '286\\n9. MARKOV CHAIN MONTE CARLO\\nand weird. But let’s take it one piece at a time. It’s actually just stuff you’ve already learned, expressed\\na little differently.\\ndata{\\nvector[170] log_gdp_std;\\nvector[170] rugged_std;\\nint cid[170];\\n}\\nparameters{\\nvector[2] a;\\nvector[2] b;\\nreal<lower=0> sigma;\\n}\\nmodel{\\nvector[170] mu;\\nsigma ~ exponential( 1 );\\nb ~ normal( 0 , 0.3 );\\na ~ normal( 1 , 0.1 );\\nfor ( i in 1:170 ) {\\nmu[i] = a[cid[i]] + b[cid[i]] * (rugged_std[i] - 0.215);\\n}\\nlog_gdp_std ~ normal( mu , sigma );\\n}\\nThis is Stan code, not R code. It is essentially the formula list you provided to ulam, with the implied\\ndefinitions of the variables made explicit. There are three “blocks.”\\nThe first block is the data block, at the top. This is where observed variables are named and\\ntheir types and sizes are declared. int cid[170] just means an integer variable named cid with 170\\nvalues. That’s our continent index. The other two are vectors of real values, continuous ruggedness\\nand log GDP variables. Each line in Stan ends in a semicolon. Don’t ask why. Just do it. You probably\\naren’t using enough semicolons in your life, anyway.\\nThe next block is parameters. These, you can probably guess, are the unobserved variables.\\nThey are described just like the observed ones. The new elements here are the <lower=0> for sigma\\nand those vector[2] things. <lower=0> tells Stan that sigma must be positive. It is constrained.\\nThis constraint corresponds to the exponential prior we assign it, which is only defined on the positive\\nreals. The vector[2] types are lists of real numbers of length 2. These are our 2 intercepts a and our\\n2 slopes b.\\nIf you haven’t used explicit and static typed languages before, these first two blocks must seem\\nweird. Why does Stan force the user to say explicitly what R and ulam figure out automatically? One\\nreason is that the code doesn’t have to do as much checking of conditions, when the types of the\\nvariables are already there and unchanging. So it can be faster. But from our perspective, a major\\nadvantage is that explicit types help us avoid a large class of programming mistakes. The kinds of run-\\ntime shenanigans common to languages like R and Python are impossible in C++. In my experience,\\npeople who have studied compiled languages see static typing as a welcome feature. People who have\\nonly worked in interpreted languages like R see it as a bother. Both groups are correct.\\nFinally, the model block is where the action is. This block computes the log-probability of the\\ndata. It runs from top to bottom, like R code does, adding mathematical terms to the log-probability.\\nSo when Stan sees sigma ~ exponential( 1 ), it doesn’t do any sampling at that moment. Instead,\\nit adds a probability term to the log-probability. This term is just dexp( sigma , 1 ). The same\\ngoes for the other lines with ~ in them. Note that the last line, for log_gdp_std, is vectorized just\\nlike R code. There are 170 outcomes values and 170 corresponding mu values. That last statement\\nprocesses all of them.\\nStan then uses the analytical gradient—derivative—of all these terms to define the physics simula-\\ntion under Hamiltonian Monte Carlo. How does Stan do this? It uses a technique called automatic\\ndifferentiation, or simply “autodiff,” to build an analytical gradient. If you know much about ma-\\nchine learning, you may have also heard about backpropagation. It’s the same thing as autodiff.\\nThis is much more accurate than a gradient approximated numerically. If you know some calculus,\\n'},\n",
       " {'index': 305,\n",
       "  'number': 287,\n",
       "  'content': '9.5. CARE AND FEEDING OF YOUR MARKOV CHAIN\\n287\\nreally all that is going on is ruthless application of the chain rule. But the algorithm is actually quite\\nclever. See the Stan manual for more details.\\nThat’s all there is to a Stan program, in the basic case. I’ll break out into boxes like this in later\\nchapters, to show more of the raw Stan code. Tools like ulam are bridges. They can do a lot of useful\\nwork, but the extra control you get from working directly in Stan is worthwhile. Especially since it\\nwon’t tie you to R or any other specific scripting language.\\n9.5. Care and feeding of your Markov chain\\nMarkov chain Monte Carlo is a highly technical and usually automated procedure. You\\nmight write your own MCMC code, for the sake of learning. But it is very easy to introduce\\nsubtle biases. A package like Stan, in contrast, is continuously tested against expected output.\\nMost people who use Stan don’t really understand what it is doing, under the hood. That’s\\nokay. Science requires division of labor, and if every one of us had to write our own Markov\\nchains from scratch, a lot less research would get done in the aggregate.\\nBut as with many technical and powerful procedures, it’s natural to feel uneasy about\\nMCMC and maybe even a little superstitious. Something magical is happening inside the\\ncomputer, and unless we make the right sacrifices and say the right words, an ancient evil\\nmight awake. So we do need to understand enough to know when the evil stirs. The good\\nnews is that HMC, unlike Gibbs sampling and ordinary Metropolis, makes it easy to tell\\nwhen the magic goes wrong. Its best feature is not how efficient it is. Rather the best feature\\nis that it complains loudly when things aren’t right. Let’s look at some complaints and along\\nthe way establish some guidelines for running chains.\\n9.5.1. How many samples do you need? You can control the number of samples from the\\nchain by using the iter and warmup parameters. The defaults are 1000 for iter and warmup\\nis set to iter/2, which gives you 500 warmup samples and 500 real samples to use for infer-\\nence. But these defaults are just meant to get you started, to make sure the chain gets started\\nokay. Then you can decide on other values for iter and warmup.\\nSo how many samples do we need for accurate inference about the posterior distribu-\\ntion? It depends. First, what really matters is the effective number of samples, not the raw\\nnumber. The effective number of samples is an estimate of the number of independent sam-\\nples from the posterior distribution, in terms of estimating some function like the posterior\\nmean. Markov chains are typically autocorrelated, so that sequential samples are not entirely\\nindependent. This happens when chains explore the posterior slowly, like in a Metropolis\\nalgorithm. Autocorrelation reduces the effective number of samples. Stan provides an esti-\\nmate of effective number of samples, for the purpose of estimating the posterior mean, as\\nn_eff. You can think of n_eff as the length of a Markov chain with no autocorrelation\\nthat would provide the same quality of estimate as your chain. One consequence of this\\ndefinition, as you saw earlier in the chapter, is that n_eff can be larger than the length of\\nyour chain, provided sequential samples are anti-correlated in the right way. While n_eff\\nis only an estimate, it is usually better than the raw number of samples, which can be very\\nmisleading.\\nSecond, what do you want to know? If all you want are posterior means, it doesn’t take\\nmany samples at all to get very good estimates. Even a couple hundred samples will do. But\\nif you care about the exact shape in the extreme tails of the posterior, the 99th percentile or\\nso, then you’ll need many more. So there is no universally useful number of samples to aim\\nfor. In most typical regression applications, you can get a very good estimate of the posterior\\n'},\n",
       " {'index': 306,\n",
       "  'number': 288,\n",
       "  'content': '288\\n9. MARKOV CHAIN MONTE CARLO\\nmean with as few as 200 effective samples. And if the posterior is approximately Gaussian,\\nthen all you need in addition is a good estimate of the variance, which can be had with one\\norder of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think\\nmore about which region of the distribution interests you. Stan will sometimes warn you\\nabout “tail ESS,” the effective sample size (similar to n_eff) in the tails of the posterior. In\\nthose cases, it is nervous about the quality of extreme intervals, like 95%. Sampling more\\nusually helps.\\nThe warmup setting is more subtle. On the one hand, you want to have the shortest\\nwarmup period necessary, so you can get on with real sampling. But on the other hand, more\\nwarmup can mean more efficient sampling. With Stan models, typically you can devote as\\nmuch as half of your total samples, the iter value, to warmup and come out very well. But\\nfor simple models like those you’ve fit so far, much less warmup is really needed. Models\\ncan vary a lot in the shape of their posterior distributions, so again there is no universally\\nbest answer. But if you are having trouble, you might try increasing the warmup. If not, you\\nmight try reducing it. There’s a practice problem at the end of the chapter that guides you in\\nexperimenting with the amount of warmup.\\nRethinking: Warmup is not burn-in. Other MCMC algorithms and software often discuss burn-\\nin. With a sampling strategy like ordinary Metropolis, it is conventional and useful to trim off the\\nfront of the chain, the “burn-in” phase. This is done because it is unlikely that the chain has reached\\nstationarity within the first few samples. Trimming off the front of the chain hopefully removes any\\ninfluence of which starting value you chose for a parameter.156\\nBut Stan’s sampling algorithms use a different approach. What Stan does during warmup is quite\\ndifferent from what it does after warmup. The warmup samples are used to adapt sampling, to find\\ngood values for the step size and the number of steps. Warmup samples are not representative of\\nthe target posterior distribution, no matter how long warmup continues. They are not burning in,\\nbut rather more like cycling the motor to heat things up and get ready for sampling. When real\\nsampling begins, the samples will be immediately from the target distribution, assuming adaptation\\nwas successful.\\n9.5.2. How many chains do you need? It is very common to run more than one Markov\\nchain, when estimating a single model. To do this with ulam or stan itself, the chains\\nargument specifies the number of independent Markov chains to sample from. And the\\noptional cores argument lets you distribute the chains across different processors, so they\\ncan run simultaneously, rather than sequentially. All of the non-warmup samples from each\\nchain will be automatically combined in the resulting inferences.\\nSo the question naturally arises: How many chains do we need? There are three answers\\nto this question. First, when initially debugging a model, use a single chain. There are some\\nerror messages that don’t display unless you use only one chain. The chain will fail with\\nmore than one chain, but the reason may not be displayed. This is why the ulam default is\\nchains=1. Second, when deciding whether the chains are valid, you need more than one\\nchain. Third, when you begin the final run that you’ll make inferences from, you only really\\nneed one chain. But using more than one chain is fine, as well. It just doesn’t matter, once\\nyou’re sure it’s working. I’ll briefly explain these answers.\\nThe first time you try to sample from a chain, you might not be sure whether the chain is\\nworking right. So of course you will check the trace plot or trank plot. Having more than one\\nchain during these checks helps to make sure that the Markov chains are all converging to the\\nsame distribution. Sometimes, individual chains look like they’ve settled down to a stable\\n'},\n",
       " {'index': 307,\n",
       "  'number': 289,\n",
       "  'content': '9.5. CARE AND FEEDING OF YOUR MARKOV CHAIN\\n289\\ndistribution, but if you run the chain again, it might settle down to a different distribution.\\nWhen you run multiple Markov chains, each with different starting positions, and see that all\\nof them end up in the same region of parameter space, it provides a check that the machine\\nis working correctly. Using 3 or 4 chains is often enough to reassure us that the sampling is\\nworking properly.\\nBut once you’ve verified that the sampling is working well, and you have a good idea of\\nhow many warmup samples you need, it’s perfectly safe to just run one long chain. For ex-\\nample, suppose we learn that we need 1000 warmup samples and about 9000 real samples in\\ntotal. Should we run one chain, with warmup=1000 and iter=10000, or rather 3 chains, with\\nwarmup=1000 and iter=4000? It doesn’t really matter, in terms of inference. But it might\\nmatter in efficiency, because the 3 chains duplicate warmup effect that just gets thrown away.\\nAnd since warmup is typically the slowest part of the chain, these extra warmup samples cost\\na disproportionate amount of your computer’s time. On the other hand, if you run the chains\\non different processor cores, then you might prefer 3 chains, because you can spread the load\\nand finish the whole job faster. My institute uses shared computing servers with 64 or more\\navailable cores. We run a lot of parallel chains.\\nThere are exotic situations in which all of the advice above must be modified. But for\\ntypical regression models, you can live by the motto one short chain to debug, four chains for\\nverification and inference.\\nThings may still go wrong. One of the perks of using HMC and Stan is that when sam-\\npling isn’t working right, it’s usually very obvious. As you’ll see in the sections to follow, bad\\nchains tend to have conspicuous behavior. Other methods of MCMC sampling, like Gibbs\\nsampling and ordinary Metropolis, aren’t so easy to diagnose.\\nRethinking: Convergence diagnostics. The default diagnostic output from Stan includes two met-\\nrics, n_eff and Rhat. The first is a measure of the effective number of samples. The second is the\\nGelman-Rubin convergence diagnostic, ˆR.157 When n_eff is much lower than the actual number of\\niterations (minus warmup) of your chains, it means the chains are inefficient, but possibly still okay.\\nWhen Rhat is above 1.00, it usually indicates that the chain has not yet converged, and probably you\\nshouldn’t trust the samples. If you draw more iterations, it could be fine, or it could never converge.\\nSee the Stan user manual for more details. It’s important however not to rely too much on these di-\\nagnostics. Like all heuristics, there are cases in which they provide poor advice. For example, Rhat\\ncan reach 1.00 even for an invalid chain. So view it perhaps as a signal of danger, but never of safety.\\nFor conventional models, these metrics typically work well.\\n9.5.3. Taming a wild chain. One common problem with some models is that there are\\nbroad, flat regions of the posterior density. This happens most often, as you might guess,\\nwhen one uses flat priors. The problem this can generate is a wild, wandering Markov chain\\nthat erratically samples extremely positive and extremely negative parameter values.\\nLet’s look at a simple example. The code below tries to estimate the mean and standard\\ndeviation of the two Gaussian observations −1 and 1. But it uses totally flat priors.\\nR code\\n9.22\\ny <- c(-1,1)\\nset.seed(11)\\nm9.2 <- ulam(\\nalist(\\ny ~ dnorm( mu , sigma ) ,\\n'},\n",
       " {'index': 308,\n",
       "  'number': 290,\n",
       "  'content': '290\\n9. MARKOV CHAIN MONTE CARLO\\nmu <- alpha ,\\nalpha ~ dnorm( 0 , 1000 ) ,\\nsigma ~ dexp( 0.0001 )\\n) , data=list(y=y) , chains=3 )\\nNow let’s look at the precis output:\\nR code\\n9.23\\nprecis( m9.2 )\\nmean\\nsd\\n5.5%\\n94.5% n_eff Rhat\\nalpha\\n69.38\\n393.89 -363.57\\n739.53\\n116 1.03\\nsigma 568.53 1247.18\\n6.81 2563.38\\n179 1.02\\nWhoa! This posterior can’t be right. The mean of −1 and 1 is zero, so we’re hoping to get\\na mean value for alpha around zero. Instead we get crazy values and implausibly wide in-\\ntervals. Inference for sigma is no better. The n_eff and Rhat diagnostics don’t look good\\neither. We drew 1500 samples total, but the estimated effective sample sizes are 116 and 179.\\nYou might get different numbers, but they will qualitatively be just as bad.\\nYou should also see several warning messages, including:\\nWarning messages:\\n1: There were 67 divergent transitions after warmup. Increasing adapt_delta\\nabove 0.95 may help. See\\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\\nThere is useful advice at the URL. The quick version is that Stan detected problems in ex-\\nploring all of the posterior. These are divergent transitions. I’ll give a more thorough\\nexplanation in a later chapter. Think of them as Stan’s way of telling you there are problems\\nwith the chains. For simple models, increasing the adapt_delta control parameter will usu-\\nally remove the divergent transitions. This is explained more in the Overthinking box at the\\nend of this section. You can try adding control=list(adapt_delta=0.99) to the ulam\\ncall—ulam’s default is 0.95. But it won’t help much in this specific case. This problem runs\\ndeeper, with the model itself.\\nYou should also see a second warning:\\n2: Examine the pairs() plot to diagnose sampling problems\\nThis refers to Stan’s pairs method, not ulam’s. To use it, try pairs( m9.2@stanfit ). This\\nis like ulam’s pairs plot, but divergent transitions are colored in red. For that reason, the plot\\nwon’t reproduce in this book. So be sure to inspect it on your own machine. The shape of\\nthe posterior alone should shake your confidence.\\nNow take a look at the trace plot for this fit, traceplot(m9.2). It’s shown in the top\\nrow of Figure 9.9. The reason for the weird estimates is that the Markov chains seem to\\ndrift around and spike occasionally to extreme values. This is not a healthy pair of chains,\\nand they do not provide useful samples. The trankplot(m9.2) is also shown. The rank\\nhistograms spend long periods with one chain above or below the others. This indicates\\npoor exploration of the posterior.\\nIt’s easy to tame this particular chain by using weakly informative priors. The reason\\nthe model above drifts wildly in both dimensions is that there is very little data, just two\\nobservations, and flat priors. The flat priors say that every possible value of the parameter\\nis equally plausible, a priori. For parameters that can take a potentially infinite number of\\n'},\n",
       " {'index': 309,\n",
       "  'number': 291,\n",
       "  'content': '9.5. CARE AND FEEDING OF YOUR MARKOV CHAIN\\n291\\n200\\n400\\n600\\n800\\n1000\\n-2000\\n0\\n2000\\nn_eff = 116\\nalpha\\n200\\n400\\n600\\n800\\n1000\\n0\\n10000\\nn_eff = 179\\nsigma\\nn_eff = 116\\nalpha\\nn_eff = 179\\nsigma\\n200\\n400\\n600\\n800\\n1000\\n-6\\n-2\\n2\\nn_eff = 478\\nalpha\\n200\\n400\\n600\\n800\\n1000\\n2\\n4\\n6\\n8\\nn_eff = 438\\nsigma\\nn_eff = 478\\nalpha\\nn_eff = 438\\nsigma\\nFigure 9.9. Diagnosing and healing a sick Markov chain. Top two rows:\\nTrace and trank plots from three chains defined by model m9.2. These\\nchains are not healthy. Bottom two rows: Adding weakly informative priors\\nin m9.3 clears up the condition right away.\\nvalues, like alpha, this means the Markov chain needs to occasionally sample some pretty\\nextreme and implausible values, like negative 30 million. These extreme drifts overwhelm\\nthe chain. If the likelihood were stronger, then the chain would be fine, because it would\\nstick closer to zero.\\nBut it doesn’t take much information in the prior to stop this foolishness, even without\\nmore data. Let’s use this model:\\nyi ∼Normal(µ, σ)\\nµ = α\\nα ∼Normal(1, 10)\\nσ ∼Exponential(1)\\n'},\n",
       " {'index': 310,\n",
       "  'number': 292,\n",
       "  'content': '292\\n9. MARKOV CHAIN MONTE CARLO\\n-15\\n-10\\n-5\\n0\\n5\\n10\\n15\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nalpha\\nDensity\\nposterior\\nprior\\n0\\n2\\n4\\n6\\n8\\n10\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nsigma\\nDensity\\nFigure 9.10. Prior (dashed) and posterior (blue) for the model with weakly\\ninformative priors, m9.3. Even with only two observations, the likelihood\\neasily overcomes these priors. Yet the posterior cannot be successfully ap-\\nproximated without them.\\nI’ve just added weakly informative priors for α and σ. We’ll plot these priors in a moment,\\nso you will be able to see just how weak they are. But let’s re-approximate the posterior first:\\nR code\\n9.24\\nset.seed(11)\\nm9.3 <- ulam(\\nalist(\\ny ~ dnorm( mu , sigma ) ,\\nmu <- alpha ,\\nalpha ~ dnorm( 1 , 10 ) ,\\nsigma ~ dexp( 1 )\\n) , data=list(y=y) , chains=3 )\\nprecis( m9.3 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat4\\nalpha 0.10 1.13 -1.60\\n1.97\\n478\\n1\\nsigma 1.52 0.72\\n0.67\\n2.86\\n438\\n1\\nThat’s much better. Take a look at the bottom portion of Figure 9.9. The trace and trank\\nplots look healthy. Both chains are stationary around the same values, and mixing is good.\\nNo more wild detours into the thousands. And those divergent transitions are gone.\\nTo appreciate what has happened, take a look at the priors (dashed) and posteriors (blue)\\nin Figure 9.10. Both the Gaussian prior for α and the exponential prior for σ contain very\\ngradual downhill slopes. They are so gradual, that even with only two observations, as in\\nthis example, the likelihood almost completely overcomes them. The mean of the prior for\\nα is 1, but the mean of the posterior is zero, just as the likelihood says it should be.\\nThese weakly informative priors have helped by providing a very gentle nudge towards\\nreasonable values of the parameters. Now values like 30 million are no longer equally plau-\\nsible as small values like 1 or 2. Lots of problematic chains want subtle priors like these,\\n'},\n",
       " {'index': 311,\n",
       "  'number': 293,\n",
       "  'content': '9.5. CARE AND FEEDING OF YOUR MARKOV CHAIN\\n293\\ndesigned to tune estimation by assuming a tiny bit of prior information about each param-\\neter. And even though the priors end up getting washed out right away—two observations\\nwere enough here—they still have a big effect on inference, by allowing us to get an answer.\\nThat answer is also a good answer. This point will be even more important for non-Gaussian\\nmodels to come.\\nRethinking: The folk theorem of statistical computing. The example above illustrates Andrew Gel-\\nman’s folk theorem of statistical computing: When you have computational problems, often\\nthere’s a problem with your model.158 Before we begin to tune the software and pour more computer\\npower into a problem, it can be useful to go over the model specification again, and the data itself, to\\nmake sure the problem isn’t in the pre-sampling stage. It’s very common when working with Bayesian\\nmodels that slow or clunky sampling is due to something as simple as having entirely omitted one or\\nmore prior distributions.\\nOverthinking: Divergent transitions are your friend. You’ll see divergent transition warnings\\noften in using ulam and Stan. They are your friend, providing a helpful warning. These warnings\\narise when the numerical simulation that HMC uses is inaccurate. HMC can detect these inaccu-\\nracies. That is one of its major advantages over other sampling approaches, most of which provide\\nfew automatic ways to discover bad chains. We’ll examine these divergent transitions in much more\\ndetail in a later chapter. We’ll also see some clever ways to work around them.\\n9.5.4. Non-identifiable parameters. Back in Chapter 6, you met the problem of highly cor-\\nrelated predictors and the non-identifiable parameters they can create. Here you’ll see what\\nsuch parameters look like inside of a Markov chain. You’ll also see how you can identify\\nthem, in principle, by using a little prior information. Most importantly, the badly behaving\\nchains produced in this example will exhibit characteristic bad behavior, so when you see\\nthe same pattern in your own models, you’ll have a hunch about the cause.\\nTo construct a non-identifiable model, we first simulate 100 observations from a Gauss-\\nian distribution with mean zero and standard deviation 1.\\nR code\\n9.25\\nset.seed(41)\\ny <- rnorm( 100 , mean=0 , sd=1 )\\nBy simulating the data, we know the right answer. Then we fit this model:\\nyi ∼Normal(µ, σ)\\nµ = α1 + α2\\nα1 ∼Normal(0, 1000)\\nα2 ∼Normal(0, 1000)\\nσ ∼Exponential(1)\\nThe linear model contains two parameters, α1 and α2, which cannot be identified. Only their\\nsum can be identified, and it should be about zero, after estimation.\\nLet’s run the Markov chain and see what happens. This chain is going to take much\\nlonger than the previous ones. But it should still finish after a few minutes.\\n'},\n",
       " {'index': 312,\n",
       "  'number': 294,\n",
       "  'content': '294\\n9. MARKOV CHAIN MONTE CARLO\\nR code\\n9.26\\nset.seed(384)\\nm9.4 <- ulam(\\nalist(\\ny ~ dnorm( mu , sigma ) ,\\nmu <- a1 + a2 ,\\na1 ~ dnorm( 0 , 1000 ),\\na2 ~ dnorm( 0 , 1000 ),\\nsigma ~ dexp( 1 )\\n) , data=list(y=y) , chains=3 )\\nprecis( m9.4 )\\nmean\\nsd\\n5.5%\\n94.5% n_eff Rhat4\\na1\\n-364.81 318.57 -792.82 240.50\\n2\\n2.80\\na2\\n365.00 318.57 -240.26 792.98\\n2\\n2.80\\nsigma\\n1.05\\n0.10\\n0.90\\n1.19\\n2\\n2.02\\nThose estimates look suspicious, and the n_eff and Rhat values are terrible. The means for\\na1 and a2 are about the same distance from zero, but on opposite sides of zero. And the\\nstandard deviations are massive. This is a result of the fact that we cannot simultaneously\\nestimate a1 and a2, but only their sum. You should also see a warning:\\nWarning messages:\\n1: There were 1199 transitions after warmup that exceeded the maximum treedepth.\\nIncrease max_treedepth above 10. See\\nhttp://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded\\nThis is confusing. If you visit the URL, you’ll see that this means the chains are inefficient,\\nbecause some internal limit was reached. These treedepth warnings usually indicate inef-\\nficient chains, but not necessarily broken chains. To increase the treedepth, you can add\\ncontrol=list(max_treedepth=15) to the ulam call. But it won’t help much. There is\\nsomething else seriously wrong here.\\nLooking at the trace plot reveals more. The left column in Figure 9.11 shows two\\nMarkov chains from the model above. These chains do not look like they are stationary, nor\\ndo they seem to be mixing very well. Indeed, when you see a pattern like this, it is reason to\\nworry. Don’t use these samples.\\nAgain, weakly regularizing priors can rescue us. Now the model fitting code is:\\nR code\\n9.27\\nm9.5 <- ulam(\\nalist(\\ny ~ dnorm( mu , sigma ) ,\\nmu <- a1 + a2 ,\\na1 ~ dnorm( 0 , 10 ),\\na2 ~ dnorm( 0 , 10 ),\\nsigma ~ dexp( 1 )\\n) , data=list(y=y) , chains=3 )\\nprecis( m9.5 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat4\\na1\\n0.01 7.16 -11.43 11.54\\n389\\n1\\na2\\n0.18 7.15 -11.41 11.57\\n389\\n1\\nsigma 1.03 0.08\\n0.92\\n1.17\\n448\\n1\\n'},\n",
       " {'index': 313,\n",
       "  'number': 295,\n",
       "  'content': '9.5. CARE AND FEEDING OF YOUR MARKOV CHAIN\\n295\\n200\\n400\\n600\\n800\\n1000\\n-1000\\n-500\\n0\\n500\\nn_eff = 2\\na1\\n200\\n400\\n600\\n800\\n1000\\n-500\\n0\\n500\\n1000\\nn_eff = 2\\na2\\n200\\n400\\n600\\n800\\n1000\\n0.9\\n1.0\\n1.1\\n1.2\\nn_eff = 2\\nsigma\\nn_eff = 2\\na1\\nn_eff = 2\\na2\\nn_eff = 2\\nsigma\\n200\\n400\\n600\\n800\\n1000\\n-20\\n0\\n10\\n20\\nn_eff = 389\\na1\\n200\\n400\\n600\\n800\\n1000\\n-20\\n0\\n10\\n20\\nn_eff = 389\\na2\\n200\\n400\\n600\\n800\\n1000\\n0.9\\n1.0\\n1.1\\n1.2\\n1.3\\nn_eff = 448\\nsigma\\nn_eff = 389\\na1\\nn_eff = 389\\na2\\nn_eff = 448\\nsigma\\nFigure 9.11. Top panel, m9.4. A chain with wandering parameters, a1 and\\na2. Bottom panel, m9.5. Same model but with weakly informative priors.\\n'},\n",
       " {'index': 314,\n",
       "  'number': 296,\n",
       "  'content': '296\\n9. MARKOV CHAIN MONTE CARLO\\nThe estimates for a1 and a2 are better identified now. Well, they still aren’t individually\\nidentified. But their sum is identified. Compare the trace and trank plots in Figure 9.11.\\nNotice also that the model sampled a lot faster. With flat priors, m9.4, sampling may take\\n3 times as long as it does for m9.5. Often, a model that is very slow to sample is under-\\nidentified. This is an aspect of the folk theorem of statistical computing (page 293).\\nIn the end, adding some weakly informative priors saves this model. You might think\\nyou’d never accidentally try to fit an unidentified model. But you’d be wrong. Even if you\\ndon’t make obvious mistakes, complex models can easily become unidentified or nearly so.\\nWith many predictors, and especially with interactions, correlations among parameters can\\nbe large. Just a little prior information telling the model “none of these parameters can be 30\\nmillion” often helps, and it has no effect on estimates. A flat prior really is flat, all the way to\\ninfinity. Unless you believe infinity is a reasonable estimate, don’t use a flat prior.\\nAdditionally, adding weak priors can speed up sampling, because the Markov chain won’t\\nfeel that it has to run out to extreme values that you, but not your model, already know are\\nhighly implausible.\\nRethinking: Hamiltonian warnings and Gibbs overconfidence. When people start using Stan, or\\nsome other Hamiltonian sampler, they often find that models they used to fit in Metropolis-Hastings\\nand Gibbs samplers like BUGS, JAGS, and MCMCglmm no longer work well. The chains are slow.\\nThere are lots of warnings. Stan is really something of a nag. Is something wrong with Stan?\\nNo. Those problems were probably always there, even in the other tools. But since Gibbs doesn’t\\nuse gradients, it doesn’t notice some issues that a Hamiltonian engine will. A culture has evolved in\\napplied statistics of running bad chains for a very long time—for millions of iterations—and then\\nthinning aggressively, praying, and publishing. Phylogenetic analyses may be particularly prone to\\nthis, since tree spaces are very difficult to explore.159 Tools like Stan and other Hamiltonian engines\\nare so important for reliable research precisely because they provide more diagnostic criteria for the\\naccuracy of the Monte Carlo approximation. Don’t resent the nagging.\\n9.6. Summary\\nThis chapter has been an informal introduction to Markov chain Monte Carlo (MCMC)\\nestimation. The goal has been to introduce the purpose and approach MCMC algorithms.\\nThe major algorithms introduced were the Metropolis, Gibbs sampling, and Hamiltonian\\nMonte Carlo algorithms. Each has its advantages and disadvantages. The ulam function in\\nthe rethinking package was introduced. It uses the Stan (mc-stan.org) Hamiltonian Monte\\nCarlo engine to fit models as they are defined in this book. General advice about diagnosing\\npoor MCMC fits was introduced by the use of a couple of pathological examples. In the next\\nchapters, we use this new power to learn new kinds of models.\\n9.7. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n9E1. Which of the following is a requirement of the simple Metropolis algorithm?\\n(1) The parameters must be discrete.\\n(2) The likelihood function must be Gaussian.\\n(3) The proposal distribution must be symmetric.\\n9E2. Gibbs sampling is more efficient than the Metropolis algorithm. How does it achieve this extra\\nefficiency? Are there any limitations to the Gibbs sampling strategy?\\n'},\n",
       " {'index': 315,\n",
       "  'number': 297,\n",
       "  'content': '9.7. PRACTICE\\n297\\n9E3. Which sort of parameters can Hamiltonian Monte Carlo not handle? Can you explain why?\\n9E4. Explain the difference between the effective number of samples, n_eff as calculated by Stan,\\nand the actual number of samples.\\n9E5. Which value should Rhat approach, when a chain is sampling the posterior distribution cor-\\nrectly?\\n9E6. Sketch a good trace plot for a Markov chain, one that is effectively sampling from the posterior\\ndistribution. What is good about its shape? Then sketch a trace plot for a malfunctioning Markov\\nchain. What about its shape indicates malfunction?\\n9E7. Repeat the problem above, but now for a trace rank plot.\\n9M1. Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior\\nfor the standard deviation, sigma. The uniform prior should be dunif(0,1). Use ulam to estimate\\nthe posterior. Does the different prior have any detectible influence on the posterior distribution of\\nsigma? Why or why not?\\n9M2. Modify theterrain ruggednessmodelagain. Thistime, change thepriorfor b[cid] to dexp(0.3).\\nWhat does this do to the posterior distribution? Can you explain it?\\n9M3. Re-estimate one of the Stan models from the chapter, but at different numbers of warmup it-\\nerations. Be sure to use the same number of sampling iterations in each case. Compare the n_eff\\nvalues. How much warmup is enough?\\n9H1. Run the model below and then inspect the posterior distribution and explain what it is accom-\\nplishing.\\nR code\\n9.28\\nmp <- ulam(\\nalist(\\na ~ dnorm(0,1),\\nb ~ dcauchy(0,1)\\n), data=list(y=1) , chains=1 )\\nCompare the samples for the parameters a and b. Can you explain the different trace plots? If you are\\nunfamiliar with the Cauchy distribution, you should look it up. The key feature to attend to is that it\\nhas no expected value. Can you connect this fact to the trace plot?\\n9H2. Recall the divorce rate example from Chapter 5. Repeat that analysis, using ulam this time,\\nfitting models m5.1, m5.2, and m5.3. Use compare to compare the models on the basis of WAIC\\nor PSIS. To use WAIC or PSIS with ulam, you need add the argument log_log=TRUE. Explain the\\nmodel comparison results.\\n9H3. Sometimes changing a prior for one parameter has unanticipated effects on other parameters.\\nThis is because when a parameter is highly correlated with another parameter in the posterior, the\\nprior influences both parameters. Here’s an example to work and think through.\\nGo back to the leg length example in Chapter 6 and use the code there to simulate height and\\nleg lengths for 100 imagined individuals. Below is the model you fit before, resulting in a highly\\ncorrelated posterior for the two beta parameters. This time, fit the model using ulam:\\nR code\\n9.29\\n'},\n",
       " {'index': 316,\n",
       "  'number': 298,\n",
       "  'content': '298\\n9. MARKOV CHAIN MONTE CARLO\\nm5.8s <- ulam(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a + bl*leg_left + br*leg_right ,\\na ~ dnorm( 10 , 100 ) ,\\nbl ~ dnorm( 2 , 10 ) ,\\nbr ~ dnorm( 2 , 10 ) ,\\nsigma ~ dexp( 1 )\\n) , data=d, chains=4,\\nstart=list(a=10,bl=0,br=0.1,sigma=1) )\\nCompare the posterior distribution produced by the code above to the posterior distribution pro-\\nduced when you change the prior for br so that it is strictly positive:\\nR code\\n9.30\\nm5.8s2 <- ulam(\\nalist(\\nheight ~ dnorm( mu , sigma ) ,\\nmu <- a + bl*leg_left + br*leg_right ,\\na ~ dnorm( 10 , 100 ) ,\\nbl ~ dnorm( 2 , 10 ) ,\\nbr ~ dnorm( 2 , 10 ) ,\\nsigma ~ dexp( 1 )\\n) , data=d, chains=4,\\nconstraints=list(br=\"lower=0\"),\\nstart=list(a=10,bl=0,br=0.1,sigma=1) )\\nNote the constraints list. What this does is constrain the prior distribution of br so that it has\\npositive probability only above zero. In other words, that prior ensures that the posterior distribution\\nfor br will have no probability mass below zero. Compare the two posterior distributions for m5.8s\\nand m5.8s2. What has changed in the posterior distribution of both beta parameters? Can you\\nexplain the change induced by the change in prior?\\n9H4. For the two models fit in the previous problem, use WAIC or PSIS to compare the effective\\nnumbers of parameters for each model. You will need to use log_lik=TRUE to instruct ulam to\\ncompute the terms that both WAIC and PSIS need. Which model has more effective parameters?\\nWhy?\\n9H5. Modify the Metropolis algorithm code from the chapter to handle the case that the island\\npopulations have a different distribution than the island labels. This means the island’s number will\\nnot be the same as its population.\\n9H6. Modify the Metropolis algorithm code from the chapter to write your own simple MCMC\\nestimator for globe tossing data and model from Chapter 2.\\n9H7. Can you write your own Hamiltonian Monte Carlo algorithm for the globe tossing data, using\\nthe R code in the chapter? You will have to write your own functions for the likelihood and gradient,\\nbut you can use the HMC2 function.\\n'},\n",
       " {'index': 317,\n",
       "  'number': 299,\n",
       "  'content': '10 Big Entropy and the Generalized Linear Model\\nMost readers of this book will share the experience of fighting with tangled electrical\\ncords. Whether behind a desk or stuffed in a box, cords and cables tend toward tying them-\\nselves in knots. Why is this? There is of course real physics at work. But at a descriptive level,\\nthe reason is entropy: There are vastly more ways for cords to end up in a knot than for them\\nto remain untied.160 So if I were to carefully lay a dozen cords in a box and then seal the box\\nand shake it, we should bet that at least some of the cords will be tangled together when I\\nagain open the box. We don’t need to know anything about the physics of cords or knots.\\nWe just have to bet on entropy. Events that can happen vastly more ways are more likely.\\nExploiting entropy is not going to untie your cords. But it will help you solve some prob-\\nlems in choosing distributions. Statistical models force many choices upon us. Some of these\\nchoices are distributions that represent uncertainty. We must choose, for each parameter,\\na prior distribution. And we must choose a likelihood function, which serves as a distribu-\\ntion of data. There are conventional choices, such as wide Gaussian priors and the Gaussian\\nlikelihood of linear regression. These conventional choices work unreasonably well in many\\ncircumstances. But very often the conventional choices are not the best choices. Inference\\ncan be more powerful when we use all of the information, and doing so usually requires\\ngoing beyond convention.\\nTo go beyond convention, it helps to have some principles to guide choice. When an\\nengineer wants to make an unconventional bridge, engineering principles help guide choice.\\nWhen a researcher wants to build an unconventional model, entropy provides one useful\\nprinciple to guide choice of probability distributions: Bet on the distribution with the biggest\\nentropy. Why? There are three sorts of justifications.\\nFirst, the distribution with the biggest entropy is the widest and least informative distri-\\nbution. Choosing the distribution with the largest entropy means spreading probability as\\nevenly as possible, while still remaining consistent with anything we think we know about\\na process. In the context of choosing a prior, it means choosing the least informative distri-\\nbution consistent with any partial scientific knowledge we have about a parameter. In the\\ncontext of choosing a likelihood, it means selecting the distribution we’d get by counting up\\nall the ways outcomes could arise, consistent with the constraints on the outcome variable.\\nIn both cases, the resulting distribution embodies the least information while remaining true\\nto the information we’ve provided.\\nSecond, nature tends to produce empirical distributions that have high entropy. Back in\\nChapter 4, I introduced the Gaussian distribution by demonstrating how any process that\\nrepeatedly adds together fluctuations will tend towards an empirical distribution with the\\ndistinctive Gaussian shape. That shape is the one that contains no information about the\\nunderlying process except its location and variance. As a result, it has maximum entropy.\\n299\\n'},\n",
       " {'index': 318,\n",
       "  'number': 300,\n",
       "  'content': '300\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\nNatural processes other than addition also tend to produce maximum entropy distributions.\\nBut they are not Gaussian. They retain different information about the underlying process.\\nThird, regardless of why it works, it tends to work. Mathematical procedures are effective\\neven when we don’t understand them. There are no guarantees that any logic in the small\\nworld (Chapter 2) will be useful in the large world. We use logic in science because it has a\\nstrong record of effectiveness in addressing real world problems. This is the historical justi-\\nfication: The approach has solved difficult problems in the past. This is no guarantee that it\\nwill work on your problem. But no approach can guarantee that.\\nThis chapter serves as a conceptual introduction to generalized linear models and\\nthe principle of maximum entropy. A generalized linear model (GLM) is much like the\\nlinear regressions of previous chapters. It is a model that replaces a parameter of a likelihood\\nfunction with a linear model. But GLMs need not use Gaussian likelihoods. Any likelihood\\nfunction can be used, and linear models can be attached to any or all of the parameters that\\ndescribe its shape. The principle of maximum entropy helps us choose likelihood functions,\\nby providing a way to use stated assumptions about constraints on the outcome variable to\\nchoose the likelihood function that is the most conservative distribution compatible with\\nthe known constraints. Using this principle recovers all the most common likelihood func-\\ntions of many statistical approaches, Bayesian or not, while simultaneously providing a clear\\nrationale for choice among them.\\nThe chapters to follow this one build computational skills for working with different\\nflavors of GLM. Chapter 11 addresses models for count variables. Chapter 12 explores more\\ncomplicated models, such as ordinal outcomes and mixtures. Portions of these chapters are\\nspecialized by model type. So you can skip sections that don’t interest you at the moment.\\nThe multilevel chapters, beginning with Chapter 13, make use of binomial count models,\\nhowever. So some familiarity with the material in Chapter 11 will be helpful.\\nRethinking: Bayesian updating is entropy maximization. Another kind of probability distribution,\\nthe posterior distribution deduced by Bayesian updating, is also a case of maximizing entropy. The\\nposterior distribution has the greatest entropy relative to the prior (the smallest cross entropy) among\\nall distributions consistent with the assumed constraints and the observed data.161 This fact won’t\\nchange how you calculate. But it should provide a deeper appreciation of the fundamental connec-\\ntions between Bayesian inference and information theory. Notably, Bayesian updating is just like\\nmaximum entropy in that it produces the least informative distribution that is still consistent with\\nour assumptions. Or you might say that the posterior distribution has the smallest divergence from\\nthe prior that is possible while remaining consistent with the constraints and data.\\n10.1. Maximum entropy\\nIn Chapter 7, you met the basics of information theory. In brief, we seek a measure of\\nuncertainty that satisfies three criteria: (1) the measure should be continuous; (2) it should\\nincrease as the number of possible events increases; and (3) it should be additive. The result-\\ning unique measure of the uncertainty of a probability distribution p with probabilities pi for\\neach possible event i turns out to be just the average log-probability:\\nH(p) = −\\nX\\ni\\npi log pi\\nThis function is known as information entropy.\\n'},\n",
       " {'index': 319,\n",
       "  'number': 301,\n",
       "  'content': '10.1. MAXIMUM ENTROPY\\n301\\nThe principle of maximum entropy applies this measure of uncertainty to the problem of\\nchoosing among probability distributions. Perhaps the simplest way to state the maximum\\nentropy principle is:\\nThe distribution that can happen the most ways is also the distribution with\\nthe biggest information entropy. The distribution with the biggest entropy\\nis the most conservative distribution that obeys its constraints.\\nThere’s nothing intuitive about this idea, so if it seems weird, you are normal.\\nTo begin to understand maximum entropy, forget about information and probability\\ntheory for the moment. Imagine instead 5 buckets and a pile of 10 individually numbered\\npebbles. You stand and toss all 10 pebbles such that each pebble is equally likely to land\\nin any of the 5 buckets. This means that every particular arrangement of the 10 individual\\npebbles is equally likely—it’s just as likely to get all 10 in bucket 3 as it is to get pebble 1 in\\nbucket 2, pebbles 2–9 in bucket 3, and pebble 10 in bucket 4.\\nBut some kinds of arrangements are much more likely. Some arrangements look the\\nsame, because they show the same number of pebbles in the same individual buckets. These\\nare distributions of pebbles. Figure 10.1 illustrates 5 such distributions. So for example\\nthere is only 1 way to arrange the individual pebbles so that all of them are in bucket 3 (plot\\nA). But there are 90 ways to arrange the individual pebbles so that 2 of them are in bucket 2,\\n8 in bucket 3, and 2 in bucket 4 (plot B). Plots C, D, and E show that the number of unique\\narrangements corresponding to a distribution grows very rapidly as the distribution places a\\nmore equal number of pebbles in each bucket. By the time there are 2 pebbles in each bucket\\n(plot E), there are 113400 ways to realize this distribution. There is no other distribution of\\nthe pebbles that can be realized a greater number of ways.\\nLet’s put each distribution of pebbles in a list:\\nR code\\n10.1\\np <- list()\\np$A <- c(0,0,10,0,0)\\np$B <- c(0,1,8,1,0)\\np$C <- c(0,2,6,2,0)\\np$D <- c(1,2,4,2,1)\\np$E <- c(2,2,2,2,2)\\nAnd let’s normalize each such that it is a probability distribution. This means we just divide\\neach count of pebbles by the total number of pebbles:\\nR code\\n10.2\\np_norm <- lapply( p , function(q) q/sum(q))\\nSince these are now probability distributions, we can compute the information entropy of\\neach. The only trick here is to remember L’Hôpital’s rule (see page 207):\\nR code\\n10.3\\n( H <- sapply( p_norm , function(q) -sum(ifelse(q==0,0,q*log(q))) ) )\\nA\\nB\\nC\\nD\\nE\\n0.0000000 0.6390319 0.9502705 1.4708085 1.6094379\\nSo distribution E, which can realized by far the greatest number of ways, also has the biggest\\nentropy. This is no coincidence. To see why, let’s compute the logarithm of number of ways\\n'},\n",
       " {'index': 320,\n",
       "  'number': 302,\n",
       "  'content': '302\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\n1\\n2\\n3\\n4\\n5\\nbucket\\npebbles\\n0\\n5\\n10\\n10\\n1\\n2\\n3\\n4\\n5\\nbucket\\npebbles\\n0\\n5\\n10\\n1\\n8\\n1\\n1\\n2\\n3\\n4\\n5\\nbucket\\npebbles\\n0\\n5\\n10\\n2\\n6\\n2\\n1\\n2\\n3\\n4\\n5\\nbucket\\npebbles\\n0\\n5\\n10\\n1\\n2\\n4\\n2\\n1\\n1\\n2\\n3\\n4\\n5\\nbucket\\npebbles\\n0\\n5\\n10\\n2\\n2\\n2\\n2\\n2\\n1 way\\n90 ways\\n1260 ways\\n37800 ways\\n113400 ways\\nA\\nB\\nC\\nD\\nE\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\n1\\n2\\n3\\n4\\n5\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n0.0\\n0.5\\n1.0\\n1.5\\nlog(ways) per pebble\\nentropy\\nA\\nB\\nC\\nD E\\nFigure 10.1. Entropy as a measure of the number of unique arrangements\\nof a system that produce the same distribution. Plots A through E show the\\nnumbers of unique ways to arrange 10 pebbles into each of 5 different dis-\\ntributions. Bottom-right: The entropy of each distribution plotted against\\nthe log number of ways per pebble to produce it.\\neach distribution can be realized, then divide that logarithm by 10, the number of pebbles.\\nThis gives us the log ways per pebble for each distribution:\\nR code\\n10.4\\nways <- c(1,90,1260,37800,113400)\\nlogwayspp <- log(ways)/10\\nThe bottom-right plot in Figure 10.1 displays these logwayspp values against the infor-\\nmation entropies H. These two sets of values contain the same information, as information\\nentropy is an approximation of the log ways per pebble (see the Overthinking box at the\\nend for details). As the number of pebbles grows larger, the approximation gets better. It’s\\n'},\n",
       " {'index': 321,\n",
       "  'number': 303,\n",
       "  'content': '10.1. MAXIMUM ENTROPY\\n303\\nalready extremely good, for just 10 pebbles. Information entropy is a way of counting how\\nmany unique arrangements correspond to a distribution.\\nThis is useful, because the distribution that can happen the greatest number of ways is the\\nmost plausible distribution. Call this distribution the maximum entropy distribution.\\nAs you might guess from the pebble example, the number of ways corresponding to the\\nmaximum entropy distribution eclipses that of any other distribution. And the numbers of\\nways for each distribution most similar to the maximum entropy distribution eclipse those\\nof less similar distributions. And so on, such that the vast majority of unique arrangements\\nof pebbles produce either the maximum entropy distribution or rather a distribution very\\nsimilar to it. And that is why it’s often effective to bet on maximum entropy: It’s the center\\nof gravity for the highly plausible distributions.\\nIts high plausibility is conditional on our assumptions, of course. To grasp the role\\nof assumptions—constraints and data—in maximum entropy, we’ll explore two examples.\\nFirst, we’ll derive the Gaussian distribution as the solution to an entropy maximization prob-\\nlem. Second, we’ll derive the binomial distribution, which we used way back in Chapter 2\\nto draw marbles and toss globes, as the solution to a different entropy maximization prob-\\nlem. These derivations will not be mathematically rigorous. Rather, they will be graphical\\nand aim to deliver a conceptual appreciation for what this thing called entropy is doing. The\\nOverthinking boxes in this section provide connections to the mathematics, for those who\\nare interested.\\nBut the most important thing is to be patient with yourself. Understanding of and in-\\ntuition for probability theory comes with experience. You can usefully apply the principle\\nof maximum entropy before you fully understand it. Indeed, it may be that no one fully\\nunderstands it. Over time, and within the contexts that you find it useful, the principle will\\nbecome more intuitive.\\nRethinking: What good is intuition? Like many aspects of information theory, maximum entropy\\nis not very intuitive. But note that intuition is just a guide to developing methods. When a method\\nworks, it hardly matters whether our intuition agrees. This point is important, because some people\\nstill debate statistical approaches on the basis of philosophical principles and intuitive appeal. Philos-\\nophy does matter, because it influences development and application. But it is a poor way to judge\\nwhether or not an approach is useful. Results are what matter. For example, the three criteria used to\\nderive information entropy, back in Chapter 7, are not also the justification for using information en-\\ntropy. The justification is rather that it has worked so well on so many problems where other methods\\nhave failed.\\nOverthinking: The Wallis derivation. Intuitively, we can justify maximum entropy just based upon\\nthe definition of information entropy. But there’s another derivation, attributed to Graham Wallis,162\\nthat doesn’t invoke “information” at all. Here’s a short version of the argument. Suppose there are M\\nobservable events, and we wish to assign a plausibility to each. We know some constraints about the\\nprocess that produces these events, such as its expected value or variance. Now imagine setting up M\\nbuckets and tossing a large number N of individual stones into them at random, in such a way that\\neach stone is equally likely to land in any of the M buckets. After all the stones have landed, we count\\nup the number of stones in each bucket i and use these counts ni to construct a candidate probability\\ndistribution defined by pi = ni/N. If this candidate distribution is consistent with our constraints, we\\nadd it to a list. If not, we empty the buckets and try again. After many rounds of this, the distribution\\nthat has occurred the most times is the fairest—in the sense that no bias was involved in tossing the\\nstones into buckets—that still obeys the constraints that we imposed.\\n'},\n",
       " {'index': 322,\n",
       "  'number': 304,\n",
       "  'content': '304\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\nIf we could employ the population of a large country in tossing stones every day for years on end,\\nwe could do this empirically. Luckily, the procedure can be studied mathematically. The probabil-\\nity of any particular candidate distribution is just its multinomial probability, the probability of the\\nobserved stone counts under uniform chances of landing in each bucket:\\nPr(n1, n2, ..., nm) =\\nN!\\nn1!n2!...nm!\\nM\\nY\\ni=1\\n\\x12 1\\nM\\n\\x13ni\\n=\\nN!\\nn1!n2!...nm!\\n\\x12 1\\nM\\n\\x13N\\n= W\\n\\x12 1\\nM\\n\\x13N\\nThe distribution that is realized most often will have the largest value of that ugly fraction W with\\nthe factorials in it. Call W the multiplicity, because it states the number of different ways a particular\\nset of counts could be realized. For example, landing all stones in the first bucket can happen only\\none way, by getting all the stones into that bucket and none in any of the other buckets. But there are\\nmany more ways to evenly distribute the stones in the buckets, because order does not matter. We\\ncare about this multiplicity, because we are seeking the distribution that would happen most often.\\nSo by selecting the distribution that maximizes this multiplicity, we can accomplish that goal.\\nWe’re almost at entropy. It’s easier to work with 1\\nN log(W), which will be maximized by the same\\ndistribution as W. Also note that ni = Npi. These changes give us:\\n1\\nN log W = 1\\nN\\n\\x00log N! −\\nX\\ni\\nlog[(Npi)!]\\n\\x01\\nNow since N is very large, we can approximate log N! with Stirling’s approximation, N log N −N:\\n1\\nN log W ≈1\\nN\\n \\nN log N −N −\\nX\\ni\\n(Npi log(Npi) −Npi)\\n!\\n= −\\nX\\ni\\npi log pi\\nAnd that’s the exact same formula as Shannon’s information entropy. Among distributions that satisfy\\nour constraints, the distribution that maximizes the expression above is the distribution that spreads\\nout probability as evenly as possible, while still obeying the constraints.\\nThis result generalizes easily to the case in which there is not an equal chance of each stone\\nlanding in each bucket.163 If we have prior information specified as a probability qi that a stone lands\\nin bucket i, then the quantity to maximize is instead:\\n1\\nN log Pr(n1, n2, ..., nm) ≈−\\nX\\ni\\npi log(pi/qi)\\nYou may recognize this as KL divergence from Chapter 7, just with a negative in front. This reveals\\nthat the distribution that maximizes entropy is also the distribution that minimizes the information\\ndistance from the prior, among distributions consistent with the constraints. When the prior is flat,\\nmaximum entropy gives the flattest distribution possible. When the prior is not flat, maximum en-\\ntropy updates the prior and returns the distribution that is most like the prior but still consistent with\\nthe constraints. This procedure is often called minimum cross-entropy. Furthermore, Bayesian updat-\\ning itself can be expressed as the solution to a maximum entropy problem in which the data represent\\nconstraints.164 Therefore Bayesian inference can be seen as producing a posterior distribution that is\\nmost similar to the prior distribution as possible, while remaining logically consistent with the stated\\ninformation.\\n10.1.1. Gaussian. When I introduced the Gaussian distribution in Chapter 4 (page 72), it\\nemerged from a generative process in which 1000 people repeatedly flipped coins and took\\nsteps left (heads) or right (tails) with each flip. The addition of steps led inevitably to a dis-\\ntribution of positions resembling the Gaussian bell curve. This process represents the most\\nbasic generative dynamic that leads to Gaussian distributions in nature. When many small\\nfactors add up, the ensemble of sums tends towards Gaussian.\\nBut obviously many other distributions are possible. The coin-flipping dynamic could\\nplace all 1000 people on the same side of the soccer field, for example. So why don’t we see\\n'},\n",
       " {'index': 323,\n",
       "  'number': 305,\n",
       "  'content': '10.1. MAXIMUM ENTROPY\\n305\\n-4\\n-2\\n0\\n2\\n4\\n0.0\\n0.2\\n0.4\\n0.6\\nvalue\\nDensity\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n1.36\\n1.38\\n1.40\\n1.42\\nshape\\nentropy\\nFigure 10.2. Maximum entropy and the Gaussian distribution. Left: Com-\\nparison of Gaussian (blue) and several other continuous distributions with\\nthe same variance. Right: Entropy is maximized when curvature of a gen-\\neralized normal distribution matches the Gaussian, where shape is equal to\\n2.\\nthose other distributions in nature? Because for every sequence of coin flips that can produce\\nsuch an imbalanced outcome, there are vastly many more that can produce an approximately\\nbalanced outcome. The bell curve emerges, empirically, because there are so many different\\ndetailed states of the physical system that can produce it. Whatever does happen, it’s bound\\nto produce an ensemble that is approximately Gaussian. So if all you know about a collection\\nof continuous values is its variance (or that it has a finite variance, even if you don’t know\\nit yet), the safest bet is that the collection ends up in one of these vastly many bell-shaped\\nconfigurations.165\\nAnd maximum entropy just seeks the distribution that can arise the largest number of\\nways, so it does a good job of finding limiting distributions like this. But since entropy is\\nmaximized when probability is spread out as evenly as possible, maximum entropy also seeks\\nthe distribution that is most even, while still obeying its constraints. In order to visualize how\\nthe Gaussian is the most even distribution for any given variance, let’s consider a family of\\ngeneralized distributions with equal variance. A generalized normal distribution is defined\\nby the probability density:\\nPr(y|µ, α, β) =\\nβ\\n2αΓ(1/β)e−\\n( |y−µ|\\nα\\n)β\\nWe want to compare a regular Gaussian distribution with variance σ2 to several generalized\\nnormals with the same variance.166\\nThe left-hand plot in Figure 10.2 presents one Gaussian distribution, in blue, together\\nwith three generalized normal distributions with the same variance. All four distributions\\nhave variance σ2 = 1. Two of the generalized distributions are more peaked, and have\\nthicker tails, than the Gaussian. Probability has been redistributed from the middle to the\\ntails, keeping the variance constant. The third generalized distribution is instead thicker\\n'},\n",
       " {'index': 324,\n",
       "  'number': 306,\n",
       "  'content': '306\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\nin the middle and thinner in the tails. It again keeps the variance constant, this time by\\nredistributing probability from the tails to the center. The blue Gaussian distribution sits\\nbetween these extremes.\\nIn the right-hand plot of Figure 10.2, β is called “shape” and varies from 1 to 4, and\\nentropy is plotted on the vertical axis. The generalized normal is perfectly Gaussian where\\nβ = 2, and that’s exactly where entropy is maximized. All of these distributions are symmet-\\nrical, but that doesn’t affect the result. There are other generalized families of distributions\\nthat can be skewed as well, and even then the bell curve has maximum entropy. See the\\nOverthinking box at the bottom of this page, if you want a more satisfying proof.\\nTo appreciate why the Gaussian shape has the biggest entropy for any continuous distri-\\nbution with this variance, consider that entropy increases as we make a distribution flatter.\\nSo we could easily make up a probability distribution with larger entropy than the blue distri-\\nbution in Figure 10.2: Just take probability from the center and put it in the tails. The more\\nuniform the distribution looks, the higher its entropy will be. But there are limits on how\\nmuch of this we can do and maintain the same variance, σ2 = 1. A perfectly uniform dis-\\ntribution would have infinite variance, in fact. So the variance constraint is actually a severe\\nconstraint, forcing the high-probability portion of the distribution to a small area around the\\nmean. Then the Gaussian distribution gets its shape by being as spread out as possible for a\\ndistribution with fixed variance.\\nThe take-home lesson from all of this is that, if all we are willing to assume about a\\ncollection of measurements is that they have a finite variance, then the Gaussian distribution\\nrepresents the most conservative probability distribution to assign to those measurements.\\nBut very often we are comfortable assuming something more. And in those cases, provided\\nour assumptions are good ones, the principle of maximum entropy leads to distributions\\nother than the Gaussian.\\nOverthinking: Proof of Gaussian maximum entropy. Proving that the Gaussian has the largest en-\\ntropy of any distribution with a given variance is easier than you might think. Here’s the shortest\\nproof I know.167 Let p(x) = (2πσ2)−1/2 exp(−(x −µ)2/(2σ2)) stand for the Gaussian probability\\ndensity function. Let q(x) be some other probability density function with the same variance σ2. The\\nmean µ doesn’t matter here, because entropy doesn’t depend upon location, just shape.\\nThe entropy of the Gaussian is H(p) = −\\nR\\np(x) log p(x)dx = 1\\n2 log(2πeσ2). We seek to prove\\nthat no distribution q(x) can have higher entropy than this, provided they have the same variance\\nand are both defined on the entire real number line, from −∞to +∞. We can accomplish this by\\nusing our old friend, from Chapter 7, KL divergence:\\nDKL(q, p) =\\nZ ∞\\n−∞\\nq(x) log\\n\\x12q(x)\\np(x)\\n\\x13\\ndx = −H(q, p) −H(q)\\nH(q) = −\\nR\\nq(x) log q(x)dx is the entropy of q(x) and H(q, p) =\\nR\\nq(x) log p(x)dx is the cross-\\nentropy of the two. Why use DKL here? Because it is always positive (or zero), which guarantees that\\n−H(q, p) ≥H(q). So while we can’t compute H(q), it turns out that we can compute H(q, p). And\\nas you’ll see, that solves the whole problem. So let’s compute H(q, p). It’s defined as:\\nH(q, p) =\\nZ ∞\\n−∞\\nq(x) log p(x)dx =\\nZ ∞\\n−∞\\nq(x) log\\n\\x14\\n(2πσ2)−1/2 exp\\n\\x12\\n−(x −µ)2\\n2σ2\\n\\x13\\x15\\ndx\\nThis will be conceptually easier if we remember that the integral above just takes the average over x.\\nSo we can rewrite the above as:\\nH(q, p) = E log\\n\\x14\\n(2πσ2)−1/2 exp\\n\\x12\\n−(x −µ)2\\n2σ2\\n\\x13\\x15\\n= −1\\n2 log(2πσ2) −\\n1\\n2σ2 E\\n\\x00(x −µ)2\\x01\\n'},\n",
       " {'index': 325,\n",
       "  'number': 307,\n",
       "  'content': '10.1. MAXIMUM ENTROPY\\n307\\nNow the term on the far right is just the average squared deviation from the mean, which is the very\\ndefinition of variance. Since the variance of the unknown function q(x) is constrained to be σ2:\\nH(q, p) = −1\\n2 log(2πσ2) −\\n1\\n2σ2 σ2 = −1\\n2\\n\\x00log(2πσ2) + 1\\n\\x01\\n= −1\\n2 log(2πeσ2)\\nAnd that is exactly −H(p). So since −H(q, p) ≥H(q) by definition, and since H(p) = −H(q, p),\\nit follows that H(p) ≥H(q). The Gaussian has the highest entropy possible for any continuous\\ndistribution with variance σ2.\\n10.1.2. Binomial. Way back in Chapter 2, I introduced Bayesian updating by drawing blue\\nand white marbles from a bag. I showed that the likelihood—the relative plausibility of an\\nobservation—arises from counting the numbers of ways that a given observation could arise,\\naccording to our assumptions. The resulting distribution is known as the binomial distri-\\nbution. If only two things can happen (blue or white marble, for example), and there’s a\\nconstant chance p of each across n trials, then the probability of observing y events of type 1\\nand n −y events of type 2 is:\\nPr(y|n, p) =\\nn!\\ny!(n −y)!py(1 −p)n−y\\nIt may help to note that the fraction with the factorials is just saying how many different\\nordered sequences of n outcomes have a count y. So a more elementary view is that the\\nprobability of any unique sequence of binary events y1 through yn is just:\\nPr(y1, y2, ..., yn|n, p) = py(1 −p)n−y\\nFor the moment, we’ll work with this elementary form, because it will make it easier to ap-\\npreciate the basis for treating all sequences with the same count y as the same outcome.\\nNow we want to demonstrate that this same distribution has the largest entropy of any\\ndistribution that satisfies these constraints: (1) only two unordered events, and (2) constant\\nexpected value. To develop some intuition for the result, let’s explore two examples in which\\nwe fix the expected value. In both examples, we have to assign probability to each possible\\noutcome, while keeping the expected value of the distribution constant. And in both exam-\\nples, the unique distribution that maximizes entropy is the binomial distribution with the\\nsame expected value.\\nHere’s the first example. Suppose again, like in Chapter 2, that we have a bag with an\\nunknown number of blue and white marbles within it. We draw two marbles from the bag,\\nwith replacement. There are therefore four possible sequences: (1) two white marbles, (2)\\none blue and then one white, (3) one white and then one blue, and (4) two blue marbles.\\nOur task is to assign probabilities to each of these possible outcomes. Suppose we know that\\nthe expected number of blue marbles over two draws is exactly 1. This is the expected value\\nconstraint on the distributions we’ll consider.\\nWe seek the distribution with the biggest entropy. Let’s consider four candidate distri-\\nbutions, shown in Figure 10.3. Here are the probabilities that define each distribution:\\nDistribution\\nww\\nbw\\nwb\\nbb\\nA\\n1/4\\n1/4\\n1/4\\n1/4\\nB\\n2/6\\n1/6\\n1/6\\n2/6\\nC\\n1/6\\n2/6\\n2/6\\n1/6\\nD\\n1/8\\n4/8\\n2/8\\n1/8\\n'},\n",
       " {'index': 326,\n",
       "  'number': 308,\n",
       "  'content': '308\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\nww\\nbw\\nwb\\nbb\\nww\\nbw\\nwb\\nbb\\nww\\nbw\\nwb\\nb\\nww\\nbw\\nwb\\nbb\\nww\\nbw\\nwb\\nbb\\nbb\\nbb\\nww\\nbw\\nwb\\nbb\\nA\\nB\\nC\\nD\\nFigure 10.3. Four different distributions\\nwith the same expected value, 1 blue marble\\nin 2 draws. The outcomes on the horizontal\\naxes correspond to 2 white marbles (ww), 1\\nblue and then 1 white (bw), 1 white and then\\n1 blue (wb), and 2 blue marbles (bb).\\nDistribution A is the binomial distribution with n = 2 and p = 0.5. The outcomes bw\\nand wb are usually collapsed into the same outcome type. But in principle they are differ-\\nent outcomes, whether we care about the order of outcomes or not. So the corresponding\\nbinomial probabilities are Pr(ww) = (1 −p)2, Pr(bw) = p(1 −p), Pr(wb) = (1 −p)p, and\\nPr(bb) = p2. Since p = 0.5 in this example, all four probabilities evaluate to 1/4.\\nThe other distributions—B, C, and D—have the same expected value, but none of them\\nis binomial. We can expediently verify this by placing them inside a list and passing each\\nto an expected value formula:\\nR code\\n10.5\\n# build list of the candidate distributions\\np <- list()\\np[[1]] <- c(1/4,1/4,1/4,1/4)\\np[[2]] <- c(2/6,1/6,1/6,2/6)\\np[[3]] <- c(1/6,2/6,2/6,1/6)\\np[[4]] <- c(1/8,4/8,2/8,1/8)\\n# compute expected value of each\\nsapply( p , function(p) sum(p*c(0,1,1,2)) )\\n[1] 1 1 1 1\\nAnd likewise we can quickly compute the entropy of each distribution:\\nR code\\n10.6\\n# compute entropy of each distribution\\nsapply( p , function(p) -sum( p*log(p) ) )\\n[1] 1.386294 1.329661 1.329661 1.213008\\nDistribution A, the binomial distribution, has the largest entropy among the four. To ap-\\npreciate why, consider that information entropy increases as a probability distribution be-\\ncomes more even. Distribution A is a flat line, as you can see in Figure 10.3. It can’t be\\nmade any more even, and each of the other distributions is clearly less even. That’s why\\nthey have smaller entropies. And since distribution A is consistent with the constraint that\\nthe expected value be 1, it follows that distribution A, which is binomial, has the maximum\\nentropy of any distribution with these constraints.\\n'},\n",
       " {'index': 327,\n",
       "  'number': 309,\n",
       "  'content': '10.1. MAXIMUM ENTROPY\\n309\\nThis example is too special to demonstrate the general case, however. It’s special because\\nwhen the expected value is 1, the distribution over outcomes can be flat and remain consistent\\nwith the constraint. But what about when the expected value constraint is not 1? Suppose for\\nour second example that the expected value must be instead 1.4 blue marbles in two draws.\\nThis corresponds to p = 0.7. So you can think of this as 7 blue marbles and 3 white marbles\\nhidden inside the bag. The binomial distribution with this expected value is:\\nR code\\n10.7\\np <- 0.7\\n( A <- c( (1-p)^2 , p*(1-p) , (1-p)*p , p^2 ) )\\n[1] 0.09 0.21 0.21 0.49\\nThis distribution is definitely not flat. So to appreciate how this distribution has maximum\\nentropy—is the flattest distribution with expected value 1.4—we’ll simulate a bunch of dis-\\ntributions with the same expected value and then compare entropies. The entropy of the\\ndistribution above is just:\\nR code\\n10.8\\n-sum( A*log(A) )\\n[1] 1.221729\\nSo if we randomly generate thousands of distributions with expected value 1.4, we expect\\nthat none will have a larger entropy than this.\\nWe can use a short R function to simulate random probability distributions that have any\\nspecified expected value. The code below will do the job. Don’t worry about how it works\\n(unless you want to168).\\nR code\\n10.9\\nsim.p <- function(G=1.4) {\\nx123 <- runif(3)\\nx4 <- ( (G)*sum(x123)-x123[2]-x123[3] )/(2-G)\\nz <- sum( c(x123,x4) )\\np <- c( x123 , x4 )/z\\nlist( H=-sum( p*log(p) ) , p=p )\\n}\\nThis function generates a random distribution with expected value G and then returns its\\nentropy along with the distribution. We want to invoke this function a large number of times.\\nHere is how to call it 100000 times and then plot the distribution of resulting entropies:\\nR code\\n10.10\\nH <- replicate( 1e5 , sim.p(1.4) )\\ndens( as.numeric(H[1,]) , adj=0.1 )\\nThe list H now holds 100,000 distributions and their entropies. The distribution of entropies\\nis shown in the left-hand plot in Figure 10.4. The letters A, B, C, and D mark different\\nexample entropies. The distributions corresponding to each are shown in the right-hand\\npart of the figure. The distribution A with the largest observed entropy is nearly identical to\\nthe binomial we calculated earlier. And its entropy is nearly identical as well.\\nYou don’t have to take my word for it. Let’s split out the entropies and distributions, so\\nthat it’s easier to work with them:\\n'},\n",
       " {'index': 328,\n",
       "  'number': 310,\n",
       "  'content': '310\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\nww\\nbw\\nwb\\nbb\\nww\\nbw\\nwb\\nbb\\nww\\nbw\\nwb\\nww\\nbw\\nwb\\nbb\\nww\\nbw\\nwb\\nbb\\nbb\\nbb\\nww\\nbw\\nwb\\nbb\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2\\n0\\n2\\n4\\n6\\n8\\nEntropy\\nDensity\\nA\\nB\\nC\\nD\\nA\\nB\\nC\\nD\\nFigure 10.4. Left: Distribution of entropies from randomly simulated dis-\\ntributions with expected value 1.4. The letters A, B, C, and D mark the en-\\ntropies of individual distributions shown on the right. Right: Individual\\nprobability distributions. As entropy decreases, going from A to D, the dis-\\ntribution becomes more uneven. The distribution marked A is the binomial\\ndistribution with np = 1.4.\\nR code\\n10.11\\nentropies <- as.numeric(H[1,])\\ndistributions <- H[2,]\\nNow we can ask what the largest observed entropy was:\\nR code\\n10.12\\nmax(entropies)\\n[1] 1.221728\\nThat value is nearly identical to the entropy of the binomial distribution we calculated before.\\nAnd the distribution with that entropy is:\\nR code\\n10.13\\ndistributions[ which.max(entropies) ]\\n[[1]]\\n[1] 0.08981599 0.21043116 0.20993686 0.48981599\\nAnd that’s almost exactly {0.09, 0.21, 0.21, 0.49}, the distribution we calculated earlier.\\nThe other distributions in Figure 10.4—B, C, and D—are all less even than A. They\\ndemonstrate how as entropy declines the probability distributions become progressively less\\neven. All four of these distributions really do have expected value 1.4. But among the infinite\\ndistributions that satisfy this constraint, it is only the most even distribution, the exact one\\nnominated by the binomial distribution, that has greatest entropy.\\nSo what? There are a few conceptual lessons to take away from this example. First, hope-\\nfully it reinforces the maximum entropy nature of the binomial distribution. When only\\ntwo un-ordered outcomes are possible—such as blue and white marbles—and the expected\\n'},\n",
       " {'index': 329,\n",
       "  'number': 311,\n",
       "  'content': '10.1. MAXIMUM ENTROPY\\n311\\nnumbers of each type of event are assumed to be constant, then the distribution that is most\\nconsistent with these constraints is the binomial distribution. This distribution spreads prob-\\nability out as evenly and conservatively as possible.\\nSecond, of course usually we do not know the expected value, but wish to estimate it. But\\nthis is actually the same problem, because assuming the distribution has a constant expected\\nvalue leads to the binomial distribution as well, but with unknown expected value np, which\\nmust be estimated from the data. (You’ll learn how to do this in Chapter 11.) If only two\\nun-ordered outcomes are possible and you think the process generating them is invariant in\\ntime—so that the expected value remains constant at each combination of predictor values—\\nthen the distribution that is most conservative is the binomial. This is analogous to how the\\nGaussian distribution is the most conservative distribution for a continuous outcome vari-\\nable with finite variance. Variables with different constraints get different maximum entropy\\ndistributions, but the underlying principle remains the same.\\nThird, back in Chapter 2, we derived the binomial distribution just by counting how\\nmany paths through the garden of forking data were consistent with our assumptions. For\\neach possible composition of the bag of marbles—which corresponds here to each possible\\nexpected value—there is a unique number of ways to realize any possible sequence of data.\\nThe likelihoods derived in that way turn out to be exactly the same as the likelihoods we get\\nby maximizing entropy. This is not a coincidence. Entropy counts up the number of different\\nways a process can produce a particular result, according to our assumptions. The garden of\\nforking data did only the same thing—count up the numbers of ways a sequence could arise,\\ngiven assumptions.\\nEntropy maximization, like so much in probability theory, is really just counting. But\\nit’s abbreviated counting that allows us to generalize lessons learned in one context to new\\nproblems in new contexts. Instead of having to tediously draw out a garden of forking data,\\nwe can instead map constraints on an outcome to a probability distribution. There is no guar-\\nantee that this is the best probability distribution for the real problem you are analyzing. But\\nthere is a guarantee that no other distribution more conservatively reflects your assumptions.\\nThat’s not everything, but nor is it nothing. Any other distribution implies hidden con-\\nstraints that are unknown to us, reflecting phantom assumptions. A full and honest account-\\ning of assumptions is helpful, because it aids in understanding how a model misbehaves.\\nAnd since all models misbehave sometimes, it’s good to be able to anticipate those times\\nbefore they happen, as well as to learn from those times when they inevitably do.\\nRethinking: Conditional independence. All this talk of constant expected value brings up an im-\\nportant question: Do these distributions necessarily assume that each observation is uncorrelated\\nwith every other observation? Not really. What is usually meant by “independence” in a probability\\ndistribution is just that each observation is uncorrelated with the others, once we know the corre-\\nsponding predictor values. This is usually known as conditional independence, the claim that\\nobservations are independent after accounting for differences in predictors, through the model. It’s\\na modeling assumption. What this assumption doesn’t cover is a situation in which an observed\\nevent directly causes the next observed event. For example, if you buy the next Nick Cave album\\nbecause I buy the next Nick Cave album, then your behavior is not independent of mine, even after\\nconditioning on the fact that we both like that sort of music.\\n'},\n",
       " {'index': 330,\n",
       "  'number': 312,\n",
       "  'content': '312\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\nOverthinking: Binomial maximum entropy. The usual way to derive a maximum entropy distribu-\\ntion is to state the constraints and then use a mathematical device called the Lagrangian to solve for\\nthe probability assignments that maximize entropy. But instead we’ll extend the strategy used in the\\nOverthinking box on page 306. As a bonus, this strategy will allow us to derive the constraints that\\nare necessary for a distribution, in this case the binomial, to be a maximum entropy distribution.\\nLet p be the binomial distribution, and let pi be the probability of a sequence of observations i\\nwith number of successes xi and number of failures n −xi. Let q be some other discrete distribution\\ndefined over the same set of observable sequences. As before, KL divergence tells us that:\\n−H(q, p) ≥H(q) =⇒−\\nX\\ni\\nqi log pi ≥−\\nX\\ni\\nqi log qi\\nWhat we’re going to do now is work with H(q, p) and simplify it until we can isolate the constraint\\nthat defines the class of distributions for which p has maximum entropy. Let λ = P\\ni pixi be the\\nexpected value of p. Then from the definition of H(q, p):\\n−H(q, p) = −\\nX\\ni\\nqi log\\n\"\\x12λ\\nn\\n\\x13xi \\x12\\n1 −λ\\nn\\n\\x13n−xi#\\n= −\\nX\\ni\\nqi\\n\\x12\\nxi log\\n\\x14λ\\nn\\n\\x15\\n+ (n −xi) log\\n\\x14\\n1 −λ\\nn\\n\\x15\\x13\\nAfter some algebra:\\n−H(q, p) = −\\nX\\ni\\nqi\\n\\x12\\nxi log\\n\\x14\\nλ\\nn −λ\\n\\x15\\n+ n log\\n\\x14n −λ\\nn\\n\\x15\\x13\\n= −n log\\n\\x14n −λ\\nn\\n\\x15\\n−log\\n\\x14\\nλ\\nn −λ\\n\\x15 X\\ni\\nqixi\\n| {z }\\n¯q\\nThe term on the far right labeled ¯q is the expected value of the distribution q. If we knew it, we could\\ncomplete the calculation, because no other term depends upon qi. This means that expected value is\\nthe constraint that defines the class of distributions for which the binomial p has maximum entropy.\\nIf we now set the expected value of q equal to λ, then H(q) = H(p). For any other expected value of\\nq, H(p) > H(q).\\nFinally, notice the term log[λ/(n −λ)]. This term is the log of the ratio of the expected number\\nof successes to the expected number of failures. That ratio is the “odds” of a success, and its logarithm\\nis called “log odds.” This quantity will feature prominently in models we construct from the binomial\\ndistribution, in Chapter 11.\\n10.2. Generalized linear models\\nThe Gaussian models of previous chapters worked by first assuming a Gaussian distribu-\\ntion over outcomes. Then, we replaced the parameter that defines the mean of that distribu-\\ntion, µ, with a linear model. This resulted in likelihood definitions of the sort:\\nyi ∼Normal(µi, σ)\\nµi = α + βxi\\nFor an outcome variable that is continuous and far from any theoretical maximum or mini-\\nmum, this sort of Gaussian model has maximum entropy.\\nBut when the outcome variable is either discrete or bounded, a Gaussian likelihood is\\nnot the most powerful choice. Consider for example a count outcome, such as the number\\nof blue marbles pulled from a bag. Such a variable is constrained to be zero or a positive\\ninteger. Using a Gaussian model with such a variable won’t result in a terrifying explosion.\\nBut it can’t be trusted to do much more than estimate the average count. It certainly can’t\\nbe trusted to produce sensible predictions, because while you and I know that counts can’t\\n'},\n",
       " {'index': 331,\n",
       "  'number': 313,\n",
       "  'content': '10.2. GENERALIZED LINEAR MODELS\\n313\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n0.0\\n0.5\\n1.0\\nx\\nprobability\\nFigure 10.5. Why we need link functions.\\nThe solid blue line is a linear model of a prob-\\nability mass. It increases linearly with a pre-\\ndictor, x, on the horizontal axis. But when it\\nreaches the maximum probability mass of 1, at\\nthe dashed boundary, it will happily continue\\nupwards, as shown by the dashed blue line. In\\nreality, further increases in x could not further\\nincrease probability, as indicated by the hori-\\nzontal continuation of the solid trend.\\nbe negative, a linear regression model does not. So it would happily predict negative values,\\nwhenever the mean count is close to zero.\\nLuckily, it’s easy to do better. By using all of our prior knowledge about the outcome\\nvariable, usually in the form of constraints on the possible values it can take, we can appeal\\nto maximum entropy for the choice of distribution. Then all we have to do is generalize the\\nlinear regression strategy—replace a parameter describing the shape of the likelihood with a\\nlinear model—to probability distributions other than the Gaussian.\\nThis is the essence of a generalized linear model.169 And it results in models that\\nlook like this:\\nyi ∼Binomial(n, pi)\\nf (pi) = α + β(xi −¯x)\\nThere are only two changes here from the familiar Gaussian model. The first is principled—\\nthe principle of maximum entropy. The second is an epicycle—a modeling trick that works\\ndescriptively but not causally—but a quite successful one. I’ll briefly explain each, before\\nmoving on in the remainder of the section to describe all of the most common distributions\\nused to construct generalized linear models. Later chapters show you how to implement\\nthem.\\nFirst, the likelihood is binomial instead of Gaussian. For a count outcome y for which\\neach observation arises from n trials and with constant expected value np, the binomial dis-\\ntribution has maximum entropy. So it’s the least informative distribution that satisfies our\\nprior knowledge of the outcomes y. If the outcome variable had different constraints, it could\\nbe a different maximum entropy distribution.\\nSecond, there is now a funny little f at the start of the second line of the model. This\\nrepresents a link function, to be determined separately from the choice of distribution.\\nGeneralized linear models need a link function, because rarely is there a “µ”, a parameter\\ndescribing the average outcome, and rarely are parameters unbounded in both directions,\\nlike µ is. For example, the shape of the binomial distribution is determined, like the Gaussian,\\nby two parameters. But unlike the Gaussian, neither of these parameters is the mean. Instead,\\nthe mean outcome is np, which is a function of both parameters. Since n is usually known\\n(but not always), it is most common to attach a linear model to the unknown part, p. But p is\\n'},\n",
       " {'index': 332,\n",
       "  'number': 314,\n",
       "  'content': '314\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\na probability mass, so pi must lie between zero and one. But there’s nothing to stop the linear\\nmodel α+βxi from falling below zero or exceeding one. Figure 10.5 plots an example. The\\nlink function f provides a solution to this common problem. This chapter will introduce the\\ntwo most common link functions. You’ll see how to use them in the chapters that follow.\\nRethinking: The scourge of Histomancy. One strategy for choosing an outcome distribution is to\\nplot the histogram of the outcome variable and, by gazing into its soul, decide what sort of distribution\\nfunction to use. Call this strategy Histomancy, the ancient art of divining likelihood functions from\\nempirical histograms. This sorcery is used, for example, when testing for normality before deciding\\nwhether or not to use a non-parametric procedure. Histomancy is a false god, because even perfectly\\ngood Gaussian variables may not look Gaussian when displayed as a histogram. Why? Because at\\nmost what a Gaussian likelihood assumes is not that the aggregated data look Gaussian, but rather that\\nthe residuals, after fitting the model, look Gaussian. So for example the combined histogram of male\\nand female body weights is certainly not Gaussian. But it is (approximately) a mixture of Gaussian\\ndistributions. So after conditioning on sex, the residuals may be quite normal. Other times, people\\ndecide not to use a Poisson model, because the variance of the aggregate outcome exceeds its mean\\n(see Chapter 11). But again, at most what a Poisson likelihood assumes is that the variance equals the\\nmean after conditioning on predictors. It may very well be that a Gaussian or Poisson likelihood is\\na poor assumption in any particular context. But this can’t easily be decided via Histomancy. This is\\nwhy we need principles, whether maximum entropy or otherwise.\\n10.2.1. Meet the family. The most common distributions used in statistical modeling are\\nmembers of a family known as the exponential family. Every member of this family is\\na maximum entropy distribution, for some set of constraints. And conveniently, just about\\nevery other statistical modeling tradition employs the exact same distributions, even though\\nthey arrive at them via justifications other than maximum entropy.\\nFigure 10.6 illustrates the representative shapes of the most common exponential fam-\\nily distributions used in GLMs. The horizontal axis in each plot represents values of a vari-\\nable, and the vertical axis represents probability density (for the continuous distributions)\\nor probability mass (for the discrete distributions). For each distribution, the figure also\\nprovides the notation (above each density plot) and the name of R’s corresponding built-in\\ndistribution function (below each density plot). The gray arrows in Figure 10.6 indicate\\nsome of the ways that these distributions are dynamically related to one another. These rela-\\ntionships arise from generative processes that can convert one distribution to another. You\\ndo not need to know these relationships in order to successfully use these distributions in\\nyour modeling. But the generative relationships do help to demystify these distributions, by\\ntying them to causation and measurement.\\nTwo of these distributions, the Gaussian and binomial, are already familiar to you. To-\\ngether, they comprise the most commonly used outcome distributions in applied statistics,\\nthrough the procedures of linear regression (Chapter 4) and logistic regression (Chapter 11).\\nThere are also three new distributions that deserve some commentary.\\nThe exponential distribution (center) is constrained to be zero or positive. It is\\na fundamental distribution of distance and duration, kinds of measurements that represent\\ndisplacement from some point of reference, either in time or space. If the probability of\\nan event is constant in time or across space, then the distribution of events tends towards\\nexponential. The exponential distribution has maximum entropy among all non-negative\\ncontinuous distributions with the same average displacement. Its shape is described by a\\n'},\n",
       " {'index': 333,\n",
       "  'number': 315,\n",
       "  'content': '10.2. GENERALIZED LINEAR MODELS\\n315\\nFigure 10.6. Some of the exponential family distributions, their notation,\\nand some of their relationships. Center: exponential distribution. Clock-\\nwise, from top-left: gamma, normal (Gaussian), binomial and Poisson dis-\\ntributions.\\nsingle parameter, the rate of events λ, or the average displacement λ−1. This distribution is\\nthe core of survival and event history analysis, which is not covered in this book.\\nThe gamma distribution (top-left) is also constrained to be zero or positive. It too\\nis a fundamental distribution of distance and duration. But unlike the exponential distribu-\\ntion, the gamma distribution can have a peak above zero. If an event can only happen after\\ntwo or more exponentially distributed events happen, the resulting waiting times will be\\ngamma distributed. For example, age of cancer onset is approximately gamma distributed,\\nsince multiple events are necessary for onset.170 The gamma distribution has maximum en-\\ntropy among all distributions with the same mean and same average logarithm. Its shape\\nis described by two parameters, but there are at least three different common descriptions\\nof these parameters, so some care is required when working with it. The gamma distribu-\\ntion is common in survival and event history analysis, as well as some contexts in which a\\ncontinuous measurement is constrained to be positive.\\nThe Poisson distribution (bottom-left) is a count distribution like the binomial. It\\nis actually a special case of the binomial, mathematically. If the number of trials n is very\\nlarge (and usually unknown) and the probability of a success p is very small, then a binomial\\ndistribution converges to a Poisson distribution with an expected rate of events per unit time\\nof λ = np. Practically, the Poisson distribution is used for counts that never get close to any\\n'},\n",
       " {'index': 334,\n",
       "  'number': 316,\n",
       "  'content': '316\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\ntheoretical maximum. As a special case of the binomial, it has maximum entropy under\\nexactly the same constraints. Its shape is described by a single parameter, the rate of events\\nλ. Poisson GLMs are detailed in the next chapter.\\nThere are many other exponential family distributions, and many of them are useful.\\nBut don’t worry that you need to memorize them all. You can pick up new distributions,\\nand the sorts of generative processes they correspond to, as needed. It’s also not important\\nthat an outcome distribution be a member of the exponential family—if you think you have\\ngood reasons to use some other distribution, then use it. But you should also check its per-\\nformance, just like you would any modeling assumption.\\nRethinking: A likelihood is a prior. In traditional statistics, likelihood functions are “objective” and\\nprior distributions “subjective.” In Bayesian statistics, likelihoods are deeply related to prior probabil-\\nity distributions: They are priors for the data, conditional on the parameters. And just like with other\\npriors, there is no correct likelihood. But there are better and worse likelihoods, depending upon\\nthe context. Useful inference does not require that the data (or residuals) be actually distributed ac-\\ncording to the likelihood anymore than it requires the posterior distribution to be like the prior. The\\nduality between likelihoods and priors will become quite explicit in Chapter 15.\\n10.2.2. Linking linear models to distributions. To build a regression model from any of\\nthe exponential family distributions is just a matter of attaching one or more linear mod-\\nels to one or more of the parameters that describe the distribution’s shape. But as hinted at\\nearlier, usually we require a link function to prevent mathematical accidents like nega-\\ntive distances or probability masses that exceed 1. So for any outcome distribution, say for\\nexample the exotic “Zaphod” distribution,171 we write:\\nyi ∼Zaphod(θi, ϕ)\\nf (θi) = α + β(xi −¯x)\\nwhere f is a link function.\\nBut what function should f be? A link function’s job is to map the linear space of a model\\nlike α + β(xi −¯x) onto the non-linear space of a parameter like θ. So f is chosen with that\\ngoal in mind. Most of the time, for most GLMs, you can use one of two exceedingly common\\nlinks, a logit link or a log link. Let’s introduce each, and you’ll work with both in later chapters.\\nThe logit link maps a parameter that is defined as a probability mass, and therefore\\nconstrained to lie between zero and one, onto a linear model that can take on any real value.\\nThis link is extremely common when working with binomial GLMs. In the context of a\\nmodel definition, it looks like this:\\nyi ∼Binomial(n, pi)\\nlogit(pi) = α + βxi\\nAnd the logit function itself is defined as the log-odds:\\nlogit(pi) = log\\npi\\n1 −pi\\nThe “odds” of an event are just the probability it happens divided by the probability it does\\nnot happen. So really all that is being stated here is:\\nlog\\npi\\n1 −pi\\n= α + βxi\\n'},\n",
       " {'index': 335,\n",
       "  'number': 317,\n",
       "  'content': '10.2. GENERALIZED LINEAR MODELS\\n317\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n-4\\n-2\\n0\\n2\\n4\\nx\\nlog-odds\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\nx\\n0.0\\n0.5\\n1.0\\nprobability\\nFigure 10.7. The logit link transforms a linear model (left) into a proba-\\nbility (right). This transformation compresses the geometry far from zero,\\nsuch that a unit change on the linear scale (left) means less and less change\\non the probability scale (right).\\nSo to figure out the definition of pi implied here, just do a little algebra and solve the above\\nequation for pi:\\npi =\\nexp(α + βxi)\\n1 + exp(α + βxi)\\nThe above function is usually called the logistic. In this context, it is also commonly called\\nthe inverse-logit, because it inverts the logit transform.\\nWhat all of this means is that when you use a logit link for a parameter, you are defining\\nthe parameter’s value to be the logistic transform of the linear model. Figure 10.7 illustrates\\nthe transformation that takes place when using a logit link. On the left, the geometry of the\\nlinear model is shown, with horizontal lines indicating unit changes in the value of the lin-\\near model as the value of a predictor x changes. This is the log-odds space, which extends\\ncontinuously in both positive and negative directions. On the right, the linear space is trans-\\nformed and is now constrained entirely between zero and one. The horizontal lines have\\nbeen compressed near the boundaries, in order to make the linear space fit within the proba-\\nbility space. This compression produces the characteristic logistic shape of the transformed\\nlinear model shown in the right-hand plot.\\nThis compression does affect interpretation of parameter estimates, because no longer\\ndoes a unit change in a predictor variable produce a constant change in the mean of the\\noutcome variable. Instead, a unit change in xi may produce a larger or smaller change in\\nthe probability pi, depending upon how far from zero the log-odds are. For example, in\\nFigure 10.7, when x = 0 the linear model has a value of zero on the log-odds scale. A half-\\nunit increase in x results in about a 0.25 increase in probability. But each addition half-unit\\nwill produce less and less of an increase in probability, until any increase is vanishingly small.\\nAnd if you think about it, a good model of probability needs to behave this way. When an\\n'},\n",
       " {'index': 336,\n",
       "  'number': 318,\n",
       "  'content': '318\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n-3\\n-2\\n-1\\n0\\n1\\n2\\n3\\nx\\nlog measurement\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\nx\\n0\\n2\\n4\\n6\\n8\\n10\\noriginal measurement\\nFigure 10.8. The log link transforms a linear model (left) into a strictly pos-\\nitive measurement (right). This transform results in an exponential scaling\\nof the linear model, with a unit change on the linear scale mapping onto\\nincreasingly larger changes on the outcome scale.\\nevent is almost guaranteed to happen, its probability cannot increase very much, no matter\\nhow important the predictor may be.\\nYou’ll find examples of this compression phenomenon in later chapters. The key lesson\\nfor now is just that no regression coefficient, such as β, from a GLM ever produces a constant\\nchange on the outcome scale. Recall that we defined interaction (Chapter 8) as a situation\\nin which the effect of a predictor depends upon the value of another predictor. Well now\\nevery predictor essentially interacts with itself, because the impact of a change in a predictor\\ndepends upon the value of the predictor before the change. More generally, every predic-\\ntor variable effectively interacts with every other predictor variable, whether you explicitly\\nmodel them as interactions or not. This fact makes the visualization of counter-factual pre-\\ndictions even more important for understanding what the model is telling you.\\nThe second very common link function is the log link. This link function maps a\\nparameter that is defined over only positive real values onto a linear model. For example,\\nsuppose we want to model the standard deviation σ of a Gaussian distribution so it is a\\nfunction of a predictor variable x. The parameter σ must be positive, because a standard\\ndeviation cannot be negative nor can it be zero. The model might look like:\\nyi ∼Normal(µ, σi)\\nlog(σi) = α + βxi\\nIn this model, the mean µ is constant, but the standard deviation scales with the value xi.\\nA log link is both conventional and useful in this situation. It prevents σ from taking on a\\nnegative value.\\nWhat the log link effectively assumes is that the parameter’s value is the exponentiation\\nof the linear model. Solving log(σi) = α + βxi for σi yields the inverse link:\\nσi = exp(α + βxi)\\n'},\n",
       " {'index': 337,\n",
       "  'number': 319,\n",
       "  'content': '10.2. GENERALIZED LINEAR MODELS\\n319\\nThe impact of this assumption can be seen in Figure 10.8. Using a log link for a linear\\nmodel (left) implies an exponential scaling of the outcome with the predictor variable (right).\\nAnother way to think of this relationship is to remember that logarithms are magnitudes. An\\nincrease of one unit on the log scale means an increase of an order of magnitude on the un-\\ntransformed scale. And this fact is reflected in the widening intervals between the horizontal\\nlines in the right-hand plot of Figure 10.8.\\nWhile using a log link does solve the problem of constraining the parameter to be posi-\\ntive, it may also create a problem when the model is asked to predict well outside the range\\nof data used to fit it. Exponential relationships grow, well, exponentially. Just like a lin-\\near model cannot be linear forever, an exponential model cannot be exponential forever.\\nHuman height cannot be linearly related to weight forever, because very heavy people stop\\ngetting taller and start getting wider. Likewise, the property damage caused by a hurricane\\nmay be approximately exponentially related to wind speed for smaller storms. But for very\\nbig storms, damage may be capped by the fact that everything gets destroyed.\\nRethinking: When in doubt, play with assumptions. Link functions are assumptions. And like all\\nassumptions, they are useful in different contexts. The conventional logit and log links are widely\\nuseful, but they can sometimes distort inference. If you ever have doubts, and want to reassure your-\\nself that your conclusions are not sensitive to choice of link function, then you can use sensitivity\\nanalysis. A sensitivity analysis explores how changes in assumptions influence inference. If none\\nof the alternative assumptions you consider have much impact on inference, that’s worth reporting.\\nLikewise, if the alternatives you consider do have an important impact on inference, that’s also worth\\nreporting. The same sort of advice follows for other modeling assumptions: likelihoods, linear mod-\\nels, priors, and even how the model is fit to data.\\nSome people are nervous about sensitivity analysis, because it feels like fishing for results, or “p-\\nhacking.”172 The goal of sensitivity analysis is really the opposite of p-hacking. In p-hacking, many\\njustifiable analyses are tried, and the one that attains statistical significance is reported. In sensitivity\\nanalysis, many justifiable analyses are tried, and all of them are described.\\nOverthinking: Parameters interacting with themselves. We can find some clarity on how GLMs\\nforce every predictor variable to interact with itself by deriving the rate of change in the outcome\\nfor a given change in the value of the predictor. In a classic Gaussian model the mean is modeled as\\nµ = α + βx. So the rate of change in µ with respect to x is just ∂µ/∂x = β. And that’s constant.\\nIt doesn’t matter what value x has. Now consider the rate of change in a binomial probability p with\\nrespect to x. The probability p is defined by:\\np =\\nexp(α + βx)\\n1 + exp(α + βx)\\nAnd now taking the derivative with respect to x yields:\\n∂p\\n∂x =\\nβ\\n2\\n\\x001 + cosh(α + βx)\\n\\x01\\nSince x appears in this answer, the impact of a change in x depends upon x. That’s an interaction with\\nitself. The rate of change in the odds is a little nicer:\\n∂p/(1 −p)\\n∂x\\n= β exp(α + βx)\\nbut it still contains the entire linear model. Sometimes people avoid non-linear models because\\nthey don’t like having to interpret non-linear effects. But if the actual phenomenon contains non-\\nlinearities, this solves only a small world problem.\\n'},\n",
       " {'index': 338,\n",
       "  'number': 320,\n",
       "  'content': '320\\n10. BIG ENTROPY AND THE GENERALIZED LINEAR MODEL\\n10.2.3. Omitted variable bias again. Back in Chapters 5 and 6, you saw some examples of\\nomitted variable bias, where leaving a causally important variable out of a model leads\\nto biased inference. The same thing can of course happen in GLMs. But it can be worse in\\nGLMs, because even a variable that isn’t technically a confounder can bias inference, once\\nwe have a link function. The reason is that the ceiling and floor effects described above can\\ndistort estimates by suppressing the causal influence of a variable.\\nSuppose for example that two variables X and Z independently influence a binary out-\\ncome Y. If either X and Z is large enough, then Y = 1. Both variables are sufficient causes\\nof Y. Now if we don’t measure Z but only X, we might consistently underestimate the causal\\neffect of X. Why? Because Z is sufficient for Y to equal 1, and we didn’t measure Z. So there\\nare cases in the data where X is small but Y = 1. These cases imply X does not influence Y\\nvery strongly, but only because we are ignoring Z. This phenomenon doesn’t occur in ordi-\\nnary linear regression, because independent causes just contribute to the mean. There are\\nno ceiling or floor effects (in theory).\\nThere is no avoiding this problem. Falling back on a linear, rather than generalized linear,\\nmodel won’t change the reality of omitted variable bias. It will just statistically disguise it.\\nThat may be a good publication strategy, but it’s not a good inferential strategy.\\n10.2.4. Absolute and relative differences. There is an important practical consequence of\\nthe way that a link function compresses and expands different portions of the linear model’s\\nrange: Parameter estimates do not by themselves tell you the importance of a predictor on\\nthe outcome. The reason is that each parameter represents a relative difference on the scale\\nof the linear model, ignoring other parameters, while we are really interested in absolute\\ndifferences in outcomes that must incorporate all parameters.\\nThis point will come up again in the context of data examples in later chapters, when it\\nwill be easier to illustrate its importance. For now, just keep in mind that a big beta-coefficient\\nmay not correspond to a big effect on the outcome.\\n10.2.5. GLMs and information criteria. What you learned in Chapter 7 about information\\ncriteria and regularizing priors applies also to GLMs. But with all these new outcome distri-\\nbutions at your command, it is tempting to use information criteria to compare models with\\ndifferent likelihood functions. Is a Gaussian or binomial better? Can’t we just let WAIC or\\ncross-validation sort it out?\\nUnfortunately, WAIC (or any other predictive criterion) cannot sort it out. The problem\\nis that deviance is part normalizing constant. The constant affects the absolute magnitude\\nof the deviance, but it doesn’t affect fit to data. Since information criteria are all based on\\ndeviance, their magnitude also depends upon these constants. That is fine, as long as all\\nof the models you compare use the same outcome distribution type—Gaussian, binomial,\\nexponential, gamma, Poisson, or another. In that case, the constants subtract out when you\\ncompare models by their differences. But if two models have different outcome distributions,\\nthe constants don’t subtract out, and you can be misled by a difference in AIC/WAIC/PSIS.\\nReally all you have to remember is to only compare models that all use the same type of\\nlikelihood. Of course it is possible to compare models that use different likelihoods, just not\\nwith information criteria. Luckily, the principle of maximum entropy ordinarily motivates\\nan easy choice of likelihood, at least for ordinary regression models. So there is no need to\\nlean on model comparison for this modeling choice.\\nThere are a few nuances with WAIC/PSIS and individual GLM types. These nuances will\\narise as examples of each GLM are worked, in later chapters.\\n'},\n",
       " {'index': 339,\n",
       "  'number': 321,\n",
       "  'content': '10.4. SUMMARY\\n321\\n10.3. Maximum entropy priors\\nThe principle of maximum entropy helps us to make modeling choices. When pressed\\nto choose an outcome distribution—a likelihood—maximum entropy nominates the least\\ninformative distribution consistent with the constraints on the outcome variable. Applying\\nthe principle in this way leads to many of the same distributional choices that are commonly\\nregarded as just convenient assumptions or useful conventions.\\nAnother way that the principle of maximum entropy helps with choosing distributions\\narises when choosing priors. GLMs are easy to use with conventional weakly informative\\npriors of the sort you’ve been using up to this point in the book. Such priors are nice, be-\\ncause they allow the data to dominate inference while also taming some of the pathologies\\nof unconstrained estimation. There were some examples of their “soft power” in Chapter 9.\\nBut sometimes, rarely, some of the parameters in a GLM refer to things we might actually\\nhave background information about. When that’s true, maximum entropy provides a way to\\ngenerate a prior that embodies the background information, while assuming as little else as\\npossible. This makes them appealing, conservative choices.\\nWe won’t be using maximum entropy to choose priors in this book, but when you come\\nacross an analysis that does, you can interpret the principle in the same way as you do with\\nlikelihoods and understand the approach as an attempt to include relevant background in-\\nformation about parameters, while introducing no other assumptions by accident.\\n10.4. Summary\\nThis chapter has been a conceptual, not practical, introduction to maximum entropy\\nand generalized linear models. The principle of maximum entropy provides an empirically\\nsuccessful way to choose likelihood functions. Information entropy is essentially a measure\\nof the number of ways a distribution can arise, according to stated assumptions. By choos-\\ning the distribution with the biggest information entropy, we thereby choose a distribution\\nthat obeys the constraints on outcome variables, without importing additional assumptions.\\nGeneralized linear models arise naturally from this approach, as extensions of the linear mod-\\nels in previous chapters. The necessity of choosing a link function to bind the linear model\\nto the generalized outcome introduces new complexities in model specification, estimation,\\nand interpretation. You’ll become comfortable with these complexities through examples in\\nlater chapters.\\n'},\n",
       " {'index': 340, 'number': 322, 'content': ''},\n",
       " {'index': 341,\n",
       "  'number': 323,\n",
       "  'content': '11 God Spiked the Integers\\nThe cold of space is named Kelvin, about 3 degrees Kelvin, or 3 degrees centigrade above\\nabsolute zero. Kelvin is also the name of a river in Scotland, near Glasgow. The same river\\ngave its name to William Thomson, the Lord Kelvin (1824–1907), the first scientist in the\\nUnited Kingdom to be granted a noble title. Thomson studied thermodynamics in his labo-\\nratory in Glasgow, and now the cold of space bears the name of a Scottish river.\\nLord Kelvin befittingly also researched water. He invented several tide prediction en-\\ngines. These were essentially mechanical computers that calculated the tides (Figure 11.1).\\nAll the gears and cables comprised a set of oscillators that produced accurate tide predic-\\ntions. But when you look at such a machine, most of it is internal states, not the predictions.\\nIt would be quite hard to inspect any one of the gears at the bottom and know when to expect\\nthe tide, because the predictions emerge from the combination of internal states.\\nGeneralized linear models (GLMs) are a lot like these early mechanical computers.\\nThe moving pieces within them, the parameters, interact to produce non-obvious predic-\\ntions. But we can’t read the parameters directly to understand the predictions. This is quite\\ndifferent than the Gaussian linear models of previous chapters, where individual parameters\\nhad clear meanings on the prediction scale. Mastering GLMs requires a little more attention.\\nThey are always confusing, when you first try to grasp how they operate.\\nThe most common and useful generalized linear models are models for counts. Counts\\nare non-negative integers—0, 1, 2, and so on. They are the basis of all mathematics, the first\\nbits that children learn. But they are also intoxicatingly complicated to model—hence the\\napocryphal slogan that titles this chapter173. The essential problem is this: When what we\\nwish to predict is a count, the scale of the parameters is never the same as the scale of the\\noutcome. A count golem, like a tide prediction engine, has a whirring machinery underneath\\nthat doesn’t resemble the output. Keeping the tide engine in mind, you can master these\\nmodels and use them responsibly.\\nWe will engineer complete examples of the two most common types of count model. Bi-\\nnomial regression is the name we’ll use for a family of related procedures that all model\\na binary classification—alive/dead, accept/reject, left/right—for which the total of both cat-\\negories is known. This is like the marble and globe tossing examples from Chapter 2. But\\nnow you get to incorporate predictor variables. Poisson regression is a GLM that models\\na count with an unknown maximum—number of elephants in Kenya, number of applica-\\ntions to a PhD program, number of significance tests in an issue of Psychological Science. As\\ndescribed in Chapter 10, the Poisson model is a special case of binomial. At the end, the\\nchapter describes some other count regressions.\\n323\\n'},\n",
       " {'index': 342,\n",
       "  'number': 324,\n",
       "  'content': '324\\n11. GOD SPIKED THE INTEGERS\\nFigure 11.1. William Thomson’s third tide prediction design.\\n(Image\\nsource: https://en.wikipedia.org/wiki/Tide-predicting_machine)\\nAll of the examples in this chapter, and the chapters to come, use all of the tools intro-\\nduced in previous chapters. Regularizing priors, information criteria, and MCMC estima-\\ntion are woven into the data analysis examples. So as you work through the examples that\\nintroduced each new type of GLM, you’ll also get to practice and better understand previous\\nlessons.\\n11.1. Binomial regression\\nThink back to the early chapters, the globe tossing model. That model was a binomial\\nmodel. The outcome was a count of water samples. But it wasn’t yet a generalized linear\\nmodel, because there were no predictor variables to relate to the outcome. That’s our work\\nnow—to mate observed counts to variables that are associated with different average counts.\\nThe binomial distribution is denoted:\\ny ∼Binomial(n, p)\\nwhere y is a count (zero or a positive whole number), p is the probability any particular “trial”\\nis a success, and n is the number of trials. As proved in the previous chapter, as the basis for\\na generalized linear model, the binomial distribution has maximum entropy when each trial\\nmust result in one of two events and the expected value is constant. There is no other pre-\\nobservation probability assumption for such a variable that will have higher entropy. It is the\\nflattest data prior we can use, given the known constraints on the values.\\nThere are two common flavors of GLM that use binomial probability functions, and they\\nare really the same model, just with the data organized in different ways.\\n'},\n",
       " {'index': 343,\n",
       "  'number': 325,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n325\\n(1) Logistic regression is the common name when the data are organized into\\nsingle-trial cases, such that the outcome variable can only take values 0 and 1.\\n(2) When individual trials with the same covariate values are instead aggregated to-\\ngether, it is common to speak of an aggregated binomial regression. In this\\ncase, the outcome can take the value zero or any positive integer up to n, the num-\\nber of trials.\\nBoth flavors use the same logit link function (page 316), so both may sometimes be called\\n“logistic” regression, as the inverse of the logit function is the logistic. Either form of bi-\\nnomial regression can be converted into the other by aggregating (logistic to aggregated) or\\nexploding (aggregated to logistic) the outcome variable. We’ll fully work an example of each.\\nLike other GLMs, binomial regression is never guaranteed to produce a nice multivari-\\nate Gaussian posterior distribution. So quadratic approximation is not always satisfactory.\\nWe’ll work some examples using quap, but we’ll also check the inferences against MCMC\\nsampling, using ulam. The reason to do it both ways is so you can get a sense of both how\\noften quadratic approximation works, even when in principle it should not, and why it fails\\nin particular contexts. This is useful, because even if you never use quadratic approximation\\nagain, your Frequentist colleagues use it all the time, and you might want to be skeptical of\\ntheir estimates.\\n11.1.1. Logistic regression: Prosocial chimpanzees. The data for this example come from\\nan experiment174 aimed at evaluating the prosocial tendencies of chimpanzees (Pan troglo-\\ndytes). The experimental structure mimics many common experiments conducted on hu-\\nman students (Homo sapiens studiensis) by economists and psychologists. A focal chim-\\npanzee sits at one end of a long table with two levers, one on the left and one on the right in\\nFigure 11.2. On the table are four dishes which may contain desirable food items. The two\\ndishes on the right side of the table are attached by a mechanism to the right-hand lever. The\\ntwo dishes on the left side are similarly attached to the left-hand lever.\\nWhen either the left or right lever is pulled by the focal animal, the two dishes on the\\nsame side slide towards opposite ends of the table. This delivers whatever is in those dishes\\nto the opposite ends. In all experimental trials, both dishes on the focal animal’s side contain\\nfood items. But only one of the dishes on the other side of the table contains a food item.\\nTherefore while both levers deliver food to the focal animal, only one of the levers delivers\\nfood to the other side of the table.\\nThere are two experimental conditions. In the partner condition, another chimpanzee\\nis seated at the opposite end of the table, as pictured in Figure 11.2. In the control condi-\\ntion, the other side of the table is empty. Finally, two counterbalancing treatments alternate\\nwhich side, left or right, has a food item for the other side of the table. This helps detect any\\nhandedness preferences for individual focal animals.\\nWhen human students participate in an experiment like this, they nearly always choose\\nthe lever linked to two pieces of food, the prosocial option, but only when another student\\nsits on the opposite side of the table. The motivating question is whether a focal chimpanzee\\nbehaves similarly, choosing the prosocial option more often when another animal is present.\\nIn terms of linear models, we want to estimate the interaction between condition (presence\\nor absence of another animal) and option (which side is prosocial).\\nLoad the data from the rethinking package:\\n'},\n",
       " {'index': 344,\n",
       "  'number': 326,\n",
       "  'content': '326\\n11. GOD SPIKED THE INTEGERS\\nFigure 11.2. Chimpanzee prosociality\\nexperiment, as seen from the perspective\\nof the focal animal.\\nThe left and right\\nlevers are indicated in the foreground.\\nPulling either expands an accordion de-\\nvice in the center, pushing the food trays\\ntowards both ends of the table. Both food\\ntrays close to the focal animal have food\\nin them. Only one of the food trays on the\\nother side contains food. The partner con-\\ndition means another animal, as pictured,\\nsits on the other end of the table. Other-\\nwise, the other end was empty.\\nR code\\n11.1\\nlibrary(rethinking)\\ndata(chimpanzees)\\nd <- chimpanzees\\nTake a look at the built-in help, ?chimpanzees, for details on all of the available variables.\\nWe’re going to focus on pulled_left as the outcome to predict, with prosoc_left and\\ncondition as predictor variables. The outcome pulled_left is a 0 or 1 indicator that the\\nfocal animal pulled the left-hand lever. The predictor prosoc_left is a 0/1 indicator that\\nthe left-hand lever was (1) or was not (0) attached to the prosocial option, the side with two\\npieces of food. The condition predictor is another 0/1 indicator, with value 1 for the partner\\ncondition and value 0 for the control condition.\\nWe’ll want to infer what happens in each combination of prosoc_left and condition.\\nThere are four combinations:\\n(1) prosoc_left= 0 and condition= 0: Two food items on right and no partner.\\n(2) prosoc_left= 1 and condition= 0: Two food items on left and no partner.\\n(3) prosoc_left= 0 and condition= 1: Two food items on right and partner present.\\n(4) prosoc_left= 1 and condition= 1: Two food items on left and partner present.\\nThe conventional thing to do here is use these dummy variables to build a linear interaction\\nmodel. We aren’t going to do that, for the reason discussed back in Chapter 5: Using dummy\\nvariables makes it hard to construct sensible priors. So instead let’s build an index variable\\ncontaining the values 1 through 4, to index the combinations above. A very quick way to do\\nthis is:\\nR code\\n11.2\\nd$treatment <- 1 + d$prosoc_left + 2*d$condition\\nNow treatment contains the values 1 through 4, matching the numbers in the list above.\\nYou can verify by using cross-tabs:\\nR code\\n11.3\\nxtabs( ~ treatment + prosoc_left + condition , d )\\n'},\n",
       " {'index': 345,\n",
       "  'number': 327,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n327\\nThe output isn’t shown. There are many ways to construct new variables like this, including\\nmutant helper functions. But often all you need is a little arithmetic.\\nNow for our target model. Since this is an experiment, the structure tells us the model\\nrelevant to inference. The model implied by the research question is, in mathematical form:\\nLi ∼Binomial(1, pi)\\nlogit(pi) = αactor[i] + βtreatment[i]\\nαj ∼to be determined\\nβk ∼to be determined\\nHere L indicates the 0/1 variable pulled_left. Since the outcome counts are just 0 or 1,\\nyou might see the same type of model defined using a Bernoulli distribution:\\nLi ∼Bernoulli(pi)\\nThis is just another way of saying Binomial(1, pi). Either way, the model above implies 7\\nα parameters, one for each chimpanzee, and 4 treatment parameters, one for each unique\\ncombination of the position of the prosocial option and the presence of a partner. In prin-\\nciple, we could specify a model that allows every chimpanzee to have their own 4 unique\\ntreatment parameters. If that sounds fun to you, I have good news. We’ll do exactly that, in\\na later chapter.\\nI’ve left the priors above “to be determined.” Let’s determine them. I was trying to warm\\nyou up for prior predictive simulation earlier in the book. Now with GLMs, it is really going\\nto pay off. Let’s consider a runt of a logistic regression, with just a single α parameter in the\\nlinear model:\\nLi ∼Binomial(1, pi)\\nlogit(pi) = α\\nα ∼Normal(0, ω)\\nWe need to pick a value for ω. To emphasize the madness of conventional flat priors, let’s\\nstart with something rather flat, like ω = 10.\\nR code\\n11.4\\nm11.1 <- quap(\\nalist(\\npulled_left ~ dbinom( 1 , p ) ,\\nlogit(p) <- a ,\\na ~ dnorm( 0 , 10 )\\n) , data=d )\\nNow let’s sample from the prior:\\nR code\\n11.5\\nset.seed(1999)\\nprior <- extract.prior( m11.1 , n=1e4 )\\nOne step remains. We need to convert the parameter to the outcome scale. This means\\nusing the inverse-link function, as discussed in the previous chapter. In this case, the\\nlink function is logit, so the inverse link is inv_logit.\\n'},\n",
       " {'index': 346,\n",
       "  'number': 328,\n",
       "  'content': '328\\n11. GOD SPIKED THE INTEGERS\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n5\\n10\\n15\\nprior prob pull left\\nDensity\\na ~ dnorm(0,10)\\na ~ dnorm(0,1.5)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\nprior diff between treatments\\nDensity\\nb ~ dnorm(0,10)\\nb ~ dnorm(0,0.5)\\nFigure 11.3. Prior predictive simulations for the most basic logistic regres-\\nsion. Black density: A flat Normal(0,10) prior on the intercept produces a\\nvery non-flat prior distribution on the outcome scale. Blue density: A more\\nconcentrated Normal(0,1.5) prior produces something more reasonable.\\nR code\\n11.6\\np <- inv_logit( prior$a )\\ndens( p , adj=0.1 )\\nI’ve displayed the resulting prior distribution in the left-hand plot of Figure 11.3. Notice\\nthat most of the probability mass is piled up near zero and one. The model thinks, before\\nit sees the data, that chimpanzees either never or always pull the left lever. This is clearly\\nsilly, and will generate unnecessary inference error. A flat prior in the logit space is not a\\nflat prior in the outcome probability space. The blue distribution in the same plot shows the\\nsame model but now with ω = 1.5. You can modify the code above to reproduce this. Now\\nthe prior probability on the outcome scale is rather flat. This is probably much flatter than\\nis optimal, since probabilities near the center are more plausible. But this is better than the\\ndefault priors most people use most of the time. We’ll use it.\\nNow we need to determine a prior for the treatment effects, the β parameters. We could\\ndefault to using the same Normal(0,1.5) prior for the treatment effects, on the reasoning\\nthat they are also just intercepts, one intercept for each treatment. But to drive home the\\nweirdness of conventionally flat priors, let’s see what Normal(0,10) looks like.\\nR code\\n11.7\\nm11.2 <- quap(\\nalist(\\npulled_left ~ dbinom( 1 , p ) ,\\nlogit(p) <- a + b[treatment] ,\\na ~ dnorm( 0 , 1.5 ),\\nb[treatment] ~ dnorm( 0 , 10 )\\n) , data=d )\\nset.seed(1999)\\n'},\n",
       " {'index': 347,\n",
       "  'number': 329,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n329\\nprior <- extract.prior( m11.2 , n=1e4 )\\np <- sapply( 1:4 , function(k) inv_logit( prior$a + prior$b[,k] ) )\\nThe code just above computes the prior probability of pulling left for each treatment. We are\\ninterested in what the priors imply about the prior differences among treatments. So let’s plot\\nthe absolute prior difference between the first two treatments.\\nR code\\n11.8\\ndens( abs( p[,1] - p[,2] ) , adj=0.1 )\\nI show this distribution on the right in Figure 11.3. Just like with α, a flat prior on the logit\\nscale piles up nearly all of the prior probability on zero and one—the model believes, before it\\nsees that data, that the treatments are either completely alike or completely different. Maybe\\nthere are contexts in which such a prior makes sense. But they don’t make sense here. Typical\\nbehavioral treatments have modest effects on chimpanzees and humans alike.\\nThe blue distribution in the same figure shows the code above repeated using a Nor-\\nmal(0,0.5) prior instead. This prior is now concentrated on low absolute differences. While\\na difference of zero has the highest prior probability, the average prior difference is:\\nR code\\n11.9\\nm11.3 <- quap(\\nalist(\\npulled_left ~ dbinom( 1 , p ) ,\\nlogit(p) <- a + b[treatment] ,\\na ~ dnorm( 0 , 1.5 ),\\nb[treatment] ~ dnorm( 0 , 0.5 )\\n) , data=d )\\nset.seed(1999)\\nprior <- extract.prior( m11.3 , n=1e4 )\\np <- sapply( 1:4 , function(k) inv_logit( prior$a + prior$b[,k] ) )\\nmean( abs( p[,1] - p[,2] ) )\\n[1] 0.09838663\\nAbout 10%. Extremely large differences are less plausible. However this is not a strong prior.\\nIf the data contain evidence of large differences, they will shine through. And keep in mind\\nthe lessons of Chapter 7: We want our priors to be skeptical of large differences, so that we\\nreduce overfitting. Good priors hurt fit to sample but are expected to improve prediction.\\nFinally, we have our complete model and are ready to add in all the individual chim-\\npanzee parameters. Let’s turn to Hamiltonian Monte Carlo to approximate the posterior, so\\nyou can get some practice with it. quap will actually do a fine job with this posterior, but\\nonly because the priors are sufficiently regularizing. In the practice problems at the end of\\nthe chapter, you’ll compare the two engines on less regularized models. First prepare the\\ndata list:\\nR code\\n11.10\\n# trimmed data list\\ndat_list <- list(\\npulled_left = d$pulled_left,\\nactor = d$actor,\\ntreatment = as.integer(d$treatment) )\\n'},\n",
       " {'index': 348,\n",
       "  'number': 330,\n",
       "  'content': '330\\n11. GOD SPIKED THE INTEGERS\\nNow we can start the Markov chain. I’ll add log_lik=TRUE to the call, so that ulam com-\\nputes the values necessary for PSIS and WAIC. There is an Overthinking box at the end that\\nexplains this in great detail.\\nR code\\n11.11\\nm11.4 <- ulam(\\nalist(\\npulled_left ~ dbinom( 1 , p ) ,\\nlogit(p) <- a[actor] + b[treatment] ,\\na[actor] ~ dnorm( 0 , 1.5 ),\\nb[treatment] ~ dnorm( 0 , 0.5 )\\n) , data=dat_list , chains=4 , log_lik=TRUE )\\nprecis( m11.4 , depth=2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\na[1] -0.45 0.32 -0.95\\n0.04\\n690\\n1\\na[2]\\n3.86 0.73\\n2.78\\n5.09\\n1417\\n1\\na[3] -0.75 0.33 -1.28 -0.23\\n765\\n1\\na[4] -0.74 0.33 -1.26 -0.21\\n887\\n1\\na[5] -0.44 0.32 -0.94\\n0.10\\n743\\n1\\na[6]\\n0.48 0.32 -0.02\\n1.00\\n894\\n1\\na[7]\\n1.95 0.40\\n1.32\\n2.63\\n882\\n1\\nb[1] -0.04 0.28 -0.51\\n0.40\\n669\\n1\\nb[2]\\n0.48 0.28\\n0.04\\n0.92\\n675\\n1\\nb[3] -0.38 0.28 -0.83\\n0.06\\n768\\n1\\nb[4]\\n0.37 0.27 -0.07\\n0.79\\n666\\n1\\nThis is the guts of the tide prediction engine. We’ll need to do a little work to interpret it. The\\nfirst 7 parameters are the intercepts unique to each chimpanzee. Each of these expresses the\\ntendency of each individual to pull the left lever. Let’s look at these on the outcome scale:\\nR code\\n11.12\\npost <- extract.samples(m11.4)\\np_left <- inv_logit( post$a )\\nplot( precis( as.data.frame(p_left) ) , xlim=c(0,1) )\\nV7\\nV6\\nV5\\nV4\\nV3\\nV2\\nV1\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nValue\\nEach row is a chimpanzee, the numbers corresponding to the values in actor. Four of the\\nindividuals—numbers 1, 3, 4, and 5—showa preference for the right lever. Two individuals—\\nnumbers 2 and 7—show the opposite preference. Number 2’s preference is very strong in-\\ndeed. If you inspect the data, you’ll see that actor 2 never once pulled the right lever in\\nany trial or treatment. There are substantial differences among the actors in their baseline\\ntendencies. This is exactly the kind of effect that makes pure experiments difficult in the\\n'},\n",
       " {'index': 349,\n",
       "  'number': 331,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n331\\nbehavioral sciences. Having repeat measurements, like in this experiment, and measuring\\nthem is very useful.\\nNow let’s consider the treatment effects, hopefully estimated more precisely because the\\nmodel could subtract out the handedness variation among actors. On the logit scale:\\nR code\\n11.13\\nlabs <- c(\"R/N\",\"L/N\",\"R/P\",\"L/P\")\\nplot( precis( m11.4 , depth=2 , pars=\"b\" ) , labels=labs )\\nL/P\\nR/P\\nL/N\\nR/N\\n-0.5\\n0.0\\n0.5\\n1.0\\nValue\\nI’ve added treatment labels in place of the parameter names. L/N means “prosocial on left /\\nno partner.” R/P means “prosocial on right / partner.” To understand these distributions, it’ll\\nhelp to consider our expectations. What we are looking for is evidence that the chimpanzees\\nchoose the prosocial option more when a partner is present. This implies comparing the\\nfirst row with the third row and the second row with the fourth row. You can probably see\\nalready that there isn’t much evidence of prosocial intention in these data. But let’s calculate\\nthe differences between no-partner/partner and make sure.\\nR code\\n11.14\\ndiffs <- list(\\ndb13 = post$b[,1] - post$b[,3],\\ndb24 = post$b[,2] - post$b[,4] )\\nplot( precis(diffs) )\\ndb24\\ndb13\\n-0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nValue\\nThese are the constrasts between the no-partner/partner treatments. The scale is log-\\nodds of pulling the left lever still. Remember the tide engine! db13 is the difference between\\nno-partner/partner treatments when the prosocial option was on the right. So if there is\\nevidence of more prosocial choice when partner is present, this will show up here as a larger\\ndifference, consistent with pulling right more when partner is present. There is indeed weak\\nevidence that individuals pulled left more when the partner was absent, but the compatibility\\ninterval is quite wide. db24 is the same difference, but for when the prosocial option was on\\nthe left. Now negative differences would be consistent with more prosocial choice when\\npartner is present. Clearly that is not the case. If anything, individuals chose prosocial more\\nwhen partner was absent. Overall, there isn’t any compelling evidence of prosocial choice in\\nthis experiment.\\n'},\n",
       " {'index': 350,\n",
       "  'number': 332,\n",
       "  'content': '332\\n11. GOD SPIKED THE INTEGERS\\nNow let’s consider a posterior prediction check. Let’s summarize the proportions of left\\npulls for each actor in each treatment and then plot against the posterior predictions. First,\\nto calculate the proportion in each combination of actor and treatment:\\nR code\\n11.15\\npl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )\\npl[1,]\\n1\\n2\\n3\\n4\\n0.3333333 0.5000000 0.2777778 0.5555556\\nThe result pl is a matrix with 7 rows and 4 columns. Each row is an individual chimpanzee.\\nEach column is a treatment. And the cells contain proportions of pulls that were of the left\\nlever. Above is the first row, showing the proportions for the first actor. The model will make\\npredictions for these values, so we can see how the posterior predictions look against the raw\\ndata. Remember that we don’t want an exact match—that would mean overfitting. But we\\nwould like to understand how the model sees the data and learn from any anomalies.\\nI’ve displayed these values, against the posterior predictions, in Figure 11.4. The top\\nplot is just the raw data. You can reproduce it with this code:\\nR code\\n11.16\\nplot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab=\"\" ,\\nylab=\"proportion left lever\" , xaxt=\"n\" , yaxt=\"n\" )\\naxis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )\\nabline( h=0.5 , lty=2 )\\nfor ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )\\nfor ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat(\"actor \",j) , xpd=TRUE )\\nfor ( j in (1:7)[-2] ) {\\nlines( (j-1)*4+c(1,3) , pl[j,c(1,3)] , lwd=2 , col=rangi2 )\\nlines( (j-1)*4+c(2,4) , pl[j,c(2,4)] , lwd=2 , col=rangi2 )\\n}\\npoints( 1:28 , t(pl) , pch=16 , col=\"white\" , cex=1.7 )\\npoints( 1:28 , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )\\nyoff <- 0.01\\ntext( 1 , pl[1,1]-yoff , \"R/N\" , pos=1 , cex=0.8 )\\ntext( 2 , pl[1,2]+yoff , \"L/N\" , pos=3 , cex=0.8 )\\ntext( 3 , pl[1,3]-yoff , \"R/P\" , pos=1 , cex=0.8 )\\ntext( 4 , pl[1,4]+yoff , \"L/P\" , pos=3 , cex=0.8 )\\nmtext( \"observed proportions\\\\n\" )\\nThere are a lot of visual embellishments in this plot, so the code is longer than it really needs\\nto be. It is just plotting the points in pl and then dressing them up. The open points are the\\nnon-partner treatments. The filled points are the partner treatments. Then the first point\\nin each open/filled pair is prosocial on the right. The second is prosocial on the left. Each\\ngroup of four point is an individual actor, labeled at the top.\\nThe bottom plot in Figure 11.4 shows the posterior predictions. We can compute these\\nusing link, just like you would with a quap model in earlier chapters:\\nR code\\n11.17\\ndat <- list( actor=rep(1:7,each=4) , treatment=rep(1:4,times=7) )\\np_post <- link( m11.4 , data=dat )\\np_mu <- apply( p_post , 2 , mean )\\n'},\n",
       " {'index': 351,\n",
       "  'number': 333,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n333\\nproportion left lever\\n0\\n0.5\\n1\\nactor 1\\nactor 2\\nactor 3\\nactor 4\\nactor 5\\nactor 6\\nactor 7\\nR/N\\nL/N\\nR/P\\nL/P\\nobserved proportions\\nproportion left lever\\n0\\n0.5\\n1\\nactor 1\\nactor 2\\nactor 3\\nactor 4\\nactor 5\\nactor 6\\nactor 7\\nposterior predictions\\nFigure 11.4. Observed data (top) and posterior predictions (bottom) for\\nthe chimpanzee data. Data are grouped by actor. Open points are no-\\npartner treatments. Filled points are partner treatments. The right R and left\\nL sides of the prosocial option are labeled in the top figure. Both left treat-\\nments and both right treatments are connected by a line segment, within\\neach actor. The bottom plot shows 89% compatibility intervals for each pro-\\nportion for each actor.\\np_ci <- apply( p_post , 2 , PI )\\nThe model expects almost no change when adding a partner. Most of the variation in pre-\\ndictions comes from the actor intercepts. Handedness seems to be the big story of this ex-\\nperiment.\\nThe data themselves show additional variation—some of the actors possibly respond\\nmore to the treatments than others do. We might consider a model that allows each unique\\nactor to have unique treatment parameters. But we’ll leave such a model until we arrive at\\nmultilevel models, because we’ll need some additional tricks to do the model well.\\nWe haven’t considered a model that splits into separate index variables the location of\\nthe prosocial option and the presence of a partner. Why not? Because the driving hypoth-\\nesis of the experiment is that the prosocial option will be chosen more when the partner is\\npresent. That is an interaction effect—the effect of the prosocial option depends upon a part-\\nner being present. But we could build a model without the interaction and use PSIS or WAIC\\nto compare it to m11.4. You can guess from the posterior distribution of m11.4 what would\\nhappen: The simpler model will do just fine, because there doesn’t seem to be any evidence\\nof an interaction between location of the prosocial option and the presence of the partner.\\n'},\n",
       " {'index': 352,\n",
       "  'number': 334,\n",
       "  'content': '334\\n11. GOD SPIKED THE INTEGERS\\nTo confirm this guess, here are the new index variables we need:\\nR code\\n11.18\\nd$side <- d$prosoc_left + 1 # right 1, left 2\\nd$cond <- d$condition + 1 # no partner 1, partner 2\\nAnd now the model. Again, we add log_lik=TRUE to the call, so we can compare the two\\nmodels with PSIS or WAIC.\\nR code\\n11.19\\ndat_list2 <- list(\\npulled_left = d$pulled_left,\\nactor = d$actor,\\nside = d$side,\\ncond = d$cond )\\nm11.5 <- ulam(\\nalist(\\npulled_left ~ dbinom( 1 , p ) ,\\nlogit(p) <- a[actor] + bs[side] + bc[cond] ,\\na[actor] ~ dnorm( 0 , 1.5 ),\\nbs[side] ~ dnorm( 0 , 0.5 ),\\nbc[cond] ~ dnorm( 0 , 0.5 )\\n) , data=dat_list2 , chains=4 , log_lik=TRUE )\\nComparing the two models with PSIS:\\nR code\\n11.20\\ncompare( m11.5 , m11.4 , func=PSIS )\\nPSIS\\nSE dPSIS\\ndSE pPSIS weight\\nm11.5 530.6 19.13\\n0.0\\nNA\\n7.6\\n0.68\\nm11.4 532.1 18.97\\n1.5 1.29\\n8.5\\n0.32\\nWAIC produces almost identical results. As we guessed, the model without the interaction is\\nreally no worse, in expected predictive accuracy, than the model with it. You should inspect\\nthe posterior distribution for m11.5 to make sure you can relate its parameters to those of\\nm11.4. They tell the same story.\\nDo note that model comparison here is for the sake of understanding how it works.\\nWe don’t need the model comparison for inference in this example. The experiment and\\nhypothesis tell us which model to use (m11.4). Then the posterior distribution is sufficient\\nfor inference.\\nOverthinking: Adding log-probability calculations to a Stan model. When we add log_lik=TRUE\\nto an ulam model, we are adding a block of code to the Stan model that calculates for each observed\\noutcome the log-probability. These calculations are returned as samples in the posterior—there will\\nbe one log-probability for each observation and each sample. So we end up with a matrix of log-\\nprobabilities that has a column for each observation and a row for each sample. You won’t see this\\nmatrix by default in precis or extract.samples. You can extract it by telling extract.samples\\nthat clean=FALSE:\\nR code\\n11.21\\npost <- extract.samples( m11.4 , clean=FALSE )\\nstr(post)\\n'},\n",
       " {'index': 353,\n",
       "  'number': 335,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n335\\nList of 4\\n$ log_lik: num [1:2000, 1:504] -0.53 -0.381 -0.441 -0.475 -0.548 ...\\n$ a\\n: num [1:2000, 1:7] -0.3675 0.0123 -0.8544 -0.2473 -0.762 ...\\n$ b\\n: num [1:2000, 1:4] 0.00915 -0.78079 0.26441 -0.25036 0.44651 ...\\n$ lp__\\n: num [1:2000(1d)] -270 -273 -270 -268 -268 ...\\nThe log_lik matrix at the top contains all of the log-probabilities needed to calculate WAIC and\\nPSIS. You can see the code that produces them by calling stancode(m11.4). Let’s review each piece\\nof the model, so you can relate it to the ulam formula. First, there is the data block, naming and\\ndefining the size of each observed variable:\\ndata{\\nint pulled_left[504];\\nint treatment[504];\\nint actor[504];\\n}\\nNext comes the parameters block, which does the same for unobserved variables:\\nparameters{\\nvector[7] a;\\nvector[4] b;\\n}\\nNow the model block, which calculates the log-posterior. The log-posterior is used in turn to com-\\npute the shape of the surface that the Hamiltonian simulations glide around on. Note that this block\\nexecutes in order, from top to bottom. The values of p must be computed before they are used in\\nbinomial( 1 , p ). This is unlike BUGS or JAGS where the lines can be in any order.\\nmodel{\\nvector[504] p;\\nb ~ normal( 0 , 0.5 );\\na ~ normal( 0 , 1.5 );\\nfor ( i in 1:504 ) {\\np[i] = a[actor[i]] + b[treatment[i]];\\np[i] = inv_logit(p[i]);\\n}\\npulled_left ~ binomial( 1 , p );\\n}\\nFinally, the reason we are here, the generated quantities block. This is an optional block that lets\\nus compute anything we’d like returned in the posterior. It executes only after a sample is accepted, so\\nit doesn’t slow down sampling much. This is unlike the model block, which is executed many times\\nduring each path needed to produce a sample.\\ngenerated quantities{\\nvector[504] log_lik;\\nvector[504] p;\\nfor ( i in 1:504 ) {\\np[i] = a[actor[i]] + b[treatment[i]];\\np[i] = inv_logit(p[i]);\\n}\\nfor ( i in 1:504 ) log_lik[i] = binomial_lpmf( pulled_left[i] | 1 , p[i] );\\n}\\nThe log-probabilities are stored in a vector of the same length as the number of observations—504\\nhere. The linear model needs to be calculated again, because while the parameters are available in\\nthis block, any variables declared inside the model block, like p, are not. So we do all of that again.\\nThere is a trick for writing the p code only once, using another optional block called transformed\\nparameters, but let’s not make things too complicated yet. Finally, we loop over the observations\\nand calculate the binomial probability of each, conditional on the parameters. The helper functions\\nPSIS and WAIC expect to see this log_lik matrix in the posterior samples. You can write a raw Stan\\nmodel, include these calculations, and still use PSIS and WAIC as before. To run this model without\\n'},\n",
       " {'index': 354,\n",
       "  'number': 336,\n",
       "  'content': '336\\n11. GOD SPIKED THE INTEGERS\\nusing ulam, you just need to put the Stan model code above into a character vector and then call stan:\\nR code\\n11.22\\nm11.4_stan_code <- stancode(m11.4)\\nm11.4_stan <- stan( model_code=m11.4_stan_code , data=dat_list , chains=4 )\\ncompare( m11.4_stan , m11.4 )\\nWAIC\\nSE dWAIC\\ndSE pWAIC weight\\nm11.4\\n531.6 18.87\\n0.0\\nNA\\n8.2\\n0.66\\nm11.4_stan 532.9 18.92\\n1.3 0.15\\n8.7\\n0.34\\nWarning message:\\nIn compare(m11.4_stan, m11.4) : Not all model fits of same class.\\nThis is usually a bad idea, because it implies they were fit by different algorithms.\\nCheck yourself, before you wreck yourself.\\nThey are the same model, as indicated by the identical (within sampling error) WAIC values. Note\\nalso the warning message. The compare function checks the types of the model objects. If there is\\nmore than one class, it carries on but with this warning. In this case, it is a false alarm—both models\\nused the same algorithm. Model m11.4 is of class ulam, which is just a wrapper for a stanfit class\\nobject. In general, it is a bad idea to compare models that approximate the posterior using different\\nalgorithms. Any difference could just be a product of the difference in algorithms. In the often quoted\\nwords of the American philosopher O’Shea Jackson, check yourself before you wreck yourself.\\n11.1.2. Relative sharkand absolutedeer. In the analysis above, I mostly focused on changes\\nin predictions on the outcome scale—how much difference does the treatment make in the\\nprobability of pulling a lever? This view of posterior prediction focuses on absolute ef-\\nfects, the difference a counter-factual change in a variable might make on an absolute scale\\nof measurement, like the probability of an event.\\nIt is more common to see logistic regressions interpreted through relative effects.\\nRelative effects are proportional changes in the odds of an outcome. If we change a variable\\nand say the odds of an outcome double, then we are discussing relative effects. You can calcu-\\nlate these proportional odds relative effect sizes by simply exponentiating the parameter\\nof interest. For example, to calculate the proportional odds of switching from treatment 2 to\\ntreatment 4 (adding a partner):\\nR code\\n11.23\\npost <- extract.samples(m11.4)\\nmean( exp(post$b[,4]-post$b[,2]) )\\n[1] 0.9206479\\nOn average, the switch multiplies the odds of pulling the left lever by 0.92, an 8% reduction\\nin odds. This is what is meant by proportional odds. The new odds are calculated by taking\\nthe old odds and multiplying them by the proportional odds, which is 0.92 in this example.\\nThe risk of focusing on relative effects, such as proportional odds, is that they aren’t\\nenough to tell us whether a variable is important or not. If the other parameters in the model\\nmake the outcome very unlikely, then even a large proportional odds like 5.0 would not make\\nthe outcome frequent. Consider for example a rare disease which occurs in 1 per 10-million\\npeople. Suppose also that reading this textbook increased the odds of the disease 5-fold.\\nThat would mean approximately 4 more cases of the disease per 10-million people. So only\\n5-in-10-million chance now. The book is safe.\\n'},\n",
       " {'index': 355,\n",
       "  'number': 337,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n337\\nBut we also shouldn’t forget about relative effects. Relative effects are needed to make\\ncausal inferences, and they can be conditionally very important, when other baseline rates\\nchange. Consider for example the parable of relative shark and absolute deer. People are very\\nafraid of sharks, but not so afraid of deer. But each year, deer kill many more people than\\nsharks do. In this comparison, absolute risks are being compared: The lifetime risk of death\\nfrom deer vastly exceeds the lifetime risk of death from shark bite.\\nHowever, this comparison is irrelevant in nearly all circumstances, because deer and\\nsharks don’t live in the same places. When you are in the water, you want to know instead\\nthe relative risk of dying from a shark attack. Conditional on being in the ocean, sharks are\\nmuch more dangerous than deer. The relative shark risk is what we want to know, for those\\nrare times when we are in the ocean.\\nNeither absolute nor relative risk is sufficient for all purposes. Relative risk can make a\\nmostly irrelevant threat, like death from deer, seem deadly. For general advice, absolute risk\\noften makes more sense. But to make general predictions, conditional on specific circum-\\nstances, we still need relative risk. Sharks are absolutely safe, while deer are relatively safe.\\nBoth are important truths.\\nOverthinking: Proportional odds and relative risk. Why does exponentiating a logistic regression\\ncoefficient compute the proportional odds? Consider the formula for the odds in a logistic regression:\\npi/(1 −pi) = exp(α + βxi)\\nThe proportional odds of the event is the number we need to multiply the odds by when we increase\\nxi by 1 unit. Let q stand for the proportional odds. Then it is defined by:\\nq = exp(α + β(xi + 1))\\nexp(α + βxi)\\n= exp(α) exp(βxi) exp(β)\\nexp(α) exp(βxi)\\n= exp(β)\\nIt’s really that simple. So if q = 2, that means a unit increase in xi generates a doubling of the odds.\\nThis a relative risk, because if the intercept α, or any combination of other predictors, makes the event\\nvery unlikely or almost certain, then a doubling of the odds might not change the probability pi much.\\nSuppose for example that the odds are pi/(1 −pi) = 1/100. Doubling this to 2/99 moves pi from\\napproximately 0.01 to approximately 0.02. Similarly, if the odds are pi/(1−pi) = 100/1, the doubling\\nmoves pi from about 0.99 to 0.995.\\n11.1.3. Aggregated binomial: Chimpanzees again, condensed. In the chimpanzees data\\ncontext, the models all calculated the likelihood of observing either zero or one pulls of the\\nleft-hand lever. The models did so, because the data were organized such that each row\\ndescribes the outcome of a single pull. But in principle the same data could be organized\\ndifferently. As long as we don’t care about the order of the individual pulls, the same infor-\\nmation is contained in a count of how many times each individual pulled the left-hand lever,\\nfor each combination of predictor variables.\\nFor example, to calculate the number of times each chimpanzee pulled the left-hand\\nlever, for each combination of predictor values:\\nR code\\n11.24\\ndata(chimpanzees)\\nd <- chimpanzees\\nd$treatment <- 1 + d$prosoc_left + 2*d$condition\\nd$side <- d$prosoc_left + 1 # right 1, left 2\\nd$cond <- d$condition + 1 # no partner 1, partner 2\\n'},\n",
       " {'index': 356,\n",
       "  'number': 338,\n",
       "  'content': '338\\n11. GOD SPIKED THE INTEGERS\\nd_aggregated <- aggregate(\\nd$pulled_left ,\\nlist( treatment=d$treatment , actor=d$actor ,\\nside=d$side , cond=d$cond ) ,\\nsum )\\ncolnames(d_aggregated)[5] <- \"left_pulls\"\\nHere are the results for the first two chimpanzees:\\ntreatment actor side cond left_pulls\\n1\\n1\\n1\\n1\\n1\\n6\\n2\\n1\\n2\\n1\\n1\\n18\\n3\\n1\\n3\\n1\\n1\\n5\\n4\\n1\\n4\\n1\\n1\\n6\\n5\\n1\\n5\\n1\\n1\\n6\\n6\\n1\\n6\\n1\\n1\\n14\\n7\\n1\\n7\\n1\\n1\\n14\\n8\\n2\\n1\\n2\\n1\\n9\\nThe left_pulls column on the right is the count of times each actor pulled the left-hand\\nlever for trials in each treatment. Recall that actor number 2 always pulled the left-hand\\nlever. As a result, the counts for actor 2 are all 18—there were 18 trials for each animal for\\neach treatment. Now we can get exactly the same inferences as before, just by defining the\\nfollowing model:\\nR code\\n11.25\\ndat <- with( d_aggregated , list(\\nleft_pulls = left_pulls,\\ntreatment = treatment,\\nactor = actor,\\nside = side,\\ncond = cond ) )\\nm11.6 <- ulam(\\nalist(\\nleft_pulls ~ dbinom( 18 , p ) ,\\nlogit(p) <- a[actor] + b[treatment] ,\\na[actor] ~ dnorm( 0 , 1.5 ) ,\\nb[treatment] ~ dnorm( 0 , 0.5 )\\n) , data=dat , chains=4 , log_lik=TRUE )\\nTake note of the 18 in the spot where a 1 used to be. Now there are 18 trials on each row, and\\nthe likelihood defines the probability of each count left_pulls out of 18 trials. Inspect the\\nprecis output. You’ll see that the posterior distribution is the same as in model m11.4.\\nHowever, the PSIS (and WAIC) scores are very different between the 0/1 and aggregated\\nmodels. Let’s compare them:\\nR code\\n11.26\\ncompare( m11.6 , m11.4 , func=PSIS )\\nSome Pareto k values are high (>0.5).\\nPSIS\\nSE dPSIS\\ndSE pPSIS weight\\n'},\n",
       " {'index': 357,\n",
       "  'number': 339,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n339\\nm11.6 113.5\\n8.41\\n0.0\\nNA\\n8.1\\n1\\nm11.4 532.1 18.97 418.6 9.44\\n8.5\\n0\\nWarning message:\\nIn compare(m11.6, m11.4, func = PSIS) :\\nDifferent numbers of observations found for at least two models.\\nModel comparison is valid only for models fit to exactly the same observations.\\nNumber of observations for each model:\\nm11.6 28\\nm11.4 504\\nThere’s a lot of output here. But let’s take it slowly, top to bottom. First, the PSIS summary\\ntable shows very different scores for the two models, even though they have the same poste-\\nrior distribution. Why is this? The major reason is the aggregated model, m11.6, contains\\nan extra factor in its log-probabilities, because of the way the data are organized. When cal-\\nculating dbinom(6,9,0.2), for example, the dbinom function contains a term for all the\\norders the 6 successes could appear in 9 trials. You’ve seen this term before:\\nPr(6|9, p) =\\n6!\\n6!(9 −6)!p6(1 −p)9−6\\nThat ugly fraction in front is the multiplicity that was so important in the first half of the\\nprevious chapter. It just counts all the ways you could see 6 successes in 9 trials. When we\\ninstead split the 6 successes apart into 9 different 0/1 trials, like in a logistic regression, there\\nis no multiplicity term to compute. So the joint probably of all 9 trials is instead:\\nPr(1, 1, 1, 1, 1, 1, 0, 0, 0|p) = p6(1 −p)9−6\\nThis makes the aggregated probabilities larger—there are more ways to see the data. So the\\nPSIS/WAIC scores end up being smaller. Go ahead and try it with the simple example here:\\nR code\\n11.27\\n# deviance of aggregated 6-in-9\\n-2*dbinom(6,9,0.2,log=TRUE)\\n# deviance of dis-aggregated\\n-2*sum(dbern(c(1,1,1,1,1,1,0,0,0),0.2,log=TRUE))\\n[1] 11.79048\\n[1] 20.65212\\nBut this difference is entirely meaningless. It is just a side effect of how we organized the\\ndata. The posterior distribution for the probability of success on each trial will end up the\\nsame, either way.\\nContinuing with the compare output, there are two warnings. The first is just to flag the\\nfact that the two models have different numbers of observations. Never compare models fit\\nto different sets of observations. The other warning is the Pareto k message at the top:\\nSome Pareto k values are high (>0.5).\\nThis is the Pareto k warning you met way back in Chapter 7. The value in these warnings is\\nmore that they inform us about the presence of highly influential observations. Observations\\nwith these high Pareto k values are usually influential—the posterior changes a lot when they\\nare dropped from the sample. As with the example from Chapter 7, looking at individual\\npoints is very helpful for understanding why the model behaves as it does. And the penalty\\nterms from WAIC contain similar information about relative influence of each observation.\\n'},\n",
       " {'index': 358,\n",
       "  'number': 340,\n",
       "  'content': '340\\n11. GOD SPIKED THE INTEGERS\\nBefore looking at the Pareto k values, you might have noticed already that we didn’t get\\na similar warning before in the disaggregated logistic models of the same data. Why not?\\nBecause when we aggregated the data by actor-treatment, we forced PSIS (and WAIC) to\\nimagine cross-validation that leaves out all 18 observations in each actor-treatment combi-\\nnation. So instead of leave-one-out cross-validation, it is more like leave-eighteen-out. This\\nmakes some observations more influential, because they are really now 18 observations.\\nWhat’s the bottom line? If you want to calculate WAIC or PSIS, you should use a logistic\\nregression data format, not an aggregated format. Otherwise you are implicitly assuming\\nthat only large chunks of the data are separable. There are times when this makes sense, like\\nwith multilevel models. But it doesn’t in most ordinary binomial regressions. If you dig into\\nthe Stan code that computes the individual log-likelihood terms, you can aggregate at any\\nlevel you like, computing effect scores that are relevant to the level you want to predict at,\\nwhether that is 0/1 events or rather new individuals with many 0/1 events.\\n11.1.4. Aggregated binomial: Graduate school admissions. In the aggregated binomial ex-\\nample above, the number of trials was always 18 on every row. This is often not the case. The\\nway to handle this is to insert a variable from the data in place of the “18”. Let’s work through\\nan example. First, load the data:\\nR code\\n11.28\\nlibrary(rethinking)\\ndata(UCBadmit)\\nd <- UCBadmit\\nThis data table only has 12 rows, so let’s look at the entire thing:\\ndept applicant.gender admit reject applications\\n1\\nA\\nmale\\n512\\n313\\n825\\n2\\nA\\nfemale\\n89\\n19\\n108\\n3\\nB\\nmale\\n353\\n207\\n560\\n4\\nB\\nfemale\\n17\\n8\\n25\\n5\\nC\\nmale\\n120\\n205\\n325\\n6\\nC\\nfemale\\n202\\n391\\n593\\n7\\nD\\nmale\\n138\\n279\\n417\\n8\\nD\\nfemale\\n131\\n244\\n375\\n9\\nE\\nmale\\n53\\n138\\n191\\n10\\nE\\nfemale\\n94\\n299\\n393\\n11\\nF\\nmale\\n22\\n351\\n373\\n12\\nF\\nfemale\\n24\\n317\\n341\\nThese are graduate school applications to 6 different academic departments at UC Berke-\\nley.175 The admit column indicates the number offered admission. The reject column\\nindicates the opposite decision. The applications column is just the sum of admit and\\nreject. Each application has a 0 or 1 outcome for admission, but since these outcomes have\\nbeen aggregated by department and gender, there are only 12 rows. These 12 rows however\\nrepresent 4526 applications, the sum of the applications column. So there is a lot of data\\nhere—counting the rows in the data table is no longer a sensible way to assess sample size.\\nWe could split these data apart into 0/1 Bernoulli trials, like in the original chimpanzees\\ndata. Then there would be 4526 rows in the data.\\nOur job is to evaluate whether these data contain evidence of gender bias in admissions.\\nWe will model the admission decisions, focusing on applicant gender as a predictor variable.\\nSo we want to fit a binomial regression that models admit as a function of each applicant’s\\n'},\n",
       " {'index': 359,\n",
       "  'number': 341,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n341\\ngender. This will estimate the association between gender and probability of admission. This\\nis what the model looks like, in mathematical form:\\nAi ∼Binomial(Ni, pi)\\nlogit(pi) = αgid[i]\\nαj ∼Normal(0, 1.5)\\nThe variable Ni indicates applications[i], the number of applications on row i. The index\\nvariable gid[i] indexes gender of an applicant. 1 indicates male, and 2 indicates female. We’ll\\nconstruct it just before fitting the model, like this:\\nR code\\n11.29\\ndat_list <- list(\\nadmit = d$admit,\\napplications = d$applications,\\ngid = ifelse( d$applicant.gender==\"male\" , 1 , 2 )\\n)\\nm11.7 <- ulam(\\nalist(\\nadmit ~ dbinom( applications , p ) ,\\nlogit(p) <- a[gid] ,\\na[gid] ~ dnorm( 0 , 1.5 )\\n) , data=dat_list , chains=4 )\\nprecis( m11.7 , depth=2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\na[1] -0.22 0.04 -0.29 -0.16\\n1232\\n1\\na[2] -0.83 0.05 -0.91 -0.75\\n1323\\n1\\nThe posterior for male applicants, a[1], is higher than that of female applicants. How much\\nhigher? We need to compute the contrast. Let’s calculate the contrast on the logit scale\\n(shark) as well as the contrast on the outcome scale (absolute deer):\\nR code\\n11.30\\npost <- extract.samples(m11.7)\\ndiff_a <- post$a[,1] - post$a[,2]\\ndiff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])\\nprecis( list( diff_a=diff_a , diff_p=diff_p ) )\\n\\'data.frame\\': 2000 obs. of 2 variables:\\nmean\\nsd 5.5% 94.5%\\nhistogram\\ndiff_a 0.61 0.07 0.50\\n0.71 ▁▁▁▁▃▇▇▃▂▁▁▁\\ndiff_p 0.14 0.01 0.12\\n0.16 ▁▁▁▂▃▇▇▅▂▁▁▁\\nThe log-odds difference is certainly positive, corresponding to a higher probability of admis-\\nsion for male applicants. On the probability scale itself, the difference is somewhere between\\n12% and 16%.\\nBefore moving on to speculate on the cause of the male advantage, let’s plot posterior pre-\\ndictions for the model. We’ll use the default posterior validation check function, postcheck,\\nand then dress it up a little by adding lines to connect data points from the same department.\\n'},\n",
       " {'index': 360,\n",
       "  'number': 342,\n",
       "  'content': '342\\n11. GOD SPIKED THE INTEGERS\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ncase\\nadmit\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\nPosterior validation check\\nA\\nB\\nC\\nD\\nE\\nF\\nFigure 11.5. Posterior check for model m11.7. Blue points are observed\\nproportions admitted for each row in the data, with points from the same\\ndepartment connected by a blue line. Open points, the tiny vertical black\\nlines within them, and the crosses are expected proportions, 89% intervals\\nof the expectation, and 89% interval of simulated samples, respectively.\\nR code\\n11.31\\npostcheck( m11.7 )\\n# draw lines connecting points from same dept\\nfor ( i in 1:6 ) {\\nx <- 1 + 2*(i-1)\\ny1 <- d$admit[x]/d$applications[x]\\ny2 <- d$admit[x+1]/d$applications[x+1]\\nlines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 )\\ntext( x+0.5 , (y1+y2)/2 + 0.05 , d$dept[x] , cex=0.8 , col=rangi2 )\\n}\\nThe result is shown as Figure 11.5. Those are pretty terrible predictions. There are only two\\ndepartments in which women had a lower rate of admission than men (C and E), and yet the\\nmodel says that women should expect to have a 14% lower chance of admission.\\nSometimes a fit this bad is the result of a coding mistake. In this case, it is not. The\\nmodel did correctly answer the question we asked of it: What are the average probabilities of\\nadmission for women and men, across all departments? The problem in this case is that men\\nand women did not apply to the same departments, and departments vary in their rates of\\nadmission. This makes the answer misleading. You can see the steady decline in admission\\nprobability for both men and women from department A to department F. Women in these\\ndata tended not to apply to departments like A and B, which had high overall admission\\nrates. Instead they applied in large numbers to departments like F, which admitted less than\\n10% of applicants.\\nSo while it is true overall that women had a lower probability of admission in these data,\\nit is clearly not true within most departments. And note that just inspecting the posterior\\n'},\n",
       " {'index': 361,\n",
       "  'number': 343,\n",
       "  'content': '11.1. BINOMIAL REGRESSION\\n343\\ndistribution alone would never have revealed that fact to us. We had to appeal to something\\noutside the fit model. In this case, it was a simple posterior validation check.\\nInstead of asking “What are the average probabilities of admission for women and men\\nacross all departments?” we want to ask “What is the average difference in probability of ad-\\nmission between women and men within departments?” In order to ask the second question,\\nwe estimate unique female and male admission rates in each department. Here’s a model\\nthat asks this new question:\\nAi ∼Binomial(Ni, pi)\\nlogit(pi) = αgid[i] + δdept[i]\\nαj ∼Normal(0, 1.5)\\nδk ∼Normal(0, 1.5)\\nwhere dept indexes department in k = 1..6. So now each department k gets its own log-odds\\nof admission, δk, but the model still estimates universal adjustments, which are the same in\\nall departments, for male and female applications.\\nFitting this model is straightforward. We’ll use the indexing notation again to construct\\nan intercept for each department. But first, we also need to construct a numerical index that\\nnumbers the departments 1 through 6. The function coerce_index can do this for us, using\\nthe dept factor as input. Here’s the code to construct the index and fit both models:\\nR code\\n11.32\\ndat_list$dept_id <- rep(1:6,each=2)\\nm11.8 <- ulam(\\nalist(\\nadmit ~ dbinom( applications , p ) ,\\nlogit(p) <- a[gid] + delta[dept_id] ,\\na[gid] ~ dnorm( 0 , 1.5 ) ,\\ndelta[dept_id] ~ dnorm( 0 , 1.5 )\\n) , data=dat_list , chains=4 , iter=4000 )\\nprecis( m11.8 , depth=2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\na[1]\\n-0.54 0.52 -1.40\\n0.27\\n763\\n1\\na[2]\\n-0.44 0.53 -1.29\\n0.38\\n768\\n1\\ndelta[1]\\n1.12 0.53\\n0.31\\n1.98\\n772\\n1\\ndelta[2]\\n1.08 0.53\\n0.25\\n1.94\\n782\\n1\\ndelta[3] -0.14 0.53 -0.97\\n0.72\\n767\\n1\\ndelta[4] -0.17 0.53 -0.99\\n0.69\\n767\\n1\\ndelta[5] -0.62 0.53 -1.44\\n0.25\\n789\\n1\\ndelta[6] -2.17 0.54 -3.03 -1.30\\n812\\n1\\nThe intercept for male applicants, a[1], is now a little smaller on average than the one for\\nfemale applicants. Let’s calculate the contrasts against, both on relative (shark) and absolute\\n(deer) scales:\\nR code\\n11.33\\npost <- extract.samples(m11.8)\\ndiff_a <- post$a[,1] - post$a[,2]\\ndiff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])\\nprecis( list( diff_a=diff_a , diff_p=diff_p ) )\\n'},\n",
       " {'index': 362,\n",
       "  'number': 344,\n",
       "  'content': '344\\n11. GOD SPIKED THE INTEGERS\\n\\'data.frame\\': 10000 obs. of 2 variables:\\nmean\\nsd\\n5.5% 94.5%\\nhistogram\\ndiff_a -0.10 0.08 -0.22\\n0.03 ▁▁▁▁▂▅▇▇▅▂▁▁▁▁\\ndiff_p -0.02 0.02 -0.05\\n0.01\\n▁▁▁▂▇▇▂▁▁\\nIf male applicants have it worse, it is only by a very small amount, about 2% on average.\\nWhy did adding departments to the model change the inference about gender so much?\\nThe earlier figure gives you a hint—the rates of admission vary a lot across departments.\\nFurthermore, women and men applied to different departments. Let’s do a quick tabulation\\nto show that:\\nR code\\n11.34\\npg <- with( dat_list , sapply( 1:6 , function(k)\\napplications[dept_id==k]/sum(applications[dept_id==k]) ) )\\nrownames(pg) <- c(\"male\",\"female\")\\ncolnames(pg) <- unique(d$dept)\\nround( pg , 2 )\\nA\\nB\\nC\\nD\\nE\\nF\\nmale\\n0.88 0.96 0.35 0.53 0.33 0.52\\nfemale 0.12 0.04 0.65 0.47 0.67 0.48\\nThese are the proportions of all applications in each department that are from either men\\n(top row) or women (bottom row). Department A receives 88% of its applications from men.\\nDepartment E receives 33% from men. Now look back at the delta posterior means in the\\nprecis output from m11.8. The departments with a larger proportion of women applicants\\nare also those with lower overall admissions rates.\\nDepartment is probably a confound, in the sense that it misleads us about the direct\\ncausal effect. But it is not a confound, in the sense that it is probably a genuine causal path\\nthrough gender influences admission. Gender influences choice of department, and depart-\\nment influences chance of admission. Controlling for department reveals a more plausible\\ndirect causal influence of gender. In DAG form:\\nA\\nD\\nG\\nThe variable G is gender, D is department, and A is acceptance. There is an indirect causal\\npath G →D →A from gender to acceptance. So to infer the direct effect G →A, we\\nneed to condition on D and close the indirect path. Model m11.8 does that. If you inspect\\npostcheck(m11.8), you’ll see that the model lines up much better now with the variation\\namong departments. This is another example of a mediation analysis.\\nDon’t get too excited however that conditioning on department is sufficient to estimate\\nthe direct causal effect of gender on admissions. What if there are unobserved confounds\\ninfluencing both department and admissions? Like this:\\n'},\n",
       " {'index': 363,\n",
       "  'number': 345,\n",
       "  'content': '11.2. POISSON REGRESSION\\n345\\nA\\nD\\nG\\nU\\nWhat could U be? How about academic ability. Ability could influence choice of department\\nand probability of admission. In that case, conditioning on department is conditioning on a\\ncollider, and it opens a non-causal path between gender and admissions, G →D ←U →A.\\nI’ll ask you to explore some possibilities like this in the practice problems at the end.\\nAs a final note, you might have noticed that model m11.8 is over-parameterized. We\\ndon’t actually need one of the parameters, either a[1] or a[2]. Why? Because the indi-\\nvidual delta parameters can stand for the acceptance rate of one of the genders in each\\ndepartment. Then we just need an average deviation across departments. If this were a non-\\nBayesian model, it wouldn’t work. But this kind of model is perfectly fine for us. The standard\\ndeviations are inflated, because there are many combinations of the a and delta parameters\\nthat can match the data. If you look at pairs(m11.8), you’ll see high posterior correlations\\namong all of the parameters. But on the outcome scale, the predictions are much tighter, as\\nyou can see if you invoke postcheck(m11.8). It’s all good.\\nWhy might we want to over-parameterize the model? Because it makes it easier to as-\\nsign priors. If we made one of the genders baseline and measured the other as a deviation\\nfrom it, we would stumble into the issue of assuming that the acceptance rate for one of the\\ngenders is pre-data more uncertain than the other. This isn’t to say that over-parameterizing\\na model is always a good idea. But it isn’t a violation of any statistical principle. You can\\nalways convert the posterior, post sampling, to any alternative parameterization. The only\\nlimitation is whether the algorithm we use to approximate the posterior can handle the high\\ncorrelations. In this case, it can, and I bumped up the number of iterations to make sure.\\nRethinking: Simpson’sparadoxisnota paradox. This empirical example is a famous one in statistical\\nteaching. It is often used to illustrate a phenomenon known as Simpson’s paradox.176 Like most\\nparadoxes, there is no violation of logic, just of intuition. And since different people have different\\nintuition, Simpson’s paradox means different things to different people. The poor intuition being\\nviolated in this case is that a positive association in the entire population should also hold within\\neach department. Overall, females in these data did have a harder time getting admitted to graduate\\nschool. But that arose because females applied to the hardest departments for anyone, male or female,\\nto gain admission to.\\nPerhaps a little more paradoxical is that this phenomenon can repeat itself indefinitely within\\na sample. Any association between an outcome and a predictor can be nullified or reversed when\\nanother predictor is added to the model. And the reversal can reveal a true causal influence or rather\\njust be a confound, as occurred in the grandparents example in Chapter 6. All that we can do about\\nthis is to remain skeptical of models and try to imagine ways they might be deceiving us. Thinking\\ncausally about these settings usually helps.177\\n11.2. Poisson regression\\nBinomial GLMs are appropriate when the outcome is a count from zero to some known\\nupper bound. If you can analogize the data to the globe tossing model, then you should\\nuse a binomial GLM. But often the upper bound isn’t known. Instead the counts never get\\n'},\n",
       " {'index': 364,\n",
       "  'number': 346,\n",
       "  'content': '346\\n11. GOD SPIKED THE INTEGERS\\nclose to any upper limit. For example, if we go fishing and return with 17 fish, what was the\\ntheoretical maximum? Whatever it is, it isn’t in our data. How do we model the fish counts?\\nIt turns out that the binomial model works here, provided we squint at it the right way.\\nWhen a binomial distribution has a very small probability of an event p and a very large num-\\nber of trials N, then it takes on a special shape. The expected value of a binomial distribution\\nis just Np, and its variance is Np(1 −p). But when N is very large and p is very small, then\\nthese are approximately the same.\\nFor example, suppose you own a monastery that is in the business, like many monas-\\nteries before the invention of the printing press, of copying manuscripts. You employ 1000\\nmonks, and on any particular day about 1 of them finishes a manuscript. Since the monks\\nare working independently of one another, and manuscripts vary in length, some days pro-\\nduce 3 or more manuscripts, and many days produce none. Since this is a binomial process,\\nyou can calculate the variance across days as Np(1−p) = 1000(0.001)(1−0.001) ≈1. You\\ncan simulate this, for example over 10,000 (1e5) days:\\nR code\\n11.35\\ny <- rbinom(1e5,1000,1/1000)\\nc( mean(y) , var(y) )\\n[1] 0.9968400 0.9928199\\nThe mean and the variance are nearly identical. This is a special shape of the binomial. This\\nspecial shape is known as the Poisson distribution, and it is useful because it allows us to\\nmodel binomial events for which the number of trials N is unknown or uncountably large.\\nSuppose for example that you come to own, through imperial drama, another monastery.\\nYou don’t know how many monks toil within it, but your advisors tell you that it produces,\\non average, 2 manuscripts per day. With this information alone, you can infer the entire\\ndistribution of numbers of manuscripts completed each day.\\nTo build models with a Poisson distribution, the model form is even simpler than it is\\nfor a binomial or Gaussian model. This simplicity arises from the Poisson’s having only one\\nparameter that describes its shape, resulting in a data probability definition like this:\\nyi ∼Poisson(λ)\\nThe parameter λ is the expected value of the outcome y. It is also the expected variance of\\nthe counts y.\\nWe also need a link function. The conventional link function for a Poisson model is the\\nlog link, as introduced in the previous chapter (page 318). To embed a linear model, we use:\\nyi ∼Poisson(λi)\\nlog(λi) = α + β(xi −¯x)\\nThe log link ensures that λi is always positive, which is required of the expected value of\\na count outcome. But as mentioned in the previous chapter, it also implies an exponential\\nrelationship between predictors and the expected value. Exponential relationships grow very\\nquickly, and few natural phenomena remain exponential for long. So one thing to always\\ncheck with a log link is whether it makes sense at all ranges of the predictor variables. Priors\\non the log scale also scale in surprising ways. So prior predictive simulation is again helpful.\\n11.2.1. Example: Oceanic tool complexity. The island societies of Oceania provide a natu-\\nral experiment in technological evolution. Different historical island populations possessed\\ntool kits of different size. These kits include fish hooks, axes, boats, hand plows, and many\\n'},\n",
       " {'index': 365,\n",
       "  'number': 347,\n",
       "  'content': '11.2. POISSON REGRESSION\\n347\\nFigure 11.6. Locations of societies\\nin the Kline data. The Equator and\\nInternational Date Line are shown.\\nother types of tools. A number of theories predict that larger populations will both develop\\nand sustain more complex tool kits. So the natural variation in population size induced by\\nnatural variation in island size in Oceania provides a natural experiment to test these ideas.\\nIt’s also suggested that contact rates among populations effectively increase population size,\\nas it’s relevant to technological evolution. So variation in contact rates among Oceanic soci-\\neties is also relevant.\\nWe’ll use this topic to develop a standard Poisson GLM analysis. And then I’ll pivot at\\nthe end and also do a non-standard, but more theoretically motivated, Poisson model. The\\ndata we’ll work with are counts of unique tool types for 10 historical Oceanic societies:178\\nR code\\n11.36\\nlibrary(rethinking)\\ndata(Kline)\\nd <- Kline\\nd\\nculture population contact total_tools mean_TU\\n1\\nMalekula\\n1100\\nlow\\n13\\n3.2\\n2\\nTikopia\\n1500\\nlow\\n22\\n4.7\\n3\\nSanta Cruz\\n3600\\nlow\\n24\\n4.0\\n4\\nYap\\n4791\\nhigh\\n43\\n5.0\\n5\\nLau Fiji\\n7400\\nhigh\\n33\\n5.0\\n6\\nTrobriand\\n8000\\nhigh\\n19\\n4.0\\n7\\nChuuk\\n9200\\nhigh\\n40\\n3.8\\n8\\nManus\\n13000\\nlow\\n28\\n6.6\\n9\\nTonga\\n17500\\nhigh\\n55\\n5.4\\n10\\nHawaii\\n275000\\nlow\\n71\\n6.6\\nThat’s the entire data set. You can see the location of these societies in the Pacific Ocean in\\nFigure 11.6. Keep in mind that the number of rows is not clearly the same as the “sample\\nsize” in a count model. The relationship between parameters and “degrees of freedom” is not\\nsimple, outside of simple linear regressions. Still, there isn’t a lot of data here, because there\\njust aren’t that many historic Oceanic societies for which reliable data can be gathered. We’ll\\nwant to use regularization to damp down overfitting, as always. But as you’ll see, a lot can\\nstill be learned from these data. Any rules you’ve been taught about minimum sample sizes\\nfor inference are just non-Bayesian superstitions. If you get the prior back, then the data\\naren’t enough. It’s that simple.\\nThe total_tools variable will be the outcome variable. We’ll model the idea that:\\n'},\n",
       " {'index': 366,\n",
       "  'number': 348,\n",
       "  'content': '348\\n11. GOD SPIKED THE INTEGERS\\n(1) The number of tools increases with the log population size. Why log? Because\\nthat’s what the theory says, that it is the order of magnitude of the population that\\nmatters, not the absolute size of it. So we’ll look for a positive association between\\ntotal_tools and log population. You can get some intuition for why a linear\\nimpact of population size can’t be right by thinking about mechanism. We’ll think\\nabout mechanism more at the end.\\n(2) The number of tools increases with the contact rate among islands. No nation is\\nan island, even when it is an island. Islands that are better networked may acquire\\nor sustain more tool types.\\n(3) The impact of population on tool counts is moderated by high contact. This\\nis to say that the association between total_tools and log population depends\\nupon contact. So we will look for a positive interaction between log population\\nand contact rate.\\nLet’s build now. First, we make some new columns with the standardized log of popu-\\nlation and an index variable for contact:\\nR code\\n11.37\\nd$P <- scale( log(d$population) )\\nd$contact_id <- ifelse( d$contact==\"high\" , 2 , 1 )\\nThe model that conforms to the research hypothesis includes an interaction between log-\\npopulation and contact rate. In math form, this is:\\nTi ∼Poisson(λi)\\nlog λi = αcid[i] + βcid[i] log Pi\\nαj ∼to be determined\\nβj ∼to be determined\\nwhere P is population and cid is contact_id.\\nWe need to figure out some sensible priors. As with binomial models, the transformation\\nof scale between the scale of the linear model and the count scale of the outcome means that\\nsomething flat on the linear model scale will not be flat on the outcome scale. Let’s consider\\nfor example just a model with an intercept and a vague Normal(0,10) prior on it:\\nTi ∼Poisson(λi)\\nlog λi = α\\nα ∼Normal(0, 10)\\nWhat does this prior look like on the outcome scale, λ? If α has a normal distribution, then\\nλ has a log-normal distribution. So let’s plot a log-normal with these values for the (normal)\\nmean and standard deviation:\\nR code\\n11.38\\ncurve( dlnorm( x , 0 , 10 ) , from=0 , to=100 , n=200 )\\nThe distribution is shown in Figure 11.7 as the black curve. I’ve used a range from 0 to 100\\non the horizontal axis, reflecting the notion that we know all historical tool kits in the Pacific\\nwere in this range. For the α ∼Normal(0, 10) prior, there is a huge spike right around\\nzero—that means zero tools on average—and a very long tail. How long? Well the mean of\\n'},\n",
       " {'index': 367,\n",
       "  'number': 349,\n",
       "  'content': '11.2. POISSON REGRESSION\\n349\\n0\\n20\\n40\\n60\\n80\\n100\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\nmean number of tools\\nDensity\\na ~ dnorm(0,10)\\na ~ dnorm(3,0.5)\\nFigure 11.7. Prior predictive distribution of\\nthe mean λ of a simple Poisson GLM, consid-\\nering only the intercept α. A flat conventional\\nprior (black) creates absurd expectations on\\nthe outcome scale. The mean of this distri-\\nbution is exp(50) ≈stupidly large. It is easy\\nto do better by shifting prior mass above zero\\n(blue).\\na log-normal distribution is exp(µ+σ2/2), which evaluates to exp(50), which is impossibly\\nlarge. If you doubt this, just simulate it:\\nR code\\n11.39\\na <- rnorm(1e4,0,10)\\nlambda <- exp(a)\\nmean( lambda )\\n[1] 9.622994e+12\\nThat’s a lot of tools, enough to cover an entire island. We can do better than this.\\nI encourage you to play around with the curve code above, trying different means and\\nstandard deviations. The fact to appreciate is that a log link puts half of the real numbers—\\nthe negative numbers—between 0 and 1 on the outcome scale. So if your prior puts half\\nits mass below zero, then half the mass will end up between 0 and 1 on the outcome scale.\\nFor Poisson models, flat priors make no sense and can wreck Prague. Here’s my weakly\\ninformative suggestion:\\nR code\\n11.40\\ncurve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200 )\\nI’ve displayed this distribution as well in Figure 11.7, as the blue curve. The mean is now\\nexp(3 + 0.52/2) ≈20. We haven’t looked at the mean of the total_tools column, and we\\ndon’t want to. This is supposed to be a prior. We want the prior predictive distribution to\\nlive in the plausible outcome space, not fit the sample.\\nNow we need a prior for β, the coefficient of log population. Again for dramatic effect,\\nlet’s consider first a conventional flat prior like β ∼Normal(0, 10). Conventional priors are\\neven flatter. We’ll simulate together with the intercept and plot 100 prior trends of standard-\\nized log population against total tools:\\nR code\\n11.41\\nN <- 100\\na <- rnorm( N , 3 , 0.5 )\\nb <- rnorm( N , 0 , 10 )\\nplot( NULL , xlim=c(-2,2) , ylim=c(0,100) )\\n'},\n",
       " {'index': 368,\n",
       "  'number': 350,\n",
       "  'content': '350\\n11. GOD SPIKED THE INTEGERS\\nfor ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=grau() )\\nI display this prior predictive distribution as the top-left plot of Figure 11.8. The pivoting\\naround zero makes sense—that’s just the average log population. The values on the horizontal\\naxis are z-scores, because the variable is standardized. So you can see that this prior thinks\\nthat the vast majority of prior relationships between log population and total tools embody\\neither explosive growth just above the mean log population size or rather catastrophic decline\\nright before the mean. This prior is terrible. Of course you will be able to confirm, once we\\nstart fitting models, that even 10 observations can overcome these terrible priors. But please\\nremember that we are practicing for when it does matter. And in any particular application,\\nit could matter.\\nSo let’s try something much tighter. I’m tempted actually to force the prior for β to be\\npositive. But I’ll resist that temptation and let the data prove that to you. Instead let’s just\\ndampen the prior’s enthusiasm for impossibly explosive relationships. After some experi-\\nmentation, I’ve settled on β ∼Normal(0, 0.2):\\nR code\\n11.42\\nset.seed(10)\\nN <- 100\\na <- rnorm( N , 3 , 0.5 )\\nb <- rnorm( N , 0 , 0.2 )\\nplot( NULL , xlim=c(-2,2) , ylim=c(0,100) )\\nfor ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=grau() )\\nThis plot is displayed in the top-right of Figure 11.8. Strong relationships are still possible,\\nbut most of the mass is for rather flat relationships between total tools and log population.\\nIt will also help to view these priors on more natural outcome scales. The standardized\\nlog population variable is good for fitting. But it is bad for thinking. Population size has a\\nnatural zero, and we want to keep that in sight. Standardizing the variable destroys that. First,\\nhere are 100 prior predictive trends between total tools and un-standardized log population:\\nR code\\n11.43\\nx_seq <- seq( from=log(100) , to=log(200000) , length.out=100 )\\nlambda <- sapply( x_seq , function(x) exp( a + b*x ) )\\nplot( NULL , xlim=range(x_seq) , ylim=c(0,500) , xlab=\"log population\" ,\\nylab=\"total tools\" )\\nfor ( i in 1:N ) lines( x_seq , lambda[i,] , col=grau() , lwd=1.5 )\\nThis plot appears in the bottom-left of Figure 11.8. Notice that 100 total tools is probably\\nthe most we expect to ever see in these data. While most the of trends are in that range,\\nsome explosive options remain. And finally let’s also view these same curves on the natural\\npopulation scale:\\nR code\\n11.44\\nplot( NULL , xlim=range(exp(x_seq)) , ylim=c(0,500) , xlab=\"population\" ,\\nylab=\"total tools\" )\\nfor ( i in 1:N ) lines( exp(x_seq) , lambda[i,] , col=grau() , lwd=1.5 )\\n'},\n",
       " {'index': 369,\n",
       "  'number': 351,\n",
       "  'content': '11.2. POISSON REGRESSION\\n351\\nFigure 11.8. Struggling with slope priors in a Poisson GLM. Top-left: A\\nflat prior produces explosive trends on the outcome scale. Top-right: A reg-\\nularizing prior remains mostly within the space of outcomes. Bottom-left:\\nHorizontal axis now on unstandardized scale. Bottom-right: Horizontal\\naxis on natural scale (raw population size).\\nThis plot lies in the bottom-right of Figure 11.8. On the raw population scale, these curves\\nbend the other direction. This is the natural consequence of putting the log of population\\ninside the linear model. Poisson models with log links create log-linear relationships with\\ntheir predictor variables. When a predictor variable is itself logged, this means we are assum-\\ning diminishing returns for the raw variable. You can see this by comparing the two plots\\nin the bottom of Figure 11.8. The curves on the left would be linear if you log them. On\\nthe natural population scale, the model imposes diminishing returns on population: Each\\nadditional person contributes a smaller increase in the expected number of tools. The curves\\nbend down and level off. Many predictor variables are better used as logarithms, for this rea-\\nson. Simulating prior predictive distributions is a useful way to think through these issues.\\n'},\n",
       " {'index': 370,\n",
       "  'number': 352,\n",
       "  'content': '352\\n11. GOD SPIKED THE INTEGERS\\nOkay, finally we can approximate some posterior distributions. I’m going to code both\\nthe interaction model presented above as well as a very simple intercept-only model. The\\nintercept-only model is here because I want to show you something interesting about Poisson\\nmodels and how parameters relate to model complexity. Here’s the code for both models:\\nR code\\n11.45\\ndat <- list(\\nT = d$total_tools ,\\nP = d$P ,\\ncid = d$contact_id )\\n# intercept only\\nm11.9 <- ulam(\\nalist(\\nT ~ dpois( lambda ),\\nlog(lambda) <- a,\\na ~ dnorm( 3 , 0.5 )\\n), data=dat , chains=4 , log_lik=TRUE )\\n# interaction model\\nm11.10 <- ulam(\\nalist(\\nT ~ dpois( lambda ),\\nlog(lambda) <- a[cid] + b[cid]*P,\\na[cid] ~ dnorm( 3 , 0.5 ),\\nb[cid] ~ dnorm( 0 , 0.2 )\\n), data=dat , chains=4 , log_lik=TRUE )\\nLet’s look at the PSIS model comparison quickly, just to flag two important facts.\\nR code\\n11.46\\ncompare( m11.9 , m11.10 , func=PSIS )\\nSome Pareto k values are high (>0.5).\\nPSIS\\nSE dPSIS\\ndSE pPSIS weight\\nm11.10\\n84.6 13.24\\n0.0\\nNA\\n6.6\\n1\\nm11.9\\n141.8 33.78\\n57.2 33.68\\n8.5\\n0\\nFirst, note that we get the Pareto k warning again. This indicates some highly influential\\npoints. That shouldn’t be surprising—this is a small data set. But it means we’ll want to take\\na look at the posterior predictions with that in mind. Second, while it’s no surprise that the\\nintercept-only model m11.9 has a worse score than the interaction model m11.10, it might\\nbe very surprising that the “effective number of parameters” pPSIS is actually larger for the\\nmodel with fewer parameters. Model m11.9 has only one parameter. Model m11.10 has four\\nparameters. This isn’t some weird thing about PSIS—WAIC tells you the same story. What\\nis going on here?\\nThe only place that model complexity—a model’s tendency to overfit—and parameter\\ncount have a clear relationship is in a simple linear regression with flat priors. Once a distri-\\nbution is bounded, for example, then parameter values near the boundary produce less over-\\nfitting than those far from the boundary. The same principle applies to data distributions.\\nAny count near zero is harder to overfit. So overfitting risk depends both upon structural\\ndetails of the model and the composition of the sample.\\n'},\n",
       " {'index': 371,\n",
       "  'number': 353,\n",
       "  'content': '11.2. POISSON REGRESSION\\n353\\nIn this sample, a major source of overfitting risk is the highly influential point flagged by\\nPSIS. Let’s plot the posterior predictions now, and I’ll scale and label the highly influential\\npoints with their Pareto k values. Here’s the code to plot the data and superimpose posterior\\npredictions for the expected number of tools at each population size and contact rate:\\nR code\\n11.47\\nk <- PSIS( m11.10 , pointwise=TRUE )$k\\nplot( dat$P , dat$T , xlab=\"log population (std)\" , ylab=\"total tools\" ,\\ncol=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,\\nylim=c(0,75) , cex=1+normalize(k) )\\n# set up the horizontal axis values to compute predictions at\\nns <- 100\\nP_seq <- seq( from=-1.4 , to=3 , length.out=ns )\\n# predictions for cid=1 (low contact)\\nlambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )\\nlmu <- apply( lambda , 2 , mean )\\nlci <- apply( lambda , 2 , PI )\\nlines( P_seq , lmu , lty=2 , lwd=1.5 )\\nshade( lci , P_seq , xpd=TRUE )\\n# predictions for cid=2 (high contact)\\nlambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )\\nlmu <- apply( lambda , 2 , mean )\\nlci <- apply( lambda , 2 , PI )\\nlines( P_seq , lmu , lty=1 , lwd=1.5 )\\nshade( lci , P_seq , xpd=TRUE )\\nThe result is shown in Figure 11.9. Open points are low contact societies. Filled points are\\nhigh contact societies. The points are scaled by their Pareto k values. The dashed curve is\\nthe low contact posterior mean. The solid curve is the high contact posterior mean.\\nThis plot is joined on its right by the same predictions shown on the natural scale, with\\nraw population sizes on the horizontal. The code to do that is very similar, but you need to\\nconvert the P_seq to the natural scale, by reversing the standardization, and then you can\\njust replace P_seq with the converted sequence in the lines and shade commands.\\nR code\\n11.48\\nplot( d$population , d$total_tools , xlab=\"population\" , ylab=\"total tools\" ,\\ncol=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,\\nylim=c(0,75) , cex=1+normalize(k) )\\nns <- 100\\nP_seq <- seq( from=-5 , to=3 , length.out=ns )\\n# 1.53 is sd of log(population)\\n# 9 is mean of log(population)\\npop_seq <- exp( P_seq*1.53 + 9 )\\nlambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )\\nlmu <- apply( lambda , 2 , mean )\\nlci <- apply( lambda , 2 , PI )\\n'},\n",
       " {'index': 372,\n",
       "  'number': 354,\n",
       "  'content': '354\\n11. GOD SPIKED THE INTEGERS\\n-1\\n0\\n1\\n2\\n0\\n20\\n40\\n60\\nlog population (std)\\ntotal tools\\nYap (0.6)\\nTrobriand (0.56)\\nTonga (0.69)\\nHawaii (1.01)\\n0\\n50000\\n150000\\n250000\\n0\\n20\\n40\\n60\\npopulation\\ntotal tools\\nFigure 11.9. Posterior predictions for the Oceanic tools model. Filled\\npoints are societies with historically high contact. Open points are those\\nwith low contact. Point size is scaled by relative PSIS Pareto k values. Larger\\npoints are more influential. The solid curve is the posterior mean for high\\ncontact societies. The dashed curve is the same for low contact societies.\\n89% compatibility intervals are shown by the shaded regions. Left: Stan-\\ndardized log population scale, as in the model code. Right: Same predic-\\ntions on the natural population scale.\\nlines( pop_seq , lmu , lty=2 , lwd=1.5 )\\nshade( lci , pop_seq , xpd=TRUE )\\nlambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )\\nlmu <- apply( lambda , 2 , mean )\\nlci <- apply( lambda , 2 , PI )\\nlines( pop_seq , lmu , lty=1 , lwd=1.5 )\\nshade( lci , pop_seq , xpd=TRUE )\\nHawaii (k = 1.01), Tonga (k = 0.69), Tap (k = 0.6), and the Trobriand Islands (k = 0.56)\\nare highly influential points. Most are not too influential, but Hawaii is very influential.\\nYou can see why in the figure: It has extreme population size and the most tools. This is\\nmost obvious on the natural scale. This doesn’t mean Hawaii is some “outlier” that should\\nbe dropped from the data. But it does mean that Hawaii strongly influences the posterior\\ndistribution. In the practice problems at the end of the chapter, I’ll ask you to drop Hawaii\\nand see what changes. For now, let’s do something much more interesting.\\nLook at the posterior predictions in Figure 11.9. Notice that the trend for societies with\\nhigh contact (solid) is higher than the trend for societies with low contact (dashed) when\\npopulation size is low, but then the model allows it to actually be smaller. The means cross\\none another at high population sizes. Of course the model is actually saying it has no idea\\nwhere the trend for high contact societies goes at high population sizes, because there are no\\nhigh population size societies with high contact. There is only low-contact Hawaii. But it is\\nstill a silly pattern that we know shouldn’t happen. A counter-factual Hawaii with the same\\n'},\n",
       " {'index': 373,\n",
       "  'number': 355,\n",
       "  'content': '11.2. POISSON REGRESSION\\n355\\n0\\n50000\\n150000\\n250000\\n20\\n30\\n40\\n50\\n60\\n70\\npopulation\\ntotal tools\\nlow contact\\nhigh contact\\nFigure 11.10. Posterior predictions for the\\nscientific model of the Oceanic tool counts.\\nCompare to the right-hand plot in Fig-\\nure 11.9. Since this model forces the trends to\\npass through the origin, as it must, its behavior\\nis more sensible, in addition to having param-\\neters with meaning outside a linear model.\\npopulation size but high contact should theoretically have at least as many tools as the real\\nHawaii. It shouldn’t have fewer.\\nThe model can produce this silly pattern, because it lets the intercept be a free parameter.\\nWhy is this bad? Because it means there is no guarantee that the trend for λ will pass through\\nthe origin where total tools equals zero and the population size equals zero. When there are\\nzero people, there are also zero tools! As population increases, tools increase. So we get the\\nintercept for free, if we stop and think.\\nLet’s stop and think. Instead of the conventional GLM above, we could use the predic-\\ntions of an actual model of the relationship between population size and tool kit complexity.\\nBy “actual model,” I mean a model constructed specifically from scientific knowledge and hy-\\npothetical causal effects. The downside of this is that it will feel less like statistics—suddenly\\ndomain-specific skills are relevant. The upside is that it will feel more like science.\\nWhat we want is a dynamic model of the cultural evolution of tools. Tools aren’t created\\nall at once. Instead they develop over time. Innovation adds them to a population. Processes\\nof loss remove them. The simplest model assumes that innovation is proportional to popu-\\nlation size, but with diminishing returns. Each additional person adds less innovation than\\nthe previous. It also assumes that tool loss is proportional to the number of tools, without\\ndiminishing returns. These forces balance to produce a tool kit of some size.\\nThe Overthinking box below presents the mathematical version of this model and shows\\nyou the code to build it in ulam. The model ends up in m11.11. Let’s call this the scientific\\nmodel and the previous m11.10 the geocentric model. Figure 11.10 shows the posterior\\npredictions for the scientific model, on the natural scale of population size. Comparing it\\nwith the analogous plot in Figure 11.9, notice that the trend for high contact societies always\\ntrends above the trend for low contact societies. Both trends always pass through the origin\\nnow, as they must. The scientific model is still far from perfect. But it provides a better\\nfoundation to learn from. The parameters have clearer meanings now. They aren’t just bits\\nof machinery in the bottom of a tide prediction engine.\\nYou might ask how the scientific model compares to the geocentric model. The expected\\naccuracy out of sample, whether you use PSIS or WAIC, is a few points better than the geo-\\ncentric model. It is still tugged around by Hawaii and Tonga. We’ll return to these data in a\\nlater chapter and approach contact rate a different way, by taking account of how close these\\nsocieties are to one another.\\n'},\n",
       " {'index': 374,\n",
       "  'number': 356,\n",
       "  'content': '356\\n11. GOD SPIKED THE INTEGERS\\nOverthinking: Modeling tool innovation. Taking the verbal model in the main text above, we can\\nwrite that the change in the expected number of tools in one time step is:\\n∆T = αPβ −γT\\nwhere P is the population size, T is the number of tools, and α, β, and γ are parameters to be estimated.\\nTo find an equilibrium number of tools T, just set ∆T = 0 and solve for T. This yields:\\nˆT = αPβ\\nγ\\nWe’re going to use this inside a Poisson model now. The noise around the outcome will still be Poisson,\\nbecause that is still the maximum entropy distribution in this context—total_tools is a count with\\nno clear upper bound. But the linear model is gone:\\nTi ∼Poisson(λi)\\nλi = αPβ\\ni /γ\\nNotice that there is no link function! All we have to do to ensure that λ remains positive is to make\\nsure the parameters are positive. In the code below, I’ll use exponential priors for β and γ and a log-\\nNormal for α. Then they all have to be positive. In building the model, we also want to allow some or\\nall of the parameters to vary by contact rate. Since contact rate is supposed to mediate the influence of\\npopulation size, let’s allow α and β. It could also influence γ, because trade networks might prevent\\ntools from vanishing over time. But we’ll leave that as an exercise for the reader. Here’s the code:\\nR code\\n11.49\\ndat2 <- list( T=d$total_tools, P=d$population, cid=d$contact_id )\\nm11.11 <- ulam(\\nalist(\\nT ~ dpois( lambda ),\\nlambda <- exp(a[cid])*P^b[cid]/g,\\na[cid] ~ dnorm(1,1),\\nb[cid] ~ dexp(1),\\ng ~ dexp(1)\\n), data=dat2 , chains=4 , log_lik=TRUE )\\nI’ve invented the exact priors behind the scenes. Let’s not get distracted with those. I encourage you\\nto play around. The lesson here is in how we build in the predictor variables. Using prior simulations\\nto design the priors is the same, although easier now that the parameters mean something. Finally,\\nthe code to produce posterior predictions is no different than the code in the main text used to plot\\npredictions for m11.10.\\n11.2.2. Negativebinomial(gamma-Poisson)models. Typically there is a lot of unexplained\\nvariation in Poisson models. Presumably this additional variation arises from unobserved\\ninfluences that vary from case to case, generating variation in the true λ’s. Ignoring this vari-\\nation, or rate heterogeneity, can cause confounds just like it can for binomial models. So a\\nvery common extension of Poisson GLMs is to swap the Poisson distribution for something\\ncalled the negative binomial distribution. This is really a Poisson distribution in disguise,\\nand it is also sometimes called the gamma-Poisson distribution for this reason. It is a Pois-\\nson in disguise, because it is a mixture of different Poisson distributions. This is the Poisson\\nanalogue of the Student-t model, which is a mixture of different normal distributions. We’ll\\nwork with mixtures in the next chapter.\\n'},\n",
       " {'index': 375,\n",
       "  'number': 357,\n",
       "  'content': '11.2. POISSON REGRESSION\\n357\\n11.2.3. Example: Exposure and the offset. The parameter λ is the expected value of a Pois-\\nson model, but it’s also commonly thought of as a rate. Both interpretations are correct, and\\nrealizing this allows us to make Poisson models for which the exposure varies across cases\\ni. Suppose for example that a neighboring monastery performs weekly totals of completed\\nmanuscripts while your monastery does daily totals. If you come into possession of both\\nsets of records, how could you analyze both in the same model, given that the counts are\\naggregated over different amounts of time, different exposures?\\nHere’s how. Implicitly, λ is equal to an expected number of events, µ, per unit time or\\ndistance, τ. This implies that λ = µ/τ, which lets us redefine the link:\\nyi ∼Poisson(λi)\\nlog λi = log µi\\nτi\\n= α + βxi\\nSince the logarithm of a ratio is the same as a difference of logarithms, we can also write:\\nlog λi = log µi −log τi = α + βxi\\nThese τ values are the “exposures.” So if different observations i have different exposures,\\nthen this implies that the expected value on row i is given by:\\nlog µi = log τi + α + βxi\\nWhen τi = 1, then log τi = 0 and we’re back where we started. But when the exposure varies\\nacross cases, then τi does the important work of correctly scaling the expected number of\\nevents for each case i. So you can model cases with different exposures just by writing a\\nmodel like:\\nyi ∼Poisson(µi)\\nlog µi = log τi + α + βxi\\nwhere τ is a column in the data. So this is just like adding a predictor, the logarithm of the\\nexposure, without adding a parameter for it. There will be an example later in this section.\\nYou can also put a parameter in front of log τi, which is one way to model the hypothesis that\\nthe rate is not constant with time.\\nFor the last Poisson example, we’ll look at a case where the exposure varies across obser-\\nvations. When the length of observation, area of sampling, or intensity of sampling varies,\\nthe counts we observe also naturally vary. Since a Poisson distribution assumes that the rate\\nof events is constant in time (or space), it’s easy to handle this. All we need to do, as ex-\\nplained above, is to add the logarithm of the exposure to the linear model. The term we add\\nis typically called an offset.\\nWe’ll simulate for this example, both to provide another example of dummy-data sim-\\nulation as well as to ensure we get the right answer from the offset approach. Suppose,\\nas we did earlier, that you own a monastery. The data available to you about the rate at\\nwhich manuscripts are completed is totaled up each day. Suppose the true rate is λ = 1.5\\nmanuscripts per day. We can simulate a month of daily counts:\\nR code\\n11.50\\nnum_days <- 30\\ny <- rpois( num_days , 1.5 )\\nSo now y holds 30 days of simulated counts of completed manuscripts.\\n'},\n",
       " {'index': 376,\n",
       "  'number': 358,\n",
       "  'content': '358\\n11. GOD SPIKED THE INTEGERS\\nAlso suppose that your monastery is turning a tidy profit, so you are considering pur-\\nchasing another monastery. Before purchasing, you’d like to know how productive the new\\nmonastery might be. Unfortunately, the current owners don’t keep daily records, so a head-\\nto-head comparison of the daily totals isn’t possible. Instead, the owners keep weekly totals.\\nSuppose the daily rate at the new monastery is actually λ = 0.5 manuscripts per day. To\\nsimulate data on a weekly basis, we just multiply this average by 7, the exposure:\\nR code\\n11.51\\nnum_weeks <- 4\\ny_new <- rpois( num_weeks , 0.5*7 )\\nAnd new y_new holds four weeks of counts of completed manuscripts.\\nTo analyze both y, totaled up daily, and y_new, totaled up weekly, we just add the loga-\\nrithm of the exposure to linear model. First, let’s build a data frame to organize the counts\\nand help you see the exposure for each case:\\nR code\\n11.52\\ny_all <- c( y , y_new )\\nexposure <- c( rep(1,30) , rep(7,4) )\\nmonastery <- c( rep(0,30) , rep(1,4) )\\nd <- data.frame( y=y_all , days=exposure , monastery=monastery )\\nTake a look at d and confirm that there are three columns: The observed counts are in y, the\\nnumber of days each count was totaled over are in days, and the new monastery is indicated\\nby monastery.\\nTo fit the model, and estimate the rate of manuscript production at each monastery, we\\njust compute the log of each exposure and then include that variable in linear model. This\\ncode will do the job:\\nR code\\n11.53\\n# compute the offset\\nd$log_days <- log( d$days )\\n# fit the model\\nm11.12 <- quap(\\nalist(\\ny ~ dpois( lambda ),\\nlog(lambda) <- log_days + a + b*monastery,\\na ~ dnorm( 0 , 1 ),\\nb ~ dnorm( 0 , 1 )\\n), data=d )\\nTo compute the posterior distributions of λ in each monastery, we sample from the posterior\\nand then just use the linear model, but without the offset now. We don’t use the offset again,\\nwhen computing predictions, because the parameters are already on the daily scale, for both\\nmonasteries.\\nR code\\n11.54\\npost <- extract.samples( m11.12 )\\nlambda_old <- exp( post$a )\\nlambda_new <- exp( post$a + post$b )\\nprecis( data.frame( lambda_old , lambda_new ) )\\n'},\n",
       " {'index': 377,\n",
       "  'number': 359,\n",
       "  'content': \"11.3. MULTINOMIAL AND CATEGORICAL MODELS\\n359\\n'data.frame': 10000 obs. of 2 variables:\\nmean\\nsd 5.5% 94.5%\\nhistogram\\nlambda_old 1.34 0.21 1.03\\n1.70\\n▁▁▃▇▅▂▁▁▁\\nlambda_new 0.52 0.14 0.33\\n0.77 ▁▁▃▇▇▃▂▁▁▁▁▁▁\\nThe new monastery produces about half as many manuscripts per day. So you aren’t going\\nto pay that much for it.\\n11.3. Multinomial and categorical models\\nThe binomial distribution is relevant when there are only two things that can happen,\\nand we count those things. In general, more than two things can happen. For example, recall\\nthe bag of marbles from way back in Chapter 2. It contained only blue and white marbles.\\nBut suppose we introduce red marbles as well. Now each draw from the bag can be one of\\nthree categories, and the count that accumulates is across all three categories. So we end up\\nwith a count of blue, white, and red marbles.\\nWhen more than two types of unordered events are possible, and the probability of each\\ntype of event is constant across trials, then the maximum entropy distribution is the multi-\\nnomial distribution. You already met the multinomial, implicitly, in Chapter 10 when\\nwe tossed pebbles into buckets as an introduction to maximum entropy. The binomial is\\nreally a special case of this distribution. And so its distribution formula resembles the bino-\\nmial, just extrapolated out to three or more types of events. If there are K types of events\\nwith probabilities p1, ..., pK, then the probability of observing y1, ..., yK events of each type\\nout of n total trials is:\\nPr(y1, ..., yK|n, p1, ..., pK) =\\nn!\\nQ\\ni yi!\\nK\\nY\\ni=1\\npyi\\ni\\nThe fraction with n! on top just expresses the number of different orderings that give the\\nsame counts y1, ..., yK. It’s the famous multiplicity from the previous chapter.\\nA model built on a multinomial distribution may also be called a categorical regres-\\nsion, usually when each event is isolated on a single row, like with logistic regression. In\\nmachine learning, this model type is sometimes known as the maximum entropy clas-\\nsifier. Building a generalized linear model from a multinomial likelihood is complicated,\\nbecause as the event types multiply, so too do your modeling choices. And there are two dif-\\nferent approaches to constructing the likelihoods, as well. The first is based directly on the\\nmultinomial likelihood and uses a generalization of the logit link. I’ll show you an example\\nof this approach, which I’ll call the explicit approach. The second approach transforms the\\nmultinomial likelihood into a series of Poisson likelihoods, oddly enough. I’ll introduce that\\napproach after I introduce Poisson GLMs.\\nThe conventional and natural link in this context is the multinomiallogit, also known\\nas the softmax function. This link function takes a vector of scores, one for each of K event\\ntypes, and computes the probability of a particular type of event k as:\\nPr(k|s1, s2, ..., sK) =\\nexp(sk)\\nPK\\ni=1 exp(si)\\nThe rethinking package provides this link as the softmax function. Combined with this\\nconventional link, this type of GLM may be called multinomial logistic regression.\\nThe biggest issue is what to do with the multiple linear models. In a binomial GLM, you\\ncan pick either of the two possible events and build a single linear model for its log odds.\\n\"},\n",
       " {'index': 378,\n",
       "  'number': 360,\n",
       "  'content': '360\\n11. GOD SPIKED THE INTEGERS\\nThe other event is handled automatically. But in a multinomial (or categorical) GLM, you\\nneed K −1 linear models for K types of events. One of the outcome values is chosen as a\\n“pivot” and the others are modeled relative to it. In each of the K −1 linear models, you can\\nuse any predictors and parameters you like—they don’t have to be the same, and there are\\noften good reasons for them to be different. In the special case of two types of events, none\\nof these choices arise, because there is only one linear model. And that’s why the binomial\\nGLM is so much easier.\\nThere are two basic cases: (1) predictors have different values for different values of the\\noutcome, and (2) parameters are distinct for each value of the outcome. The first case is\\nuseful when each type of event has its own quantitative traits, and you want to estimate the\\nassociation between those traits and the probability each type of event appears in the data.\\nThe second case is useful when you are interested instead in features of some entity that\\nproduces each event, whatever type it turns out to be. Let’s consider each case separately and\\ntalk through an empirically motivated example of each. You can mix both cases in the same\\nmodel. But it’ll be easier to grasp the distinction in pure examples of each.\\nI’m going to build the models in this section with pure Stan code. We could make the\\nmodels with quap or ulam. But using Stan directly will provide some additional clarity about\\nthe data structures needed to manage multiple, simultaneous linear models. It will also make\\nit easier for you to modify these models for your purposes, including adding varying effects\\nand other gizmos later on.179\\n11.3.1. Predictors matched to outcomes. For example, suppose you are modeling choice\\nof career for a number of young adults. One of the relevant predictor variables is expected\\nincome. In that case, the same parameter βincome appears in each linear model, in order to\\nestimate the impact of the income trait on the probability a career is chosen. But a different\\nincome value multiplies the parameter in each linear model.\\nHere’s a simulated example in R code. This code simulates career choice from three\\ndifferent careers, each with its own income trait. These traits are used to assign a score to each\\ntype of event. Then when the model is fit to the data, one of these scores is held constant, and\\nthe other two scores are estimated, using the known income traits. It is a little confusing. Step\\nthrough the implementation, and it’ll make more sense. First, we simulate career choices:\\nR code\\n11.55\\n# simulate career choices among 500 individuals\\nN <- 500\\n# number of individuals\\nincome <- c(1,2,5)\\n# expected income of each career\\nscore <- 0.5*income\\n# scores for each career, based on income\\n# next line converts scores to probabilities\\np <- softmax(score[1],score[2],score[3])\\n# now simulate choice\\n# outcome career holds event type values, not counts\\ncareer <- rep(NA,N)\\n# empty vector of choices for each individual\\n# sample chosen career for each individual\\nset.seed(34302)\\nfor ( i in 1:N ) career[i] <- sample( 1:3 , size=1 , prob=p )\\nTo fit the model to these fake data, we use the dcategorical likelihood, which is the multi-\\nnomial logistic regression distribution. It works when each value in the outcome variable,\\n'},\n",
       " {'index': 379,\n",
       "  'number': 361,\n",
       "  'content': '11.3. MULTINOMIAL AND CATEGORICAL MODELS\\n361\\nhere career, contains the individual event types on each row. To convert all the scores to\\nprobabilities, we’ll use the multinomial logit link, which is called softmax. Then each pos-\\nsible career gets its own linear model with its own features. There are no intercepts in the\\nsimulation above. But if income doesn’t predict career choice, you still want an intercept to\\naccount for differences in frequency. Here’s the code:\\nR code\\n11.56\\ncode_m11.13 <- \"\\ndata{\\nint N; // number of individuals\\nint K; // number of possible careers\\nint career[N]; // outcome\\nvector[K] career_income;\\n}\\nparameters{\\nvector[K-1] a; // intercepts\\nreal<lower=0> b; // association of income with choice\\n}\\nmodel{\\nvector[K] p;\\nvector[K] s;\\na ~ normal( 0 , 1 );\\nb ~ normal( 0 , 0.5 );\\ns[1] = a[1] + b*career_income[1];\\ns[2] = a[2] + b*career_income[2];\\ns[3] = 0; // pivot\\np = softmax( s );\\ncareer ~ categorical( p );\\n}\\n\"\\nThen we set up the data list and invoke stan:\\nR code\\n11.57\\ndat_list <- list( N=N , K=3 , career=career , career_income=income )\\nm11.13 <- stan( model_code=code_m11.13 , data=dat_list , chains=4 )\\nprecis( m11.13 , 2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\na[1]\\n0.06 0.21 -0.31\\n0.37\\n423\\n1\\na[2] -0.49 0.38 -1.19\\n0.04\\n435\\n1\\nb\\n0.27 0.19\\n0.02\\n0.61\\n460\\n1\\nYou might have gotten some divergent transitions above. Can you figure out why?\\nBe aware that the estimates you get from these models are extraordinarily difficult to\\ninterpret. Since the parameters are relative to the pivot outcome value, they could end up\\npositive or negative, depending upon the context. In the example above, I chose the last\\noutcome type, the third career. If you choose another, you’ll get different estimates, but the\\nsame predictions on the outcome scale. It really is a tide prediction engine. So you absolutely\\nmust convert them to a vector of probabilities to make much sense of them. However, in this\\ncase, it’s clear that the coefficient on career income b is positive. It’s just not clear at all how\\nbig the effect is.\\n'},\n",
       " {'index': 380,\n",
       "  'number': 362,\n",
       "  'content': \"362\\n11. GOD SPIKED THE INTEGERS\\nTo conduct a counterfactual simulation, we can extract the samples and make our own.\\nThe goal is to compare a counterfactual career in which the income is changed. How much\\ndoes the probability change, in the presence of these competing careers? This is a subtle kind\\nof question, because the probability change always depends upon the other choices. So let’s\\nimagine doubling the income of career 2 above:\\nR code\\n11.58\\npost <- extract.samples( m11.13 )\\n# set up logit scores\\ns1 <- with( post , a[,1] + b*income[1] )\\ns2_orig <- with( post , a[,2] + b*income[2] )\\ns2_new <- with( post , a[,2] + b*income[2]*2 )\\n# compute probabilities for original and counterfactual\\np_orig <- sapply( 1:length(post$b) , function(i)\\nsoftmax( c(s1[i],s2_orig[i],0) ) )\\np_new <- sapply( 1:length(post$b) , function(i)\\nsoftmax( c(s1[i],s2_new[i],0) ) )\\n# summarize\\np_diff <- p_new[2,] - p_orig[2,]\\nprecis( p_diff )\\n'data.frame': 4000 obs. of 1 variables:\\nmean\\nsd 5.5% 94.5%\\nhistogram\\np_diff 0.13 0.09 0.01\\n0.29 ▇▇▅▅▃▂▁▁▁▁\\nSo on average a 13% increase in probability of choosing the career, when the income is dou-\\nbled. Note that value is conditional on comparing to the other careers in the calculation.\\nThese models do not produce predictions independent of a specific set of options. That’s not\\na bug. That’s just how choice works.\\n11.3.2. Predictors matched to observations. Now consider an example in which each ob-\\nserved outcome has unique predictor values. Suppose you are still modeling career choice.\\nBut now you want to estimate the association between each person’s family income and\\nwhich career they choose. So the predictor variable must have the same value in each lin-\\near model, for each row in the data. But now there is a unique parameter multiplying it in\\neach linear model. This provides an estimate of the impact of family income on choice, for\\neach type of career.\\nR code\\n11.59\\nN <- 500\\n# simulate family incomes for each individual\\nfamily_income <- runif(N)\\n# assign a unique coefficient for each type of event\\nb <- c(-2,0,2)\\ncareer <- rep(NA,N)\\n# empty vector of choices for each individual\\nfor ( i in 1:N ) {\\nscore <- 0.5*(1:3) + b*family_income[i]\\np <- softmax(score[1],score[2],score[3])\\ncareer[i] <- sample( 1:3 , size=1 , prob=p )\\n\"},\n",
       " {'index': 381,\n",
       "  'number': 363,\n",
       "  'content': '11.3. MULTINOMIAL AND CATEGORICAL MODELS\\n363\\n}\\ncode_m11.14 <- \"\\ndata{\\nint N; // number of observations\\nint K; // number of outcome values\\nint career[N]; // outcome\\nreal family_income[N];\\n}\\nparameters{\\nvector[K-1] a; // intercepts\\nvector[K-1] b; // coefficients on family income\\n}\\nmodel{\\nvector[K] p;\\nvector[K] s;\\na ~ normal(0,1.5);\\nb ~ normal(0,1);\\nfor ( i in 1:N ) {\\nfor ( j in 1:(K-1) ) s[j] = a[j] + b[j]*family_income[i];\\ns[K] = 0; // the pivot\\np = softmax( s );\\ncareer[i] ~ categorical( p );\\n}\\n}\\n\"\\ndat_list <- list( N=N , K=3 , career=career , family_income=family_income )\\nm11.14 <- stan( model_code=code_m11.14 , data=dat_list , chains=4 )\\nprecis( m11.14 , 2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\na[1] -1.41 0.28 -1.88 -0.97\\n2263\\n1\\na[2] -0.64 0.20 -0.96 -0.33\\n2163\\n1\\nb[1] -2.72 0.60 -3.69 -1.79\\n2128\\n1\\nb[2] -1.72 0.39 -2.32 -1.10\\n2183\\n1\\nAgain, computing implied predictions is the safest way to interpret these models. They do\\na great job of classifying discrete, unordered events. But the parameters are on a scale that\\nis very hard to interpret. In this case, b[2] ended up negative, because it is relative to the\\npivot, for which family income has a positive effect. If you produce posterior predictions on\\nthe probability scale, you’ll see this.\\n11.3.3. Multinomial in disguise as Poisson. Another way to fit a multinomial/categorical\\nmodel is to refactor it into a series of Poisson likelihoods.180 That should sound a bit crazy.\\nBut it’s actually both principled and commonplace to model multinomial outcomes this way.\\nIt’s principled, because the mathematics justifies it. And it’s commonplace, because it is usu-\\nally computationally easier to use Poisson rather than multinomial likelihoods. Here I’ll give\\nan example of an implementation. For the mathematical details of the transformation, see\\nthe Overthinking box at the end.\\n'},\n",
       " {'index': 382,\n",
       "  'number': 364,\n",
       "  'content': \"364\\n11. GOD SPIKED THE INTEGERS\\nI appreciate that this kind of thing—modeling the same data different ways but getting\\nthe same inferences—is exactly the kind of thing that makes statistics maddening for sci-\\nentists. So I’ll begin by taking a binomial example from earlier in the chapter and doing it\\nover as a Poisson regression. Since the binomial is just a special case of the multinomial,\\nthe approach extrapolates to any number of event types. Think again of the UC Berkeley\\nadmissions data. Let’s load it again:\\nR code\\n11.60\\nlibrary(rethinking)\\ndata(UCBadmit)\\nd <- UCBadmit\\nNow let’s use a Poisson regression to model both the rate of admission and the rate of re-\\njection. And we’ll compare the inference to the binomial model’s probability of admission.\\nHere are both the binomial and Poisson models:\\nR code\\n11.61\\n# binomial model of overall admission probability\\nm_binom <- quap(\\nalist(\\nadmit ~ dbinom(applications,p),\\nlogit(p) <- a,\\na ~ dnorm( 0 , 1.5 )\\n), data=d )\\n# Poisson model of overall admission rate and rejection rate\\n# 'reject' is a reserved word in Stan, cannot use as variable name\\ndat <- list( admit=d$admit , rej=d$reject )\\nm_pois <- ulam(\\nalist(\\nadmit ~ dpois(lambda1),\\nrej ~ dpois(lambda2),\\nlog(lambda1) <- a1,\\nlog(lambda2) <- a2,\\nc(a1,a2) ~ dnorm(0,1.5)\\n), data=dat , chains=3 , cores=3 )\\nLet’s consider just the posterior means, for the sake of simplicity. But keep in mind that the\\nentire posterior is what matters. First, the inferred binomial probability of admission, across\\nthe entire data set, is:\\nR code\\n11.62\\ninv_logit(coef(m_binom))\\na\\n0.3877596\\nAnd in the Poisson model, the implied probability of admission is given by:\\npadmit =\\nλ1\\nλ1 + λ2\\n=\\nexp(a1)\\nexp(a1) + exp(a2)\\nIn code form:\\n\"},\n",
       " {'index': 383,\n",
       "  'number': 365,\n",
       "  'content': \"11.4. SUMMARY\\n365\\nR code\\n11.63\\nk <- coef(m_pois)\\na1 <- k['a1']; a2 <- k['a2']\\nexp(a1)/(exp(a1)+exp(a2))\\n[1] 0.3872366\\nThat’s the same inference as in the binomial model. These days, you can just as easily use a\\ncategorical distribution, as in the previous section. But sometimes this Poisson factorization\\nis easier. And you might encounter it elsewhere. So it’s good to know that it’s not insane.\\nOverthinking: Multinomial-Poisson transformation. The Poisson distribution was introduced ear-\\nlier in this chapter. The Poisson probability of y1 events of type 1, assuming a rate λ1, is given by:\\nPr(y1|λ1) = e−λ1λy1\\n1\\ny1!\\nI’ll show you a magic trick for extracting this expression from the multinomial probability expres-\\nsion. The multinomial probability is just an extrapolation of the binomial to more than two types of\\nevents. So we’ll work here with the binomial distribution, but in multinomial form, just to make the\\nderivation a little easier. The probability of counts y1 and y2 for event types 1 and 2 with probabilities\\np1 and p2, respectively, out of n trials, is:\\nPr(y1, y2|n, p1, p2) =\\nn!\\ny1!y2!py1\\n1 py2\\n2\\nWe need some definitions now. Let Λ = λ1 +λ2, p1 = λ1/Λ, and p2 = λ2/Λ. Substituting these into\\nthe binomial probability:\\nPr(y1, y2|n, λ1, λ2) =\\nn!\\ny1!y2!\\n\\x12λ1\\nΛ\\n\\x13y1 \\x12λ2\\nΛ\\n\\x13y2\\n=\\nn!\\nΛy1Λy2\\nλy1\\n1\\ny1!\\nλy2\\n2\\ny2! = n!\\nΛn\\nλy1\\n1\\ny1!\\nλy2\\n2\\ny2!\\nNow we simultaneously multiply and divide by both e−λ1 and e−λ2, then perform some strategic\\nrearrangement:\\nPr(y1, y2|n, λ1, λ2) = n!\\nΛn\\ne−λ1\\ne−λ1\\nλy1\\n1\\ny1!\\ne−λ2\\ne−λ2\\nλy2\\n2\\ny2! =\\nn!\\nΛne−λ1e−λ2\\ne−λ1λy1\\n1\\ny1!\\ne−λ2λy2\\n2\\ny2!\\n=\\nn!\\ne−ΛΛn\\n| {z }\\nPr(n)−1\\ne−λ1λy1\\n1\\ny1!\\n| {z }\\nPr(y1)\\ne−λ2λy2\\n2\\ny2!\\n| {z }\\nPr(y2)\\nThe final expression is the product of the Poisson probabilities Pr(y1) and Pr(y2), divided by the\\nPoisson probability of n, Pr(n). It makes sense that the product is divided by Pr(n), because this is a\\nconditional probability for y1 and y2. All of this means that if there are k event types, you can model\\nmultinomial probabilities p1, ..., pk using Poisson rate parameters λ1, ..., λk. And you can recover the\\nmultinomial probabilities using the definition pi = λi/ P\\nj λj.\\n11.4. Summary\\nThis chapter described some of the most common generalized linear models, those used\\nto model counts. It is important to never convert counts to proportions before analysis,\\nbecause doing so destroys information about sample size. A fundamental difficulty with\\nthese models is that parameters are on a different scale, typically log-odds (for binomial)\\nor log-rate (for Poisson), than the outcome variable they describe. Therefore computing\\nimplied predictions is even more important than before.\\n\"},\n",
       " {'index': 384,\n",
       "  'number': 366,\n",
       "  'content': '366\\n11. GOD SPIKED THE INTEGERS\\n11.5. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n11E1. If an event has probability 0.35, what are the log-odds of this event?\\n11E2. If an event has log-odds 3.2, what is the probability of this event?\\n11E3. Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about\\nthe proportional change in odds of the outcome?\\n11E4. Why do Poisson regressions sometimes require the use of an offset? Provide an example.\\n11M1. As explained in the chapter, binomial data can be organized in aggregated and disaggregated\\nforms, without any impact on inference. But the likelihood of the data does change when the data are\\nconverted between the two formats. Can you explain why?\\n11M2. If a coefficient in a Poisson regression has value 1.7, what does this imply about the change\\nin the outcome?\\n11M3. Explain why the logit link is appropriate for a binomial generalized linear model.\\n11M4. Explain why the log link is appropriate for a Poisson generalized linear model.\\n11M5. What would it imply to use a logit link for the mean of a Poisson generalized linear model?\\nCan you think of a real research problem for which this would make sense?\\n11M6. State the constraints for which the binomial and Poisson distributions have maximum en-\\ntropy. Are the constraints different at all for binomial and Poisson? Why or why not?\\n11M7. Use quap to construct a quadratic approximate posterior distribution for the chimpanzee\\nmodel that includes a unique intercept for each actor, m11.4 (page 330). Compare the quadratic\\napproximation to the posterior distribution produced instead from MCMC. Can you explain both\\nthe differences and the similarities between the approximate and the MCMC distributions? Relax the\\nprior on the actor intercepts to Normal(0,10). Re-estimate the posterior using both ulam and quap.\\nDo the differences increase or decrease? Why?\\n11M8. Revisit the data(Kline) islands example. This time drop Hawaii from the sample and refit\\nthe models. What changes do you observe?\\n11H1. Use WAIC or PSIS to compare the chimpanzee model that includes a unique intercept for\\neach actor, m11.4 (page 330), to the simpler models fit in the same section. Interpret the results.\\n11H2. The data contained in library(MASS);data(eagles) are records of salmon pirating at-\\ntempts by Bald Eagles in Washington State. See ?eagles for details. While one eagle feeds, some-\\ntimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the “victim” and\\nthe thief the “pirate.” Use the available data to build a binomial GLM of successful pirating attempts.\\n(a) Consider the following model:\\nyi ∼Binomial(ni, pi)\\nlogit(pi) = α + βPPi + βVVi + βAAi\\nα ∼Normal(0, 1.5)\\nβP, βV, βA ∼Normal(0, 0.5)\\nwhere y is the number of successful attempts, n is the total number of attempts, P is a dummy variable\\nindicating whether or not the pirate had large body size, V is a dummy variable indicating whether\\nor not the victim had large body size, and finally A is a dummy variable indicating whether or not\\n'},\n",
       " {'index': 385,\n",
       "  'number': 367,\n",
       "  'content': '11.5. PRACTICE\\n367\\nthe pirate was an adult. Fit the model above to the eagles data, using both quap and ulam. Is the\\nquadratic approximation okay?\\n(b) Now interpret the estimates. If the quadratic approximation turned out okay, then it’s okay\\nto use the quap estimates. Otherwise stick to ulam estimates. Then plot the posterior predictions.\\nCompute and display both (1) the predicted probability of success and its 89% interval for each row (i)\\nin the data, as well as (2) the predicted success count and its 89% interval. What different information\\ndoes each type of posterior prediction provide?\\n(c) Now try to improve the model. Consider an interaction between the pirate’s size and age\\n(immature or adult). Compare this model to the previous one, using WAIC. Interpret.\\n11H3. The data contained in data(salamanders) are counts of salamanders (Plethodon elongatus)\\nfrom 47 different 49-m2 plots in northern California.181 The column SALAMAN is the count in each\\nplot, and the columns PCTCOVER and FORESTAGE are percent of ground cover and age of trees in the\\nplot, respectively. You will model SALAMAN as a Poisson variable.\\n(a) Model the relationship between density and percent cover, using a log-link (same as the ex-\\nample in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic\\napproximation again, by comparing quap to ulam. Then plot the expected counts and their 89% in-\\nterval against percent cover. In which ways does the model do a good job? A bad job?\\n(b) Can you improve the model by using the other predictor, FORESTAGE? Try any models you\\nthink useful. Can you explain why FORESTAGE helps or does not help with prediction?\\n11H4. The data in data(NWOGrants) areoutcomes for scientific fundingapplications fortheNether-\\nlands Organization for Scientific Research (NWO) from 2010–2012 (see van der Lee and Ellemers\\n(2015) for data and context). These data have a very similar structure to the UCBAdmit data discussed\\nin the chapter. I want you to consider a similar question: What are the total and indirect causal ef-\\nfects of gender on grant awards? Consider a mediation path (a pipe) through discipline. Draw the\\ncorresponding DAG and then use one or more binomial GLMs to answer the question. What is your\\ncausal interpretation? If NWO’s goal is to equalize rates of funding between men and women, what\\ntype of intervention would be most effective?\\n11H5. Suppose that the NWO Grants sample has an unobserved confound that influences both\\nchoice of discipline and the probability of an award. One example of such a confound could be the\\ncareer stage of each applicant. Suppose that in some disciplines, junior scholars apply for most of the\\ngrants. In other disciplines, scholars from all career stages compete. As a result, career stage influences\\ndiscipline as well as the probability of being awarded a grant. Add these influences to your DAG from\\nthe previous problem. What happens now when you condition on discipline? Does it provide an\\nun-confounded estimate of the direct path from gender to an award? Why or why not? Justify your\\nanswer with the backdoor criterion. If you have trouble thinking this though, try simulating fake\\ndata, assuming your DAG is true. Then analyze it using the model from the previous problem. What\\ndo you conclude? Is it possible for gender to have a real direct causal influence but for a regression\\nconditioning on both gender and discipline to suggest zero influence?\\n11H6. The data in data(Primates301) are 301 primate species and associated measures. In this\\nproblem, you will consider how brain size is associated with social learning. There are three parts.\\n(a) Model the number of observations of social_learning for each species as a function of the\\nlog brain size. Use a Poisson distribution for the social_learning outcome variable. Interpret the\\nresulting posterior. (b) Some species are studied much more than others. So the number of reported\\ninstances of social_learning could be a product of research effort. Use the research_effort\\nvariable, specifically its logarithm, as an additional predictor variable. Interpret the coefficient for log\\nresearch_effort. How does this model differ from the previous one? (c) Draw a DAG to represent\\nhow you think the variables social_learning, brain, and research_effort interact. Justify the\\nDAG with the measured associations in the two models above (and any other models you used).\\n'},\n",
       " {'index': 386, 'number': 368, 'content': ''},\n",
       " {'index': 387,\n",
       "  'number': 369,\n",
       "  'content': '12 Monsters and Mixtures\\nIn Hawaiian legend, Nanaue was the son of a shark who fell in love with a human. He\\ngrew into a murderous man with a shark mouth in the middle of his back. In Greek legend,\\nthe minotaur was a man with the head of a bull. He was the spawn of a human mother and\\na bull father. The gryphon is a legendary monster that is part eagle and part lion. Maori\\nlegends speak of Taniwha, monsters with features of serpents and birds and even sharks,\\nmuch like the dragons of Chinese and European mythology.\\nBy piecing together parts of different creatures, it’s easy to make a monster. Many mon-\\nsters are hybrids. Many statistical models are too. This chapter is about constructing likeli-\\nhood and link functions by piecing together the simpler components of previous chapters.\\nLike legendary monsters, these hybrid likelihoods contain pieces of other model types. En-\\ndowed with some properties of each piece, they help us model outcome variables with in-\\nconvenient, but common, properties. Being monsters, these models are both powerful and\\ndangerous. They are often harder to estimate and to understand. But with some knowledge\\nand caution, they are important tools.\\nWe’ll consider three common and useful examples. The first are models for handling\\nover-dispersion. These models extend the binomial and Poisson models of the previous\\nchapter to cope a bit with unmeasured sources of variation. The second type is a family of\\nzero-inflated and zero-augmented models, each of which mixes a binary event with\\nan ordinary GLM likelihood like a Poisson or binomial. The third type is the ordered\\ncategorical model, useful for categorical outcomes with a fixed ordering. This model is\\nbuilt by merging a categorical likelihood function with a special kind of link function, usually\\na cumulative link. We’ll also learn how to construct ordered categorical predictors.\\nThese model types help us transform our modeling to cope with the inconvenient real-\\nities of measurement, rather than transforming measurements to cope with the constraints\\nof our models. There are lots of other model types that arise for this purpose and in this way,\\nby mixing bits of simpler models together. We can’t possibly cover them all. But when you\\nencounter a new type, at least you’ll have a framework in which to understand it. And if you\\never need to construct your own unique monster, feel free to do so. Just be sure to validate it\\nby simulating dummy data and then recovering the data-generating process through fitting\\nthe model to the dummy data.\\n12.1. Over-dispersed counts\\nIn an earlier chapter (Chapter 7), I argued that models based on normal distributions can\\nbe overly sensitive to extreme observations. The problem isn’t necessarily that “outliers” are\\nbad data. Rather processes are often variable mixtures and this results in thicker tails. Models\\n369\\n'},\n",
       " {'index': 388,\n",
       "  'number': 370,\n",
       "  'content': '370\\n12. MONSTERS AND MIXTURES\\nthat assume a thin tail, like a pure Gaussian model, can be easily excited. Using something\\nlike a Student-t instead can produce better inference and out-of-sample predictions.\\nThe same goes for count models. When counts arise from a mixture of different pro-\\ncesses, then there may be more variation—thicker tails—than a pure count model expects.\\nThis can again lead to overly excited models. When counts are more variable than a pure\\nprocess, they exhibit over-dispersion. The variance of a variable is sometimes called its\\ndispersion. For a counting process like a binomial, the variance is a function of the same\\nparameters as the expected value. For example, the expected value of a binomial is Np and\\nits variance is Np(1 −p). When the observed variance exceeds this amount—after condi-\\ntioning on all the predictor variables—this implies that some omitted variable is producing\\nadditional dispersion in the observed counts.\\nThat isn’t necessarily bad. Such a model could still produce perfectly good inferences.\\nBut ignoring over-dispersion can also lead to all of the same problems as ignoring any pre-\\ndictor variable. Heterogeneity in counts can be a confound, hiding effects of interest or\\nproducing spurious inferences. So it’s worth trying grappling with over-dispersion. The best\\nsolution would of course be to discover the omitted source of dispersion and include it in\\nthe model. But even when no additional variables are available, it is possible to mitigate the\\neffects of over-dispersion. We’ll consider two common and useful strategies.\\nIn this chapter, we’ll consider continuous mixture models in which a linear model\\nis attached not to the observations themselves but rather to a distribution of observations.\\nWe’ll spend the rest of this section outlining this kind of model, using the common beta-\\nbinomial and gamma-Poisson (negative-binomial) models of this type. These models were\\nmentioned at the end of the previous chapter, but now we’ll actually define them.\\nIn the next chapters, we’ll see how to employ multilevel models that estimate both the\\nresiduals of each observation and the distribution of those residuals. In practice, it is often\\neasier to use multilevel models (GLMMs, Chapter 13) in place of continuous mixtures. The\\nreason is that multilevel models are much more flexible. They can handle over-dispersion\\nand other kinds of heterogeneity at the same time.\\n12.1.1. Beta-binomial. A beta-binomial model is a mixture of binomial distributions.\\nIt assumes that each binomial count observation has its own probability of success.182 We\\nestimate the distribution of probabilities of success instead of a single probability of success.\\nAny predictor variables describe the shape of this distribution.\\nThis will be easier to understand in the context of an example. For example, the UCBadmit\\ndata that you met last chapter is quite over-dispersed, as long as we ignore department. This\\nis because the departments vary a lot in baseline admission rates. You’ve already seen that\\nignoring this variation leads to an incorrect inference about applicant gender. Now let’s fit\\na beta-binomial model, ignoring department, and see how it picks up on the variation that\\narises from the omitted variable.\\nWhat a beta-binomial model of these data will assume is that each observed count on\\neach row of the data table has its own unique, unobserved probability of admission. These\\nprobabilities of admission themselves have a common distribution. This distribution is de-\\nscribed using a beta distribution, which is a probability distribution for probabilities. Why\\nuse a beta distribution? Because it makes the mathematics easy. When we use a beta, it is\\nmathematically possible to solve for a closed form likelihood function that averages over the\\nunknown probabilities for each observation. See the Overthinking box at the end of this\\nsection (page 375) for details.\\n'},\n",
       " {'index': 389,\n",
       "  'number': 371,\n",
       "  'content': '12.1. OVER-DISPERSED COUNTS\\n371\\nA beta distribution has two parameters, an average probability ¯p and a shape parameter\\nθ.183 The shape parameter θ describes how spread out the distribution is. When θ = 2,\\nevery probability from zero to 1 is equally likely. As θ increases above 2, the distribution of\\nprobabilities grows more concentrated. When θ < 2, the distribution is so dispersed that\\nextreme probabilities near zero and 1 are more likely than the mean. You can play around\\nwith the parameters to get a feel for the shapes this distribution can take:\\nR code\\n12.1\\npbar <- 0.5\\ntheta <- 5\\ncurve( dbeta2(x,pbar,theta) , from=0 , to=1 ,\\nxlab=\"probability\" , ylab=\"Density\" )\\nExplore different values for pbar and theta in the code above. Remember, this is a distribu-\\ntion for probabilities, so the horizontal axis you’ll see represents different possible probability\\nvalues, and the vertical axis is the density with which each probability on the horizontal is\\nsampled from the distribution. It’s weird, but you’ll get used to it.\\nWe’re going to bind our linear model to ¯p, so that changes in predictor variables change\\nthe central tendency of the distribution. In mathematical form, the model is:\\nAi ∼BetaBinomial(Ni, ¯pi, θ)\\nlogit(¯pi) = αgid[i]\\nαj ∼Normal(0, 1.5)\\nθ = ϕ + 2\\nϕ ∼Exponential(1)\\nwhere the outcome A is admit, the size N is applications, and gid[i] is gender index, 1 for\\nmale and 2 for female. I’ve introduced a trick with the prior on θ. We want to assume that\\nthe dispersion is at least 2, which means flat. Less than 2 would be piling up probability on\\nzero and 1. Greater than 2 is increasingly heaped on a single value. Which distribution has\\na minimum of 2? We can make one. The exponential has a minimum of zero. But if we add\\n2 to any exponentially distribution variable, then the minimum of the new variable is 2. So\\nthe model above defines ϕ with an exponential distribution.\\nThe code below will load the data and then fit, using ulam, the beta-binomial model:\\nR code\\n12.2\\nlibrary(rethinking)\\ndata(UCBadmit)\\nd <- UCBadmit\\nd$gid <- ifelse( d$applicant.gender==\"male\" , 1L , 2L )\\ndat <- list( A=d$admit , N=d$applications , gid=d$gid )\\nm12.1 <- ulam(\\nalist(\\nA ~ dbetabinom( N , pbar , theta ),\\nlogit(pbar) <- a[gid],\\na[gid] ~ dnorm( 0 , 1.5 ),\\ntranspars> theta <<- phi + 2.0,\\nphi ~ dexp(1)\\n), data=dat , chains=4 )\\n'},\n",
       " {'index': 390,\n",
       "  'number': 372,\n",
       "  'content': '372\\n12. MONSTERS AND MIXTURES\\nI tagged theta with transpars> (transformed parameters) so that Stan will return it in the\\nsamples. Let’s take a quick look at the posterior means. But let’s also go ahead and compute\\nthe constrast between the two genders first:\\nR code\\n12.3\\npost <- extract.samples( m12.1 )\\npost$da <- post$a[,1] - post$a[,2]\\nprecis( post , depth=2 )\\nulam posterior: 2000 samples from m12.1\\nmean\\nsd 5.5% 94.5%\\nhistogram\\na[1]\\n-0.45 0.41 -1.1\\n0.21\\n▁▁▇▇▂▁\\na[2]\\n-0.34 0.40 -1.0\\n0.27\\n▁▁▃▇▂▁\\nphi\\n1.05 0.78\\n0.1\\n2.44 ▇▇▅▃▂▁▁▁▁▁▁\\ntheta\\n3.05 0.78\\n2.1\\n4.44 ▇▇▅▃▂▁▁▁▁▁▁\\nda\\n-0.11 0.57 -1.0\\n0.76\\n▁▁▁▃▇▇▂▁▁▁\\nThe parameter a[1] is the log-odds of admission for male applicants. It is lower than a[2],\\nthe log-odds for female applicants. But the difference between the two, da, is highly uncer-\\ntain. There isn’t much evidence here of a difference between male and female admission\\nrates. Recall that in the previous chapter, a binomial model of these data that omitted de-\\npartment ended up being misleading, because there is an indirect path from gender through\\ndepartment to admission. That confound resulted in a spurious indication that female ap-\\nplicants had lower odds of admission. But the model above is not confounded, despite not\\ncontaining the department variable. How is this?\\nThe beta-binomial model allows each row in the data—each combination of department\\nand gender—to have its own unobserved intercept. These unobserved intercepts are sampled\\nfrom a beta distribution with mean ¯pi and dispersion θ. To see what this beta distribution\\nlooks like, we can just plot it.\\nR code\\n12.4\\ngid <- 2\\n# draw posterior mean beta distribution\\ncurve( dbeta2(x,mean(logistic(post$a[,gid])),mean(post$theta)) , from=0 , to=1 ,\\nylab=\"Density\" , xlab=\"probability admit\", ylim=c(0,3) , lwd=2 )\\n# draw 50 beta distributions sampled from posterior\\nfor ( i in 1:50 ) {\\np <- logistic( post$a[i,gid] )\\ntheta <- post$theta[i]\\ncurve( dbeta2(x,p,theta) , add=TRUE , col=col.alpha(\"black\",0.2) )\\n}\\nmtext( \"distribution of female admission rates\" )\\nThe result is shown on the left in Figure 12.1. Remember that a posterior distribution si-\\nmultaneously scores the plausibility of every combination of parameter values. This plot\\nshows 50 combinations of ¯p and θ, sampled from the posterior. The thick curve is the beta\\ndistribution corresponding the posterior mean. The central tendency is for low probabili-\\nties of admission, less than 0.5. But the most plausible distributions allow for departments\\nthat admit most applicants. What the model has done is accommodate the variation among\\ndepartments—there is a lot of variation! As a result, it is no longer tricked by department\\nvariation into a false inference about gender.\\n'},\n",
       " {'index': 391,\n",
       "  'number': 373,\n",
       "  'content': '12.1. OVER-DISPERSED COUNTS\\n373\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\nprobability admit\\nDensity\\ndistribution of female admission rates\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ncase\\nA\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n12\\nPosterior validation check\\nFigure 12.1. Left: Posterior distribution of beta distributions for m12.1.\\nThe thick curve is the posterior mean beta distribution. The lighter curves\\nrepresent 100 combinations of ¯p and θ sampled from the posterior. Right:\\nPosterior validation check for m12.1. As a result of the widely dispersed\\nbeta distributions on the left, the raw data (blue) is contained within the\\nprediction intervals.\\nTo get a sense of how the beta distribution of probabilities of admission influences pre-\\ndicted counts of applications admitted, let’s look at the posterior validation check:\\nR code\\n12.5\\npostcheck( m12.1 )\\nThis plot is shown on the right in Figure 12.1. The vertical axis shows the predicted propor-\\ntion admitted, for each case on the horizontal. The blue points show the empirical propor-\\ntion admitted on each row of the data. The open circles are the posterior mean ¯p, with 89%\\npercentile interval, and the + symbols mark the 89% interval of predicted counts of admis-\\nsion. There is a lot of dispersion expected here. The model can’t see departments, because\\nwe didn’t tell it about them. But it does see heterogeneity across rows, and it uses the beta\\ndistribution to estimate and anticipate that heterogeneity.\\n12.1.2. Negative-binomial or gamma-Poisson. A negative-binomial model, more use-\\nfully called a gamma-Poisson model, assumes that each Poisson count observation has its\\nown rate.184 It estimates the shape of a gamma distribution to describe the Poisson rates\\nacross cases. Predictor variables adjust the shape of this distribution, not the expected value\\nof each observation. The gamma-Poisson model is very much like a beta-binomial model,\\nwith the gamma distribution of rates (or expected values) replacing the beta distribution of\\nprobabilities of success. Why gamma? Because it makes the mathematics easy—there is a\\nsimple analytical expression for Poisson probabilities that are mixed together with gamma\\ndistributed rates. These gamma-Poisson models are very useful. The reason is that Poisson\\ndistributions are very narrow. The variance must equal the mean, recall.\\n'},\n",
       " {'index': 392,\n",
       "  'number': 374,\n",
       "  'content': '374\\n12. MONSTERS AND MIXTURES\\nThe gamma-Poisson distribution has two parameters, one for the mean (rate) and an-\\nother for the dispersion (scale) of the rates across cases.\\nyi ∼Gamma-Poisson(λi, ϕ)\\nThe λ parameter can be treated like the rate of an ordinary Poisson. The ϕ parameter must\\nbe positive and controls the variance. The variance of the gamma-Poisson is λ + λ2/ϕ. So\\nlarger ϕ values mean the distribution is more similar to a pure Poisson process.\\nLet’s see how this works with the Oceanic tools example from the previous chapter. There\\nwas a highly influential point, Hawaii, that will become much less influential in the equivalent\\ngamma-Poisson model. Why? Because gamma-Poisson expects more variation around the\\nmean rate. As a result, Hawaii ends up pulling the regression trend less.\\nR code\\n12.6\\nlibrary(rethinking)\\ndata(Kline)\\nd <- Kline\\nd$P <- standardize( log(d$population) )\\nd$contact_id <- ifelse( d$contact==\"high\" , 2L , 1L )\\ndat2 <- list(\\nT = d$total_tools,\\nP = d$population,\\ncid = d$contact_id )\\nm12.2 <- ulam(\\nalist(\\nT ~ dgampois( lambda , phi ),\\nlambda <- exp(a[cid])*P^b[cid] / g,\\na[cid] ~ dnorm(1,1),\\nb[cid] ~ dexp(1),\\ng ~ dexp(1),\\nphi ~ dexp(1)\\n), data=dat2 , chains=4 , log_lik=TRUE )\\nThe posterior predictions are displayed against the data in Figure 12.2. The pure Poisson\\nmodel from the previous chapter, m11.11, is shown next to it. Recall that Hawaii was a highly\\ninfluential point in the pure Poisson model. It does all the work of pulling the low-contact\\ntrend down. In this new model, Hawaii is still influential, but it exerts a lot less influence\\non the trends. Now the high and low contact trends are much more similar, very hard to\\nreliably distinguish. This is because the gamma-Poisson model expects rate variation, and\\nthe estimated amount of variation is quite large. Population is still strongly related to the\\ntotal tools, but the influence of contact rate has greatly diminished.\\n12.1.3. Over-dispersion, entropy, and information criteria. Both the beta-binomial and\\ngamma-Poisson models are maximum entropy for the same constraints as the regular bi-\\nnomial and Poisson. They just try to account for unobserved heterogeneity in probabilities\\nand rates. So while they can be a lot harder to fit to data, they can be usefully conceptual-\\nized much like ordinary binomial and Poisson GLMs. So in terms of model comparison us-\\ning information criteria, a beta-binomial model is a binomial model, and a gamma-Poisson\\n(negative-binomial) is a Poisson model.\\n'},\n",
       " {'index': 393,\n",
       "  'number': 375,\n",
       "  'content': '12.1. OVER-DISPERSED COUNTS\\n375\\n0\\n50000\\n150000\\n250000\\n20\\n30\\n40\\n50\\n60\\n70\\npopulation\\ntotal tools\\npure Poisson model\\n0\\n50000\\n150000\\n250000\\n20\\n30\\n40\\n50\\n60\\n70\\npopulation\\ntotal tools\\nlow contact\\nhigh contact\\ngamma-Poisson model\\nFigure 12.2. The Poisson model of Oceanic tools (left) is highly influenced\\nby Hawaii. The equivalent gamma-Poisson model (right) is much less influ-\\nenced by Hawaii, because the model expects more variation. And you can\\nsee the increased variation in the size of the shaded regions.\\nYou should not use WAIC and PSIS with these models, however, unless you are very\\nsure of what you are doing. The reason is that while ordinary binomial and Poisson models\\ncan be aggregated and disaggregated across rows in the data, without changing any causal as-\\nsumptions, the same is not true of beta-binomial and gamma-Poisson models. The reason is\\nthat a beta-binomial or gamma-Poisson likelihood applies an unobserved parameter to each\\nrow in the data. When we then go to calculate log-likelihoods, how the data are structured\\nwill determine how the beta-distributed or gamma-distributed variation enters the model.\\nFor example, a beta-binomial model like the one examined earlier in this chapter has\\ncounts on each row. The rows were combinations of departments and gender in that case,\\nand all of the applications for each department/gender combination were assumed to have\\nthe same unknown baseline probability of acceptance. What we’d like to do is treat each\\napplication as an observation, calculating WAIC over applications, so we get an estimate of\\naccuracy for a new application to a known department/gender. We could disaggregate the\\ndata so each row is a single application. But if we do that, then we lose the fact that the beta-\\nbinomial model implies the same latent probability for all of the applicants from the same\\nrow in the data. This is a huge bother.\\nWhat to do? Once you see how to incorporate over-dispersion with multilevel models,\\nin the next chapter, this obstacle will be reduced. Why? Because a multilevel model can\\nassign heterogeneity in probabilities or rates at any level of aggregation.\\nOverthinking: Continuous mixtures. A distribution like the beta-binomial is called a continuous\\nmixture, because every binomial count is assumed to have its own independent beta-distributed prob-\\nability of success, and the beta distribution is continuous rather than discrete. So the parameters of\\nthe beta-binomial are just the number of draws in each case (the same as the “size” n of the ordinary\\nbinomial distribution) and the two parameters that describe the shape of the beta distribution. This\\n'},\n",
       " {'index': 394,\n",
       "  'number': 376,\n",
       "  'content': '376\\n12. MONSTERS AND MIXTURES\\nimplies that the probability of observing a number of successes y from a beta-binomial process is:\\nf (y|n, ¯p, θ) =\\nZ 1\\n0\\ng(y|n, p)h(p|¯p, θ)dp\\nwhere f is the beta-binomial density, g is the binomial distribution, and h is the beta density. The\\nintegral above, like most integrals in applied probability, just computes an average: the probability\\nof y, averaged over all values of p. The p values are drawn from the beta distribution with mean ¯p\\nand scale θ. The probability of a success p is no longer a free parameter, as it is produced by the beta\\ndistribution. The gamma-Poisson density has a similar form, but averaging a Poisson probability over\\na gamma distribution of rates.\\nIn the case of the beta-binomial, as well as the gamma-Poisson, it is possible to close the integral\\nabove. You can look up the closed-form expressions anytime you need the analytic forms. The R\\nfunctions dbetabinom and dgampois provide computations from them.\\n12.2. Zero-inflated outcomes\\nVery often, the things we can measure are not emissions from any pure process. Instead,\\nthey are mixtures of multiple processes. Whenever there are different causes for the same\\nobservation, then a mixture model may be useful. A mixture model uses more than one\\nsimple probability distribution to model a mixture of causes. In effect, these models use more\\nthan one likelihood for the same outcome variable.\\nCount variables are especially prone to needing a mixture treatment. The reason is that a\\ncount of zero can often arise more than one way. A “zero” means that nothing happened, and\\nnothing can happen either because the rate of events is low or rather because the process that\\ngenerates events failed to get started. If we are counting scrub jays in the woods, we might\\nrecord a zero because there were no scrub jays in the woods or rather because we scared\\nthem all off before we started looking. Either way, the data contains a zero.\\nSo in this section you’ll see how to construct simple zero-inflated models. You’ll be able\\nto use the same components from earlier models, but they’ll be assembled in a different way.\\nSo even if you never need to use or interpret a zero-inflated model, seeing how they are\\nconstructed should expand your modeling imagination.\\nRethinking: Breaking the law. In the sciences, there is sometimes a culture of anxiety surrounding\\nstatistical inference. Itused tobe thatresearchers couldn’t easilyconstruct andstudy theirown custom\\nmodels, because they had to rely upon statisticians to properly study the models first. This led to\\nconcerns about unconventional models, concerns about breaking the laws of statistics. But statistical\\ncomputing is much more capable now. Now you can imagine your own generative process, simulate\\ndata from it, write the model, and verify that it recovers the true parameter values. You don’t have to\\nwait for a mathematician to legalize the model you need.\\n12.2.1. Example: Zero-inflated Poisson. Back in Chapter 11, I introduced Poisson GLMs\\nby using the example of a monastery producing manuscripts. Each day, a large number of\\nmonks finish copying a small number of manuscripts. The process is binomial, but with a\\nlarge number of trials and very low probability, so the distribution tends towards Poisson.\\nNow imagine that the monks take breaks on some days. On those days, no manuscripts\\nare completed. Instead, the wine cellar is opened and more earthly delights are practiced.\\nAs the monastery owner, you’d like to know how often the monks drink. The obstacle for\\n'},\n",
       " {'index': 395,\n",
       "  'number': 377,\n",
       "  'content': '12.2. ZERO-INFLATED OUTCOMES\\n377\\np\\n1 – p\\nobserve y = 0\\nobserve y > 0\\nDrink\\nWork\\n0\\n1\\n2\\n3\\n4\\n5\\n0\\n50\\n100\\n150\\nmanuscripts completed\\nFrequency\\nFigure 12.3. Left: Structure of the zero-inflated likelihood calculation. Be-\\nginning at the top, the monks drink p of the time or instead work 1 −p of\\nthe time. Drinking monks always produce an observation y = 0. Working\\nmonks may produce either y = 0 or y > 0. Right: Frequency distribution of\\nzero-inflated observations. The blue line segment over zero shows the y = 0\\nobservations that arose from drinking. In real data, we typically cannot see\\nwhich zeros come from which process.\\ninference is that there will be zeros on honest non-drinking days, as well, just by chance. So\\nhow can you estimate the number of days spent drinking?\\nLet’s make a mixture to solve this problem.185 We want to consider that any zero in the\\ndata can arise from two processes: (1) the monks spent the day drinking and (2) they worked\\nthat day but nevertheless failed to complete any manuscripts. Let p be the probability the\\nmonks spend the day drinking. Let λ be the mean number of manuscripts completed, when\\nthe monks work.\\nTo get this model going, we need to define a likelihood function that mixes these two\\nprocesses. To grasp how we can construct such a monster, think of the monks’ drinking as\\nresulting from a coin flip (Figure 12.3). The “coin” shows a cask of wine on one side and a\\nquill on the other. The probability the wine cask shows is p, which could be any value from\\n0 to 1. Depending upon the outcome of the coin flip, the monks either begin drinking or\\nrather begin copying. Drinking monks always produce zero completed manuscripts. Work-\\ning monks produce a Poisson number of completed manuscripts with some average rate λ.\\nSo it is possible still to observe a zero, even when the monks work.\\nWith these assumptions, the likelihood of observing a zero is:\\nPr(0|p, λ) = Pr(drink|p) + Pr(work|p) × Pr(0|λ)\\n= p + (1 −p) exp(−λ)\\nSince the Poisson likelihood of y is Pr(y|λ) = λy exp(−λ)/y!, the likelihood of y = 0 is just\\nexp(−λ). The above is just the mathematics for:\\n'},\n",
       " {'index': 396,\n",
       "  'number': 378,\n",
       "  'content': '378\\n12. MONSTERS AND MIXTURES\\nThe probability of observing a zero is the probability that the monks didn’t\\ndrink OR (+) the probability that the monks worked AND (×) failed to\\nfinish anything.\\nAnd the likelihood of a non-zero value y is:\\nPr(y|y > 0, p, λ) = Pr(drink|p)(0) + Pr(work|p) Pr(y|λ) = (1 −p)λy exp(−λ)\\ny!\\nSince drinking monks never produce y > 0, the expression above is just the chance the\\nmonks both work, 1 −p, and finish y manuscripts.\\nDefine ZIPoisson as the distribution above, with parameters p (probability of a zero) and\\nλ (mean of Poisson) to describe its shape. Then a zero-inflated Poisson regression takes the\\nform:\\nyi ∼ZIPoisson(pi, λi)\\nlogit(pi) = αp + βpxi\\nlog(λi) = αλ + βλxi\\nNotice that there are two linear models and two link functions, one for each process in the\\nZIPoisson. The parameters of the linear models differ, because any predictor such as x may\\nbe associated differently with each part of the mixture. In fact, you don’t even have to use\\nthe same predictors in both models—you can construct the two linear models however you\\nwish, depending upon your hypothesis.\\nWe have everything we need now, except for some data. So let’s simulate the monks’\\ndrinking and working. Then you’ll see the code used to recover the parameter values used\\nin the simulation.\\nR code\\n12.7\\n# define parameters\\nprob_drink <- 0.2 # 20% of days\\nrate_work <- 1\\n# average 1 manuscript per day\\n# sample one year of production\\nN <- 365\\n# simulate days monks drink\\nset.seed(365)\\ndrink <- rbinom( N , 1 , prob_drink )\\n# simulate manuscripts completed\\ny <- (1-drink)*rpois( N , rate_work )\\nThe outcome variable we get to observe is y, which is just a list of counts of completed\\nmanuscripts, one count for each day of the year. Take a look at the outcome variable:\\nR code\\n12.8\\nsimplehist( y , xlab=\"manuscripts completed\" , lwd=4 )\\nzeros_drink <- sum(drink)\\nzeros_work <- sum(y==0 & drink==0)\\nzeros_total <- sum(y==0)\\nlines( c(0,0) , c(zeros_work,zeros_total) , lwd=4 , col=rangi2 )\\n'},\n",
       " {'index': 397,\n",
       "  'number': 379,\n",
       "  'content': '12.2. ZERO-INFLATED OUTCOMES\\n379\\nThis plot is shown on the right-hand side of Figure 12.3. The zeros produced by drinking\\nare shown in blue. Those from work are shown in black. The total number of zeros is inflated,\\nrelative to a typical Poisson distribution.\\nAnd to fit the model, the rethinking package provides the zero-inflated Poisson like-\\nlihood as dzipois. For more detail on how it relates to the mathematics above, see the\\nOverthinking box at the end of this section. Using dzipois is straightforward. I’m also go-\\ning to nudge the prior for the probability of drinking so that there is more mass below 0.5\\nthan above it—the monks probably do not drink more often than not.\\nR code\\n12.9\\nm12.3 <- ulam(\\nalist(\\ny ~ dzipois( p , lambda ),\\nlogit(p) <- ap,\\nlog(lambda) <- al,\\nap ~ dnorm( -1.5 , 1 ),\\nal ~ dnorm( 1 , 0.5 )\\n) , data=list(y=y) , chains=4 )\\nprecis( m12.3 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nap -1.28 0.35 -1.89 -0.79\\n657\\n1\\nal\\n0.01 0.09 -0.14\\n0.16\\n759\\n1\\nOn the natural scale, those posterior means are:\\nR code\\n12.10\\npost <- extract.samples( m12.3 )\\nmean( inv_logit( post$ap ) ) # probability drink\\nmean( exp( post$al ) )\\n# rate finish manuscripts, when not drinking\\n[1] 0.2241255\\n[1] 1.017643\\nNotice that we can get an accurate estimate of the proportion of days the monks drink, even\\nthough we can’t say for any particular day whether or not they drank.\\nThis example is the simplest possible. In real problems, you might have predictor vari-\\nables that are associated with one or both processes inside the zero-inflated Poisson mixture.\\nIn that case, you add those variables and their parameters to either or both linear models.\\nOverthinking: Zero-inflated Poisson calculations in Stan. The function dzipois is implemented\\nin a way that guards against some kinds of numerical error. So its code looks confusing—just type\\n“dzipois” at the R prompt and see. But really all it’s doing is implementing the likelihood formula\\ndefined in the section above. Let’s focus on how this is implemented in Stan. When you tell ulam to\\nuse dzipois, it understands it like this:\\nR code\\n12.11\\nm12.3_alt <- ulam(\\nalist(\\ny|y>0 ~ custom( log1m(p) + poisson_lpmf(y|lambda) ),\\ny|y==0 ~ custom( log_mix( p , 0 , poisson_lpmf(0|lambda) ) ),\\nlogit(p) <- ap,\\nlog(lambda) <- al,\\nap ~ dnorm(-1.5,1),\\n'},\n",
       " {'index': 398,\n",
       "  'number': 380,\n",
       "  'content': '380\\n12. MONSTERS AND MIXTURES\\nal ~ dnorm(1,0.5)\\n) , data=list(y=as.integer(y)) , chains=4 )\\nThat is the same model, but with explicit mixtures and some raw Stan code inside the custom lines.\\nIf you look at stancode(m12.3_alt), you’ll see the corresponding lines:\\nif ( y[i] > 0 ) target += log1m(p) + poisson_lpmf(y[i] | lambda);\\nif ( y[i] == 0 ) target += log_mix(p, 0, poisson_lpmf(0 | lambda));\\nThat target thing is a chain of terms for calculating the log-posterior. When we use it with +=, we add\\nanother term to the stack. Stan will later use this stack to figure out the gradient, through aggressive\\nand systematic use of the chain rule from calculus. Then there are some important tricks for doing\\nthis calculation. The log1m function computes the log of one-minus a value. We need log(1−p), but\\nif p is very close to 1, then this can round catastrophically to zero and then the log will be negative\\ninfinity. Using log1m makes this much less likely. The function log_mix mixes together two log-\\nprobabilities, which is what we need for the probability of a zero. But it also uses clever techniques to\\navoid rounding error. It’s equivalent in this case to:\\nif ( y[i] == 0 ) target += log( p + (1-p)*exp(-lambda) );\\nbut more stable under extreme values of p. In this case, it makes no difference—the less fancy direct\\napproach works fine. But it’s good to know the better approach. More complex models won’t work\\nright otherwise. Finally, note that I coerced y to integer in the data list. When you use ulam’s built-in\\ndistributions, it will try to coerce variables into the correct Stan type. But if you build your own, you\\nneed to do this yourself.\\n12.3. Ordered categorical outcomes\\nIt is very common in the social sciences, and occasional in the natural sciences, to have\\nan outcome variable that is discrete, like a count, but in which the values merely indicate\\ndifferent ordered levels along some dimension. For example, if I were to ask you how much\\nyou like to eat fish, on a scale from 1 to 7, you might say 5. If I were to ask 100 people the\\nsame question, I’d end up with 100 values between 1 and 7. In modeling each outcome value,\\nI’d have to keep in mind that these values are ordered, because 7 is greater than 6, which is\\ngreater than 5, and so on. The result is a set of ordered categories. Unlike a count, the\\ndifferences in value are not necessarily equal. It might be much harder to move someone’s\\npreference for fish from 1 to 2 than it is to move it from 5 to 6. Just treating ordered categories\\nas continuous measures is not a good idea.186\\nLuckily, there is a standard and accessible solution. In principle, an ordered categorical\\nvariable is just a multinomial prediction problem (page 359). But the constraint that the\\ncategories be ordered demands a special treatment. What we’d like is for any associated pre-\\ndictor variable, as it increases, to move predictions progressively through the categories in\\nsequence. So for example if preference for ice cream is positively associated with years of\\nage, then the model should sequentially move predictions upwards as age increases: 3 to 4,\\n4 to 5, 5 to 6, etc. This presents a challenge: how to ensure that the linear model maps onto\\nthe outcomes in the right order.\\nThe conventional solution is to use a cumulative link function.187 The cumulative\\nprobability of a value is the probability of that value or any smaller value. In the context of\\nordered categories, the cumulative probability of 3 is the sum of the probabilities of 3, 2, and\\n1. Ordered categories by convention begin at 1, so a result less than 1 has no probability at all.\\nBy linking a linear model to cumulative probability, it is possible to guarantee the ordering\\nof the outcomes.\\n'},\n",
       " {'index': 399,\n",
       "  'number': 381,\n",
       "  'content': '12.3. ORDERED CATEGORICAL OUTCOMES\\n381\\nI’ll explain why in two steps. Step 1 is to explain how to parameterize a distribution of\\noutcomes on the scale of log-cumulative-odds. Step 2 is to introduce a predictor (or more\\nthan one predictor) to these log-cumulative-odds values, allowing you to model associations\\nbetween predictors and the outcome while obeying the ordered nature of prediction.\\nBoth steps will unfold in context of a data example, to make the discussion more con-\\ncrete. So next you meet some data.\\n12.3.1. Example: Moral intuition. The data for this example come from a series of experi-\\nments conducted by philosophers.188 Yes, philosophers do sometimes conduct experiments.\\nIn this case, the experiments aim to collect empirical evidence relevant to debates about\\nmoral intuition, the forms of reasoning through which people develop judgments about the\\nmoral goodness and badness of actions. These debates are relevant to all of the social sci-\\nences, because they touch on broader issues of reasoning, the role of emotions in decision\\nmaking, and theories of moral development, both in individuals and groups.\\nThese experiments get measurements of moral judgment by using scenarios known as\\n“trolley problems.” The classic version invokes a runaway trolley, but what these scenarios\\nshare is that they have proved vexing or paradoxical to moral philosophers. Here’s a tradi-\\ntional example, using a “boxcar” in place of a “trolley”:\\nStanding by the railroad tracks, Dennis sees an empty, out-of-control boxcar about\\nto hit five people. Next to Dennis is a lever that can be pulled, sending the boxcar\\ndown a side track and away from the five people. But pulling the lever will also\\nlower the railing on a footbridge spanning the side track, causing one person to fall\\noff the footbridge and onto the side track, where he will be hit by the boxcar. If\\nDennis pulls the lever the boxcar will switch tracks and not hit the five people, and\\nthe one person to fall and be hit by the boxcar. If Dennis does not pull the lever the\\nboxcar will continue down the tracks and hit five people, and the one person will\\nremain safe above the side track.\\nHow morally permissible is it for Dennis to pull the lever?\\nThe reason these scenarios can be philosophically vexing is that the analytical content of\\ntwo scenarios can be identical, and yet people reliably reach different judgments about the\\nmoral permissibility of the same action in the different scenarios. Before you jump to the\\nconclusion that this stuff is silly, the moral intuitions people have in these experiments are\\nsimilar to the reactions they have to laws and how behavior is classified as criminal or not.\\nThe law is full of moral paradoxes. We need to understand them.\\nPrevious research has led to at least three important principles of unconscious reasoning\\nthat may explain variations in judgment. These principles are:\\nThe action principle: Harm caused by action is morally worse than equivalent harm\\ncaused by omission.\\nThe intention principle: Harm intended as the means to a goal is morally worse than\\nequivalent harm foreseen as the side effect of a goal.\\nThe contact principle: Using physical contact to cause harm to a victim is morally\\nworse than causing equivalent harm to a victim without using physical contact.\\nThe experimental context within which we’ll explore these principles comprises stories\\nthat vary the principles, while keeping many of the basic objects and actors the same. For\\nexample, the version of the boxcar story quoted just above implies the action principle, but\\nnot the others. Since the actor (Dennis) had to do something to create the outcome, rather\\nthan remain passive, this is an action scenario. However, the harm caused to the one man\\n'},\n",
       " {'index': 400,\n",
       "  'number': 382,\n",
       "  'content': '382\\n12. MONSTERS AND MIXTURES\\nwho will fall is not necessary, or intended, in order to save the five. Thus it is not an example\\nof the intention principle. And there is no direct contact, so it is also not an example of the\\ncontact principle.\\nYou can construct a boxcar story with the same outline, but now with both the action\\nprinciple and the intention principle. That is, in this version, the actor both does something\\nto change the outcome and the action must cause harm to the one person in order to save\\nthe other five:\\nStanding by the railroad tracks, Evan sees an empty, out-of-control boxcar about\\nto hit five people. Next to Evan is a lever that can be pulled, lowering the railing\\non a footbridge that spans the main track, and causing one person to fall off the\\nfootbridge and onto the main track, where he will be hit by the boxcar. The boxcar\\nwill slow down because of the one person, therefore preventing the five from being\\nhit. If Evan pulls the lever the one person will fall and be hit by the boxcar, and\\ntherefore the boxcar will slow down and not hit the five people. If Evan does not\\npull the lever the boxcar will continue down the tracks and hit the five people, and\\nthe one person will remain safe above the main track.\\nMost people judge that, if Evan pulls the lever, it is worse (less permissible) than when Dennis\\npulls the lever. You’ll see by how much, as we analyze these data. Load the data:\\nR code\\n12.12\\nlibrary(rethinking)\\ndata(Trolley)\\nd <- Trolley\\nThere are 12 columns and 9930 rows, comprising data for 331 unique individuals. The out-\\ncome we’ll be interested in is response, which is an integer from 1 to 7 indicating how\\nmorally permissible the participant found the action to be taken (or not) in the story. Since\\nthis type of rating is categorical and ordered, it’s exactly the right type of problem for our\\nordered model.\\n12.3.2. Describing an ordered distribution with intercepts. First, let’s see how to describe a\\ndistribution of discrete ordered values. Take a look at the overall distribution, the histogram,\\nof the outcome variable.\\nR code\\n12.13\\nsimplehist( d$response , xlim=c(1,7) , xlab=\"response\" )\\nThe result is shown in the left-hand plot in Figure 12.4.\\nOur goal is to re-describe this histogram on the log-cumulative-odds scale. This just\\nmeans constructing the odds of a cumulative probability and then taking a logarithm. Why\\ndo this arcane thing? Because this is the cumulative analog of the logit link we used in pre-\\nvious chapters. The logit is log-odds, and cumulative logit is log-cumulative-odds. Both are\\ndesigned to constrain the probabilities to the 0/1 interval. Then when we decide to add pre-\\ndictor variables, we can safely do so on the cumulative logit scale. The link function takes\\ncare of converting the parameter estimates to the proper probability scale.\\nThe first step in the conversion is to compute cumulative probabilities:\\nR code\\n12.14\\n# discrete proportion of each response value\\npr_k <- table( d$response ) / nrow(d)\\n'},\n",
       " {'index': 401,\n",
       "  'number': 383,\n",
       "  'content': '12.3. ORDERED CATEGORICAL OUTCOMES\\n383\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n500\\n1000\\n1500\\n2000\\nresponse\\nFrequency\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nresponse\\ncumulative proportion\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n-2\\n-1\\n0\\n1\\nresponse\\nlog-cumulative-odds\\nFigure 12.4. Re-describing a discrete distribution using log-cumulative-\\nodds. Left: Histogram of discrete response in the sample. Middle: Cumu-\\nlative proportion of each response. Right: Logarithm of cumulative odds\\nof each response. Note that the log-cumulative-odds of response value 7 is\\ninfinity, so it is not shown.\\n# cumsum converts to cumulative proportions\\ncum_pr_k <- cumsum( pr_k )\\n# plot\\nplot( 1:7 , cum_pr_k , type=\"b\" , xlab=\"response\" ,\\nylab=\"cumulative proportion\" , ylim=c(0,1) )\\nAnd the result is shown as the middle plot in Figure 12.4.\\nThen to re-describe the histogram as log-cumulative odds, we’ll need a series of intercept\\nparameters. Each intercept will be on the log-cumulative-odds scale and stand in for the cu-\\nmulative probability of each outcome. So this is just the application of the link function. The\\nlog-cumulative-odds that a response value yi is equal-to-or-less-than some possible outcome\\nvalue k is:\\nlog\\nPr(yi ≤k)\\n1 −Pr(yi ≤k) = αk\\n(12.1)\\nwhere αk is an “intercept” unique to each possible outcome value k. We can compute these\\nintercept parameters directly:\\nR code\\n12.15\\nlogit <- function(x) log(x/(1-x)) # convenience function\\nround( lco <- logit( cum_pr_k ) , 2 )\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n-1.92 -1.27 -0.72\\n0.25\\n0.89\\n1.77\\nInf\\nThese values are plotted in the right-hand panel of Figure 12.4. Notice that the cumulative\\nlogit of the largest response, 7, is infinity. This is because log(1/(1 −1)) = ∞. Since the\\nlargest response value always has a cumulative probability of 1, we effectively do not need a\\n'},\n",
       " {'index': 402,\n",
       "  'number': 384,\n",
       "  'content': '384\\n12. MONSTERS AND MIXTURES\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nresponse\\ncumulative proportion\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nFigure 12.5. Cumulative probability and or-\\ndered likelihood. The horizontal axis displays\\npossible observable outcomes, from 1 through\\n7. The vertical axis displays cumulative proba-\\nbility. The gray bars over each outcome show\\ncumulative probability. These keep growing\\nwith each successive outcome value. The blue\\nline segments show the discrete probability of\\neach individual outcome. These are the likeli-\\nhoods that go into Bayes’ theorem.\\nparameter for it. We get it for free, from the law of total probability. So for K = 7 possible\\nresponse values, we only need K −1 = 6 intercepts.\\nAll of the above is very nice, but what we really want is the posterior distribution of\\nthese intercepts. This will allow us to take into account sample size and prior information,\\nas well as insert predictor variables (in the next section). To use Bayes’ theorem to compute\\nthe posterior distribution of these intercepts, we’ll need to compute the likelihood of each\\npossible response value. So the last step in constructing the basic model fitting engine for\\nordered categorical outcomes is to use the cumulative probabilities, Pr(yi ≤k), to compute\\nlikelihood, Pr(yi = k).\\nFigure 12.5 illustrates how this is done. Each intercept αk implies a cumulative prob-\\nability for each k. You just use the inverse link to translate from log-cumulative-odds back\\nto cumulative probability. So when we observe k and need its likelihood, we can get the\\nlikelihood by subtraction:\\npk = Pr(yi = k) = Pr(yi ≤k) −Pr(yi ≤k −1)\\n(12.2)\\nThe blue line segments in Figure 12.5 are these likelihoods, computed by subtraction. With\\nthese in hand, the posterior distribution is computed the usual way.\\nLet’s go ahead and see how it’s done as a model. Conventions for writing mathematical\\nforms of the ordered logit vary a lot. We’ll use this convention:\\nRi ∼Ordered-logit(ϕi, κ)\\n[probability of data]\\nϕi = 0\\n[linear model]\\nκk ∼Normal(0, 1.5)\\n[common prior for each intercept]\\nBut we can express the model more literally as well. It starts with a categorical distribution:\\nRi ∼Categorical(p)\\n[probability of data]\\n'},\n",
       " {'index': 403,\n",
       "  'number': 385,\n",
       "  'content': '12.3. ORDERED CATEGORICAL OUTCOMES\\n385\\nAnd then all the conversions needed to build the vector of probabilities p:\\np1 = q1\\n[probabilities of each value k]\\npk = qk −qk−1\\nfor K > k > 1\\npK = 1 −qk−1\\nlogit(qk) = κk −ϕi\\n[cumulative logit link]\\nϕi = terms of linear model\\n[linear model]\\nκk ∼Normal(0, 1.5)\\n[common prior for each intercept]\\nThis second form is cruel, but it exposes that an ordered-logit distribution is really just a\\ncategorical distribution that takes a vector p = {p1, p2, p3, p4, p5, p6} of probabilities of each\\nresponse value below the maximum response (7 in this example). Each response value k in\\nthis vector is defined by its link to an intercept parameter, αk. Finally, some weakly regu-\\nlarizing priors are placed on these intercepts. In this example, there is a lot of data, so just\\nabout any prior will be overwhelmed. As always, in small sample contexts, you’ll have to\\nthink much harder about priors. Consider for example that we know α1 < α2, before we\\neven see the data.\\nIn code form for either quap and ulam, the link function will be embedded in the likeli-\\nhood function already. This makes the calculations more efficient and avoids forcing you to\\ncode all the routine intermediate calculations above. So to fit the basic model, incorporating\\nno predictor variables:\\nR code\\n12.16\\nm12.4 <- ulam(\\nalist(\\nR ~ dordlogit( 0 , cutpoints ),\\ncutpoints ~ dnorm( 0 , 1.5 )\\n) , data=list( R=d$response ), chains=4 , cores=4 )\\nThat zero in the dordlogit is a placeholder for the linear model that we’ll construct later.\\nIf you want to use this model in quap instead, you’ll need to specify the start values for\\nthe cutpoints. Otherwise it’ll have a very hard time getting started. The exact values aren’t\\nimportant, but their ordering is. This code will work:\\nR code\\n12.17\\nm12.4q <- quap(\\nalist(\\nresponse ~ dordlogit( 0 , c(a1,a2,a3,a4,a5,a6) ),\\nc(a1,a2,a3,a4,a5,a6) ~ dnorm( 0 , 1.5 )\\n) , data=d , start=list(a1=-2,a2=-1,a3=0,a4=1,a5=2,a6=2.5) )\\nThe posterior distribution of the cutpoints is on the log-cumulative-odds scale:\\nR code\\n12.18\\nprecis( m12.4 , depth=2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\ncutpoints[1] -1.92 0.03 -1.96 -1.87\\n1460\\n1\\ncutpoints[2] -1.27 0.02 -1.31 -1.23\\n2091\\n1\\ncutpoints[3] -0.72 0.02 -0.75 -0.68\\n2480\\n1\\ncutpoints[4]\\n0.25 0.02\\n0.22\\n0.28\\n2701\\n1\\n'},\n",
       " {'index': 404,\n",
       "  'number': 386,\n",
       "  'content': '386\\n12. MONSTERS AND MIXTURES\\ncutpoints[5]\\n0.89 0.02\\n0.85\\n0.92\\n2373\\n1\\ncutpoints[6]\\n1.77 0.03\\n1.72\\n1.81\\n2345\\n1\\nSince there is a lot of data here, the posterior for each intercept is quite precisely estimated,\\nas you can see from the tiny standard deviations. To get cumulative probabilities back:\\nR code\\n12.19\\nround( inv_logit(coef(m12.4)) , 3 )\\ncutpoints[1] cutpoints[2] cutpoints[3] cutpoints[4] cutpoints[5] cutpoints[6]\\n0.128\\n0.220\\n0.328\\n0.562\\n0.709\\n0.854\\nAnd of course those are the same as the values in cum_pr_k that we computed earlier. But\\nnow we also have a posterior distribution around these values, which provides a measure of\\nuncertainty. And we’re ready to add predictor variables in the next section.\\n12.3.3. Adding predictor variables. This flurry of computation has gotten us very little so\\nfar, aside from a Bayesian representation of a histogram. But all of it has been necessary\\nin order to prepare the model for the addition of predictor variables that obey the ordered\\nconstraint on the outcomes.\\nTo include predictor variables, we define the log-cumulative-odds of each response k as\\na sum of its intercept αk and a typical linear model. Suppose for example we want to add\\na predictor x to the model. We’ll do this by defining a linear model ϕi = βxi. Then each\\ncumulative logit becomes:\\nlog\\nPr(yi ≤k)\\n1 −Pr(yi ≤k) = αk −ϕi\\nϕi = βxi\\nThis form automatically ensures the correct ordering of the outcome values, while still mor-\\nphing the likelihood of each individual value as the predictor xi changes value. Why is the\\nlinear model ϕ subtracted from each intercept? Because if we decrease the log-cumulative-\\nodds of every outcome value k below the maximum, this necessarily shifts probability mass\\nupwards towards higher outcome values. So then positive values of β mean increasing x also\\nincreases the mean y. You could add ϕ instead like αk + ϕi. But then β > 0 would indicate\\nincreasing x decreases the mean.\\nFor example, suppose we take the posterior means from m12.4 and subtract 0.5 from\\neach. The function dordlogit makes the calculation of the probabilities straightforward:\\nR code\\n12.20\\nround( pk <- dordlogit( 1:7 , 0 , coef(m12.4) ) , 2 )\\n[1] 0.13 0.09 0.11 0.23 0.15 0.15 0.15\\nThese probabilities imply an average outcome value of:\\nR code\\n12.21\\nsum( pk*(1:7) )\\n[1] 4.198989\\nAnd now subtracting 0.5 from each:\\nR code\\n12.22\\nround( pk <- dordlogit( 1:7 , 0 , coef(m12.4)-0.5 ) , 2 )\\n[1] 0.08 0.06 0.08 0.21 0.16 0.18 0.22\\n'},\n",
       " {'index': 405,\n",
       "  'number': 387,\n",
       "  'content': '12.3. ORDERED CATEGORICAL OUTCOMES\\n387\\nCompare these to the probabilities just above and notice that the values on the left have\\ndiminished while the values on the right have increased. The expected value is now:\\nR code\\n12.23\\nsum( pk*(1:7) )\\n[1] 4.729394\\nAnd that’s why we subtract ϕ, the linear model βxi, from each intercept, rather than add it.\\nThis way, a positive β value indicates that an increase in the predictor variable x results in an\\nincrease in the average response.\\nNow we can turn back to our “trolley” data and include predictor variables to help ex-\\nplain variation in responses. The predictor variables of interest are going to be action,\\nintention, and contact, each an indicator variable corresponding to each principle out-\\nlined earlier. There are several ways we could code these indicator variables into treatments.\\nConsider that contact always implies action. The way that contact is coded here, it excludes\\naction, treating the two features as mutually exclusive. But each can be combined with in-\\ntention. This gives us 6 possible story combinations:\\n(1) No action, contact, or intention\\n(2) Action\\n(3) Contact\\n(4) Intention\\n(5) Action and intention\\n(6) Contact and intention\\nThe last two represent interactions—the influence of intention may depend upon the simulta-\\nneous presence of action or contact. I’ll use the indicator variables directly this time, instead\\nof an index variable. This will let me show you a useful trick for defining interactions that\\ncan make your models easier to read and debug.\\nThe log-cumulative-odds of each response k will now be:\\nlog\\nPr(yi ≤k)\\n1 −Pr(yi ≤k) = αk −ϕi\\nϕi = βAAi + βCCi + BI,iIi\\nBI,i = βI + βIAAi + βICCi\\nwhere Ai indicates the value of action on row i, Ii indicates the value of intention on row i,\\nand Ci indicates the value of contact on row i. What we’ve done here is define the log-odds\\nof each possible response to be an additive model of the features of the story corresponding to\\neach response. For the interactions of intention with action and contact, I used an accessory\\nlinear model, BI. This just makes the notation clearer, by defining the relationship between\\nintention and response as a function of the other variables. You could substitute BI into ϕi\\nwithout changing anything.\\nYou fit this model just as you’d expect, by adding the slopes and predictor variables to\\nthe phi parameter inside dordlogit. Here’s a working model:\\nR code\\n12.24\\ndat <- list(\\nR = d$response,\\nA = d$action,\\nI = d$intention,\\n'},\n",
       " {'index': 406,\n",
       "  'number': 388,\n",
       "  'content': '388\\n12. MONSTERS AND MIXTURES\\nC = d$contact )\\nm12.5 <- ulam(\\nalist(\\nR ~ dordlogit( phi , cutpoints ),\\nphi <- bA*A + bC*C + BI*I ,\\nBI <- bI + bIA*A + bIC*C ,\\nc(bA,bI,bC,bIA,bIC) ~ dnorm( 0 , 0.5 ),\\ncutpoints ~ dnorm( 0 , 1.5 )\\n) , data=dat , chains=4 , cores=4 )\\nprecis( m12.5 )\\n6 vector or matrix parameters omitted in display. Use depth=2 to show them.\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nbIC -1.23 0.09 -1.38 -1.09\\n1245\\n1\\nbIA -0.43 0.08 -0.55 -0.31\\n1132\\n1\\nbC\\n-0.35 0.07 -0.45 -0.24\\n1229\\n1\\nbI\\n-0.29 0.06 -0.38 -0.20\\n1025\\n1\\nbA\\n-0.47 0.05 -0.56 -0.39\\n1064\\n1\\nI’ve suppressed the cutpoints. They aren’t of much interest at the moment. But look at the\\nposterior distributions of the slopes. They are all reliably negative. Each of these story fea-\\ntures reduces the rating—the acceptability of the story. Plotting the marginal posterior dis-\\ntributions makes the relative effect sizes much clearer:\\nR code\\n12.25\\nplot( precis(m12.5) , xlim=c(-1.4,0) )\\nbA\\nbI\\nbC\\nbIA\\nbIC\\n-1.4\\n-1.2\\n-1.0\\n-0.8\\n-0.6\\n-0.4\\n-0.2\\n0.0\\nValue\\nThe combination of intention and contact is the worst. This is curious, because it seems that\\nneither intention nor contact by itself has a large impact on ratings.\\nAs always, this will all be easier to see if we plot the posterior predictions. There is no\\nperfect way to plot the predictions of these log-cumulative-odds models. Why? Because\\neach prediction is really a vector of probabilities, one for each possible outcome value. So\\nas a predictor variable changes value, the entire vector changes. This kind of thing can be\\nvisualized in several different ways.\\nOne common and useful way is to use the horizontal axis for a predictor variable and the\\nvertical axis for cumulative probability. Then you can plot a curve for each response value,\\nas it changes across values of the predictor variable. After plotting a curve for each response\\nvalue, you’ll end up mapping the distribution of responses, as it changes across values of the\\npredictor variable.\\nSo let’s do that. First, let’s make an empty plot:\\n'},\n",
       " {'index': 407,\n",
       "  'number': 389,\n",
       "  'content': '12.3. ORDERED CATEGORICAL OUTCOMES\\n389\\nR code\\n12.26\\nplot( NULL , type=\"n\" , xlab=\"intention\" , ylab=\"probability\" ,\\nxlim=c(0,1) , ylim=c(0,1) , xaxp=c(0,1,1) , yaxp=c(0,1,2) )\\nNow we’ll set up a data list that contains the different combinations of predictor values. Then\\nwe pass it to link to get phi samples for each combination: Now loop over the first 100\\nsamples in post and plot their predictions, across values of intention:\\nR code\\n12.27\\nkA <- 0\\n# value for action\\nkC <- 0\\n# value for contact\\nkI <- 0:1\\n# values of intention to calculate over\\npdat <- data.frame(A=kA,C=kC,I=kI)\\nphi <- link( m12.5 , data=pdat )$phi\\nFinally loop over the first 50 samples in and plot their predictions, across values of intention.\\nThe trick here is to use pordlogit to compute the cumulative probability for each possible\\noutcome value, from 1 to 7, using the samples in phi and the cutpoints.\\nR code\\n12.28\\npost <- extract.samples( m12.5 )\\nfor ( s in 1:50 ) {\\npk <- pordlogit( 1:6 , phi[s,] , post$cutpoints[s,] )\\nfor ( i in 1:6 ) lines( kI , pk[,i] , col=grau(0.1) )\\n}\\nBy modifying the above code to change the values in kA and kC, you can make a triptych\\n(page 252) for model m12.5. The results are shown in the top row of Figure 12.6, with a\\nlittle extra decoration, to show the raw data as points on the margins. In each plot, the black\\nlines indicate the boundaries between response values, numbered 1 through 7, bottom to\\ntop. The thickness of the lines corresponds to the variation in predictions due to variation\\nin samples from the posterior. Since there is so much data in this example, the path of the\\npredicted boundaries is quite certain. The horizontal axis represents values of intention,\\neither zero or one. The change in height of each boundary going from left to right in each\\nplot indicates the predicted impact of changing a story from non-intention to intention. Fi-\\nnally, each plot sets the other two predictor variables, action and contact, to either zero\\nor one. In the upper-left, both are set to zero. This plot shows the predicted effect of taking a\\nstory with no-action, no-contact, and no-intention and adding intention to it. In the upper-\\nright, action is now set to one. This plot shows the predicted impact of taking a story with\\naction and no-intention (action and contact never go together in this experiment, recall) and\\nadding intention. This upper-right plot demonstrates the interaction between action and\\nintention. Finally, in the lower-left, contact is set to one. This plot shows the predicted\\nimpact of taking a story with contact and no-intention and adding intention to it. This plot\\nshows the large interaction effect between contact and intention, the largest estimated\\neffect in the model.\\nAnother plotting option is to show the implied histogram of outcomes. All we have to\\ndo is use sim to simulate posterior outcomes:\\n'},\n",
       " {'index': 408,\n",
       "  'number': 390,\n",
       "  'content': '390\\n12. MONSTERS AND MIXTURES\\n0\\n1\\n0.0\\n0.5\\n1.0\\nintention\\nprobability\\naction=0, contact=0\\n0\\n1\\n0.0\\n0.5\\n1.0\\nintention\\nprobability\\naction=1, contact=0\\n0\\n1\\n0.0\\n0.5\\n1.0\\nintention\\nprobability\\naction=0, contact=1\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n50\\n100\\n150\\n200\\nresponse\\nFrequency\\naction=0, contact=0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n50\\n100\\n200\\nresponse\\nFrequency\\naction=1, contact=0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0\\n50\\n150\\n250\\nresponse\\nFrequency\\naction=0, contact=1\\nFigure 12.6. Posterior predictions of the ordered categorical model with\\ninteractions, m12.5. Each plot shows how the distribution of predicted re-\\nsponses varies by intention. The top row shows the distribution of poste-\\nrior probabilities of each outcome across values of intention for different\\nvalues of the other predictors. The bottom row shows the same interactions,\\nbut visualized as histograms of simulated outcomes. The black line seg-\\nments are intention equal to 0. The blue segments are when intention\\nis equal to 1.\\nR code\\n12.29\\nkA <- 0\\n# value for action\\nkC <- 1\\n# value for contact\\nkI <- 0:1\\n# values of intention to calculate over\\npdat <- data.frame(A=kA,C=kC,I=kI)\\ns <- sim( m12.5 , data=pdat )\\nsimplehist( s , xlab=\"response\" )\\nI’ve included these histograms in the bottom row of Figure 12.6. The black line segments are\\nthe simulated frequencies when intention is 0. The blue segments are the frequencies when\\nintention is 1. Notice the weight given to the middle response, 5, and the end responses\\nin each case. You can see this fact as well in the top-row plots, but the histograms make it\\nmuch more obvious. This is a general feature of ordered categories—some of the values are\\n'},\n",
       " {'index': 409,\n",
       "  'number': 391,\n",
       "  'content': '12.4. ORDERED CATEGORICAL PREDICTORS\\n391\\nmuch more salient than others. This is one of the reasons they are better than treating the\\noutcome as an ordinary metric variable.\\nRethinking: Staring into the abyss. The plotting code for ordered logistic models is complicated,\\ncompared to that of models from previous chapters. But as models become more monstrous, so too\\ndoes the code needed to compute predictions and display them. With power comes hardship. It’s\\nbetter to see the guts of the machine than to live in awe or fear of it. Software can be and often is\\nwritten to hide all the monstrosity from us. But this doesn’t make it go away. Instead, it just makes\\nthe models forever mysterious. For some users, mystery translates into awe. For others, it translates\\ninto skepticism. Neither condition is necessary, as long as we’re willing to learn the structure of the\\nmodels we are using. And if you aren’t willing to learn the structure of the models, then don’t do your\\nown statistics. Instead, collaborate with or hire a statistician.\\n12.4. Ordered categorical predictors\\nWe can handle ordered outcome variables using a categorical model with a cumulative\\nlink. That was the previous section. What about ordered predictor variables? We could\\njust include them as continuous predictors like in any linear model. But this isn’t ideal. Just\\nlike with ordered outcomes, we don’t really want to assume that the distance between each\\nordinal value is the same. Luckily, we don’t have to. We can construct ordered effects as well\\nas ordered outcomes.189\\nThe Trolley data from the previous section contains a good example. Let’s look at the\\nedu variable, which contains levels of completed education for each individual:\\nR code\\n12.30\\nlibrary(rethinking)\\ndata(Trolley)\\nd <- Trolley\\nlevels(d$edu)\\n[1] \"Bachelor\\'s Degree\"\\n\"Elementary School\"\\n\"Graduate Degree\"\\n[4] \"High School Graduate\" \"Master\\'s Degree\"\\n\"Middle School\"\\n[7] \"Some College\"\\n\"Some High School\"\\nThere are 8 different levels of completed education in the sample. Unfortunately, they aren’t\\nactually in order, from lowest to highest. This is typical with R, when it constructs a factor\\nvariable from character data. So the first step is to code these into an ordered variable, with\\nthe lowest level being 1 and the highest 8. Then we’ll think about constructing ordered ef-\\nfects out of it. The proper order is: [2] Elementary School, [6] Middle School, [8] Some High\\nSchool, [4] High School Graduate, [7] Some College, [1] Bachelor’s Degree, [5] Master’s De-\\ngree, and [3] Graduate Degree. We can just make a vector of new values to map onto those,\\nlike this:\\nR code\\n12.31\\nedu_levels <- c( 6 , 1 , 8 , 4 , 7 , 2 , 5 , 3 )\\nd$edu_new <- edu_levels[ d$edu ]\\nNow edu_new contains values from 1 to 8 in the right order of ascending education.\\nNow for the fun part. The notion with ordered predictor variables is that each step up in\\nvalue comes with its own incremental, or marginal, effect on the outcome. So that implies\\nwe want to infer, using a parameter, each of those incremental effects. With 8 education\\n'},\n",
       " {'index': 410,\n",
       "  'number': 392,\n",
       "  'content': '392\\n12. MONSTERS AND MIXTURES\\nlevels, we’ll need 7 parameters. The first level (Elementary School) will be absorbed into the\\nintercept. Then the first increment comes from moving from Elementary School to Middle\\nSchool. In that case we’ll add the first effect to the linear model:\\nϕi = δ1 + other stuff\\nwhere the parameter δ1 is the effect of completing Middle School and “other stuff” is all of\\nthe other terms you want in your linear model. Another individual goes on to finish the third\\nlevel, Some High School, and that individual’s linear model is:\\nϕi = δ1 + δ2 + other stuff\\nwhere δ2 is the incremental effect of finishing some (but not all) High School. It goes on\\nlike this, adding another incremental effect for each completed level. An individual with a\\nGraduate Degree, level 8, gets the linear model:\\nϕi =\\n7\\nX\\nj=1\\nδj + other stuff\\nAnd this sum of all the δ parameters is the maximum education effect. It will be very con-\\nvenient for interpretation if we call this maximum sum an ordinary coefficient like βE and\\nthen let the δ parameters be fractions of it. If we also make a dummy δ0 = 0 then we can\\nwrite it all very compactly. Like this:\\nϕi = βE\\nEi−1\\nX\\nj=0\\nδj + other stuff\\nwhere Ei is the completed education level of individual i. Now the sum of every δj is 1, and\\nwe can interpret the maximum education effect by looking at βE. In the case of an individual\\nwith Ei = 1, βE does’t appear in the linear model, because βEδ0 = 0.\\nThis βE move also helps us define priors. If the prior expectation is that all of the levels\\nhave the same incremental effect, then we want all the δj’s to have the same prior. We can do\\nthat now and still set a separate prior for maximum effect on βE. βE can be negative as well,\\nin which case all of the incremental effects are incrementally negative.\\nI appreciate that all of this is rather bizarre. We are deep inside the tide prediction en-\\ngine (Chapter 11) now. Understanding always comes with use and practice. So let’s build\\neducation into the ordered logit model as an ordered predictor. First, here’s a mathematical\\nversion of the full model. The probability of the outcome and the linear model are:\\nRi ∼Ordered-logit(ϕi, κ)\\nϕi = βE\\nEi−1\\nX\\nj=0\\nδj + βAAI + βIIi + βCCi\\nAnd so we need a bunch of priors. The priors for the cutpoints are on the logit scale, so we’ll\\nuse our regular(-izing) prior with standard deviation 1.5. The slopes get narrower priors—\\neach of these is a log-odds difference.\\nκk ∼Normal(0, 1.5)\\nβA, βI, βC, βE ∼Normal(0, 1)\\nδ ∼Dirichlet(α)\\n'},\n",
       " {'index': 411,\n",
       "  'number': 393,\n",
       "  'content': '12.4. ORDERED CATEGORICAL PREDICTORS\\n393\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\nindex\\nprobability\\nFigure 12.7. Simulated draws from a Dirich-\\nlet prior with α = {2, 2, 2, 2, 2, 2, 2}. The\\nhighlighted vector isn’t special but just serves\\nto show how much variation can exist in a sin-\\ngle vector. This prior doesn’t expect all the\\nprobabilities to be equal. Instead it expects\\nthat any of the probabilities could be bigger or\\nsmaller than the others.\\nThe last line is the new part. The prior for the δ vector is a Dirichlet distribution.190\\nThe Dirichlet distribution is the multivariate extension of the beta distribution. We met\\nthe beta distribution earlier in this chapter. Like the beta, the Dirichlet is a distribution for\\nprobabilities, values between zero and one that all sum to one. The beta is a distribution\\nfor two probabilities. The Dirichlet is a distribution for any number. And just like the beta,\\nthe Dirichlet is parameterized by pseudo-counts of observations. In the beta, these were the\\nparameters α and β, the prior counts of success and failures, respectively. In the Dirichlet,\\nthere is a just a long vector α with pseudo-counts for each possibility. If we assign the same\\nvalue to each, it is a uniform prior. The larger the α values, the more prior information that\\nthe probabilities are all the same.\\nWe’ll use a very weak prior with each value inside α being 2. Let’s simulate from this\\nprior and visualize the implications for prior vectors of δ values.\\nR code\\n12.32\\nlibrary(gtools)\\nset.seed(1805)\\ndelta <- rdirichlet( 10 , alpha=rep(2,7) )\\nstr(delta)\\nnum [1:10, 1:7] 0.1053 0.2504 0.1917 0.1241 0.0877 ...\\nWe end up with 10 vectors of 7 probabilities, each summing to 1. Let’s plot these vectors:\\nR code\\n12.33\\nh <- 3\\nplot( NULL , xlim=c(1,7) , ylim=c(0,0.4) , xlab=\"index\" , ylab=\"probability\" )\\nfor ( i in 1:nrow(delta) ) lines( 1:7 , delta[i,] , type=\"b\" ,\\npch=ifelse(i==h,16,1) , lwd=ifelse(i==h,4,1.5) ,\\ncol=ifelse(i==h,\"black\",col.alpha(\"black\",0.7)) )\\nFigure 12.7 displays the result. I’ve highlighted one of the vectors to show the variation in\\na single vector. The prior doesn’t expect all of the probabilities to be the same so much as it\\ndoesn’t expect any particular value to be bigger or smaller than the others.\\nIn coding this model, we need some variable fiddling to handle the δ0 = 0 bit. Let me\\nshow you the model code and then explain.\\n'},\n",
       " {'index': 412,\n",
       "  'number': 394,\n",
       "  'content': '394\\n12. MONSTERS AND MIXTURES\\nR code\\n12.34\\ndat <- list(\\nR = d$response ,\\naction = d$action,\\nintention = d$intention,\\ncontact = d$contact,\\nE = as.integer( d$edu_new ),\\n# edu_new as an index\\nalpha = rep( 2 , 7 ) )\\n# delta prior\\nm12.6 <- ulam(\\nalist(\\nR ~ ordered_logistic( phi , kappa ),\\nphi <- bE*sum( delta_j[1:E] ) + bA*action + bI*intention + bC*contact,\\nkappa ~ normal( 0 , 1.5 ),\\nc(bA,bI,bC,bE) ~ normal( 0 , 1 ),\\nvector[8]: delta_j <<- append_row( 0 , delta ),\\nsimplex[7]: delta ~ dirichlet( alpha )\\n), data=dat , chains=4 , cores=4 )\\nThe top part just builds the data list. This is familiar to you by now. Notice that the data\\nlist contains the alpha prior. We’re passing it in as “data,” but it is just the definition of\\nthe Dirichlet prior in the formula. The model itself is just like the models in the previous\\nsection, except for the bE term in the linear model and the last two lines of the formula,\\ndefining delta_j and delta. I’m also using some more advanced syntax in the model. But\\nwe can take this one piece at a time.\\nIn order to sum over the δ parameters, the linear model contains the term bE*sum(\\ndelta_j[1:E] ). This bit of code computes the expression βE\\nPEi−1\\nj=0 δj. The vector delta_j\\nhas 8 values in it. The first one is δ0 = 0. The other 7 values are the other δ parameters. The\\n[1:E] pulls out the first E values, where E is the education level of each individual.\\nThe code builds the delta_j vector by appending the actual delta parameter vector\\nonto a zero: delta_j <<- append_row( 0 , delta ). The append_row function is not\\nan R function, but rather a Stan function. It just glues together two vectors into one longer\\nvector. It’s like doing c(0,delta) in R. Notice the vector[8]: in front of this line. That\\nis an explicit type and dimension declaration. I’m telling Stan to make delta_j a vector of\\nlength 8. This kind of index fiddling is the joyless reality of statistical programming. You do\\nhave to be careful and keep track of what is going where. It gets easier the more you do it.\\nFinally, we reach the prior distribution for the δ/delta parameters themselves. Recall\\nthat these delta values must sum to one. This kind of vector, in which all the values sum to\\none (or any other constant), has a special name, a simplex. Stan kindly provides a special\\nvariable type, simplex, which enforces the sum-to-one constraint for you. And then we can\\nassign the delta vector the Dirichlet prior.\\nAnd it runs. This model samples more slowly than the other models so far in the book.\\nBut it still won’t take that long. On my most ancient 2013 edition laptop, it took 20 min-\\nutes total. If you don’t have 4 cores so that the 4 chains can run in parallel, it’ll take longer.\\nRegardless, it is important to get comfortable with waiting for a good approximation of the\\nposterior, instead of using some terrible-but-fast approximation.\\nLet’s look at the marginal posterior distributions, leaving out the kappa cutpoints:\\n'},\n",
       " {'index': 413,\n",
       "  'number': 395,\n",
       "  'content': '12.4. ORDERED CATEGORICAL PREDICTORS\\n395\\nR code\\n12.35\\nprecis( m12.6 , depth=2 , omit=\"kappa\" )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nbE\\n-0.32 0.17 -0.61 -0.07\\n1062\\n1\\nbC\\n-0.96 0.05 -1.03 -0.88\\n1716\\n1\\nbI\\n-0.72 0.04 -0.78 -0.66\\n2588\\n1\\nbA\\n-0.70 0.04 -0.76 -0.64\\n2089\\n1\\ndelta[1]\\n0.23 0.13\\n0.05\\n0.47\\n1422\\n1\\ndelta[2]\\n0.14 0.09\\n0.03\\n0.30\\n2611\\n1\\ndelta[3]\\n0.19 0.11\\n0.05\\n0.38\\n2388\\n1\\ndelta[4]\\n0.17 0.10\\n0.04\\n0.35\\n2234\\n1\\ndelta[5]\\n0.04 0.05\\n0.01\\n0.11\\n1111\\n1\\ndelta[6]\\n0.10 0.07\\n0.02\\n0.23\\n2305\\n1\\ndelta[7]\\n0.12 0.08\\n0.03\\n0.26\\n2118\\n1\\nThe overall association of education bE is negative—more educated individuals disapproved\\nmore of everything. The association is smaller than the treatment effects—at the posterior\\nmean, the most educated individuals in the sample disapprove of everything by about −0.3,\\nwhile adding action to a story reduces approval by about 0.7. Careful not to think of this\\nassociation causally yet. Education is not a randomized treatment variable!\\nTo see what’s going on with the incremental effects, the delta parameters, we’ll have to\\nlook at them as a multivariate distribution. The easiest way to do this is the use pairs:\\nR code\\n12.36\\ndelta_labels <- c(\"Elem\",\"MidSch\",\"SHS\",\"HSG\",\"SCol\",\"Bach\",\"Mast\",\"Grad\")\\npairs( m12.6 , pars=\"delta\" , labels=delta_labels )\\nThis is displayed as Figure 12.8. First notice that all of these parameters are negatively cor-\\nrelated with one another. This is a result of the constraint that they sum to one. If one gets\\nlarger, the others have to get smaller. Next notice that all but one level of education produces\\nsome modest increment on average. Is it is only Some College (SCol) that seems to have only\\na tiny, if any, incremental effect.\\nIt’ll be instructive to compare the posterior above to the inference we get from a more\\nconventional model with education entered as an ordinary continuous variable. We’ll nor-\\nmalize education level first, so that it ranges from 0 to 1. This will make the resulting param-\\neter comparable to the one in the model above.\\nR code\\n12.37\\ndat$edu_norm <- normalize( d$edu_new )\\nm12.7 <- ulam(\\nalist(\\nR ~ ordered_logistic( mu , cutpoints ),\\nmu <- bE*edu_norm + bA*action + bI*intention + bC*contact,\\nc(bA,bI,bC,bE) ~ normal( 0 , 1 ),\\ncutpoints ~ normal( 0 , 1.5 )\\n), data=dat , chains=4 , cores=4 )\\nprecis( m12.7 )\\n6 vector or matrix parameters hidden. Use depth=2 to show them.\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nbE -0.10 0.09 -0.24\\n0.04\\n2224\\n1\\n'},\n",
       " {'index': 414,\n",
       "  'number': 396,\n",
       "  'content': '396\\n12. MONSTERS AND MIXTURES\\nFigure 12.8. Posterior distribution of incremental education effects. Every\\nadditional level of education tends to add a little more disapproval, except\\nfor Some College (SCol), which adds very little, if anything.\\nbC -0.96 0.05 -1.04 -0.88\\n2237\\n1\\nbI -0.72 0.04 -0.78 -0.66\\n2051\\n1\\nbA -0.70 0.04 -0.77 -0.64\\n1995\\n1\\nThis model seems to think that education is much more weakly associated with rating. This\\nis possibly because the effect isn’t actually linear. Different levels have different incremental\\nassociations.\\nThis example has been fine for teaching how to build ordered predictors. But from a\\ncausal perspective, a lurking concern must be whether the association with education is\\nspurious. Education is highly correlated with age, because age causes (for lack of a better\\nword) the completion of levels of education. So there is plausibly a backdoor from educa-\\ntion through age to rating. In the practice problems at the end of the chapter, I’ll ask you to\\ndraw the DAG that this implies and investigate it with some new modeling.\\n'},\n",
       " {'index': 415,\n",
       "  'number': 397,\n",
       "  'content': '12.6. PRACTICE\\n397\\n12.5. Summary\\nThis chapter introduced several new types of regression, all of which are generalizations\\nof generalized linear models (GLMs). Ordered logistic models are useful for categorical\\noutcomes with a strict ordering. They are built by attaching a cumulative link function to\\na categorical outcome distribution. Zero-inflated models mix together two different out-\\ncome distributions, allowing us to model outcomes with an excess of zeros. Models for over-\\ndispersion, such as beta-binomial and gamma-Poisson, draw the expected value of each ob-\\nservation from a distribution that changes shape as a function of a linear model. The next\\nchapter further generalizes these model types by introducing multilevel models.\\n12.6. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n12E1. What is the difference between an ordered categorical variable and an unordered one? Define\\nand then give an example of each.\\n12E2. What kind of link function does an ordered logistic regression employ? How does it differ\\nfrom an ordinary logit link?\\n12E3. When count data are zero-inflated, using a model that ignores zero-inflation will tend to in-\\nduce which kind of inferential error?\\n12E4. Over-dispersion is common in count data. Give an example of a natural process that might\\nproduce over-dispersed counts. Can you also give an example of a process that might produce under-\\ndispersed counts?\\n12M1. At a certain university, employees are annually rated from 1 to 4 on their productivity, with\\n1 being least productive and 4 most productive. In a certain department at this certain university\\nin a certain year, the numbers of employees receiving each rating were (from 1 to 4): 12, 36, 7, 41.\\nCompute the log cumulative odds of each rating.\\n12M2. Make a version of Figure 12.5 for the employee ratings data given just above.\\n12M3. Can you modify the derivation of the zero-inflated Poisson distribution (ZIPoisson) from\\nthe chapter to construct a zero-inflated binomial distribution?\\n12H1. In 2014, a paper was published that was entitled “Female hurricanes are deadlier than male\\nhurricanes.”191 As the title suggests, the paper claimed that hurricanes with female names have caused\\ngreater loss of life, and the explanation given is that people unconsciously rate female hurricanes\\nas less dangerous and so are less likely to evacuate. Statisticians severely criticized the paper after\\npublication. Here, you’ll explore the complete data used in the paper and consider the hypothesis\\nthat hurricanes with female names are deadlier. Load the data with:\\nR code\\n12.38\\nlibrary(rethinking)\\ndata(Hurricanes)\\nAcquaint yourself with the columns by inspecting the help ?Hurricanes. In this problem, you’ll fo-\\ncus on predicting deaths using femininity of each hurricane’s name. Fit and interpret the simplest\\npossible model, a Poisson model of deaths using femininity as a predictor. You can use quap or\\nulam. Compare the model to an intercept-only Poisson model of deaths. How strong is the asso-\\nciation between femininity of name and deaths? Which storms does the model fit (retrodict) well?\\nWhich storms does it fit poorly?\\n'},\n",
       " {'index': 416,\n",
       "  'number': 398,\n",
       "  'content': '398\\n12. MONSTERS AND MIXTURES\\n12H2. Counts are nearly always over-dispersed relative to Poisson. So fit a gamma-Poisson (aka\\nnegative-binomial) model to predict deaths using femininity. Show that the over-dispersed model\\nno longer shows as precise a positive association between femininity and deaths, with an 89% interval\\nthat overlaps zero. Can you explain why the association diminished in strength?\\n12H3. In the data, there are two measures of a hurricane’s potential to cause death: damage_norm\\nand min_pressure. Consult ?Hurricanes for their meanings. It makes some sense to imagine that\\nfemininity of a name matters more when the hurricane is itself deadly. This implies an interaction\\nbetween femininity and either or both of damage_norm and min_pressure. Fit a series of models\\nevaluating these interactions. Interpret and compare the models. In interpreting the estimates, it\\nmay help to generate counterfactual predictions contrasting hurricanes with masculine and feminine\\nnames. Are the effect sizes plausible?\\n12H4. In the original hurricanes paper, storm damage (damage_norm) was used directly. This as-\\nsumption implies that mortality increases exponentially with a linear increase in storm strength, be-\\ncause a Poisson regression uses a log link. So it’s worth exploring an alternative hypothesis: that the\\nlogarithm of storm strength is what matters. Explore this by using the logarithm of damage_norm as\\na predictor. Using the best model structure from the previous problem, compare a model that uses\\nlog(damage_norm) to a model that uses damage_norm directly. Compare their PSIS/WAIC values\\nas well as their implied predictions. What do you conclude?\\n12H5. One hypothesis from developmental psychology, usually attributed to Carol Gilligan, pro-\\nposes that women and men have different average tendencies in moral reasoning. Like most hypothe-\\nses in social psychology, it is descriptive, not causal. The notion is that women are more concerned\\nwith care (avoiding harm), while men are more concerned with justice and rights. Evaluate this hy-\\npothesis, using the Trolley data, supposing that contact provides a proxy for physical harm. Are\\nwomen more or less bothered by contact than are men, in these data? Figure out the model(s) that is\\nneeded to address this question.\\n12H6. The data in data(Fish) are records of visits to a national park. See ?Fish for details. The\\nquestion of interest is how many fish an average visitor takes per hour, when fishing. The problem is\\nthat not everyone tried to fish, so the fish_caught numbers are zero-inflated. As with the monks\\nexample in the chapter, there is a process that determines who is fishing (working) and another pro-\\ncess that determines fish per hour (manuscripts per day), conditional on fishing (working). We want\\nto model both. Otherwise we’ll end up with an underestimate of rate of fish extraction from the park.\\nYou will model these data using zero-inflated Poisson GLMs. Predict fish_caught as a function\\nof any of the other variables you think are relevant. One thing you must do, however, is use a proper\\nPoisson offset/exposure in the Poisson portion of the zero-inflated model. Then use the hours vari-\\nable to construct the offset. This will adjust the model for the differing amount of time individuals\\nspent in the park.\\n12H7. In the trolley data—data(Trolley)—we saw how education level (modeled as an ordered\\ncategory) is associated with responses. But is this association causal? One plausible confound is that\\neducation is also associated with age, through a causal process: People are older when they finish\\nschool than when they begin it. Reconsider the Trolley data in this light. Draw a DAG that repre-\\nsents hypothetical causal relationships among response, education, and age. Which statical model or\\nmodels do you need to evaluate the causal influence of education on responses? Fit these models to\\nthe trolley data. What do you conclude about the causal relationships among these three variables?\\n12H8. Consider one more variable in the trolley data: Gender. Suppose that gender might influence\\neducation as well as response directly. Draw the DAG now that includes response, education, age, and\\ngender. Using only the DAG, is it possible that the inferences from 12H7 above are confounded by\\ngender? If so, define any additional models you need to infer the causal influence of education on\\nresponse. What do you conclude?\\n'},\n",
       " {'index': 417,\n",
       "  'number': 399,\n",
       "  'content': '13 Models With Memory\\nIn the year 1985, Clive Wearing lost his mind, but not his music.192 Wearing was a\\nmusicologist and accomplished musician, but the same virus that causes cold sores, Herpes\\nsimplex, snuck into his brain and ate his hippocampus. The result was chronic anterograde\\namnesia—he cannot form new long-term memories. He remembers how to play the piano,\\nthough he cannot remember that he played it 5 minutes ago. Wearing now lives moment to\\nmoment, unaware of anything more than a few minutes into the past. Every cup of coffee is\\nthe first he has ever had.\\nMany statistical models also have anterograde amnesia. As the models move from one\\ncluster—individual, group, location—in the data to another, estimating parameters for each\\ncluster, they forget everything about the previous clusters. They behave this way, because\\nthe assumptions force them to. Any of the models from previous chapters that used dummy\\nvariables (page 153) to handle categories are programmed for amnesia. These models im-\\nplicitly assume that nothing learned about any one category informs estimates for the other\\ncategories—the parameters are independent of one another and learn from completely sepa-\\nrate portions of the data. This would be like forgetting you had ever been in a café, each time\\nyou go to a new café. Cafés do differ, but they are also alike.\\nAnterograde amnesia is bad for learning about the world. We want models that instead\\nuse all of the information in savvy ways. This does not mean treating all clusters as if they\\nwere the same. Instead it means learning simultaneously about each cluster while learning\\nabout the population of clusters. Doing both estimation tasks at the same time allows us to\\ntransfer information across clusters, and that transfer improves accuracy. That is the value\\nof remembering.\\nConsider cafés again. Suppose we program a robot to visit two cafés, order coffee, and\\nestimate the waiting times at each. The robot begins with a vague prior for the waiting times,\\nsay with a mean of 5 minutes and a standard deviation of 1. After ordering a cup of coffee\\nat the first café, the robot observes a waiting time of 4 minutes. It updates its prior, using\\nBayes’ theorem of course, with this information. This gives it a posterior distribution for the\\nwaiting time at the first café.\\nNow the robot moves on to a second café. When this robot arrives at the next café, what\\nis its prior? It could just use the posterior distribution from the first café as its prior for the\\nsecond café. But that implicitly assumes that the two cafés have the same average waiting\\ntime. Cafés are all pretty much the same, but they aren’t identical. Likewise, it doesn’t make\\nmuch sense to ignore the observation from the first café. That would be anterograde amnesia.\\nSo how can the coffee robot do better? It needs to represent the population of cafés and\\nlearn about that population. The distribution of waiting times in the population becomes\\nthe prior for each café. But unlike priors in previous chapters, this prior is actually learned\\n399\\n'},\n",
       " {'index': 418,\n",
       "  'number': 400,\n",
       "  'content': '400\\n13. MODELS WITH MEMORY\\nfrom the data. This means the robot tracks a parameter for each café as well as at least two\\nparameters to describe the population of cafés: an average and a standard deviation. As the\\nrobot observes waiting times, it updates everything: the estimates for each café as well as the\\nestimates for the population. If the population seems highly variable, then the prior is flat\\nand uninformative and, as a consequence, the observations at any one café do very little to\\nthe estimate at another. If instead the population seems to contain little variation, then the\\nprior is narrow and highly informative. An observation at any one café will have a big impact\\non estimates at any other café.\\nIn this chapter, you’ll see the formal version of this argument and how it leads us to mul-\\ntilevel models. These models remember features of each cluster in the data as they learn\\nabout all of the clusters. Depending upon the variation among clusters, which is learned\\nfrom the data as well, the model pools information across clusters. This pooling tends to\\nimprove estimates about each cluster. This improved estimation leads to several, more prag-\\nmatic sounding, benefits of the multilevel approach. I mentioned them in Chapter 1. They\\nare worth repeating.\\n(1) Improved estimates for repeat sampling. When more than one observation arises\\nfrom the same individual, location, or time, then traditional, single-level models\\neither maximally underfit or overfit the data.\\n(2) Improved estimates for imbalance in sampling. When some individuals, locations,\\nor times are sampled more than others, multilevel models automatically cope with\\ndiffering uncertainty across these clusters. This prevents over-sampled clusters\\nfrom unfairly dominating inference.\\n(3) Estimates of variation. If our research questions include variation among individu-\\nals or other groups within the data, then multilevel models are a big help, because\\nthey model variation explicitly.\\n(4) Avoid averaging, retain variation. Frequently, scholars pre-average some data to\\nconstruct variables. This can be dangerous, because averaging removes variation,\\nand there are also typically several different ways to perform the averaging. Aver-\\naging therefore both manufactures false confidence and introduces arbitrary data\\ntransformations. Multilevel models allow us to preserve the uncertainty and avoid\\ndata transformations.\\nAll of these benefits flow out of the same strategy and model structure. You learn one basic\\ndesign and you get all of this for free.\\nWhen it comes to regression, multilevel regression deserves to be the default approach.\\nThere are certainly contexts in which it would be better to use an old-fashioned single-level\\nmodel. But the contexts in which multilevel models are superior are much more numer-\\nous. It is better to begin to build a multilevel analysis, and then realize it’s unnecessary, than\\nto overlook it. And once you grasp the basic multilevel strategy, it becomes much easier\\nto incorporate related tricks such as allowing for measurement error in the data and even\\nmodeling missing data itself (Chapter 15).\\nThere are costs of the multilevel approach. The first is that we have to make some new\\nassumptions. We have to define the distributions from which the characteristics of the clus-\\nters arise. Luckily, conservative maximum entropy distributions do an excellent job in this\\ncontext. Second, there are new estimation challenges that come with the full multilevel ap-\\nproach. These challenges lead us headfirst into MCMC estimation. Third, multilevel models\\ncan be hard to understand, because they make predictions at different levels of the data. In\\n'},\n",
       " {'index': 419,\n",
       "  'number': 401,\n",
       "  'content': '13.1. EXAMPLE: MULTILEVEL TADPOLES\\n401\\nmany cases, we are interested in only one or a few of those levels, and as a consequence,\\nmodel comparison using metrics like DIC and WAIC becomes more subtle. The basic logic\\nremains unchanged, but now we have to make more decisions about which parameters in\\nthe model we wish to focus on.\\nThis chapter has the following progression. First, we’ll work through an extended exam-\\nple of building and fitting a multilevel model for clustered data. Then we’ll simulate clustered\\ndata, to demonstrate the improved accuracy the approach delivers. This improved accuracy\\narises from the same underfitting and overfitting trade-off you met in Chapter 7. Then we’ll\\nfinish by looking at contexts in which there is more than one type of clustering. All of this\\nwork lays a foundation for more advanced multilevel examples in the next two chapters.\\nRethinking: A model by any other name. Multilevel models go by many different names, and some\\nstatisticians use the same names for different specialized variants, while others use them all inter-\\nchangeably. The most common synonyms for “multilevel” are hierarchical and mixed effects.\\nThe type of parameters that appear in multilevel models are most commonly known as random ef-\\nfects, which itself can mean very different things to different analysts and in different contexts.193\\nAnd even the innocent term “level” can mean different things to different people. There’s really no\\ncure for this swamp of vocabulary aside from demanding a mathematical or algorithmic definition\\nof the model. Otherwise, there will always be ambiguity.\\n13.1. Example: Multilevel tadpoles\\nThe heartwarming focus of this example are experiments exploring Reed frog (Hyper-\\nolius spinigularis) tadpole mortality.194 The natural history background to these data is very\\ninteresting. Take a look at the full paper, if amphibian life history dynamics interests you.\\nBut even if it doesn’t, load the data and acquaint yourself with the variables:\\nR code\\n13.1\\nlibrary(rethinking)\\ndata(reedfrogs)\\nd <- reedfrogs\\nstr(d)\\n\\'data.frame\\': 48 obs. of\\n5 variables:\\n$ density : int\\n10 10 10 10 10 10 10 10 10 10 ...\\n$ pred\\n: Factor w/ 2 levels \"no\",\"pred\": 1 1 1 1 1 1 1 1 2 2 ...\\n$ size\\n: Factor w/ 2 levels \"big\",\"small\": 1 1 1 1 2 2 2 2 1 1 ...\\n$ surv\\n: int\\n9 10 7 10 9 9 10 9 4 9 ...\\n$ propsurv: num\\n0.9 1 0.7 1 0.9 0.9 1 0.9 0.4 0.9 ...\\nFor now, we’ll only be interested in number surviving, surv, out of an initial count, density.\\nIn the practice at the end of the chapter, you’ll consider the other variables, which are exper-\\nimental manipulations.\\nThere is a lot of variation in these data. Some of the variation comes from experimental\\ntreatment. But a lot of it comes from other sources. Think of each row as a “tank,” an exper-\\nimental environment that contains tadpoles. There are lots of unmeasured things peculiar\\nto each tank, and these unmeasured factors create variation in survival across tanks, even\\nwhen all the predictor variables have the same value. These tanks are an example of a cluster\\nvariable. Multiple observations, the tadpoles in this case, are made within each cluster.\\n'},\n",
       " {'index': 420,\n",
       "  'number': 402,\n",
       "  'content': '402\\n13. MODELS WITH MEMORY\\nSo we have repeat measures and heterogeneity across clusters. If we ignore the clusters,\\nassigning the same intercept to each of them, then we risk ignoring important variation in\\nbaseline survival. This variation could mask association with other variables. If we instead\\nestimate a unique intercept for each cluster, using a dummy variable for each tank, we in-\\nstead practice anterograde amnesia. After all, tanks are different but each tank does help us\\nestimate survival in the other tanks. So it doesn’t make sense to forget entirely, moving from\\none tank to another.\\nA multilevel model, in which we simultaneously estimate both an intercept for each tank\\nand the variation among tanks, is what we want. This will be a varying intercepts model.\\nVarying intercepts are the simplest kind of varying effects.195 For each cluster in the data,\\nwe use a unique intercept parameter. This is no different than the categorical variable exam-\\nples from previous chapters, except now we also adaptively learn the prior that is common\\nto all of these intercepts. This adaptive learning is the absence of amnesia discussed at the\\nstart of the chapter. When what we learn about each cluster informs all the other clusters,\\nwe learn the prior simultaneous to learning the intercepts.\\nHere is a model for predicting tadpole mortality in each tank, using the regularizing\\npriors of earlier chapters:\\nSi ∼Binomial(Ni, pi)\\nlogit(pi) = αtank[i]\\n[unique log-odds for each tank]\\nαj ∼Normal(0, 1.5)\\nfor j = 1..48\\nAnd you can approximate this posterior using ulam as in previous chapters:\\nR code\\n13.2\\n# make the tank cluster variable\\nd$tank <- 1:nrow(d)\\ndat <- list(\\nS = d$surv,\\nN = d$density,\\ntank = d$tank )\\n# approximate posterior\\nm13.1 <- ulam(\\nalist(\\nS ~ dbinom( N , p ) ,\\nlogit(p) <- a[tank] ,\\na[tank] ~ dnorm( 0 , 1.5 )\\n), data=dat , chains=4 , log_lik=TRUE )\\nIf you inspect the posterior, precis(m13.1,depth=2), you’ll see 48 different intercepts, one\\nfor each tank. To get each tank’s expected survival probability, just take one of the a values\\nand then use the logistic transform. So far there is nothing new here.\\nNow let’s do the multilevel model, which adaptively pools information across tanks. All\\nthat is required to enable adaptive pooling is to make the prior for the a parameters a func-\\ntion of some new parameters. Here is the multilevel model, in mathematical form, with the\\n'},\n",
       " {'index': 421,\n",
       "  'number': 403,\n",
       "  'content': '13.1. EXAMPLE: MULTILEVEL TADPOLES\\n403\\nchanges from the previous model highlighted in blue:\\nSi ∼Binomial(Ni, pi)\\nlogit(pi) = αtank[i]\\nαj ∼Normal(¯α, σ)\\n[adaptive prior]\\n¯α ∼Normal(0, 1.5)\\n[prior for average tank]\\nσ ∼Exponential(1)\\n[prior for standard deviation of tanks]\\nNotice that the prior for the tank intercepts is now a function of two parameters, ¯α and\\nσ. You can say ¯α like “bar alpha.” The bar means average. These two parameters inside the\\nprior is where the “multi” in multilevel arises.196 The Gaussian distribution with mean ¯α and\\nstandard deviation σ is the prior for each tank’s intercept. But that prior itself has priors for ¯α\\nand σ. So there are two levels in the model, each resembling a simpler model. In the top level,\\nthe outcome is S, the parameters are the vector α, and the prior is αj ∼Normal(¯α, σ). In the\\nsecond level, the “outcome” variable is the vector of intercept parameters, α. The parameters\\nare ¯α and σ, and their priors are ¯α ∼Normal(0, 1.5) and σ ∼Exponential(1).\\nThese two parameters, ¯α and σ, are often referred to as hyperparameters. They are\\nparameters for parameters. And their priors are often called hyperpriors. In principle,\\nthere is no limit to how many “hyper” levels you can install in a model. For example, different\\npopulations of tanks could be embedded within different regions of habitat. But in practice\\nthere are limits, both because of computation and our ability to understand the model.\\nRethinking: Why Gaussian tanks? In the multilevel tadpole model, the population of tanks is as-\\nsumed to be Gaussian. Why? The least satisfying answer is “convention.” The Gaussian assumption\\nis extremely common. A more satisfying answer is “pragmatism.” The Gaussian assumption is easy\\nto work with, and it generalizes easily to more than one dimension. This generalization will be impor-\\ntant for handling varying slopes in the next chapter. But my preferred answer is instead “entropy.” If\\nall we are willing to say about a distribution is the mean and variance, then the Gaussian is the most\\nconservative assumption (Chapter 10). Using a Gaussian here does not force the resulting posterior\\ndistribution of α parameters to be symmetric or have a Gaussian shape. The only information in a\\nGaussian prior (or likelihood) is finite variance. The distribution looks symmetric, because if you\\ndon’t say how it is skewed, then symmetric is the maximum entropy shape. Above all, there is no rule\\nrequiring the Gaussian distribution of varying effects. So if you have a good reason to use another\\ndistribution, then do so. The practice problems at the end of the chapter provide an example.\\nComputing the posterior computes both levels simultaneously, in the same way that our\\nrobot at the start of the chapter learned both about each café and the variation among cafés.\\nBut you cannot fit this model with quap. Why? Because the probability of the data must now\\naverage over the level 2 parameters ¯α and σ. But quap just hill climbs, using static values for\\nall of the parameters. It can’t see the levels. For more explanation, see the Overthinking box\\nfurther down. You can however fit this model with ulam:\\nR code\\n13.3\\nm13.2 <- ulam(\\nalist(\\nS ~ dbinom( N , p ) ,\\nlogit(p) <- a[tank] ,\\na[tank] ~ dnorm( a_bar , sigma ) ,\\na_bar ~ dnorm( 0 , 1.5 ) ,\\n'},\n",
       " {'index': 422,\n",
       "  'number': 404,\n",
       "  'content': '404\\n13. MODELS WITH MEMORY\\nsigma ~ dexp( 1 )\\n), data=dat , chains=4 , log_lik=TRUE )\\nThis model provides posterior distributions for 50 parameters: one overall sample intercept\\n¯α, the standard deviation among tanks σ, and then 48 per-tank intercepts. Let’s check WAIC\\nthough to see the effective number of parameters. We’ll compare the earlier model, m13.1,\\nwith the new multilevel model:\\nR code\\n13.4\\ncompare( m13.1 , m13.2 )\\nWAIC\\nSE dWAIC\\ndSE pWAIC weight\\nm13.2 200.0 7.19\\n0.0\\nNA\\n20.9\\n1\\nm13.1 215.9 4.43\\n15.9 4.03\\n26.2\\n0\\nThere are two facts to note here. First, the multilevel model has only 21 effective parameters.\\nThere are 28 fewer effective parameters than actual parameters, because the prior assigned\\nto each intercept shrinks them all towards the mean ¯α. In this case, the prior is reason-\\nably strong. Check the mean of sigma with precis and you’ll see it’s around 1.6. This is a\\nregularizing prior, like you’ve used in previous chapters, but now the amount of regu-\\nlarization has been learned from the data itself.197 Second, notice that the multilevel model\\nm13.2 has fewer effective parameters than the ordinary fixed model m13.1. This is despite\\nthe fact that the ordinary model has fewer actual parameters, only 48 instead of 50. The ex-\\ntra two parameters in the multilevel model allowed it to learn a more aggressive regularizing\\nprior, to adaptively regularize. This resulted in a less flexible posterior and therefore fewer\\neffective parameters.\\nOverthinking: QUAP fails, MCMC succeeds. Why doesn’t simple quadratic approximation, using\\nfor example quap, work with multilevel models? When a prior is itself a function of parameters, there\\nare two levels of uncertainty. This means that the probability of the data, conditional on the parame-\\nters, must average over each level. Ordinary quadratic approximation cannot handle the averaging in\\nthe likelihood, because in general it’s not possible to derive an analytical solution. That means there is\\nno unified function for calculating the log-posterior. So your computer cannot directly find its mini-\\nmum (the maximum of the posterior). Some other computational approach is needed. It is possible\\nto extend the mode-finding optimization strategy to these models, but we don’t want to be stuck with\\noptimization in general. One reason is that the posterior of these models is routinely non-Gaussian.\\nAnother is that optimization tends to be fragile in high dimensions.\\nStan actually does optimization. See ?optimizing. This is sometimes useful for getting an initial\\nestimate or verifying that your model compiles and runs.\\nTo appreciate the impact of this adaptive regularization, let’s plot and compare the pos-\\nterior means from models m13.1 and m13.2. The code that follows is long, only because it\\ndecorates the plot with informative labels. The basic code is just the first part, which extracts\\nsamples and computes means.\\nR code\\n13.5\\n# extract Stan samples\\npost <- extract.samples(m13.2)\\n# compute mean intercept for each tank\\n'},\n",
       " {'index': 423,\n",
       "  'number': 405,\n",
       "  'content': '13.1. EXAMPLE: MULTILEVEL TADPOLES\\n405\\n# also transform to probability with logistic\\nd$propsurv.est <- logistic( apply( post$a , 2 , mean ) )\\n# display raw proportions surviving in each tank\\nplot( d$propsurv , ylim=c(0,1) , pch=16 , xaxt=\"n\" ,\\nxlab=\"tank\" , ylab=\"proportion survival\" , col=rangi2 )\\naxis( 1 , at=c(1,16,32,48) , labels=c(1,16,32,48) )\\n# overlay posterior means\\npoints( d$propsurv.est )\\n# mark posterior mean probability across tanks\\nabline( h=mean(inv_logit(post$a_bar)) , lty=2 )\\n# draw vertical dividers between tank densities\\nabline( v=16.5 , lwd=0.5 )\\nabline( v=32.5 , lwd=0.5 )\\ntext( 8 , 0 , \"small tanks\" )\\ntext( 16+8 , 0 , \"medium tanks\" )\\ntext( 32+8 , 0 , \"large tanks\" )\\nYou can see the result in Figure 13.1. The horizontal axis is tank index, from 1 to 48. The\\nvertical is proportion of survivors in a tank. The filled blue points show the raw proportions,\\ncomputed from the observed counts. These values are already present in the data frame, in\\nthe propsurv column. The black circles are instead the varying intercepts. The horizontal\\ndashed line at about 0.8 is the estimated median survival proportion in the population of\\ntanks, α. It is not the same as the empirical mean survival. The vertical lines divide tanks\\nwith different initial counts of tadpoles—10 (left), 25 (middle), and 35 (right).\\nFirst, notice that in every case, the multilevel estimate is closer to the dashed line than\\nthe raw empirical estimate is. It’s as if the entire distribution of black circles has been shrunk\\ntowards the dashed line at the center of the data, leaving the blue points behind on the out-\\nside. This phenomenon is sometimes called shrinkage, and it results from regularization\\n(as in Chapter 7). Second, notice that the estimates for the smaller tanks have shrunk far-\\nther from the blue points. As you move from left to right in the figure, the initial densities\\nof tadpoles increase from 10 to 25 to 35, as indicated by the vertical dividers. In the small-\\nest tanks, it is easy to see differences between the open estimates and empirical blue points.\\nBut in the largest tanks, there is little difference between the blue points and open circles.\\nVarying intercepts for the smaller tanks, with smaller sample sizes, shrink more. Third, note\\nthat the farther a blue point is from the dashed line, the greater the distance between it and\\nthe corresponding multilevel estimate. Shrinkage is stronger, the further a tank’s empirical\\nproportion is from the global average α.\\nAll three of these phenomena arise from a common cause: pooling information across\\nclusters (tanks) to improve estimates. What pooling means here is that each tank provides\\ninformation that can be used to improve the estimates for all of the other tanks. Each tank\\nhelps in this way, because we made an assumption about how the varying log-odds in each\\ntank related to all of the others. We assumed a distribution, the normal distribution in this\\ncase. Once we have a distributional assumption, we can use Bayes’ theorem to optimally (in\\nthe small world only) share information among the clusters.\\n'},\n",
       " {'index': 424,\n",
       "  'number': 406,\n",
       "  'content': '406\\n13. MODELS WITH MEMORY\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ntank\\nproportion survival\\n1\\n16\\n32\\n48\\nsmall tanks\\nmedium tanks\\nlarge tanks\\nFigure 13.1. Empirical proportions of survivors in each tadpole tank,\\nshown by the filled blue points, plotted with the 48 per-tank parameters\\nfrom the multilevel model, shown by the black circles. The dashed line lo-\\ncates the average proportion of survivors across all tanks. The vertical lines\\ndivide tanks with different initial densities of tadpoles: small tanks (10 tad-\\npoles), medium tanks (25), and large tanks (35). In every tank, the posterior\\nmean from the multilevel model is closer to the dashed line than the empir-\\nical proportion is. This reflects the pooling of information across tanks, to\\nhelp with inference about each tank.\\nWhat does the inferred population distribution of survival look like? We can visualize\\nit by sampling from the posterior distribution, as usual. First we’ll plot 100 Gaussian dis-\\ntributions, one for each of the first 100 samples from the posterior distribution of both α\\nand σ. Then we’ll sample 8000 new log-odds of survival for individual tanks. The result will\\nbe a posterior distribution of variation in survival in the population of tanks. Before we do\\nthe sampling though, remember that “sampling” from a posterior distribution is not a sim-\\nulation of empirical sampling. It’s just a convenient way to characterize and work with the\\nuncertainty in the distribution. Now the sampling:\\nR code\\n13.6\\n# show first 100 populations in the posterior\\nplot( NULL , xlim=c(-3,4) , ylim=c(0,0.35) ,\\nxlab=\"log-odds survive\" , ylab=\"Density\" )\\nfor ( i in 1:100 )\\ncurve( dnorm(x,post$a_bar[i],post$sigma[i]) , add=TRUE ,\\ncol=col.alpha(\"black\",0.2) )\\n# sample 8000 imaginary tanks from the posterior distribution\\nsim_tanks <- rnorm( 8000 , post$a_bar , post$sigma )\\n# transform to probability and visualize\\ndens( inv_logit(sim_tanks) , lwd=2 , adj=0.1 )\\n'},\n",
       " {'index': 425,\n",
       "  'number': 407,\n",
       "  'content': '13.1. EXAMPLE: MULTILEVEL TADPOLES\\n407\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\nprobability survive\\nDensity\\nFigure 13.2. The inferred population of survival across tanks. Left: 100\\nGaussian distributions of the log-odds of survival, sampled from the poste-\\nrior of m13.2. Right: Survival probabilities for 8000 new simulated tanks,\\naveraging over the posterior distribution on the left.\\nThe results are displayed in Figure 13.2. Notice that there is uncertainty about both the\\nlocation, α, and scale, σ, of the population distribution of log-odds of survival. All of this\\nuncertainty is propagated into the simulated probabilities of survival.\\nRethinking: Varying intercepts as over-dispersion. In the previous chapter (page 369), the beta-\\nbinomial and gamma-Poisson models were presented as ways for coping with over-dispersion\\nof count data. Varying intercepts accomplish the same thing, allowing count outcomes to be over-\\ndispersed. They accomplish this, because when each observed count gets its own unique intercept, but\\nthese intercepts are pooled through a common distribution, the predictions expect over-dispersion\\njust like a beta-binomial or gamma-Poisson model would. Multilevel models are also mixtures. Com-\\npared to a beta-binomial or gamma-Poisson model, a binomial or Poisson model with a varying in-\\ntercept on every observed outcome will often be easier to estimate and easier to extend. There will be\\nan example of this approach, later in this chapter.\\nOverthinking: Priors for variance components. The examples in this book use weakly regularizing\\nexponential priors for variance components, the σ parameters that estimate the variation across clus-\\nters in the data. These exponential priors work very well in routine multilevel modeling. They express\\nonly a rough notion of an average standard deviation and regularize towards zero. But there are two\\ncommon contexts in which they can be problematic. First, sometimes there isn’t much information\\nin the data with which to estimate the variance. For example, if you only have 5 clusters, then that’s\\nsomething like trying to estimate a variance with 5 data points. In that case, you might need some-\\nthing much more informative. Second, in non-linear models with logit and log links, floor and ceiling\\neffects sometimes render extreme values of the variance equally plausible as more realistic values. In\\nsuch cases, the trace plot for the variance parameters may swing around over very large values. It\\ncan do this, because the exponential prior has a long tail. Such large values are typically a priori im-\\npossible. Often, the chain will still sample validly, but it might be highly inefficient, exhibiting small\\nn_eff values and possibly many divergent transitions.\\n'},\n",
       " {'index': 426,\n",
       "  'number': 408,\n",
       "  'content': '408\\n13. MODELS WITH MEMORY\\nTo improve such a model, instead of using exponential priors for the variance components, you\\ncan use half-Normal priors or some other prior with a thin tail. A half-Normal is a Normal distribu-\\ntion with all mass above zero. It is just cut off below zero. For example:\\nSi ∼Binomial(Ni, pi)\\nlogit(pi) = αtank[i]\\nαj ∼Normal(¯α, σ)\\nα ∼Normal(0, 1.5)\\nσ ∼Half-Normal(0, 1)\\nInside an ulam formula, you’d use dhalfnorm. Inside a Stan model, you just assign a lower bound to\\nthe parameter of lower=0.\\n13.2. Varying effects and the underfitting/overfitting trade-off\\nVarying intercepts are just regularized estimates, but adaptively regularized by estimat-\\ning how diverse the clusters are while estimating the features of each cluster. This fact is not\\neasy to grasp, so if it still seems mysterious, this section aims to further relate the properties\\nof multilevel estimates to the foundational underfitting/overfitting dilemma from Chapter 7.\\nA major benefit of using varying effects estimates, instead of the empirical raw estimates,\\nis that they provide more accurate estimates of the individual cluster (tank) intercepts.198 On\\naverage, the varying effects actually provide a better estimate of the individual tank (cluster)\\nmeans. The reason that the varying intercepts provide better estimates is that they do a better\\njob of trading off underfitting and overfitting.\\nTo understand this in the context of the reed frog example, suppose that instead of exper-\\nimental tanks we had natural ponds, so that we might be concerned with making predictions\\nfor the same clusters in the future. We’ll approach the problem of predicting future survival\\nin these ponds, from three perspectives:\\n(1) Complete pooling. This means we assume that the population of ponds is invariant,\\nthe same as estimating a common intercept for all ponds.\\n(2) No pooling. This means we assume that each pond tells us nothing about any other\\npond. This is the model with amnesia.\\n(3) Partial pooling. This means using an adaptive regularizing prior, as in the previous\\nsection.\\nFirst, suppose you ignore the varying intercepts and just use the overall mean across all\\nponds, α, to make your predictions for each pond. A lot of data contributes to your estimate\\nof α, and so it can be quite precise. However, your estimate of α is unlikely to exactly match\\nthe mean of any particular pond. As a result, the total sample mean underfits the data. This\\nis the complete pooling approach, pooling the data from all ponds to produce a single\\nestimate that is applied to every pond. This sort of model is equivalent to assuming that the\\nvariation among ponds is zero—all ponds are identical.\\nSecond, suppose you use the survival proportions for each pond to make predictions.\\nThis means using a separate intercept for each pond. The blue points in Figure 13.1 are this\\nsame kind of estimate. In each particular pond, quite little data contributes to each estimate,\\nand so these estimates are rather imprecise. This is particularly true of the smaller ponds,\\nwhere less data goes into producing the estimates. As a consequence, the error of these esti-\\nmates is high, and they are rather overfit to the data. Standard errors for each intercept can\\n'},\n",
       " {'index': 427,\n",
       "  'number': 409,\n",
       "  'content': '13.2. VARYING EFFECTS AND THE UNDERFITTING/OVERFITTING TRADE-OFF\\n409\\nbe very large, and in extreme cases, even infinite. These are sometimes called the no pool-\\ning estimates. No information is shared across ponds. It’s like assuming that the variation\\namong ponds is infinite, so nothing you learn from one pond helps you predict another.\\nThird, when you estimate varying intercepts, you use partial pooling of information\\nto produce estimates for each cluster that are less underfit than the grand mean and less\\noverfit than the no-pooling estimates. As a consequence, they tend to be better estimates\\nof the true per-cluster (per-pond) means. This will be especially true when ponds have few\\ntadpoles in them, because then the no pooling estimates will be especially overfit. When a\\nlot of data goes into each pond, then there will be less difference between the varying effect\\nestimates and the no pooling estimates.\\nTo demonstrate this fact, we’ll simulate some tadpole data. That way, we’ll know the\\ntrue per-pond survival probabilities. Then we can compare the no-pooling estimates to the\\npartial pooling estimates, by computing how close each gets to the true values they are trying\\nto estimate. The rest of this section shows how to do such a simulation.\\nLearning to simulate and validate models and model fitting in this way is extremely valu-\\nable. Once you start using more complex models, you will want to ensure that your code is\\nworking and that you understand the model. You can help in this project by simulating data\\nfrom the model, with specified parameter values, and then making sure that your method of\\nestimation can recover the parameters within tolerable ranges of precision. Even just simu-\\nlating data from a model structure has a huge impact on understanding.\\n13.2.1. The model. The first step is to define the model we’ll be using. I’ll use the same basic\\nmultilevel binomial model as before, but now with “ponds” instead of “tanks”:\\nSi ∼Binomial(Ni, pi)\\nlogit(pi) = αpond[i]\\nαj ∼Normal(¯α, σ)\\n¯α ∼Normal(0, 1.5)\\nσ ∼Exponential(1)\\nSo to simulate data from this process, we need to assign values to:\\n• ¯α, the average log-odds of survival in the entire population of ponds\\n• σ, the standard deviation of the distribution of log-odds of survival among ponds\\n• α, a vector of individual pond intercepts, one for each pond\\nWe’ll also need to assign sample sizes, Ni, to each pond. But once we’ve made all of those\\nchoices, we can easily simulate counts of surviving tadpoles, straight from the top-level bi-\\nnomial process, using rbinom. We’ll do it all one step at a time.\\nNote that the priors are part of the model when we estimate, but not when we simu-\\nlate. Why? Because priors are epistemology, not ontology. They represent the initial state of\\ninformation of our robot, not a statement about how nature chooses parameter values.\\n13.2.2. Assignvaluestotheparameters. I’m going to assign specific values representative of\\nthe actual tadpole data, to make the upcoming plot that demonstrates the increased accuracy\\nof the varying effects estimates. But you can come back to this step later and change them to\\nwhatever you want.\\nHere’s the code to initialize the values of α, σ, the number of ponds, and the sample size\\nni in each pond.\\n'},\n",
       " {'index': 428,\n",
       "  'number': 410,\n",
       "  'content': '410\\n13. MODELS WITH MEMORY\\nR code\\n13.7\\na_bar <- 1.5\\nsigma <- 1.5\\nnponds <- 60\\nNi <- as.integer( rep( c(5,10,25,35) , each=15 ) )\\nI’ve chosen 60 ponds, with 15 each of initial tadpole density 5, 10, 25, and 35. I’ve chosen\\nthese densities to illustrate how the error in prediction varies with sample size. The use of\\nas.integer in the last line arises from a subtle issue with how Stan, and therefore ulam,\\nworks. See the Overthinking box at the bottom of the page for an explanation.\\nThe values ¯α = 1.4 and σ = 1.5 define a Gaussian distribution of individual pond log-\\nodds of survival. So now we need to simulate all 60 of these intercept values from the implied\\nGaussian distribution with mean ¯α and standard deviation σ:\\nR code\\n13.8\\nset.seed(5005)\\na_pond <- rnorm( nponds , mean=a_bar , sd=sigma )\\nGo ahead and inspect the contents of a_pond. It should contain 60 log-odds values, one for\\neach simulated pond.\\nFinally, let’s bundle some of this information in a data frame, just to keep it organized.\\nR code\\n13.9\\ndsim <- data.frame( pond=1:nponds , Ni=Ni , true_a=a_pond )\\nGo ahead and inspect the contents of dsim, the simulated data. The first column is the pond\\nindex, 1 through 60. The second column is the initial tadpole count in each pond. The third\\ncolumn is the true log-odds survival for each pond.\\nOverthinking: Data types and Stan models. There are two basic types of numerical data in R, in-\\ntegers and real values. A number like “3” could be either. Inside your computer, integers and real\\n(“numeric”) values are represented differently. For example, here is the same vector of values gener-\\nated as both:\\nR code\\n13.10\\nclass(1:3)\\nclass(c(1,2,3))\\n[1] \"integer\"\\n[1] \"numeric\"\\nUsually, you don’t have to manage these types, because R manages them for you. But when you\\npass values to Stan, or another external program, often the internal representation does matter. In\\nparticular, Stan and ulam sometimes require explicit integers. For example, in a binomial model,\\nthe “size” variable that specifies the number of trials must be of integer type. Stan may provide a\\nmysterious warning message about a function not being found, when the size variable is instead of\\n“real” type, or what R calls numeric. Using as.integer before passing the data to Stan or ulam will\\nresolve the issue.\\n'},\n",
       " {'index': 429,\n",
       "  'number': 411,\n",
       "  'content': '13.2. VARYING EFFECTS AND THE UNDERFITTING/OVERFITTING TRADE-OFF\\n411\\n13.2.3. Simulatesurvivors. Now we’re ready to simulate the binomial survival process. Each\\npond i has ni potential survivors, and nature flips each tadpole’s coin, so to speak, with prob-\\nability of survival pi. This probability pi is implied by the model definition, and is equal to:\\npi =\\nexp(αi)\\n1 + exp(αi)\\nThe model uses a logit link, and so the probability is defined by the logistic function.\\nPutting the logistic into the random binomial function, we can generate a simulated\\nsurvivor count for each pond:\\nR code\\n13.11\\ndsim$Si <- rbinom( nponds , prob=logistic(dsim$true_a) , size=dsim$Ni )\\nAs usual with R, if you give it a list of values, it returns a new list of the same length. In the\\nabove, each paired αi (dsim$true_a) and Ni (dsim$Ni) is used to generate a random sur-\\nvivor count with the appropriate probability of survival and maximum count. These counts\\nare stored in a new column in dsim.\\n13.2.4. Compute the no-pooling estimates. We’re ready to start analyzing the simulated\\ndata now. The easiest task is to just compute the no-pooling estimates. We can accomplish\\nthis straight from the empirical data, just by calculating the proportion of survivors in each\\npond. I’ll keep these estimates on the probability scale, instead of translating them to the\\nlog-odds scale, because we’ll want to compare the quality of the estimates on the probability\\nscale later.\\nR code\\n13.12\\ndsim$p_nopool <- dsim$Si / dsim$Ni\\nNow there’s another column in dsim, containing the empirical proportions of survivors in\\neach pond. These are the same no-pooling estimates you’d get by fitting a model with a\\ndummy variable for each pond and flat priors that induce no regularization.\\n13.2.5. Compute the partial-pooling estimates. Now to fit the model to the simulated data,\\nusing ulam. I’ll use a single long chain in this example, but keep in mind that you need to\\nuse multiple chains to check convergence to the right posterior distribution. In this case, it’s\\nsafe. But don’t get cocky.\\nR code\\n13.13\\ndat <- list( Si=dsim$Si , Ni=dsim$Ni , pond=dsim$pond )\\nm13.3 <- ulam(\\nalist(\\nSi ~ dbinom( Ni , p ),\\nlogit(p) <- a_pond[pond],\\na_pond[pond] ~ dnorm( a_bar , sigma ),\\na_bar ~ dnorm( 0 , 1.5 ),\\nsigma ~ dexp( 1 )\\n), data=dat , chains=4 )\\nWe’ve fit the basic varying intercept model above. You can take a look at the estimates for ¯α\\nand σ with the usual precis approach:\\n'},\n",
       " {'index': 430,\n",
       "  'number': 412,\n",
       "  'content': '412\\n13. MODELS WITH MEMORY\\nR code\\n13.14\\nprecis( m13.3 , depth=2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\na_pond[1]\\n0.29 0.81 -0.97\\n1.59\\n3225 1.00\\na_pond[2]\\n2.76 1.15\\n1.13\\n4.78\\n2050 1.00\\n...\\na_pond[59]\\n1.87 0.46\\n1.17\\n2.66\\n3579 1.00\\na_pond[60]\\n2.38 0.55\\n1.58\\n3.32\\n2829 1.00\\na_bar\\n1.82 0.22\\n1.48\\n2.19\\n1706 1.00\\nsigma\\n1.41 0.21\\n1.11\\n1.78\\n708 1.01\\nI’ve abbreviated the output, since there are 60 intercept parameters, one for each pond.\\nNow let’s compute the predicted survival proportions and add those proportions to our\\ngrowing simulation data frame. To indicate that it contains the partial pooling estimates, I’ll\\ncall the column p_partpool.\\nR code\\n13.15\\npost <- extract.samples( m13.3 )\\ndsim$p_partpool <- apply( inv_logit(post$a_pond) , 2 , mean )\\nIf we want to compare to the true per-pond survival probabilities used to generate the data,\\nthen we’ll also need to compute those, using the true_a column:\\nR code\\n13.16\\ndsim$p_true <- inv_logit( dsim$true_a )\\nThe last thing we need to do, before we can plot the results and realize the point of this lesson,\\nis to compute the absolute error between the estimates and the true varying effects. This is\\neasy enough, using the existing columns:\\nR code\\n13.17\\nnopool_error <- abs( dsim$p_nopool - dsim$p_true )\\npartpool_error <- abs( dsim$p_partpool - dsim$p_true )\\nNow we’re ready to plot. This is enough to get the basic display:\\nR code\\n13.18\\nplot( 1:60 , nopool_error , xlab=\"pond\" , ylab=\"absolute error\" ,\\ncol=rangi2 , pch=16 )\\npoints( 1:60 , partpool_error )\\nI’ve decorated this plot with some additional information, displayed in Figure 13.3. The\\nfilled blue points in Figure 13.3 display the no-pooling estimates. The black circles show\\nthe varying effect estimates. The horizontal axis is the pond index, from 1 through 60. The\\nvertical axis is the distance between the mean estimated probability of survival and the ac-\\ntual probability of survival. So points close to the bottom had low error, while those near\\nthe top had a large error, more than 20% off in some cases. The vertical lines divide the\\ngroups of ponds with different initial densities of tadpoles. And finally, the horizontal blue\\nand black line segments show the average error of the no-pooling and partial pooling esti-\\nmates, respectively, for each group of ponds with the same initial size. You can calculate\\nthese average error rates using aggregate:\\n'},\n",
       " {'index': 431,\n",
       "  'number': 413,\n",
       "  'content': '13.2. VARYING EFFECTS AND THE UNDERFITTING/OVERFITTING TRADE-OFF\\n413\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\npond\\nabsolute error\\ntiny ponds (5)\\nsmall ponds (10)\\nmedium ponds (25)\\nlarge ponds (35)\\nFigure 13.3. Error of no-pooling and partial pooling estimates, for the sim-\\nulated tadpole ponds. The horizontal axis displays pond number. The verti-\\ncal axis measures the absolute error in the predicted proportion of survivors,\\ncompared to the true value used in the simulation. The higher the point,\\nthe worse the estimate. No-pooling shown in blue. Partial pooling shown\\nin black. The blue and dashed black lines show the average error for each\\nkind of estimate, across each initial density of tadpoles (pond size). Smaller\\nponds produce more error, but the partial pooling estimates are better on\\naverage, especially in smaller ponds.\\nR code\\n13.19\\nnopool_avg <- aggregate(nopool_error,list(dsim$Ni),mean)\\npartpool_avg <- aggregate(partpool_error,list(dsim$Ni),mean)\\nThe first thing to notice about Figure 13.3 plot is that both kinds of estimates are much\\nmore accurate for larger ponds, on the right side. This arises because more data means better\\nestimates, assuming there is no confounding. If there is confounding, more data may just\\nmakes things worse. But there is no confounding in this simulated example. In the small\\nponds, sample size is small, and neither no-pooling nor partial-pooling can work magic.\\nTherefore, prediction suffers on the left side of the plot. Second, note that the blue line is\\nalways above or very close to the black dashed line. This indicates that the no-pool estimates,\\nshown by the blue points, have higher average error in each group of ponds, except for the\\nmedium ponds. Partial pooling isn’t always better. It’s just better on average in the long run.\\nEven though both kinds of estimates get worse as sample size decreases, the varying effect\\nestimates have the advantage, on average. Third, the distance between the blue line and the\\nblack dashed line grows as ponds get smaller. So while both kinds of estimates suffer from\\nreduced sample size, the partial pooling estimates suffer less.\\nThe pattern displayed in the figure is representative, but only one random simulation.\\nTo see how to quickly re-run the model on newly simulated data, without re-compiling the\\nmodel, see the Overthinking box at the end of this section.\\n'},\n",
       " {'index': 432,\n",
       "  'number': 414,\n",
       "  'content': '414\\n13. MODELS WITH MEMORY\\nOkay, so what are we to make of all of this? Remember, back in Figure 13.1 (page 406),\\nthe smaller tanks demonstrated more shrinkage towards the mean. Here, the ponds with the\\nsmallest sample size show the greatest improvement over the naive no-pooling estimates.\\nThis is no coincidence. Shrinkage towards the mean results from trying to negotiate the\\nunderfitting and overfitting risks of the grand mean on one end and the individual means\\nof each pond on the other. The smaller tanks/ponds contain less information, and so their\\nvarying estimates are influenced more by the pooled information from the other ponds. In\\nother words, small ponds are prone to overfitting, and so they receive a bigger dose of the un-\\nderfit grand mean. Likewise, the larger ponds shrink much less, because they contain more\\ninformation and are prone to less overfitting. Therefore they need less correcting. When in-\\ndividual ponds are very large, pooling in this way does hardly anything to improve estimates,\\nbecause the estimates don’t have far to go. But in that case, they also don’t do any harm, and\\nthe information pooled from them can substantially help prediction in smaller ponds.\\nThe partially pooled estimates are better on average. They adjust individual cluster (pond)\\nestimates to negotiate the trade-off between underfitting and overfitting. This is a form of reg-\\nularization, just like in Chapter 7, but now with an amount of regularization that is learned\\nfrom the data itself.\\nBut there are some cases in which the no-pooling estimates are better. These exceptions\\noften result from ponds with extreme probabilities of survival. The partial pooling estimates\\nshrink such extreme ponds towards the mean, because few ponds exhibit such extreme be-\\nhavior. But sometimes outliers really are outliers.\\nOverthinking: Repeating the pond simulation. This model samples pretty quickly. Compiling the\\nmodel takes up most of the execution time. Luckily the compilation only has to be done once. Then\\nyou can pass new data to the compiled model and get new estimates. Once you’ve compiled m13.3\\nonce, you can use this code to re-simulate ponds and sample from the new posterior, without waiting\\nfor the model to compile again:\\nR code\\n13.20\\na <- 1.5\\nsigma <- 1.5\\nnponds <- 60\\nNi <- as.integer( rep( c(5,10,25,35) , each=15 ) )\\na_pond <- rnorm( nponds , mean=a , sd=sigma )\\ndsim <- data.frame( pond=1:nponds , Ni=Ni , true_a=a_pond )\\ndsim$Si <- rbinom( nponds,prob=inv_logit( dsim$true_a ),size=dsim$Ni )\\ndsim$p_nopool <- dsim$Si / dsim$Ni\\nnewdat <- list(Si=dsim$Si,Ni=dsim$Ni,pond=1:nponds)\\nm13.3new <- stan( fit=m13.3@stanfit , data=newdat , chains=4 )\\npost <- extract.samples( m13.3new )\\ndsim$p_partpool <- apply( inv_logit(post$a_pond) , 2 , mean )\\ndsim$p_true <- inv_logit( dsim$true_a )\\nnopool_error <- abs( dsim$p_nopool - dsim$p_true )\\npartpool_error <- abs( dsim$p_partpool - dsim$p_true )\\nplot( 1:60 , nopool_error , xlab=\"pond\" , ylab=\"absolute error\" , col=rangi2 , pch=16 )\\npoints( 1:60 , partpool_error )\\nThe stan function reuses the compiled model in m13.3, which is stored in the stanfit slot, passes\\nit the new data, and returns the new samples in m13.3new. This is a useful trick, in case you want to\\nperform a simulation study of a particular model structure.\\n'},\n",
       " {'index': 433,\n",
       "  'number': 415,\n",
       "  'content': '13.3. MORE THAN ONE TYPE OF CLUSTER\\n415\\n13.3. More than one type of cluster\\nWe can use and often should use more than one type of cluster in the same model. For\\nexample, the observations in data(chimpanzees), which you met back in Chapter 11, are\\nlever pulls. Each pull is within a cluster of pulls belonging to an individual chimpanzee. But\\neach pull is also within an experimental block, which represents a collection of observations\\nthat happened on the same day. So each observed pull belongs to both an actor (1 to 7) and\\na block (1 to 6). There may be unique intercepts for each actor as well as for each block.\\nSo in this section we’ll reconsider the chimpanzees data, using both types of clusters\\nsimultaneously. This will allow us to use partial pooling on both categorical variables, actor\\nand block, at the same time. We’ll also get estimates of the variation among actors and\\namong blocks.\\nRethinking: Cross-classification and hierarchy. The kind of data structure in data(chimpanzees)\\nis usually called a cross-classified multilevel model. It is cross-classified, because actors are not\\nnested within unique blocks. If each chimpanzee had instead done all of his or her pulls on a single\\nday, within a single block, then the data structure would instead be hierarchical. However, the model\\nspecification would typically be the same. So the model structure and code you’ll see below will apply\\nboth to cross-classified designs and hierarchical designs. Other software sometimes forces you to treat\\nthese differently, on account of using a conditioning engine substantially less capable than MCMC.\\nThere are other types of “hierarchical” multilevel models, types that make adaptive priors for adaptive\\npriors. It’s turtles all the way down, recall (page 14). You’ll see an example in the next chapter. But\\nfor the most part, people (or their software) nearly always use the same kind of model in both cases.\\n13.3.1. Multilevelchimpanzees. Let’s proceed by taking the chimpanzees model from Chap-\\nter 11 (m11.4, page 330) and add varying intercepts. To add varying intercepts to this model,\\nwe just replace the fixed regularizing prior with an adaptive prior. We’ll also add a second\\ncluster type. To add the second cluster type, block, we merely replicate the structure for the\\nactor cluster. This means the linear model gets yet another varying intercept, αblock[i], and\\nthe model gets another adaptive prior and yet another standard deviation parameter.\\nHere is the mathematical form of the model, with the new pieces of the machine high-\\nlighted in blue:\\nLi ∼Binomial(1, pi)\\nlogit(pi) = αactor[i] + γblock[i] + βtreatment[i]\\nβj ∼Normal(0, 0.5)\\n, for j = 1..4\\nαj ∼Normal(¯α, σα)\\n, for j = 1..7\\nγj ∼Normal(0, σγ)\\n, for j = 1..6\\n¯α ∼Normal(0, 1.5)\\nσα ∼Exponential(1)\\nσγ ∼Exponential(1)\\nEach cluster gets its own vector of parameters. For actors, the vector is α, and it has length 7,\\nbecause there are 7 chimpanzees in the sample. For blocks, the vector is γ, and it has length 6,\\nbecause there are 6 blocks. Each cluster variable needs its own standard deviation parameter\\n'},\n",
       " {'index': 434,\n",
       "  'number': 416,\n",
       "  'content': '416\\n13. MODELS WITH MEMORY\\nthat adapts the amount of pooling across units, be they actors or blocks. These are σα and σγ,\\nrespectively. Finally, note that there is only one global mean parameter ¯α. We can’t identify a\\nseparate mean for each varying intercept type, because both intercepts are added to the same\\nlinear prediction. If you do include a mean for each cluster type, it won’t be the end of the\\nworld, however. It’ll be like the right leg and left leg example from Chapter 6.\\nNow to run the model that uses both actor and block:\\nR code\\n13.21\\nlibrary(rethinking)\\ndata(chimpanzees)\\nd <- chimpanzees\\nd$treatment <- 1 + d$prosoc_left + 2*d$condition\\ndat_list <- list(\\npulled_left = d$pulled_left,\\nactor = d$actor,\\nblock_id = d$block,\\ntreatment = as.integer(d$treatment) )\\nset.seed(13)\\nm13.4 <- ulam(\\nalist(\\npulled_left ~ dbinom( 1 , p ) ,\\nlogit(p) <- a[actor] + g[block_id] + b[treatment] ,\\nb[treatment] ~ dnorm( 0 , 0.5 ),\\n## adaptive priors\\na[actor] ~ dnorm( a_bar , sigma_a ),\\ng[block_id] ~ dnorm( 0 , sigma_g ),\\n## hyper-priors\\na_bar ~ dnorm( 0 , 1.5 ),\\nsigma_a ~ dexp(1),\\nsigma_g ~ dexp(1)\\n) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )\\nYou’ll end up with 2000 samples from 4 independent chains. As always, be sure to inspect the\\ntrace plots and the diagnostics. As soon as you start trusting the machine, the machine will\\nbetray your trust. In this case, you should see a warning about divergent transitions:\\nWarning messages:\\n1: There were 22 divergent transitions after warmup.\\nThe model did actually sample fine. But these warnings indicate that it had some trouble\\nefficiently exploring the posterior. In the next section, I’ll show you how to fix this. For now,\\nwe can keep moving and interpret the posterior.\\nThis is easily the most complicated model we’ve used in the book so far. So let’s look at\\nthe posterior and take note of a few important features:\\nR code\\n13.22\\nprecis( m13.4 , depth=2 )\\nplot( precis(m13.4,depth=2) ) # also plot\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nb[1]\\n-0.12 0.30 -0.59\\n0.39\\n158 1.03\\n'},\n",
       " {'index': 435,\n",
       "  'number': 417,\n",
       "  'content': '13.3. MORE THAN ONE TYPE OF CLUSTER\\n417\\nb[2]\\n0.40 0.30 -0.07\\n0.88\\n310 1.02\\nb[3]\\n-0.48 0.30 -0.96\\n0.00\\n515 1.01\\nb[4]\\n0.30 0.31 -0.17\\n0.80\\n186 1.02\\na[1]\\n-0.37 0.36 -0.94\\n0.24\\n446 1.01\\na[2]\\n4.61 1.20\\n2.98\\n6.83\\n915 1.01\\na[3]\\n-0.67 0.36 -1.24 -0.08\\n709 1.01\\na[4]\\n-0.68 0.37 -1.26 -0.09\\n235 1.02\\na[5]\\n-0.37 0.36 -0.93\\n0.19\\n338 1.01\\na[6]\\n0.57 0.35\\n0.01\\n1.12\\n560 1.01\\na[7]\\n2.09 0.45\\n1.41\\n2.82\\n721 1.01\\ng[1]\\n-0.17 0.22 -0.57\\n0.07\\n426 1.01\\ng[2]\\n0.05 0.18 -0.19\\n0.36\\n921 1.01\\ng[3]\\n0.05 0.19 -0.22\\n0.39\\n1062 1.01\\ng[4]\\n0.02 0.18 -0.25\\n0.31\\n939 1.01\\ng[5]\\n-0.02 0.18 -0.31\\n0.24\\n873 1.00\\ng[6]\\n0.12 0.19 -0.11\\n0.49\\n533 1.01\\na_bar\\n0.58 0.74 -0.58\\n1.79\\n800 1.00\\nsigma_a\\n2.00 0.66\\n1.17\\n3.16\\n1106 1.00\\nsigma_g\\n0.21 0.17\\n0.03\\n0.52\\n229 1.02\\nThe precis plot is shown in the left-hand part of Figure 13.4 (page 418).\\nFirst, notice that the number of effective samples, n_eff, varies quite a lot across pa-\\nrameters. This is common in complex models. Why? There are many reasons for this. But\\nin this sort of model a common reason is that some parameter spends a lot of time near a\\nboundary. Here, that parameter is sigma_g. It spends a lot of time near its minimum of\\nzero. Some Rhat values are also slightly above 1.00 now. All of this is a sign of inefficient\\nsampling, which we’ll fix in the next section.\\nSecond, compare sigma_a to sigma_g and notice that the estimated variation among\\nactors is a lot larger than the estimated variation among blocks. This is easy to appreciate,\\nif we plot the marginal posterior distributions of these two parameters. I’ve shown this on\\nthe right in Figure 13.4. While there’s uncertainty about the variation among actors, this\\nmodel is confident that actors vary more than blocks. You can easily see this variation in the\\nvarying intercept distributions: the a distributions are much more scattered than are the g\\ndistributions. The chimpanzees vary, but the blocks are all the same.\\nAs a consequence, adding block to this model hasn’t added a lot of overfitting risk. Let’s\\ncompare the model with only varying intercepts on actor to the model with both kinds of\\nvarying intercepts. The model that ignores block is:\\nR code\\n13.23\\nset.seed(14)\\nm13.5 <- ulam(\\nalist(\\npulled_left ~ dbinom( 1 , p ) ,\\nlogit(p) <- a[actor] + b[treatment] ,\\nb[treatment] ~ dnorm( 0 , 0.5 ),\\na[actor] ~ dnorm( a_bar , sigma_a ),\\na_bar ~ dnorm( 0 , 1.5 ),\\nsigma_a ~ dexp(1)\\n) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )\\nComparing to the model with both clusters:\\n'},\n",
       " {'index': 436,\n",
       "  'number': 418,\n",
       "  'content': '418\\n13. MODELS WITH MEMORY\\nsigma_g\\nsigma_a\\na_bar\\ng[6]\\ng[5]\\ng[4]\\ng[3]\\ng[2]\\ng[1]\\na[7]\\na[6]\\na[5]\\na[4]\\na[3]\\na[2]\\na[1]\\nb[4]\\nb[3]\\nb[2]\\nb[1]\\n0\\n2\\n4\\n6\\nValue\\n0\\n1\\n2\\n3\\n4\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\nstandard deviation\\nDensity\\nactor\\nblock\\nFigure 13.4. Left: Posterior means and 89% compatibility intervals for\\nm13.4. The greater variation across actors than blocks can be seen imme-\\ndiately in the a and g distributions. Right: Posterior distributions of the\\nstandard deviations of varying intercepts by actor (black) and block (blue).\\nR code\\n13.24\\ncompare( m13.4 , m13.5 )\\nWAIC\\nSE dWAIC\\ndSE pWAIC weight\\nm13.5 531.3 19.25\\n0\\nNA\\n8.6\\n0.63\\nm13.4 532.3 19.33\\n1 1.71\\n10.7\\n0.37\\nLook at the pWAIC column, which reports the “effective number of parameters.” While m13.4\\nhas 7 more parameters than m13.5 does, it has only 2 more effective parameters. Why? Be-\\ncause the posterior distribution for sigma_g ended up close to zero. This means each of\\nthe 6 g parameters is strongly shrunk towards zero—they are relatively inflexible. In con-\\ntrast, the a parameters are shrunk towards zero much less, because the estimated variation\\nacross actors is much larger, resulting in less shrinkage. But as a consequence, each of the a\\nparameters contributes much more to the pWAIC value.\\nYou might also notice that the difference in WAIC between these models is small, only\\nabout 1. This is especially small compared to the standard error of the difference. These two\\nmodels imply nearly identical predictions, and so their expected out-of-sample accuracy is\\nnearly identical. The block parameters have been shrunk so much towards zero that they do\\nvery little work in the model.\\nIf you are feeling the urge to “select” m13.4 as the best model, pause for a moment.\\nThere is nothing to gain here by selecting either model. The comparison of the two models\\ntells a richer story—whether we include block or not hardly matters, and the g and sigma_g\\n'},\n",
       " {'index': 437,\n",
       "  'number': 419,\n",
       "  'content': '13.3. MORE THAN ONE TYPE OF CLUSTER\\n419\\nestimates tell us why. By retaining and reporting both models, we and our readers learn more\\nabout the experiment. Model comparison is of value. To select a model, we’d rather want\\nto test conditional independencies of different causal models. Since this is an experiment,\\nthere is nothing to really select. The experimental design tells us the relevant causal model\\nto inspect.\\n13.3.2. Even more clusters. You might notice that the treatment effects, the b parameters,\\nlook a lot like the a and g parameters. Could we also use partial pooling on the treatment\\neffects? Yes, we could. Some people will scream “No!” at this suggestion, because they have\\nbeen taught that varying effects are only for variables that were not experimentally controlled.\\nSince treatment was “fixed” by the experiment, the thinking goes, we should use un-pooled\\n“fixed” effects.\\nThis is all wrong. The reason to use varying effects is because they provide better infer-\\nences. It doesn’t matter how the clusters arise. If the individual units are exchangable—\\nthe index values could be reassigned without changing the meaning of the model—then\\npartial pooling could help.\\nIn this case, there are only four treatments and there is a lot of data on each treatment.\\nSo partial pooling isn’t going to make any difference anyway. Here is m13.4 but now with\\npartial pooling on the treatments:\\nR code\\n13.25\\nset.seed(15)\\nm13.6 <- ulam(\\nalist(\\npulled_left ~ dbinom( 1 , p ) ,\\nlogit(p) <- a[actor] + g[block_id] + b[treatment] ,\\nb[treatment] ~ dnorm( 0 , sigma_b ),\\na[actor] ~ dnorm( a_bar , sigma_a ),\\ng[block_id] ~ dnorm( 0 , sigma_g ),\\na_bar ~ dnorm( 0 , 1.5 ),\\nsigma_a ~ dexp(1),\\nsigma_g ~ dexp(1),\\nsigma_b ~ dexp(1)\\n) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )\\ncoeftab( m13.4 , m13.6 )\\nm13.4\\nm13.6\\nb[1]\\n-0.13\\n-0.14\\nb[2]\\n0.39\\n0.35\\nb[3]\\n-0.48\\n-0.47\\nb[4]\\n0.28\\n0.24\\nI cut off the rest of the coeftab output. We’re only interested in the b parameters right now.\\nThese are not identical, but they are very close. If you look at sigma_b, you’ll see that it\\nis small. The treatments don’t vary a lot, on the logit scale, because they don’t make much\\ndifference in the first place. And there is a lot of data in each treatment, so they don’t get\\npooled much in any event. If you compare model m13.6 with m13.4, using either WAIC or\\nPSIS, you’ll see they are no different on purely predictive criteria. This is the typical result,\\nwhen each cluster (each treatment here) has a lot of data to inform its parameters.\\nWhat you do get from m13.6 are more divergent transitions. So in the next section,\\nlet’s finally deal with those.\\n'},\n",
       " {'index': 438,\n",
       "  'number': 420,\n",
       "  'content': '420\\n13. MODELS WITH MEMORY\\n13.4. Divergent transitions and non-centered priors\\nWith the models in the previous section, Stan reported warnings about divergent\\ntransitions. You first heard about these back in Chapter 9, and I promised to explain them\\nlater. Now is the time to learn what these things are and a few useful ways to fix them. When\\nyou work with multilevel models, divergent transitions are commonplace. So you need to\\nknow how to fix them, and that requires knowing something about what causes them.\\nOne of the best things about Hamiltonian Monte Carlo is that it provides internal checks\\nof efficiency and accuracy. One of these checks comes free, arising from the constraints on\\nthe physics simulation. Recall that HMC simulates the frictionless flow of a particle on a\\nsurface. In any given transition, which is just a single flick of the particle, the total energy at\\nthe start should be equal to the total energy at the end. That’s how energy in a closed system\\nworks. And in a purely mathematical system, the energy is always conserved correctly. It’s\\njust a fact about the physics.\\nBut in a numerical system, it might not be. Sometimes the total energy is not the same\\nat the end as it was at the start. In these cases, the energy is divergent. How can this happen?\\nIt tends to happen when the posterior distribution is very steep in some region of parameter\\nspace. Steep changes in probability are hard for a discrete physics simulation to follow. When\\nthat happens, the algorithm notices by comparing the energy at the start to the energy at the\\nend. When they don’t match, it indicates numerical problems exploring that part of the\\nposterior distribution.\\nDivergent transitions are rejected. They don’t directly damage your approximation of\\nthe posterior distribution. But they do hurt it indirectly, because the region where divergent\\ntransitions happen is hard to explore correctly. And even when there aren’t any divergent\\ntransitions, distributions with steep regions are hard to explore. The chains will be less effi-\\ncient. And unfortunately this happens quite often in multilevel models.\\nThere are two easy tricks for reducing the impact of divergent transitions. The first is\\nto tune the simulation so that it doesn’t overshoot the valley wall. This means doing more\\nwarmup with a higher target acceptance rate, Stan’s adapt_delta. But for many models, you\\ncan never tune the sampler enough to remove the divergent transitions. The second trick is\\nto write the statistical model in a new way, to reparameterize it. For any given statistical\\nmodel, it can be written in several forms that are mathematically identical but numerically\\ndifferent. Switching a model from one form to another is called reparameterization. Let’s\\nwork through two examples.\\nRethinking: No free samples. When Hamiltonian Monte Carlo complains about divergent transi-\\ntions, it is tempting to fall back on some other sampler that complains less. This is a mistake. A Gibbs\\nsampler, for example, will never complain. It will just silently fail. It is true that Gibbs sampling\\ndoesn’t have the same problem with steep curvature that HMC has. But Gibbs still has problems with\\nthe same posterior distributions. It just provides no warnings.\\nThe general issue—warnings of unreliable approximations—arises in all parts of statistics. The\\nR package lme4 is a nice package for fitting multilevel models. It isn’t Bayesian, but instead uses a\\nclever non-Bayesian algorithm. Sometimes that algorithm is unreliable, and lme4 is very good about\\nwarning the user. Alternative packages that try to fit the same multilevel models may not produce\\nwarnings nearly as often. But those packages are no more reliable. They are just less cautious.\\n'},\n",
       " {'index': 439,\n",
       "  'number': 421,\n",
       "  'content': '13.4. DIVERGENT TRANSITIONS AND NON-CENTERED PRIORS\\n421\\n13.4.1. The Devil’s Funnel. You don’t need a fancy model to experience divergent transi-\\ntions. Suppose we have this joint distribution of two variables, v and x:\\nv ∼Normal(0, 3)\\nx ∼Normal(0, exp(v))\\nThere are no data here, just a joint distribution to sample from. This distribution might seem\\nweird, but it represents a typical multilevel distribution, in which the scale of one variable\\n(here x) depends upon another variable (here v). We’ll visualize it on the next page. You can\\ntry this in ulam():\\nR code\\n13.26\\nm13.7 <- ulam(\\nalist(\\nv ~ normal(0,3),\\nx ~ normal(0,exp(v))\\n), data=list(N=1) , chains=4 )\\nprecis( m13.7 )\\nmean\\nsd\\n5.5%\\n94.5% n_eff Rhat\\nv\\n1.90\\n2.08\\n-1.49\\n5.42\\n39 1.06\\nx 18.12 135.97 -31.78 123.84\\n102 1.04\\nThis looks like an easy problem—only two parameters—but it’s a disaster. You should see\\nlots of divergent transitions. And the n_eff and Rhat values are very poor. Take a glance at\\nthe trace plot, traceplot(m13.7), too.\\nThis example is The Devil’s Funnel.199 In the left panel of Figure 13.5, I show the distri-\\nbution’s contours. At low values of v, the distribution of x contracts around zero. This forms\\na very steep valley that the Hamiltonian particle needs to explore. Steep surfaces are hard to\\nsimulate, because the simulation is not actually continuous. It happens in discrete steps. If\\nthe steps are too big, the simulation will overshoot. This error effectively changes the total\\nenergy in the system. What happens next is unpredictable.\\nAs in the examples in Chapter 9, the simulation in Figure 13.5 (left panel) starts at the\\n×. The simulation finds the valley. But then it misses its turn and careens into space. The\\nopen point is a divergent transition, a proposal for which the energy at the start of the transi-\\ntion is not the same as the energy at the end of the transition. When you try to sample from\\nthis distribution, you get lots of these divergent transitions and a very unreliable approxima-\\ntion of the posterior distribution. We can prove that in this case, because it is a very simple\\ndistribution that we can compute with grid approximation.\\nWe can fix this problem by reparameterizing the funnel. There are two general ways\\nto parameterize models in which the distribution of one parameter is a function of another\\nparameter. In this example, the distribution of x is a function of v:\\nx ∼Normal(0, exp(v))\\nThis is the source of the funnel: As v changes, the distribution of x changes in a very incon-\\nvenient way. This parameterization is known as the centered parameterization. This\\nis not a very intuitive name. It just indicates that the distribution of x is conditional on one\\nor more other parameters.\\nThe alternative is a non-centered parameterization. A non-centered parameter-\\nization moves the embedded parameter, v in this case, out of the definition of the other\\n'},\n",
       " {'index': 440,\n",
       "  'number': 422,\n",
       "  'content': '422\\n13. MODELS WITH MEMORY\\n-4\\n-2\\n0\\n2\\n4\\n-4\\n-2\\n0\\n2\\n4\\nx\\nv\\nCentered parameterization\\n-2\\n-1\\n0\\n1\\n2\\n-4\\n-2\\n0\\n2\\n4\\nz\\nv\\nFigure 13.5. Divergent transitions happen when the posterior is steep and\\nthe HMC simulation is too coarse to follow it. These numerical errors are\\ndetected automatically. Left: The posterior distribution here is a steep valley\\naround x = 0 when v is small. The divergent transition (open point) over-\\nshoots the wall of the valley and then careens wildly into space. Right: The\\nsame model, but with a non-centered parameterization that flattens the val-\\nley. See the model definitions in the text. See examples in ?HMC_2D_sample\\nfor code to reproduce these figures.\\nparameter. For The Devil’s Funnel, we can accomplish that like this:\\nv ∼Normal(0, 3)\\nz ∼Normal(0, 1)\\nx = z exp(v)\\nThis looks crazy. So to understand what just happened, consider the common procedure\\nof standardizing a variable. Many times so far in this book, we’ve standardized data before\\nrunning a model. The procedure is to subtract the mean and then divide by the standard\\ndeviation. The new, standardized variable has mean zero and standard deviation one. To get\\nthe original variable back, you would perform these steps in reverse. First you’d multiply the\\nstandardized variable by the original standard deviation. Then you’d add the original mean.\\nThe reparameterization above has just defined z as the standardized x. Since it is stan-\\ndardized, it has mean zero and standard deviation one. Then to compute x, we reverse the\\nstandardization by multiplying z by the standard deviation, exp(v). There is no mean to add\\nback, because the mean in both cases is zero. But if there were a different mean, we’d add\\nit back in this step as well. The result is that x in the non-centered version has the same\\ndistribution as x in the original, centered version. It’s the same joint distribution of v and x.\\nBut when we run the Markov chain, it’s rather different. We don’t sample x directly\\nnow. Instead we sample z. The right-hand panel of Figure 13.5 shows the non-centered\\ndistribution’s contours—it’s just a bivariate Gaussian now—and the HMC simulation on top.\\nLet’s run the model again in ulam:\\n'},\n",
       " {'index': 441,\n",
       "  'number': 423,\n",
       "  'content': '13.4. DIVERGENT TRANSITIONS AND NON-CENTERED PRIORS\\n423\\nR code\\n13.27\\nm13.7nc <- ulam(\\nalist(\\nv ~ normal(0,3),\\nz ~ normal(0,1),\\ngq> real[1]:x <<- z*exp(v)\\n), data=list(N=1) , chains=4 )\\nprecis( m13.7nc )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nv -0.04\\n2.88\\n-4.63\\n4.58\\n1612\\n1\\nz\\n0.01\\n0.99\\n-1.57\\n1.62\\n1555\\n1\\nx -3.70 260.03 -25.35 23.12\\n1511\\n1\\nAll is well. If you plot x against v, you will see the funnel. We managed to sample it by\\nsampling a different variable and then transforming it. That is the non-centered parameteri-\\nzation. It’s used often when working with multilevel models. However, there are times when\\nthe centered prior is better. So it pays to be comfortable with both.\\n13.4.2. Non-centered chimpanzees. For a real example, let’s return to the chimpanzees. In\\nmodel m13.4, the adaptive priors that make it a multilevel model have parameters inside\\nthem. These are causing regions of steep curvature and generating divergent transitions. We\\ncan fix that though.\\nBefore reparameterizing, the first thing you can try is to increase Stan’s target acceptance\\nrate. This is controlled by the adapt_delta control parameter. The ulam default is 0.95,\\nwhich means that it aims to attain a 95% acceptance rate. It tries this during the warmup\\nphase, adjusting the step size of each leapfrog step (go back to Chapter 9 if these terms aren’t\\nfamiliar). When adapt_delta is set high, it results in a smaller step size, which means a\\nmore accurate approximation of the curved surface. It can also mean slower exploration of\\nthe distribution.\\nIncreasing adapt_delta will often, but not always, help with divergent transitions. For\\nexample, model m13.4 in the previous section presented a few divergent transitions. We can\\nre-run the model, using a higher target acceptance rate, with:\\nR code\\n13.28\\nset.seed(13)\\nm13.4b <- ulam( m13.4 , chains=4 , cores=4 , control=list(adapt_delta=0.99) )\\ndivergent(m13.4b)\\n[1] 2\\nSo that did help. But sometimes this won’t be enough. And while the divergent transitions\\nare gone, the chain still isn’t very efficient—look at the precis output and notice that many\\nof the n_eff values are still far below the true number of samples (2000 in this case: 4 chains,\\n500 from each).\\nWe can do much better with the non-centered version of the model. What we want is a\\nversion of m13.4 (page 415) in which we get the parameters out of the adaptive priors and\\ninstead into the linear model. There are two adaptive priors to transform:\\nαj ∼Normal(¯α, σα)\\n[Intercepts for actors]\\nγj ∼Normal(0, σγ)\\n[Intercepts for blocks]\\n'},\n",
       " {'index': 442,\n",
       "  'number': 424,\n",
       "  'content': '424\\n13. MODELS WITH MEMORY\\nThere are three embedded (“centered”) parameters to smuggle out of these priors: ¯α, σα, σγ.\\nAs before with the funnel, we’ll define some new variables that are given standard Normal dis-\\ntributions, and then we’ll reconstruct the original variables by undoing the transformation.\\nThis time, we’ll do that reconstruction in the linear model. The completed non-centered\\nmodel looks like this (with altered bits in blue):\\nLi ∼Binomial(1, pi)\\nlogit(pi) = ¯α + zactor[i]σα\\n|\\n{z\\n}\\nαactor[i]\\n+ xblock[i]σγ\\n|\\n{z\\n}\\nγblock[i]\\n+ βtreatment[i]\\nβj ∼Normal(0, 0.5)\\n, for j = 1..4\\nzj ∼Normal(0, 1)\\n[Standardized actor intercepts]\\nxj ∼Normal(0, 1)\\n[Standardized block intercepts]\\n¯α ∼Normal(0, 1.5)\\nσα ∼Exponential(1)\\nσγ ∼Exponential(1)\\nThe vector z gives the standardized intercept for each actor, and the vector x gives the stan-\\ndardized intercept for each block. Inside the linear model logit(pi), all of the previously\\nembedded parameters reappear. Each actor intercept is defined by\\nαj = ¯α + zjσα\\nand each block intercept by\\nγj = xjσγ\\nSo these expressions appear now in the linear model.\\nLet’s sample from this posterior now and see what the reparameterization gains us.\\nR code\\n13.29\\nset.seed(13)\\nm13.4nc <- ulam(\\nalist(\\npulled_left ~ dbinom( 1 , p ) ,\\nlogit(p) <- a_bar + z[actor]*sigma_a + # actor intercepts\\nx[block_id]*sigma_g +\\n# block intercepts\\nb[treatment] ,\\nb[treatment] ~ dnorm( 0 , 0.5 ),\\nz[actor] ~ dnorm( 0 , 1 ),\\nx[block_id] ~ dnorm( 0 , 1 ),\\na_bar ~ dnorm( 0 , 1.5 ),\\nsigma_a ~ dexp(1),\\nsigma_g ~ dexp(1),\\ngq> vector[actor]:a <<- a_bar + z*sigma_a,\\ngq> vector[block_id]:g <<- x*sigma_g\\n) , data=dat_list , chains=4 , cores=4 )\\nNow let’s compare the n_eff, numbers of effective samples, for these two forms. To do this\\nfairly, we should ignore the z and x parameters and instead compare a and g parameters.\\nThat is why I added those gq> lines at the bottom of the formula above, so that Stan would\\n'},\n",
       " {'index': 443,\n",
       "  'number': 425,\n",
       "  'content': '13.4. DIVERGENT TRANSITIONS AND NON-CENTERED PRIORS\\n425\\n500\\n1000\\n1500\\n2000\\n500\\n1000\\n1500\\n2000\\nn_eff (centered)\\nn_eff (non-centered)\\nFigure 13.6. Comparing the centered (hori-\\nzonal) and non-centered (vertical) parameter-\\nizations of the multilevel chimpanzees model,\\nm13.4. Each point is a parameter. All but two\\nparameters lie above the diagonal, indicating\\nbetter sampling for the non-centered parame-\\nterization.\\ndo the calculations for us while it ran. The code below pulls the matching n_eff values out\\nof the precis tables for both models. Then it plots them against one another.\\nR code\\n13.30\\nprecis_c <- precis( m13.4 , depth=2 )\\nprecis_nc <- precis( m13.4nc , depth=2 )\\npars <- c( paste(\"a[\",1:7,\"]\",sep=\"\") , paste(\"g[\",1:6,\"]\",sep=\"\") ,\\npaste(\"b[\",1:4,\"]\",sep=\"\") , \"a_bar\" , \"sigma_a\" , \"sigma_g\" )\\nneff_table <- cbind( precis_c[pars,\"n_eff\"] , precis_nc[pars,\"n_eff\"] )\\nplot( neff_table , xlim=range(neff_table) , ylim=range(neff_table) ,\\nxlab=\"n_eff (centered)\" , ylab=\"n_eff (non-centered)\" , lwd=2 )\\nabline( a=0 , b=1 , lty=2 )\\nThe result is displayed in Figure 13.6. The diagonal shows where both models produce the\\nsame effective number of samples. For all but two parameters, the non-centered parameter-\\nization performs much better.\\nSo should we always use the non-centered parameterization? No. Sometimes the cen-\\ntered form is better. It could even be true that the centered form is better for one cluster in\\na model while the non-centered form is better for another cluster in the same model. It all\\ndepends upon the details. Typically, a cluster with low variation, like the blocks in m13.4,\\nwill sample better with a non-centered prior. And if you have a large number of units inside\\na cluster, but not much data for each unit, then the non-centered is also usually better. But\\nbeing able to switch back and forth as needed is very useful.\\nWe can reparameterize distributions other than the Gaussian. For example, an exponen-\\ntial distribution has a single scale parameter, usually called λ, that can be factored out and\\nsmuggled into a linear model:\\nx = zλ\\nz ∼Exponential(1)\\nThis is the same as x ∼Exponential(λ). And in the next chapter, I’ll show you how to\\nreparameterize multivariate distributions so to place an entire correlation matrix inside a\\nlinear model. Algebra makes many things possible.\\n'},\n",
       " {'index': 444,\n",
       "  'number': 426,\n",
       "  'content': '426\\n13. MODELS WITH MEMORY\\n13.5. Multilevel posterior predictions\\nWay back in Chapter 3 (page 63), I commented on the importance of model checking.\\nSoftware does not always work as expected, and one robust way to discover mistakes is to\\ncompare the sample to the posterior predictions of a fit model. The same procedure, produc-\\ning implied predictions from a fit model, is very helpful for understanding what the model\\nmeans. Every model is a merger of sense and nonsense. When we understand a model,\\nwe can find its sense and control its nonsense. But as models get more complex, it is very\\ndifficult to impossible to understand them just by inspecting tables of posterior means and\\nintervals. Exploring implied posterior predictions helps much more.\\nOnce you believe the posterior is correct, implied predictions are needed to consider\\nthe causal effects. What is the estimated effect of intervening on one or more variables? We\\nneed counterfactual posterior predictions for this question. We saw an example of this in\\nChapter 5.\\nAnother role for constructing implied predictions is in computing information cri-\\nteria, like AIC and WAIC. These criteria provide simple estimates of out-of-sample model\\naccuracy, the KL divergence. In practical terms, information criteria provide a rough meas-\\nure of a model’s flexibility and therefore overfitting risk. This was the big conceptual mission\\nof Chapter 7.\\nAll of this advice applies to multilevel models as well. We still often need model checks,\\ncounterfactual predictions for understanding, and information criteria. The introduction of\\nvarying effects does introduce nuance, however.\\nFirst, we should no longer expect the model to exactly retrodict the sample, because\\nadaptive regularization has as its goal to trade off poorer fit in sample for better inference\\nand hopefully better fit out of sample. That is what shrinkage does for us. Of course, we\\nshould never be trying to really retrodict the sample. But now you have to expect that even\\na perfectly good model fit will differ from the raw data in a systematic way.\\nSecond, “prediction” in the context of a multilevel model requires additional choices. If\\nwe wish to validate a model against the specific clusters used to fit the model, that is one\\nthing. But if we instead wish to compute predictions for new clusters, other than the ones\\nobserved in the sample, that is quite another. We’ll consider each of these in turn, continuing\\nto use the chimpanzees model from the previous section.\\n13.5.1. Posterior prediction for same clusters. When working with the same clusters as\\nyou used to fit a model, varying intercepts are just parameters. The only trick is to ensure\\nthat you use the right intercept for each case in the data. If you use link and sim to do your\\nwork for you, this is handled automatically. Otherwise, you just use the model definition.\\nFor example, in data(chimpanzees), there are 7 unique actors. These are the clusters.\\nThe varying intercepts model, m13.4, estimated an intercept for each, in addition to two\\nparameters to describe the mean and standard deviation of the population of actors. We’ll\\nconstruct posterior predictions (retrodictions), using both the automated link approach\\nand doing it from scratch, so there is no confusion.\\nBefore computing predictions, note again that we should no longer expect the posterior\\npredictive distribution to match the raw data, even when the model worked correctly. Why?\\nThe whole point of partial pooling is to shrink estimates towards the grand mean. So the\\nestimates should not necessarily match up with the raw data, once you use pooling.\\nThe code needed to compute posterior predictions is just like the code from Chapter 11.\\nHere it is again, computing posterior predictions for actor number 2:\\n'},\n",
       " {'index': 445,\n",
       "  'number': 427,\n",
       "  'content': '13.5. MULTILEVEL POSTERIOR PREDICTIONS\\n427\\nR code\\n13.31\\nchimp <- 2\\nd_pred <- list(\\nactor = rep(chimp,4),\\ntreatment = 1:4,\\nblock_id = rep(1,4)\\n)\\np <- link( m13.4 , data=d_pred )\\np_mu <- apply( p , 2 , mean )\\np_ci <- apply( p , 2 , PI )\\nTo construct the same calculations without using link, we just have to remember the model.\\nThe only difficulty is that when we work with the samples from the posterior, the varying\\nintercepts will be a matrix of samples. Let’s take a look:\\nR code\\n13.32\\npost <- extract.samples(m13.4)\\nstr(post)\\nList of 6\\n$ b\\n: num [1:2000, 1:4] -0.107 -0.491 -0.644 -0.368 0.105 ...\\n$ a\\n: num [1:2000, 1:7] -0.0166 -0.2078 0.3102 0.1337 -0.191 ...\\n$ g\\n: num [1:2000, 1:6] -0.7116 -0.1728 -0.5689 -0.0299 0.0133 ...\\n$ a_bar\\n: num [1:2000(1d)] 1.2031 -0.0998 1.3569 0.6167 -0.0248 ...\\n$ sigma_a: num [1:2000(1d)] 3.1 3.57 2.92 2.15 2.19 ...\\n$ sigma_g: num [1:2000(1d)] 0.393 0.287 0.418 0.119 0.13 ...\\nThe a matrix has samples on the rows and actors on the columns. So to plot, for example,\\nthe density for actor 5:\\nR code\\n13.33\\ndens( post$a[,5] )\\nThe [,5] means “all samples for actor 5.”\\nTo construct posterior predictions, we build our own link function. I’ll use the with\\nfunction here, so we don’t have to keep typing post$ before every parameter name:\\nR code\\n13.34\\np_link <- function( treatment , actor=1 , block_id=1 ) {\\nlogodds <- with( post ,\\na[,actor] + g[,block_id] + b[,treatment] )\\nreturn( inv_logit(logodds) )\\n}\\nThe linear model is identical to the one used to define the model, but with a single comma\\nadded inside the brackets after a. Now to compute predictions:\\nR code\\n13.35\\np_raw <- sapply( 1:4 , function(i) p_link( i , actor=2 , block_id=1 ) )\\np_mu <- apply( p_raw , 2 , mean )\\np_ci <- apply( p_raw , 2 , PI )\\nAt some point, you will have to work with a model that link will mangle. At that time, you\\ncan return to this section and peer hard at the code above and still make progress. No matter\\n'},\n",
       " {'index': 446,\n",
       "  'number': 428,\n",
       "  'content': '428\\n13. MODELS WITH MEMORY\\nwhat the model is, if it is a Bayesian model, then it is generative. This means that predictions\\nare made by pushing samples up through the model to get distributions of predictions. Then\\nyou summarize the distributions to summarize the predictions.\\n13.5.2. Posterior prediction for new clusters. The problem of making predictions for new\\nclusters is really a problem of generalizing from the sample. In general, there is no unique\\nprocedure for generalizing predictions outside of a sample. The right thing to do depends\\nupon the causal model, the statistical model, and your goals. But if you have a generative\\nmodel, then you can often think your way through it. The key idea is to use the posterior to\\nparameterize a simulation that embodies the target generalization.\\nLet’s consider some simple examples.\\nSuppose you want to predict how chimpanzees in another population would respond\\nto our lever pulling experiment. The particular 7 chimpanzees in the sample allowed us to\\nestimate 7 unique intercepts. But these individual actor intercepts aren’t of interest, because\\nnone of these 7 individuals is in the new population.\\nOne way to grasp the task of constructing posterior predictions for new clusters is to\\nimagine leaving out one of the clusters when you fit the model to the data. For example,\\nsuppose we leave out actor number 7 when we fit the chimpanzees model. Now how can we\\nassess the model’s accuracy for predicting actor number 7’s behavior? We can’t use any of the\\na parameter estimates, because those apply to other individuals. But we can make good use\\nof the a_bar and sigma_a parameters. These parameters describe a statistical population of\\nactors, and we can simulate new actors from it.\\nFirst, let’s see how to construct posterior predictions for a new, previously unobserved\\naverage actor. By “average,” I mean an individual chimpanzee with an intercept exactly at\\na_bar (¯α), the population mean. Since there is uncertainty about the population mean, there\\nis still uncertainty about this average individual’s intercept. But as you’ll see, the uncertainty\\nis much smaller than it really should be, if we wish to honestly represent the problem of what\\nto expect from a new individual.\\nWhat we need is our own link function, but now with a twist:\\nR code\\n13.36\\np_link_abar <- function( treatment ) {\\nlogodds <- with( post , a_bar + b[,treatment] )\\nreturn( inv_logit(logodds) )\\n}\\nNotice that the function ignores block. This is because we are extrapolating to new blocks,\\nso we assume the average block effect is about zero (which it was in the sample). Call this\\nfunction and summarize just as before:\\nR code\\n13.37\\npost <- extract.samples(m13.4)\\np_raw <- sapply( 1:4 , function(i) p_link_abar( i ) )\\np_mu <- apply( p_raw , 2 , mean )\\np_ci <- apply( p_raw , 2 , PI )\\nplot( NULL , xlab=\"treatment\" , ylab=\"proportion pulled left\" ,\\nylim=c(0,1) , xaxt=\"n\" , xlim=c(1,4) )\\naxis( 1 , at=1:4 , labels=c(\"R/N\",\"L/N\",\"R/P\",\"L/P\") )\\nlines( 1:4 , p_mu )\\n'},\n",
       " {'index': 447,\n",
       "  'number': 429,\n",
       "  'content': '13.5. MULTILEVEL POSTERIOR PREDICTIONS\\n429\\nshade( p_ci , 1:4 )\\nThe result is displayed in Figure 13.7, on the left. The gray region shows the 89% compati-\\nbility interval for an actor with an average intercept. This kind of calculation makes it easy\\nto see the impact of prosoc_left, as well as uncertainty about where the average is, but it\\ndoesn’t show the variation among actors.\\nTo show the variation among actors, we’ll need to use sigma_a in the calculation. First\\nwe simply use rnorm to sample some random chimpanzees, using mean a_bar and standard\\ndeviation sigma_a. Then we write a link function that references those simulated chim-\\npanzees, not the ones in the posterior. It’s important to do the chimpanzee sampling outside\\nthe link function, because we want to reference the same simulate chimpanzee, whichever\\ntreatment we consider. This is the code:\\nR code\\n13.38\\na_sim <- with( post , rnorm( length(post$a_bar) , a_bar , sigma_a ) )\\np_link_asim <- function( treatment ) {\\nlogodds <- with( post , a_sim + b[,treatment] )\\nreturn( inv_logit(logodds) )\\n}\\np_raw_asim <- sapply( 1:4 , function(i) p_link_asim( i ) )\\nSummarizing and plotting is exactly as before, and the result is displayed in the middle of\\nFigure 13.7. These posterior predictions are marginal of actor, which means that they av-\\nerage over the uncertainty among actors. In contrast, the predictions on the left just set the\\nactor to the average, ignoring variation among actors.\\nAt this point, students usually ask, “So which one should I use?” The answer is, “It de-\\npends.” Both are useful, depending upon the question. The predictions for an average actor\\nhelp to visualize the impact of treatment. The predictions that are marginal of actor illus-\\ntrate how variable different chimpanzees are, according to the model. You probably want to\\ncompute both for yourself, when trying to understand a model. But which you include in a\\nreport will depend upon context.\\nIn this case, we can do better by making a plot that displays both the treatment effect and\\nthe variation among actors. We can do this by forgetting about intervals and instead simu-\\nlating a series of new actors in each of the four treatments. By drawing a line for each actor\\nacross all four treatments, we’ll be able to visualize both the zig-zag impact of prosoc_left\\nas well as the variation among individuals.\\nWe don’t really need new code here. We just need to use the rows in p_raw_asim from\\nabove. Each row contains a single trend, a single simulated chimpanzee. So instead of sum-\\nmarizing with mean and PI, we can just loop over rows and plot:\\nR code\\n13.39\\nplot( NULL , xlab=\"treatment\" , ylab=\"proportion pulled left\" ,\\nylim=c(0,1) , xaxt=\"n\" , xlim=c(1,4) )\\naxis( 1 , at=1:4 , labels=c(\"R/N\",\"L/N\",\"R/P\",\"L/P\") )\\nfor ( i in 1:100 ) lines( 1:4 , p_raw_asim[i,] , col=grau(0.25) , lwd=2 )\\nThe result is shown in the right-hand plot of Figure 13.7. Each trend is a simulated actor,\\nacross all four treatments on the horizontal axis. It is much easier in this plot to see both the\\n'},\n",
       " {'index': 448,\n",
       "  'number': 430,\n",
       "  'content': '430\\n13. MODELS WITH MEMORY\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ntreatment\\nproportion pulled left\\nR/N\\nL/N\\nR/P\\nL/P\\naverage actor\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ntreatment\\nproportion pulled left\\nR/N\\nL/N\\nR/P\\nL/P\\nmarginal of actor\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ntreatment\\nproportion pulled left\\nR/N\\nL/N\\nR/P\\nL/P\\nsimulated actors\\nFigure 13.7. Posterior predictive distributions for the chimpanzees vary-\\ning intercept model, m13.4. The solid lines are posterior means and the\\nshaded regions are 80% percentile intervals. Left: Setting the varying inter-\\ncept a to the mean a_bar produces predictions for an average actor. These\\npredictions ignore uncertainty arising from variation among actors. Mid-\\ndle: Simulating varying intercepts using the posterior standard deviation\\namong actors, sigma_a, produces predictions that account for variation\\namong actors. Right: 100 simulated actors with unique intercepts sampled\\nfrom the posterior. Each simulation maintains the same parameter values\\nacross all four treatments.\\nzig-zag impact of treatment and the variation among actors that is induced by the posterior\\ndistribution of sigma_a.\\nAlso note the interaction of treatment and the variation among actors. Because this is\\na binomial model, in principle all parameters interact, due to ceiling and floor effects. For\\nactors with very large intercepts, near the top of the plot, treatment has very little effect. These\\nactors have strong handedness preferences. But actors with intercepts nearer the mean are\\ninfluenced by treatment.\\n13.5.3. Post-stratification. A common problem is how to use a non-representative sample\\nof a population to generate representative predictions for the same population. For example,\\nwe might survey potential voters, asking about their voting intentions. Such samples are\\nbiased—different groups respond to such surveys at different rates. So if we just use the\\nsurvey average, we’ll make the wrong prediction about the election. How can we do better?\\nOne technique is post-stratification.200 The idea is to fit a model in which each de-\\nmographic slice of the population—a specific combination of age, economic, and educational\\nvariables for example—has its own voting intention. Then the estimates of these intentions\\nare re-weighted using general census information about the full voting population. Because\\nthere are usually many demographic categories, and samples can be small in some of them,\\npost-stratification is often combined with multilevel modeling, in which case it is called\\nMRP, pronounced “Mister P,” for multilevel regression and post-stratification.\\nHow does it work? Supposing you have estimates pi for each demographic category i,\\nthen the post-stratified prediction for the whole population (not the sample) just re-weights\\n'},\n",
       " {'index': 449,\n",
       "  'number': 431,\n",
       "  'content': '13.7. PRACTICE\\n431\\nthese estimates using the number of individuals Ni in each category:\\nP\\ni Nipi\\nP\\ni Ni\\nCompute this for each sample in the posterior distribution, then you’ll have a posterior dis-\\ntribution of predictions as usual.\\nPost-stratification does not always work. It is not justified, for example, when selection\\nbias is itself caused by the outcome of interest. Suppose that responding to the survey R is\\ninfluenced by age A, and that age A influences voting intention V: R ←A →V. In that\\ncase it is possible to estimate the influence of A on V. But if V →R, then there is little hope.\\nSuppose for example that only supporters respond. Then V = 1 for everyone who responds.\\nSelection on the outcome variable is one of the worst things that can happen in statistics.\\nA general framework for generalizability is transportability.201 Post-stratification is\\na special case of this framework, as are meta-analyses and the application of estimates across\\npopulations. The details are complicated. But acquainting yourself with the framework is\\nworthwhile, even if only to recognize special cases and connections among them.\\n13.6. Summary\\nThis chapter has been an introduction to the motivation, implementation, and inter-\\npretation of basic multilevel models. It focused on varying intercepts, which achieve better\\nestimates of baseline differences among clusters in the data. They achieve better estimates,\\nbecause they simultaneously model the population of clusters and use inferences about the\\npopulation to pool information among parameters. From another perspective, varying inter-\\ncepts are adaptively regularized parameters, relying upon a prior that is itself learned from\\nthe data. All of this is a foundation for the next chapter, which extends these concepts to\\nadditional types of parameters and models.\\n13.7. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n13E1. Which of the following priors will produce more shrinkage in the estimates? (a) αtank ∼\\nNormal(0, 1); (b) αtank ∼Normal(0, 2).\\n13E2. Rewrite the following model as a multilevel model.\\nyi ∼Binomial(1, pi)\\nlogit(pi) = αgroup[i] + βxi\\nαgroup ∼Normal(0, 1.5)\\nβ ∼Normal(0, 0.5)\\n13E3. Rewrite the following model as a multilevel model.\\nyi ∼Normal(µi, σ)\\nµi = αgroup[i] + βxi\\nαgroup ∼Normal(0, 5)\\nβ ∼Normal(0, 1)\\nσ ∼Exponential(1)\\n13E4. Write a mathematical model formula for a Poisson regression with varying intercepts.\\n'},\n",
       " {'index': 450,\n",
       "  'number': 432,\n",
       "  'content': '432\\n13. MODELS WITH MEMORY\\n13E5. Write a mathematical model formula for a Poisson regression with two different kinds of vary-\\ning intercepts, a cross-classified model.\\n13M1. Revisit the Reed frog survival data, data(reedfrogs), and add the predation and size\\ntreatment variables to the varying intercepts model. Consider models with either main effect alone,\\nboth main effects, as well as a model including both and their interaction. Instead of focusing on\\ninferences about these two predictor variables, focus on the inferred variation across tanks. Explain\\nwhy it changes as it does across models.\\n13M2. Compare the models you fit just above, using WAIC. Can you reconcile the differences in\\nWAIC with the posterior distributions of the models?\\n13M3. Re-estimatethe basic Reed frog varying intercept model, butnow using a Cauchy distribution\\nin place of the Gaussian distribution for the varying intercepts. That is, fit this model:\\nsi ∼Binomial(ni, pi)\\nlogit(pi) = αtank[i]\\nαtank ∼Cauchy(α, σ)\\nα ∼Normal(0, 1)\\nσ ∼Exponential(1)\\n(You are likely to see many divergent transitions for this model. Can you figure out why? Can you\\nfix them?) Compare the posterior means of the intercepts, αtank, to the posterior means produced\\nin the chapter, using the customary Gaussian prior. Can you explain the pattern of differences? Take\\nnote of any change in the mean α as well.\\n13M4. Now use a Student-t distribution with ν = 2 for the intercepts:\\nαtank ∼Student(2, α, σ)\\nRefer back to the Student-t example in Chapter 7 (page 234), if necessary. Compare the resulting\\nposterior to both the original model and the Cauchy model in 13M3. Can you explain the differences\\nand similarities in shrinkage in terms of the properties of these distributions?\\n13M5. Modify the cross-classified chimpanzees model m13.4 so that the adaptive prior for blocks\\ncontains a parameter ¯γ for its mean:\\nγj ∼Normal(¯γ, σγ)\\n¯γ ∼Normal(0, 1.5)\\nCompare this model to m13.4. What has including ¯γ done?\\n13M6. Sometimes the prior and the data (through the likelihood) are in conflict, because they con-\\ncentrate around different regions of parameter space. What happens in these cases depends a lot upon\\nthe shape of the tails of the distributions.202 Likewise, the tails of distributions strongly influence can\\noutliers are shrunk or not towards the mean. I want you to consider four different models to fit to\\none observation at y = 0. The models differ only in the distributions assigned to the likelihood and\\nprior. Here are the four models:\\nModel NN:\\ny ∼Normal(µ, 1)\\nµ ∼Normal(10, 1)\\nModel TN:\\ny ∼Student(2, µ, 1)\\nµ ∼Normal(10, 1)\\nModel NT:\\ny ∼Normal(µ, 1)\\nµ ∼Student(2, 10, 1)\\nModel TT:\\ny ∼Student(2, µ, 1)\\nµ ∼Student(2, 10, 1)\\nEstimate the posterior distributions for these models and compare them. Can you explain the results,\\nusing the properties of the distributions?\\n'},\n",
       " {'index': 451,\n",
       "  'number': 433,\n",
       "  'content': '13.7. PRACTICE\\n433\\n13H1. In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the\\nyear 2000, a typical Bengali woman had only 2 or 3. You’re going to look at a historical set of data,\\nwhen contraception was widely available but many families chose not to use it. These data reside in\\ndata(bangladesh) and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934\\nwomen. There are six variables, but you can focus on two of them for this practice problem:\\n(1) district: ID number of administrative district each woman resided in\\n(2) use.contraception: An indicator (0/1) of whether the woman was using contraception\\nThe first thing to do is ensure that the cluster variable, district, is a contiguous set of integers. Recall\\nthat these values will be index values inside the model. If there are gaps, you’ll have parameters for\\nwhich there is no data to inform them. Worse, the model probably won’t run. Look at the unique\\nvalues of the district variable:\\nR code\\n13.40\\nsort(unique(d$district))\\n[1]\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\\n[51] 51 52 53 55 56 57 58 59 60 61\\nDistrict 54 is absent. So district isn’t yet a good index variable, because it’s not contiguous. This is\\neasy to fix. Just make a new variable that is contiguous. This is enough to do it:\\nR code\\n13.41\\nd$district_id <- as.integer(as.factor(d$district))\\nsort(unique(d$district_id))\\n[1]\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\\n[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\\n[51] 51 52 53 54 55 56 57 58 59 60\\nNow there are 60 values, contiguous integers 1 to 60. Now, focus on predicting use.contraception,\\nclustered by district_id. Fit both (1) a traditional fixed-effects model that uses an index variable for\\ndistrict and (2) a multilevel model with varying intercepts for district. Plot the predicted proportions\\nof women in each district using contraception, for both the fixed-effects model and the varying-effects\\nmodel. That is, make a plot in which district ID is on the horizontal axis and expected proportion\\nusing contraception is on the vertical. Make one plot for each model, or layer them on the same\\nplot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement? In\\nparticular, can you explain the most extreme cases of disagreement, both why they happen where\\nthey do and why the models reach different inferences?\\n13H2. Return to data(Trolley) from Chapter 12. Define and fit a varying intercepts model for\\nthese data. Cluster intercepts on individual participants, as indicated by the unique values in the\\nid variable. Include action, intention, and contact as ordinary terms. Compare the varying\\nintercepts model and a model that ignores individuals, using both WAIC and posterior predictions.\\nWhat is the impact of individual variation in these data?\\n13H3. The Trolley data are also clustered by story, which indicates a unique narrative for each\\nvignette. Define and fit a cross-classified varying intercepts model with both id and story. Use the\\nsame ordinary terms as in the previous problem. Compare this model to the previous models. What\\ndo you infer about the impact of different stories on responses?\\n13H4. Revisit the Reed frog survival data, data(reedfrogs), and add the predation and size\\ntreatment variables to the varying intercepts model. Consider models with either predictor alone,\\nboth predictors, as well as a model including their interaction. What do you infer about the causal\\ninfluence of these predictor variables? Also focus on the inferred variation across tanks (the σ across\\ntanks). Explain why it changes as it does across models with different predictors included.\\n'},\n",
       " {'index': 452, 'number': 434, 'content': ''},\n",
       " {'index': 453,\n",
       "  'number': 435,\n",
       "  'content': '14 Adventures in Covariance\\nRecall the coffee robot from the introduction to the previous chapter (page 399). This\\nrobot is programmed to move among cafés, order coffee, and record the waiting time. The\\nprevious chapter focused on the fact that the robot learns more efficiently when it pools\\ninformation among the cafés. Varying intercepts are a mechanism for achieving that pooling.\\nNow suppose that the robot also records the time of day, morning or afternoon. The av-\\nerage wait time in the morning tends to be longer than the average wait time in the afternoon.\\nThis is because cafés are busier in the morning. But just like cafés vary in their average wait\\ntimes, they also vary in their differences between morning and afternoon. In conventional\\nregression, these differences in wait time between morning and afternoon are slopes, since\\nthey express the change in expectation when an indictor (or dummy, page 154) variable for\\ntime of day changes value. The linear model might look like this:\\nµi = αcafé[i] + βcafé[i]Ai\\nwhere Ai is a 0/1 indicator for afternoon and βcafé[i] is a parameter for the expected difference\\nbetween afternoon and morning for each café.\\nSince the robot more efficiently learns about the intercepts, αcafé[i] above, when it pools\\ninformation about intercepts, it likewise learns more efficiently about the slopes when it also\\npools information about slopes. And the pooling is achieved in the same way, by estimating\\nthe population distribution of slopes at the same time the robot estimates each slope. The\\ndistributions assigned to both intercepts and slopes enable pooling for both, as the model\\n(robot) learns the prior from the data.\\nThis is the essence of the general varying effects strategy: Any batch of parameters\\nwith exchangeable index values can and probably should be pooled. Exchangeable just means\\nthe index values have no true ordering, because they are arbitrary labels. There’s nothing\\nspecial about intercepts; slopes can also vary by unit in the data, and pooling information\\namong them makes better use of the data. So our coffee robot should be programmed to\\nmodel both the population of intercepts and the population of slopes. Then it can use pooling\\nfor both and squeeze more information out of the data.\\nBut here’s a fact that will help us to squeeze even more information out of the data: Cafés\\ncovary in their intercepts and slopes. Why? At a popular café, wait times are on average long\\nin the morning, because staff are very busy (Figure 14.1). But the same café will be much\\nless busy in the afternoon, leading to a large difference between morning and afternoon wait\\ntimes. At such a popular café, the intercept is high and the slope is far from zero, because the\\ndifference between morning and afternoon waits is large. But at a less popular café, the dif-\\nference will be small. Such an unpopular café makes you wait less in the morning—because\\n435\\n'},\n",
       " {'index': 454,\n",
       "  'number': 436,\n",
       "  'content': '436\\n14. ADVENTURES IN COVARIANCE\\n2\\n4\\n6\\n8\\nwait time (minutes)\\nM\\nA\\nM\\nA\\nM\\nA\\nM\\nA\\nM\\nA\\n2\\n4\\n6\\n8\\nwait time (minutes)\\nM\\nA\\nM\\nA\\nM\\nA\\nM\\nA\\nM\\nA\\nFigure 14.1. Waiting times at two cafés.\\nTop: A busy café at which wait times nearly\\nalways improve in the afternoon. Bottom:\\nAn unpopular café where wait times are\\nnearly always short.\\nIn a population of\\ncafés like these, long morning waits (in-\\ntercepts) covary with larger differences be-\\ntween morning and afternoon (slopes).\\nit’s not busy—but there isn’t much improvement in the afternoon. In the entire population\\nof cafés, including both the popular and the unpopular, intercepts and slopes covary.\\nThis covariation is information that the robot can use. If we can figure out a way to\\npool information across parameter types—intercepts and slopes—what the robot learns in\\nthe morning can improve learning about afternoons, and vice versa. Suppose for example\\nthat the robot arrives at a new café in the morning. It observes a long wait for its coffee. Even\\nbefore it orders a coffee at the same café in the afternoon, it can update its expectation for\\nhow long it will wait. In the population of cafés, a long wait in the morning is associated with\\na shorter wait in the afternoon.\\nIn this chapter, you’ll see how to really do this, to specify varying slopes in combina-\\ntion with the varying intercepts of the previous chapter. This will enable pooling that will im-\\nprove estimates of how different units respond to or are influenced by predictor variables. It\\nwill also improve estimates of intercepts, by borrowing information across parameter types.\\nEssentially, varying slopes models are massive interaction machines. They allow every unit\\nin the data to have its own response to any treatment or exposure or event, while also improv-\\ning estimates via pooling. When the variation in slopes is large, the average slope is of less\\ninterest. Sometimes, the pattern of variation in slopes provides hints about omitted variables\\nthat explain why some units respond more or less. We’ll see an example in this chapter.\\nThe machinery that makes such complex varying effects possible will be used later in the\\nchapter to extend the varying effects strategy to more subtle model types, including the use\\nof continuous categories, using Gaussian processes. Ordinary varying effects work only\\nwith discrete, unordered categories, such as individuals, countries, or ponds. In these cases,\\neach category is equally different from all of the others. But it is possible to use pooling with\\ncategories such as age or location. In these cases, some ages and some locations are more\\nsimilar than others. You’ll see how to model covariation among continuous categories of\\nthis kind, as well as how to generalize the strategy to seemingly unrelated types of models\\nsuch as phylogenetic and network regressions. Finally, we’ll circle back to causal inference\\n'},\n",
       " {'index': 455,\n",
       "  'number': 437,\n",
       "  'content': '14.1. VARYING SLOPES BY CONSTRUCTION\\n437\\nand use our new powers over covariance to go beyond the tools of Chapter 6, introducing in-\\nstrumental variables. Instruments are ways of inferring cause without closing backdoor\\npaths. However they are very tricky both in design and estimation.\\nThe material in this chapter is difficult. So if it suddenly seems both conceptually and\\ncomputationally much more difficult, that only means you are paying attention. Material like\\nthis requires repetition, discussion, and learning from mistakes. The struggle is definitely\\nworth it. You don’t have to understand it all at once.\\n14.1. Varying slopes by construction\\nHow should the robot pool information across intercepts and slopes? By modeling the\\njoint population of intercepts and slopes, which means by modeling their covariance. In\\nconventional multilevel models, the device that makes this possible is a joint multivariate\\nGaussian distribution for all of the varying effects, both intercepts and slopes. So instead\\nof having two independent Gaussian distributions of intercepts and of slopes, the robot can\\ndo better by assigning a two-dimensional Gaussian distribution to both the intercepts (first\\ndimension) and the slopes (second dimension).\\nYou’ve been working with multivariate Gaussian distributions ever since Chapter 4, when\\nyou began using the quadratic approximation for the posterior distribution. The variance-\\ncovariance matrix, vcov, for a fit model describes how each parameter’s posterior probability\\nis associated with each other parameter’s posterior probability. Now we’ll use the same kind\\nof distribution to describe the variation within and covariation among different kinds of\\nvarying effects. Varying intercepts have variation, and varying slopes have variation. Inter-\\ncepts and slopes covary.\\nIn order to see how this works and how varying slopes are specified and interpreted,\\nlet’s simulate the coffee robot from the introduction. Like previous simulation exercises, this\\nwill simultaneously help you see how to conduct your own prospective power analyses, in\\naddition to reemphasizing the generative nature of Bayesian statistical models.\\nRethinking: Why Gaussian? There is no reason the multivariate distribution of intercepts and slopes\\nmust be Gaussian. But there are both practical and epistemological justifications. On the practical\\nside, there aren’t many multivariate distributions that are easy to work with. The only common ones\\nare multivariate Gaussian and multivariate Student-t distributions. On the epistemological side, if all\\nwe want to say about these intercepts and slopes is their means, variances, and covariances, then the\\nmaximum entropy distribution is multivariate Gaussian. But thin Gaussian tails can still be risky.\\n14.1.1. Simulate the population. Begin by defining the population of cafés that the robot\\nmight visit. This means we’ll define the average wait time in the morning and the afternoon,\\nas well as the correlation between them. These numbers are sufficient to define the average\\nproperties of the cafés. Let’s define these properties, then we’ll sample cafés from them.\\nR code\\n14.1\\na <- 3.5\\n# average morning wait time\\nb <- (-1)\\n# average difference afternoon wait time\\nsigma_a <- 1\\n# std dev in intercepts\\nsigma_b <- 0.5\\n# std dev in slopes\\nrho <- (-0.7)\\n# correlation between intercepts and slopes\\n'},\n",
       " {'index': 456,\n",
       "  'number': 438,\n",
       "  'content': '438\\n14. ADVENTURES IN COVARIANCE\\nThese values define the entire population of cafés. To use these values to simulate a sample\\nof cafés for the robot, we’ll need to build them into a 2-dimensional multivariate Gaussian\\ndistribution. This means we need a vector of two means and 2-by-2 matrix of variances and\\ncovariances. The means are easiest. The vector we need is just:\\nR code\\n14.2\\nMu <- c( a , b )\\nThat’s it. The value in a is the mean intercept, the wait in the morning. And the value in b is\\nthe mean slope, the difference in wait between afternoon and morning.\\nThe matrix of variances and covariances is arranged like this:\\n\\uf8eb\\n\\uf8ed\\nvariance of intercepts\\ncovariance of intercepts & slopes\\ncovariance of intercepts & slopes\\nvariance of slopes\\n\\uf8f6\\n\\uf8f8\\nAnd now in mathematical form:\\n \\nσ2\\nα\\nσασβρ\\nσασβρ\\nσ2\\nβ\\n!\\nThe variance in intercepts is σ2\\nα, and the variance in slopes is σ2\\nβ. These are found along the\\ndiagonal of the matrix. The other two elements of the matrix are the same, σασβρ. This is the\\ncovariance between intercepts and slopes. It’s just the product of the two standard deviations\\nand the correlation. It might help to imagine an ordinary variance as the covariance of a\\nvariable with itself. If you are rusty on the definition of a covariance—it’s okay, most people\\nare—then see the Overthinking box further down.\\nTo build this matrix with R code, there are several options. I’ll show you two, both very\\ncommon. The first is to just use matrix to build the entire covariance matrix directly:\\nR code\\n14.3\\ncov_ab <- sigma_a*sigma_b*rho\\nSigma <- matrix( c(sigma_a^2,cov_ab,cov_ab,sigma_b^2) , ncol=2 )\\nThe awkward thing is that R matrices defined this way fill down each column before moving\\nto the next row over. So the order inside the code above looks odd, but works. To see what I\\nmean by “fill down each column,” try this:\\nR code\\n14.4\\nmatrix( c(1,2,3,4) , nrow=2 , ncol=2 )\\n[,1] [,2]\\n[1,]\\n1\\n3\\n[2,]\\n2\\n4\\nThe first column filled, and then R started over at the top of the second column.\\nThe other common way to build the covariance matrix is conceptually very useful, be-\\ncause it treats the standard deviations and correlations separately. Then it matrix multiplies\\nthem to produce the covariance matrix. We’re going to use this approach later on, to define\\npriors, so it’s worth seeing it now. Here’s how it’s done:\\nR code\\n14.5\\nsigmas <- c(sigma_a,sigma_b) # standard deviations\\nRho <- matrix( c(1,rho,rho,1) , nrow=2 ) # correlation matrix\\n'},\n",
       " {'index': 457,\n",
       "  'number': 439,\n",
       "  'content': '14.1. VARYING SLOPES BY CONSTRUCTION\\n439\\n# now matrix multiply to get covariance matrix\\nSigma <- diag(sigmas) %*% Rho %*% diag(sigmas)\\nIf you are not sure what diag(sigmas) accomplishes, then try typing just diag(sigmas)\\nat the R prompt.\\nNow we’re ready to simulate some cafés, each with its own intercept and slope. Let’s\\ndefine the number of cafés:\\nR code\\n14.6\\nN_cafes <- 20\\nAnd to simulate their properties, we just sample randomly from the multivariate Gaussian\\ndistribution defined by Mu and Sigma:\\nR code\\n14.7\\nlibrary(MASS)\\nset.seed(5) # used to replicate example\\nvary_effects <- mvrnorm( N_cafes , Mu , Sigma )\\nNote the set.seed(5) line above. That’s there so you can replicate the precise results in\\nthe example figures. The particular number, 5, produces a particular sequence of random\\nnumbers. Each unique number generates a unique sequence. Including a set.seed line\\nlike this in your code allows others to exactly replicate your analyses. Later you’ll want to\\nrepeat the example without repeating the set.seed call, or with a different number, so you\\ncan appreciate the variation across simulations.\\nLook at the contents of vary_effects now. It should be a matrix with 20 rows and\\n2 columns. Each row is a café. The first column contains intercepts. The second column\\ncontains slopes. For transparency, let’s split these columns apart into nicely named vectors:\\nR code\\n14.8\\na_cafe <- vary_effects[,1]\\nb_cafe <- vary_effects[,2]\\nTo visualize these intercepts and slopes, go ahead and plot them against one another. This\\ncode will also show the distribution’s contours:\\nR code\\n14.9\\nplot( a_cafe , b_cafe , col=rangi2 ,\\nxlab=\"intercepts (a_cafe)\" , ylab=\"slopes (b_cafe)\" )\\n# overlay population distribution\\nlibrary(ellipse)\\nfor ( l in c(0.1,0.3,0.5,0.8,0.99) )\\nlines(ellipse(Sigma,centre=Mu,level=l),col=col.alpha(\"black\",0.2))\\nFigure 14.2 displays a typical result. In any particular simulation, the correlation may not\\nbe as obvious. But on average, the intercepts in a_cafe and the slopes in b_cafe will have a\\ncorrelation of −0.7, and you’ll be able to see this in the scatterplot. The contour lines in the\\nplot, produced by the ellipse package (make sure you install it), display the multivariate\\nGaussian population of intercepts and slopes that the 20 cafés were sampled from.\\n'},\n",
       " {'index': 458,\n",
       "  'number': 440,\n",
       "  'content': '440\\n14. ADVENTURES IN COVARIANCE\\n2\\n3\\n4\\n5\\n6\\n-2.0\\n-1.5\\n-1.0\\n-0.5\\nintercepts (a_cafe)\\nslopes (b_cafe)\\nFigure 14.2. 20 cafés sampled from a statisti-\\ncal population. The horizontal axis is the inter-\\ncept (average morning wait) for each cafe. The\\nvertical axis is the slope (average difference be-\\ntween afternoon and morning wait) for each\\ncafé.\\nThe gray ellipses illustrate the multi-\\nvariate Gaussian population of intercepts and\\nslopes.\\nOverthinking: Variance, covariance, correlation. In typical statistical usage, we define covariance\\nusing three parameters: (1) the standard deviation of the first variable (σα for example), (2) the stan-\\ndard deviation of the second variable (σβ for example), and (3) the correlation between the two vari-\\nables (ραβ for example). Why is the covariance equal to σασβραβ?\\nThe usual definition of the covariance between two variables x and y is cov(x, y) = E(xy) −\\nE(x) E(y). You can say this as “the covariance is the difference between the average product and the\\nproduct of the averages.” The variance is just a special case of this, the covariance of a variable with\\nitself: var(x) = cov(x, x) = E(x2) −E(x)2. If we consider only random variables with expecta-\\ntion zero—no harm done, since we can recenter at will—then these are just cov(x, y) = E(xy) and\\nvar(x) = E(x2).\\nA correlation is just a rescaled covariance, so that the minimum is −1 and the maximum is\\n1. We can standardize a covariance this way by dividing it by the maximum possible covariance,\\nwhich turns out to be\\np\\nvar(x) var(y), the product of the standard deviations. Now to show you that\\nthis is the largest that cov(x, y) = E(xy) can ever be. A covariance will be largest when the second\\nvariable y is just a rescaled copy of x. For example, let yi = pxi, where p is some proportion like\\n0.5 or 1.5. So y = px is just a stretched x. The covariance is now cov(x, y) = E(px2) = p E(x2).\\nThe variances are var(x) = E(x2) and var(y) = E(y2) = E(p2x2) = p2 E(x2). Having fun yet?\\nHere comes the end. var(x) var(y) = p2 E(x2)2 and so\\np\\nvar(x) var(y) = p E(x2) = cov(x, y).\\nThat’s the largest the covariance can get. So if we want a standardized measure of association, the\\ncorrelation, we divide the covariance by this maximum value, which gives us the usual definition of\\na correlation coefficient, ρxy = cov(x, y)/\\np\\nvar(x) var(y). Solve this equation for cov(x, y) and you\\nget cov(x, y) =\\np\\nvar(x) var(y)ρxy. Whew. All of this is just to show that the applied statistics usage\\nof covariance as cov(x, y) = σxσyρxy is as justified as it is convenient.\\n14.1.2. Simulate observations. We’re almost done simulating. What we did above was sim-\\nulate individual cafés and their average properties. Now all that remains is to simulate our\\nrobot visiting these cafés and collecting data. The code below simulates 10 visits to each café,\\n5 in the morning and 5 in the afternoon. The robot records the wait time during each visit.\\nThen it combines all of the visits into a common data frame.\\n'},\n",
       " {'index': 459,\n",
       "  'number': 441,\n",
       "  'content': '14.1. VARYING SLOPES BY CONSTRUCTION\\n441\\nR code\\n14.10\\nset.seed(22)\\nN_visits <- 10\\nafternoon <- rep(0:1,N_visits*N_cafes/2)\\ncafe_id <- rep( 1:N_cafes , each=N_visits )\\nmu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon\\nsigma <- 0.5\\n# std dev within cafes\\nwait <- rnorm( N_visits*N_cafes , mu , sigma )\\nd <- data.frame( cafe=cafe_id , afternoon=afternoon , wait=wait )\\nGo ahead and look inside the data frame d now. You’ll find exactly the sort of data that is\\nwell-suited to a varying slopes model. There are multiple clusters in the data. These are the\\ncafés. And each cluster is observed under different conditions. So it’s possible to estimate\\nboth an individual intercept for each cluster, as well as an individual slope.\\nIn this example, everything is balanced: Each café has been observed exactly 10 times,\\nand the time of day is always balanced as well, with 5 morning and 5 afternoon observations\\nfor each café. But in general the data do not need to be balanced. Just like the tadpoles ex-\\nample from the previous chapter, lack of balance can really favor the varying effects analysis,\\nbecause partial pooling uses information about the population where it is needed most.\\nRethinking: Simulation and misspecification. In this exercise, we are simulating data from a genera-\\ntive process and then analyzing that data with a model that reflects exactly the correct structure of that\\nprocess. But in the real world, we’re never so lucky. Instead we are always forced to analyze data with\\na model that is misspecified: The true data-generating process is different than the model. Simula-\\ntion can be used however to explore misspecification. Just simulate data from a process and then see\\nhow a number of models, none of which match exactly the data-generating process, perform. And\\nalways remember that Bayesian inference does not depend upon data-generating assumptions, such\\nas the likelihood, being true. Non-Bayesian approaches may depend upon sampling distributions for\\ntheir inferences, but this is not the case for a Bayesian model. In a Bayesian model, a likelihood is a\\nprior for the data, and inference about parameters can be surprisingly insensitive to its details.\\n14.1.3. The varying slopes model. Now we’re ready to play the process in reverse. We just\\ngenerated data from a set of 20 cafés, and those cafés were themselves generated from a\\nstatistical population of cafés. Now we’ll use that data to learn about the data-generating\\nprocess, through a model.\\nThe model is much like the varying intercepts models from the previous chapter. But\\nnow the joint population of intercepts and slopes appears, instead of just a distribution of\\nvarying intercepts. This is the varying slopes model, with explanation to follow. First we\\nhave the probability of the data and the linear model:\\nWi ∼Normal(µi, σ)\\n[likelihood]\\nµi = αcafé[i] + βcafé[i]Ai\\n[linear model]\\nThen comes the matrix of varying intercepts and slopes, with it’s covariance matrix:\\n\\x14\\nαcafé\\nβcafé\\n\\x15\\n∼MVNormal\\n\\x12\\x14\\nα\\nβ\\n\\x15\\n, S\\n\\x13\\n[population of varying effects]\\nS =\\n\\x12\\nσα\\n0\\n0\\nσβ\\n\\x13\\nR\\n\\x12\\nσα\\n0\\n0\\nσβ\\n\\x13\\n[construct covariance matrix]\\n'},\n",
       " {'index': 460,\n",
       "  'number': 442,\n",
       "  'content': '442\\n14. ADVENTURES IN COVARIANCE\\nThese lines state that each café has an intercept αcafé and slope βcafé with a prior distribution\\ndefined by the two-dimensional Gaussian distribution with means α and β and covariance\\nmatrix S. This statement of prior will adaptively regularize the individual intercepts, slopes,\\nand the correlation among them. The second line above defines how we’re constructing the\\ncovariance matrix S, by factoring it into separate standard deviations, σα and σβ, and a corre-\\nlation matrix R. There are other ways to go about this, but by splitting the covariance up into\\nstandard deviations and correlations, it’ll be easier to later understand the inferred structure\\nof the varying effects.\\nAnd then come the hyper-priors, the priors that define the adaptive varying effects prior:\\nα ∼Normal(5, 2)\\n[prior for average intercept]\\nβ ∼Normal(−1, 0.5)\\n[prior for average slope]\\nσ ∼Exponential(1)\\n[prior stddev within cafés]\\nσα ∼Exponential(1)\\n[prior stddev among intercepts]\\nσβ ∼Exponential(1)\\n[prior stddev among slopes]\\nR ∼LKJcorr(2)\\n[prior for correlation matrix]\\nThe final line probably looks unfamiliar. The correlation matrix R needs a prior. It isn’t easy\\nto conceptualize what a distribution of matrices means. But in this introductory case, it isn’t\\nso hard. This particular correlation matrix is only 2-by-2 in size. So it looks like this:\\nR =\\n\\x12\\n1\\nρ\\nρ\\n1\\n\\x13\\nwhere ρ is the correlation between intercepts and slopes. So there’s just one parameter to\\ndefine a prior for. In larger matrices, with additional varying slopes, it gets more complicated.\\nSo whatever is the LKJcorr distribution? What LKJcorr(2) does is define a weakly infor-\\nmative prior on ρ that is skeptical of extreme correlations near −1 or 1.203 You can think of\\nit as a regularizing prior for correlation matrices. This distribution has a single parameter,\\nη, that controls how skeptical the prior is of large correlations in the matrix. When we use\\nLKJcorr(1), the prior is flat over all valid correlation matrices. When the value is greater\\nthan 1, such as the 2 we used above, then extreme correlations are less likely. To visualize\\nthis family of priors, it will help to sample random matrices from it:\\nR code\\n14.11\\nR <- rlkjcorr( 1e4 , K=2 , eta=2 )\\ndens( R[,1,2] , xlab=\"correlation\" )\\nThis is shown in Figure 14.3, along with two other η values. When the matrix is larger, there\\nare more correlations inside it, but the nature of the distribution remains the same. There is\\nan example density for a 3-by-3 matrix in the help page examples, ?rlkjcorr.\\nTo fit the model, we use a list of formulas that closely mirrors the model definition above.\\nNote the use of c() to combine parameters into a vector.\\nR code\\n14.12\\nset.seed(867530)\\nm14.1 <- ulam(\\nalist(\\nwait ~ normal( mu , sigma ),\\nmu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,\\n'},\n",
       " {'index': 461,\n",
       "  'number': 443,\n",
       "  'content': '14.1. VARYING SLOPES BY CONSTRUCTION\\n443\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ncorrelation\\nDensity\\neta=4\\neta=2\\neta=1\\nFigure 14.3. LKJcorr(η) probability density.\\nThe plot shows the distribution of correla-\\ntion coefficients extracted from random 2-by-\\n2 correlation matrices, for three values of η.\\nWhen η = 1, all correlations are equally plau-\\nsible. As η increases, extreme correlations be-\\ncome less plausible.\\nc(a_cafe,b_cafe)[cafe] ~ multi_normal( c(a,b) , Rho , sigma_cafe ),\\na ~ normal(5,2),\\nb ~ normal(-1,0.5),\\nsigma_cafe ~ exponential(1),\\nsigma ~ exponential(1),\\nRho ~ lkj_corr(2)\\n) , data=d , chains=4 , cores=4 )\\nThe distribution multi_normal is a multivariate Gaussian notation that takes a vector of\\nmeans, c(a,b), a correlation matrix, Rho, and a vector of standard deviations, sigma_cafe.\\nIt constructs the covariance matrix internally. If you are interested in the details, you can\\npeek at the raw Stan code with stancode(m14.1). The name multi_normal is what Stan\\nuses in its raw code. The similar R functions are dmvnorm and dmvnorm2.\\nNow instead of looking at the marginal posterior distributions in the precis output, let’s\\ngo straight to inspecting the posterior distribution of varying effects. First, let’s examine the\\nposterior correlation between intercepts and slopes.\\nR code\\n14.13\\npost <- extract.samples(m14.1)\\ndens( post$Rho[,1,2] , xlim=c(-1,1) ) # posterior\\nR <- rlkjcorr( 1e4 , K=2 , eta=2 )\\n# prior\\ndens( R[,1,2] , add=TRUE , lty=2 )\\nThe result is shown in Figure 14.4, with some additional decoration and the addition of\\nthe prior for comparison. The blue density is the posterior distribution of the correlation\\nbetween intercepts and slopes. The posterior is concentrated on negative values, because the\\nmodel has learned the negative correlation you can see in Figure 14.2. Keep in mind that\\nthe model did not get to see the true intercepts and slopes. All it had to work from was the\\nobserved wait times in morning and afternoon.\\nIf you are curious about the impact of the prior, then you should change the prior and\\nrepeat the analysis. I suggest trying a flat prior, LKJcorr(1), and then a more strongly regu-\\nlarizing prior like LKJcorr(4) or LKJcorr(5).\\n'},\n",
       " {'index': 462,\n",
       "  'number': 444,\n",
       "  'content': '444\\n14. ADVENTURES IN COVARIANCE\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\ncorrelation\\nDensity\\nposterior\\nprior\\nFigure 14.4. Posterior distribution of the cor-\\nrelation between intercepts and slopes. Blue:\\nPosterior distribution of the correlation, reli-\\nably below zero. Dashed: Prior distribution,\\nthe LKJcorr(2) density.\\nNext, consider the shrinkage. The multilevel model estimates posterior distributions for\\nintercepts and slopes of each café. The inferred correlation between these varying effects\\nwas used to pool information across them. This is just as the inferred variation among in-\\ntercepts pools information among them, as well as how the inferred variation among slopes\\npools information among them. All together, the variances and correlation define an in-\\nferred multivariate Gaussian prior for the varying effects. And this prior, learned from the\\ndata, adaptively regularizes both the intercepts and slopes.\\nTo see the consequence of this adaptive regularization, shrinkage, let’s plot the posterior\\nmean varying effects. Then we can compare them to raw, unpooled estimates. We’ll also\\nshow the contours of the inferred prior—the population of intercepts and slopes—and this\\nwill help us visualize the shrinkage. Here’s code to plot the unpooled estimates and posterior\\nmeans.\\nR code\\n14.14\\n# compute unpooled estimates directly from data\\na1 <- sapply( 1:N_cafes ,\\nfunction(i) mean(wait[cafe_id==i & afternoon==0]) )\\nb1 <- sapply( 1:N_cafes ,\\nfunction(i) mean(wait[cafe_id==i & afternoon==1]) ) - a1\\n# extract posterior means of partially pooled estimates\\npost <- extract.samples(m14.1)\\na2 <- apply( post$a_cafe , 2 , mean )\\nb2 <- apply( post$b_cafe , 2 , mean )\\n# plot both and connect with lines\\nplot( a1 , b1 , xlab=\"intercept\" , ylab=\"slope\" ,\\npch=16 , col=rangi2 , ylim=c( min(b1)-0.1 , max(b1)+0.1 ) ,\\nxlim=c( min(a1)-0.1 , max(a1)+0.1 ) )\\npoints( a2 , b2 , pch=1 )\\nfor ( i in 1:N_cafes ) lines( c(a1[i],a2[i]) , c(b1[i],b2[i]) )\\nAnd to superimpose the contours of the population:\\n'},\n",
       " {'index': 463,\n",
       "  'number': 445,\n",
       "  'content': '14.1. VARYING SLOPES BY CONSTRUCTION\\n445\\nR code\\n14.15\\n# compute posterior mean bivariate Gaussian\\nMu_est <- c( mean(post$a) , mean(post$b) )\\nrho_est <- mean( post$Rho[,1,2] )\\nsa_est <- mean( post$sigma_cafe[,1] )\\nsb_est <- mean( post$sigma_cafe[,2] )\\ncov_ab <- sa_est*sb_est*rho_est\\nSigma_est <- matrix( c(sa_est^2,cov_ab,cov_ab,sb_est^2) , ncol=2 )\\n# draw contours\\nlibrary(ellipse)\\nfor ( l in c(0.1,0.3,0.5,0.8,0.99) )\\nlines(ellipse(Sigma_est,centre=Mu_est,level=l),\\ncol=col.alpha(\"black\",0.2))\\nThe result appears on the left in Figure 14.5. The blue points are the unpooled estimates for\\neach café. The open points are the posterior means from the varying effects model. A line\\nconnects the points that belong to the same café. Each open point is displaced from the blue\\ntowards the center of the contours, as a result of shrinkage in both dimensions. Blue points\\nfarther from the center experience more shrinkage, because they are less plausible, given the\\ninferred population.\\nBut notice too that shrinkage is not in direct lines towards the center. This is most ob-\\nvious for the café that appears in the top-middle of the plot. That particular café had an\\naverage intercept, so it lies in the middle of the horizontal axis. But it also had an unusu-\\nally high slope, so it lies at the top of the vertical axis. Pooled information from the other\\ncafés results in skepticism about the slope. But since intercepts and slopes are correlated in\\nthe population as a whole, shrinking the slope down also shrinks the intercept. So all those\\nangled shrinkage lines reflect the negative correlation between intercepts and slopes.\\nThe right-hand plot in Figure 14.5 displays the same information, but now on the out-\\ncome scale. You can compute these average outcomes from knowledge of the linear model:\\nR code\\n14.16\\n# convert varying effects to waiting times\\nwait_morning_1 <- (a1)\\nwait_afternoon_1 <- (a1 + b1)\\nwait_morning_2 <- (a2)\\nwait_afternoon_2 <- (a2 + b2)\\n# plot both and connect with lines\\nplot( wait_morning_1 , wait_afternoon_1 , xlab=\"morning wait\" ,\\nylab=\"afternoon wait\" , pch=16 , col=rangi2 ,\\nylim=c( min(wait_afternoon_1)-0.1 , max(wait_afternoon_1)+0.1 ) ,\\nxlim=c( min(wait_morning_1)-0.1 , max(wait_morning_1)+0.1 ) )\\npoints( wait_morning_2 , wait_afternoon_2 , pch=1 )\\nfor ( i in 1:N_cafes )\\nlines( c(wait_morning_1[i],wait_morning_2[i]) ,\\nc(wait_afternoon_1[i],wait_afternoon_2[i]) )\\nabline( a=0 , b=1 , lty=2 )\\n'},\n",
       " {'index': 464,\n",
       "  'number': 446,\n",
       "  'content': '446\\n14. ADVENTURES IN COVARIANCE\\n2\\n3\\n4\\n5\\n6\\n-2.5\\n-2.0\\n-1.5\\n-1.0\\n-0.5\\n0.0\\nintercept\\nslope\\n2\\n3\\n4\\n5\\n6\\n1\\n2\\n3\\n4\\n5\\nmorning wait\\nafternoon wait\\nFigure 14.5. Shrinkage in two dimensions. Left: Raw unpooled intercepts\\nand slopes (filled blue) compared to partially pooled posterior means (open\\ncircles). The gray contours show the inferred population of varying effects.\\nRight: The same estimates on the outcome scale.\\nTo add the contour, we need the variances and covariance. We could use a formula—there\\nare some simple relations among Gaussian random variables. But to make this lesson more\\ngeneral, let’s simulate instead, so you can see how to compute anything of interest.\\nR code\\n14.17\\n# now shrinkage distribution by simulation\\nv <- mvrnorm( 1e4 , Mu_est , Sigma_est )\\nv[,2] <- v[,1] + v[,2] # calculate afternoon wait\\nSigma_est2 <- cov(v)\\nMu_est2 <- Mu_est\\nMu_est2[2] <- Mu_est[1]+Mu_est[2]\\n# draw contours\\nlibrary(ellipse)\\nfor ( l in c(0.1,0.3,0.5,0.8,0.99) )\\nlines(ellipse(Sigma_est2,centre=Mu_est2,level=l),\\ncol=col.alpha(\"black\",0.5))\\nThe horizontal axis in the plot shows the expected morning wait, in minutes, for each café.\\nThe vertical axis shows the expected afternoon wait. Again the blue points are unpooled em-\\npirical estimates from the data. The open points are posterior predictions, using the pooled\\nestimates. The diagonal dashed line shows where morning wait is equal to afternoon wait.\\nWhat I want you to appreciate in this plot is that shrinkage on the parameter scale naturally\\nproduces shrinkage where we actually care about it: on the outcome scale. And it also implies\\na population of wait times, shown by the gray contours. That population is now positively\\ncorrelated—cafés with longer morning waits also tend to have longer afternoon waits. They\\nare popular, after all. But the population lies mostly below the dashed line where the waits\\nare equal. You’ll wait less in the afternoon, on average.\\n'},\n",
       " {'index': 465,\n",
       "  'number': 447,\n",
       "  'content': '14.2. ADVANCED VARYING SLOPES\\n447\\n14.2. Advanced varying slopes\\nTo see how to construct a model with more than two varying effects—varying intercepts\\nplus more than one varying slope—as well as with more than one type of cluster, we’ll return\\nto the chimpanzee experiment data that was introduced in Chapter 11. In these data, there\\nare two types of clusters: actors and blocks. We explored cross-classification with two\\nkinds of varying intercepts back on page 415. We also modeled the experiment with two\\ndifferent slopes: one for the effect of the prosocial option (the side of the table with two\\npieces of food) and one for the interaction between the prosocial option and the presence of\\nanother chimpanzee. Now we’ll model both types of clusters and place varying effects on the\\nintercepts and both slopes. All of this machinery is not always necessary. But sometimes it\\nis, and this is a relatively simple example to lay it all out.\\nI’ll also use this example to emphasize the importance of non-centered parameteri-\\nzation for some multilevel models. For any given multilevel model, there are several differ-\\nent ways to write it down. These ways are called “parameterizations.” Mathematically, these\\nalternative parameterizations are equivalent, but inside the MCMC engine they are not. Re-\\nmember, how you fit the model is part of the model. Choosing a better parameterization\\nis an awesome way to improve sampling for your MCMC model fit, and the non-centered\\nparameterization tends to help a lot with complex varying effect models like the one you’ll\\nwork with in this section. I’ll hide the details of the technique in the main text. But as usual,\\nthere is an Overthinking box at the end that provides some detail.\\nOkay, let’s construct a cross-classified varying slopes model. To maintain some sanity\\nwith this complicated model, we’ll use more than one linear model in the formulas. This will\\nallow us to compartmentalize sub-models for the intercepts and each slope. Here’s what the\\nlikelihood and its linear model looks like:\\nLi ∼Binomial(1, pi)\\nlogit(pi) = γtid[i] + αactor[i],tid[i] + βblock[i],tid[i]\\nThe linear model for logit(pi) contains an average log-odds for each treatment, γtid[i], an\\neffect for each actor in each treatment, αactor[i],tid[i], and finally an effect for each block in\\neach treatment, βblock[i],tid[i]. This is essentially an interaction model that allows the effect of\\neach treatment to vary by each actor and each block. This is to say that the average treatment\\neffect can vary by block, and each individual chimpanzee can also respond (across blocks) to\\neach treatment differently. This yields a total of 4 + 7 × 4 + 6 × 4 = 56 parameters. Pooling\\nis really needed here.\\nSo let’s do some pooling. The next part of the model are the adaptive priors. Since\\nthere are two cluster types, actors and blocks, there are two multivariate Gaussian priors.\\nThe multivariate Gaussian priors are both 4-dimensional, in this example, because there are\\n4 treatments. But in general, you can choose to have different varying effects in different\\ncluster types. Here are the two priors in this case:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nαj,1\\nαj,2\\nαj,3\\nαj,4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb∼MVNormal\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb, Sactor\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n'},\n",
       " {'index': 466,\n",
       "  'number': 448,\n",
       "  'content': '448\\n14. ADVENTURES IN COVARIANCE\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\nβj,1\\nβj,2\\nβj,3\\nβj,4\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb∼MVNormal\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0\\n0\\n0\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb, Sblock\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\nWhat these priors state is that actors and blocks come from two different statistical popu-\\nlations. Within each, the 4 features of each actor or block are related through a covariance\\nmatrix S specific to that population. There are no means in these priors, just because we\\nalready placed the average treatment effects—γ—in the linear model.\\nAnd the ulam code for this model looks as you’d expect, given previous examples. To\\ndefine the multiple linear models, just write each into the formula list in order. I’ll add some\\nwhite space and comments to this formula list, to make it easier to read.\\nR code\\n14.18\\nlibrary(rethinking)\\ndata(chimpanzees)\\nd <- chimpanzees\\nd$block_id <- d$block\\nd$treatment <- 1L + d$prosoc_left + 2L*d$condition\\ndat <- list(\\nL = d$pulled_left,\\ntid = d$treatment,\\nactor = d$actor,\\nblock_id = as.integer(d$block_id) )\\nset.seed(4387510)\\nm14.2 <- ulam(\\nalist(\\nL ~ dbinom(1,p),\\nlogit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],\\n# adaptive priors\\nvector[4]:alpha[actor] ~ multi_normal(0,Rho_actor,sigma_actor),\\nvector[4]:beta[block_id] ~ multi_normal(0,Rho_block,sigma_block),\\n# fixed priors\\ng[tid] ~ dnorm(0,1),\\nsigma_actor ~ dexp(1),\\nRho_actor ~ dlkjcorr(4),\\nsigma_block ~ dexp(1),\\nRho_block ~ dlkjcorr(4)\\n) , data=dat , chains=4 , cores=4 )\\nWhen sampling from this model, you will notice many “divergent transitions”:\\nWarning messages:\\n1: There were 154 divergent transitions after warmup.\\nWe first discussed these back in Chapter 9. If you look at the diagnostics and the trankplot,\\nyou see that the chains are not mixing quite right. In the previous chapter, we saw how re-\\nparameterizing the model can help. We’ll do that again here. Our goal is to factor all the\\n'},\n",
       " {'index': 467,\n",
       "  'number': 449,\n",
       "  'content': '14.2. ADVANCED VARYING SLOPES\\n449\\nparameters out of the adaptive priors and place them instead in the linear model. But now\\nthat we have covariance matrixes in the priors, how are we going to do that?\\nThe basic strategy is the same, just extrapolated to matrixes. What we’ll do is again make\\nsome z-scores for each random effect. But now we need matrixes of z-scores, just like we\\nhad matrixes of random effects in the previous model. Then we’ll want to multiply those\\nz-scores into a covariance matrix so that we get back the random effects on the right scale\\nfor the linear model. There is a special matrix algebra trick for this, and ulam has a function\\ncompose_noncentered for performing this trick. The Overthinking box at the end of the\\nsection explains in more detail. This is how the non-centered version of the model looks:\\nR code\\n14.19\\nset.seed(4387510)\\nm14.3 <- ulam(\\nalist(\\nL ~ binomial(1,p),\\nlogit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],\\n# adaptive priors - non-centered\\ntranspars> matrix[actor,4]:alpha <-\\ncompose_noncentered( sigma_actor , L_Rho_actor , z_actor ),\\ntranspars> matrix[block_id,4]:beta <-\\ncompose_noncentered( sigma_block , L_Rho_block , z_block ),\\nmatrix[4,actor]:z_actor ~ normal( 0 , 1 ),\\nmatrix[4,block_id]:z_block ~ normal( 0 , 1 ),\\n# fixed priors\\ng[tid] ~ normal(0,1),\\nvector[4]:sigma_actor ~ dexp(1),\\ncholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky( 2 ),\\nvector[4]:sigma_block ~ dexp(1),\\ncholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky( 2 ),\\n# compute ordinary correlation matrixes from Cholesky factors\\ngq> matrix[4,4]:Rho_actor <<- Chol_to_Corr(L_Rho_actor),\\ngq> matrix[4,4]:Rho_block <<- Chol_to_Corr(L_Rho_block)\\n) , data=dat , chains=4 , cores=4 , log_lik=TRUE )\\nNo more divergent transitions! There are several advanced features of ulam on display above.\\nOne important bit to note is the last two lines. These compute the ordinary correlation ma-\\ntrixes from those Cholesky factors. This will help you interpret the correlations, if you want.\\nThat gq> tag in front of each line tells Stan to do this calculation only at the end of each tran-\\nsition. This is more efficient. If you are still curious about the details, see the Overthinking\\nbox further down for the raw Stan version of this model.\\nHow has the non-centered parameterization helped here? If you compare the precis\\noutput of the two models, you’ll see that they arrive at roughly the same inferences. But the\\nn_eff values for m14.2 are much larger, and it sampled more quickly in real time. Let’s show\\nthe difference in effective samples visually, using a simple scatterplot:\\n'},\n",
       " {'index': 468,\n",
       "  'number': 450,\n",
       "  'content': '450\\n14. ADVENTURES IN COVARIANCE\\n0\\n200\\n400\\n600\\n800\\n1000\\n1000\\n1400\\n1800\\n2200\\ncentered (default)\\nnon-centered (cholesky)\\nFigure\\n14.6. Distributions\\nof\\neffective\\nsamples,\\nn_eff,\\nfor\\nthe\\ncentered\\nand\\nnon-centered parameterizations of the cross-\\nclassified varying slopes model, m14.2 and\\nm14.3, respectively.\\nBoth models arrive at\\nequivalent inferences, but the non-centered\\nversion samples much more efficiently.\\nR code\\n14.20\\n# extract n_eff values for each model\\nneff_nc <- precis(m14.3,3,pars=c(\"alpha\",\"beta\"))$n_eff\\nneff_c <- precis(m14.2,3,pars=c(\"alpha\",\"beta\"))$n_eff\\nplot( neff_c , neff_nc , xlab=\"centered (default)\" ,\\nylab=\"non-centered (cholesky)\" , lwd=1.5 )\\nabline(a=0,b=1,lty=2)\\nFigure 14.6 displays the result. The non-centered version of the model samples much more\\nefficiently, producing more effective samples per parameter. In practice, this means you don’t\\nneed as many actual iterations, iter, to arrive at an equally good portrait of the posterior\\ndistribution. For larger data sets, the savings can mean hours of time. And in some problems,\\nthe centered version of the model just won’t give you a useful posterior.\\nThis model has 76 parameters: 4 average treatment effects, 4×7 varying effects on actor,\\n4×6 varying effects on block, 8 standard deviations, and 12 free correlation parameters. You\\ncan check them all for yourself with precis(m14.3,depth=3). But effectively the model\\nhas only about 27 parameters—check WAIC(m14.3). The two varying effects populations,\\none for actors and one for blocks, regularize the varying effects themselves. So as usual, each\\nvarying intercept or slope counts less than one effective parameter.\\nWe can inspect the standard deviation parameters to get a sense of how aggressively the\\nvarying effects are being regularized:\\nR code\\n14.21\\nprecis( m14.3 , depth=2 , pars=c(\"sigma_actor\",\"sigma_block\") )\\nmean\\nsd 5.5% 94.5% n_eff Rhat\\nsigma_actor[1] 1.37 0.47 0.77\\n2.20\\n832\\n1\\nsigma_actor[2] 0.91 0.40 0.42\\n1.62\\n1108\\n1\\nsigma_actor[3] 1.85 0.55 1.12\\n2.82\\n961\\n1\\nsigma_actor[4] 1.58 0.58 0.87\\n2.58\\n1109\\n1\\nsigma_block[1] 0.40 0.32 0.04\\n0.98\\n1112\\n1\\nsigma_block[2] 0.42 0.33 0.03\\n1.03\\n903\\n1\\nsigma_block[3] 0.31 0.28 0.02\\n0.80\\n1740\\n1\\n'},\n",
       " {'index': 469,\n",
       "  'number': 451,\n",
       "  'content': '14.2. ADVANCED VARYING SLOPES\\n451\\nsigma_block[4] 0.48 0.37 0.04\\n1.16\\n942\\n1\\nWhile these are just posterior means, and the amount of shrinkage averages over the entire\\nposterior, you can get a sense from the small values that shrinkage is pretty aggressive here,\\nespecially in the case of the blocks. This is what takes the model from 76 actual parameters\\nto 27 effective parameters, as measured by WAIC (or PSIS—it agrees in this case).\\nThis is a good example of how varying effects adapt to the data. The overfitting risk is\\nmuch milder here than it would be with ordinary fixed effects. It can of course be challenging\\nto define and fit these models. But if you don’t check for variation in slopes, you may never\\nnotice it. And even if the average slope is almost zero, there might still be substantial variation\\nin slopes across clusters.\\nBefore leaving this example behind, let’s look at the posterior predictions against the\\naverage for each actor and each treatment, as we did back in Chapter 11. This is going to be\\na big chunk of code, just like it was back in the earlier chapter. But there is nothing new here\\nreally. I’ll use block number 5 in these predictions, because it had almost zero effect, and we\\nwant to average over blocks in this visualization.\\nR code\\n14.22\\n# compute mean for each actor in each treatment\\npl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )\\n# generate posterior predictions using link\\ndatp <- list(\\nactor=rep(1:7,each=4) ,\\ntid=rep(1:4,times=7) ,\\nblock_id=rep(5,times=4*7) )\\np_post <- link( m14.3 , data=datp )\\np_mu <- apply( p_post , 2 , mean )\\np_ci <- apply( p_post , 2 , PI )\\n# set up plot\\nplot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab=\"\" ,\\nylab=\"proportion left lever\" , xaxt=\"n\" , yaxt=\"n\" )\\naxis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )\\nabline( h=0.5 , lty=2 )\\nfor ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )\\nfor ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat(\"actor \",j) , xpd=TRUE )\\nxo <- 0.1 # offset distance to stagger raw data and predictions\\n# raw data\\nfor ( j in (1:7)[-2] ) {\\nlines( (j-1)*4+c(1,3)-xo , pl[j,c(1,3)] , lwd=2 , col=rangi2 )\\nlines( (j-1)*4+c(2,4)-xo , pl[j,c(2,4)] , lwd=2 , col=rangi2 )\\n}\\npoints( 1:28-xo , t(pl) , pch=16 , col=\"white\" , cex=1.7 )\\npoints( 1:28-xo , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )\\nyoff <- 0.175\\ntext( 1-xo , pl[1,1]-yoff , \"R/N\" , pos=1 , cex=0.8 )\\ntext( 2-xo , pl[1,2]+yoff , \"L/N\" , pos=3 , cex=0.8 )\\ntext( 3-xo , pl[1,3]-yoff , \"R/P\" , pos=1 , cex=0.8 )\\ntext( 4-xo , pl[1,4]+yoff , \"L/P\" , pos=3 , cex=0.8 )\\n# posterior predictions\\n'},\n",
       " {'index': 470,\n",
       "  'number': 452,\n",
       "  'content': '452\\n14. ADVENTURES IN COVARIANCE\\nproportion left lever\\n0\\n0.5\\n1\\nactor 1\\nactor 2\\nactor 3\\nactor 4\\nactor 5\\nactor 6\\nactor 7\\nR/N\\nL/N\\nR/P\\nL/P\\nFigure 14.7. Posterior predictions, in black, against the raw data, in blue,\\nfor model m14.3, the cross-classified varying effects model. The line seg-\\nments are 89% compatibility intervals. Open circles are treatments without\\na partner. Filled circles are treatments with a partner. The prosocial loca-\\ntion alternates right-left-right-left, as labeled in actor 1.\\nfor ( j in (1:7)[-2] ) {\\nlines( (j-1)*4+c(1,3)+xo , p_mu[(j-1)*4+c(1,3)] , lwd=2 )\\nlines( (j-1)*4+c(2,4)+xo , p_mu[(j-1)*4+c(2,4)] , lwd=2 )\\n}\\nfor ( i in 1:28 ) lines( c(i,i)+xo , p_ci[,i] , lwd=1 )\\npoints( 1:28+xo , p_mu , pch=16 , col=\"white\" , cex=1.3 )\\npoints( 1:28+xo , p_mu , pch=c(1,1,16,16) )\\nThe result appears as Figure 14.7. The raw data are shown in blue. The posterior means and\\n89% compatibility intervals are shown in black. As in the earlier chapter, open circles are\\ntreatments without a partner. Filled circles are those with a partner. The prosocial treatments\\nalternate right-left-right-left, as labeled in actor 1. The most obvious difference from earlier\\nis that the model accommodates a lot more variation among individuals. Letting each actor\\nhave his or her own parameters allows this, at least when there is sufficient data for each actor.\\nNotice however that the posterior does not just repeat the data—there is shrinkage in several\\nplaces. Actor 2 is the most obvious. Recall that actor 2 always, in every treatment and block,\\npulled the left lever. The blue points cling to the top. But the posterior predictions shrink\\ninward. Why do they shrink inward more for some treatments, like 1 and 2, than others?\\nBecause those treatments had less variation among actors. Look back at the precis output\\non the previous page. The less variation among actors in a treatment, the more shrinkage\\namong actors in that same treatment.\\nOur interpretation of this experiment has not changed. These chimpanzees simply did\\nnot behave in any consistently different way in the partner treatments. The model we’ve\\nused here does have some advantages, though. Since it allows for some individuals to differ\\nin how they respond to the treatments, it could reveal a situation in which a treatment has no\\neffect on average, even though some of the individuals respond strongly. That wasn’t the case\\nhere. But often we are more interested in the distribution of responses than in the average\\nresponse, so a model that estimates the distribution of treatment effects is very useful.\\n'},\n",
       " {'index': 471,\n",
       "  'number': 453,\n",
       "  'content': \"14.2. ADVANCED VARYING SLOPES\\n453\\nSuppose for example that we are testing a pain reliever, like aspirin. For many medica-\\ntions, only some people benefit. The average treatment effect is not really as interesting as\\nthe distribution of treatment effects, in such cases.\\nOverthinking: Non-centered parameterization of the multilevel model. When there are inefficient\\nchains, often running the chains long enough will produce reliable samples from the posterior. This\\nwas the case with m14.2 in the main text. But this is both inefficient and unreliable. The chains could\\nstill be biased in subtle ways that are hard to detect. Better to re-parameterize, as explained in the\\npreceding section.204 How does this work in the case of covariance matrixes?\\nModel m14.3 uses a trick known as the Cholesky decomposition to smuggle the covariance ma-\\ntrix out of the prior. The top part of the model is the same as the centered version, m14.2. The changes\\nare the extra lines that construct the adaptive priors:\\n# adaptive priors - non-centered\\ntranspars> matrix[actor,4]:alpha <-\\ncompose_noncentered( sigma_actor , L_Rho_actor , z_actor ),\\ntranspars> matrix[block_id,4]:beta <-\\ncompose_noncentered( sigma_block , L_Rho_block , z_block ),\\nmatrix[4,actor]:z_actor ~ normal( 0 , 1 ),\\nmatrix[4,block_id]:z_block ~ normal( 0 , 1 ),\\nThese two lines that begin with transpars> define the matrixes of varying effects alpha and beta.\\nEach is a matrix with a row for each actor/block and a column for each effect. As a convenience,\\ncompose_noncentered mixes the vector of standard deviations, the correlation matrix, and the z-\\nscores together to make a matrix of parameters on the correct scale for the linear model. This means\\nthat the matrixes of z-scores—the third and fourth lines above—can just be normal(0,1). The other\\nchange to the model, to make it non-centered, is that the correlation matrixes have been replaced\\nwith something called a Cholesky factor, cholesky_factor_corr to be precise.\\nSo what is compose_concentered doing? And what are these mysterious Cholesky factors?\\nA Cholesky decomposition L is a way to represent a square, symmetric matrix like a correlation\\nmatrix R such that R = LL⊺. It is a marvelous fact that you can multiply L by a matrix of uncorrelated\\nsamples (z-scores) and end up with a matrix of correlated samples (the varying effects). This is the\\ntrick that lets us take the covariance matrix out of the prior. We just sample a matrix of uncorrelated\\nz-scores and then multiply those by the Cholesky factor and the standard deviations to get the varying\\neffects with the correct scale and correlation. It would be magic, except that it is just algebra.\\nLet’s look at the raw Stan code, to demystify all of this and help you transition to building mod-\\nels directly in Stan, where you will have more control. Those transpars> flags in the ulam code\\ndefine the matrixes alpha and beta as transformed parameters, which means that Stan will\\ninclude them in the posterior, even though they are just functions of parameters. So if you look at\\nstancode(m14.3), you’ll see a new block above the model block:\\ntransformed parameters{\\nmatrix[7,4] alpha;\\nmatrix[6,4] beta;\\nbeta = (diag_pre_multiply(sigma_block, L_Rho_block) * z_block)';\\nalpha = (diag_pre_multiply(sigma_actor, L_Rho_actor) * z_actor)';\\n}\\nThese are the calculations thatmergevectors ofstandarddeviations, sigma_actor and sigma_block,\\nwith Choleskycorrelation factors, L_Rho_actor and L_Rho_block. Thefunction diag_pre_multiply\\ndoes this—all it does is make a diagonal matrix from the sigma vector and then multiply, producing\\na Cholesky factor for the right covariance matrix. Finally, that Cholesky covariance factor is matrix\\nmultiplied by the matrix of z-scores. For convenience, the thing is transposed—that ' on the end\\nof each line—so we can index it as alpha[actor,effect] instead of alpha[effect,actor]. But\\nreally that step isn’t necessary.\\nThen down in the model block, the matrixes alpha and beta are just available as parameters, so\\nthe linear model part looks the same:\\n\"},\n",
       " {'index': 472,\n",
       "  'number': 454,\n",
       "  'content': '454\\n14. ADVENTURES IN COVARIANCE\\nmodel{\\nvector[504] p;\\nL_Rho_block ~ lkj_corr_cholesky( 2 );\\nsigma_block ~ exponential( 1 );\\nL_Rho_actor ~ lkj_corr_cholesky( 2 );\\nsigma_actor ~ exponential( 1 );\\ng ~ normal( 0 , 1 );\\nto_vector( z_block ) ~ normal( 0 , 1 );\\nto_vector( z_actor ) ~ normal( 0 , 1 );\\nfor ( i in 1:504 ) {\\np[i] = g[tid[i]] + alpha[actor[i], tid[i]] + beta[block_id[i], tid[i]];\\np[i] = inv_logit(p[i]);\\n}\\nL ~ binomial( 1 , p );\\n}\\nFrom top to bottom: The vector p is declared to hold our linear model calculations for each case, then\\nthe priors are defined in terms of Cholesky correlation factors and vectors of standard deviations.\\nThe z-score matrixes are assigned their prior using to_vector, because normal(0,1) applies to\\nvectors, not matrixes. The z-scores are still stored in matrix format—this to_vector stuff is just\\nneeded to force the same normal(0,1) prior on each cell in the matrix. Finally the linear model is\\ncomputed, using the alpha and beta matrixes from the transformed parameters block, and then\\nthe probability of the data is defined as usual.\\nThe last bit is generated quantities, where variables that are functions of each sample can be cal-\\nculated. This block is used here to transform the Cholesky factors into ordinary correlation matrixes,\\nso they can be interpreted as such, as well as to compute the log-probabilities needed to calculate\\nWAIC or PSIS.\\ngenerated quantities{\\nvector[504] log_lik;\\nvector[504] p;\\nmatrix[4,4] Rho_actor;\\nmatrix[4,4] Rho_block;\\nRho_block = multiply_lower_tri_self_transpose(L_Rho_block);\\nRho_actor = multiply_lower_tri_self_transpose(L_Rho_actor);\\nfor ( i in 1:504 ) {\\np[i] = g[tid[i]] + alpha[actor[i], tid[i]] + beta[block_id[i], tid[i]];\\np[i] = inv_logit(p[i]);\\n}\\nfor ( i in 1:504 ) log_lik[i] = binomial_lpmf( L[i] | 1 , p[i] );\\n}\\nThe function multiply_lower_tri_self_transpose is just a compact and efficient way to perform\\nthe matrix algebra needed to turn the Cholesky factor L into the corresponding matrix R = LL⊺.\\nThere is an obvious cost to these non-centered forms: They look a lot more confusing. Hard-\\nto-read models and model code limit our ability to share implementations with our colleagues, and\\nsharing is a principal goal of scientific computation.\\nFinally, not all combinations of model structure and data benefit from the non-centered parame-\\nterization. Sometimes the centered version—putting the means and standard deviations in the prior—\\nis better. So you might try the form that is most natural for you personally. If it gives you trouble,\\ntry an alternative form. With some experience, different forms of the same model become familiar.\\nThere is a practice problem at the end of this chapter that may help.\\n'},\n",
       " {'index': 473,\n",
       "  'number': 455,\n",
       "  'content': '14.3. INSTRUMENTS AND CAUSAL DESIGNS\\n455\\n14.3. Instruments and causal designs\\nBack in Chapter 6, you met a framework for deciding which variables to use in a regres-\\nsion. The key idea is that, in a graphic model like a DAG, many paths may connect a variable\\nto an outcome. Some of those paths are causal, so we want to leave them open. Other paths\\nare non-causal, for example backdoor paths. We want to close those, as well as not acciden-\\ntally open them by including the wrong variables in the model.\\nOf course sometimes it won’t be possible to close all of the non-causal paths or rule of\\nunobserved confounds. What can be done in that case? More than nothing. If you are lucky,\\nthere are ways to exploit a combination of natural experiments and clever modeling that\\nallow causal inference even when non-causal paths cannot be closed.\\nWe’ll start with the most famous, and possibly least intuitive, example. Then we’ll move\\non to describe some other approaches.\\n14.3.1. Instrumental variables. What is the impact of education E on wages W? Does more\\nschool improve future wages? If we just regress wages on achieved education, we expect the\\ninference to be biased by factors that influence both wages and education. For example,\\nindustrious people may both complete more education and earn higher wages, generating a\\ncorrelation between education and wages. But that doesn’t necessarily mean that education\\ncauses higher wages. It is often difficult to measure, or even imagine, all of the possible\\nconfounds of this kind. We end up with a DAG like this:\\nE\\nU\\nW\\nThe backdoor path E ←U →W ruins our day.\\nEven though we cannot condition on U, since we haven’t observed it, there might be\\nsomething we can do. If we can find a suitable instrumental variable. In causal terms,\\nan instrumental variable is a variable that acts like a natural experiment on the exposure E.\\nIn technical terms, an instrumental variable Q is a variable that satisfies these criteria:\\n(1) Independent of U (Q ⊥⊥U)\\n(2) Not independent of E (Q ̸⊥⊥E)\\n(3) Q cannot influence W except through E\\nThis last line is sometimes called the exclusion restriction. It cannot be strictly tested,\\nand it is often implausible. Similarly, the first line above cannot be tested. But if you have a\\nstrong understanding of the system, so that you believe these criteria, then magic can hap-\\npen. Also, while we can’t test independence implications for instruments, there may be other\\nimplications in the form of inequality constraints.205\\nIt is much easier to understand instruments with a DAG. In our education and wages\\nexample, the simplest instrument for education looks like this:\\nE\\nQ\\nU\\nW\\n'},\n",
       " {'index': 474,\n",
       "  'number': 456,\n",
       "  'content': '456\\n14. ADVENTURES IN COVARIANCE\\nThe instrument here is Q. Given this DAG, Q satisfies all of the criteria for a valid instru-\\nmental variable. Note that valid instruments can be embedded in much more complicated\\ngraphs. If you can condition on other variables, in order to satisfy the criteria listed above,\\nthen you have an instrument.\\nHow do we use Q in a model? You cannot just add it to a regression like any other\\npredictor variable. Why not? Suppose we regress W on E. This is the relationship we’d like\\nto know. The association is however confounded by the backdoor path through U. What\\nhappens if we then add Q to the model as another predictor? Bad stuff happens. There is\\nno backdoor path through Q, as you can see. But there is a non-causal path from Q to W\\nthrough U: Q →E ←U →W. This is a non-causal path, because changing Q doesn’t result\\nin any change in W through this path. But since we are conditioning on E in the same model,\\nand E is a collider of Q and U, the non-causal path is open. This confounds the coefficient\\non Q. It won’t be zero, because it’ll pick up the association between U and W. And then, as a\\nresult, the coefficient on E can get even more confounded. Used this way, an instrument like\\nQ might be called a bias amplifier.206\\nThis is all very confusing. Consider this example. Suppose Q indicates which quarter\\nof the year—winter, spring, summer, fall—a person was born in. Why might this influence\\neducation? Because people born earlier in the year tend to get less schooling. This is both\\nbecause they are biologically older when they start school and because they become eligible\\nto drop out of school earlier. Now, if it is true that Q influences W only through E, and\\nQ is also not influenced by confounds U, then Q is one of these mysterious instrumental\\nvariables. This means we can use it in a special way to make a valid causal inference about\\nE →W without measuring U.\\nThis example is based on a real study,207 but let’s simulate the data, both to keep it simple\\nand to be sure what the right answer is. Remember: With real data, you never know what the\\nright answer is. That is why studying simulated examples is so important, both for verifying\\nthat algorithms work and for schooling our intuition. Here are 500 simulated people:\\nR code\\n14.23\\nset.seed(73)\\nN <- 500\\nU_sim <- rnorm( N )\\nQ_sim <- sample( 1:4 , size=N , replace=TRUE )\\nE_sim <- rnorm( N , U_sim + Q_sim )\\nW_sim <- rnorm( N , U_sim + 0*E_sim )\\ndat_sim <- list(\\nW=standardize(W_sim) ,\\nE=standardize(E_sim) ,\\nQ=standardize(Q_sim) )\\nThe instrument Q varies from 1 to 4. Largest values are associated with more education,\\nthrough the addition of Q_sim to the mean of E_sim. I’ve assumed that the true influence\\nof education on wages is zero. This is just for the sake of the example. But the instrument Q\\ndoes influence education, so it can serve as an instrument for discovering E →W.\\nLet’s consider three models. First, if we naively regress wages on education, the model\\nwill be confident that education causes higher wages:\\n'},\n",
       " {'index': 475,\n",
       "  'number': 457,\n",
       "  'content': '14.3. INSTRUMENTS AND CAUSAL DESIGNS\\n457\\nR code\\n14.24\\nm14.4 <- ulam(\\nalist(\\nW ~ dnorm( mu , sigma ),\\nmu <- aW + bEW*E,\\naW ~ dnorm( 0 , 0.2 ),\\nbEW ~ dnorm( 0 , 0.5 ),\\nsigma ~ dexp( 1 )\\n) , data=dat_sim , chains=4 , cores=4 )\\nprecis( m14.4 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\naW\\n0.00 0.04 -0.06\\n0.06\\n2024\\n1\\nbEW\\n0.40 0.04\\n0.33\\n0.46\\n1996\\n1\\nsigma 0.92 0.03\\n0.87\\n0.97\\n1861\\n1\\nThis is just an ordinary confound, where the unmeasured U is ruining our inference. If you\\nhave incentives to believe that education enhances wages, you might report this inference as\\nis. But even if E does increase W, the estimate from this model will be biased upwards. It’s\\nnot enough to just know that E positively influences W. Accuracy matters.\\nNext let’s consider what happens when we add Q as an ordinary predictor. Modifying\\nthe model above:\\nR code\\n14.25\\nm14.5 <- ulam(\\nalist(\\nW ~ dnorm( mu , sigma ),\\nmu <- aW + bEW*E + bQW*Q,\\naW ~ dnorm( 0 , 0.2 ),\\nbEW ~ dnorm( 0 , 0.5 ),\\nbQW ~ dnorm( 0 , 0.5 ),\\nsigma ~ dexp( 1 )\\n) , data=dat_sim , chains=4 , cores=4 )\\nprecis( m14.5 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\naW\\n0.00 0.04 -0.06\\n0.06\\n1526\\n1\\nbEW\\n0.64 0.05\\n0.56\\n0.71\\n1381\\n1\\nbQW\\n-0.41 0.05 -0.48 -0.33\\n1416\\n1\\nsigma\\n0.86 0.03\\n0.82\\n0.90\\n1823\\n1\\nThis is a disaster. As expected from study of the DAG, bQW picks up an association from U.\\nAnd bEW is even further from the truth now. It was 0.4 above. Now it’s 0.64. That is bias\\namplification in action.\\nNow we’re ready to see how to correctly use Q. The answer is actually pretty simple. We\\njust use the generative model. Let’s write a simple generative version of the DAG. It really\\nhas four sub-models. First, there is model for how wages W are caused by education E and\\nthe unobserved confound U. In mathematical notation:\\nWi ∼Normal(µw,i, σw)\\nµw,i = αw + βewEi + Ui\\n'},\n",
       " {'index': 476,\n",
       "  'number': 458,\n",
       "  'content': '458\\n14. ADVENTURES IN COVARIANCE\\nSecond, there is a model for how education levels E are caused by quarter of birth Q—this is\\nour instrument recall—and the same unobserved confound U.\\nEi ∼Normal(µe,i, σe)\\nµe,i = αe + βqeQi + Ui\\nThe third model is for Q. The model just says that one-quarter of all people are born in each\\nquarter of the year.\\nQi ∼Categorical([0.25, 0.25, 0.25, 0.25])\\nThe fourth model says that the unobserved confound U is normally distributed with mean\\nzero and standard deviation one.\\nUi ∼Normal(0, 1)\\nU could have some other distribution. But this is the generative model at the moment.\\nNow we translate this generative model into a statistical model. We could do it by brute\\nforce, just treating the Ui values as missing data and imputing them. But you won’t see how\\nto do that until the next chapter. Besides, it is much more efficient to average over them and\\nestimate instead the covariance between W and E. That’s what we’ll do: Define W and E as\\ncoming from a common multivariate normal distribution. Like this:\\n\\x12\\nWi\\nEi\\n\\x13\\n∼MVNormal\\n\\x12\\x12\\nµw,i\\nµe,i\\n\\x13\\n, S\\n\\x13\\n[Joint wage & education model]\\nµw,i = αw + βewEi\\nµe,i = αe + βqeQi\\nThe matrix S in the first line is the error covariance between wages and education. It’s not\\nthe descriptive covariance between these variables, but rather the matrix equivalent of the\\ntypical σ we stick in a Gaussian regression. The above is a multivariate linear model,\\na regression with multiple simultaneous outcomes, all modeled with a joint error structure.\\nEach variable gets its own linear model, yielding the two µ definitions. It might bother you to\\nsee education E as both an outcome and a predictor inside the mean for W. But this statistical\\nrelationship is an implication of the DAG. There is nothing illegal about it. All it says is that\\nE might influence W and that also pairs of W, E values might have some residual correlation.\\nThat correlation arises, presuming the DAG, through the unobserved confound U.\\nThe full model also needs priors, of course. We standardized the variables, so we can use\\nour default priors for standardized linear regression. Here’s the ulam code:\\nR code\\n14.26\\nm14.6 <- ulam(\\nalist(\\nc(W,E) ~ multi_normal( c(muW,muE) , Rho , Sigma ),\\nmuW <- aW + bEW*E,\\nmuE <- aE + bQE*Q,\\nc(aW,aE) ~ normal( 0 , 0.2 ),\\nc(bEW,bQE) ~ normal( 0 , 0.5 ),\\nRho ~ lkj_corr( 2 ),\\nSigma ~ exponential( 1 )\\n), data=dat_sim , chains=4 , cores=4 )\\nprecis( m14.6 , depth=3 )\\n'},\n",
       " {'index': 477,\n",
       "  'number': 459,\n",
       "  'content': '14.3. INSTRUMENTS AND CAUSAL DESIGNS\\n459\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\naE\\n0.00 0.03 -0.06\\n0.05\\n1351\\n1\\naW\\n0.00 0.04 -0.07\\n0.07\\n1432\\n1\\nbQE\\n0.59 0.04\\n0.53\\n0.64\\n1321\\n1\\nbEW\\n-0.05 0.08 -0.18\\n0.07\\n1024\\n1\\nRho[1,1]\\n1.00 0.00\\n1.00\\n1.00\\nNaN\\nNaN\\nRho[1,2]\\n0.54 0.05\\n0.46\\n0.62\\n1080\\n1\\nRho[2,1]\\n0.54 0.05\\n0.46\\n0.62\\n1080\\n1\\nRho[2,2]\\n1.00 0.00\\n1.00\\n1.00\\n1361\\n1\\nSigma[1]\\n1.02 0.05\\n0.95\\n1.10\\n1085\\n1\\nSigma[2]\\n0.81 0.02\\n0.77\\n0.85\\n1768\\n1\\nThere is a lot going on here. But we can take it one piece at a time. First look at bEW, the es-\\ntimated influence of education on wages. It is small and straddles both sides of zero. That is\\nthe correct causal inference. Second, the correlation Rho[1,2] between the two outcomes,\\nwages and education, is reliably positive. That reflects the common influence of U. Remem-\\nber: This correlation is conditional on E (for W) and Q (for E). It isn’t the raw empirical\\ncorrelation, but rather the residual correlation.\\nIt’s a good idea to adjust the simulation and try other scenarios. To speed up your play,\\nyou can avoid re-compiling the models as long as you keep N=500 and run these lines to\\nsample from the posterior distributions:\\nR code\\n14.27\\nm14.4x <- ulam( m14.4 , data=dat_sim , chains=4 , cores=4 )\\nm14.6x <- ulam( m14.6 , data=dat_sim , chains=4 , cores=4 )\\nTo begin, you might try a scenario in which education has a positive influence but the con-\\nfound hides it:\\nR code\\n14.28\\nset.seed(73)\\nN <- 500\\nU_sim <- rnorm( N )\\nQ_sim <- sample( 1:4 , size=N , replace=TRUE )\\nE_sim <- rnorm( N , U_sim + Q_sim )\\nW_sim <- rnorm( N , -U_sim + 0.2*E_sim )\\ndat_sim <- list(\\nW=standardize(W_sim) ,\\nE=standardize(E_sim) ,\\nQ=standardize(Q_sim) )\\nYou should find that E and W have a negative correlation in their residual variance, because\\nthe confound positively influences one and negatively influences the other.\\nInstrumental variables are hard to understand. But there are some excellent tools to help\\nyou. For example, the dagitty package contains a function instrumentalVariables that\\nwill find instruments, if they are present in a DAG. In this example, we could define the DAG\\nand query the instrument this way:\\nR code\\n14.29\\nlibrary(dagitty)\\ndagIV <- dagitty( \"dag{ Q -> E <- U -> W <- E }\" )\\ninstrumentalVariables( dagIV , exposure=\"E\" , outcome=\"W\" )\\n'},\n",
       " {'index': 478,\n",
       "  'number': 460,\n",
       "  'content': '460\\n14. ADVENTURES IN COVARIANCE\\nQ\\nThis is no substitute for understanding. But it can help you develop understanding.\\nThe hardest thing about instrumental variables is believing in any particular instrument.\\nIf you believe in your DAG, they are easy to believe. But should you believe in your DAG?\\nAs an example, a study of islands employed wind direction as an instrument for inferring the\\nimpact of colonialism on economic development.208 Colonial history and economic perfor-\\nmance are confounded by many things, like the natural resources of an island. If however\\nwind direction influences date of colonization—because when ships used sails, trade winds\\nmade some islands easier to reach—but not economic performance directly, then it could\\nserve as an instrument. This is a very clever idea. But it is easy to imagine that wind in-\\nfluences many things about an island, including its pre-colonial history of contact and its\\necology, and that these variables will influence current economies.\\nA much more common type of instrument is distance to some service. If for example we\\nwant to estimate the influence of health care on the wellbeing of mothers, we cannot easily\\nrandomize health care among mothers. It would be unethical, for starters. But if mothers\\nnaturally vary in distance to care centers, and these distances are random with respect to\\npre-existing health variables, then distance might be an instrument that influences use of\\nhealth care but does not influence health directly. However, it’s not hard to think of ways\\nthat distance from a hospital could be associated with factors influencing health, violating\\nthe exclusion restriction.209\\nIn general, it is not possible to statistically prove whether a variable is a good instrument.\\nAs always, we need scientific knowledge outside of the data to make sense of the data.\\nRethinking: Two-stage worst squares. The instrumental variable model is often discussed with an\\nestimation procedure known as two-stage least squares (2SLS). This procedure involves two\\nlinear regressions. The predicted values of the first regression are fed into the second as data, with\\nadjustments so that the standard errors make sense. Amazingly, when the weather is nice, this pro-\\ncedure works. It relies upon large-sample approximations and has well-known problems.210 Like all\\ngolems, you just have to use it responsibly. Sometimes people mistake 2SLS for the model of instru-\\nmental variables. They are not the same thing. Any model can be estimated through a number of\\ndifferent procedures, each with its own benefits and costs. If we have count outcomes, measurement\\nerrors, missing values, or need varying effects, 2SLS is unreliable. Now that more capable procedures\\nexist, it is easier to fit instrumental variable models. But it can still be difficult. There are no guar-\\nantees that an effect can be estimated, just because the DAG says it is possible. Another issue that\\nwill always remain, no matter how you approximate the posterior, is that it is very hard to be sure the\\ninstrumental variable is any good.\\n14.3.2. Other designs. Instrumental variables are natural experiments that impersonate\\nrandomized experiments. In the example in the previous section, quarter of birth Q is like\\nan external manipulation of education E. That external shock to education is like an exper-\\nimental manipulation, in the sense that it allows us to estimate the impact of that external\\nshock and thereby derive a causal estimate.\\nThere are potentially many ways to find natural experiments. Not all of them are strictly\\ninstrumental variables. But they can provide theoretically correct designs for causal infer-\\nence, if you can believe the assumptions. Let’s consider two more.\\nIn addition to the backdoor criterion you met in Chapter 6, there is something called the\\nfront-door criterion. It is relevant in a DAG like this:\\n'},\n",
       " {'index': 479,\n",
       "  'number': 461,\n",
       "  'content': '14.3. INSTRUMENTS AND CAUSAL DESIGNS\\n461\\nU\\nX\\nY\\nZ\\nWe are interested, as usual, in the causal influence of X on Y. But there is an unobserved\\nconfound U, again as usual. It turns out that, if we can find a perfect mediator Z, then we can\\npossibly estimate the causal effect of X on Y. It isn’t crazy to think that causes are mediated by\\nother causes. Everything has a mechanism. Z in the DAG above is such a mechanism. If you\\nhave a believable Z variable, then the causal effect of X on Y is estimated by expressing the\\ngenerative model as a statistical model, similar to the instrumental variable example before.\\nIn special cases, such as when everything is linear and Gaussian, there is a formula. But we\\ndon’t need formulas. We just need to think generatively and use Bayes.\\nThe front-door criterion isn’t used much. This may be because it is relatively new or\\nrather that believable Z variables are rare. A possible example is the influence of social ties\\nformed in college on voting behavior in the United States Senate.211 The question is whether\\nsenators who went to the same college vote more similarly, because their social ties produce\\ncoordinated votes. The pure association between attending the same college and voting the\\nsame way is obviously confounded by lots of things. The front-door trick is to find some\\nmechanism through which social ties must act. In the case of the United States Senate, a\\nmechanism could be who sits next to who. It is easier to talk to and coordinate with people\\nsitting nearby. And since junior members are often assigned seats effectively at random,\\nseating is unlikely to share the same confounds as college attendance. Now consider some\\nsenators who attended UCLA. Some of them end up seated near one another. Others end\\nup seated next to rival UC Berkeley alums. If the ones seated near one another vote more\\nsimilarly to one another than to the UCLA alums seated elsewhere, that could be causal\\nevidence that social ties influence voting, as mediated by proximity on the Senate floor.\\nA more common design is regression discontinuity (or RDD). Suppose that we\\nwant to estimate the effect of winning an academic award on future success.212 This is con-\\nfounded by unobserved factors, like ability, that influence both the award and later success.\\nBut if we compare individuals who were just below the cutoff for the award to those who\\nwere just above the cutoff, these individuals should be similar in the unobserved factors. It’s\\nas if the award were applied at random, for individuals close to the cutoff. This is the idea\\nbehind regression discontinuity. In practice, one trend is fit for individuals above the cutoff\\nand another to those below the cutoff. Then an estimate of the causal effect is the average dif-\\nference between individuals just above and just below the cutoff. While the difference near\\nthe cutoff is of interest, the entire function influences this difference. So some care is needed\\nin choosing functions for the overall relationship between the exposure and the outcome.213\\nRethinking: Inevitable confounds. Much of the time, it is not possible to rule out confounding, even\\nif you have found a clever instrument or RDD. Reviewers or readers sometimes ignore estimates in\\nthese cases. This is a mistake. In these cases, it is still helpful to report estimates, because such es-\\ntimates provide information about the possible magnitude of the confounds. Combined with some\\nstructural assumptions, it is possible to calculate the influence that hypothetical confounding has on\\nyour estimates. This kind of sensitivity analysis is very useful, both for designing better stud-\\nies and for interpreting published ones.214 Of course all of this requires being honest about likely\\nconfounding, not eagerly interpreting every causal salad estimate as a causal effect.\\n'},\n",
       " {'index': 480,\n",
       "  'number': 462,\n",
       "  'content': '462\\n14. ADVENTURES IN COVARIANCE\\n0\\n20\\n40\\n60\\n80\\n100\\n0\\n20\\n40\\n60\\n80\\n100\\ngifts household A to household B\\ngifts B to A\\nFigure 14.8. Distribution of dyadic gifts\\nin data(KosterLeckie).\\n25 households\\npresent 300 dyads, with an overall correlation\\nof 0.24. But to get a sensible measure of bal-\\nance of gift giving, we need to make a model\\nthat deals with the repeat presence of specific\\nhouseholds across dyads.\\n14.4. Social relations as correlated varying effects\\nOnce you grasp the basic strategy of using covariance matrixes to represent populations\\nof correlated effects, you can accomplish a lot of different and scientifically relevant modeling\\ngoals. In this section, I present an example that constructs a custom covariance matrix with\\nspecial scientific meaning.\\nThe data we’ll work with are data(KosterLeckie), which loads two different tables,\\nkl_dyads and kl_households. See ?KosterLeckie for more details.215\\nR code\\n14.30\\nlibrary(rethinking)\\ndata(KosterLeckie)\\nFor now, we want to use the variables in kl_dyads. Each row in this table is a dyad of\\nhouseholds from a community in Nicaragua. We are interested in modeling gift exchanges\\namong these households. The outcome variables giftsAB and giftsBA in each row are the\\ncount of gifts in each direction within each dyad. The variables hidA and hidB tell us the\\nhousehold IDs in each dyad, and did is a unique dyad ID number. We’ll ignore the other\\nvariables for now.\\nFigure 14.8 shows the raw distribution of gifts across dyads. The overall correlation here\\nis 0.24. But taking this as a measure of balance of exchange would be a bad idea. First, the\\ncorrelation changes if we switch the A/B labels. Since the labels are arbitrary, that means the\\nmeasured correlation is also somewhat arbitrary. Second, the generative model in the back-\\nground is that gifts can be explained both by the special relationship in each dyad—some\\nhouseholds tend to exchange gifts frequently—as well as by the fact that some households\\ngive or receive a lot across all dyads, without regard to any special relationships among house-\\nholds. For example, if a household is poor, it might not give many gifts, but it might receive\\nmany. In order to statistically separate balanced exchange from generalized differences in\\ngiving and receiving, we need a model that treats these as separate. The type of model we’ll\\nconsider is often called a social relations model, or SRM.\\nSpecifically, we’ll model gifts from household A to household B as a combination of vary-\\ning effects specific to the household and the dyad. The outcome variables, the gift counts, are\\nPoisson variables—they are counts with no obvious upper bound. We’ll attach our varying\\n'},\n",
       " {'index': 481,\n",
       "  'number': 463,\n",
       "  'content': '14.4. SOCIAL RELATIONS AS CORRELATED VARYING EFFECTS\\n463\\neffects to these counts with a log link, as in the previous chapters. This gives us the first part\\nof the model:\\nyA→B ∼Poisson(λAB)\\nlog λAB = α + gA + rB + dAB\\nThe linear model has an intercept α that represents the average gifting rate (on the log scale)\\nacross all dyads. The other effects will be offsets from this average. Then gA is a varying effect\\nparameter for the generalized giving tendency of household A, regardless of dyad. The effect\\nrB is the generalized receiving of household B, regardless of dyad. Finally the effect dAB is\\nthe dyad-specific rate that A gives to B. There is a corresponding linear model for the other\\ndirection within the same dyad:\\nyB→A ∼Poisson(λBA)\\nlog λBA = α + gB + rA + dBA\\nTogether, this all implies that each household H needs varying effects, a gH and a rH. In\\naddition each dyad AB has two varying effects, dAB and dBA. We want to allow the g and r\\nparameters to be correlated—do people who give a lot also get a lot? We also want to allow the\\ndyad effects to be correlated—is there balance within dyads? We can do all of this with two\\ndifferent multi-normal priors. The first will represent the population of household effects:\\n\\x12\\ngi\\nri\\n\\x13\\n∼MVNormal\\n\\x12\\x12\\n0\\n0\\n\\x13\\n,\\n\\x12\\nσ2\\ng\\nσgσrρgr\\nσgσrρgr\\nσ2\\nr\\n\\x13\\x13\\nFor any household i, a pair of g and r parameters are assigned a prior with a typical covariance\\nmatrix with two standard deviations and a correlation parameter. There’s nothing new here.\\nThe second multi-normal prior will represent the population of dyad effects:\\n\\x12\\ndij\\ndji\\n\\x13\\n∼MVNormal\\n\\x12\\x12\\n0\\n0\\n\\x13\\n,\\n\\x12\\nσ2\\nd\\nσ2\\ndρd\\nσ2\\ndρd\\nσ2\\nd\\n\\x13\\x13\\nFor a dyad with households i and j, there is a pair of dyad effects with a prior with another\\ncovariance matrix. But this matrix is funny. Take a close look and you’ll see that there is only\\none standard deviation parameter, σd. Why? Because the labels in each dyad are arbitrary.\\nIt isn’t meaningful which household comes first or second. So both parameters must have\\nthe same variance. But we do want to estimate their correlation, and that is what ρd will do\\nfor us. If ρd is positive, then when one household gives more within a dyad, so too does the\\nother. If ρd is negative, then when one households gives more, the other gives less. If ρd is\\ninstead near zero, then there is no pattern within dyads.\\nLet’s build this model now. We need to construct the dyad covariance matrix in a custom\\nway, and we need to be careful with indexing the varying effects. Here is the model:\\nR code\\n14.31\\nkl_data <- list(\\nN = nrow(kl_dyads),\\nN_households = max(kl_dyads$hidB),\\ndid = kl_dyads$did,\\nhidA = kl_dyads$hidA,\\nhidB = kl_dyads$hidB,\\ngiftsAB = kl_dyads$giftsAB,\\ngiftsBA = kl_dyads$giftsBA\\n)\\n'},\n",
       " {'index': 482,\n",
       "  'number': 464,\n",
       "  'content': '464\\n14. ADVENTURES IN COVARIANCE\\nm14.7 <- ulam(\\nalist(\\ngiftsAB ~ poisson( lambdaAB ),\\ngiftsBA ~ poisson( lambdaBA ),\\nlog(lambdaAB) <- a + gr[hidA,1] + gr[hidB,2] + d[did,1] ,\\nlog(lambdaBA) <- a + gr[hidB,1] + gr[hidA,2] + d[did,2] ,\\na ~ normal(0,1),\\n## gr matrix of varying effects\\nvector[2]:gr[N_households] ~ multi_normal(0,Rho_gr,sigma_gr),\\nRho_gr ~ lkj_corr(4),\\nsigma_gr ~ exponential(1),\\n## dyad effects\\ntranspars> matrix[N,2]:d <-\\ncompose_noncentered( rep_vector(sigma_d,2) , L_Rho_d , z ),\\nmatrix[2,N]:z ~ normal( 0 , 1 ),\\ncholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky( 8 ),\\nsigma_d ~ exponential(1),\\n## compute correlation matrix for dyads\\ngq> matrix[2,2]:Rho_d <<- Chol_to_Corr( L_Rho_d )\\n), data=kl_data , chains=4 , cores=4 , iter=2000 )\\nI’ve broken this up into sections, to make it easier to read. The top section is the two out-\\ncomes, each direction of gifting in the dyad. Each linear model contains the intercept a. Then\\ncomes a giving effect for the household giving on that line, gr[hidA,1] or gr[hidB,1].\\nThat “1” is for the first column of the gr matrix. Then comes the receiving effect for the\\nhousehold receiving, either gr[hidB,2] or gr[hidA,2]. Finally, the dyad effects d[did,1]\\nfor household A and d[did,2] for household B. This is because we put household A in the\\nfirst column of the d matrix. The order is arbitrary. A and B are just labels.\\nThe next chunk of code defines the matrix of giving and receiving effects. The matrix\\ngr will have a row for each household and 2 columns. The first column will be the giving\\nvarying effect and the second column will be the receiving varying effect, just like in the\\nlinear models.\\nThe third chunk defines the special dyad matrix. These are non-centered, for the sake of\\nefficient mixing. The special piece is the rep_vector(sigma_d,2). This copies the standard\\ndeviation into a vector of length 2 and composes the covariance matrix from there. So we\\nend up with the correct covariance matrix, with the same variance for both effects.\\nFinally, there is a single line at the bottom that computes the correlation matrix for the\\ndyads. This is necessary, because the model is parameterized using a Cholesky factor. The\\nfunction Chol_to_Corr multiplies a matrix by its own transpose. This is how a Cholesky\\nfactor is made back into its original matrix. If you want to interpret the correlations among\\nthe effects, then this is a useful calculation. The gq> at the start of the line places the line in\\nStan’s generated quantities block, which holds code that is executed after each Hamiltonian\\ntransition. So anything you want calculated from each sample should be tagged in this way.\\nIt will show up in the posterior distribution.\\n'},\n",
       " {'index': 483,\n",
       "  'number': 465,\n",
       "  'content': '14.4. SOCIAL RELATIONS AS CORRELATED VARYING EFFECTS\\n465\\nThis model contains a lot of parameters. There are 600 dyad parameters, for example.\\nBut we can get some useful information from the covariance matrix components:\\nR code\\n14.32\\nprecis( m14.7 , depth=3 , pars=c(\"Rho_gr\",\"sigma_gr\") )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nRho_gr[1,1]\\n1.00 0.00\\n1.00\\n1.00\\nNaN\\nNaN\\nRho_gr[1,2] -0.40 0.20 -0.70 -0.07\\n1475\\n1\\nRho_gr[2,1] -0.40 0.20 -0.70 -0.07\\n1475\\n1\\nRho_gr[2,2]\\n1.00 0.00\\n1.00\\n1.00\\n3834\\n1\\nsigma_gr[1]\\n0.83 0.14\\n0.64\\n1.07\\n2371\\n1\\nsigma_gr[2]\\n0.42 0.09\\n0.29\\n0.57\\n1251\\n1\\nAs in other models with covariance matrixes, since the diagonal cells are always 1, you can\\nignore those lines in the output. The parameters Rho_gr[1,2] and Rho_gr[2,1] are actu-\\nally the same parameter, because the matrix is symmetric. The correlation between general\\ngiving and receiving is negative, with an 89% compatibility interval from about −0.7 to −0.1.\\nThis implies that individuals who give more across all dyads tend to receive less. The stan-\\ndard deviation parameters sigma_gr[1] and sigma_gr[2] show clear evidence that rates\\nof giving are more variable than rates of receiving.\\nLet’s plot these giving and receiving effects, so you can see this covariance structure in\\nthe parameters. We want to calculate, for each household, its posterior predictive giving and\\nreceiving rates, across all dyads. We can do this by using the linear model directly to add the\\nintercept a to each giving or receiving parameter:\\nR code\\n14.33\\npost <- extract.samples( m14.7 )\\ng <- sapply( 1:25 , function(i) post$a + post$gr[,i,1] )\\nr <- sapply( 1:25 , function(i) post$a + post$gr[,i,2] )\\nEg_mu <- apply( exp(g) , 2 , mean )\\nEr_mu <- apply( exp(r) , 2 , mean )\\nIf you look at str(g), you’ll see a matrix with 4000 rows (samples) and 25 columns (house-\\nholds). These are the posterior distributions of giving for each household. The matrix r is\\nthe same for receiving. Eg_mu and Er_mu holds the means on the outcome scale. That’s why\\nthey were exponentiated.\\nBefore plotting those points, I’d like to also show the uncertainty around each. How\\ncan we do that? There is uncertainty in both directions, because there is a distribution\\nwith some correlation structure here. We could just plot the columns in g and r. Try\\nplot(exp(g[,1]),exp(r[,1])) for example to show the posterior distribution of giv-\\ning/receiving for household number 1. That is messy, but it does show the uncertainty in\\neach household’s values.\\nWe can produce a cleaner visualization with some contours. On the latent scale of the\\nlinear model, the bivariate distribution of each g and r is approximately Gaussian. So we can\\ndescribe its shape with an ellipse. If we then project this ellipse onto the outcome scale, we’ll\\nhave a clean contour for the uncertainty.\\nR code\\n14.34\\nplot( NULL , xlim=c(0,8.6) , ylim=c(0,8.6) , xlab=\"generalized giving\" ,\\nylab=\"generalized receiving\" , lwd=1.5 )\\nabline(a=0,b=1,lty=2)\\n'},\n",
       " {'index': 484,\n",
       "  'number': 466,\n",
       "  'content': '466\\n14. ADVENTURES IN COVARIANCE\\n0\\n2\\n4\\n6\\n8\\n0\\n2\\n4\\n6\\n8\\ngeneralized giving\\ngeneralized receiving\\nFigure 14.9. Left: Expected giving and receiving, absent any dyad-specific\\neffects. Each point is a household and the ellipses show 50% compatibility\\nregions. There is a negative relationship between average giving and aver-\\nage receiving across households. Right: Dyad-specific effects, absent gener-\\nalized giving and receiving. After accounting for overall rates of giving and\\nreceiving, residual gifts are strongly correlated within dyads.\\n# ellipses\\nlibrary(ellipse)\\nfor ( i in 1:25 ) {\\nSigma <- cov( cbind( g[,i] , r[,i] ) )\\nMu <- c( mean(g[,i]) , mean(r[,i]) )\\nfor ( l in c(0.5) ) {\\nel <- ellipse( Sigma , centre=Mu , level=l )\\nlines( exp(el) , col=col.alpha(\"black\",0.5) )\\n}\\n}\\n# household means\\npoints( Eg_mu , Er_mu , pch=21 , bg=\"white\" , lwd=1.5 )\\nThe left side of Figure 14.9 shows the result. Note the negative relationship between giving\\non the horizontal and receiving on the vertical. The dashed line shows where the two rates\\nwould be equal. The households with the lowest rates of giving have some of the highest rates\\nof receiving. This likely reflects need-based gifts. Likewise the households with the highest\\nrates of giving have some of the lowest rates of receiving. That is the negative correlation we\\nsaw in the precis output. Note also the greater variation in giving rates. That corresponds\\nto the standard deviation parameters.\\nNow what about the dyad effects? Let’s look at that covariance matrix:\\n'},\n",
       " {'index': 485,\n",
       "  'number': 467,\n",
       "  'content': '14.5. CONTINUOUS CATEGORIES AND THE GAUSSIAN PROCESS\\n467\\nR code\\n14.35\\nprecis( m14.7 , depth=3 , pars=c(\"Rho_d\",\"sigma_d\") )\\nmean\\nsd 5.5% 94.5% n_eff Rhat\\nRho_d[1,1] 1.00 0.00 1.00\\n1.00\\nNaN\\nNaN\\nRho_d[1,2] 0.88 0.03 0.83\\n0.93\\n1287\\n1\\nRho_d[2,1] 0.88 0.03 0.83\\n0.93\\n1287\\n1\\nRho_d[2,2] 1.00 0.00 1.00\\n1.00\\nNaN\\nNaN\\nsigma_d\\n1.11 0.06 1.02\\n1.20\\n1583\\n1\\nThe correlation here is positive and strong. And there is more variation among dyads than\\nthere is amonghousehold in giving rates. This implies that pairs of households are balanced—\\nif one household gives less than average (after accounting for generalized giving and receiv-\\ning), then the other probably gives less as well. We can plot the raw dyad effects to see how\\nstrong this pattern is:\\nR code\\n14.36\\ndy1 <- apply( post$d[,,1] , 2 , mean )\\ndy2 <- apply( post$d[,,2] , 2 , mean )\\nplot( dy1 , dy2 )\\nThe result is the right-hand plot in Figure 14.9. These are only posterior means—there is a\\nlot of uncertainty about each dyad. But there is an astonishing amount of balance. This could\\nreflect reciprocity, adjusted for overall wealth levels. Or it could reflect types of relationships\\namong households, like kin obligations, that we haven’t included in the model.\\nThe full data set contains a number of covariates that can be used to explain these effects:\\neconomic activities, relationships, distances among households. A model like this one, with\\nonly varying effects, can partition the variation and show us where the action is. But our goal\\nis to gain some causal understanding through adding more information to the model.\\nRethinking: Where everybody knows your name. The gift example is a social network model. In\\nthat light, an important feature missing from this model is the transitivity of social relationships.\\nIf household A is friends with household B, and household C is friends with household B, then house-\\nholds A and C are more likely to be friends. This isn’t magic. It just arises from unobserved factors\\nthat create correlated relationships. For example, people who go to the same pub tend to know one\\nanother. The pub is an unmeasured confound for inferring causes of social relations. Models that can\\nestimate and expect transitivity can be better. This can be done using something called a stochastic\\nblock model. To fit such a model, however, we’ll need some techniques in the next chapter.\\n14.5. Continuous categories and the Gaussian process\\nAll of the varying effects so far, whether they were intercepts or slopes, have been de-\\nfined over discrete, unordered categories. For example, cafés are unique places, and there\\nis no sense in which café 1 comes before café 2. The “1” and “2” are just labels for unique\\nthings. The same goes for tadpole ponds, academic departments, or individual chimpanzees.\\nBy estimating unique parameters for each cluster of this kind, we can quantify some of the\\nunique features that generate variation across clusters and covariation among the observa-\\ntions within each cluster. Pooling across the clusters improves accuracy and simultaneously\\nprovides a picture of the variation.\\n'},\n",
       " {'index': 486,\n",
       "  'number': 468,\n",
       "  'content': '468\\n14. ADVENTURES IN COVARIANCE\\nBut what about continuous dimensions of variation like age or income or stature? Indi-\\nviduals of the same age share some of the same exposures. They listened to some of the same\\nmusic, heard about the same politicians, and experienced the same weather events. And\\nindividuals of similar ages also experienced some of these same exposures, but to a lesser\\nextent than individuals of the same age. The covariation falls off as any two individuals be-\\ncome increasingly dissimilar in age or income or stature or any other dimension that indexes\\nbackground similarity. It doesn’t make sense to estimate a unique varying intercept for all\\nindividuals of the same age, ignoring the fact that individuals of similar ages should have\\nmore similar intercepts. And of course, it’s likely that every individual in your sample has a\\nunique age. So then continuous differences in similarity are all you have to work with.\\nLuckily, there is a way to apply the varying effects approach to continuous categories of\\nthis kind. This will allow us to estimate a unique intercept (or slope) for any age, while still\\nregarding age as a continuous dimension in which similar ages have more similar intercepts\\n(or slopes). The general approach is known as Gaussian process regression.216 This\\nname is unfortunately wholly uninformative about what it is for and how it works.\\nWe’ll proceed to work through a basic example that demonstrates both what it is for and\\nhow it works. The general purpose is to define some dimension along which cases differ.\\nThis might be individual differences in age. Or it could be differences in location. Then we\\nmeasure the distance between each pair of cases. What the model then does is estimate a\\nfunction for the covariance between pairs of cases at different distances. This covariance\\nfunction provides one continuous category generalization of the varying effects approach.\\n14.5.1. Example: Spatial autocorrelation in Oceanic tools. When we looked at the com-\\nplexity of tool kits among historic Oceanic societies, back in Chapter 11 (page 346), we used\\na crude binary contact predictor as a proxy for possible exchange among societies. But that\\nvariable is pretty unsatisfying. First, it takes no note of which other societies each had contact\\n(or not) with. If all of your neighbors are small islands, then high rate of contact with them\\nmay not do much at all to tool complexity. Second, if indeed tools were exchanged among\\nsocieties—and we know they were—then the total number of tools for each are truly not inde-\\npendent of one another, even after we condition on all of the predictors. Instead we expect\\nclose geographic neighbors to have more similar tool counts, because of exchange. Third,\\ncloser islands may share unmeasured geographic features like sources of stone or shell that\\nlead to similar technological industries. So space could matter in multiple ways.\\nThis is a classic setting in which to use Gaussian process regression. We’ll define a dis-\\ntance matrix among the societies. Then we can estimate how similarity in tool counts de-\\npends upon geographic distance. You’ll see how to simultaneously incorporate ordinary\\npredictors, so that the covariation among societies with distance will both control for and be\\ncontrolled by other factors that influence technology.\\nLet’s begin by loading the data and inspecting the geographic distance matrix. I’ve al-\\nready gone ahead and looked up the as-the-crow-flies navigation distance between each pair\\nof societies. These distances are measured in thousands of kilometers, and the matrix of\\nthem is in the rethinking package:\\nR code\\n14.37\\n# load the distance matrix\\nlibrary(rethinking)\\ndata(islandsDistMatrix)\\n'},\n",
       " {'index': 487,\n",
       "  'number': 469,\n",
       "  'content': '14.5. CONTINUOUS CATEGORIES AND THE GAUSSIAN PROCESS\\n469\\n# display (measured in thousands of km)\\nDmat <- islandsDistMatrix\\ncolnames(Dmat) <- c(\"Ml\",\"Ti\",\"SC\",\"Ya\",\"Fi\",\"Tr\",\"Ch\",\"Mn\",\"To\",\"Ha\")\\nround(Dmat,1)\\nMl\\nTi\\nSC\\nYa\\nFi\\nTr\\nCh\\nMn\\nTo\\nHa\\nMalekula\\n0.0 0.5 0.6 4.4 1.2 2.0 3.2 2.8 1.9 5.7\\nTikopia\\n0.5 0.0 0.3 4.2 1.2 2.0 2.9 2.7 2.0 5.3\\nSanta Cruz 0.6 0.3 0.0 3.9 1.6 1.7 2.6 2.4 2.3 5.4\\nYap\\n4.4 4.2 3.9 0.0 5.4 2.5 1.6 1.6 6.1 7.2\\nLau Fiji\\n1.2 1.2 1.6 5.4 0.0 3.2 4.0 3.9 0.8 4.9\\nTrobriand\\n2.0 2.0 1.7 2.5 3.2 0.0 1.8 0.8 3.9 6.7\\nChuuk\\n3.2 2.9 2.6 1.6 4.0 1.8 0.0 1.2 4.8 5.8\\nManus\\n2.8 2.7 2.4 1.6 3.9 0.8 1.2 0.0 4.6 6.7\\nTonga\\n1.9 2.0 2.3 6.1 0.8 3.9 4.8 4.6 0.0 5.0\\nHawaii\\n5.7 5.3 5.4 7.2 4.9 6.7 5.8 6.7 5.0 0.0\\nNotice that the diagonal is all zeros, because each society is zero kilometers from itself. Also\\nnotice that the matrix is symmetric around the diagonal, because the distance between two\\nsocieties is the same whichever society we measure from.\\nWe’ll use these distances as a measure of similarity in technology exposure. This will\\nallow us to estimate varying intercepts for each society that account for non-independence\\nin tools as a function of their geographical similarly. The notion is that the expected number\\nof tools for each society gets a varying intercept, based on a continuous distance measure,\\nthat makes it correlated with the tool counts of its neighbors.\\nWe’ll use the “scientific” tool model from Chapter 11. In that model, the first part of the\\nmodel is a familiar Poisson probably of the outcome variable. Then there is a model-derived\\nexpected number of tools:\\nTi ∼Poisson(λi)\\nλi = αPβ\\ni /γ\\nWe’d like to have these λ values adjusted by a varying intercept parameter. We could just\\nadd the intercept to the expression above, but then λi might end up negative. So instead let’s\\nmake the varying intercepts multiplicative:\\nTi ∼Poisson(λi)\\nλi = exp(ksociety[i])αPβ\\ni /γ\\nwhere ksociety[i] is the varying intercept. But unlike typical varying intercepts, it will be esti-\\nmated in light of geographic distance, not distinct category membership.\\nThe heart of the Gaussian process is the multivariate prior for these intercepts:\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\nk1\\nk2\\nk3\\n. . .\\nk10\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n∼MVNormal\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n\\uf8eb\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ec\\n\\uf8ed\\n0\\n0\\n0\\n. . .\\n0\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n, K\\n\\uf8f6\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f7\\n\\uf8f8\\n[prior for intercepts]\\nKij = η2 exp(−ρ2D2\\nij) + δijσ2\\n[deﬁne covariance matrix]\\n'},\n",
       " {'index': 488,\n",
       "  'number': 470,\n",
       "  'content': '470\\n14. ADVENTURES IN COVARIANCE\\nThe first line is the 10-dimensional Gaussian prior for the intercepts. It has 10 dimensions,\\nbecause there are 10 societies in the distance matrix. The vector of means is all zeros, which\\nmeans that inside the linear model the average society will multiply λ by exp(0) = 1. So\\nthe average doesn’t change the expectation. Negative k values will reduce λ, and positive k\\nvalues will increase it.\\nThe covariance matrix for these intercepts is named K, and the covariance between any\\npair of societies i and j is Kij. This covariance is defined by the formula on the second line\\nabove. This formula uses three parameters—η, ρ, and σ—to model how covariance among\\nsocieties changes with distances among them. It probably looks very unfamiliar. I’ll walk\\nyou through it in pieces.\\nThe part of the formula for K that gives the covariance model its shape is exp(−ρ2D2\\nij).\\nDij is the distance between the i-th and j-th societies. So what this function says is that the\\ncovariance between any two societies i and j declines exponentially with the squared distance\\nbetween them. The parameter ρ determines the rate of decline. If it is large, then covariance\\ndeclines rapidly with squared distance.\\nWhy square the distance? You don’t have to. This is just a model. But the squared\\ndistance is the most common assumption, both because it is easy to fit to data and has the\\noften-realistic property of allowing covariance to decline more quickly as distance grows.\\nThis will be easy to appreciate, if we plot this function under the linear-decline alternative,\\nexp(−ρ2Dij), and compare. We’ll use a value ρ2 = 1, just for the example.\\nR code\\n14.38\\n# linear\\ncurve( exp(-1*x) , from=0 , to=4 , lty=2 )\\n# squared\\ncurve( exp(-1*x^2) , add=TRUE )\\nThe result is shown in Figure 14.10. The vertical axis here is just part of the total covariance\\nfunction. You can think of it as the proportion of the maximum correlation between two\\nsocieties i and j. The dashed curve is the linear distance function. It produces an exact expo-\\nnential shape. The solid curve is the squared distance function. It produces a half-Gaussian\\ndecline that is initially slower than the exponential but rapidly accelerates and then becomes\\nfaster than exponential.\\nThe last two pieces of Kij are simpler. η2 is the maximum covariance between any two\\nsocieties i and j. The term on the end, δijσ2, provides for extra covariance beyond η2 when\\ni = j. It does this because the function δij is equal to 1 when i = j but is zero otherwise. In\\nthe Oceanic societies data, this term will not matter, because we only have one observation\\nfor each society. But if we had more than one observation per society, σ here describes how\\nthese observations covary.\\nThe model computes the posterior distribution of ρ, η, and σ. But it also needs priors\\nfor them. We’ll define priors for the square of each, and estimate them on the same scale,\\nbecause that’s computationally easier. We don’t need σ in this model, so we’ll instead just fix\\nit at an irrelevant constant.\\nTo finish the model, we need priors for the covariance function:\\nη2 ∼Exponential(2)\\nρ2 ∼Exponential(0.5)\\n'},\n",
       " {'index': 489,\n",
       "  'number': 471,\n",
       "  'content': '14.5. CONTINUOUS CATEGORIES AND THE GAUSSIAN PROCESS\\n471\\n0\\n1\\n2\\n3\\n4\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\ndistance\\ncorrelation\\nFigure 14.10. Shape of the function relating\\ndistance to the covariance Kij. The horizontal\\naxis is distance. The vertical is the correlation,\\nrelative to maximum, between any two soci-\\neties i and j. The dashed curve is the linear dis-\\ntance function. The solid curve is the squared\\ndistance function.\\nNote that ρ2 and η2 must be positive, so we place exponential priors on them. A little knowl-\\nedge of Pacific navigation would probably allow us a smart, informative prior on ρ2 at least.\\nWe will inspect the prior predictive simulations in a moment.\\nWe’re finally ready to fit the model. The distribution to use, to signal to ulam that you\\nwant the squared distance Gaussian process prior, is GPL2. The rest should be familiar.\\nR code\\n14.39\\ndata(Kline2) # load the ordinary data, now with coordinates\\nd <- Kline2\\nd$society <- 1:10 # index observations\\ndat_list <- list(\\nT = d$total_tools,\\nP = d$population,\\nsociety = d$society,\\nDmat=islandsDistMatrix )\\nm14.8 <- ulam(\\nalist(\\nT ~ dpois(lambda),\\nlambda <- (a*P^b/g)*exp(k[society]),\\nvector[10]:k ~ multi_normal( 0 , SIGMA ),\\nmatrix[10,10]:SIGMA <- cov_GPL2( Dmat , etasq , rhosq , 0.01 ),\\nc(a,b,g) ~ dexp( 1 ),\\netasq ~ dexp( 2 ),\\nrhosq ~ dexp( 0.5 )\\n), data=dat_list , chains=4 , cores=4 , iter=2000 )\\nBe sure to check the chains. They should sample well, but we could also improve sampling\\nby de-centering the prior for k. We’ll do that in a box at the end of this section. Let’s check\\nthe posterior:\\n'},\n",
       " {'index': 490,\n",
       "  'number': 472,\n",
       "  'content': '472\\n14. ADVENTURES IN COVARIANCE\\nR code\\n14.40\\nprecis( m14.8 , depth=3 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nk[1]\\n-0.17 0.30 -0.65\\n0.29\\n714 1.00\\nk[2]\\n-0.03 0.29 -0.48\\n0.43\\n538 1.01\\nk[3]\\n-0.08 0.28 -0.51\\n0.35\\n527 1.01\\nk[4]\\n0.34 0.26 -0.04\\n0.74\\n593 1.01\\nk[5]\\n0.07 0.25 -0.32\\n0.46\\n590 1.01\\nk[6]\\n-0.39 0.27 -0.84\\n0.00\\n789 1.00\\nk[7]\\n0.13 0.25 -0.26\\n0.53\\n606 1.01\\nk[8]\\n-0.22 0.26 -0.64\\n0.16\\n726 1.01\\nk[9]\\n0.26 0.25 -0.11\\n0.64\\n668 1.01\\nk[10] -0.18 0.35 -0.75\\n0.35\\n868 1.01\\ng\\n0.60 0.56\\n0.08\\n1.68\\n1536 1.00\\nb\\n0.28 0.08\\n0.15\\n0.41\\n1107 1.00\\na\\n1.41 1.08\\n0.24\\n3.39\\n1811 1.00\\netasq\\n0.20 0.20\\n0.03\\n0.56\\n863 1.00\\nrhosq\\n1.31 1.60\\n0.08\\n4.41\\n1931 1.00\\nFirst, note that the coefficient for log population, bp, is very much as it was before we added\\nall this Gaussian process stuff. This suggests that it’s hard to explain all of the association\\nbetween tool counts and population as a side effect of geographic contact. Second, those g\\nparameters are the Gaussian process varying intercepts for each society. Like a and bp, they\\nare on the log-count scale, so they are hard to interpret raw.\\nIn order to understand the parameters that describe the covariance with distance, rhosq\\nand etasq, we’ll want to plot the function they imply. Actually the joint posterior distribu-\\ntion of these two parameters defines a posterior distribution of covariance functions. We can\\nget a sense of this distribution of functions—I know, this is rather meta—by plotting a bunch\\nof them. Here we’ll sample 50 from the posterior and display them along with the posterior\\nmean. But as always, it is the entire distribution that matters. Be careful: The uncertainty of\\nthe function is not the same as the uncertainty of the mean function.\\nR code\\n14.41\\npost <- extract.samples(m14.8)\\n# plot the posterior median covariance function\\nplot( NULL , xlab=\"distance (thousand km)\" , ylab=\"covariance\" ,\\nxlim=c(0,10) , ylim=c(0,2) )\\n# compute posterior mean covariance\\nx_seq <- seq( from=0 , to=10 , length.out=100 )\\npmcov <- sapply( x_seq , function(x) post$etasq*exp(-post$rhosq*x^2) )\\npmcov_mu <- apply( pmcov , 2 , mean )\\nlines( x_seq , pmcov_mu , lwd=2 )\\n# plot 50 functions sampled from posterior\\nfor ( i in 1:50 )\\ncurve( post$etasq[i]*exp(-post$rhosq[i]*x^2) , add=TRUE ,\\ncol=col.alpha(\"black\",0.3) )\\n'},\n",
       " {'index': 491,\n",
       "  'number': 473,\n",
       "  'content': '14.5. CONTINUOUS CATEGORIES AND THE GAUSSIAN PROCESS\\n473\\n0\\n2\\n4\\n6\\n8\\n10\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\ndistance (thousand km)\\ncovariance\\nGaussian process prior\\n0\\n2\\n4\\n6\\n8\\n10\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\ndistance (thousand km)\\ncovariance\\nGaussian process posterior\\nFigure 14.11. Left: Prior distribution of spatial covariance functions. Each\\ncurve shows a joint sample from the prior of ρ2 and η2. Right: Posterior dis-\\ntribution of the spatial covariance. The dark curve displays the posterior\\nmean covariance at each distance. The thin curves show 50 functions sam-\\npled from the joint posterior distribution of ρ2 and η2.\\nFigure 14.11 shows the result. Each combination of values for ρ2 and η2 produces a relation-\\nship between covariance and distance. The posterior median function, shown by the thick\\ncurve, represents a center of plausibility. But the other curves show that there’s a lot of un-\\ncertainty about the spatial covariance. Curves that peak at twice the posterior median peak,\\naround 0.2, are commonplace. And curves that peak at half the median are very common,\\nas well. There’s a lot of uncertainty about how strong the spatial effect is, but the majority of\\nposterior curves decline to zero covariance before 4000 kilometers.\\nIt’s hard to interpret these covariances directly, because they are on the log-count scale,\\njust like everything else in a Poisson GLM. So let’s consider the correlations among societies\\nthat are implied by the posterior median. First, we push the parameters back through the\\nfunction for K, the covariance matrix:\\nR code\\n14.42\\n# compute posterior median covariance among societies\\nK <- matrix(0,nrow=10,ncol=10)\\nfor ( i in 1:10 )\\nfor ( j in 1:10 )\\nK[i,j] <- median(post$etasq) *\\nexp( -median(post$rhosq) * islandsDistMatrix[i,j]^2 )\\ndiag(K) <- median(post$etasq) + 0.01\\nSecond, we convert K to a correlation matrix:\\nR code\\n14.43\\n# convert to correlation matrix\\nRho <- round( cov2cor(K) , 2 )\\n# add row/col names for convenience\\n'},\n",
       " {'index': 492,\n",
       "  'number': 474,\n",
       "  'content': '474\\n14. ADVENTURES IN COVARIANCE\\ncolnames(Rho) <- c(\"Ml\",\"Ti\",\"SC\",\"Ya\",\"Fi\",\"Tr\",\"Ch\",\"Mn\",\"To\",\"Ha\")\\nrownames(Rho) <- colnames(Rho)\\nRho\\nMl\\nTi\\nSC\\nYa\\nFi\\nTr\\nCh\\nMn\\nTo Ha\\nMl 1.00 0.79 0.70 0.00 0.31 0.05 0.00 0.00 0.08\\n0\\nTi 0.79 1.00 0.87 0.00 0.31 0.05 0.00 0.01 0.06\\n0\\nSC 0.70 0.87 1.00 0.00 0.17 0.11 0.01 0.02 0.02\\n0\\nYa 0.00 0.00 0.00 1.00 0.00 0.01 0.16 0.14 0.00\\n0\\nFi 0.31 0.31 0.17 0.00 1.00 0.00 0.00 0.00 0.61\\n0\\nTr 0.05 0.05 0.11 0.01 0.00 1.00 0.09 0.56 0.00\\n0\\nCh 0.00 0.00 0.01 0.16 0.00 0.09 1.00 0.32 0.00\\n0\\nMn 0.00 0.01 0.02 0.14 0.00 0.56 0.32 1.00 0.00\\n0\\nTo 0.08 0.06 0.02 0.00 0.61 0.00 0.00 0.00 1.00\\n0\\nHa 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\\n1\\nThe cluster of small societies in the upper-left of the matrix—Malekula (Ml), Tikopia (Ti),\\nand Santa Cruz (SC)—are highly correlated, all above 0.8 with one another. As you’ll see in a\\nmoment, these societies are very close together, and they also have similar tool totals. These\\ncorrelations were estimating with log population in the model, remember, and so suggest\\nsome additional resemblance even accounting for the average association between popula-\\ntion and tools. On the other end of spectrum is Hawaii (Ha), which is so far from all of the\\nother societies that the correlation decays to zero everyplace. Other societies display a range\\nof correlations.\\nTo make some sense of the variation in these correlations, let’s plot them on a crude map\\nof the Pacific Ocean. The Kline2 data frame provides latitude and longitude for each society,\\nto make this easy. I’ll also scale the size of each society on the map in proportion to its log\\npopulation.\\nR code\\n14.44\\n# scale point size to logpop\\npsize <- d$logpop / max(d$logpop)\\npsize <- exp(psize*1.5)-2\\n# plot raw data and labels\\nplot( d$lon2 , d$lat , xlab=\"longitude\" , ylab=\"latitude\" ,\\ncol=rangi2 , cex=psize , pch=16 , xlim=c(-50,30) )\\nlabels <- as.character(d$culture)\\ntext( d$lon2 , d$lat , labels=labels , cex=0.7 , pos=c(2,4,3,3,4,1,3,2,4,2) )\\n# overlay lines shaded by Rho\\nfor( i in 1:10 )\\nfor ( j in 1:10 )\\nif ( i < j )\\nlines( c( d$lon2[i],d$lon2[j] ) , c( d$lat[i],d$lat[j] ) ,\\nlwd=2 , col=col.alpha(\"black\",Rho[i,j]^2) )\\nThe result appears on the left side of Figure 14.12. Darker lines indicate stronger correla-\\ntions, with pure white being zero correlation and pure black 100% correlation. The cluster of\\nthree close societies—Malekula, Tikopia, and Santa Cruz—stand out. Close societies have\\n'},\n",
       " {'index': 493,\n",
       "  'number': 475,\n",
       "  'content': '14.5. CONTINUOUS CATEGORIES AND THE GAUSSIAN PROCESS\\n475\\n-40\\n-20\\n0\\n20\\n-20\\n-10\\n0\\n10\\n20\\nlongitude\\nlatitude\\nMalekula\\nTikopia\\nSanta Cruz\\nYap\\nLau Fiji\\nTrobriand\\nChuuk\\nManus\\nTonga\\nHawaii\\nSa\\nand\\nn\\nS\\nT\\nz\\nnt\\nd\\na\\nSan\\nuz\\nnt\\nnt\\nnta\\nta\\nSan\\nSan\\nCr\\nnta\\nnta\\nta \\nta\\nT\\nt\\nii\\nnd\\nC\\nT\\nC\\na\\nSa\\nia\\nnd\\nand\\nCr\\nTik\\nL\\nC\\nSa\\nSa\\nSa\\nu\\n7\\n8\\n9\\n10\\n11\\n12\\n20\\n30\\n40\\n50\\n60\\n70\\nlog population\\ntotal tools\\nMalekula\\nTikopia\\nSanta Cruz\\nYap\\nLau Fiji\\nTrobriand\\nChuuk\\nManus\\nTonga\\nHawaii\\nLa\\nia\\nMa\\nFij\\nta\\nant\\nM\\nF\\nu \\nCh\\nL\\npi\\nL\\na\\nFij\\npiaia\\nS\\nLau\\npia\\np a\\npia\\nC\\nFij\\nFij\\np\\n \\nu \\nji\\nSa\\nSa\\niji\\nhuu\\ni\\nnt\\ni\\nTong\\na\\nhu\\nChu\\na Cr\\nC\\n Cru\\nM\\na\\nh\\nT\\nFigure 14.12. Left: Posterior correlations among societies in geographic\\nspace. Right: Same posterior correlations, now shown against relationship\\nbetween total tools and log population.\\nstronger correlations. But since we can’t see total tools on this map, it’s hard to see what the\\nconsequence of these correlations is supposed to be.\\nMore sense can be made of these correlations, if we also compare against the simultane-\\nous relationship between tools and log population. Here’s a plot that combines the average\\nposterior predictive relationship between log population and total tools with the shaded cor-\\nrelation lines for each pair of societies:\\nR code\\n14.45\\n# compute posterior median relationship, ignoring distance\\nlogpop.seq <- seq( from=6 , to=14 , length.out=30 )\\nlambda <- sapply( logpop.seq , function(lp) exp( post$a + post$bp*lp ) )\\nlambda.median <- apply( lambda , 2 , median )\\nlambda.PI80 <- apply( lambda , 2 , PI , prob=0.8 )\\n# plot raw data and labels\\nplot( d$logpop , d$total_tools , col=rangi2 , cex=psize , pch=16 ,\\nxlab=\"log population\" , ylab=\"total tools\" )\\ntext( d$logpop , d$total_tools , labels=labels , cex=0.7 ,\\npos=c(4,3,4,2,2,1,4,4,4,2) )\\n# display posterior predictions\\nlines( logpop.seq , lambda.median , lty=2 )\\nlines( logpop.seq , lambda.PI80[1,] , lty=2 )\\nlines( logpop.seq , lambda.PI80[2,] , lty=2 )\\n# overlay correlations\\nfor( i in 1:10 )\\nfor ( j in 1:10 )\\nif ( i < j )\\n'},\n",
       " {'index': 494,\n",
       "  'number': 476,\n",
       "  'content': '476\\n14. ADVENTURES IN COVARIANCE\\nlines( c( d$logpop[i],d$logpop[j] ) ,\\nc( d$total_tools[i],d$total_tools[j] ) ,\\nlwd=2 , col=col.alpha(\"black\",Rho[i,j]^2) )\\nThis plot appears in the right-hand side of Figure 14.12. Now it’s easier to appreciate that\\nthe correlations among Malekula, Tikopia, and Santa Cruz describe the fact that they are\\nbelow the expected number of tools for their populations. All three societies lying below\\nthe expectation, and being so close, is consistent with spatial covariance. The posterior cor-\\nrelations merely describe this feature of the data. Similarly, Manus and the Trobriands are\\ngeographically close, have a substantial posterior correlation, and fewer tools than expected\\nfor their population sizes. Tonga has more tools than expected for its population, and its\\nproximity to Fiji counteracts some of the tug Fiji’s smaller neighbors—Malekula, Tikopia,\\nand Santa Cruz—exert on it. So the model seems to think Fiji would have fewer tools, if it\\nweren’t for Tonga.\\nOf course the correlations that this model describes by geographic distance may be the\\nresult of other, unmeasured commonalities between geographically close societies. For ex-\\nample, Manus and the Trobriands are geologically and ecologically quite different from Fiji\\nand Tonga. So it could be availability of, for example, tool stone that explains some of the\\ncorrelations. The Gaussian process regression is a grand and powerful descriptive model. As\\na result, its output is always compatible with many different causal explanations.\\nRethinking: Dispersion by other names. The model in this section uses a Poisson likelihood, which\\nis often sensitive to outliers, like the Hawaii data. You could use a gamma-Poisson likelihood instead,\\nas explained in Chapter 12. But note that the varying effects in this example already induce additional\\ndispersion around the Poisson mean. Adding Gaussian noise to each Poisson observation is another\\ntraditional way to handle over-dispersion in Poisson models. But do try the model with gamma-\\nPoisson as well, so you can compare.\\nOverthinking: Non-centered islands. To build a non-centered Gaussian process, we can use the\\nsame general trick of converting the covariance matrix to a Cholesky factor and then multiplying\\nthat factor by the z-scores of each varying effect. The covariance matrix is defined the same way. We\\njust end up with some intermediate steps. Here is the Oceanic societies Gaussian process model in\\nnon-centered form:\\nR code\\n14.46\\nm14.8nc <- ulam(\\nalist(\\nT ~ dpois(lambda),\\nlambda <- (a*P^b/g)*exp(k[society]),\\n# non-centered Gaussian Process prior\\ntranspars> vector[10]: k <<- L_SIGMA * z,\\nvector[10]: z ~ normal( 0 , 1 ),\\ntranspars> matrix[10,10]: L_SIGMA <<- cholesky_decompose( SIGMA ),\\ntranspars> matrix[10,10]: SIGMA <- cov_GPL2( Dmat , etasq , rhosq , 0.01 ),\\nc(a,b,g) ~ dexp( 1 ),\\netasq ~ dexp( 2 ),\\nrhosq ~ dexp( 0.5 )\\n'},\n",
       " {'index': 495,\n",
       "  'number': 477,\n",
       "  'content': '14.5. CONTINUOUS CATEGORIES AND THE GAUSSIAN PROCESS\\n477\\n), data=dat_list , chains=4 , cores=4 , iter=2000 )\\nThe new element above is the Stan function cholesky_decompose, which takes covariance (or corre-\\nlation) matrix and returns its Cholesky factor. That Cholesky factor can then be mixed with z-scores\\nas before to produce varying effects on the right scale. If you check the posterior, you’ll see this ver-\\nsion samples more efficiently. As always, the cost is that the model is harder to read. With a very large\\nSIGMA matrix, often there is no choice but to use the Cholesky (non-centered) parameterization. The\\nnext example, for example, is like this.\\n14.5.2. Example: Phylogenetic distance. Species, like islands, are more or less distance\\nfrom one another. However their distance is not physical but rather temporal—how long\\nsince a common ancestor? Evolutionary biologists investigate how phylogenetic relation-\\nships influence patterns of variation in the bodies and brains of different species. It’s a fact\\nthat species with more recent common ancestors have higher trait correlations. Do these\\ncorrelations matter?\\nPhylogenetic distance can have two important causal influences. The first is that two\\nspecies that only recently separated tend to be more similar, assuming their traits are not\\nmaintained by selection but rather drifting neutrally around. The second causal influence\\nis indirect. Phylogenetic distance is a proxy for unobserved variables that generate covaria-\\ntion among species, even when selection matters. Closely related species likely share more\\nof these, but distantly related species share many fewer. For example, all mammals nurse\\ntheir young with milk. Flight in birds similarly influences many traits. These discrete, life\\nhistory altering traits can have strong causal influence on other traits. When not observed,\\nphylogenetic distance is a potentially useful proxy for these variables. But only if the trait\\nmodel captures the right details.217 These methods do not just work automatically, as they\\nare too often ritually presented in journals.\\nConsider as an example the causal influence of group size (G) on brain size (B). Hypothe-\\nses connecting these variables are popular, because primates (including humans) are unusual\\nin both. Most primates live in social groups. Most mammals do not. Second, primates have\\nrelatively large brains. There is a family of hypotheses linking these two features. Suppose\\nfor example that group living, whatever its cause, could select for larger brains, because once\\nyou live with others, a larger brain helps to cope with the complexity of cooperation and\\nmanipulation. This hypothesis implies a causal time series. Let’s draw it:\\nB1\\nB2\\nG1\\nG2\\nU1\\nU2\\nThe subscripts are time points in the evolutionary history of different populations. So G1 is\\ngroup size at time 1 and G2 is group size in the next time point. There are plausibly many\\npotential confounds, shown here as U1 and U2. Each variable influences itself in the next time\\n'},\n",
       " {'index': 496,\n",
       "  'number': 478,\n",
       "  'content': '478\\n14. ADVENTURES IN COVARIANCE\\nstep, as you might expect in an evolving system. There is also a causal influence of G1 on B2—\\na species’ recent group size influenced its current brain size. This is what we’d like to estimate.\\nHowever the confounds U1 also possibly influence everything. As in previous examples,\\ncircled variables are unobserved. So we can’t just condition on U1 to block confounding. We\\nalso don’t even have G1 to use in a model, but only its descendant G2. But note that if we did\\nhave measurements of G1 and U1, we could use these and not worry at all about phylogeny.\\nSince we haven’t observed the past, we need some way to estimate its influence. This\\nis where the branching history of the species might help. Phylogeny is associated with the\\npatterns of covariation across species, because recently diverged species tend to be more sim-\\nilar. So phylogenetic relationships, expressed as distance, can be used to partially reconstruct\\nconfounds. This depends upon having both a good phylogeny and a good model of the rela-\\ntionship between phylogenetic distance and trait evolution. Neither is a trivial problem. But\\nthe approach is justified in theory, if not always possible in practice.\\nIt will help to draw this approach and then use it in an actual model.\\nB\\nG\\nM\\nP\\nU\\nThere’s a lot going on here, but we can take it one piece at a time. Again, we’re interested in\\nG →B. There is one confound we know for sure, body mass (M). It possibly influences both\\nG and B. So we’ll include that in the model. The unobserved confounds U could potentially\\ninfluence all three variables. Finally, we let the phylogenetic relationships (P) influence U.\\nHow is P causal? If we traveled back in time and delayed a split between two species, it could\\ninfluence the expected differences in their traits. So it is really the timing of the split that is\\ncausal, not the phylogeny. Of course P may also influence G and B and M directly. But those\\narrows aren’t our concern right now, so I’ve omitted them for clarity.\\nWe want to be sure any association between group size G and brain size B is not through a\\nbackdoor. As always, we look for all the paths between G and B, identify which are backdoors,\\nand consider if there are any methods for closing the backdoor paths. In the DAG above,\\nthere are backdoor paths through M and through U. We can condition on M to block that\\nconfound. But we can’t condition on U. But if we can use P to somehow reconstruct the\\ncovariation that U induces between G and B, that could be enough.\\nThat’s the strategy. Now implementing that strategy is famously hard. GLMs that try to\\ninclude phylogenetic distance often go by the name phylogenetic regression. The orig-\\ninal phylogenetic regression approach treats phylogenetic distance in a highly constrained\\nand unrealistic way, based on a neutral model of divergence with time.218 There are many\\nvariants. But all of them use some function of phylogenetic distance to model the covari-\\nation among species. So learning the basic phylogenetic regression model helps bootstrap\\nyour understanding, even though you really should use something better in your own anal-\\nyses. After introducing the basic phylogenetic regression, I’ll show you how to more flexibly\\nmodel phylogenetic distance. There is no universally correct function that maps phylogeny\\nonto the confounds that matter. So flexibility is needed.\\nTo begin, load the primates data and its phylogeny as well:\\n'},\n",
       " {'index': 497,\n",
       "  'number': 479,\n",
       "  'content': '14.5. CONTINUOUS CATEGORIES AND THE GAUSSIAN PROCESS\\n479\\nAllenopithecus nigroviridis\\nCercopithecus albogularis\\nCercopithecus ascanius\\nCercopithecus campbelli\\nCercopithecus campbelli lowei\\nCercopithecus cephus\\nCercopithecus cephus cephus\\nCercopithecus cephus ngottoensis\\nCercopithecus diana\\nCercopithecus erythrogaster\\nCercopithecus erythrogaster erythrogaster\\nCercopithecus erythrotis\\nCercopithecus hamlyni\\nCercopithecus lhoesti\\nCercopithecus mitis\\nCercopithecus mona\\nCercopithecus neglectus\\nCercopithecus nictitans\\nCercopithecus petaurista\\nCercopithecus pogonias\\nCercopithecus preussi\\nCercopithecus solatus\\nCercopithecus wolfi\\nChlorocebus aethiops\\nChlorocebus pygerythrus\\nChlorocebus pygerythrus cynosurus\\nChlorocebus sabaeus\\nChlorocebus tantalus\\nErythrocebus patas\\nMiopithecus talapoin\\nAllocebus trichotis\\nArchaeolemur majori\\nAvahi cleesei\\nAvahi laniger\\nAvahi occidentalis\\nAvahi unicolor\\nCheirogaleus crossleyi\\nCheirogaleus major\\nCheirogaleus medius\\nDaubentonia madagascariensis\\nEulemur coronatus\\nEulemur fulvus albifrons\\nEulemur fulvus albocollaris\\nEulemur fulvus collaris\\nEulemur fulvus fulvus\\nEulemur fulvus mayottensis\\nEulemur fulvus rufus\\nEulemur fulvus sanfordi\\nEulemur macaco flavifrons\\nEulemur macaco macaco\\nEulemur mongoz\\nEulemur rubriventer\\nHapalemur aureus\\nHapalemur griseus\\nHapalemur griseus alaotrensis\\nHapalemur griseus griseus\\nHapalemur griseus meridionalis\\nHapalemur griseus occidentalis\\nHapalemur simus\\nIndri indri\\nLemur catta\\nLepilemur aeeclis\\nLepilemur ankaranensis\\nLepilemur dorsalis\\nLepilemur edwardsi\\nLepilemur hubbardorum\\nLepilemur leucopus\\nLepilemur manasamody\\nLepilemur microdon\\nLepilemur mitsinjoensis\\nLepilemur mustelinus\\nLepilemur otto\\nLepilemur randrianasoli\\nLepilemur ruficaudatus\\nLepilemur sahamalazensis\\nLepilemur seali\\nLepilemur septentrionalis\\nMicrocebus berthae\\nMicrocebus bongolavensis\\nMicrocebus danfossi\\nMicrocebus griseorufus\\nMicrocebus jollyae\\nMicrocebus lehilahytsara\\nMicrocebus lokobensis\\nMicrocebus macarthurii\\nMicrocebus mamiratra\\nMicrocebus mittermeieri\\nMicrocebus murinus\\nMicrocebus myoxinus\\nMicrocebus ravelobensis\\nMicrocebus rufus\\nMicrocebus sambiranensis\\nMicrocebus simmonsi\\nMicrocebus tavaratra\\nMirza coquereli\\nMirza zaza\\nPhaner furcifer\\nPhaner furcifer pallescens\\nPropithecus coquereli\\nPropithecus deckenii\\nPropithecus diadema\\nPropithecus edwardsi\\nPropithecus tattersalli\\nPropithecus verreauxi\\nVarecia rubra\\nVarecia variegata variegata\\nAlouatta belzebul\\nAlouatta caraya\\nAlouatta guariba\\nAlouatta palliata\\nAlouatta pigra\\nAlouatta sara\\nAlouatta seniculus\\nAteles belzebuth\\nAteles fusciceps\\nAteles geoffroyi\\nAteles paniscus\\nBrachyteles arachnoides\\nLagothrix lagotricha\\nAotus azarai\\nAotus azarai boliviensis\\nAotus brumbacki\\nAotus infulatus\\nAotus lemurinus\\nAotus lemurinus griseimembra\\nAotus nancymaae\\nAotus nigriceps\\nAotus trivirgatus\\nAotus vociferans\\nCallimico goeldii\\nCallithrix argentata\\nCallithrix aurita\\nCallithrix emiliae\\nCallithrix geoffroyi\\nCallithrix humeralifera\\nCallithrix jacchus\\nCallithrix kuhli\\nCallithrix mauesi\\nCallithrix penicillata\\nCallithrix pygmaea\\nCebus albifrons\\nCebus apella\\nCebus capucinus\\nCebus olivaceus\\nCebus xanthosternos\\nLeontopithecus chrysomelas\\nLeontopithecus chrysopygus\\nLeontopithecus rosalia\\nSaguinus bicolor\\nSaguinus fuscicollis\\nSaguinus fuscicollis melanoleucus\\nSaguinus geoffroyi\\nSaguinus imperator\\nSaguinus leucopus\\nSaguinus midas\\nSaguinus mystax\\nSaguinus niger\\nSaguinus oedipus\\nSaguinus tripartitus\\nSaimiri boliviensis\\nSaimiri oerstedii\\nSaimiri sciureus\\nSaimiri ustus\\nArctocebus aureus\\nArctocebus calabarensis\\nLoris lydekkerianus\\nLoris tardigradus\\nNycticebus bengalensis\\nNycticebus coucang\\nNycticebus javanicus\\nNycticebus menagensis\\nNycticebus pygmaeus\\nPerodicticus potto\\nBunopithecus hoolock\\nGorilla beringei\\nGorilla gorilla gorilla\\nGorilla gorilla graueri\\nHomo sapiens\\nHomo sapiens neanderthalensis\\nHylobates agilis\\nHylobates klossii\\nHylobates lar\\nHylobates moloch\\nHylobates muelleri\\nHylobates pileatus\\nNomascus concolor\\nNomascus gabriellae\\nNomascus leucogenys\\nNomascus nasutus\\nNomascus siki\\nPan paniscus\\nPan troglodytes schweinfurthii\\nPan troglodytes troglodytes\\nPan troglodytes vellerosus\\nPan troglodytes verus\\nPongo abelii\\nPongo pygmaeus\\nSymphalangus syndactylus\\nCacajao calvus\\nCacajao melanocephalus\\nCallicebus donacophilus\\nCallicebus hoffmannsi\\nCallicebus moloch\\nCallicebus personatus\\nCallicebus torquatus\\nChiropotes satanas\\nPithecia irrorata\\nPithecia pithecia\\nCercocebus agilis\\nCercocebus galeritus\\nCercocebus torquatus\\nCercocebus torquatus atys\\nLophocebus albigena\\nLophocebus aterrimus\\nMacaca arctoides\\nMacaca assamensis\\nMacaca brunnescens\\nMacaca cyclopis\\nMacaca fascicularis\\nMacaca fuscata\\nMacaca hecki\\nMacaca leonina\\nMacaca maura\\nMacaca mulatta\\nMacaca munzala\\nMacaca nemestrina\\nMacaca nemestrina leonina\\nMacaca nemestrina siberu\\nMacaca nigra\\nMacaca nigrescens\\nMacaca ochreata\\nMacaca pagensis\\nMacaca radiata\\nMacaca silenus\\nMacaca sinica\\nMacaca sylvanus\\nMacaca thibetana\\nMacaca tonkeana\\nMandrillus leucophaeus\\nMandrillus sphinx\\nPapio anubis\\nPapio cynocephalus\\nPapio hamadryas\\nPapio papio\\nPapio ursinus\\nRungwecebus kipunji\\nTheropithecus gelada\\nColobus angolensis\\nColobus angolensis palliatus\\nColobus guereza\\nColobus polykomos\\nColobus satanas\\nColobus vellerosus\\nNasalis larvatus\\nPiliocolobus badius\\nPiliocolobus foai\\nPiliocolobus gordonorum\\nPiliocolobus kirkii\\nPiliocolobus pennantii\\nPiliocolobus preussi\\nPiliocolobus rufomitratus\\nPiliocolobus tephrosceles\\nPiliocolobus tholloni\\nPresbytis comata\\nPresbytis melalophos\\nProcolobus verus\\nPygathrix cinerea\\nPygathrix nemaeus\\nRhinopithecus avunculus\\nRhinopithecus bieti\\nRhinopithecus brelichi\\nRhinopithecus roxellana\\nSemnopithecus entellus\\nTrachypithecus auratus\\nTrachypithecus cristatus\\nTrachypithecus delacouri\\nTrachypithecus francoisi\\nTrachypithecus geei\\nTrachypithecus germaini\\nTrachypithecus johnii\\nTrachypithecus laotum\\nTrachypithecus obscurus\\nTrachypithecus phayrei\\nTrachypithecus pileatus\\nTrachypithecus poliocephalus\\nTrachypithecus vetulus\\nEuoticus elegantulus\\nGalago alleni\\nGalago gallarum\\nGalago granti\\nGalago matschiei\\nGalago moholi\\nGalago senegalensis\\nGalagoides demidoff\\nGalagoides zanzibaricus\\nOtolemur crassicaudatus\\nOtolemur garnettii\\nTarsius bancanus\\nTarsius dentatus\\nTarsius lariang\\nTarsius syrichta\\nFigure 14.13. Consensus phylogeny for 301 primate species. See the cita-\\ntions in ?Primates301 for sources.\\nR code\\n14.47\\nlibrary(rethinking)\\ndata(Primates301)\\ndata(Primates301_nex)\\n# plot it using ape package - install.packages(\\'ape\\') if needed\\nlibrary(ape)\\nplot( ladderize(Primates301_nex) , type=\"fan\" , font=1 , no.margin=TRUE ,\\nlabel.offset=1 , cex=0.5 )\\nI’ve plotted this phylogeny as Figure 14.13. We’re going to use this tree as a way to model\\nunobserved confounds. At the same time, we’d like to deal with the fact that some groups\\nof closely related species may be over-represented in nature. There are lots of lemurs for\\nexample. This produces an imbalance in sampling issue, analogous to an ordinary multilevel\\nmodeling context. And varying effects can help us here as well. But we’ll get the varying\\neffects, as it were, from the phylogenetic tree structure.\\n'},\n",
       " {'index': 498,\n",
       "  'number': 480,\n",
       "  'content': '480\\n14. ADVENTURES IN COVARIANCE\\nBefore we do anything with the tree, however, let’s run an ordinary regression analyzing\\n(log) group size as a function of (log) brain size and (log) body size. But I want to build this\\nordinary regression in an un-ordinary style, because it will help you understand the next\\nstep, where we stick the phylogenetic information inside. Think of all of the species as a\\nsingle variable, a vector of 301 trait values. Of course some of these values are more similar\\nto one another. In a typical regression, we model those similarities using predictor variables.\\nAfter conditioning on the predictor variables, the model expects correlations. So we can\\nwrite such a model using a big, multi-variate outcome distribution. It looks like this:\\nB ∼MVNormal(µ, S)\\nµi = α + βGGi + βMMi\\nwhere B is a vector of species brain sizes and S is a covariance matrix with as many rows and\\ncolumns as there are species. In an ordinary regression, this matrix takes the form:\\nS = σ2I\\nwhere σ is the same standard deviation you’ve used since Chapter 4 and I is an identity\\nmatrix, which is just a matrix with 1 along the diagonal and zeros everywhere else. You can\\nthink of it as a correlation matrix in which all of the correlations are zero. So multiplying the\\nvariance into it just gives each species the same (residual) variance. It’s an ordinary linear\\nregression, but thought of as having a single, multi-variate outcome.\\nLet’s fit this model to the primate data. First we need to trim down to the species for\\nwhich we have group size, brain size, and body size data:\\nR code\\n14.48\\nd <- Primates301\\nd$name <- as.character(d$name)\\ndstan <- d[ complete.cases( d$group_size , d$body , d$brain ) , ]\\nspp_obs <- dstan$name\\nYou should have 151 species left. Now to make a list with standardized logged variables and\\npass it all to ulam:\\nR code\\n14.49\\ndat_list <- list(\\nN_spp = nrow(dstan),\\nM = standardize(log(dstan$body)),\\nB = standardize(log(dstan$brain)),\\nG = standardize(log(dstan$group_size)),\\nImat = diag(nrow(dstan)) )\\nm14.9 <- ulam(\\nalist(\\nB ~ multi_normal( mu , SIGMA ),\\nmu <- a + bM*M + bG*G,\\nmatrix[N_spp,N_spp]: SIGMA <- Imat * sigma_sq,\\na ~ normal( 0 , 1 ),\\nc(bM,bG) ~ normal( 0 , 0.5 ),\\nsigma_sq ~ exponential( 1 )\\n), data=dat_list , chains=4 , cores=4 )\\nprecis( m14.9 )\\n'},\n",
       " {'index': 499,\n",
       "  'number': 481,\n",
       "  'content': '14.5. CONTINUOUS CATEGORIES AND THE GAUSSIAN PROCESS\\n481\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\na\\n0.00 0.02 -0.03\\n0.03\\n1859\\n1\\nbG\\n0.12 0.02\\n0.09\\n0.16\\n1572\\n1\\nbM\\n0.89 0.02\\n0.86\\n0.93\\n1481\\n1\\nsigma_sq 0.05 0.01\\n0.04\\n0.06\\n2040\\n1\\nLooks like a reliably positive association between brain size and group size, as well as a strong\\nassociation between body mass and brain size. There is no basis yet to interpret these asso-\\nciations causally, because we know these data are swirling with confounds.\\nNow we’ll conduct two different kinds of phylogenetic regression. In both, all we have\\nto do is replace the covariance matrix S above with a different matrix that encodes some\\nphylogenetic information. The first regression is one of the oldest and most conservative,\\na Brownian motion interpretation of the phylogeny that implies a very particular covari-\\nance matrix. Brownian motion just means Gaussian random walks. If species traits drift\\nrandomly with respect to one another after speciation, then the covariance between a pair of\\nspecies ends up being linearly related to the phylogenetic branch distance between them—\\nthe further apart, the less covariance, as a proportion of distance. Of course the traits we\\nare interested in obviously do not evolve neutrally, and they also evolve at different rates in\\ndifferent parts of the tree. But what you are about to do is unfortunately the most common\\nmethod of phylogenetic control.\\nLet’s compute the implied covariance matrix, the distance matrix, and show how they\\nare related. The ape R package has all of the functions you need.\\nR code\\n14.50\\nlibrary(ape)\\ntree_trimmed <- keep.tip( Primates301_nex, spp_obs )\\nRbm <- corBrownian( phy=tree_trimmed )\\nV <- vcv(Rbm)\\nDmat <- cophenetic( tree_trimmed )\\nplot( Dmat , V , xlab=\"phylogenetic distance\" , ylab=\"covariance\" )\\nI don’t show the plot here, but if you run the code, you’ll see a scatterplot with pairs of species\\nas points. The horizontal axis is phylogenetic, or patristic, distance. The vertical is the co-\\nvariance under the Brownian model. They are really just inverses of one another. You can\\nsee this even more clearly if you use image(V) and image(Dmat) to plot heat maps of each.\\nNow we can just insert this new matrix into our regression. The model is otherwise the\\nsame. But first we need to get the rows and columns in the same order as the rest of the data\\nand then convert it to a correlation matrix, so we can estimate the residual variance. Then\\nwe can just replace the identity matrix with our new correlation matrix and go.\\nR code\\n14.51\\n# put species in right order\\ndat_list$V <- V[ spp_obs , spp_obs ]\\n# convert to correlation matrix\\ndat_list$R <- dat_list$V / max(V)\\n# Brownian motion model\\nm14.10 <- ulam(\\nalist(\\nB ~ multi_normal( mu , SIGMA ),\\nmu <- a + bM*M + bG*G,\\n'},\n",
       " {'index': 500,\n",
       "  'number': 482,\n",
       "  'content': '482\\n14. ADVENTURES IN COVARIANCE\\nmatrix[N_spp,N_spp]: SIGMA <- R * sigma_sq,\\na ~ normal( 0 , 1 ),\\nc(bM,bG) ~ normal( 0 , 0.5 ),\\nsigma_sq ~ exponential( 1 )\\n), data=dat_list , chains=4 , cores=4 )\\nprecis( m14.10 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\na\\n-0.20 0.17 -0.47\\n0.06\\n2152\\n1\\nbG\\n-0.01 0.02 -0.04\\n0.02\\n2691\\n1\\nbM\\n0.70 0.04\\n0.64\\n0.76\\n1935\\n1\\nsigma_sq\\n0.16 0.02\\n0.13\\n0.19\\n2251\\n1\\nThis model annihilates group size—the posterior mean is almost zero and there is a lot of\\nmass on both sides of zero. The big change from the previous model suggests that there is a\\nlot of clustering of brain size in the tree and that this produces a spurious relationship with\\ngroup size, which also clusters in the tree. How the model uses this clustering depends upon\\nthe details of the correlation matrix we gave it.\\nThe Brownian motion model is a special kind of Gaussian process in which the covari-\\nance declines in a very rigid way with increasing distance. There is no need to be so rigid and\\ngood reason to think evolution is not well-described by Brownian motion. It’s very common\\nto use something called Pagel’s lambda to modify the Brownian motion model. But all\\nthis does is scale all of the species correlations by a common factor. It maintains the same\\narbitrary and unrealistic distance model. Another common alternative is the Ornstein–\\nUhlenbeck process (or OU process), which is a damped Brownian motion process that\\ntends to return towards some mean (or means). What this does in practice is constrain\\nthe variation, making the relationship between phylogenetic distance and covariance non-\\nlinear.219 More precisely, the OU process just defines the covariance between two species i\\nand j as:\\nK(i, j) = η2 exp(−ρ2Dij)\\nThis is an exponential distance kernel, unlike the quadratic kernel in the previous example.\\nThe exponential kernel says that covariance between points (species) declines rapidly, mak-\\ning for much less smooth functions. It is also usually harder to fit to data, since it is a much\\nrougher function. This means in practice that you’ll need to be careful about priors, poten-\\ntially making them narrower.\\nBut the OU process is still a Gaussian process, and you can fit it the same way as the\\nquadratic kernel in the previous section. The literature on phylogenetic regression has not\\nemphasized this fact. But expressing the model as a Gaussian process makes it possible to\\ncustomize the function space as the problem requires.220 This framing isn’t yet common.\\nBiologists tend to use phylogenies under a cloud of superstition and fearful button pushing.\\nThis is however a rapidly changing area, including new approaches that are not yet easy to im-\\nplement.221 Hopefully this also makes clear that there is no uniquely correct way to include\\nphylogenetic distance. If the goal is to estimate a causal effect, then it isn’t good enough to\\nreject some null model. We need to usefully reconstruct patterns among unmeasured con-\\nfounds. And different evolutionary histories will require different models. It will often be\\ntrue that the information in a phylogeny is inadequate for causal inference.\\n'},\n",
       " {'index': 501,\n",
       "  'number': 483,\n",
       "  'content': '14.5. CONTINUOUS CATEGORIES AND THE GAUSSIAN PROCESS\\n483\\nTo build the Gaussian process regression, we need a distance matrix. We already have\\nthat—you computed it earlier. Then we just need the Gaussian process construction line of\\ncode. In this example, we’ll use the OU process kernel, which is known more generally as the\\nL1 norm, which ulam provides as cov_GPL1. But see the Overthinking box further down,\\nto see how to write your own Gaussian process kernels.\\nR code\\n14.52\\n# add scaled and reordered distance matrix\\ndat_list$Dmat <- Dmat[ spp_obs , spp_obs ] / max(Dmat)\\nm14.11 <- ulam(\\nalist(\\nB ~ multi_normal( mu , SIGMA ),\\nmu <- a + bM*M + bG*G,\\nmatrix[N_spp,N_spp]: SIGMA <- cov_GPL1( Dmat , etasq , rhosq , 0.01 ),\\na ~ normal(0,1),\\nc(bM,bG) ~ normal(0,0.5),\\netasq ~ half_normal(1,0.25),\\nrhosq ~ half_normal(3,0.25)\\n), data=dat_list , chains=4 , cores=4 )\\nprecis( m14.11 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\na\\n-0.07 0.08 -0.19\\n0.06\\n2168\\n1\\nbG\\n0.05 0.02\\n0.01\\n0.09\\n2634\\n1\\nbM\\n0.83 0.03\\n0.79\\n0.88\\n2280\\n1\\netasq\\n0.03 0.01\\n0.03\\n0.05\\n2060\\n1\\nrhosq\\n2.79 0.26\\n2.36\\n3.20\\n2192\\n1\\nNow group size is seemingly associated with brain size again. The association is small, but\\nmost of the posterior mass is above zero. Why are the results different? The answer must be\\nthat the inferred covariance function looks rather different than the Brownian motion model.\\nSo let’s look at the posterior covariance functions implied by etasq and rhosq. Remember\\nthat these two parameters interact to produce the covariance function, and they are almost\\nalways strongly correlated in the posterior, so you can’t really see what’s going on by looking\\nat them separately. We need to extract them and push them back through the Gaussian\\nprocess covariance function:\\nR code\\n14.53\\npost <- extract.samples(m14.11)\\nplot( NULL , xlim=c(0,max(dat_list$Dmat)) , ylim=c(0,1.5) ,\\nxlab=\"phylogenetic distance\" , ylab=\"covariance\" )\\n# posterior\\nfor ( i in 1:30 )\\ncurve( post$etasq[i]*exp(-post$rhosq[i]*x) , add=TRUE , col=rangi2 )\\n# prior mean and 89% interval\\neta <- abs(rnorm(1e3,1,0.25))\\nrho <- abs(rnorm(1e3,3,0.25))\\nd_seq <- seq(from=0,to=1,length.out=50)\\nK <- sapply( d_seq , function(x) eta*exp(-rho*x) )\\n'},\n",
       " {'index': 502,\n",
       "  'number': 484,\n",
       "  'content': '484\\n14. ADVENTURES IN COVARIANCE\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\n0.5\\n1.0\\n1.5\\nphylogenetic distance\\ncovariance\\nprior\\nposterior\\nFigure 14.14. Posterior covariance functions\\nfor the Gaussian process phylogenetic regres-\\nsion (blue), compared to the prior (gray). Un-\\nlike the Brownian motion model, in which co-\\nvariance starts high and decays linearly with\\ndistance, this model favors a very small covari-\\nation at all distances.\\nlines( d_seq , colMeans(K) , lwd=2 )\\nshade( apply(K,2,PI) , d_seq )\\ntext( 0.5 , 0.5 , \"prior\" )\\ntext( 0.2 , 0.1 , \"posterior\" , col=rangi2 )\\nThe result is shown in Figure 14.14. The horizontal axis is the standardized phylogenetic\\ndistance—1 just means the longest distance in the sample. The vertical axis is covariance.\\nThe blue curves are 30 draws from the posterior distribution. The black curve is the prior\\nmean. The posterior is pressed up against the bottom axis, indicating a very low covariance\\nbetween species at any distance. There just isn’t a lot of phylogenetic covariance for brain\\nsizes, at least according to this model and these data. As a result, the phylogenetic distance\\ndoesn’t completely explain away the association between group size and brain size, as it did\\nin the Brownian motion model.\\nOverthinking: Building custom kernels. The rethinking package provides cov_GPL1 (the OU\\nkernel) and cov_GPL2 (the quadratic kernel) for building Gaussian process covariance matrices. But\\nit’s easy to build your own, if you use Stan directly. Let’s look at stancode(m14.11). The top part is\\na custom functions block, containing the cov_GPL1 function:\\nfunctions{\\nmatrix cov_GPL1(matrix x, real sq_alpha, real sq_rho, real delta) {\\nint N = dims(x)[1];\\nmatrix[N, N] K;\\nfor (i in 1:(N-1)) {\\nK[i, i] = sq_alpha + delta;\\nfor (j in (i + 1):N) {\\nK[i, j] = sq_alpha * exp(-sq_rho * x[i,j] );\\nK[j, i] = K[i, j];\\n}\\n}\\nK[N, N] = sq_alpha + delta;\\nreturn K;\\n}\\n}\\n'},\n",
       " {'index': 503,\n",
       "  'number': 485,\n",
       "  'content': '14.7. PRACTICE\\n485\\nThis function takes as input a distance matrix x and the parameters of the Gaussian process. It then\\nloops over all the cells in the covariance matrix K, computing the value of each. To modify the kernel,\\nyou’d change the line that computes each covariance:\\nK[i, j] = sq_alpha * exp(-sq_rho * x[i,j] );\\nFor example, the quadratic kernel just squares the x[i,j]. All that remains is to call the function\\ninside the model block.\\n14.6. Summary\\nThis chapter extended the basic multilevel strategy of partial pooling to slopes as well\\nas intercepts. Accomplishing this meant modeling covariation in the statistical population\\nof parameters. The LKJcorr prior was introduced as a convenient family of priors for corre-\\nlation matrices. You saw how covariance models can be applied to causal inference, using\\ninstrumental variables and the front-door criterion. Gaussian processes represent a practi-\\ncal method of extending the varying effects strategy to continuous dimensions of similarity,\\nsuch as spatial, network, phylogenetic, or any other abstract distance between entities in the\\ndata. The next chapter continues to develop the broader multilevel approach by applying it\\nto commonplace problems in statistical inference: measurement error and missing data.\\n14.7. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n14E1. Add to the following model varying slopes on the predictor x.\\nyi ∼Normal(µi, σ)\\nµi = αgroup[i] + βxi\\nαgroup ∼Normal(α, σα)\\nα ∼Normal(0, 10)\\nβ ∼Normal(0, 1)\\nσ ∼Exponential(1)\\nσα ∼Exponential(1)\\n14E2. Think up a context in which varying intercepts will be positively correlated with varying\\nslopes. Provide a mechanistic explanation for the correlation.\\n14E3. When is it possible for a varying slopes model to have fewer effective parameters (as estimated\\nby WAIC or PSIS) than the corresponding model with fixed (unpooled) slopes? Explain.\\n14M1. Repeat the café robot simulation from the beginning of the chapter. This time, set rho to zero,\\nso that there is no correlation between intercepts and slopes. How does the posterior distribution of\\nthe correlation reflect this change in the underlying simulation?\\n'},\n",
       " {'index': 504,\n",
       "  'number': 486,\n",
       "  'content': '486\\n14. ADVENTURES IN COVARIANCE\\n14M2. Fit this multilevel model to the simulated café data:\\nWi ∼Normal(µi, σ)\\nµi = αcafé[i] + βcafé[i]Ai\\nαcafé ∼Normal(α, σα)\\nβcafé ∼Normal(β, σβ)\\nα ∼Normal(0, 10)\\nβ ∼Normal(0, 10)\\nσ, σα, σβ ∼Exponential(1)\\nUse WAIC to compare this model to the model from the chapter, the one that uses a multi-variate\\nGaussian prior. Explain the result.\\n14M3. Re-estimate the varying slopes model for the UCBadmit data, now using a non-centered pa-\\nrameterization. Compare the efficiency of the forms of the model, using n_eff. Which is better?\\nWhich chain sampled faster?\\n14M4. Use WAIC to compare the Gaussian process model of Oceanic tools to the models fit to the\\nsame data in Chapter 11. Pay special attention to the effective numbers of parameters, as estimated\\nby WAIC.\\n14M5. Modify the phylogenetic distance example to use group size as the outcome and brain size as\\na predictor. Assuming brain size influences group size, what is your estimate of the effect? How does\\nphylogeny influence the estimate?\\n14H1. Let’s revisit the Bangladesh fertility data, data(bangladesh), from the practice problems for\\nChapter 13. Fit a model with both varying intercepts by district_id and varying slopes of urban\\nby district_id. You are still predicting use.contraception. Inspect the correlation between the\\nintercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern\\nof contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates\\nfor both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more\\neasily think through what it means to have a particular correlation. Plotting predicted proportion of\\nwomen using contraception, with urban women on one axis and rural on the other, might also help.\\n14H2. Now consider the predictor variables age.centered and living.children, also contained\\nin data(bangladesh). Suppose that age influences contraceptive use (changing attitudes) and num-\\nber of children (older people have had more time to have kids). Number of children may also directly\\ninfluence contraceptive use. Draw a DAG that reflects these hypothetical relationships. Then build\\nmodels needed to evaluate the DAG. You will need at least two models. Retain district and urban,\\nas in 14H1. What do you conclude about the causal influence of age and children?\\n14H3. Modify any models from 14H2 that contained that children variable and model the variable\\nnow as a monotonic ordered category, like education from the week we did ordered categories. Ed-\\nucation in that example had 8 categories. Children here will have fewer (no one in the sample had\\n8 children). So modify the code appropriately. What do you conclude about the causal influence of\\neach additional child on use of contraception?\\n14H4. Varying effects models are useful for modeling time series, as well as spatial clustering. In a\\ntime series, the observations cluster by entities that have continuity through time, such as individuals.\\nSince observations within individuals are likely highly correlated, the multilevel structure can help\\nquite a lot. You’ll use the data in data(Oxboys), which is 234 height measurements on 26 boys from\\nan Oxford Boys Club (I think these were like youth athletic leagues?), at 9 different ages (centered\\nand standardized) per boy. You’ll be interested in predicting height, using age, clustered by Subject\\n(individual boy). Fit a model with varying intercepts and slopes (on age), clustered by Subject. Present\\n'},\n",
       " {'index': 505,\n",
       "  'number': 487,\n",
       "  'content': '14.7. PRACTICE\\n487\\nand interpret the parameter estimates. Which varying effect contributes more variation to the heights,\\nthe intercept or the slope?\\n14H5. Now consider the correlation between the varying intercepts and slopes. Can you explain its\\nvalue? How would this estimated correlation influence your predictions about a new sample of boys?\\n14H6. Use mvrnorm (in library(MASS)) or rmvnorm (in library(mvtnorm)) to simulate a new\\nsample of boys, based upon the posterior mean values of the parameters. That is, try to simulate\\nvarying intercepts and slopes, using the relevant parameter estimates, and then plot the predicted\\ntrends of height on age, one trend for each simulated boy you produce. A sample of 10 simulated\\nboys is plenty, to illustrate the lesson. You can ignore uncertainty in the posterior, just to make the\\nproblem a little easier. But if you want to include the uncertainty about the parameters, go for it. Note\\nthat you can construct an arbitrary variance-covariance matrix to pass to either mvrnorm or rmvnorm\\nwith something like:\\nR code\\n14.54\\nS <- matrix( c( sa^2 , sa*sb*rho , sa*sb*rho , sb^2 ) , nrow=2 )\\nwhere sa is the standard deviation of the first variable, sb is the standard deviation of the second\\nvariable, and rho is the correlation between them.\\n'},\n",
       " {'index': 506, 'number': 488, 'content': ''},\n",
       " {'index': 507,\n",
       "  'number': 489,\n",
       "  'content': '15 Missing Data and Other Opportunities\\nA big advantage of Bayesian inference is that it obviates the need to be clever. For ex-\\nample, there’s a classic probability puzzle known as Bertrand’s box paradox.222 The version\\nthat I prefer involves pancakes. Suppose I cook three pancakes. The first pancake is burnt\\non both sides (BB). The second pancake is burnt on only one side (BU). The third pancake\\nis not burnt at all (UU). Now I serve you—at random—one of these pancakes, and the side\\nfacing up on your plate is burnt. What is the probability that the other side is also burnt?\\nThis is a hard problem, if we rely upon intuition. Most people say “one-half,” but that\\nis quite wrong. And with no false modesty, my intuition is no better. But I have learned to\\nsolve these problems by cold hard ruthless application of conditional probability. There’s no\\nneed to be clever when you can be ruthless.\\nSo let’s get ruthless. Applying conditional probability means using what we do know to\\nrefine our knowledge about what we wish to know. In other words:\\nPr(want to know|already know)\\nIn this case, we know the up side is burnt. We want to know whether or not the down side\\nis burnt. The definition of conditional probability tells us:\\nPr(burnt down|burnt up) = Pr(burnt up, burnt down)\\nPr(burnt up)\\nThis is just the definition of conditional probability, labeled with our pancake problem.\\nWe want to know if the down side is burnt, and the information we have is that the up\\nside is burnt. We condition on the information, so we update our state of information in\\nlight of it. The definition tells us that the probability we want is just the probability of the\\nburnt/burnt pancake divided by the probability of seeing a burnt side up. The probability\\nof the burnt/burnt pancake is 1/3, because a pancake was selected at random. The probabil-\\nity the up side is burnt must average over each way we can get dealt a burnt top side of the\\npancake. This is:\\nPr(burnt up) = Pr(BB)(1) + Pr(BU)(0.5) + Pr(UU)(0) = (1/3) + (1/3)(1/2) = 0.5\\nSo all together:\\nPr(burnt down|burnt up) = 1/3\\n1/2 = 2\\n3\\nIf you don’t quite believe this answer, you can do a quick simulation to confirm it.\\n489\\n'},\n",
       " {'index': 508,\n",
       "  'number': 490,\n",
       "  'content': '490\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nR code\\n15.1\\n# simulate a pancake and return randomly ordered sides\\nsim_pancake <- function() {\\npancake <- sample(1:3,1)\\nsides <- matrix(c(1,1,1,0,0,0),2,3)[,pancake]\\nsample(sides)\\n}\\n# sim 10,000 pancakes\\npancakes <- replicate( 1e4 , sim_pancake() )\\nup <- pancakes[1,]\\ndown <- pancakes[2,]\\n# compute proportion 1/1 (BB) out of all 1/1 and 1/0\\nnum_11_10 <- sum( up==1 )\\nnum_11 <- sum( up==1 & down==1 )\\nnum_11/num_11_10\\n[1] 0.6777889\\nTwo-thirds.\\nIf you want to derive some intuition now at the end, having seen the right answer, the\\ntrick is to count sides of the pancakes, not the pancakes themselves. Yes, there are 2 pancakes\\nthat have at least one burnt side. And only one of those has 2 burnt sides. But it is the sides,\\nnot the pancakes, that matter. Conditional on the up side being burnt, there are three sides\\nthat could be down. Two of those sides are burnt. So the probability is 2 out of 3.\\nProbability theory is not difficult mathematically. It is just counting. But it is hard to\\ninterpret and apply. Doing so often seems to require some cleverness, and authors have an\\nincentive to solve problems in clever ways, just to show off. But we don’t need that clever-\\nness, if we ruthlessly apply conditional probability. And that’s the real trick of the Bayesian\\napproach: to apply conditional probability in all places, for data and parameters. The ben-\\nefit is that once we define our information state—our assumptions—we can let the rules of\\nprobability do the rest. The work that gets done is the revelation of the implications of our\\nassumptions. Model fitting, as we’ve been practicing it, is the same un-clever approach. We\\ndefine the model and introduce the data, and conditional probability does the rest, revealing\\nthe implications of our assumptions, in light of the evidence.\\nIn this chapter, you’ll meet two commonplace applications of this assume-and-deduce\\nstrategy. The first is the incorporation of measurement error into our models. The sec-\\nond is the estimation of missing data through Bayesian imputation. You’ll see a fully\\nworked, introductory example of each.\\nIn neither application do you have to intuit the consequences of measurement errors\\nnor the implications of missing values in order to design the models. All you have to do\\nis state your information about the error or about the variables with missing values. Logic\\ndoes the rest. Well, your computer does the rest. But it’s just using fancy algorithms to\\nperform Bayesian updating. It’s not at all clever. But the implications it reveals are both\\ncounterintuitive and valuable.\\n'},\n",
       " {'index': 509,\n",
       "  'number': 491,\n",
       "  'content': '15.1. MEASUREMENT ERROR\\n491\\n15.1. Measurement error\\nBack in Chapter 5, you met the divorce and marriage data for the United States. Those\\ndata demonstrated a simple spurious association among the predictors, as well as how mul-\\ntiple regression can sort it out. What we ignored at the time is that both the divorce rate\\nvariable and the marriage rate variable are measured with substantial error, and that error is\\nreported in the form of standard errors. Importantly, the amount of error varies a lot across\\nStates. Here, you’ll see a simple and useful way to incorporate that information into the\\nmodel. Then we’ll let logic reveal the implications.\\nLet’s begin by plotting the measurement error of the outcome as an error bar:\\nR code\\n15.2\\nlibrary(rethinking)\\ndata(WaffleDivorce)\\nd <- WaffleDivorce\\n# points\\nplot( d$Divorce ~ d$MedianAgeMarriage , ylim=c(4,15) ,\\nxlab=\"Median age marriage\" , ylab=\"Divorce rate\" )\\n# standard errors\\nfor ( i in 1:nrow(d) ) {\\nci <- d$Divorce[i] + c(-1,1)*d$Divorce.SE[i]\\nx <- d$MedianAgeMarriage[i]\\nlines( c(x,x) , ci )\\n}\\nThe plot is shown on the left in Figure 15.1. Notice that there is a lot of variation in how\\nuncertain the observed divorce rate is, as reflected in varying lengths of the vertical line seg-\\nments. Why does the error vary so much? Large States provide better samples, so their\\nmeasurement error is smaller. The data are displayed this way, to show the association be-\\ntween the population size of each State and its measurement error, in the right-hand plot in\\nFigure 15.1.\\nSince the values in same States are more certain than in others, it makes sense for the\\nmore certain estimates to influence the regression more. There are all manner of ad hoc pro-\\ncedures for weighting some points more than others, and these can help. But they leave a lot\\nof information on the table. And they prevent a helpful phenomenon that arises automati-\\ncally in the fully Bayesian approach: Information flows among the measurements to provide\\nimproved estimates of the data itself. So let’s see how to state the information as a model.\\nRethinking: Generative thinking, Bayesian inference. Bayesian models are generative, meaning they\\ncan be used to simulate observations just as well as they can be used to estimate parameters. One\\nbenefit of this fact is that a statistical model can be developed by thinking hard about how the data\\nmight have arisen. This includes sampling and measurement, as well as the nature of the process we\\nare studying. Then let Bayesian updating discover the implications. These implications may include\\nthe inability to infer the generative process from data. Bayes is an honest partner. It is not afraid to\\nhurt your feelings.\\n15.1.1. Error on the outcome. To incorporate measurement error, let’s begin by thinking\\ngeneratively. If we were to simulate measurement error, what would it look like? The first\\n'},\n",
       " {'index': 510,\n",
       "  'number': 492,\n",
       "  'content': '492\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n4\\n6\\n8\\n10\\n12\\n14\\nMedian age marriage\\nDivorce rate\\n0\\n1\\n2\\n3\\n4\\n6\\n8\\n10\\n12\\n14\\nlog population\\nDivorce rate\\nFigure 15.1. Left: Divorce rate by median age of marriage, States of the\\nUnited States. Vertical bars show plus and minus one standard deviation\\nof the Gaussian uncertainty in measured divorce rate. Right: Divorce rate,\\nagain with standard deviations, against log population of each State. Smaller\\nStates produce more uncertain estimates.\\nstep would be to generate the true values of the variables. Then we simulate the observation\\nprocess itself, where the measurement error arises. It is just part of the statistical model and\\nlikewise part of the causal model.\\nRecall the causal model of the divorce example from Chapter 5. Let’s take that same\\nmodel and now add observation error on the outcome:\\nA\\nD\\nDobs\\nM\\neD\\nThere’s a lotgoing on here. But we can proceed one step at a time. The left triangle of this DAG\\nis the same system that we worked with back in Chapter 5. Age at marriage (A) influences\\ndivorce (D) both directly and indirectly, passing through marriage rate (M). Then we have\\nthe observation model. The true divorce rate D cannot be observed, so it is circled as an\\nunobserved node. However we do get to observe Dobs, which is a function of both the true\\nrate D and some unobserved error eD.\\nWhat are we supposed to do now? Note that Dobs is a descendent of D. Using it in place of\\nD doesn’t necessarily introduce confounding. Probably the majority of regressions are really\\nusing proxies like Dobs, because most variables are measurements with some error. But even\\nthough it doesn’t necessarily open a non-causal path, using a proxy can introduce systematic\\nbias, distorting the estimates. Since the extent of measurement error varies across States in\\na way that is associated with variables of interest, that is likely in this example.\\nWe could do better by using D instead of Dobs. But we don’t have D. However we can\\ntry to reconstruct it, respecting the uncertainty to avoid false confidence. In these data, the\\n'},\n",
       " {'index': 511,\n",
       "  'number': 493,\n",
       "  'content': '15.1. MEASUREMENT ERROR\\n493\\nreported standard errors Divorce.SE were calculated with knowledge of the process that\\nproduces the errors eD. How can we use this information in a statistical model? It’s just like a\\nsimulation, but in reverse. If you wanted to simulate measurement error, you would assign a\\ndistribution to each observation and sample from it. For example, suppose the true value of\\na measurement is 10 meters. If it is measured with Gaussian error with standard deviation\\nof 2 meters, this implies a probability distribution for any realized measurement y:\\ny ∼Normal(10, 2)\\nAs the measurement error here shrinks, all the probability piles up on 10. But when there is\\nerror, many measurements are more and less plausible. This is what I mean by saying that\\nordinary data are a special case of a distribution. And here is the key insight: If we don’t know\\nthe true value (10 in this example), then we can just put a parameter there and let Bayes do\\nthe rest.\\nHere’s how to define the error distribution for each divorce rate. For each observed value\\nDobs,i, there will be one parameter, Dtrue,i, defined by:\\nDobs,i ∼Normal(Dtrue,i, Dse,i)\\nAll this does is define the measurement Dobs,i as having the specified Gaussian distribution\\ncentered on the unknown parameter Dtrue,i. So the above defines a probability for each\\nState i’s observed divorce rate, given a known measurement error. If you simulated observed\\ndivorce rates from known true rates, it would look like:\\nD_obs <- rnorm( N_states , D_true , D_se )\\nA simulation like this goes from assumptions about the distribution to data. When we in-\\nstead estimate D_true, we run it in reverse, using Bayesian updating to go from data to\\ndistribution. This is what we’ve been doing since the beginning.\\nThis is a lot to take in. But we’ll go one step at a time. Recall that the goal is to model\\ndivorce rate D as a linear function of age at marriage A and marriage rate M. Here’s what the\\nmodel looks like, with the measurement errors highlighted in blue:\\nDobs,i ∼Normal(Dtrue,i, Dse,i)\\n[distribution for observed values]\\nDtrue,i ∼Normal(µi, σ)\\n[distribution for true values]\\nµi = α + βAAi + βMMi\\n[linear model to assess A →D]\\nα ∼Normal(0, 0.2)\\nβA ∼Normal(0, 0.5)\\nβM ∼Normal(0, 0.5)\\nσ ∼Exponential(1)\\nThis is like a linear regression, but with the addition of the top line that connects the obser-\\nvation to the true value. Each Dtrue parameter also gets a second role as the mean of another\\ndistribution, one that predicts the observed measurement. A cool implication that will arise\\nhere is that information flows in both directions—the uncertainty in measurement influences\\nthe regression parameters in the linear model, and the regression parameters in the linear\\nmodel also influence the uncertainty in the measurements. There will be shrinkage.\\nHere is the ulam version of the model, with all the variables standardized:\\nR code\\n15.3\\ndlist <- list(\\nD_obs = standardize( d$Divorce ),\\n'},\n",
       " {'index': 512,\n",
       "  'number': 494,\n",
       "  'content': '494\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nD_sd = d$Divorce.SE / sd( d$Divorce ),\\nM = standardize( d$Marriage ),\\nA = standardize( d$MedianAgeMarriage ),\\nN = nrow(d)\\n)\\nm15.1 <- ulam(\\nalist(\\nD_obs ~ dnorm( D_true , D_sd ),\\nvector[N]:D_true ~ dnorm( mu , sigma ),\\nmu <- a + bA*A + bM*M,\\na ~ dnorm(0,0.2),\\nbA ~ dnorm(0,0.5),\\nbM ~ dnorm(0,0.5),\\nsigma ~ dexp(1)\\n) , data=dlist , chains=4 , cores=4 )\\nConsider the posterior means (abbreviating the precis output below):\\nR code\\n15.4\\nprecis( m15.1 , depth=2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nD_true[1]\\n1.18 0.37\\n0.60\\n1.78\\n1696 1.00\\nD_true[2]\\n0.68 0.58 -0.20\\n1.63\\n2137 1.00\\nD_true[3]\\n0.43 0.34 -0.09\\n0.96\\n1953 1.00\\n...\\nD_true[48]\\n0.55 0.46 -0.15\\n1.30\\n2564 1.00\\nD_true[49] -0.64 0.27 -1.09 -0.20\\n3153 1.00\\nD_true[50]\\n0.84 0.59 -0.13\\n1.77\\n1815 1.00\\na\\n-0.06 0.10 -0.21\\n0.11\\n1314 1.00\\nbA\\n-0.61 0.16 -0.86 -0.37\\n1021 1.01\\nbM\\n0.05 0.17 -0.21\\n0.31\\n936 1.01\\nsigma\\n0.60 0.11\\n0.44\\n0.78\\n628 1.00\\nIf you look back at Chapter 5, you’ll see that the former estimate for bA was about −1. Now\\nit’s almost half that, but still reliably negative. So compared to the original regression that\\nignores measurement error, the association between divorce and age at marriage has been\\nreduced. The effect that measurement error has depends upon the context. Sometimes it\\nexaggerates effects, as in this example. Other times it hides them. But you can’t safely assume\\nthat measurement error makes estimates conservative.223\\nIf you look again at Figure 15.1, you can see a hint of why this has happened. States with\\nextremely low and high ages at marriage tend to also have more uncertain divorce rates. As\\na result those rates have been shrunk towards the expected mean defined by the regression\\nline. Figure 15.2 displays this shrinkage phenomenon. On the left of the figure, the differ-\\nence between the observed and estimated divorce rates is shown on the vertical axis, while\\nthe standard error of the observed is shown on the horizontal. The dashed line at zero indi-\\ncates no change from observed to estimated. Notice that States with more uncertain divorce\\nrates—farther right on the plot—have estimates more different from observed. This is your\\n'},\n",
       " {'index': 513,\n",
       "  'number': 495,\n",
       "  'content': '15.1. MEASUREMENT ERROR\\n495\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nD_sd\\nD_est – D_obs\\nAL\\nAK\\nAR\\nDC\\nID\\nME\\nNH\\nND\\nRI\\nSD\\nUT\\nVT\\nWY\\n-2\\n-1\\n0\\n1\\n2\\n3\\n-2\\n-1\\n0\\n1\\n2\\nmedian age marriage (std)\\ndivorce rate (std)\\nAR\\nID\\nME\\nMN\\nND\\nRI\\nWY\\nFigure 15.2. Left: Shrinkage resulting from modeling measurement error.\\nThe less error in the original measurement, the less shrinkage in the pos-\\nterior. Right: Comparison of regression that ignores measurement error\\n(dashed line and gray shading) with one that incorporates measurement er-\\nror (blue line and shading). The points and line segments show the posterior\\nmeans and standard deviations for each divorce rate, Dest,i.\\nfriend shrinkage from the previous two chapters. Less certain estimates are improved by\\npooling information from more certain estimates.\\nThis shrinkage results in pulling divorce rates towards the regression line, as seen in\\nthe right-hand plot in the same figure. This plot shows the posterior mean divorce rate for\\neach State against its observed median age at marriage. The vertical line segments show the\\nposterior standard deviations of each divorce rate—the estimates have moved, but they are\\nstill uncertain.\\nAs a result of their movement, however, the regression trend has moved. The old no-\\nerror regression is shown in gray. The fancy new with-error regression is shown in blue.\\nWell, really both the estimates and the trend have moved one another at the same time. For\\na State with an uncertain divorce rate, the trend has strongly influenced the new estimate of\\ndivorce rate. For a State with a fairly certain divorce rate—a small standard error—the State\\nhas instead strongly influenced the trend. The balance of all of this information is the shift\\nin both the estimated divorce rates and the regression relationship.\\n15.1.2. Error on both outcome and predictor. What happens when there is measurement\\nerror on predictor variables as well? The basic approach is the same. Again, consider the\\nproblem generatively: Each observed predictor value is a draw from a distribution with an\\nunknown mean, the true value, but known standard deviation. So we define a vector of\\nparameters, one for each unknown true value, and then make those parameters the means\\nof a family of Gaussian distributions with known standard deviations. Here’s the updated\\nDAG:\\n'},\n",
       " {'index': 514,\n",
       "  'number': 496,\n",
       "  'content': '496\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nA\\nD\\nDobs\\nM\\nMobs\\neD\\neM\\nNow there is a Mobs to mirror Dobs. Likewise there is an error eM to match. This DAG\\nassumes that the errors eD and eM are independent of one another. This is not necessarily\\nthe case.\\nHere’s the updated model, with the new bits in blue:\\nDobs,i ∼Normal(Dtrue,i, Dse,i)\\n[distribution for observed D values]\\nDtrue,i ∼Normal(µi, σ)\\n[distribution for true D values]\\nµi = α + βAAi + βMMtrue,i\\n[linear model]\\nMobs,i ∼Normal(Mtrue,i, Mse,i)\\n[distribution for observed M values]\\nMtrue,i ∼Normal(0, 1)\\n[distribution for true M values]\\nα ∼Normal(0, 0.2)\\nβA ∼Normal(0, 0.5)\\nβM ∼Normal(0, 0.5)\\nσ ∼Exponential(1)\\nThe Mtrue parameters will hold the posterior distributions of the true marriage rates. And\\nfitting the model is much like before:\\nR code\\n15.5\\ndlist <- list(\\nD_obs = standardize( d$Divorce ),\\nD_sd = d$Divorce.SE / sd( d$Divorce ),\\nM_obs = standardize( d$Marriage ),\\nM_sd = d$Marriage.SE / sd( d$Marriage ),\\nA = standardize( d$MedianAgeMarriage ),\\nN = nrow(d)\\n)\\nm15.2 <- ulam(\\nalist(\\nD_obs ~ dnorm( D_true , D_sd ),\\nvector[N]:D_true ~ dnorm( mu , sigma ),\\nmu <- a + bA*A + bM*M_true[i],\\nM_obs ~ dnorm( M_true , M_sd ),\\nvector[N]:M_true ~ dnorm( 0 , 1 ),\\na ~ dnorm(0,0.2),\\nbA ~ dnorm(0,0.5),\\nbM ~ dnorm(0,0.5),\\nsigma ~ dexp( 1 )\\n) , data=dlist , chains=4 , cores=4 )\\n'},\n",
       " {'index': 515,\n",
       "  'number': 497,\n",
       "  'content': '15.1. MEASUREMENT ERROR\\n497\\n-1\\n0\\n1\\n2\\n-2\\n-1\\n0\\n1\\n2\\nmarriage rate (std)\\ndivorce rate (std)\\nFigure 15.3. Shrinkage of both divorce rate\\nand marriage rate. Solid points are the ob-\\nserved values.\\nOpen points are posterior\\nmeans. Lines connect pairs of points for the\\nsame State. Both variables are shrunk towards\\nthe inferred regression relationship.\\nIf you inspect the precis output, you’ll see that the coefficients for age at marriage and\\nmarriage rate are essentially unchanged from the previous model. So adding error on the\\npredictor didn’t change the major inference. But it did provide updated estimates of marriage\\nrate itself. We can visualize this by the shrinkage of both marriage and divorce rates:\\nR code\\n15.6\\npost <- extract.samples( m15.2 )\\nD_true <- apply( post$D_true , 2 , mean )\\nM_true <- apply( post$M_true , 2 , mean )\\nplot( dlist$M_obs , dlist$D_obs , pch=16 , col=rangi2 ,\\nxlab=\"marriage rate (std)\" , ylab=\"divorce rate (std)\" )\\npoints( M_true , D_true )\\nfor ( i in 1:nrow(d) )\\nlines( c( dlist$M_obs[i] , M_true[i] ) , c( dlist$D_obs[i] , D_true[i] ) )\\nThe result is Figure 15.3. What has happened is that since the States with highly uncer-\\ntain marriage rates tend to be small States with high marriage rates, pooling has resulted in\\nsmaller estimates for those States.\\nThe big take home point for this section is that when you have a distribution of values,\\ndon’t reduce it down to a single value to use in a regression. Instead, use the entire distribu-\\ntion. Anytime we use an average value, discarding the uncertainty around that average, we\\nrisk overconfidence and spurious inference. This doesn’t only apply to measurement error,\\nbut also to cases in which data are averaged before analysis.\\nIn the previous model, with error on both the outcome and one of the predictors, we\\nused a standardized Normal(0,1) prior for the M values. This is okay, but it ignores some\\ninformation. Consider again the DAG for this system: A →M →D, A →D. This implies\\nthat a better prior for the M values would include A as a predictor. In other words, the entire\\ngenerative model belongs. We’ll attempt this in a practice problem at the end of the chapter.\\n15.1.3. Measurement terrors. In the models above, measurement error is rather benign.\\nThe errors are uncorrelated with one another and with the other variables in the model.\\nThis means there are no new confounds (non-causal paths) introduced by the errors. But\\nsometimes errors are more difficult to manage.\\n'},\n",
       " {'index': 516,\n",
       "  'number': 498,\n",
       "  'content': '498\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nConsider for example a DAG in which the errors on D and M are correlated with one\\nanother, because they are both influenced by a variable P:\\nA\\nD\\nDobs\\nM\\nMobs\\nP\\neD\\neM\\nIn this case, if we naively regress Dobs on Mobs, then there is an open non-causal path through\\nP. If we have information about the measurement process, such that we can model the true\\nvariables D and M, there is still hope. But we’ll need to consider the covariance between the\\nerrors. This is computationally similar to how we did instrumental variable regression in the\\nprevious chapter. There’s a problem at the end of this chapter where I ask you to attempt this.\\nAnother unfortunate situation can arise when another variable influences the error and\\ncreates another non-causal path. For example, suppose that the true marriage rate M influ-\\nences the error on divorce rate D:\\nA\\nD\\nDobs\\nM\\nMobs\\neD\\neM\\nWhy might this happen? If marriages are rare, then there aren’t as many couples that could\\npossibly get divorced. This means a smaller sample size to measure the divorce rate. So\\nsmaller M induces a larger error eD. This produces a non-causal path from Mobs to Dobs\\nthat passes through eD. And again, if we can average over the uncertainty in the true M\\nand D, using information about the measurement process, then we might do alright. But\\nignoring the measurement error isn’t alright. And that’s what almost everyone does almost\\nevery time.224\\nAnother pattern of measurement error to worry about is when a causal variable is mea-\\nsured less precisely than a non-causal variable. Suppose for example that we know D and M\\nvery precisely but that now A is measured with error. Also assume that M has zero causal\\neffect on D, like this:\\nA\\nAobs\\nD\\nM\\neA\\nIn this circumstance, it can happen that a naive regression of D on Aobs and M will strongly\\nsuggest that M influences D. The reason is that M contains information about the true A.\\nAnd M is measured more precisely than A is. It’s like a proxy A. Here’s a small simulation\\nyou can toy with that will produce such a frustration:\\n'},\n",
       " {'index': 517,\n",
       "  'number': 499,\n",
       "  'content': '15.2. MISSING DATA\\n499\\nR code\\n15.7\\nN <- 500\\nA <- rnorm(N)\\nM <- rnorm(N,-A)\\nD <- rnorm(N,A)\\nA_obs <- rnorm(N,A)\\nWhen you have your own data and your own particular measurement concerns, all of\\nthis can be overwhelming. But the way to proceed is the same as always: Use your back-\\nground knowledge to write down a generative model or models, simulate data from these\\nmodels in order to understand the inferential risks, and design a statistical approach that\\ncan work at least in theory.\\n15.2. Missing data\\nWith measurement error, the insight is to realize that any uncertain piece of data can be\\nreplaced by a distribution that reflects uncertainty. But sometimes data are simply missing—\\nno measurement is available at all. At first, this seems like a lost cause. What can be done\\nwhen there is no measurement at all, not even one with error?\\nThe most common treatment of missing values is just to drop all cases with any missing\\nvalues. This is known as complete case analysis. It is the default and silent behavior of\\nmost statistical software. Another common response is to replace missing values with some\\nassumed value, like the mean of the variable or a reference value like zero. Neither of these\\ntreatments is safe. Complete case analysis is at best inefficient. It throws away data. But it can\\nalso produce bias, depending upon the causal details. Replacing missing values with static\\nvalues is never warranted—we do not know those values, and if you fix them, the model will\\nthink it knows them with certainty.\\nSo what can we do instead? We can think causally about missingness, and we can use\\nthe model to impute missing values. A generative model tells you whether the process that\\nproduced the missing values will also prevent the identification of causal effects. Sometimes\\nit does. Other times it does not. Luckily, we can add missingness to a DAG and use the same\\ncriteria you already learned to figure out whether it produces confounding. A generative\\nmodel also provides information about values you have not yet seen.225 And this information\\ncan be used to average over our uncertainty and make full use of the non-missing values,\\ndropping nothing.\\nAll this will become clearer, if we draw some diagrams. We’ll start with some simple,\\nfictional examples. Then we’ll turn to some real examples.\\nRethinking: Missing data are meaningful data. The fact that a variable has an unobserved value is\\nstill an observation. It is data, just with a very special value. The meaning of this value depends upon\\nthe context. Consider for example a questionnaire on personal income. If some people refuse to fill\\nin their income, this may be associated with low (or high) income. Therefore a model that tries to\\npredict the missing values can be enlightening. In ecology, the absence of an observation of a species\\nis a subtle kind of observation. It could mean the species isn’t there. Or it could mean it is there but\\nyou didn’t see it. An entire category of models, occupancy models,226 exists to take this duality\\ninto account. Missing values are always produced by some process, and thinking about that process\\ncan sometimes solve big problems.\\n'},\n",
       " {'index': 518,\n",
       "  'number': 500,\n",
       "  'content': '500\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\n(a)\\nH*\\nD\\nH\\nS\\n(b)\\nH*\\nD\\nH\\nS\\n(c)\\nH*\\nD\\nH\\nS\\nX\\n(d)\\nH*\\nD\\nH\\nS\\nFigure 15.4. Four causal scenarios\\nfor the missing homework. See text\\nfor a complete explanation. (a) Dogs\\n(D) eat homework (H) completely at\\nrandom. (b) Dogs eat homework of\\nstudents who study (S) too much. (c)\\nDogs eat more homework in noisy (X)\\nhomes, where the homework is also\\nworse.\\n(d) Dogs prefer to eat bad\\nhomework.\\n15.2.1. DAG ate my homework. Consider a sample of students, all of whom own dogs.\\nThe students produce homework (H). This homework varies in quality, influenced by how\\nmuch each student studies (S). We could simulate 100 students, their attributes, and their\\nhomework like this:\\nR code\\n15.8\\nN <- 100\\nS <- rnorm( N )\\nH <- rbinom( N , size=10 , inv_logit(S) )\\nI’ve assumed here that homework H will be graded on a 10-point scale. More studying pro-\\nduces more points, on average.\\nAnd then some dogs eat some homework. One way to get a grasp on the problem of\\nmissing data is to think of missingness as its own variable, a 0/1 indicator for missingness.\\nSo let D be a 0/1 indicator variable for whether each dog ate homework. Once homework has\\nbeen eaten, we cannot observe the true distribution of homework. But we do get to observe\\nH∗, a copy of H with missing values where D = 1. In DAG form, this implies H →H∗←D.\\nWe’d like to learn the causal influence of studying (S) on homework (H), S →H. But\\nsince we don’t observe H, we have to use H∗instead. So we are relying on S →H∗being a\\ngood approximation of S →H. When will this be true? The impact of any missing values in\\nH∗depends upon how the missing values are generated. It depends upon their cause. Let’s\\nconsider four scenarios, depicted as DAGs in Figure 15.4.\\nThe simplest scenario, (a) in the upper left, is when dogs are completely random. A\\ndog’s decision to eat a piece of homework or not is not influenced by any relevant variable.\\nTherefore there is no arrow entering D in the DAG. Let’s simulate some random eating:\\nR code\\n15.9\\nD <- rbern( N ) # dogs completely random\\nHm <- H\\nHm[D==1] <- NA\\nThat Hm variable is H∗. We can’t use * in a variable name. Look inside Hm and you’ll see\\nrandom NAs scattered about. Is this a problem? We can decide by considering whether the\\noutcome H is independent of D. More generally, a minimal condition for missing values to\\n'},\n",
       " {'index': 519,\n",
       "  'number': 501,\n",
       "  'content': '15.2. MISSING DATA\\n501\\nbe benign is that the outcome is independent of (d-separated from) them. In this case, H is\\nindependent of D (H ⊥⊥D), because H∗is a collider.\\nA more intuitive way to think about this scenario is the following. Since the missing val-\\nues are completely random, missingness doesn’t necessarily change the overall distribution\\nof homework scores. It removes data, and that makes estimation less efficient. But missing\\nhomework doesn’t necessarily bias our estimate of the causal effect of studying. You should\\ntry to build a binomial model to estimate the causal effect of S on H, using both the com-\\npletely observed data and the data with missing values. There’s a practice problem at the end\\nof this chapter that asks you to do this.\\nNow consider DAG (b) in the upper right of Figure 15.4. Here studying influences\\nwhether a dog eats homework, S →D. Suppose for example that students who study a lot\\ndo not play with their dogs. Then the dogs take revenge by eating homework. Again let’s\\nsimulate:\\nR code\\n15.10\\nD <- ifelse( S > 0 , 1 , 0 )\\nHm <- H\\nHm[D==1] <- NA\\nNow every student who studies more than average (0) is missing homework. This scenario\\nisn’t as benign as the previous. But it isn’t doom either. Notice that there is now a non-causal\\npath (a backdoor path) from H →H∗←D ←S. If we don’t close this path, it will confound\\ninference along S →H. Luckily, we can close the non-causal path by conditioning on S, and\\nwe want to condition on S anyway. So this scenario isn’t necessarily bad, as long as we can\\ncondition on the variable that influences missingness (the dogs D). Again there is a problem\\nat the end that asks you to compare inference with all the homework and without missing\\nhomework.\\nThis doesn’t mean there is no danger here. If we get the functions or distributions wrong,\\nthen we may get the wrong answer and the missing data may prevent us from seeing the\\nabsurdity of it in posterior predictive checks. Suppose for example that studying doesn’t\\nhelp at all until a student does more than the average amount (0). In that case, we never get\\nto see homework from those students, so we can’t possibly figure out the function that relates\\nstudy effort to homework score.\\nThe next scenario, Figure 15.4 (c), is more difficult. The basic situation is the same:\\nThere is a variable that influences both H and D. Previously this was S. Now it is a new\\nvariable X, the noisy level of the student’s house. In a noisy house, students produce worse\\nhomework, X →H. Simultaneously, dogs in noisy houses tend to misbehave, X →D. I’ve\\nput a circle around X to signal that it is unobserved. Now when we regress H∗on S, a new\\nnon-causal path is in play: H∗←D ←X →H.\\nThe tricky question, however, is what effect this path has on our estimate of S →H. Let’s\\nactually code this one out, using the simulated data. Here’s a complete data simulation for\\nthe DAG in Figure 15.4 (c):\\nR code\\n15.11\\nset.seed(501)\\nN <- 1000\\nX <- rnorm(N)\\nS <- rnorm(N)\\nH <- rbinom( N , size=10 , inv_logit( 2 + S - 2*X ) )\\n'},\n",
       " {'index': 520,\n",
       "  'number': 502,\n",
       "  'content': '502\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nD <- ifelse( X > 1 , 1 , 0 )\\nHm <- H\\nHm[D==1] <- NA\\nAssuming a simple binomial model, first let’s see what we get when we fully observe H. Re-\\nmember, we haven’t observed X, so we can’t put it in the model.\\nR code\\n15.12\\ndat_list <- list(\\nH = H,\\nS = S )\\nm15.3 <- ulam(\\nalist(\\nH ~ binomial( 10 , p ),\\nlogit(p) <- a + bS*S,\\na ~ normal( 0 , 1 ),\\nbS ~ normal( 0 , 0.5 )\\n), data=dat_list , chains=4 )\\nprecis( m15.3 )\\nmean\\nsd 5.5% 94.5% n_eff Rhat\\na\\n1.11 0.03 1.07\\n1.15\\n1265\\n1\\nbS 0.69 0.03 0.65\\n0.73\\n1366\\n1\\nThe true coefficient on S should be 1.00. We don’t expect to get that exactly, but the estimate\\nabove is way off. This model used the complete data, before dogs ate any homework, so\\nit can’t be missingness that is the problem. This is just a case of omitted variable bias\\n(Chapter 10). Recall that in a generalized linear model, even if an unobserved variable like\\nX doesn’t structurally confound or interact with the predictor of interest like S, that doesn’t\\nmean that it won’t cause bias in estimation of the effect of S. The reason is that there are ceiling\\nand floor effects on the outcome variable that induce interactions among all predictors.\\nNow what impact does missing data have? Surely it will make things even worse. Let’s\\nsee. We’ll run the same model now, but with H∗instead of H, dropping cases where D = 1.\\nR code\\n15.13\\ndat_list0 <- list( H = H[D==0] , S = S[D==0] )\\nm15.4 <- ulam(\\nalist(\\nH ~ binomial( 10 , p ),\\nlogit(p) <- a + bS*S,\\na ~ normal( 0 , 1 ),\\nbS ~ normal( 0 , 0.5 )\\n), data=dat_list0 , chains=4 )\\nprecis( m15.4 )\\nmean\\nsd 5.5% 94.5% n_eff Rhat\\na\\n1.80 0.04 1.74\\n1.85\\n1051\\n1\\nbS 0.83 0.03 0.78\\n0.88\\n1060\\n1\\n'},\n",
       " {'index': 521,\n",
       "  'number': 503,\n",
       "  'content': '15.2. MISSING DATA\\n503\\nThe estimate for bS is still biased, but not as badly. This is only one example, but you can run\\nthousands of simulations like this one (I show you how in the Overthinking box at the end\\nof the section), and you’ll get this pattern on average. How has dropping students helped our\\nestimate? The homework that is missing is from noisy houses. And it is noisy houses that\\nmess up our estimate of bS, through omitted variable bias. So when we delete those houses\\nfrom the data, the estimate actually gets better.\\nNote that this improvement is not a general property of missing data in such a DAG. For\\nexample, if you change the missingness rule instead to:\\nR code\\n15.14\\nD <- ifelse( abs(X) < 1 , 1 , 0 )\\nNow missingness makes things worse. Give it a try. What happens under missingness de-\\npends upon the details of the functions in the full structural causal model. The DAG isn’t\\nenough to say what will happen. But the DAG is enough to say that we should be wary.\\nJust one more set of dogs remain. In Figure 15.4 (d), there is no X, but there is a path\\nfrom H →D. Now dogs prefer to eat bad homework. This is possibly because their owners\\nfeed it to them, but maybe it somehow tastes better too. To simulate from this DAG:\\nR code\\n15.15\\nN <- 100\\nS <- rnorm(N)\\nH <- rbinom( N , size=10 , inv_logit(S) )\\nD <- ifelse( H < 5 , 1 , 0 )\\nHm <- H; Hm[D==1] <- NA\\nGo ahead and try to estimate the causal effect S →H. You won’t be able to do a good job.\\nAnd there is nothing to do here, because there is nothing we can condition on to block the\\nnon-causal path S →H →D →H∗. This type of missingness, in which the variable causes\\nits own missing values, is the worst. Unless you know the mechanism that produces the\\nmissingness (D in this case), there is little hope. But even if you do know the mechanism,\\nsometimes the only solution is to take better measurements.\\nThe point of these examples is not to give you nightmares. The point is to illustrate\\nthe diverse consequences of missing data. But the diversity is explicable causally, in terms of\\nwhich variables cause missing values in which other variables. And the point of emphasizing\\nsimulation is to empower you to explore your own scenarios, the ones relevant to your own\\nresearch. Even when we cannot completely eliminate the impact of missing data, we might\\nbe able to show, through simulation, that the expected impact is rather small.\\nRethinking: Naming completely at random. Statistical terminology can be very confusing. The\\nfield uses ordinary words in highly technical ways. The everyday meanings of words like likelihood,\\nsignificant, and confidence barely resemble their statistical definitions. The topic of missing data is\\nno better. The dog-homework scenarios (Figure 15.4) sometimes go by the unhelpful names (a)\\nmissing completely at random (MCAR), (b) and (c) missing at random (MAR), and (d) the\\nimpressively absurd missing not at random (MNAR).227 The semantic difference between random\\nand completely random is insignificant for nearly all people. No one likes these terms, but you’ll still\\nsee them in use. Even if these terms were easy to remember, they are not sufficient to decide how to\\nhandle missing data, as the difference between scenarios (b) and (c) demonstrates. Don’t worry about\\ncategorization. Sketch the causal model, and then figure out your next move.\\n'},\n",
       " {'index': 522,\n",
       "  'number': 504,\n",
       "  'content': '504\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\n15.2.2. Imputing primates. Addressing missing data often involves the imputation of\\nmissing values. We impute both to avoid biased estimation and so that we can use all of\\nthe observed (not missing) data. The key idea with imputation is that any generative model\\nnecessarily contains information about variables that have not been observed. Some data\\ngo missing, but the model stays the same. In theory then imputing missing data is easy. In\\npractice there can be challenges, as always.\\nTo see how this works, let’s return to the primate milk example, from Chapter 5. We\\nused data(milk) to illustrate masking, using both neocortex percent and body mass to\\npredict milk energy. One aspect of those data are 12 missing values in the neocortex.perc\\ncolumn. We used a complete-case analysis back then, which means we dropped those 12\\ncases from the analysis. That means we also dropped 12 perfectly good body mass and milk\\nenergy values. That left us with only 17 cases to work with. Was that a bad idea?\\nTo answer that question, we need to think more clearly about why those values are miss-\\ning. The basic DAG from this example is:\\nB\\nK\\nM\\nU\\nwhere M is body mass, B is neocortex percent, K is milk energy, and U is some unobserved\\nvariable that renders M and B positively correlated. We want to add missingness to this\\ngraph, just like we added missingness to the dog-homework graphs in the previous section.\\nWe haven’t observed B (neocortex percent). We’ve instead observed B∗, a partially observed\\nset of values generated by B and some process. Which process? We don’t know yet. All\\nwe know is that the observed values B∗are a function of B and the “missingness” process.\\nWhatever the process, it generates a variable RB that indicates which species have missing\\nvalues. The variable RB is like the vector of dogs D in the dog-homework section.\\nThe crucial question is which variables influence RB. Let’s consider three possibilities.\\nB*\\nB\\nK\\nM\\nRB\\nU\\nB*\\nB\\nK\\nM\\nRB\\nU\\nB*\\nB\\nK\\nM\\nRB\\nU\\nIn all three DAGs above, the variable B is circled now to indicate that it is unobserved. Each\\nDAG is a different hypothesis about what causes the missing brain values RB. Let’s consider\\neach, going from left to right.\\nOn the left, nothing influences RB. It is completely random. In this case, there is no\\nnew non-causal path introduced. Dropping the species with missing brain values wastes\\ninformation—it means dropping all the observed mass values too—but it doesn’t necessarily\\nbias inference.\\nIn the middle, now body mass M influences which species have missing values. This\\ncould happen, for example, if smaller primates like lemurs are less often studied than larger\\n'},\n",
       " {'index': 523,\n",
       "  'number': 505,\n",
       "  'content': '15.2. MISSING DATA\\n505\\nprimates like gorillas. If M influences RB, it also creates a new non-causal path B∗←RB ←\\nM →K. But luckily conditioning on M blocks this path, and we want to condition on M\\nanyway. We still want to impute missing values, so that we don’t throw away information.\\nHow do we know if M influences RB? You could test this idea by trying to measure the\\ncausal influence of M on RB. But keep in mind that all that backdoor path stuff still applies.\\nDo you think you can estimate the causal influence of M on RB?\\nThe third example DAG, on the right, shows brain size B itself influencing RB. This could\\nhappen because anthropologists are more interested in large-brained species. There is a lot\\nmore research on chimpanzees, for example, than on lemurs. This scenario is awful. If true,\\nit means that estimation of B →K will be biased by a non-causal path through RB. It will\\nalso not be possible to test, with these data, whether B influences RB. Lots of different graphs\\ncan lead to this scenario. Here’s another possibility:\\nB*\\nB\\nK\\nM\\nRB\\nU\\nV\\nNow it isn’t the B values themselves that produce missingness. Rather there is an unobserved\\nvariable V that influences both B and RB. V could be for example phylogenetic similarity to\\nhumans. Humans have an unreasonable amount of neocortex—that is the reason we pay\\nattention to it—and other primates closely related to us also tend to have more neocortex.\\nIf those primates are studied more intensely, B values will be missing more as distance from\\nhumans increases. Just about the only hope in this scenario is to have detailed knowledge of\\nthe process that produces RB, allowing imputation of B. And that will nearly always require\\nstrong modeling assumptions, assumptions which usually cannot be tested with the data.\\nIn every DAG described above, we want to impute missing values of B. In the first and\\nsecond, we do so in order to not throw away corresponding values of M. In the third, we\\nhave to impute to hope for any sensible estimate of B →K. So let’s see how to actually do\\nthe imputation.\\nThe statistical trick with Bayesian imputation is to model the variable that has missing\\nvalues. Each missing value is assigned a unique parameter. The observed values give us\\ninformation about the distribution of the values. This distribution becomes a prior for the\\nmissing values. This prior will then be updated by full model. So there will be a posterior\\ndistribution for each missing value. Conceptually this is like the measurement error case—if\\nwe don’t know something, we condition it on what we know and let Bayes figure it out.\\nIn our case, the variable with missing values is neocortex percent. Again, we’ll call it B,\\nfor “brain”:\\nB = [0.55, B2, B3, B4, 0.65, 0.65, ..., 0.76, 0.75]\\nFor every index i at which there is a missing value, there is also a parameter Bi that will form\\na posterior distribution for it. The simplest model will simply impute B from its own normal\\n'},\n",
       " {'index': 524,\n",
       "  'number': 506,\n",
       "  'content': '506\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\ndistribution. Here it is, with the neocortex pieces in blue:\\nKi ∼Normal(µi, σ)\\n[distribution for outcome k]\\nµi = α + βBBi + βM log Mi\\n[linear model]\\nBi ∼Normal(ν, σB)\\n[distribution for obs/missing B]\\nα ∼Normal(0, 0.5)\\nβB ∼Normal(0, 0.5)\\nβM ∼Normal(0, 0.5)\\nσ ∼Exponential(1)\\nν ∼Normal(0.5, 1)\\nσB ∼Exponential(1)\\nThis model ignores that B and M are associated through U. But let’s start with this model, just\\nto keep things simple. The interpretation of Bi ∼Normal(ν, σB) is awkward at first. Note\\nthat when Bi is observed, then this line is a likelihood, just like any old linear regression.\\nThe model learns the distributions of ν and σB that are consistent with the data. But when\\nBi is missing and therefore a parameter, that same line is interpreted as a prior. Since the\\nparameters ν and σB are also estimated, the prior is learned from the data, just like the varying\\neffects in previous chapters.\\nOne issue with this model is that it assumes each B value has a standardized Gaussian\\nuncertainty. But we know that these values are bounded between zero and one, because they\\nare proportions. So it is possible to do a little better. In the practice problems at the end of\\nthe chapter, you’ll see how. But keep in mind that assigning a Gaussian distribution doesn’t\\nreally mean that the frequency distribution of the variable is a bell curve. It just means we\\nwill use only the mean and variance to describe it. The Gaussian is a very conservative choice,\\nbecause it is the flattest unbounded distribution with a given variance (Chapter 10). But as\\ndescribed way back in Chapter 7, if you have reason to suspect the tails of the distribution\\nare thick, then definitely do not use a Gaussian distribution.\\nImplementing an imputation model can be done several different ways. All of the ways\\nare a little awkward, because the locations of missing values have to respected, and that means\\nplenty of index management. The approach I’ll use here hews closely to the discussion just\\nabove: We’ll merge the observed values and parameters into a vector that we’ll use as “data”\\nin the regression. For convenience, ulam can automate this merging. The Overthinking box\\nat the end of this section presents a full implementation in raw Stan code.\\nTo fit the model with ulam, first get the data loaded and transform the predictors:\\nR code\\n15.16\\nlibrary(rethinking)\\ndata(milk)\\nd <- milk\\nd$neocortex.prop <- d$neocortex.perc / 100\\nd$logmass <- log(d$mass)\\ndat_list <- list(\\nK = standardize( d$kcal.per.g ),\\nB = standardize( d$neocortex.prop ),\\nM = standardize( d$logmass ) )\\nThe model code looks absolutely ordinary, except for defining a distribution for B.\\n'},\n",
       " {'index': 525,\n",
       "  'number': 507,\n",
       "  'content': '15.2. MISSING DATA\\n507\\nR code\\n15.17\\nm15.5 <- ulam(\\nalist(\\nK ~ dnorm( mu , sigma ),\\nmu <- a + bB*B + bM*M,\\nB ~ dnorm( nu , sigma_B ),\\nc(a,nu) ~ dnorm( 0 , 0.5 ),\\nc(bB,bM) ~ dnorm( 0, 0.5 ),\\nsigma_B ~ dexp( 1 ),\\nsigma ~ dexp( 1 )\\n) , data=dat_list , chains=4 , cores=4 )\\nWhen you start the model, it will notify you that it found 12 NA values and is trying to impute\\nthem. Once it finishes, take a look at the posterior summary:\\nR code\\n15.18\\nprecis( m15.5 , depth=2 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nnu\\n-0.04 0.20 -0.35\\n0.28\\n2013\\n1\\na\\n0.03 0.16 -0.22\\n0.28\\n2319\\n1\\nbM\\n-0.55 0.21 -0.88 -0.21\\n1238\\n1\\nbB\\n0.50 0.25\\n0.09\\n0.88\\n909\\n1\\nsigma_B\\n1.00 0.17\\n0.77\\n1.31\\n1593\\n1\\nsigma\\n0.84 0.15\\n0.63\\n1.11\\n1266\\n1\\nB_impute[1]\\n-0.56 0.91 -1.95\\n0.95\\n2602\\n1\\nB_impute[2]\\n-0.69 0.91 -2.10\\n0.79\\n2025\\n1\\nB_impute[3]\\n-0.68 0.94 -2.10\\n0.84\\n2086\\n1\\nB_impute[4]\\n-0.25 0.87 -1.61\\n1.15\\n3091\\n1\\nB_impute[5]\\n0.48 0.85 -0.93\\n1.82\\n2532\\n1\\nB_impute[6]\\n-0.16 0.85 -1.50\\n1.16\\n2626\\n1\\nB_impute[7]\\n0.19 0.85 -1.08\\n1.58\\n2640\\n1\\nB_impute[8]\\n0.28 0.86 -1.06\\n1.62\\n3697\\n1\\nB_impute[9]\\n0.52 0.87 -0.93\\n1.84\\n2574\\n1\\nB_impute[10] -0.46 0.89 -1.87\\n0.93\\n2092\\n1\\nB_impute[11] -0.27 0.86 -1.61\\n1.09\\n2650\\n1\\nB_impute[12]\\n0.17 0.85 -1.21\\n1.49\\n2749\\n1\\nEach of the 12 imputed distributions for missing values is shown here, along with the ordi-\\nnary regression parameters above them. To see how including all cases has impacted infer-\\nence, let’s do a quick comparison to the estimates that drop missing cases. I’ll drop the cases\\nwith missing values, but the model will be identical.\\nR code\\n15.19\\nobs_idx <- which( !is.na(d$neocortex.prop) )\\ndat_list_obs <- list(\\nK = dat_list$K[obs_idx],\\nB = dat_list$B[obs_idx],\\nM = dat_list$M[obs_idx] )\\nm15.6 <- ulam(\\nalist(\\nK ~ dnorm( mu , sigma ),\\n'},\n",
       " {'index': 526,\n",
       "  'number': 508,\n",
       "  'content': '508\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nmu <- a + bB*B + bM*M,\\nB ~ dnorm( nu , sigma_B ),\\nc(a,nu) ~ dnorm( 0 , 0.5 ),\\nc(bB,bM) ~ dnorm( 0, 0.5 ),\\nsigma_B ~ dexp( 1 ),\\nsigma ~ dexp( 1 )\\n) , data=dat_list_obs , chains=4 , cores=4 )\\nprecis( m15.6 )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nnu\\n0.00 0.22 -0.34\\n0.37\\n1821\\n1\\na\\n0.10 0.20 -0.21\\n0.42\\n1923\\n1\\nbM\\n-0.63 0.25 -1.01 -0.21\\n1276\\n1\\nbB\\n0.59 0.27\\n0.14\\n1.01\\n1244\\n1\\nsigma_B\\n1.04 0.18\\n0.79\\n1.36\\n1458\\n1\\nsigma\\n0.88 0.19\\n0.64\\n1.20\\n1145\\n1\\nComparing this posterior to the previous will be easier with a plot:\\nR code\\n15.20\\nplot( coeftab(m15.5,m15.6) , pars=c(\"bB\",\"bM\") )\\nm15.3\\nm15.4\\nm15.3\\nm15.4\\nbB\\nbM\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\nValue\\nThe model that imputes the missing values, m15.3, has narrower marginal distributions for\\nboth effects. How could this happen? We used more information, the values of body mass\\nthat are not missing but are discarded by m15.4. These values suggest a slightly smaller\\ninfluence of body mass, bM, and this also cascades into bB.\\nLet’s do some plotting to visualize what’s happened here.\\nR code\\n15.21\\npost <- extract.samples( m15.5 )\\nB_impute_mu <- apply( post$B_impute , 2 , mean )\\nB_impute_ci <- apply( post$B_impute , 2 , PI )\\n# B vs K\\nplot( dat_list$B , dat_list$K , pch=16 , col=rangi2 ,\\nxlab=\"neocortex percent (std)\" , ylab=\"kcal milk (std)\" )\\nmiss_idx <- which( is.na(dat_list$B) )\\nKi <- dat_list$K[miss_idx]\\npoints( B_impute_mu , Ki )\\nfor ( i in 1:12 ) lines( B_impute_ci[,i] , rep(Ki[i],2) )\\n# M vs B\\n'},\n",
       " {'index': 527,\n",
       "  'number': 509,\n",
       "  'content': '15.2. MISSING DATA\\n509\\n-2.0\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nneocortex percent (std)\\nkcal milk (std)\\n-2\\n-1\\n0\\n1\\n2\\n-2.0\\n-1.0\\n0.0\\n0.5\\n1.0\\n1.5\\nlog body mass (std)\\nneocortex percent (std)\\nFigure 15.5. Left: Inferred distribution of milk energy (vertical) and neo-\\ncortex proportion (horizontal), with imputed values shown by open points.\\nThe line segments are 89% posterior compatibility intervals. Right: In-\\nferred distribution between the two predictors, neocortex proportion and\\nlog mass. Imputed values again shown by open points.\\nplot( dat_list$M , dat_list$B , pch=16 , col=rangi2 ,\\nylab=\"neocortex percent (std)\" , xlab=\"log body mass (std)\" )\\nMi <- dat_list$M[miss_idx]\\npoints( Mi , B_impute_mu )\\nfor ( i in 1:12 ) lines( rep(Mi[i],2) , B_impute_ci[,i] )\\nFigure 15.5 displays both the inferred relationship between milk energy and neocortex (left)\\nand the relationship between the two predictors (right). Both plots show imputed neocortex\\nvalues in blue, with 89% compatibility intervals shown by the line segments. Although there’s\\na lot of uncertainty in the imputed values—hey, Bayesian inference isn’t magic, just logic—\\nthey do show a gentle tilt towards the regression relationship. This has happened because\\nthe observed values provide information that guides the estimation of the missing values.\\nThe right-hand plot shows the inferred relationship between the two predictors. We al-\\nready know that these two predictors are positively associated—that’s what creates the mask-\\ning problem. But notice here that the imputed values do not show an upward slope. They\\ndo not, because the imputation model—the first regression with neocortex (observed and\\nmissing) as the outcome—assumed no relationship.\\nWe can improve this model by changing the imputation model to estimate the relation-\\nship between the two predictors. This really just means that we use the entire generative\\nmodel. In the DAG, B and M are associated as a result of U. If we can include that fact in the\\nmodel, we might make better imputations and therefore better inferences. The technique is\\nonly to change the imputation line of the model from the simple:\\nBi ∼Normal(ν, σB)\\n'},\n",
       " {'index': 528,\n",
       "  'number': 510,\n",
       "  'content': '510\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nto a bivariate normal that includes both M and B:\\n(Mi, Bi) ∼MVNormal((µM, µB), S)\\nThe S matrix is another covariance matrix, and it will measure the correlation between M\\nand B, using the observed cases, and then use that correlation to infer the missing B values.\\nNote that this is the simplest model we could have of the association between M and B. It\\nassumes that the covariance is sufficient to describe their relationship. That will not always\\nbe the case, as many different bivariate relationships can produce the same covariance. If you\\nhave a better idea, then you should use that instead.\\nHere’s the ulam implementation. This is complex code, because we have to construct\\na variable that includes both the observed M values and the merged list of observed and\\nimputed B values. I’ll also do the merging more explicitly. In the Overthinking box at the\\nend, I walk through the Stan code, explaining some of the coding details.\\nR code\\n15.22\\nm15.7 <- ulam(\\nalist(\\n# K as function of B and M\\nK ~ dnorm( mu , sigma ),\\nmu <- a + bB*B_merge + bM*M,\\n# M and B correlation\\nMB ~ multi_normal( c(muM,muB) , Rho_BM , Sigma_BM ),\\nmatrix[29,2]:MB <<- append_col( M , B_merge ),\\n# define B_merge as mix of observed and imputed values\\nvector[29]:B_merge <- merge_missing( B , B_impute ),\\n# priors\\nc(a,muB,muM) ~ dnorm( 0 , 0.5 ),\\nc(bB,bM) ~ dnorm( 0, 0.5 ),\\nsigma ~ dexp( 1 ),\\nRho_BM ~ lkj_corr(2),\\nSigma_BM ~ dexp(1)\\n) , data=dat_list , chains=4 , cores=4 )\\nprecis( m15.7 , depth=3 , pars=c(\"bM\",\"bB\",\"Rho_BM\" ) )\\nmean\\nsd\\n5.5% 94.5% n_eff Rhat\\nbM\\n-0.65 0.22 -1.00 -0.30\\n1262\\n1\\nbB\\n0.58 0.26\\n0.16\\n0.99\\n1048\\n1\\nRho_BM[1,1]\\n1.00 0.00\\n1.00\\n1.00\\nNaN\\nNaN\\nRho_BM[1,2]\\n0.60 0.13\\n0.37\\n0.78\\n1592\\n1\\nRho_BM[2,1]\\n0.60 0.13\\n0.37\\n0.78\\n1592\\n1\\nRho_BM[2,2]\\n1.00 0.00\\n1.00\\n1.00\\n1981\\n1\\nThe slopes bM and bB haven’t changed much, although bM is perhaps a little more precise now.\\nWe’re interested in that correlation and how it has influenced the imputed values. The pos-\\nterior correlation is quite strong, 0.6 on average. This shows the strong positive relationship\\nbetween M and B that we already knew existed.\\nWhat does this correlation do to the imputed values? You can use the same plotting\\ncode as before. Figure 15.6 displays the same kind of plots as before, but now for the new\\n'},\n",
       " {'index': 529,\n",
       "  'number': 511,\n",
       "  'content': '15.2. MISSING DATA\\n511\\n-2.0\\n-1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n-1.0\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nneocortex percent (std)\\nkcal milk (std)\\n-2\\n-1\\n0\\n1\\n2\\n-2.0\\n-1.0\\n0.0\\n0.5\\n1.0\\n1.5\\nlog body mass (std)\\nneocortex percent (std)\\nFigure 15.6. Same relationships as shown in Figure 15.5, but now for the\\nimputation model that estimates the association between the predictors.\\nThe information in the association between predictors has been used to in-\\nfer a stronger relationship between milk energy and the imputed values.\\nimputation model. On the right, you can see now that the model has imputed in a way to\\npreserve the positive association between neocortex and log mass. Although in this example\\nthis doesn’t make a big difference in the inferred relationships with the outcome, it is clearly\\nbetter. Doing better is good.\\nRethinking: Multiple imputations. Missing data imputation has a messy history. There are many\\nforms of imputation, and most of them are ad hoc devices without a strong basis in probability the-\\nory: Hot-deck imputation, cold-deck imputation, mean substitution, stochastic imputation, among\\nothers. None of these procedures is considered respectable today. A common non-Bayesian pro-\\ncedure is multiple imputation.228 Multiple imputation was developed in the context of survey\\nnon-response, and it actually has a Bayesian justification. But it was invented when Bayesian im-\\nputation on the desktop was impractical, so it tries to approximate the full Bayesian solution to a\\n“missing at random” missingness model. If you aren’t comfortable dropping incomplete cases, then\\nyou shouldn’t be comfortable using multiple imputation either. The procedure performs multiple\\ndraws from an approximate posterior distribution of the missing values, performs separate analyses\\nwith these draws, and then combines the analyses in a way that approximates full Bayesian imputa-\\ntion. Multiple imputation is more limited than full Bayesian imputation, so now we just use the real\\nthing. But lots of non-Bayesian analyses still use multiple imputation. Remember that frequentist\\nstatistics isn’t a theory of how to produce estimates but rather just a theory of how to evaluate them.\\nOverthinking: Stan imputation algorithm. In principle, imputation is just using the same model but\\nreplacing data with parameters. Data are observed variables. Parameters are unobserved variables.\\nThe same generative model allows us to learn about both. But in practice, additional programming\\nis necessary. It’s necessary, because we have to construct a new variable that is a mix of observed and\\nunobserved values. The ulam code for m15.5 automates this. But it is worth seeing the guts of the\\nmachine, because it will increase understanding and teach you how to do this manually, in raw Stan\\ncode.\\n'},\n",
       " {'index': 530,\n",
       "  'number': 512,\n",
       "  'content': '512\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nIf you inspect the Stan code stancode(m15.5), you’ll see a functions block at the top. This is\\nwhere you can put special code that you don’t want cluttering up the model block. In this case:\\nfunctions{\\nvector merge_missing( int[] miss_indexes , vector x_obs , vector x_miss ) {\\nint N = dims(x_obs)[1];\\nint N_miss = dims(x_miss)[1];\\nvector[N] merged;\\nmerged = x_obs;\\nfor ( i in 1:N_miss )\\nmerged[ miss_indexes[i] ] = x_miss[i];\\nreturn merged;\\n}\\n}\\nThis code exists only to merge a vector of observed values with a vector of parameters to stand in\\nplace of missing values. It is called in the model block. Here are the important lines:\\nB_merge = merge_missing(B_missidx, to_vector(B), B_impute);\\nB_merge ~ normal( nu , sigma_B );\\nfor ( i in 1:29 ) {\\nmu[i] = a + bB * B_merge[i] + bM * M[i];\\n}\\nK ~ normal( mu , sigma );\\nThe first line above merges the observed data B with the imputation parameters in B_impute. The\\nvector B_missidx is just a list of the index positions of the missing values. If you use ulam, it builds\\nB_missidx for you. But if you use Stan directly, you’ll need to build it yourself. One line is enough:\\nR code\\n15.23\\nB_missidx <- which( is.na( dat_list$B ) )\\nYou pass B_missidx to the Stan model as data. The function merge_missing replaces each missing\\nvalue with the value of each corresponding parameter in B_impute. This is a bit awkward—it is joyless\\nindex shuffling. But it gets the job done, and in the end we have a vector B_merge that contains both\\nobserved values and imputation parameters in all the right places. The next lines of code then use\\nB_merge. The second line above is just the probability of the brain (neocortex percent) values, as\\nstated by the model. Then the loop constructs the linear predictor mu for each species, with B_merge\\nappearing, so that both observed values and imputation parameters are used as appropriate.\\nYou can use merge_missing directly in ulam models as well. It will declare the merged vector\\nand the vector of imputation parameters. The model m15.5 contains an example. Even m15.5 in-\\nserts merge_missing behind the scenes. See: m15.5@formula_parsed$formula. If you use Stan\\ndirectly, you’ll need to declare all of this yourself. You can see the necessary declarations in the\\nparameters and model blocks of stancode(m15.5).\\n15.2.3. Where is your god now? Sometimes there are no statistical solutions to scientific\\nproblems. But even then, careful statistical thinking can be useful because it will tell us that\\nthere is no statistical solution. Here’s an example involving missing data.\\nReligion is a human universal, as common among human societies as walking on two\\nlegs and naming stars. Anthropologists, archaeologists, and scholars of religion are some-\\ntimes curious about the impact of religious beliefs on the welfare of human societies. Some\\nof the most successful religious traditions involve gods (and other supernatural entities) that\\nenforce moral norms. For example, in the Abrahamic traditions, God punishes the wicked\\nand rewards the just. Such gods might be called “moralizing gods.” In other traditions, gods\\nbehave in their own self-interest, with no interest in encouraging humans to cooperate with\\none another. Does such a difference in belief have any consequences for the society? For ex-\\nample, if people who believe in a moralizing god are better at cooperating with one another,\\n'},\n",
       " {'index': 531,\n",
       "  'number': 513,\n",
       "  'content': '15.2. MISSING DATA\\n513\\nthen maybe societies that believe in moralizing gods grow faster and tend to replace societies\\nwith less moralizing gods.\\nLet’s look at a set of historical data that was used to evaluate this idea.229\\nR code\\n15.24\\ndata(Moralizing_gods)\\nstr(Moralizing_gods)\\n\\'data.frame\\': 864 obs. of\\n5 variables:\\n$ polity\\n: Factor w/ 30 levels \"Big Island Hawaii\",..: 1 1 1 1 1 1 ...\\n$ year\\n: int\\n1000 1100 1200 1300 1400 1500 1600 1700 1800 -600 ...\\n$ population\\n: num\\n3.73 3.73 3.6 4.03 4.31 ...\\n$ moralizing_gods: int\\nNA NA NA NA NA NA NA NA 1 NA ...\\n$ writing\\n: int\\n0 0 0 0 0 0 0 0 0 0 ...\\nThese data are population sizes (on the log scale) of different regions (polity) in differ-\\nent centuries (year). The key explanatory variable is moralizing_gods, which indicates\\nwhether members of a society believed in supernatural enforcement of morality (1), did not\\nbelieve (0), or there is insufficient evidence for assigning a value (NA). This last value (NA)\\nis usually associated with lack of any written evidence about religious belief. There is also an\\nindicator variable for literacy (writing).\\nDoes belief in moralizing gods increase the rate of population growth? This is a difficult\\ncausal query. There are plausibly many unobserved confounds that could produce a non-\\ncausal association between population growth rate and the content of religious traditions.\\nAnd belief in moralizing gods may not produce an immediately detectable increase in pop-\\nulation. Instead the causal effect could work over long time periods or only during periods\\nof conflict or ecological stress. Minimally, what we need is some comparison of popula-\\ntion growth rates before and after each society adopts moralizing gods. This is not a causal\\nidentification strategy that does anything about confounds—the appearance of moralizing\\ngods and larger populations could still be driven by other (unmeasured) variables. There is\\nno sense in which we can think of the year that moralizing gods appear as being a random\\ntreatment, in the sense of a regression discontinuity (Chapter 14, page 461). But if we\\nplayfully assume that there are no confounds, how should we go about this analysis?\\nThe first obstacle is that there are a lot of missing values in the moralizing_gods vari-\\nable. This prevents us from knowing exactly when (if ever) each society adopts belief in\\nmoralizing gods. How many values are missing? Let’s count:\\nR code\\n15.25\\ntable( Moralizing_gods$moralizing_gods , useNA=\"always\" )\\n0\\n1 <NA>\\n17\\n319\\n528\\nOf 864 cases, 528 of them (60%) are missing. Only 17 of the observed cases are zeros, which\\nmeans “no moralizing gods.” This is a lot of missing data, to be sure. But the raw amount\\nof missing data is not necessarily a reason to worry. Remember the homework-eating dogs\\nfrom earlier—the impact of missing data depends upon the process that produces missing\\ndata. If the missing gods are scattered at random, then we’re in luck. It’ll be useful to visualize\\nthe missingness pattern.\\n'},\n",
       " {'index': 532,\n",
       "  'number': 514,\n",
       "  'content': '514\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\n-10000\\n-8000\\n-6000\\n-4000\\n-2000\\n0\\n2000\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nTime (year)\\nPopulation size\\nMoralizing gods present\\nMoralizing gods absent\\nMoralizing gods unknown\\nFigure 15.7. Missing values in the Moralizing_gods data.\\nThe blue\\npoints, both open and filled, are observed values for the presence of beliefs\\nabout moralizing gods. The x symbols are unknowns, the missing values.\\nR code\\n15.26\\nsymbol <- ifelse( Moralizing_gods$moralizing_gods==1 , 16 , 1 )\\nsymbol <- ifelse( is.na(Moralizing_gods$moralizing_gods) , 4 , symbol )\\ncolor <- ifelse( is.na(Moralizing_gods$moralizing_gods) , \"black\" , rangi2 )\\nplot( Moralizing_gods$year , Moralizing_gods$population , pch=symbol ,\\ncol=color , xlab=\"Time (year)\" , ylab=\"Population size\" , lwd=1.5 )\\nThe result is shown in Figure 15.7. I’ve just plotted log population against year. The sym-\\nbols show the value of moralizing_gods. Filled blue points have value 1 (belief in moral-\\nizing gods known to be present). The open blue points have value 0 (belief in moralizing\\ngods known to be absent). The × symbols are points where the value is NA. This is a highly\\nnon-random missingness pattern. The reason is that written records are usually needed to\\ndetermine historical religious beliefs. Let’s look at the cross-tabulation of gods and literacy:\\nR code\\n15.27\\nwith( Moralizing_gods ,\\ntable( gods=moralizing_gods , literacy=writing , useNA=\"always\" ) )\\nliteracy\\ngods\\n0\\n1 <NA>\\n0\\n16\\n1\\n0\\n1\\n9 310\\n0\\n<NA> 442\\n86\\n0\\n442 (84%) of 528 missing values are for non-literate polities. No writing means no evidence\\nof any kind, in most cases. And as you can see in Figure 15.7, missing values are associated\\nwith smaller polities. This is possibly because smaller polities were (in the past) less likely\\nto be literate. These data are structured by the strong association between literacy, moral-\\nizing gods, and missing values. Beneath that mass of × symbols in Figure 15.7, belief in\\nmoralizing gods could be common or rare, depending on your theoretical preference.\\n'},\n",
       " {'index': 533,\n",
       "  'number': 515,\n",
       "  'content': '15.2. MISSING DATA\\n515\\nThis situation cannot be saved by statistics, but it is useful to reason why. After all, in\\nmany cases missing data don’t block inference. First we must consider whether we can just\\nignore the missing values, using a complete case analysis. But doing that in this context\\nwill almost certainly bias our inference, because the missingness is strongly associated with\\nother variables, like writing, which are in turn strongly associated with the outcome. It’ll\\nhelp to consider the causal structure of missingness. Here’s an optimistic guess:\\nG*\\nG\\nP\\nRG\\nW\\nHere P is rate of population growth (not the same as the population size variable in the\\ndata), G is the presence of belief in moralizing gods (which is unobserved), G∗is the observed\\nvariable with missing values, W is writing, and RG is the missing values indicator. This is an\\noptimistic scenario, because it assumes there are no unobserved confounds among P, G, and\\nW. These are purely observational data, recall. But the goal is to use this example to think\\nthrough the impact of missing data. If we can’t recover from missing data with the DAG\\nabove, adding confounds isn’t going to help.\\nRemember from the previous sections that the goal is to determine whether the outcome\\n(here P) is independent of missingness (here RG). This is clearly not a dog-eats-homework-\\nat-random situation, because RG is not completely random. It assumes missingness RG is\\nexplained entirely by an observed variable (W). Unfortunately, if P influences W, if we con-\\ndition on W to try to separate P and RG, it could makes things worse. It’s like conditioning on\\nthe outcome. A variable caused by the outcome will naturally have a strong association with\\nthe outcome and potentially explain away causal associations with other variables. I’ve made\\na practice problem at the end of the chapter to explain this better. Furthermore, in this case,\\nwriting is very strongly associated with missing values. Conditioning on RG would not help,\\nand so conditioning on a variable that almost uniquely determines it would not necessarily\\nhelp. We could make very favorable assumptions about the functional relationships among\\nthe variables, so that confounding would be weak. But structurally there isn’t any reason to\\ntrust an estimate of G →P here.\\nThere is still hope, if we are willing to make strong assumptions. If we could somehow\\ncondition on G instead of G∗, we’d be safe and clear. This is where imputation can help, by\\nreconstructing G with appropriate uncertainty. This is not trivial, however, because success-\\nful imputation requires a good approximation of the generative model of the variable. How\\nis G generated? There is no obvious answer. Consider for example the data for Hawaii. By\\n1778, Hawaii was a large and complex polity with moralizing gods. What happened in 1778?\\nCaptain James Cook and his crew finally made contact. Here is Hawaii:\\nR code\\n15.28\\nhaw <- which( Moralizing_gods$polity==\"Big Island Hawaii\" )\\ncolumns <- c(\"year\",\"writing\",\"moralizing_gods\")\\nt( Moralizing_gods[ haw , columns ] )\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nyear\\n1000 1100 1200 1300 1400 1500 1600 1700 1800\\nwriting\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\nmoralizing_gods\\nNA\\nNA\\nNA\\nNA\\nNA\\nNA\\nNA\\nNA\\n1\\n'},\n",
       " {'index': 534,\n",
       "  'number': 516,\n",
       "  'content': '516\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nAfter Captain Cook, Hawaii is correctly coded with 1 for belief in moralizing gods. It is also\\na fact that Hawaii never developed its own writing system. So there is no direct evidence of\\nwhen moralizing gods appeared in Hawaii. Any imputation model needs to decide how to\\nfill in those NA values. With so much missing data, any imputation model would necessarily\\nmake very strong assumptions.\\nThe strongest assumption would be just to replace all of the NA values with some constant,\\nlike zero. This implies a generative model in which any polity that believes in moralizing\\ngods will never produce a missing value. In the case of Hawaii, it assumes that moralizing\\ngods appear only after Captain Cook arrives. This procedure results in biased estimates of\\ntime of adoption of moralizing gods, because presumably more than just Hawaii believed in\\nmoralizing gods before they started writing about them. You might think no analyst would\\nimpute missing values this way. But this sort of arbitrary imputation is not rare.230\\nWhat else could we do? In principle we could perform a model-based imputation of\\nthe missing values in moralizing_gods. But we don’t have any obviously correct way to\\ndo this. We can’t just associate presence/absence of moralizing gods with population size,\\nbecause that’s the very question under investigation. Assuming the answer seems like a bad\\nidea. Sometimes all that statistics can do for us is confirm that we’ll just have to gather more\\nevidence. Here that means doing research to replace NA values with observations.\\nBut if we were going to try to impute the missing values, there is another obstacle. The\\nmoralizing_gods variable is discrete. It can take the values of zero or one only. Whether\\nimputing or dealing with measurement error, discrete variables are computationally trickier\\nthan continuous variables. The next section shows you how to handle them.\\nRethinking: Present details about missing data. The moralizing gods example contains a lot of\\nmissing data—60% of the primary exposure variable is NA. Obviously in cases like this one, it is very\\nimportant to inform readers about missing data and carefully justify how they were handled. But\\neven in more routine contexts, with more modest amounts of missing data, clear documentation\\nof missing data and its treatment is necessary. This is best done with a causal model that makes\\ntransparent what is being assumed about the source of missing values and simultaneously justifies\\nhow they are handled. But the minimum is to report the counts of missing values in each variable\\nand what was done with them.\\n15.3. Categorical errors and discrete absences\\nThe examples above focused on nice continuous variables. In the section on measure-\\nment error, the variables were continuous. In the section on missing data, neocortex percent\\nis continuous. When a variable is continuous, you can just assign a parameter to each un-\\nknown value—whether it is measured with error or rather completely missing—and let the\\nMarkov chain do the hard part.\\nBut when a variable is instead discrete—0/1 or 1,2,3,4 for example—then the Markov\\nchain needs some extra tutoring. Discrete unobserved variables require discrete parameters.\\nThere are two issues with discrete parameters. First, a discrete variable will not produce\\na smooth surface for Hamiltonian Monte Carlo to glide around on. HMC just doesn’t do\\ndiscrete variables. Second, other estimation approaches also have problems with discrete\\nparameter spaces, because discrete jumps are difficult to calibrate. Chains tend to get stuck\\nfor long periods.\\nBut that doesn’t mean we are stuck. In almost every case, we don’t need to sample\\ndiscrete parameters at all. Instead we can use a special technique, known to experts as a\\n'},\n",
       " {'index': 535,\n",
       "  'number': 517,\n",
       "  'content': '15.3. CATEGORICAL ERRORS AND DISCRETE ABSENCES\\n517\\n“weighted average,” to remove discrete parameters from the model. After sampling the other\\nparameters, we can then use their samples to compute the posterior distribution of any dis-\\ncrete parameter that we removed. So no information is given up. And removing the discrete\\nparameters actually makes the Markov chain more efficient, whatever engine you are using,\\nso it is usually worth doing, even if you aren’t using HMC. The technique can even be useful\\nwhen the parameters aren’t discrete, because removing continuous parameters also speeds\\nup the chain.\\nThis all sounds too good to be true. It is all true. But implementing it is not at all obvious.\\nIn this section, I’ll teach you how to do it, using the simplest example possible. The key idea,\\nwhatever the context, is that whether a variable is observed (data) or not (parameter), the\\ngenerative model defines its information. There is a little bit of mathematics in this section,\\nbut no more than you learned in secondary school. Once you grasp the general approach,\\nyou can apply it to discrete variables that are not binary, including count and categorical\\nvariables.\\n15.3.1. Discrete cats. Imagine a neighborhood in which every house contains a songbird.\\nSuppose we survey the neighborhood and sample one minute of song from each house,\\nrecording the number of notes. You notice that some houses also have house cats, and won-\\nder if the presence of a cat changes the amount that each bird sings. So you try to also figure\\nout which houses have cats. You can do this easily in some cases, either by seeing the cat or\\nby asking a human resident. But in about 20% of houses, you can’t determine whether or not\\na cat lives there.\\nThis very silly example sets us a very practical working example of how to cope with\\ndiscrete missing data. We will translate this story into a generative model, simulate data\\nfrom it, and then build a statistical model that copes with the missing values. Let’s consider\\nthe story above first as a DAG:\\nC*\\nC\\nN\\nRC\\nThe presence/absence of a cat C influences the number of sung notes N. Because of missing\\nvalues RC however, we only observe C∗. To make this into a fully generative model, we must\\nnow pick functions for each arrow above. Here are my choices, in statistical notation:\\nNi ∼Poisson(λi)\\n[Probability of notes sung]\\nlog λi = α + βCi\\n[Rate of notes as function of cat]\\nCi ∼Bernoulli(k)\\n[Probability cat is present]\\nRC,i ∼Bernoulli(r)\\n[Probability of not knowing Ci]\\nAnd then to actually simulate some demonstration data, we’ll have to pick values for α, β, k,\\nand r. Here’s a working simulation.\\nR code\\n15.29\\nset.seed(9)\\nN_houses <- 100L\\nalpha <- 5\\nbeta <- (-3)\\nk <- 0.5\\nr <- 0.2\\n'},\n",
       " {'index': 536,\n",
       "  'number': 518,\n",
       "  'content': '518\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\ncat <- rbern( N_houses , k )\\nnotes <- rpois( N_houses , alpha + beta*cat )\\nR_C <- rbern( N_houses , r )\\ncat_obs <- cat\\ncat_obs[R_C==1] <- (-9L)\\ndat <- list(\\nnotes = notes,\\ncat = cat_obs,\\nRC = R_C,\\nN = as.integer(N_houses) )\\nAt the end, I’ve replaced each unknown value of cat_obs with −9. There is nothing special\\nabout this value. The model will skip them. But it is usually good to use some invalid value,\\nso that if you make a mistake in coding, an error will result. In this case, since cat has\\na Bernoulli distribution, if the model ever asks for the probability of observing −9, there\\nshould be an error, because −9 is impossible.\\nTo program this model, we cannot declare a parameter for each unobserved cat. So\\ninstead we’ll just average over our uncertainty in whether the cat was there or not. What this\\nmeans, precisely, is that the likelihood of observing Ni notes, unconditional on Ci, is:\\nPr(Ni) = (probability of a cat)(probability of Ni when there is a cat)\\n+ (probability of no cat)(probability of Ni when there is no cat)\\nPr(Ni) = Pr(Ci = 1) Pr(Ni|Ci = 1) + Pr(Ci = 0) Pr(Ni|Ci = 0)\\nWhen we don’t know Ci, we compute the likelihood of Ni for each possible value of Ci—here\\none or zero—and then average these likelihoods using the probabilities that Ci takes on each\\nvalue. The above expression is what we need to code into the model. We can do this either by\\nusing Stan directly or by using custom distribution in ulam(). Let me show you the ulam()\\ncode. Then I’ll explain it.\\nR code\\n15.30\\nm15.8 <- ulam(\\nalist(\\n# singing bird model\\n## cat known present/absent:\\nnotes|RC==0 ~ poisson( lambda ),\\nlog(lambda) <- a + b*cat,\\n## cat NA:\\nnotes|RC==1 ~ custom( log_sum_exp(\\nlog(k) + poisson_lpmf( notes | exp(a + b) ),\\nlog(1-k) + poisson_lpmf( notes | exp(a) )\\n) ),\\n# priors\\na ~ normal(0,1),\\nb ~ normal(0,0.5),\\n# sneaking cat model\\ncat|RC==0 ~ bernoulli(k),\\n'},\n",
       " {'index': 537,\n",
       "  'number': 519,\n",
       "  'content': '15.3. CATEGORICAL ERRORS AND DISCRETE ABSENCES\\n519\\nk ~ beta(2,2)\\n), data=dat , chains=4 , cores=4 )\\nThe likelihood of notes at the top is split into two cases. You can read notes|RC==0 as “the\\nprobability of N when RC = 0.” So the first line in the model code above is just the ordinary\\nPoisson probability when the cat is known present or absent (RC = 0). The next lines are\\nthe average likelihood, when we haven’t observed the presence or absence of the cat, when\\nRC = 1. It looks complicated, but it is just the previous expression on the log scale. The term\\nlog(k) + poisson_lpmf( notes | exp(a + b) ) is log(Pr(Ci = 1) Pr(Ni|Ci = 1)),\\nand log(1-k) + poisson_lpmf( notes | exp(a) ) is log(Pr(Ci = 0) Pr(Ni|Ci = 0)).\\nThese two terms are then combined to make the weighted sum, on the log scale, using the\\nhelper function log_sum_exp. This function just takes a vector of log-probabilities, expo-\\nnentiates them, sums them, and then returns the log of the sum. But it does all of this in a\\nnumerically stable way.\\nThe rest of the model above is more familiar. Be sure to note however the cat pres-\\nence/absence model at the bottom. When the cat is known present or absent, RC = 0, we\\nwant to use that observation to update the parameter k, the probability a cat is present. This\\nis the same k in the likelihood. This means that the non-missing observations inform the\\nprior k for the missing observations. Take a look at the posterior of m15.6 and verify that it\\nmixes well and produces results that are consistent with the data generating process.\\nNow suppose we want to infer the unknown C values. To compute the probability that\\nany particular cat was present or absent, we can refer back to the generative model. The thing\\nwe want to know is Pr(Ci = 1). Prior to seeing the data, this is just the prior Pr(Ci = 1) = k.\\nOnce we observe Ni, the number of notes sung, we can update this prior with Bayes’ rule. In\\nthis case:\\nPr(Ci = 1|Ni) =\\nPr(Ni|Ci = 1) Pr(Ci = 1)\\nPr(Ni|Ci = 1) Pr(Ci = 1) + Pr(Ni|Ci = 0) Pr(Ci = 0)\\nThis looks like a mess. But really it is just a definition. The top is the probability of Ni notes\\nwhen Ci = 1. The bottom is just the average probability of Ni notes. There are just two\\nterms to calculate, and we actually already used them in our model. The denominator in the\\nexpression above is the same average probability of Ni that we wrote into the model code.\\nTo compute Pr(Ci = 1|Ni) for each i, we just need a few extra lines in the model code.\\nWe’ll perform these calculations in Stan’s generated quantities block, which means the\\ncalculations are performed only once per HMC transition and are saved in the returned\\nsamples. When using ulam, we can tag a line with gq> to indicate this is what we want. Here\\nis the updated model, with the new lines at the bottom:\\nR code\\n15.31\\nm15.9 <- ulam(\\nalist(\\n# singing bird model\\nnotes|RC==0 ~ poisson( lambda ),\\nnotes|RC==1 ~ custom( log_sum_exp(\\nlog(k) + poisson_lpmf( notes | exp(a + b) ),\\nlog(1-k) + poisson_lpmf( notes | exp(a) )\\n) ),\\nlog(lambda) <- a + b*cat,\\n'},\n",
       " {'index': 538,\n",
       "  'number': 520,\n",
       "  'content': '520\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\na ~ normal(0,1),\\nb ~ normal(0,0.5),\\n# sneaking cat model\\ncat|RC==0 ~ bernoulli(k),\\nk ~ beta(2,2),\\n# imputed values\\ngq> vector[N]:PrC1 <- exp(lpC1)/(exp(lpC1)+exp(lpC0)),\\ngq> vector[N]:lpC1 <- log(k) + poisson_lpmf( notes[i] | exp(a+b) ),\\ngq> vector[N]:lpC0 <- log(1-k) + poisson_lpmf( notes[i] | exp(a) )\\n), data=dat , chains=4 , cores=4 )\\nThose three lines that begin with gq> perform the calculations for Pr(Ci = 1|Ni). The first\\none defines a vector to hold the probabilities, and the formula is just the mathematical expres-\\nsion from before, Bayes rule. The exp stuff is necessary because we do the other calculations\\non the log scale, as always. The next two lines are just the same likelihood calculations as\\nbefore, the likelihoods of Ni conditional on the cat being present (lpC1) or absent (lpC0).\\nIn the practice problems at the end, I’ll ask you to compare the posterior probabilities\\nin PrC1 to the true values from the simulation. You can process these samples just like any\\nother parameter, even though we computed them in an unusual way.\\nThe strategy presented here extrapolates to discrete variables with more than two possi-\\nble values. In that case, you just need more than two terms in your average likelihood. For\\nexample, if houses can have up to two cats, then cats might be instead binomially distributed\\nacross houses. Then the code for the likelihood might be instead:\\nnotes|RC==1 ~ custom( log_sum_exp(\\nbinomial_lpmf(2|2,k) + poisson_lpmf( notes | exp(a + b*2) ),\\nbinomial_lpmf(1|2,k) + poisson_lpmf( notes | exp(a + b*1) ),\\nbinomial_lpmf(0|2,k) + poisson_lpmf( notes | exp(a + b*0) )\\n) )\\nRead each line above as the log probability of a specific number of cats, assuming cats are\\nbinomially distributed with maximum 2 and probability k, plus the log probability of a cer-\\ntain number of notes, assuming that specific number of cats. Unordered categories work the\\nsame way, but the leading terms would be from some simplex of probabilities.\\nThe same approach also works when you have more than one discrete variable with miss-\\ning values. In that case, you need a different average likelihood (custom() distribution) for\\neach combination of missing values. For example, suppose we also classify each house i by\\nwhether or not a dog (Di) lives there. So a house can have one of four possible observed\\ncombinations: (1) a cat and a dog, (2) a cat, (3) a dog, (4) neither a cat nor a dog (sad). Again\\nfor some fraction of houses, we were unable to learn whether or not they have a dog. Now\\nin the data, a house can have either or both the cat variable and the dog variable NA. If both\\nare NA, then we must average over all four possibilities listed above, with terms for both the\\nprior probability of a cat and a dog, like this:\\nPr(Ni) = Pr(Ci = 1) Pr(Di = 1) Pr(Ni|Ci = 1, Di = 1)\\n+ Pr(Ci = 1) Pr(Di = 0) Pr(Ni|Ci = 1, Di = 0)\\n+ Pr(Ci = 0) Pr(Di = 1) Pr(Ni|Ci = 0, Di = 1)\\n'},\n",
       " {'index': 539,\n",
       "  'number': 521,\n",
       "  'content': '15.5. PRACTICE\\n521\\n+ Pr(Ci = 0) Pr(Di = 0) Pr(Ni|Ci = 0, Di = 0)\\nIf only the cat is NA and the dog is known present (Di = 1), then we only have to average\\nover possibilities (1) and (3), like this:\\nPr(Ni) = Pr(Ci = 1) Pr(Ni|Ci = 1, Di = 1) + Pr(Ci = 0) Pr(Ni|Ci = 0, Di = 1)\\nIf only the dog is NA and the cat is known absent (Ci = 0), we average over possibilities (3)\\nand (4), like this:\\nPr(Ni) = Pr(Di = 1) Pr(Ni|Ci = 0, Di = 1) + Pr(Di = 0) Pr(Ni|Ci = 0, Di = 0)\\nIn principle, this is algorithmic and easy. In practice, it makes for complicated code. You have\\nto account all combinations of missingness and assign each a different average likelihood.\\nWe’ll see this general technique again in the next chapter, where we’ll encounter a state\\nspace model. State space models can have a large number of discrete (or continuous) un-\\nobserved variables. Typically we don’t write out each possibility in the code, but instead use\\nan algorithm to work over all of the possibilities and compute the necessary average likeli-\\nhood. For example, in a hidden Markov model, an algorithm known as the forward\\nalgorithm is used to do the averaging. The Stan user manual provides an example.\\n15.3.2. Discrete error. The example above concerned missing data. But when the data are\\nmeasured instead with error, the procedure is very similar. Suppose for example that in the\\nexample above each house is assigned a probability of a cat being present. Call this probability\\nki. When we are sure there is a cat there, ki = 1. When we are sure there is no cat, ki = 0.\\nWhen we think it is a coin flip, ki = 0.5. These ki values replace the parameter k in the\\nprevious model, becoming the weights for averaging over our uncertainty.\\n15.4. Summary\\nThis chapter has been a quick introduction to the design and implementation of meas-\\nurement error and missing data models. Measurement error and missing data have causes.\\nIncorporating those causes into the generative model helps us decide how error and missing-\\nness impact inference as well as how to design a statistical procedure. This chapter highlights\\nthe general principles of the book, that effective statistical modeling requires both careful\\nthought about how the data were generated and delicate attention to numerical algorithms.\\nNeither can lift inference alone.\\n15.5. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n15E1. Rewrite the Oceanic tools model (from Chapter 11) below so that it assumes measured error\\non the log population sizes of each society. You don’t need to fit the model to data. Just modify the\\nmathematical formula below.\\nTi ∼Poisson(µi)\\nlog µi = α + β log Pi\\nα ∼Normal(0, 1.5)\\nβ ∼Normal(0, 1)\\n15E2. Rewrite the same model so that it allows imputation of missing values for log population.\\nThere aren’t any missing values in the variable, but you can still write down a model formula that\\nwould imply imputation, if any values were missing.\\n'},\n",
       " {'index': 540,\n",
       "  'number': 522,\n",
       "  'content': '522\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\n15M1. Using the mathematical form of the imputation model in the chapter, explain what is being\\nassumed about how the missing values were generated.\\n15M2. Reconsider the primate milk missing data example from the chapter. This time, assign B a\\ndistribution that is properly bounded between zero and 1. A beta distribution, for example, is a good\\nchoice.\\n15M3. Repeat the divorce data measurement error models, but this time double the standard errors.\\nCan you explain how doubling the standard errors impacts inference?\\n15M4. Simulate data from this DAG: X →Y →Z. Now fit a model that predicts Y using both X\\nand Z. What kind of confound arises, in terms of inferring the causal influence of X on Y?\\n15M5. Return to the singing bird model, m15.9, and compare the posterior estimates of cat presence\\n(PrC1) to the true simulated values. How good is the model at inferring the missing data? Can you\\nthink of a way to change the simulation so that the precision of the inference is stronger?\\n15M6. Return to the four dog-eats-homework missing data examples. Simulate each and then fit\\none or more models to try to recover valid estimates for S →H.\\n15H1. The data in data(elephants) are counts of matings observed for bull elephants of differing\\nages. There is a strong positive relationship between age and matings. However, age is not always\\nassessed accurately. First, fit a Poisson model predicting MATINGS with AGE as a predictor. Second,\\nassume that the observed AGE values are uncertain and have a standard error of ±5 years. Re-estimate\\nthe relationship between MATINGS and AGE, incorporating this measurement error. Compare the\\ninferences of the two models.\\n15H2. Repeat the model fitting problem above, now increasing the assumed standard error on AGE.\\nHow large does the standard error have to get before the posterior mean for the coefficient on AGE\\nreaches zero?\\n15H3. The fact that information flows in all directions among parameters sometimes leads to rather\\nunintuitive conclusions. Here’s an example from missing data imputation, in which imputation of a\\nsingle datum reverses the direction of an inferred relationship. Use these data:\\nR code\\n15.32\\nset.seed(100)\\nx <- c( rnorm(10) , NA )\\ny <- c( rnorm(10,x) , 100 )\\nd <- list(x=x,y=y)\\nThese data comprise 11 cases, one of which has a missing predictor value. You can quickly confirm\\nthat a regression of y on x for only the complete cases indicates a strong positive relationship between\\nthe two variables. But now fit this model, imputing the one missing value for x:\\nyi ∼Normal(µi, σ)\\nµi = α + βxi\\nxi ∼Normal(0, 1)\\nα ∼Normal(0, 100)\\nβ ∼Normal(0, 100)\\nσ ∼Exponential(1)\\nBe sure to run multiple chains. What happens to the posterior distribution of β? Be sure to inspect\\nthe full density. Can you explain the change in inference?\\n'},\n",
       " {'index': 541,\n",
       "  'number': 523,\n",
       "  'content': '15.5. PRACTICE\\n523\\n15H4. Using data(Primates301), consider the relationship between brain volume (brain) and\\nbody mass (body). These variables are presented as single values for each species. However, there\\nis always a range of sizes in a species, and some of these measurements are taken from very small\\nsamples. So these values are measured with some unknown error.\\nWe don’t have the raw measurements to work with—that would be best. But we can imagine\\nwhat might happen if we had them. Suppose error is proportional to the measurement. This makes\\nsense, because larger animals have larger variation. As a consequence, the uncertainty is not uniform\\nacross the values and this could mean trouble.\\nLet’s make up some standard errors for these measurements, to see what might happen. Load\\nthe data and scale the the measurements so the maximum is 1 in both cases:\\nR code\\n15.33\\nlibrary(rethinking)\\ndata(Primates301)\\nd <- Primates301\\ncc <- complete.cases( d$brain , d$body )\\nB <- d$brain[cc]\\nM <- d$body[cc]\\nB <- B / max(B)\\nM <- M / max(M)\\nNow I’ll make up some standard errors for B and M, assuming error is 10% of the measurement.\\nR code\\n15.34\\nBse <- B*0.1\\nMse <- M*0.1\\nLet’s model these variables with this relationship:\\nBi ∼Log-Normal(µi, σ)\\nµi = α + β log Mi\\nThis says that brain volume is a log-normal variable, and the mean on the log scale is given by µ.\\nWhat this model implies is that the expected value of B is:\\nE(Bi|Mi) = exp(α)Mβ\\ni\\nSo this is a standard allometric scaling relationship—incredibly common in biology.\\nIgnoring measurement error, the corresponding ulam model is:\\nR code\\n15.35\\ndat_list <- list( B = B , M = M )\\nm15H4 <- ulam(\\nalist(\\nB ~ dlnorm( mu , sigma ),\\nmu <- a + b*log(M),\\na ~ normal(0,1),\\nb ~ normal(0,1),\\nsigma ~ exponential(1)\\n) , data=dat_list )\\nYour job is to add the measurement errors to this model. Use the divorce/marriage example in the\\nchapter as a guide. It might help to initialize the unobserved true values of B and M using the observed\\nvalues, by adding a list like this to ulam:\\nR code\\n15.36\\nstart=list( M_true=dat_list$M , B_true=dat_list$B )\\n'},\n",
       " {'index': 542,\n",
       "  'number': 524,\n",
       "  'content': '524\\n15. MISSING DATA AND OTHER OPPORTUNITIES\\nCompare theinferenceof themeasurementerror modeltothose of m1.1 above. Hasanything changed?\\nWhy or why not?\\n15H5. Now consider missing values—this data set is lousy with them. You can ignore measurement\\nerror in this problem. Let’s get a quick idea of the missing values by counting them in each variable:\\nR code\\n15.37\\nlibrary(rethinking)\\ndata(Primates301)\\nd <- Primates301\\ncolSums( is.na(d) )\\nWe’ll continue to focus on just brain and body, to stave off insanity. Consider only those species with\\nmeasured body masses:\\nR code\\n15.38\\ncc <- complete.cases( d$body )\\nM <- d$body[cc]\\nM <- M / max(M)\\nB <- d$brain[cc]\\nB <- B / max( B , na.rm=TRUE )\\nYou should end up with 238 species and 56 missing brain values among them.\\nFirst, consider whether there is a pattern to the missing values. Does it look like missing values\\nare associated with particular values of body mass? Draw a DAG that represents how missingness\\nworks in this case. Which type (MCAR, MAR, MNAR) is this?\\nSecond, impute missing values for brain size. It might help to initialize the 56 imputed variables\\nto a valid value:\\nR code\\n15.39\\nstart=list( B_impute=rep(0.5,56) )\\nThis just helps the chain get started.\\nCompare the inferences to an analysis that drops all the missing values. Has anything changed?\\nWhy or why not? Hint: Consider the density of data in the ranges where there are missing values.\\nYou might want to plot the imputed brain sizes together with the observed values.\\n15H6. Return to the divorce rate measurement error model. This time try to incorporate the full\\ngenerative system: A →M →D, A →D. What this means is that the prior for M should include A\\nsomehow, because it is influenced by A.\\n15H7. Some lad named Andrew made an eight-sided spinner. He wanted to know if it is fair. So he\\nspun it a bunch of times, recording the counts of each value. Then he accidentally spilled coffee over\\nthe 4s and 5s. The surviving data are summarized below.\\nValue\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\nFrequency\\n18\\n19\\n22\\nNA\\nNA\\n19\\n20\\n22\\nYour job is to impute the two missing values in the table above. Andrew doesn’t remember how many\\ntimes he spun the spinner. So you will have to assign a prior distribution for the total number of spins\\nand then marginalize over the unknown total. Andrew is not sure the spinner is fair (every value is\\nequally likely), but he’s confident that none of the values is twice as likely as any other. Use a Dirichlet\\ndistribution to capture this prior belief. Plot the joint posterior distribution of 4s and 5s.\\n'},\n",
       " {'index': 543,\n",
       "  'number': 525,\n",
       "  'content': '16 Generalized Linear Madness\\nWhen I asked my high school physics teacher about statistics, she told me a joke. Here’s\\nhow I remember it. A physicist, an engineer, and a statistician go bow hunting together. After\\nmany hours, they spot a deer in the distance. The physicist does a quick ballistic calculation,\\nignoring air resistance. The arrow flies true but falls a few meters short of the target. The\\ndeer doesn’t notice. The engineer smirks, introduces a fudge factor for air resistance, and\\nshoots. The second arrow lands instead a few meters long. The deer still doesn’t notice. The\\nstatistician takes the average and yells, “We got it!”\\nThe sciences construct theories of natural processes. Eventually these theories are ex-\\npressed formally, as mathematical models. Such models are specialized, make precise pre-\\ndictions, and can fail in equally precise ways. Being wrong in precise ways is useful, because\\nthe failures borrow meaning from the cause and effect relationships built into the models.\\nThis is true of the physicist and the engineer in the joke. They were wrong in very precise\\nways that give us hints about which causes were at fault.\\nApplied statistics has to apply to all the sciences, and so it is often much vaguer about\\nmodels. Instead it focuses on average performance, regardless of the model. The generalized\\nlinear models in the preceding chapters are not credible scientific models of most natural\\nprocesses. They are powerful, geocentric (Chapter 4) descriptions of associations. In combi-\\nnation with a logic of causal inference, for example DAGs and do-calculus, generalized linear\\nmodels can nevertheless be unreasonably powerful.\\nBut there are problems with this GLMs-plus-DAGs approach. Not everything can be\\nmodeled as a GLM—a linear combination of variables mapped onto a non-linear outcome.\\nBut if it is the only approach you know, then you have to use it. Other times the theory of\\ninterest can be expressed as a GLM, but the theory implies that some of the parameters are\\nfixed at special values. We might never notice, if we start with GLMs instead of real models.\\nAnd when a GLM fails, it’s not easy to learn from the failure. Debugging epicycles is a game\\nno one can win. If we could replace the heuristic DAG with an actual structural causal model,\\nwe might solve all these problems at once.\\nIn this chapter, I will go beyond generalized linear madness. I’ll work through\\nexamples in which the scientific context provides a causal model that will breathe life into the\\nstatistical model. I’ve chosen examples which are individually distinct and highlight different\\nchallenges in developing and translating causal models into bespoke (see the Rethinking box\\nbelow) statistical models. You won’t require any specialized scientific expertise to grasp these\\nexamples. And the basic strategy is the same as it has been from the start: Define a generative\\nmodel of a phenomenon and then use that model to design strategies for causal inference and\\nstatistical estimation.\\n525\\n'},\n",
       " {'index': 544,\n",
       "  'number': 526,\n",
       "  'content': '526\\n16. GENERALIZED LINEAR MADNESS\\nUnlike the other chapters in this book, there is some mathematics in this chapter, and it\\nreally cannot be avoided. But all you need is some algebra. We won’t so much do math as\\nexpress ideas with math. We will also work directly with Stan model code, since ulam() is\\nnot flexible enough for some of the examples. If you aren’t interested in the code, you can\\nignore it. But as usual, seeing the implementation often helps to clarify the concepts.\\nRethinking: Bespoken for. Mass production has some advantages, but it also makes our clothes fit\\nbadly. Garments bought off-the-shelf are not manufactured with you in mind. They are not bespoke\\nproducts, designed for any particular person with a particular body. Unless you are lucky to have a\\nperfectly average body shape, you will need a tailor to get better.\\nStatistical analyses are similar. Generalized linear models are off-the-shelf products, mass pro-\\nduced for a consumer market of impatient researchers with diverse goals. Science asked statisticians\\nfor tools that could be used anywhere. And so they delivered. But the clothes don’t always fit.\\nOne problem with off-the-shelf models is that they interrupt expertise. A typical researcher\\nknows a lot about their subject. Evidence of this is the detailed objections a scientist makes when\\nsomeone from another specialty tries to build a theoretical model for their subject. But then when\\nthose scientists turn to analyze their own data, they use tools that forbid the use of that knowledge.\\nThere is no way in a standard GLM to incorporate it. Even worse, if the only models researchers are\\never taught are GLMs (or GLMMs), these models may crowd out the formation of informed, bespoke\\nscientific models. GLMs are unreasonably powerful. But we should remember that they are usually\\nonly geocentric devices. Better bespoke models are eventually necessary, both for better fit and better\\ninference.\\n16.1. Geometric people\\nBack in Chapter 4, you met linear regression in the context of building a predictive model\\nof height using weight. You even saw how to measure non-linear associations between the\\ntwo variables. But nothing in that example was scientifically satisfying. The height-weight\\nmodel was just a statistical device. It contains no biological information and tells us noth-\\ning about how the association between height and weight arises. Consider for example that\\nweight obviously does not cause height, at least not in humans. If anything, the causal rela-\\ntionship is the reverse.\\nSo now let’s try to do better. Why? Because when the model is scientifically inspired,\\nrather than just statistically required, disagreements between model and data are informative\\nof real causal relationships.\\nSuppose for example that a person is shaped like a cylinder. Of course a person isn’t\\nexactly shaped like a cylinder. There are arms and a head. But let’s see how far this cylinder\\nmodel gets us. The weight of the cylinder is a consequence of the volume of the cylinder.\\nAnd the volume of the cylinder is a consequence of growth in the height and width of the\\ncylinder. So if we can relate the height to the volume, then we’d have a model to predict\\nweight from height.\\n16.1.1. The scientific model. Let’s do it. Sometime a long time ago you learned, and sensibly\\nforgot, that the formula for the volume of a person-cylinder is:\\nV = πr2h\\nwhere r is the person’s radius and h is its height. See Figure 16.1.231 We don’t know each\\nindividual’s radius, but let’s assume that each individual’s radius is some constant proportion\\n'},\n",
       " {'index': 545,\n",
       "  'number': 527,\n",
       "  'content': '16.1. GEOMETRIC PEOPLE\\n527\\nFigure 16.1. The “Vitruvian Can” model of\\nhuman weight as a function of height. If Vit-\\nruvian Man were a cylinder, we could estimate\\nhis weight by calculating his volume V as a\\nfunction of his height h and radius r.\\np of height. This means r = ph. Substituting this into the formula:\\nV = π(ph)2h = πp2h3\\nFinally, weight is some proportion of volume—how many kilograms are there per cubic cen-\\ntimeter? So we need a parameter k to express this translation between volume and weight.\\nW = kV = kπp2h3\\nAnd this is our formula for expected weight, given an individual’s height h. This is not ob-\\nviously an ordinary generalized linear model. But that’s okay. It has a causal structure, it\\nmakes predictions, and we can fit it to data.\\nRethinking: Spherical cows. Useful mathematical modeling typically involves ridiculous assump-\\ntions. For example, the assumption above that people are shaped like cylinders. This type of as-\\nsumption can be called a spherical cow, after the book Consider a Spherical Cow: A Course in\\nEnvironmental Problem Solving.232 Strategic, simplifying assumptions are features of all useful mod-\\nels. By first understanding the simplified model, it is easier to later add in relevant detail, where the\\nflaws in the simpler model help us decide which details are relevant. Non-mathematical models are\\nalso simplifications, but usually the simplifications are not explicit. This makes it harder to identify\\ntheir flaws.233 And sometimes simple models perform well, because they are simple in the right ways.\\n16.1.2. The statistical model. We can use the cylinder formula in a statistical model. To do\\nso however, we need to make some more choices. Here’s the model outline. I’ll explain each\\npiece afterwards.\\nWi ∼Log-Normal(µi, σ)\\n[Distribution for weight]\\nexp(µi) = kπp2h3\\ni\\n[expected median of weight]\\nk ∼some prior\\n[prior relation between weight and volume]\\np ∼some prior\\n[prior proportionality of radius to height]\\nσ ∼Exponential(1)\\n[our old friend, sigma]\\nFrom the top, the first thing to decide is the distribution for the observed outcome variable,\\nweight Wi. This variable is positive—weight can’t be negative—and continuous. So I’ve cho-\\nsen a Log-Normal distribution. The Log-Normal distribution is parameterized by the mean\\nof the logarithm, which is called µi. The median of the Log-Normal is exp(µi). In the model\\n'},\n",
       " {'index': 546,\n",
       "  'number': 528,\n",
       "  'content': '528\\n16. GENERALIZED LINEAR MADNESS\\nabove, I’ve assigned this median to be the cylinder function. Finally, we need priors for the\\nthree parameters k, p, and σ.\\nOne of the major advantages of having a scientifically inspired model is that the parame-\\nters have meanings. These meanings constitute prior information that we can use to choose\\ninformative distributions. This is especially useful in these contexts, because often there are\\nmore scientifically-required parameters than can be directly identified by the data. We can\\nnevertheless do useful estimation, given some scientific constraints on the parameters. That\\nis the case in this example.\\nThe first thing to notice about the parameters k and p is that they are multiplied in the\\nmodel and the data have no way to estimate anything except their product. The technical\\nway this problem could be described is that k and p, given this model and these data, are not\\nidentifiable. We could just replace the product kp2 with a new parameter θ and estimate\\nthat instead. Like this:\\nexp(µi) = πθh3\\ni\\nWe’ll get the same predictions. What we won’t get is an easy way to assign a prior to θ. So\\neven if we are going to use θ = kp2 trick, we’ll need to think still about k and p.\\nLet’s think about the parameter p. It is the ratio of the radius to the height, p = r/h. So\\nit must be greater than zero. It must also be less than one, because few people are wider than\\nthey are tall. It is almost certainly less than one-half, because a person as wide as they are\\ntall would have 2r = h, making p = (h/2)/h = 0.5. So p is probably much less than 0.5.\\nPutting all of this together, what we want is a distribution bounded between zero and one\\nwith most of the prior mass below 0.5. A beta distribution will do:\\np ∼Beta(2, 18)\\nThis prior will have mean 2/(2 + 18) = 0.1. We really need to do some prior predictive\\nsimulations to do better (see the practice problems at the end of this chapter). But that takes\\ncare of p for the moment.\\nThe parameter k is the proportion of volume that is weight. It really just translates mea-\\nsurement scales, because changing the units of volume or weight will change its value. For\\nexample, if height is measured in centimeters and weight is measured in kilograms, then\\nvolume has units cm3, and so k must have units kg/cm3. The definition of k, in that case, is\\njust how many kilograms there are per cubic centimeter. So to scale the prior right, we need\\nto have some information about how heavy a cubic centimeter of person is. We could look\\nthat up, or maybe use our own bodies to get a prior.\\nRethinking: Priors are never arbitrary. It’s commonplace to hear the fearful claim that Bayes is\\nuntrustworthy because priors are arbitrary. It is true that people sometimes treat priors that way. But\\npriors are only arbitrary when scientists ignore domain knowledge. Even when we stick with GLMs,\\nprior predictive simulations force us to engage with background knowledge to produce useful, non-\\narbitrary priors. When we have a more scientifically grounded model, the parameters have even\\nmore meaning. The p and k parameters in the cylinder example have scientific meanings that let us\\nassign priors that could even be measured physically. Using flat priors in this example, out of some\\nmetaphysical commitment to ignorance, would be a mistake.\\n'},\n",
       " {'index': 547,\n",
       "  'number': 529,\n",
       "  'content': '16.1. GEOMETRIC PEOPLE\\n529\\nBut suppose you couldn’t look it up. What then? A very useful trick is to instead get rid of\\nthe measurement scales altogether—measurement scales are arbitrary human inventions—\\nand then use the known biological constraints to locate the prior. How do we get rid of\\nmeasurement scale? We can divide the observed variables by some reference values. This\\nwill divide out the units. For example, suppose that we divide both height and weight by\\ntheir mean values.\\nR code\\n16.1\\nlibrary(rethinking)\\ndata(Howell1)\\nd <- Howell1\\n# scale observed variables\\nd$w <- d$weight / mean(d$weight)\\nd$h <- d$height / mean(d$height)\\nThe new variables w and h have means of 1. There is nothing special about using the means\\nhere. We just need some reference value to divide out the units. Now consider what a plausi-\\nble value of k might be, under this scaling. Suppose we have an individual of average height\\nand weight. In that case wi = 1 and hi = 1. Plugging these into the formula:\\n1 = kπp213\\nAssuming p < 0.5, then k must be greater than 1. I suggest we constrain k to be positive (it\\nhas to be) and give it a prior mean around 2.\\nk ∼Exponential(0.5)\\nWe could certainly do better than this, with some prior predictive simulation. But this will\\nget us started.\\nNow let’s pull all the threads together into a tapestry of code.\\nR code\\n16.2\\nm16.1 <- ulam(\\nalist(\\nw ~ dlnorm( mu , sigma ),\\nexp(mu) <- 3.141593 * k * p^2 * h^3,\\np ~ beta( 2 , 18 ),\\nk ~ exponential( 0.5 ),\\nsigma ~ exponential( 1 )\\n), data=d , chains=4 , cores=4 )\\nTake a look at the precis output. Can you make sense of the posterior distributions of p\\nand k? How were the priors updated?\\nWhile you think of answers to those questions, let’s inspect what the posterior does with\\nthe lack of identifiability of k and p. The pairs(m16.1) plot is the easiest way to appreciate\\nit. I show this plot in Figure 16.2, on the left. There is a narrow curved ridge in the posterior\\nwhere combinations of k and p produce the same product kp2. This results in a strong nega-\\ntive correlation between the two parameters—if one gets bigger, the other has to get smaller\\nto maintain the same product. Because we used informative priors, we were able to fit this\\nmodel anyway. But there is still no independent information about these parameters in the\\ndata itself. At least not with this model. There’s no reason in principle that k and p aren’t\\nalso functions of height (or age). For example, muscle and fat have very different densities.\\n'},\n",
       " {'index': 548,\n",
       "  'number': 530,\n",
       "  'content': '530\\n16. GENERALIZED LINEAR MADNESS\\nFigure 16.2. Left: Posterior distribution of k and p. Because only the prod-\\nuct kp2 appears in the model definition, the data alone cannot identify k and\\np, but only the product. The prior distributions make estimation possible.\\nRight: The cylinder model fit to the data. Note the poor fit at short heights.\\nSo k isn’t necessarily a constant, because relative muscle mass isn’t a constant. Similarly, the\\nratio of body width to height isn’t constant over development. So p may change as well.\\nThe idea that p may change can help us understand the posterior predictions. Let’s plot\\nthe posterior predictive distribution across the observed range of height.\\nR code\\n16.3\\nh_seq <- seq( from=0 , to=max(d$h) , length.out=30 )\\nw_sim <- sim( m16.1 , data=list(h=h_seq) )\\nmu_mean <- apply( w_sim , 2 , mean )\\nw_CI <- apply( w_sim , 2 , PI )\\nplot( d$h , d$w , xlim=c(0,max(d$h)) , ylim=c(0,max(d$w)) , col=rangi2 ,\\nlwd=2 , xlab=\"height (scaled)\" , ylab=\"weight (scaled)\" )\\nlines( h_seq , mu_mean )\\nshade( w_CI , h_seq )\\nThe result is displayed in the right panel of Figure 16.2. First, note that the model gets the\\ngeneral scaling relationship right. The exponent on height is fixed by theory at 3. We didn’t\\nestimate it. But it does a great job. Second, note the poor fit for the smallest heights in the\\nsample. This is possibly a symptom of p being different for children, as well as possibly k.\\nThe important lesson is that misfit for a scientific model gives us useful hints. If this were\\njust a linear regression, the parameters wouldn’t have biological meanings and we would fix\\nit by spinning up some epicycles.\\n16.1.3. GLM in disguise. Before moving on to the next example, consider what happens to\\nthis model when we relate the logarithm of weight to height. In that case, the expectation is:\\nlog wi = µi = log(kπp2h3\\ni )\\n'},\n",
       " {'index': 549,\n",
       "  'number': 531,\n",
       "  'content': \"16.2. HIDDEN MINDS AND OBSERVED BEHAVIOR\\n531\\nNow since multiplication becomes addition on the log scale, we can rewrite this as:\\nlog wi = log(k) + log(π) + 2 log(p) + 3 log(hi)\\nOn the log scale, this is a linear regression. The first three terms above comprise the intercept.\\nThen the term 3 log(hi) is a predictor variable with a fixed coefficient of 3. Theory gave us the\\nvalue of that coefficient. We didn’t need to estimate it. But it still has the form of an ordinary\\nlinear regression term.\\nI point this out to highlight one of the reasons that generalized linear models are so\\npowerful. Lots of natural relationships are GLM relationships, on a specific scale of mea-\\nsurement. At the same time, the GLM approach wants to simply estimate parameters which\\nmay be informed by a proper theory, as in this case.\\n16.2. Hidden minds and observed behavior\\nThe so-called inverse problem is one of the most basic problems in scientific infer-\\nence: How to figure out causes from observations. It is a problem, because many different\\ncauses can produce the same evidence. So while it can be easy to go forward from a known\\ncause to predicted observations, it can be hard to go backwards from observation to cause.\\nEvery branch of science has its own inverse problems. In this section, we’ll consider\\na simple example from developmental psychology. Children may possess many different\\ncognitive strategies for making decisions. Given some observations of their behavior, which\\nstrategy was the cause? Let’s consider specifically an experiment in which 629 children aged\\n4 to 14 saw four other children choose among three differently colored boxes (Figure 16.3).\\nEach child then made their own choice. In each trial, three demonstrators chose the same\\ncolor. The fourth demonstrator chose a different color. So in each trial, one of the colors was\\nthe majority choice, another was the minority choice, and the final color was unchosen. How\\ndo we figure out from this experiment whether children are influenced by the majority?\\nLet’s load the data234 and take a closer look.\\nR code\\n16.4\\nlibrary(rethinking)\\ndata(Boxes)\\nprecis(Boxes)\\n'data.frame': 629 obs. of 5 variables:\\nmean\\nsd 5.5% 94.5%\\nhistogram\\ny\\n2.12 0.73\\n1\\n3\\n▃▁▁▁▇▁▁▁▁▅\\nmale\\n0.51 0.50\\n0\\n1\\n▇▁▁▁▁▁▁▁▁▇\\nage\\n8.03 2.50\\n5\\n13\\n▇▃▅▃▃▃▂▂▂▁\\nFigure 16.3. The apparatus used in the experiment. The\\n“choice box” has three tubes, each with a different color.\\nWhen a ball is dropped into a tube, a toy comes out of the\\nbox. Four children demonstrated. Then the choice of a\\nfifth child was recorded. How did the choices of the first\\nfour influence the fifth child’s choice?\\n\"},\n",
       " {'index': 550,\n",
       "  'number': 532,\n",
       "  'content': '532\\n16. GENERALIZED LINEAR MADNESS\\nmajority_first 0.48 0.50\\n0\\n1\\n▇▁▁▁▁▁▁▁▁▇\\nculture\\n3.75 1.96\\n1\\n8 ▃▂▁▇▁▂▁▂▁▂▁▁▁▁\\nThe outcome y here takes the values 1, 2, and 3. It indicates which of the three options were\\nchosen, where 1 indicates the unchosen color, 2 indicates the majority demonstrated color,\\nand 3 indicates the minority demonstrated color. The other variable that we’ll use in this\\nexample is majority_first, which indicates whether the majority color was demonstrated\\nbefore the minority color. This is counter balanced across trials. The other variables are also\\ninteresting. But let’s set them aside for the moment.\\nWe’re interested in using the outcome y to infer the strategies the children used to choose\\na color. The distribution of the outcome contains 45% majority color choices:\\nR code\\n16.5\\ntable( Boxes$y ) / length( Boxes$y )\\n1\\n2\\n3\\n0.2114467 0.4562798 0.3322734\\nDoes this mean that 45% of the children used the strategy of following the majority? No.\\nThe core inferential problem is that there are three choices and many possible strategies.\\nAnd different strategies can produce the same choice in the same trial. For example, a child\\ncould just choose at random. This will result one-third of the time in the same prediction as a\\nchild who follows the majority. A GLM of these choices would infer frequencies of behavior.\\nBut we want to infer strategy. How can we do this?\\n16.2.1. The scientific model. The key, as always, is to think generatively. Consider for ex-\\nample a group of children in which half of them choose at random and the other half follow\\nthe majority. If we simulate choices for these children, we can figure out how often we might\\nsee the “2” choice, the one that indicates the majority color.\\nR code\\n16.6\\nset.seed(7)\\nN <- 30 # number of children\\n# half are random\\n# sample from 1,2,3 at random for each\\ny1 <- sample( 1:3 , size=N/2 , replace=TRUE )\\n# half follow majority\\ny2 <- rep( 2 , N/2 )\\n# combine and shuffle y1 and y2\\ny <- sample( c(y1,y2) )\\n# count the 2s\\nsum(y==2)/N\\n[1] 0.6333333\\nAbout two-thirds of the choices are for the majority color, but only half the children are\\nactually following the majority. The above is only one simulation, but it demonstrates the\\nproblem. When different hidden strategies can produce the same behavior, inference about\\nstrategy is more complicated than just counting behavior.\\n'},\n",
       " {'index': 551,\n",
       "  'number': 533,\n",
       "  'content': '16.2. HIDDEN MINDS AND OBSERVED BEHAVIOR\\n533\\nWe’ll consider 5 different strategies children might use.\\n(1) Follow the Majority: Copy the majority demonstrated color.\\n(2) Follow the Minority: Copy the minority demonstrated color.\\n(3) Maverick: Choose the color that no demonstrator chose.\\n(4) Random: Choose a color at random, ignoring the demonstrators.\\n(5) Follow First: Copy the color that was demonstrated first. This was either the ma-\\njority color (when majority_first equals 1) or the minority color (when 0).\\nEach strategy entails a vector of three probabilities, one for each choice. For example, Ran-\\ndom is [1/3, 1/3, 1/3]. The complicated one is Follow First, which depends upon the order\\nof presentation.\\nAn obvious question is: Why these strategies? Because they seem a priori plausible. If\\nthere are some that you think are not plausible, or other strategies that you feel are more\\nplausible, the same generative framework can accomodate them.\\n16.2.2. The statistical model. Now we need a statistical model that reflects the generative\\nmodel above. Remember, statistical models run in reverse of generative models. In the gen-\\nerative model, we assume strategies and simulate observed behavior. In the statistical model,\\nwe instead assume observed behavior (the data) and simulate strategies (parameters).\\nIn this example, we can’t directly measure each child’s strategy. It is an unobserved vari-\\nable. But each strategy has a specific probability of producing each choice. We can use that\\nfact to compute the probability of each choice, given parameters which specify the proba-\\nbility of each strategy. Then we let Bayes loose and get the posterior distribution of each\\nstrategy back. Before we can let Bayes loose, we’ll need to enumerate the parameters, assign\\npriors to each, and also figure out some technical issues for coding. I’ll move through these\\ntasks slowly.\\nThe unobserved variables are the probabilities that a child uses each of the five strategies.\\nThis means five values, but since these must sum to one, we need only four parameters. There\\nis a variable type called a simplex that handles this for us. A simplex is a vector of values\\nthat must sum to some constant, usually one. Stan allows us to declare a vector of parameters\\nas a simplex, and then Stan handles the bookkeeping of the constant sum for us. We can\\ngive this simplex a Dirichlet prior, which is a prior for probability distributions. We used\\nboth Dirichlet and a simplex already back in Chapter 12 to construct ordered categorical\\npredictors (page 393). We’ll use a weak uniform prior on the simplex of strategy probabilities,\\nwhich we’ll label p:\\np ∼Dirichlet([4, 4, 4, 4, 4])\\nAs you saw back in Chapter 12, this prior doesn’t mean that we expect the strategies to be\\nequally probable. Instead it means that we expect that any one of them could be more or less\\nprobable than any other. If you make those 4s larger, the prior starts to say that we expect\\nthem to be actually equal.\\nNow how to express the probability of the data, the likelihood? For each observed choice\\nyi, each strategy s implies a probability of seeing yi. Call this Pr(yi|s), the probability of the\\ndata, conditional on assuming a specific strategy s. For example assuming s = 1, the majority\\nstrategy, then Pr(yi = 2|s = 1) = 1. This is just the mathy way of saying that a child using\\nthe majority strategy always follows the majority color choice.\\nWe don’t know s though. We can’t observe it directly. However we do have a probability\\nfor each s in the model. These are the elements of the simplex p. So to get the unconditional\\n'},\n",
       " {'index': 552,\n",
       "  'number': 534,\n",
       "  'content': '534\\n16. GENERALIZED LINEAR MADNESS\\nprobability of the data Pr(yi) we just need to use p to average over the unknown strategy s:\\nPr(yi) =\\n5\\nX\\ns=1\\nps Pr(yi|s)\\nRead this as the probability of yi is the weighted average of the probabilities of yi conditional on\\neach strategy s. This expression is a mixture, as in earlier chapters. Sometimes you’ll read that\\nthis marginalizes out the unknown strategy. This just means averaging over the strategies,\\nusing some probability of each to get the weight of each in the average. Above, the values in\\np provide these weights.\\nOkay, so we have our statistical model now. Let’s write it in a more conventional form:\\nyi ∼Categorical(θ)\\nθj =\\n5\\nX\\ns=1\\nps Pr(j|s)\\nfor j = 1...3\\np ∼Dirichlet([4, 4, 4, 4, 4])\\nThe vector θ holds the average probability of each behavior, conditional on p. As a generative\\nmodel, the above implies that all children are identical—each child on each trial has some\\nprobability ps of using strategy s. Of course there are individual differences among the chil-\\ndren. But since we don’t have any repeat observations of each child in these data, we can’t\\ndo much better than the above. But if we did have repeat observations, we’d assign a unique\\nsimplex p to each child, power up the partial pooling, and enjoy the fireworks.\\n16.2.3. Coding the statistical model. Coding this model means explicitly coding the logic\\nof each strategy, those Pr(j|s) terms above. We will write this model directly in Stan, because\\nit will actually make it both easier to code and easier to extend. There have been some op-\\ntional Stan models in previous chapters. But now it’s not optional. I’ve included the model\\ncode in the rethinking package. You can load and display it with:\\nR code\\n16.7\\ndata(Boxes_model)\\ncat(Boxes_model)\\nI’ll put the explanation of the Stan code in the Overthinking box further down, so you can\\nfocus on the coding details later.\\nTo run the sampler, all that remains is to prepare the data list and then invoke stan().\\nThe data list needs only the sample size N, the vector of choices y, and the vector of presenta-\\ntion order majority_first.\\nR code\\n16.8\\n# prep data\\ndat_list <- list(\\nN = nrow(Boxes),\\ny = Boxes$y,\\nmajority_first = Boxes$majority_first )\\n# run the sampler\\nm16.2 <- stan( model_code=Boxes_model , data=dat_list , chains=3 , cores=3 )\\n# show marginal posterior for p\\n'},\n",
       " {'index': 553,\n",
       "  'number': 535,\n",
       "  'content': '16.2. HIDDEN MINDS AND OBSERVED BEHAVIOR\\n535\\np_labels <- c(\"1 Majority\",\"2 Minority\",\"3 Maverick\",\"4 Random\",\\n\"5 Follow First\")\\nplot( precis(m16.2,2) , labels=p_labels )\\n5 Follow First\\n4 Random\\n3 Maverick\\n2 Minority\\n1 Majority\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\nValue\\nRecall that 45% of the sample chose the majority color. But the posterior distribution is\\nconsistent with somewhere between 20% and 30% of children following the majority copying\\nstrategy. Conditional on this model, a similar proportion just copied the first color that was\\ndemonstrated. This is what hidden state models can do for us—prevent us from confusing\\nbehavior with strategy.\\nThis model can be extended to allow the probabilities of each strategy to vary by age,\\ngender, or anything else. In principle, this is easy—you just make ps conditional on the pre-\\ndictor variables. In practice, there are coding decisions to make. I say more about this in the\\nOverthinking box below.\\nOverthinking: Stancode forthe Boxesmodel. A Stan model needs three “blocks” of code. I’ll explain\\neach in order. The first block is the data block. This block just names the observed variables and\\ndeclares their types. For this model, it looks like this:\\ndata{\\nint N;\\nint y[N];\\nint majority_first[N];\\n}\\nThe integer N is just a count of observed cases. It’s the number of rows in data(Boxes). Then the\\noutcome y and predictor majority_first are declared as integer vectors of length N. You could\\nhard-code the length as the number 629. But then you have to change the model code every time the\\nnumber of cases changes. The second block a Stan model needs is the parameters block. This is\\nlike the data block, but for unobserved variables. These are the variables that we get posterior samples\\nfor. In this model, it contains only the simplex p:\\nparameters{\\nsimplex[5] p;\\n}\\nThe third block is the heart, the model block. This block calculates the log-probability of the vari-\\nables, both observed (data) and unobserved (parameters). This is the numerator in Bayes’ theorem,\\nand Stan uses it to run the Hamiltonian simulation (see Chapter 9). I’ll take this block in pieces. At\\nthe top, we declare a vector to hold probability calculations for each strategy. We’ll reuse this vector\\non each row of the data, to compute different probabilities.\\nmodel{\\nvector[5] phi;\\nNext we assign the prior.\\n// prior\\n'},\n",
       " {'index': 554,\n",
       "  'number': 536,\n",
       "  'content': '536\\n16. GENERALIZED LINEAR MADNESS\\np ~ dirichlet( rep_vector(4,5) );\\nNow the heart of the matter. We loop over all rows. For each row i, we compute the log-probability\\nof the observed y[i]. Each strategy has its own if...then to assign the probability of the data, con-\\nditional on that strategy. This gives us:\\n// probability of data\\nfor ( i in 1:N ) {\\nif ( y[i]==2 ) phi[1]=1; else phi[1]=0; // majority\\nif ( y[i]==3 ) phi[2]=1; else phi[2]=0; // minority\\nif ( y[i]==1 ) phi[3]=1; else phi[3]=0; // maverick\\nphi[4]=1.0/3.0;\\n// random\\nif ( majority_first[i]==1 )\\n// follow first\\nif ( y[i]==2 ) phi[5]=1; else phi[5]=0;\\nelse\\nif ( y[i]==3 ) phi[5]=1; else phi[5]=0;\\nNow we need to include the p parameters. We do this by adding each log(ps) to the log-probabilities\\ncomputed above. Then we add the average probability to the target, which is just Stan’s name for\\nthe total log-probability.\\n// compute log( p_s * Pr(y_i|s) )\\nfor ( s in 1:5 ) phi[s] = log(p[s]) + log(phi[s]);\\n// compute average log-probability of y_i\\ntarget += log_sum_exp( phi );\\nThat log_sum_exp function computes the marginal log-probability of the data, log Pr(yi), as defined\\nin the main text. log_sum_exp takes the phi vector, which contains the individual log-probabilities\\nfor each strategy, and returns the logarithm of their sum on the probability scale. It’s used a lot in\\nStan models like this, models with discrete parameters.\\nTo modify the model to include predictor variables, there are many options. So falling back again\\non some real theory will help to focus the effort. The simplest sort of modification is to allow the p\\nsimplex to vary by some discrete category, like gender. In that case, we add the variable gender to\\nthe data block and add a dimension to p in the parameters block, like this:\\nsimplex[5] p[2];\\nAnd then in the model block, just index p by both strategy and gender with p[gender[i],s]:\\nfor ( s in 1:5 ) phi[s] = log(p[gender[i],s]) + log(phi[s]);\\nThis model is in the rethinking package as data(Boxes_model_gender). A continuous covariate\\nlike age presents many more choices. Gaussian processes, splines, polynomials can all manage the\\njob. Each must be coded a different way. The Stan model data(Boxes_model_age) shows a simple\\nlinear age trend example, in which each p is assigned a linear model on the logit scale, and these are\\ntransformed with multi-inverse-logit to the simplex scale. This is entirely geocentric. If you have a\\nstronger theory, it helps.\\n16.2.4. State space models. The Boxes model above resembles a broader class of model\\nknown as a state space model. These models posit multiple hidden states that produce\\nobservations. Typically the states are dynamic, changing over time. When the states are dis-\\ncrete categories, the model may be called a hidden Markov model (HMM). Many time\\nseries models are state space models, since the true state of the time series is not observed,\\nonly the noisy measures. There is an example later in this chapter.\\n16.3. Ordinary differential nut cracking\\nThe Panda nut has nothing to do with bears. It is a big, hard nut produced by the ever-\\ngreen tree Panda oleosa. People have been eating delicious Panda nuts for millennia, cracking\\nthem open with stone and steel tools. Other animals have a harder time getting into these\\nnuts. But the chimpanzees of Ivory Coast manage the same way people do, by using tools.\\n'},\n",
       " {'index': 555,\n",
       "  'number': 537,\n",
       "  'content': '16.3. ORDINARY DIFFERENTIAL NUT CRACKING\\n537\\nThe chimpanzees use stone and wooden hammers to open Panda nuts, and they do so with\\nhigh efficiency.\\nIn this section, we’re going to model the development of nut opening skill among these\\nchimpanzees. Let’s load the data and outline the project:\\nR code\\n16.9\\nlibrary(rethinking)\\ndata(Panda_nuts)\\nThese data are records of individual bouts of nut opening.235 Each row is an individual-bout\\npair. The variables of immediate interest are the outcome nuts_opened, the duration pf the\\nbout in seconds, and the individual’s age. The research question is how nut opening skill\\ndevelops and which factors contribute to it. One reason to care about this question is that\\ntool use in primates is very rare. Yet humans cannot live without tools. How did we end up\\nthis way? Understanding the evolution of human technology benefits from species compar-\\nisons that tease apart the relative contributions of cognition, dexterity, social learning, and\\nstrength. We’re not going to achieve all that in this section. But we will get started. And we\\nwon’t use a GLM.\\n16.3.1. Scientific model. We need a generative model of nut opening rate as it varies by age.\\nLet’s consider the dumbest model, which is nevertheless smarter than a GLM. Suppose the\\nonly factor that matters is the individual’s strength. As the individual ages, it gets stronger\\nand nut opening rate increases. Obviously the ape needs some knowledge, but we’ll assume\\nthis comes easy and that body strength is the limiting factor. If the model does a poor job,\\nthen we’ll have a good reason to reconsider this assumption.\\nIn animals with terminal growth—they reach a stable adult body mass—size increases in\\nproportion to the distance remaining to maximum size. This implies that the instantaneous\\nrate of change in mass with age t is:\\ndM\\ndt = k(Mmax −Mt)\\nwhere k is a parameter that measures the rate of skill gain with age. The equation above tells\\nus how fast mass changes at any given age. But we need a formula for the mass at a given age.\\nSolving differential equations is beyond the level of this book. But you don’t actually have\\nto know how to solve it—any computer algebra system can do it. This particular differential\\nequation is actually a biology classic,236 and its solution is:\\nMt = Mmax\\n\\x001 −exp(−kt)\\n\\x01\\nWe’ll plot this function later, when we do prior predictive simulations. It makes decelerating\\ncurves that level off at Mmax. If you want to glance ahead, examples are shown on the left in\\nFigure 16.4 (page 540).\\nWe actually care about strength. Mass isn’t strength. So suppose now that strength is\\nproportional to mass: St = βMt. The parameter β simply measures the proportionality.\\nNow we need some way to relate strength to the rate of nut cracking. We could assume it\\ntoo is simply proportional. But consider that strength helps in at least three ways. First, it\\nlet’s the animal lift a heavier hammer. Heavier hammers have greater momentum. Second, it\\nlet’s the animal accelerate the hammer faster than gravity. Third, stronger animals also have\\nlonger limbs, which gives them more efficient levers. So it makes sense to assume increasing\\n'},\n",
       " {'index': 556,\n",
       "  'number': 538,\n",
       "  'content': '538\\n16. GENERALIZED LINEAR MADNESS\\nreturns to strength. Mathematically, this implies a function for the rate of nut opening like:\\nλ = αSθ\\nt = α\\n\\x00βMmax(1 −exp(−kt))\\n\\x01θ\\nwhere θ is some exponent greater than 1. A realistic implication of assuming increasing\\nreturns to strength is that there will be a threshold below which an individual cannot open a\\nsingle nut in reasonable time. The new parameter α expresses the proportionality of strength\\nto nut opening. It translates Newtons of force into nuts per second.\\nNow we have a function for the rate of nuts opened, λ. But it is a soup of parameters.\\nWe can simplify it, however. First, we can just rescale body mass Mmax so that it equals 1.\\nThis might seem like cheating. But measurement scales are arbitrary. So making Mmax = 1\\njust sets the measurement scale. Doing this gives us:\\nλ = αβθ(1 −exp(−kt))θ\\nThe product αβθ in the front just rescales strength to nuts-opened-per-second. So we can\\nreplace it with a single parameter:\\nλ = ϕ(1 −exp(−kt))θ\\nThat’s much better. One cost to this simplification is that it has hidden some useful facts. For\\nexample, average adult mass differs for males and females. An adult male chimpanzee can\\nbe 10 kilograms heavier than an adult female chimpanzee. You’ll attempt to express that fact\\nin a practice problem at the end of the chapter.\\n16.3.2. Statistical model. To use the model above for estimation, we need a likelihood func-\\ntion and priors. The likelihood is straightforward. If the number of nuts opened is far less\\nthan the number of available nuts, then the Poisson distribution has the right constraints.\\nThis gives us:\\nni ∼Poisson(λi)\\nλi = diϕ(1 −exp(−kti))θ\\nwhere ni is the number of nuts opened, di is the duration spent opening nuts, and ti is the\\nindividual’s age on observation i. The only thing we’ve added is the exposure di. Back in\\nChapter 11, we added an exposure to a Poisson model by adding the log of the duration to\\nthe linear model. We don’t use the log here, because the model isn’t linear and has no log link\\nfunction. We are coding the rate λ directly. So the duration di just multiplies the rate to give\\nus the expected number of nuts opened. It is like if I told you that I can open λ = 0.4 nuts\\nper second. To calculate how many nuts I could open in d = 10 seconds, you just multiply\\n0.4 by 10 to get 4 nuts per 10 seconds.\\nWhat about priors? To get sensible priors here, we need to consider relevant biological\\nfacts and also simulate to see how to translate those facts into distributional assumptions.\\nThe most relevant fact is that a chimpanzee reaches adult mass around 12 years of age. So\\nthe prior growth curves need to plateau around 12. We need distributions for k and θ that\\naccomplish this. And then the prior for ϕ should have a mean around the maximum rate\\nof nut opening. I am not really an expert on nut opening. But let’s suppose a professional\\nchimpanzee could open one nut per second—several nuts can be pounded at once.\\n'},\n",
       " {'index': 557,\n",
       "  'number': 539,\n",
       "  'content': '16.3. ORDINARY DIFFERENTIAL NUT CRACKING\\n539\\nHere are my suggestions for priors:\\nϕ ∼Log-Normal(log(1), 0.1)\\nk ∼Log-Normal(log(2), 0.25)\\nθ ∼Log-Normal(log(5), 0.25)\\nAll three are Log-Normal, because all three parameters have to be positive and continuous.\\nWe can simulate from these priors and draw the implied prior growth and rate curves.\\nR code\\n16.10\\nN <- 1e4\\nphi <- rlnorm( N , log(1) , 0.1 )\\nk <- rlnorm( N , log(2), 0.25 )\\ntheta <- rlnorm( N , log(5) , 0.25 )\\n# relative grow curve\\nplot( NULL , xlim=c(0,1.5) , ylim=c(0,1) , xaxt=\"n\" , xlab=\"age\" ,\\nylab=\"body mass\" )\\nat <- c(0,0.25,0.5,0.75,1,1.25,1.5)\\naxis( 1 , at=at , labels=round(at*max(Panda_nuts$age)) )\\nfor ( i in 1:20 ) curve( (1-exp(-k[i]*x)) , add=TRUE , col=grau() , lwd=1.5 )\\n# implied rate of nut opening curve\\nplot( NULL , xlim=c(0,1.5) , ylim=c(0,1.2) , xaxt=\"n\" , xlab=\"age\" ,\\nylab=\"nuts per second\" )\\nat <- c(0,0.25,0.5,0.75,1,1.25,1.5)\\naxis( 1 , at=at , labels=round(at*max(Panda_nuts$age)) )\\nfor ( i in 1:20 ) curve( phi[i]*(1-exp(-k[i]*x))^theta[i] , add=TRUE ,\\ncol=grau() , lwd=1.5 )\\nThe plots are displayed in Figure 16.4. It will help to inspect the distribution of each pa-\\nrameter with dens(). But these plots that combine all of the parameters are essential for\\nunderstanding their implications.\\nCoding this model presents no new problems. We just build the usual data list and ex-\\npress the likelihood and priors in ulam:\\nR code\\n16.11\\ndat_list <- list(\\nn = as.integer( Panda_nuts$nuts_opened ),\\nage = Panda_nuts$age / max(Panda_nuts$age),\\nseconds = Panda_nuts$seconds )\\nm16.4 <- ulam(\\nalist(\\nn ~ poisson( lambda ),\\nlambda <- seconds*phi*(1-exp(-k*age))^theta,\\nphi ~ lognormal( log(1) , 0.1 ),\\nk ~ lognormal( log(2) , 0.25 ),\\ntheta ~ lognormal( log(5) , 0.25 )\\n), data=dat_list , chains=4 )\\n'},\n",
       " {'index': 558,\n",
       "  'number': 540,\n",
       "  'content': '540\\n16. GENERALIZED LINEAR MADNESS\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nage\\nbody mass\\n0\\n4\\n8\\n12\\n16\\n20\\n24\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nage\\nnuts per second\\n0\\n4\\n8\\n12\\n16\\n20\\n24\\nFigure 16.4. Prior predictive simulation for the nut opening model. Left:\\nPrior growth curves, normalizing average adult mass to 1. This prior tries\\nto start leveling off around age 12, like a real chimpanzee. Right: Prior nut\\nopening rates. This prior allows many different patterns. But they are all\\nincreasing with age and assume that baby chimpanzees cannot open nuts.\\nNow do your duty by checking the chain diagnostics. The marginal distribution of each\\nparameter isn’t as interesting here as the posterior developmental curve. So let’s go straight\\nto producing that.\\nR code\\n16.12\\npost <- extract.samples(m16.4)\\nplot( NULL , xlim=c(0,1) , ylim=c(0,1.5) , xlab=\"age\" ,\\nylab=\"nuts per second\" , xaxt=\"n\" )\\nat <- c(0,0.25,0.5,0.75,1,1.25,1.5)\\naxis( 1 , at=at , labels=round(at*max(Panda_nuts$age)) )\\n# raw data\\npts <- dat_list$n / dat_list$seconds\\npoint_size <- normalize( dat_list$seconds )\\npoints( jitter(dat_list$age) , pts , col=rangi2 , lwd=2 , cex=point_size*3 )\\n# 30 posterior curves\\nfor ( i in 1:30 ) with( post ,\\ncurve( phi[i]*(1-exp(-k[i]*x))^theta[i] , add=TRUE , col=grau() ) )\\nThe result is shown in Figure 16.5. The blue points are the raw data, with size scaled by\\nthe duration of each observation. The curves are 30 skill curves drawn from the posterior\\ndistribution. These curves level off around the age of maximum body size, consistent with\\nthe idea that strength is the main limiting factor. This doesn’t mean that there isn’t knowledge\\ninvolved. There is still plenty of variation to explain.\\n16.3.3. Covariates and individual differences. The model here is stupidly simple. But it\\nis a scientifically reasonable start. You could extend it to include covariates like sex and\\n'},\n",
       " {'index': 559,\n",
       "  'number': 541,\n",
       "  'content': '16.4. POPULATION DYNAMICS\\n541\\n0.0\\n0.5\\n1.0\\n1.5\\nage\\nnuts per second\\n0\\n4\\n8\\n12\\n16\\nFigure 16.5. Posterior predictive distribu-\\ntion for the nut opening model. Blue points\\nare raw data, number opened divided by sec-\\nonds. Point size is proportional to the dura-\\ntion of that observation. The curves are 30\\ndraws from the posterior distribution.\\nindividual differences in strength. There are repeat observations of individuals, and even\\nrepeat observations across different years, that could be used to estimate individual varying\\neffects. The practice problems at the end of the chapter explore these applications.\\nNote for the moment that some of the model parameters make sense as varying by indi-\\nvidual while others do not. The scaling parameter θ for example is a feature of the physics,\\nnot of an individual. Which parameters are allowed to vary by individual is something to\\nbe decided by scientific knowledge of the parameters. And this is another reason to avoid\\nGLMs, so that the parameters have firmer scientific meaning.\\nYet another improvement to this model might be to use a more realistic model of chim-\\npanzee growth. There are detailed published growth curves for chimpanzees.237 Male chim-\\npanzees do experience a growth spurt around age 10. So their growth rate actually increases\\nshortly before reaching maximum size. Incorporating this into the model might help im-\\nprove predictions for males at least.\\n16.4. Population dynamics\\nIt all starts with radiation released by fusion reactions inside a dwarf star in a minor arm\\nof an insignificant spiral galaxy. Eight minutes away as the photon travels, on the third planet,\\nthat radiation allows clever plants to make sugar. Then the hare eats those clever plants and\\nsteals their sugar. The clever lynx eats the hare. Everyone is just eating starlight.\\nThe population of hares and lynx fluctuate over time, and understanding nature requires\\nunderstanding such fluctuations. The numbers of hares and lynx at any time influence the\\nnumbers in the near future. You might say that the most important cause of hares is hares.\\nBut predators, like the lynx, are also causes. To model phenomena like this, variables at one\\ntime influence the values of those same variables in the next.\\nIn this section, we’ll model a time series of hare and lynx populations.238 Load the data\\nand display it:\\nR code\\n16.13\\nlibrary(rethinking)\\ndata(Lynx_Hare)\\nplot( 1:21 , Lynx_Hare[,3] , ylim=c(0,90) , xlab=\"year\" ,\\nylab=\"thousands of pelts\" , xaxt=\"n\" , type=\"l\" , lwd=1.5 )\\n'},\n",
       " {'index': 560,\n",
       "  'number': 542,\n",
       "  'content': '542\\n16. GENERALIZED LINEAR MADNESS\\n0\\n20\\n40\\n60\\n80\\nyear\\nthousands of pelts\\n1900\\n1910\\n1920\\nLepus\\nLynx\\nFigure 16.6. Twenty years of lynx (Lynx canadensis) and hare (Lepus amer-\\nicanus) pelts, as recorded by the Hudson Bay Company.\\nat <- c(1,11,21)\\naxis( 1 , at=at , labels=Lynx_Hare$Year[at] )\\nlines( 1:21 , Lynx_Hare[,2] , lwd=1.5 , col=rangi2 )\\npoints( 1:21 , Lynx_Hare[,3] , bg=\"black\" , col=\"white\" , pch=21 , cex=1.4 )\\npoints( 1:21 , Lynx_Hare[,2] , bg=rangi2 , col=\"white\" , pch=21 , cex=1.4 )\\ntext( 17 , 80 , \"Lepus\" , pos=2 )\\ntext( 19 , 50 , \"Lynx\" , pos=2 , col=rangi2 )\\nFigure 16.6 displays this time series. These are odd data, records of pelts not live animals.239\\nThe number of hare pelts and number of lynx pelts seem to be related somehow. Both fluc-\\ntuate, but they seem to fluctuate together.\\nA common geocentric way to model a time series like this would be to use something\\ncalled an autoregressive model. In an autoregressive model, the value of the outcome\\nin the previous time step is called a lag variable and added as a predictor to the right side of\\na linear model. For example, we might model the mean number of hares at time t as:\\nE(Ht) = α + β1Ht−1\\nwhere Ht is the number of hares at time t. If β1 is less than 1, then hares tend to regress to\\nsome mean population size α. We could continue by adding an epicycle for the predator:\\nE(Ht) = α + β1Ht−1 + β2Lt−1\\nwhere Lt−1 is the number of lynx in the previous time period. Sometimes people add even\\ndeeper lags, like this:\\nE(Ht) = α + β1Ht−1 + β2Lt−1 + β3Ht−2\\nNow not only does the most recent population size Ht−1 predict the present, but so too does\\nthe population size two time periods ago Ht−2. Everything from prices to temperature to\\nwars has been modeled this way.\\n'},\n",
       " {'index': 561,\n",
       "  'number': 543,\n",
       "  'content': '16.4. POPULATION DYNAMICS\\n543\\nThere are several famous problems with autoregressive models, despite how often they\\nare used. They are surely generalized linear madness. First, nothing that happened two time\\nperiods ago causes the present, except through its influence on the state of the system one\\ntime period ago. So no lag beyond one period makes any causal sense. It’s pure predictive\\nepicycle. Of course some causal influences act slower than others. But that means you need\\nanother variable, not that the distance past can influence the present. Second, if the state of\\nthe system, Ht and Lt here, are measured with error, then the model is propagating error.\\nIt isn’t the observed Ht−1 that influences Ht, but rather the real unobserved Ht−1. In other\\nwords, what we really need is a state space model. Third, in most cases there is no bio-\\nlogical, economic, or physical interpretation of the parameters. Consider for example the α\\nintercept in the equations above. It implies that even when there are no hares, Ht−1 = 0,\\nthere can be α hares in the next period. Sometimes all this nonsense is okay, if all you care\\nabout is forecasting. But often these models don’t even make good forecasts, because getting\\nthe future right often depends upon having a decent causal model.\\nIt’s easy to do better, if you use a little science. In this section, we’ll model the hares and\\nlynx using an incredibly basic ecological model. In the process, you’ll see how to fit systems\\nof ordinary differential equations (ODEs) to data.\\n16.4.1. The scientific model. The hare population reproduces at a rate that depends upon\\nthe plants. And it shrinks at a rate that depends upon predators. Let Ht be the number of\\nhares at time t. Then we can assert that the rate of change in the hare population is:\\ndH\\ndt = Ht × (birth rate) −Ht × (death rate)\\nEverything is multiplied by Ht, because if there are no hares, then there are no births or\\ndeaths. Reproduction and death are per capita processes. The simplest ecological model\\nmakes birth and death rates constant. Let’s call the birth rate bH and the mortality rate mH.\\ndH\\ndt = HtbH −HtmH = Ht(bH −mH)\\nThe per capita growth rate is the difference between the birth rate and the death rate. I think\\nof this as the first law of ecology. Every model must include it in some form.\\nThe form we want to use here modifies the mortality rate so it also depends upon the\\npresence of a predator, the clever lynx. Let Lt be the number of lynx at time t. Then we can\\nwrite:\\ndH\\ndt = Ht(bH −LtmH)\\nSimilar logic gives us a similar equation for the rate of change in the lynx population:\\ndL\\ndt = Lt(HtbL −mL)\\nIn this case, it is birth that depends upon the other species and mortality that is a constant.\\nNow we have a model in which the population dynamics of the two species are deter-\\nmined by two coupled ordinary differential equations (ODEs). This isn’t a realistic model.\\nThe plants that hares eat are not constantly available, and lynx eat more than just hares. But\\nlet’s see how far we can get with this model, a biological one in which the parameters mean\\nsomething. Failures teach us.\\nThis particular model is a famous one, the Lotka-Volterra model.240 It models sim-\\nple predator-prey interactions and demonstrates several important things about ecological\\n'},\n",
       " {'index': 562,\n",
       "  'number': 544,\n",
       "  'content': '544\\n16. GENERALIZED LINEAR MADNESS\\ndynamics. Lots can be proved about it without using any data at all. For example, the pop-\\nulation tends to be unstable, cycling up and down like in Figure 16.6. This is interesting,\\nbecause it suggests that, while nature is more complicated, all that is necessary to see cyclical\\npopulation dynamics is captured in a stupidly simple model.\\nThe previous section also used a differential equation model. In that case we could ex-\\nplicitly solve it to get an expression for the value of the variable at any time t. We can’t do\\nthat here. These equations have no explicit solution that tells us which Ht and Lt to expect\\nat any time t. So how do we use them? We solve them numerically, through simulation. Let\\nme show you how. Then we’ll turn to making this into a statistical model.\\nA differential equation is just a way to update a variable. The equation dH/dt tells us how\\nto update H after each tiny unit of passing time dt. This means that once we have a value for\\nH, we can update it by just applying the equation dH/dt over and over again. Specifically, we\\nupdate like this:\\nHt+dt = Ht + dtdH\\ndt = Ht + dtHt(bH −LtmH)\\nWe do have to be careful how we do this, because math in a computer is tricky, as you’ve seen\\nbefore. In particular, the value we choose for dt needs to be small enough to provide a good\\napproximation of continuous time. But this tactic really does work. And it allows us to see\\nwhat the model implies, before we’ve fit it to data.\\nLet’s write a function to simulate lynx-hare dynamics. This function just needs to apply\\nthe strategy above to both H and L. Here’s some code that is hopefully easy to read:\\nR code\\n16.14\\nsim_lynx_hare <- function( n_steps , init , theta , dt=0.002 ) {\\nL <- rep(NA,n_steps)\\nH <- rep(NA,n_steps)\\nL[1] <- init[1]\\nH[1] <- init[2]\\nfor ( i in 2:n_steps ) {\\nH[i] <- H[i-1] + dt*H[i-1]*( theta[1] - theta[2]*L[i-1] )\\nL[i] <- L[i-1] + dt*L[i-1]*( theta[3]*H[i-1] - theta[4] )\\n}\\nreturn( cbind(L,H) )\\n}\\nWe tell this function how long to simulate with n_steps, which initial population sizes to\\nuse with init, and which values of the parameters to use with theta. The time interval is\\ndt. I’ve set it to default to 0.002, which works in this example. But the right value in general\\ndepends upon the model and the parameters.\\nNow let’s use the function to simulate.\\nR code\\n16.15\\ntheta <- c( 0.5 , 0.05 , 0.025 , 0.5 )\\nz <- sim_lynx_hare( 1e4 , as.numeric(Lynx_Hare[1,2:3]) , theta )\\nplot( z[,2] , type=\"l\" , ylim=c(0,max(z[,2])) , lwd=2 , xaxt=\"n\" ,\\nylab=\"number (thousands)\" , xlab=\"\" )\\nlines( z[,1] , col=rangi2 , lwd=2 )\\nmtext( \"time\" , 1 )\\n'},\n",
       " {'index': 563,\n",
       "  'number': 545,\n",
       "  'content': '16.4. POPULATION DYNAMICS\\n545\\n0\\n10\\n20\\n30\\n40\\nnumber (thousands)\\ntime\\nFigure 16.7. Simulated population dynamics\\nfrom the lynx-hare model. Black: Hare pop-\\nulation. Blue: Lynx population. This model\\nproduces repeating cycles of predators and\\nprey.\\nThe result is displayed in Figure 16.7. The black curve is the hare population, and the blue\\nis the lynx population. This model produces cycles, similar to what we see in the data. The\\nmodel behaves this way, because lynx eat hares. Once the hares are eaten, the lynx begin to\\ndie off. Then the cycle repeats.\\n16.4.2. The statistical model. To turn the lynx-hare model into a statistical analysis, we\\nneed to connect the deterministic population dynamics to the observed data. Observed data\\nhave many reasons not to exactly match a deterministic expectation. The most obvious is\\nthat we never get to count every hare and lynx. We just have partial samples. So we need to\\nmodel both the underlying population dynamics and the observation process.\\nLet Ht and Lt as before represent the numbers of hares and lynx at time t. And now\\nlet ht and ℓt represent the observed numbers of hares and lynx. While Ht causes Ht+dt, the\\nobserved ht does not cause anything. It’s just a pale reflection of the unobserved state of the\\nsystem at time t. We have to use a statistical model to project it back to the underlying model\\nof Ht and Lt. Then we can make a prediction for ht+dt and ℓt+dt.\\nTo do this, we need to assign an error distribution to the observation process. To do this\\nin a principled way, we should outline the generative process that goes from the true state of\\nnature, Ht, to the measurement, ht. First, hares get trapped. Suppose each hare is trapped\\nwith some probability pt which varies year to year, for all sorts of reasons. Third, the actual\\nnumber of pelts were rounded to the nearest 100 and divided by 1000. So they are no longer\\ncounts exactly. This all sounds like a mess. That’s measurement for you.\\nWe can do this though. Suppose for example there is a population of Ht = 104 hares.\\nSuppose also that the annual trapping rate varies according to a beta distribution pt ∼\\nBeta(2, 18). This means the average is 10%, but it is very rarely double that. We get a bi-\\nnomial count of pelts sampled for the population of hares, and then that is rounded to the\\nnearest 100 and divided by 1000. Let’s see what this sort of distribution looks like:\\nR code\\n16.16\\nN <- 1e4\\nHt <- 1e4\\np <- rbeta(N,2,18)\\nh <- rbinom( N , size=Ht , prob=p )\\nh <- round( h/1000 , 2 )\\n'},\n",
       " {'index': 564,\n",
       "  'number': 546,\n",
       "  'content': '546\\n16. GENERALIZED LINEAR MADNESS\\n0\\n1\\n2\\n3\\n4\\n5\\n0.0\\n0.2\\n0.4\\n0.6\\nthousand of pelts\\nDensity\\nFigure 16.8. Simulated distribution for the\\nobservation model in which trapping proba-\\nbility varies from year to year. In this case, a\\nwide range of pelt counts are consistent with\\nthe same true population size. This makes in-\\nference about population size difficult.\\ndens( h , xlab=\"thousand of pelts\" , lwd=2 )\\nI show this density in Figure 16.8. The variation in pt leads to a skewed error distribution.\\nTry changing Ht and the distribution of p in the code above and see how the distribution\\nchanges.\\nThere are several reasonable ways to approximate this distribution. We could for example\\njust use a Log-Normal distribution. It has the right constraints and skew. For example:\\nht ∼Log-Normal(log(pHt), σH)\\nThis gives ht a median of pHt, the expected proportion of the hare population that is trapped.\\nThe parameter σH controls the dispersion. An important fact about this measurement pro-\\ncess is that there is no good way to estimate p, not without lots of data at least. So we’re going\\nto just fix it with a strong prior. If this makes you uncomfortable, notice that the model has\\nforced us to realize that we cannot do any better than relative population estimates, unless\\nwe have a good way to know p. A typical time series model would just happily spin on its\\nepicycles, teaching us nothing this useful.\\nWe will ignore rounding error, since it is at most 50/4000 = 0.0125 = 1.25% of the pelt\\ncount. But if you are curious how to incorporate rounding into a statistical model, see the\\nOverthinking box later on. It isn’t hard to do—we just think generatively and that provides\\nthe solution.\\nLet’s lay out the full statistical model now. First we have the probabilities of the observed\\nvariables, the pelts:\\nht ∼Log-Normal(log(pHHt), σH)\\n[Prob observed hare pelts]\\nℓt ∼Log-Normal(log(pLLt), σL)\\n[Prob observed lynx pelts]\\n'},\n",
       " {'index': 565,\n",
       "  'number': 547,\n",
       "  'content': '16.4. POPULATION DYNAMICS\\n547\\nThen we need to define the unobserved variables. Let’s start with the unobserved population\\nsizes of lynx Lt and hare Ht.\\nH1 ∼Log-Normal(log 10, 1)\\n[Prior for initial hare population]\\nL1 ∼Log-Normal(log 10, 1)\\n[Prior for initial lynx population]\\nHT>1 = H1 +\\nZ T\\n1\\nHt(bH −mHLt)dt\\n[Model for hare population]\\nLT>1 = L1 +\\nZ T\\n1\\nLt(bLHt −mL)dt\\n[Model for lynx population]\\nThe first two lines above assign priors to the initial population sizes at time t = 1. In the\\nthird and fourth lines, the differential equation model defines all times after that, through\\nintegration. This just means summing up all the changes in the time interval to T. And\\nfinally all the parameters need priors.\\nσH ∼Exponential(1)\\n[Prior for measurement dispersion]\\nσL ∼Exponential(1)\\n[Prior for measurement dispersion]\\npH ∼Beta(αH, βH)\\n[Prior for hare trap probability]\\npL ∼Beta(αL, βL)\\n[Prior for lynx trap probability]\\nbH ∼Half-Normal(1, 0.5)\\n[Prior hare birth rate]\\nbL ∼Half-Normal(0.05, 0.05)\\n[Prior lynx birth rate]\\nmH ∼Half-Normal(0.05, 0.05)\\n[Prior hare mortality rate]\\nmL ∼Half-Normal(1, 0.5)\\n[Prior lynx mortality rate]\\nIn the problems at the end of the chapter, I’ll ask you to conduct prior predictive simulations\\nwith these priors.\\nNow we’re ready to start engineering the sampler. The obstacle in this model is com-\\nputing Ht and Lt for each time t. The differential equations define these variables, but our\\nsampler needs to numerically solve them on each iteration. So we need to write the numer-\\nical integration we did earlier, when we simulated the model, into our Bayesian sampler.\\nFortunately, Stan already has functions for solving differential equations. So this will be eas-\\nier than it sounds. The Stan User’s Guide (https://mc-stan.org) contains a full section on\\nprogramming this type of model, with several examples.\\nWe’ll do this model directly in Stan. You can load the Stan code and display it with:\\nR code\\n16.17\\ndata(Lynx_Hare_model)\\ncat(Lynx_Hare_model)\\nI won’t reproduce the entire model here. But I will point out the unusual pieces that handle\\nthe differential equations. The first unusual piece is at the top, the functions block. This is\\nan optional block that lets us write special calculations that we can use in the model. This is\\nwhere we put a function that computes the values of the differential equations. Look at the\\ncode—seriously, look at it—and you’ll see the dpop_dt function at the start of the model.\\nThe pop here is for population. This function returns the rates of change in the population.\\nIt takes as input the time t, the initial state of the population pop_init, and a vector of\\nparameters theta. Then it computes the rates of change in lynx and hares.\\n'},\n",
       " {'index': 566,\n",
       "  'number': 548,\n",
       "  'content': '548\\n16. GENERALIZED LINEAR MADNESS\\nThe model uses this function to determine the values of Ht and Lt. All we really have to do\\nis pass the name of the function and its inputs to Stan’s helpful integrate_ode_rk45 func-\\ntion. This function does the integration for us. In this model, we do this in the transformed\\nparameters block, so the results will appear as parameters in the posterior. But they are ac-\\ntually deterministic functions of the other parameters, the birth and mortality rates and the\\ninitial population sizes. The results are stored in a matrix called pop, which has a row for\\neach observed time point and a column for each species.\\nThe rest of the model is rather ordinary. The model block declares the priors and relates\\nthe solved equations to the observed data with:\\nfor ( t in 1:N )\\nfor ( k in 1:2 )\\npelts[t,k] ~ lognormal( log(pop[t,k]*p[k]) , sigma[k] );\\nThere is also code in generated quantities to go ahead and perform posterior predictive\\nsimulations. We’ll plot those after sampling.\\nNow we’re ready. Prepare the data list and fire up the engines:\\nR code\\n16.18\\ndat_list <- list(\\nN = nrow(Lynx_Hare),\\npelts = Lynx_Hare[,2:3] )\\nm16.5 <- stan( model_code=Lynx_Hare_model , data=dat_list , chains=3 ,\\ncores=3 , control=list( adapt_delta=0.95 ) )\\nAs always, check the chains. But sampling should be rapid and smooth. You could inspect the\\nparameters. Each has a biological meaning. But they all cooperate in a very non-linear way to\\nproduce the population dynamics, so it isn’t easy to read the dynamics from the individual\\nparameters. So let’s plot posterior predictions, at both the pelt (observed) and population\\n(unobserved) levels. For the pelts, this will plot the raw data and overlay 21 simulated time\\nseries from the posterior.\\nR code\\n16.19\\npost <- extract.samples(m16.5)\\npelts <- dat_list$pelts\\nplot( 1:21 , pelts[,2] , pch=16 , ylim=c(0,120) , xlab=\"year\" ,\\nylab=\"thousands of pelts\" , xaxt=\"n\" )\\nat <- c(1,11,21)\\naxis( 1 , at=at , labels=Lynx_Hare$Year[at] )\\npoints( 1:21 , pelts[,1] , col=rangi2 , pch=16 )\\n# 21 time series from posterior\\nfor ( s in 1:21 ) {\\nlines( 1:21 , post$pelts_pred[s,,2] , col=col.alpha(\"black\",0.2) , lwd=2 )\\nlines( 1:21 , post$pelts_pred[s,,1] , col=col.alpha(rangi2,0.3) , lwd=2 )\\n}\\n# text labels\\ntext( 17 , 90 , \"Lepus\" , pos=2 )\\ntext( 19 , 50 , \"Lynx\" , pos=2 , col=rangi2 )\\nThe result is shown in the top plot of Figure 16.9. The black points and trends are the hare\\npelts. The blue points and trends are the lynx pelts. Note the jaggedness of the predicted\\ntrends. This is a result of the model assuming uncorrelated measurement errors across time\\n'},\n",
       " {'index': 567,\n",
       "  'number': 549,\n",
       "  'content': '16.4. POPULATION DYNAMICS\\n549\\nFigure 16.9. Posterior predictions for the lynx-hare model. Top: Posterior\\npelts. The points are the data, black for hares and blue for lynx. Each trend\\nis a predicted time series from the posterior distribution. The jagged path is\\ncaused by uncorrelated measurement error. Bottom: Posterior populations.\\nUnlike the pelt predictions, these are smooth trajectories without measure-\\nment error.\\npoints. The underlying population may be smooth, but the measurements will not be. This\\nis an example of why it is almost always a mistake to model a time series as if observed data\\ncause observed data in the next time step. This is what is often done in autoregressive models.\\nBut if there is measurement error, and there always is, the data are emissions of some unseen\\nstate. The hidden states are the causes. The measurements don’t cause anything.\\nIt is helpful to compare the pelt predictions to the population predictions. So here are\\n21 simulations of population dynamics from the posterior:\\nR code\\n16.20\\nplot( NULL , pch=16 , xlim=c(1,21) , ylim=c(0,500) , xlab=\"year\" ,\\nylab=\"thousands of animals\" , xaxt=\"n\" )\\nat <- c(1,11,21)\\n'},\n",
       " {'index': 568,\n",
       "  'number': 550,\n",
       "  'content': '550\\n16. GENERALIZED LINEAR MADNESS\\naxis( 1 , at=at , labels=Lynx_Hare$Year[at] )\\nfor ( s in 1:21 ) {\\nlines( 1:21 , post$pop[s,,2] , col=col.alpha(\"black\",0.2) , lwd=2 )\\nlines( 1:21 , post$pop[s,,1] , col=col.alpha(rangi2,0.4) , lwd=2 )\\n}\\nThe result is the bottom plot in Figure 16.9. Compared to the pelt time series, these popu-\\nlation dynamics are smooth. There is a lot of uncertainty about population size, of course.\\nBut each trajectory connects smoothly, because there is no measurement error at this level.\\nThe differential equation model is deterministic, so it shows no stochasticity.\\n16.4.3. Lynx lessons. There are good reasons to doubt that this model is a good explanation\\nof the population dynamics of hares and lynx. While lynx really do depend almost exclu-\\nsively on hares at times, hares are eaten by lots of predators. So the hare cycles are probably\\nnot caused by the lynx. In other words, there is a confound lurking here. Real ecologies are\\ncomplicated. In the practice problems at the end, I’ll ask you to use this model on an exper-\\nimental predator-prey system that lacks all those complexities. I’ll also ask you to compare\\nan autoregressive model and see how many epicycles you need to approach the forecasting\\nquality of the simple predator-prey model.\\n16.5. Summary\\nThis chapter demonstrated four analyses in which a statistical model is motived directly\\nby a scientific model. This approach stands in contrast to the customary approach of going\\ndirectly from a vague scientific model, whether a DAG or just a bowl of variables, to a general-\\nized linear model. The goal was to illustrate both the advantages and challenges of translating\\nscientifically informed structural causal models into statistical machines. The goal was not\\nto persuade you to never use a generalized linear model. But hopefully it inspires you to see\\nthe use of a GLM as a decision in itself, not an obligation.\\n16.6. Practice\\nProblems are labeled Easy (E), Medium (M), and Hard (H).\\n16E1. What are some disadvantages of generalized linear models (GLMs)?\\n16E2. Can you think of one or more famous scientific models which do not have the additive struc-\\nture of a GLM?\\n16E3. Some models do not look like GLMs at first, but can be transformed through a logarithm into\\nan additive combination of terms. Do you know any scientific models like this?\\n16M1. Modify the cylinder height model, m16.1, so that the exponent 3 on height is instead a free\\nparameter. Do you recover the value of 3 or not? Plot the posterior predictions for the new model.\\nHow do they differ from those of m16.1?\\n16M2. Conduct a prior predictive simulation for the cylinder height model. Begin with the priors\\nin the chapter. Do these produce reasonable prior height distributions? If not, which modifications\\ndo you suggest?\\n16M3. Use prior predictive simulations to investigate the lynx-hare model. Begin with the priors in\\nthe chapter. Which population dynamics do these produce? Can you suggest any improvements to\\nthe priors, on the basis of your simulations?\\n'},\n",
       " {'index': 569,\n",
       "  'number': 551,\n",
       "  'content': '16.6. PRACTICE\\n551\\n16M4. Modify the cylinder height model to use a sphere instead of a cylinder. What choices do you\\nhave to make now? Is this a better model, on a predictive basis? Why or why not?\\n16H1. Modify the Panda nut opening model so that male and female chimpanzees have different\\nmaximum adult body mass. The sex variable in data(Panda_nuts) provides the information you\\nneed. Be sure to incorporate the fact that you know, prior to seeing the data, that males are on average\\nlarger than females at maturity.\\n16H2. Now return to the Panda nut model and try to incorporate individual differences. There are\\ntwo parameters, ϕ and k, which plausibly vary by individual. Pick one of these, allow it to vary by indi-\\nvidual, and use partial pooling to avoid overfitting. The variable chimpanzee in data(Panda_nuts)\\ntells you which observations belong to which individuals.\\n16H3. The chapter asserts that a typical, geocentric time series model might be one that uses lag vari-\\nables. Here you’ll fit such a model and compare it to the ODE model in the chapter. An autoregressive\\ntime series uses earlier values of the state variables to predict new values of the same variables. These\\nearlier values are called lag variables. You can construct the lag variables here with:\\nR code\\n16.21\\ndata(Lynx_Hare)\\ndat_ar1 <- list(\\nL = Lynx_Hare$Lynx[2:21],\\nL_lag1 = Lynx_Hare$Lynx[1:20],\\nH = Lynx_Hare$Hare[2:21],\\nH_lag1 = Lynx_Hare$Hare[1:20] )\\nNow you can use L_lag1 and H_lag1 as predictors of the outcomes L and H. Like this:\\nLt ∼Log-Normal(log µL,t, σL)\\nµL,t = αL + βLLLt−1 + βLHHt−1\\nHt ∼Log-Normal(log µH,t, σH)\\nµH,t = αH + βHHHt−1 + βHLLt−1\\nwhere Lt−1 and Ht−1 are the lag variables. Use ulam() to fit this model. Be careful of the priors of\\nthe α and β parameters. Compare the posterior predictions of the autoregressive model to the ODE\\nmodel in the chapter. How do the predictions differ? Can you explain why, using the structures of\\nthe models?\\n16H4. Adapt the autoregressive model to use a two-step lag variable. This means that Lt−2 and Ht−2,\\nin addition to Lt−1 and Ht−1, will appear in the equation for µ. This implies that prediction depends\\nupon not only what happened just before now, but also on what happened two time steps ago. How\\ndoes this model perform, compared to the ODE model?\\n16H5. Population dynamic models are typically very difficult to fit to empirical data. The lynx-hare\\nexample in the chapter was easy, partly because the data are unusually simple and partly because the\\nchapter did the difficult prior selection for you. Here’s another data set that will impress upon you\\nboth how hard the task can be and how badly Lotka-Volterra fits empirical data in general. The data\\nin data(Mites) are numbers of predator and prey mites living on fruit.241 Model these data using\\nthe same Lotka-Volterra ODE system from the chapter. These data are actual counts of individuals,\\nnot just their pelts. You will need to adapt the Stan code in data(Lynx_Hare_model). Note that the\\npriors will need to be rescaled, because the outcome variables are on a different scale. Prior predictive\\nsimulation will help. Keep in mind as well that the time variable and the birth and death parameters\\ngo together. If you rescale the time dimension, that implies you must also rescale the parameters.\\n'},\n",
       " {'index': 570, 'number': 552, 'content': ''},\n",
       " {'index': 571,\n",
       "  'number': 553,\n",
       "  'content': '17 Horoscopes\\nStatistics courses and books—this one included—tend to resemble horoscopes. There\\nare two senses to this resemblance. First, in order to remain plausibly correct, they must\\nremain tremendously vague. This is because the targets of the advice, for both horoscopes\\nand statistical advice, are diverse. But only the most general advice applies to all cases. A\\nhoroscope uses only the basic facts of birth to forecast life events, and a textbook statistical\\nguide uses only the basic facts of measurement and design to dictate a model. It is easy to do\\nbetter, once more detail is available. In the case of statistical analysis, it is typically only the\\nscientist who can provide that detail, not the statistician.242\\nSecond, there are strong incentives for both astrologers and statisticians to exaggerate\\nthe power and importance of their advice. No one likes an astrologer who forecasts doom,\\nand few want a statistician who admits the answers as desired are not in the data as collected.\\nScientists desire results, and they will buy and attend to statisticians and statistical procedures\\nthat promise them. What we end up with is too often horoscopic: vague and optimistic, but\\nstill claiming critical importance.243\\nStatistical inference is indeed critically important. But only as much as every other part\\nof research. Scientific discovery is not an additive process, in which sin in one part can\\nbe atoned by virtue in another. Everything interacts.244 So equally when science works as\\nintended as when it does not, every part of the process deserves our attention. Statistical\\nanalysis can neither be uniquely credited with science’s success, nor can it be uniquely blamed\\nfor its failures and follies.\\nAnd there are plenty of failures and follies. Science, you may have heard, is not perfect.\\nThe Lancet is one of the oldest and most prestigious medical journals in the world. This is\\nwhat its editor-in-chief, Richard Horton, wrote in its pages in 2015:245\\nThe case against science is straightforward: much of the scientific literature,\\nperhaps half, may simply be untrue. Afflicted by studies with small sam-\\nple sizes, tiny effects, invalid exploratory analyses, and flagrant conflicts of\\ninterest, together with an obsession for pursuing fashionable trends of du-\\nbious importance, science has taken a turn towards darkness.\\nRethinking: Mercury rising. If I should offer you horoscopic advice, this is what I’d say. Thinking\\ngeneratively—how the data could arise—solves many problems. Many statistical problems cannot\\nbe solved with statistics. All variables are measured with error. Conditioning on variables creates\\nas many problems as it solves. There is no inference without assumption, but do not choose your\\nassumptions for the sake of inference. Build complex models one piece at a time. Be critical. Be kind.\\n553\\n'},\n",
       " {'index': 572,\n",
       "  'number': 554,\n",
       "  'content': '554\\n17. HOROSCOPES\\nHow do we know that much of the published scientific literature is untrue? There are two\\nmajor methods.\\nFirst, it is hard to repeat many published findings, even those in the best journals.246\\nSome of this lack of repeatability arises from methodological subtleties, not because the find-\\nings are false. But many famous findings cannot be repeated, no matter who tries. There\\nis a sense in which this should be unsurprising, given the nature of statistical testing. But\\nthe high false-discovery rate has become a great concern, partly because many placed unre-\\nalistic faith in significance testing and partly because it is hugely expensive to try to develop\\ndrugs and therapies from unrepeatable medical findings. It is even more expensive to design\\npolicy around false nutritional, psychological, economic, or ecological discoveries.247 But\\nthe basic reputation of science is also at stake, all material costs aside. Why pay attention to\\nbreathlessly announced new discoveries, when as many as half of them turn out to be false?\\nSecond, the history of the sciences is equal parts wonder and blunder. The periodic table\\nof the elements looks impressive now, but its story is unglamorous. There were more false\\nelemental discoveries than there are current elements in the periodic table.248 Don’t think\\nthat all these false discoveries were performed by frauds and cranks. Enrico Fermi (1901–\\n1954) was one of the greatest physicists of the twentieth century. He discovered two heavy\\nelements, ausonium (Ao, atomic number 93) and hesperium (Es, atomic number 94). These\\natomic numbers are now assigned to neptunium and plutonium, because Fermi had not ac-\\ntually discovered either. He mistook a mix of lighter already-discovered elements. These\\nsorts of errors, and many other sorts of errors, were routine on the path to the current peri-\\nodic table. Its story is one of error, ego, fraud, and correction. Other sciences look similar.\\nPhilosophers of science actually have a term, the pessimistic induction, for the observation\\nthat because most science has been wrong, most science is wrong.249\\nHow can we reconcile such messy history, and widespread contemporary failure, with\\nobvious successes like General Relativity? Science is a population-level process of variation\\nand selective retention. It does not operate on individual hypotheses, but rather on popu-\\nlations of hypotheses. It comprises a mix of dynamics that may, over long periods of time,\\nreveal the clockwork of nature.250 But these same dynamics generate error. So it’s entirely\\npossible for most findings at any one point in time to be false but for science in the long\\nterm to still function. This is analogous to how natural selection can adapt a biological pop-\\nulation to its environment, even though most individual variation in any one generation is\\nmaladaptive.\\nWhat is included in these dynamics? Here’s a list of some salient pieces of the dynamic\\nof scientific discovery, in no particular order. You might make your own list here, as there’s\\nnothing special about mine.\\n(1) Quality of theory and predictions: If most theories are wrong, most findings will be\\nfalse positives. Karl Popper argued that all that matters for a theory to be scientific\\nis that it be falsifiable. But for science to be effective, we must require more of\\ntheory. There was a brief quantitative version of this argument on page 51. A good\\ntheory specifies precise predictions that provide precise tests, and more than one\\nmodel is usually necessary.\\n(2) Dynamics of funding: Who gets funded, and how does the process select for partic-\\nular forms of research? If there are no sources of long-term funding, then necessary\\nlong-term research will not be done. If people who already have funding judge who\\ngets new funding, research may become overly conservative and possibly corrupt.\\n'},\n",
       " {'index': 573,\n",
       "  'number': 555,\n",
       "  'content': '17. HOROSCOPES\\n555\\n(3) Quality of measurement: Research design matters, all agree; but often this is for-\\ngotten when interpreting results. A persistent problem is designs with low signal-\\nto-noise ratios.251 Poor signal will not mean no findings, just unreliable ones.\\n(4) Quality of data analysis: The topic of this book, but still much broader than it has\\nindicated. Many common practices in the sciences exacerbate false discovery.252 If\\nyou are not designing your analysis before you see the data, then your analysis may\\noverfit the data in ways that regularization cannot reliably address.\\n(5) Quality of peer review: Good pre-publication peer review is invaluable. But much\\nof it is not so good. Many mistakes get through, and many brilliant papers do not.\\nPeer review selects for hyperbole, since honestly admitting limitations of work only\\nhurts a paper’s chances. Is this nevertheless the best system we can devise? Let’s\\nhope not.\\n(6) Publication: We agonize over bias in measurement and statistical analysis, but then\\nallow it all back in during publication.253 Incentives for positive findings and news-\\nworthy results distort the design of research and how it is summarized.254\\n(7) Post-publication peer review: What happens to a finding after publication is just\\nas important as what happens before. It is common for invalid analyses to be pub-\\nlished in top-tier journals, only to be torn apart on blogs.255 But there is no system\\nfor linking published papers to later peer criticism, and there are few formal incen-\\ntives to conduct it. Even retracted papers continue to be cited.\\n(8) Replication and meta-analysis: The most important aspects of science are repeti-\\ntion and synthesis.256 No single study is definitive, but incentives to replicate and\\nsummarize are weaker than incentives to produce novel findings. Top-tier journals\\nprioritize news. But if the literature is biased, then aggregating the literature just\\nmagnifies bias.\\nWe tend to focus on the statistical analysis, perhaps because it is the only piece for which we\\nhave formulas and theorems. But every piece deserves attention and improvement. Sadly,\\nmany pieces are not under individual control, so social solutions are needed.\\nBut there is an aspect of science that you do personally control: openness. Pre-plan your\\nresearch together with the statistical analysis. Doing so will improve both the research design\\nand the statistics. Document it in the form of a mock analysis that you would not be ashamed\\nto share with a colleague. Register it publicly, perhaps in a simple repository, like Github or\\nany other. But your webpage will do just fine, as well. Then collect the data. Then analyze\\nthe data as planned. If you must change the plan, that’s fine. But document the changes and\\njustify them. Provide all of the data and scripts necessary to repeat your analysis. Do not\\nprovide scripts and data “on request,” but rather put them online so reviewers of your paper\\ncan access them without your interaction. There are of course cases in which full data cannot\\nbe released, due to privacy concerns. But the bulk of science is not of that sort.\\nThe data and its analysis are the scientific product. The paper is just an advertisement.\\nIf you do your honest best to design, conduct, and document your research, so that others\\ncan build directly upon it, you can make a difference.\\n'},\n",
       " {'index': 574, 'number': 556, 'content': ''},\n",
       " {'index': 575,\n",
       "  'number': 557,\n",
       "  'content': 'Endnotes\\nChapter 1\\n1. I draw this metaphor from Collins and Pinch (1998), The Golem: What You Should Know about Science. It is\\nvery similar to E. T. Jaynes’ 2003 metaphor of statistical models as robots, although with a less precise and more\\nmonstrous implication. [1]\\n2. There are probably no algorithms nor machines that never break, bend, or malfunction. A common citation\\nfor this observation is Wittgenstein (1953), Philosophical Investigations, section 193. Malfunction will interest\\nus, later in the book, when we consider more complex models and the procedures needed to fit them to data. [2]\\n3. See Mulkay and Gilbert (1981). I sometimes teach a PhD core course that includes some philosophy of sci-\\nence, and PhD students are nearly all shocked by how little their causal philosophy resembles that of Popper or\\nany other philosopher of science. The first half of Ian Hacking’s Representing and Intervening (1983) is probably\\nthe quickest way into the history of the philosophy of science. It’s getting out of date, but remains readable and\\nbroad minded. [4]\\n4. Maybe best to begin with Popper’s last book, The Myth of the Framework (1996). I also recommend interested\\nreaders to go straight to a modern translation of Popper’s earlier Logic of Scientific Discovery. Chapters 6, 8, 9\\nand 10 in particular demonstrate that Popper appreciated the difficulties with describing science as an exercise\\nin falsification. Other later writings, many collected in Objective Knowledge: An Evolutionary Approach, show\\nthat Popper viewed the generation of scientific knowledge as an evolutionary process that admits many different\\nmethods. [4]\\n5. Meehl (1967) observed that this leads to a methodological paradox, as improvements in measurement make\\nit easier to reject the null. But since the research hypothesis has not made any specific quantitative prediction,\\nmore accurate measurement doesn’t lead to stronger corroboration. See also Andrew Gelman’s comments in a\\nSeptember 5, 2014 blog post: http://andrewgelman.com/2014/09/05/confirmationist-falsificationist-paradigms-\\nscience/. [5]\\n6. George E. P. Box is famous for this dictum. As far as I can tell, his first published use of it was as a section\\nheading in a 1979 paper (Box, 1979). Population biologists like myself are more familiar with a philosophically\\nsimilar essay about modeling in general by Richard Levins, “The Strategy of Model Building in Population Biol-\\nogy” (Levins, 1966). [5]\\n7. Ohta and Gillespie (1996). [5]\\n8. Hubbell (2001). The theory has been productive in that it has forced greater clarity of modeling and under-\\nstanding of relations between theory and data. But the theory has had its difficulties. See Clark (2012). For a\\nmore general skeptical attitude towards “neutrality,” see Proulx and Adler (2010). [5]\\n9. For direct application of Kimura’s model to cultural variation, see for example Hahn and Bentley (2003). All\\nof the same epistemic problems reemerge here, but in a context with much less precision of theory. Hahn and\\nBentley have since adopted a more nuanced view of the issue. See their comment to Lansing and Cox (2011), as\\nwell as the similar comment by Feldman. [5]\\n10. Gillespie (1977). [5]\\n557\\n'},\n",
       " {'index': 576,\n",
       "  'number': 558,\n",
       "  'content': '558\\nENDNOTES\\n11. Lansing and Cox (2011). See objections by Hahn, Bentley, and Feldman in the peer commentary to the arti-\\ncle. [7]\\n12. See Cho (2011) for a December 2011 summary focusing on debates about measurement. [8]\\n13. For an autopsy of the experiment, see (posted 2012) http://profmattstrassler.com/articles-and-posts/particle-\\nphysics-basics/ neutrinos/neutrinos-faster-than-light/opera-what-went-wrong/. [9]\\n14. See Mulkay and Gilbert (1981) for many examples of “Popperism” from practicing scientists, including fa-\\nmous ones. [9]\\n15. For an accessible history of some measurement issues in the development of physics and biology, including\\nearly experiments on relativity and abiogenesis, I recommend Collins and Pinch (1998). Some scientists have\\nread this book as an attack on science. However, as the authors clarify in the second edition, this was not their in-\\ntention. Science makes myths, like all cultures do. That doesn’t necessarily imply that science does not work. See\\nalso Daston and Galison (2007), which tours concepts of objective measurement, spanning several centuries. [9]\\n16. The first chapter of Sober (2008) contains a similar discussion of modus tollens. Note that the statistical phi-\\nlosophy of Sober’s book is quite different from that of the book you are holding. In particular, Sober is weakly\\nanti-Bayesian. This is important, because it emphasizes that rejecting modus tollens as a model of statistical in-\\nference has nothing to do with any debates about Bayesian versus non-Bayesian tools. [9]\\n17. Popper himself had to deal with this kind of theory, because the rise of quantum mechanics in his lifetime\\npresented rather serious challenges to the notion that measurement was unproblematic. See Chapter 9 in his\\nLogic of Scientific Discovery, for example. [9]\\n18. See the Afterword to the 2nd edition of Collins and Pinch (1998) for examples of textbooks getting it wrong\\nby presenting tidy fables about the definitiveness of evidence. [10]\\n19. A great deal has been written about the sociology of science and the interface of science and public interest.\\nInterested novices might begin with Kitcher (2011), Science in a Democratic Society, which has a very broad top-\\nical scope and so can serve as an introduction to many dilemmas. [10]\\n20. Yes, even procedures that claim to be free of assumptions do have assumptions and are a kind of model. All\\nsystems of formal representation, including numbers, do not directly reference reality. For example, there is\\nmore than one way to construct “real” numbers in mathematics, and there are important consequences in some\\napplications. In application, all formal systems are like models. See http://plato.stanford.edu/entries/philosophy-\\nmathematics/ for a short overview of some different stances that can be sustained towards reasoning in mathe-\\nmatical systems. [10]\\n21. Most scholars trace frequentism to British logician John Venn (1834–1923), as for example presented in his\\n1876 book. Speaking of the proportion of male births in all births, Venn said, “probability is nothing but that\\nproportion” (page 84). Venn taught Fisher some of his maths, so this may be where Fisher acquired his opposi-\\ntion to Bayesian probability. Regardless, it seems to be a peculiar English invention. [11]\\n22. Fisher (1956). See also Fisher (1955), the first major section of which discusses the same point. Some peo-\\nple would dispute that Fisher was a “frequentist,” because he championed his own likelihood methods over the\\nmethods of Neyman and Pearson. But Fisher definitely rejected the broader Bayesian approach to probability\\ntheory. See Endnote 27. [11]\\n23. This last sentence is a rephrasing from Lindley (1971): “A statistician faced with some data often embeds it in\\na family of possible data that is just as much a product of his fantasy as is a prior distribution.” Dennis V. Lindley\\n(1923–2013) was a prominent defender of Bayesian data analysis when it had very few defenders. [11]\\n24. It’s hard to find an accessible introduction to image analysis, because it’s a very computational subject. At\\nthe intermediate level, see Marin and Robert (2007), Chapter 8. You can hum over their mathematics and still\\nacquaint yourself with the different goals and procedures. See also Jaynes (1984) for spirited comments on the\\nhistory of Bayesian image analysis and his pessimistic assessment of non-Bayesian approaches. There are better\\nnon-Bayesian approaches since. [11]\\n'},\n",
       " {'index': 577,\n",
       "  'number': 559,\n",
       "  'content': 'ENDNOTES\\n559\\n25. Binmore (2009) describes the history within economics and related fields and provides a critique that I am\\nsympathetic to. [12]\\n26. See Gigerenzer et al. (2004). [12]\\n27. Fisher (1925), page 9. See Gelman and Robert (2013) for reflection on intemperate anti-Bayesian attitudes\\nfrom the middle of last century. [13]\\n28. See McGrayne (2011) for a non-technical history of Bayesian data analysis. See also Fienberg (2006),\\nwhich describes (among many other things) applied use of Bayesian multilevel models in election prediction,\\nbeginning in the early 1960s. [13]\\n29. Silver (2012) calls overfitting the most important thing in statistics that you’ve never heard of. This re-\\nflects overfitting’s importance and how rarely it features in introductory statistics courses. Silver’s book is a\\nwell-written, non-technical survey of modeling and prediction in a range of domains. [13]\\n30. See Theobald (2010) for a fascinating example in which multiple non-null phylogenetic models are con-\\ntrasted. [14]\\n31. See Sankararaman et al. (2012) for a thorough explanation, including why current evidence suggests that\\nthere really was interbreeding. [14]\\n32. See Fienberg (2006), page 24. [16]\\n33. See Wang et al. (2015) for a vivid example. [16]\\n34. The biologist Sewall Wright (1889-1988) began developing his “path analysis” approach to causal inference\\nin genetics around the year 1918. See Wright 1921. The next largest contributions came from Donald Rubin’s\\npotential-outcomes approach Rubin (1974) and Judea Pearl’s more graphic approach (Pearl, 2000). A spirited,\\nopinionated, and accessible overview is given by Pearl in his 2018 book (Pearl and MacKenzie, 2018). [17]\\n35. Some philosophers and statisticians have held this view. Karl Pearson, one of the most important statisticians\\nof the twentieth century, wrote: “Beyond such discarded fundamentals as ‘matter’ and ‘force’ lies still another\\nfetish among the inscrutable arcana of modern science, namely, the category of cause and effect.” (Pearson,\\n1911, p. vi of 3rd edition) This quote is playful, but the book contains an entire chapter of “Contingency and\\nCorrelation” with a section titled “The Category of Association, as replacing Causation.” The general message\\nwas that “cause” is a primitive concept that science should grow beyond and replace with refined notions of\\nassociation and variation. [17]\\n36. The phrase “causal salad” comes from Jag Bhalla’s 2018 blog post: https://bigthink.com/errors-we-live-\\nby/judea-pearls-the-book-of-why-brings-news-of-a-new-science-of-causes.\\nThe post reviews Pearl and\\nMacKenzie (2018). [17]\\nChapter 2\\n37. Morison (1942). Globe illustration modified from public domain illustration at the Wikipedia entry for Mar-\\ntin Behaim. In addition to underestimating the circumference, Colombo also overestimated the size of Asia and\\nthe distance between mainland China and Japan. [19]\\n38. This distinction and vocabulary derive from Savage (1962). Savage used the terms to express a range of\\nmodels considering less and more realism. Statistical models are rarely large worlds. And smaller worlds can\\nsometimes be more useful than large ones. [19]\\n39. See Robert (2007) for thorough coverage of the decision-theoretic optimality of Bayesian inference. [19]\\n40. See Simon (1969) and chapters in Gigerenzer et al. (2000). [20]\\n41. See Cox (1946). Jaynes (2003) and Van Horn (2003) explain the Cox theorem and its role in inference.\\nSee also Skilling and Knuth (2019), which demonstrates how this view of probability theory unifies seemingly\\ndifferent domains. [24]\\n'},\n",
       " {'index': 578,\n",
       "  'number': 560,\n",
       "  'content': '560\\nENDNOTES\\n42. See Gelman and Robert (2013) for examples. [24]\\n43. I first encountered this globe tossing strategy in Gelman and Nolan (2002). Since I’ve been using it in class-\\nrooms, several people have told me that they have seen it in other places, but I’ve been unable to find a primeval\\ncitation, if there is one. [28]\\n44. There is actually a set of theorems, the No Free Lunch theorems. These theorems—and others which are\\nsimilar but named and derived separately—effectively state that there is no optimal way to pick priors (for\\nBayesians) or select estimators or procedures (for non-Bayesians).\\nSee Wolpert and Macready (1997) for\\nexample. [31]\\n45. This is a subtle point that will be expanded in other places. On the topic of accuracy of assumptions versus in-\\nformation processing, see e.g. Appendix A of Jaynes (1985): The Gaussian, or normal, error distribution needn’t\\nbe physically correct in order to be the most useful assumption. [32]\\n46. Kronecker (1823–1891), an important number theorist, was quoted as stating “God made the integers, all\\nelse is the work of humans” (Die ganzen Zahlen hat der liebe Gott gemacht, alles andere ist Menschenwerk).\\nThere appears to be no consensus among mathematicians about which parts of mathematics are discovered\\nrather than invented. But all admit that applied mathematical models are “the work of humans.” [32]\\n47. The usual non-Bayesian definition of “likelihood’ is a function of the parameters that is conditional on the\\ndata, written L(θ|y). Mathematically this function is indeed a probability distribution, but only over the data\\ny. In Bayesian statistics, it is fine to write f(y|θ), so it makes no sense to say the “likelihood” isn’t a probability\\ndistribution over the data. If you get confused, just remember that the mathematical function returns a number\\nthat has a specific meaning. That meaning, in this case, is the probability (or probability density) of the data,\\ngiven the parameters. [33]\\n48. This approach is usually identified with Bruno de Finetti and L. J. Savage. See Kadane (2011) for review and\\nexplanation. [35]\\n49. See Berger and Berry (1988), for example, for further exploration of these ideas. [35]\\nChapter 3\\n50. Gigerenzer and Hoffrage (1995). There is a large empirical literature, which you can find by searching for-\\nward on the Gigerenzer and Hoffrage paper. [50]\\n51. Feynman (1967) provides a good defense of this device in scientific discovery. [50]\\n52. For a binary outcome problem of this kind, the posterior density is given by dbeta(p,w+1,n-w+1), where\\np is the proportion of interest, w is the observed count of water, and n is the number of tosses. If you’re curious\\nabout how to prove this fact, look up “beta-binomial conjugate prior.” I avoid discussing the analytical approach\\nin this book, because very few problems are so simple that they have exact analytical solutions like this. [51]\\n53. See Ioannidis (2005) for another narrative of the same idea. The problem is possibly worse than the simple\\ncalculation suggests. On the other hand, real scientific inference is more subtle than mere truth or falsehood of\\nan hypothesis. I personally don’t like to frame scientific discovery in this way. But many, if not most, scientists\\ntend to think in such binary terms, so this calculation should be disturbing. [51]\\n54. I learned this term from Sander Greenland and his collaborators. See Amrhein et al. (2019) and Gelman and\\nGreenland (2019). [54]\\n55. Fisher (1925), in Chapter III within section 12 on the normal distribution. There are a couple of other places\\nin the book in which the same resort to convenience or convention is used. Fisher seems to indicate that the 5%\\nmark was already widely practiced by 1925 and already without clear justification. [56]\\n56. Fisher (1956). [56]\\n57. See Box and Tiao (1973), page 84 and then page 122 for a general discussion. [56]\\n58. Gelman et al. (2013), page 33, comment on differences between percentile intervals and HPDIs. [57]\\n'},\n",
       " {'index': 579,\n",
       "  'number': 561,\n",
       "  'content': 'ENDNOTES\\n561\\n59. See Henrion and Fischoff (1986) for examples from the estimation of physical constants, such as the speed\\nof light. [58]\\n60. Robert (2007) provides concise proofs of optimal estimators under several standard loss functions, like this\\none. It also covers the history of the topic, as well as many related issues in deriving good decisions from statis-\\ntical procedures. [60]\\n61. Rice (2010) presents an interesting construction of classical Fisherian testing through the adoption of loss\\nfunctions. [61]\\n62. See Hauer (2004) for three tales from transportation safety in which testing resulted in premature incorrect\\ndecisions and a demonstrable and continuing loss of human life. [61]\\n63. It is poorly appreciated that coin tosses are very hard to bias, as long as you catch them in the air. Once they\\nland and bounce and spin, however, it is very easy to bias them. [66]\\n64. E. T. Jaynes (1922–1998) said all of this much more succinctly: Jaynes (1985), page 351, “It would be very\\nnice to have a formal apparatus that gives us some ‘optimal’ way of recognizing unusual phenomena and\\ninventing new classes of hypotheses that are most likely to contain the true one; but this remains an art for the\\ncreative human mind.” See also Box (1980) for a similar perspective. [68]\\nChapter 4\\n65. Leo Breiman, at the start of Chapter 9 of his classic book on probability theory (Breiman, 1968), says “there\\nis really no completely satisfying answer” to the question “why normal?” Many mathematical results remain\\nmysterious, even after we prove them. So if you don’t quite get why the normal distribution is the limiting dis-\\ntribution, you are in good company. [73]\\n66. For the reader hungry for mathematical details, see Frank (2009) for a nicely illustrated explanation of this,\\nusing Fourier transforms. [73]\\n67. Technically, the distribution of sums converges to normal only when the original distribution has finite vari-\\nance. What this means practically is that the magnitude of any newly sampled value cannot be so big as to\\noverwhelm all of the previous values. There are natural phenomena with effectively infinite variance, but we\\nwon’t be working with any. Or rather, when we do, I won’t comment on it. [74]\\n68. The most famous non-technical book about this topic is Taleb (2007). This book has had a large impact.\\nThere is also a quite large technical literature on the topic. Note that the terms heavy tail and fat tail sometimes\\nhave precise technical definitions. [76]\\n69. A very nice essay by Pasquale Cirillo and Nassim Nicholas Taleb, “The Decline of Violent Conflicts: What\\nDo The Data Really Say?,” focuses on this issue. [76]\\n70. Howell (2010) and Howell (2000). See also Lee and DeVore (1976). Much more raw data is available for\\ndownload from https://tspace.library.utoronto.ca/handle/1807/10395. [79]\\n71. Jaynes (2003), page 21–22. See that book’s index for other mentions in various statistical arguments. [81]\\n72. See Jaynes (1986) for an entertaining example concerning the beer preferences of left-handed kangaroos.\\nThere is an updated 1996 version of this paper available online. [81]\\n73. The strategy is the same grid approximation strategy as before (page 39). But now there are two dimensions,\\nand so there is a geometric (literally) increase in bother. The algorithm is mercifully short, however, if not trans-\\nparent. Think of the code as being six distinct commands. The first two lines of code just establish the range of\\nµ and σ values, respectively, to calculate over, as well as how many points to calculate in-between. The third line\\nof code expands those chosen µ and σ values into a matrix of all of the combinations of µ and σ. This matrix\\nis stored in a data frame, post. In the monstrous fourth line of code, shown in expanded form to make it easier\\nto read, the log-likelihood at each combination of µ and σ is computed. This line looks so awful, because we\\nhave to be careful here to do everything on the log scale. Otherwise rounding error will quickly make all of the\\nposterior probabilities zero. So what sapply does is pass the unique combination of µ and σ on each row of\\npost to a function that computes the log-likelihood of each observed height, and adds all of these log-likelihoods\\n'},\n",
       " {'index': 580,\n",
       "  'number': 562,\n",
       "  'content': '562\\nENDNOTES\\ntogether (sum). In the fifth line, we multiply the prior by the likelihood to get the product that is proportional\\nto the posterior density. The priors are also on the log scale, and so we add them to the log-likelihood, which is\\nequivalent to multiplying the raw densities by the likelihood. Finally, the obstacle for getting back on the proba-\\nbility scale is that rounding error is always a threat when moving from log-probability to probability. If you use\\nthe obvious approach, like exp( post$prod ), you’ll get a vector full of zeros, which isn’t very helpful. This\\nis a result of R’s rounding very small probabilities to zero. Remember, in large samples, all unique samples are\\nunlikely. This is why you have to work with log-probability. The code in the box dodges this problem by scaling\\nall of the log-products by the maximum log-product. As a result, the values in post$prob are not all zero, but\\nthey also aren’t exactly probabilities. Instead they are relative posterior probabilities. But that’s good enough for\\nwhat we wish to do with these values. [85]\\n74. The most accessible of Galton’s writings on the topic has been reprinted as Galton (1989). [92]\\n75. See Reilly and Zeringue (2004) for an example using predator-prey dynamics.\\nWe’ll engage with this\\nexample in Chapter 16. [94]\\n76. The implied definition of α in a parabolic model is α = E yi −β1 E xi −β2 E x2\\ni . Now even when the average\\nxi is zero, E xi = 0, the average square will likely not be zero. So α becomes hard to directly interpret again. [112]\\n77. For much more discussion of knot choice, see Fahrmeir et al. (2013) and Wood (2017). A common approach\\nis to use Wood’s knot choice algorithm as implemented by default in the R package mgcv. [117]\\n78. A very popular and comprehensive text is Wood (2017). [120]\\nChapter 5\\n79. “How to Measure a Storm’s Fury One Breakfast at a Time.” The Wall Street Journal: September 1, 2011. [123]\\n80. See Meehl (1990), in particular the “crud factor” described on page 204. [123]\\n81. Debates about causal inference go back a long time. David Hume is key citation. One curious obstacle\\nin modern statistics is that classic causal reasoning requires that if A causes B, then B will always appear\\nwhen A appears. But with probabilistic relationships, like those described in most contemporary scientific\\nmodels, it is unsurprising to talk about probabilistic causes, in which B only sometimes follows A. See\\nhttp://plato.stanford.edu/entries/causation-probabilistic/. [124]\\n82. See Pearl (2014) for an accessible introduction, with discussion. See also Rubin (2005) for a related approach.\\nAn important perspective missing in these is an emphasis on rigorous scientific models that make precise\\npredictions. This tension builds throughout the book and asserts itself in Chapter 16. [124]\\n83. See Freckleton (2002). [137]\\n84. Data from Table 2 of Hinde and Milligan (2011). [144]\\n85. See Decety et al. (2015) for the original and retraction notice. See Shariff et al. (2016) for the reanalysis. [153]\\n86. See Gelman and Stern (2006) for further explanation, and see Nieuwenhuis et al. (2011) for some evidence\\nof how commonly this mistake occurs. [158]\\nChapter 6\\n87. This example is joint work with Paul Smaldino. I think we sketched it on a napkin at a conference in Jena,\\nGermany in 2017. [161]\\n88. See Berkson (1946) A related phenomenon is range restriction that results from selection, which reduces the\\ncorrelation between criteria and subsequent performance. This is one reason that standardized test scores do\\nnot correlate with success in school. They might also just not predict success at all. But even if they did, it’s not\\nsurprising that they are uncorrelated with success after selection. See Dawes (1975). [161]\\n89. Rosenbaum (1984) calls it concomitant variable bias. See also Chapter 9 in Gelman and Hill (2007). There\\nisn’t really any standard terminology for this issue. It is a component of generalized mediation analysis, and some\\nfields discuss it under that banner. [170]\\n'},\n",
       " {'index': 581,\n",
       "  'number': 563,\n",
       "  'content': 'ENDNOTES\\n563\\n90. See Pearl (2016), chapter 2. You’ll often see the “d” in d-separation defined as “dependency.” That would\\ncertainly make more sense. But the term d-separation comes from a more general theory of graphs. Directed\\ngraphs involve d-separation and undirected graphs involve instead u-separation. Anyway, if you want to call it\\n“dependency separation,” I won’t mind. [174]\\n91. Montgomery et al. (2018) found that almost half of experimental studies in three top Political Science jour-\\nnals conditioned on post-treatment variables, despite the fact that most political science programs warn against\\nthis. The paper contains a number of examples to help you think through post-treatment conditioning. [175]\\n92. I learned this example from Dr. Julia Rohrer. See her 2017 blog post http://www.the100.ci/2017/04/21/whats-\\nan-age-effect-net-of-all-time-varying-covariates/ as well as the papers Rohrer (2017) and Glenn (2009). [176]\\n93. This example is from Breen (2018). [180]\\n94. See Pearl (2014). [183]\\n95. This definition is actually a little too narrow. Experimental manipulation is not required, just blocking of\\nnon-causal paths. [183]\\n96. See Blom et al. (2018). [188]\\n97. See Pearl (2000), as well as Pearl and MacKenzie (2018). [188]\\nChapter 7\\n98. De Revolutionibus, Book 1, Chapter 10. [191]\\n99. See e.g. Akaike (1978), as well as discussion in Burnham and Anderson (2002). [193]\\n100. When priors are flat and models are simple, this will always be true. But later in the book, you’ll work with\\nother types of models, like multilevel regressions, for which adding parameters does not necessarily lead to bet-\\nter fit to sample. [194]\\n101. Data from Table 1 of McHenry and Coffing (2000). [194]\\n102. Gauss 1809, Theoria motus corporum coelestium in sectionibus conicis solem ambientum. [196]\\n103. See Grünwald (2007) for a book-length treatment of these ideas. [201]\\n104. There are many discussions of bias and variance in the literature, some much more mathematical than\\nothers. For a broad treatment, I recommend Chapter 7 of Hastie, Tibshirani and Friedman’s 2009 book, which\\nexplores BIC, AIC, cross-validation and other measures, all in the context of the bias-variance trade-off. [201]\\n105. I first encountered this kind of example in Jaynes (1976), page 246. Jaynes himself credits G. David Forney’s\\n1972 information theory course notes. Forney is an important figure in information theory, having won several\\nawards for his contributions. [203]\\n106. As of 2019, calibration and Brier scores are available online https://projects.fivethirtyeight.com/checking-\\nour-work/.\\nSilver (2012) contains a chapter, Chapter 4, that unfortunately pushes calibration as the most\\nimportant diagnostic for prediction. There is a more nuanced endnote, however, that makes the same point as I\\ndo in the Rethinking box. [204]\\n107. Calibration makes sense to frequentists, who define probability as objective frequency. Among Bayesians,\\nin contrast, there is no agreement.\\nStrictly speaking, there are no “true” probabilities of events, because\\nprobability is epistemological and nature is deterministic. See Jaynes (2003), Chapter 9. Gneiting et al. (2007)\\nprovide a flexible definition: Consistency between the distributional forecasts and the observations. They\\ndevelop a useful approach, but they admit it has a “frequentist flavour” (page 264). No one recommends\\nclaiming that predictions are good, just because they are calibrated. [204]\\n108. Shannon (1948). For a more accessible introduction, see the venerable textbook Elements of Information\\nTheory, by Cover and Thomas. Slightly more advanced, but having lots of added value, is Jaynes’ (2003, Chapter\\n'},\n",
       " {'index': 582,\n",
       "  'number': 564,\n",
       "  'content': '564\\nENDNOTES\\n11) presentation. A foundational book in applying information theory to statistical inference is Kullback (1959),\\nbut it’s not easy reading. [205]\\n109. See two famous editorials on the topic: Shannon (1956) and Elias (1958). Elias’ editorial is a clever work\\nof satire and remains as current today as it was in 1958. Both of these one-page editorials are readily available\\nonline. [205]\\n110. I really wish I could say there is an accessible introduction to maximum entropy, at the level of most natural\\nand social scientists’ math training. If there is, I haven’t found it yet. Jaynes (2003) is an essential source, but if\\nyour integral calculus is rusty, progress will be very slow. Better might be Steven Frank’s papers (2009; 2011)\\nthat explain the approach and relate it to common distributions in nature. You can mainly hum over the maths\\nin these and still get the major concepts. See also Harte (2011), for a textbook presentation of applications in\\necology. [207]\\n111. Kullback and Leibler (1951). Note however that Kullback and Leibler did not name this measure after\\nthemselves. See Kullback (1987) for Solomon Kullback’s reflections on the nomenclature. For what it’s worth,\\nKullback and Leibler make it clear in their 1951 paper that Harold Jeffreys had used this measure already in the\\ndevelopment of Bayesian statistics. [207]\\n112. In non-Bayesian statistics, under somewhat general conditions, a difference between two deviances has a\\nchi-squared distribution. The factor of 2 is there to scale it the proper way. Wilks (1938) is the usually primordial\\ncitation. [210]\\n113. See Zhang and Yang (2015). [217]\\n114. Gelfand (1996). [217]\\n115. Vehtari et al. (2016). [217]\\n116. See Gelfand (1996). There is also a very clear presentation in Magnusson et al. (2019). [218]\\n117. See Vehtari et al. (2019b). [218]\\n118. Akaike (1973). See also Akaike (1974, 1978, 1981a), where AIC was further developed and related to\\nBayesian approaches. Ecologists tend to know about AIC from Burnham and Anderson (2002). [219]\\n119. A common approximation in the case of small N is AICc = Dtrain +\\n2k\\n1−(k+1)/N. As N grows, this expression\\napproaches AIC. See Burnham and Anderson (2002). [219]\\n120. Lunn et al. (2013) contains a fairly understandable presentation of DIC, including a number of different\\nways to compute it. [219]\\n121. Quote in Akaike (1981b). [219]\\n122. Watanabe (2010). Gelman et al. (2014) re-dub WAIC the “Watanabe-Akaike Information Criterion” to give\\nexplicit credit to Watanabe, in the same way people renamed AIC after Akaike. Gelman et al. (2014) is worth-\\nwhile also for the broad perspective it takes on the inference problem. [220]\\n123. There was a tribal exchange over this issue in 2018. See Gronau and Wagenmakers (2019) and Vehtari et al.\\n(2019c). The exchange focused on comparing Bayes factors to PSIS, but it is relevant to WAIC as well. This\\nexchange is reminiscent of similar debates over BIC and AIC from the 1990s. [221]\\n124. Schwarz (1978). [221]\\n125. Gelman and Rubin (1995). See also section 7.4, page 182, of Gelman et al. (2013). [221]\\n126. See Watanabe (2018b) and Watanabe (2018a). Watanabe has some useful material on his website. See\\nhttp://watanabe-www.math.dis.titech.ac.jp/users/swatanab/psiscv.html. [223]\\n127. See results reported in Watanabe (2018b). See also Vehtari et al. (2016). See also some simulations reported\\non Watanabe’s website: http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/ [223]\\n'},\n",
       " {'index': 583,\n",
       "  'number': 565,\n",
       "  'content': 'ENDNOTES\\n565\\n128. This is closely related to minimum description length. See Grünwald (2007). [225]\\n129. Aki Vehtari and colleagues are working on conditions for the reliability of the normal error approximation.\\nIt’s worth checking his working papers for updates. [229]\\n130. The first edition had a section on model averaging, but the topic has been dropped in this edition to save\\nspace. The approach is really focused on prediction, not inference, and so it doesn’t fit the flow of the second\\nedition. But it is an important approach. The traditional approach is to use weights to average predictions (not\\nparameters) of each model. But if the set of models isn’t carefully chosen, one can do better with model “stack-\\ning.” See Yao et al. (2018). [229]\\n131. The distributions name comes from a 1908 paper by William Sealy Gosset, which he published under the\\npseudonym “Student.” One story told is that Gosset was required by his employer (Guinness Brewery) to publish\\nanonymously, or rather he voluntarily hid his identity, to disguise that Guinness was using statistics to improve\\nbeer. Regardless, the distribution was derived earlier in 1876, within the Bayesian framework. See Pfanzagl and\\nSheynin (1996). [233]\\n132. Specifically, if the variance has an inverse-gamma distribution σ2 ∼inverse-gamma(ν/2, ν/2), then the\\nresulting distribution is Student-t with shape parameter (degrees of freedom) ν. [233]\\n133. See “The Decline of Violent Conflicts: What Do The Data Really Say?” by Pasquale Cirillo and Nassim\\nNicholas Taleb, Nobel Foundation Symposium 161: The Causes of Peace. You can find it readily by searching\\nonline. [234]\\n134. William Henry Harrison’s military history earned him the nickname “Old Tippecanoe.” Tippecanoe was\\nthe sight of a large battle between Native Americans and Harrison, in 1811. In popular imagination, Harrison\\nwas cursed by the Native Americans in the aftermath of the battle. [234]\\nChapter 8\\n135. All manatee facts here taken from Lightsey et al. (2006); Rommel et al. (2007). Scar chart in figure from the\\nfree educational materials at http://www.learner.org/jnorth/tm/manatee/RollCall.html. [237]\\n136. Wald (1943). See Mangel and Samaniego (1984) for a more accessible presentation and historical context.\\n[237]\\n137. Wald (1950). Wald’s foundational paper is Wald (1939). Fienberg (2006) is a highly recommended read\\nfor historical context. For more technical discussions, see Berger (1985), Robert (2007), and Jaynes (2003)\\npage 406. [239]\\n138. GDP is Gross Domestic Product. It’s the most common measure of economic performance, but also one\\nof the silliest. Using GDP to measure the health of an economy is like using heat to measure the quality of a\\nchemical reaction. [239]\\n139. Riley et al. (1999). [239]\\n140. From Nunn and Puga (2012). [242]\\n141. A good example is the extensive modern tunnel system in the Faroe Islands. The natural geology of the\\nislands is very rugged, such that it has historically been much easier to travel by water than by land. But in the\\nlate twentieth century, the Danish government invested heavily in tunnel construction, greatly reducing the ef-\\nfective ruggedness of the islands. [252]\\n142. Modified example from Grafen and Hails (2002), which is a great non-Bayesian applied statistics book you\\nmight also enjoy. It has a rather unique geometric presentation of some of the standard linear models. [253]\\n143. Data from Nettle (1998). [261]\\n'},\n",
       " {'index': 584,\n",
       "  'number': 566,\n",
       "  'content': '566\\nENDNOTES\\nChapter 9\\n144. See the introduction of Gigerenzer et al. (1990) for more on this history. See also Rao (1997) for an example\\npage from a book of random numbers, with similar commentary on the cultural shift. [263]\\n145. The traveling individual metaphor is one of two common metaphors. The other is of a mountain climber\\nwho maps a mountain range by random jumps. See Kruscke (2011) for a very similar story-based explanation\\nabout a politician who raises funds at different locations. Kruschke’s book is excellent. It has a rather different\\nstyle and coverage than this one, so may bring a lot of added value to the reader, in terms of getting a different\\nperspective and a different set of examples. [264]\\n146. Metropolis et al. (1953). The algorithm has been named after the first author of this paper, however it’s not\\nclear how each co-author participated in discovery and implementation of the algorithm. Among the other au-\\nthors were Edward Teller, most famous as the father of the hydrogen bomb, and Marshall Rosenbluth, a renown\\nphysicist in his own right, as well as their wives Augusta and Arianna (respectively), who did much of the com-\\nputer programming. Nicholas Metropolis lead the research group. Their work was in turn based on earlier work\\nwith Stanislaw Ulam: Metropolis and Ulam (1949). [267]\\n147. Hastings (1970). [267]\\n148. Geman and Geman (1984) is the original. See Casella and George (1992) as well. Note that Gibbs sampling\\nis named after physicist and mathematician J. W. Gibbs, one of the founders of statistical physics. However,\\nGibbs died in the year 1903, long before even the Metropolis algorithm was invented. Instead it is named after\\nGibbs, both to honor him and in light of the algorithm’s connections to statistical physics. [267]\\n149. Chapter 16 of Jaynes (2003). [271]\\n150. See Neal (2012) and Betancourt (2017). [273]\\n151. Not actually the total, but rather the sum of squared momentums: K = ∑\\ni p2\\ni /2, where p is a vector of\\nmomentum values. This expression takes its form from energy conservation, which is something we’ll discuss\\nlater on under the topic of divergent transitions. [274]\\n152. See Hoffman and Gelman (2011), as well as additional details in the Stan user manual. [274]\\n153. See the code in Neal (2012). [277]\\n154. See Robert and Casella (2011) for a concise history of MCMC that covers both computation and\\nmathematical foundations. [278]\\n155. See Vehtari et al. (2019a), https://arxiv.org/abs/1903.08008. The term “trank plot” is my own. I’m trying to\\nmake fetch happen. [284]\\n156. For some more detail and background citations, see Chapter 6 in Brooks et al. (2011). [288]\\n157. Gelman and Rubin (1992). [289]\\n158. Gelman 2008: https://andrewgelman.com/2008/05/13/the_folk_theore/ [293]\\n159. As an example, a 2018 paper published in a high impact journal based its conclusions on chains of 5-million\\nsamples with effective sample sizes (n_eff) of 66. See Muñoz-Rodríguez et al. (2018) and critical analysis at\\nhttps://github.com/mmatschiner/kumara. [296]\\nChapter 10\\n160. Grosberg (1998). For topological perspective, see Raymer and Smith (2007). [299]\\n161. Williams (1980). See also Caticha and Griffin (2007); Griffin (2008) for a clearer argument with some\\nworked examples. See Jaynes (1988) for historical context. [300]\\n'},\n",
       " {'index': 585,\n",
       "  'number': 567,\n",
       "  'content': 'ENDNOTES\\n567\\n162. Jaynes (2003), page 351. [303]\\n163. Williams (1980). [304]\\n164. Williams (1980). See also Caticha and Griffin (2007); Griffin (2008) for a clearer argument with some\\nworked examples. See Jaynes (1988) for historical context. [304]\\n165. E. T. Jaynes called this phenomenon “entropy concentration.” See Jaynes (2003), pages 365–370. [305]\\n166. A generalized normal distribution has variance α2Γ(3/β)/Γ(1/β). We can define a family of such distri-\\nbutions with equal variance by choosing the shape β and solving for the α that makes the variance expression\\nequal to any chosen σ2. The solution is α = σ\\n√\\nΓ(1/β)\\nΓ(3/β). This density is provided by rethinking as dgnorm, in\\ncase you want to play around with it. [305]\\n167. I learned this proof from Keith Conrad’s “Probability distributions and maximum entropy” notes, found\\nonline. [306]\\n168. The first line of the function just samples 3 uniform random numbers, with no joint constraint. The second\\nline then solves for the relative value of the 4th value, by using the stated expected value G. The rest of the function\\njust normalizes to a probability distribution and computes entropy. [309]\\n169. McCullagh and Nelder (1989) is the central citation for the conventional generalized linear models. The\\nterm “generalized linear model” is due to Nelder and Wedderburn (1972). The terminology can be confusing, be-\\ncause there is also the “general linear model.” Nelder later regretted the choice. See Senn (2003), page 127. [313]\\n170. Frank (2007). [315]\\n171. Not a real distribution. [316]\\n172. Nuzzo (2014). See also Simmons et al. (2011). [319]\\nChapter 11\\n173. Leopold Kroneker was supposed to have said, “God made the integers, all else is the work of man.” (Die\\nganzen Zahlen hat der liebe Gott gemacht, alles andere ist Menschenwerk. [323]\\n174. Silk et al. (2005). [325]\\n175. Bickel et al. (1975). [340]\\n176. Simpson (1951). [345]\\n177. See Pearl (2014), for example. So much has been written about Simpson’s paradox that you can find it\\nexplained in seemingly contradictory ways. [345]\\n178. Kline and Boyd (2010). [347]\\n179. See Koster and McElreath (2017) for a published Stan example with varying effects, applied to behavioral\\nchoice. [360]\\n180. There seems to be no primordial citation for this transformation. A common citation is Baker (1994), who\\ncites a lot of prior ad hoc use. McCullagh and Nelder (1989) explain the transformation beginning on page 209.\\n[363]\\n181. Welsh and Lind (1995). [367]\\nChapter 12\\n182. Williams (1975, 1982). Bolker (2008) contains a clear presentation in the context of ecological data. [370]\\n183. Another very common parameterization is α = ¯pθ and β = (1 −¯p)θ. The ¯p and θ version is more useful\\nfor modeling, because we typically want to attach a linear model to the beta distribution’s central tendency, one\\nmeasure of which is ¯p. [371]\\n'},\n",
       " {'index': 586,\n",
       "  'number': 568,\n",
       "  'content': '568\\nENDNOTES\\n184. Hilbe (2011) is an entire book devoted to gamma-Poisson regression. [373]\\n185. See Lambert (1992) for the first presentation of this type of model. The basic zero-inflated approach is older,\\nbut Lambert presented the version we use here, with log and logit links to two separate linear models. [377]\\n186. See Bürkner and Vuorre (2018) and Liddell and Kruschke (2018). [380]\\n187. McCullagh (1980) is credited with introducing and popularizing this approach. See also Fullerton (2009)\\nfor an overview with comparison of different model types. [380]\\n188. Cushman et al. (2006). [381]\\n189. The construction in this section is based on the strategy in Bürkner and Charpentier (2018). This is the\\nsame technique that is built into the brms package, which also uses Stan to perform sampling. [391]\\n190. Named after Peter Dirichlet (1805–1859), a German mathematician. His name, and the distribution, are of-\\nten pronounced either like diRIKlay or diRISHlay. Legend has it that Peter himself pronounced it with the hard\\nK. Dirichlet had the best mathematical teachers and made great contributions in many areas of mathematics.\\nHe also married Rebecka Mendelssohn, who was Felix and Fanny Mendelssohn’s younger sister. [393]\\n191. Jung et al. (2014). [397]\\nChapter 13\\n192. Wearing’s wife Deborah has written a book about their life after the illness (Wearing, 2005). His story has\\nalso appeared in a number of documentaries. A quick internet search will turn up a number of news articles, as\\nwell. [399]\\n193. See section 6, page 20, of Gelman (2005) for an entertaining list of wildly different definitions of “random\\neffect.” [401]\\n194. Vonesh and Bolker (2005). [401]\\n195. I adopt the terminology of Gelman (2005), who argues that the common term random effects hardly aids\\nwith understanding, for most people. Indeed, it seems to encourage misunderstanding, partly because the terms\\nfixed and random mean different things to different statisticians. See pages 20–21 of Gelman’s paper. I fully\\nrealize, however, that by trying to spread Gelman’s alternative jargon, I am essentially spitting into a very strong\\nwind. [402]\\n196. It’s also common for the “multi” to refer to multiple linear models. This is especially true in the literature\\non “hierarchical linear models.” Regardless, we’re talking about the same kind of robot here. [403]\\n197. Note that there is still uncertainty about the regularization. So this model isn’t exactly the same as just as-\\nsuming a regularizing prior with a constant standard deviation 1.6. Instead the intercepts for each tank average\\nover the uncertainty in σ (and ¯α). [404]\\n198. This fact has been understood much longer than multilevel models have been practical to use. See Stein\\n(1955) for an influential non-Bayesian paper. [408]\\n199. This example is from Neal (2003), page 732. In that paper, he just calls it a “funnel.” The Devil never comes\\nup. [421]\\n200. See Gelman and Little (1997) for an early paper. There are many recent applications, as well as extensions.\\nSee Gao et al. (2019). [430]\\n201. See Pearl and Bareinboim (2014). See also Balzer (2017) for an overview from the perspective of epidemi-\\nology. [431]\\n202. See O’Hagan (1979). [432]\\n'},\n",
       " {'index': 587,\n",
       "  'number': 569,\n",
       "  'content': 'ENDNOTES\\n569\\nChapter 14\\n203. Lewandowski et al. (2009). The “LKJ” part of the name comes from the first letters of the last names of the\\nauthors, who themselves called the approach the “onion method.” For use in Bayesian models, see the explana-\\ntion in the latest version of the Stan reference manual. [442]\\n204. See Gelfand et al. (1995), as well as Roberts and Sahu (1997). See also Papaspiliopoulos et al. (2007) for\\na more recent overview. See Betancourt and Girolami (2013) for a discussion focusing of Hamiltonian Monte\\nCarlo. [453]\\n205. See Pearl (1995). There is a sizable and largely-pessimistic literature about testing instrumental variable\\nassumptions. If you can find something aimed at your own field, the examples will be more meaningful. [455]\\n206. See Pearl (2011). [456]\\n207. See Angrist and Krueger (1991). [456]\\n208. Feyrer and Sacerdote (2009). [460]\\n209. See Caniglia et al. (2019). [460]\\n210. See Angrist and Krueger (1995) and Kleibergen and Zivot (2003). To my knowledge, there is still no\\nsystematic and theoretically-informed understanding of parametric instrumental variable estimators, Bayesian\\nor otherwise. This is odd, because there is a formal non-parametric theory for them, arising from DAGs. But\\nthe truth is probably that estimation is often impractical, even when the DAG says there is an instrument. [460]\\n211. See Cohen and Malloy (2014). I learned this example from Alex Chino’s blog. See the 2011 posting:\\nhttp://www.alexchinco.com/example-front-door-criterion/ [461]\\n212. Thistlethwaite and Campbell (1960). [461]\\n213. See Gelman and Imbens (2019) for some pointed examples and advice. [461]\\n214. See Cinelli and Hazlett (2020) for a recent advance in causal sensitivity analysis. [461]\\n215. Koster and Leckie (2014). [462]\\n216. See Neal (1998) for a highly cited overview, with notes on implementation. [468]\\n217. See Uyeda et al. (2018) for discussion of problems with traditional methods and the impact of powerful\\nbinary traits like milk. [477]\\n218. See Felsenstein (1985) and Grafen (1989). [478]\\n219. Uhlenbeck and Ornstein (1930). Also see Cooper et al. (2016) for problems fitting these models. [482]\\n220. See Jones and Moriarty (2013), Landis et al. (2013), and Meagher et al. (2018). [482]\\n221. See for example Blomberg et al. (2019). [482]\\nChapter 15\\n222. Joseph Bertrand, 1889, Calcul des probabilités. [489]\\n223. There are several good articles on this topic, each with its own style and variation of notation. See Hernán\\nand Cole (2009), Loken and Gelman (2017), Brakenhoff et al. (2018), van Smeden et al. (2019). [494]\\n224. See Hernán and Cole (2009) for constructive complaints about this. [498]\\n225. See Molenberghs et al. (2014) for an overview of contemporary approaches, Bayesian and otherwise. [499]\\n226. See MacKenzie et al. (2017), which is a comprehensive book with applied intent. [499]\\n'},\n",
       " {'index': 588,\n",
       "  'number': 570,\n",
       "  'content': '570\\nENDNOTES\\n227. See Rubin (1976); Rubin and Little (2002) for background and additional terminology. Section 4 of Rubin’s\\n1976 article is valuable for the clear definitions of causes of missing data. [503]\\n228. Rubin (1987). [511]\\n229. Whitehouse et al. (2019). See raw data download in supplemental. The version here drops some extra vari-\\nables, but otherwise is the same data necessary to replicate the results in the paper. See full documentation and\\ndata at https://github.com/babeheim/moralizing-gods-reanalysis. [513]\\n230. There are two analyses in the original paper (Whitehouse et al., 2019), and both treat NA as zero. The pa-\\nper doesn’t mention missing data in the moralizing gods variable, so it wasn’t noticed during peer review. But\\nbecause the authors were responsible and provided all the data and analysis code, several people independently\\nnoticed the NA-to-zero issue after publication. The authors deserve much credit for their transparency. For the\\nrecord, the original authors still defend the decision to replace NA with zero. You can read the criticisms and the\\nauthors responses for yourself: Beheim et al. (2019), Savage et al. (2019). In my opinion, the debate is confused\\nby many irrelevant arguments. No reliable inference can be made from these data, but some agents on all sides\\nwant to say the evidence supports their existing positions. [516]\\nChapter 16\\n231. “Vitruvian Can” pun donated by Clint Johns @DrClintonJohns via Twitter. [526]\\n232. Harte (1988). [527]\\n233. There are many good articles about the philosophy of model building. I’ll recommend three: Levins (1966),\\nWimsatt (2002), Smaldino (2017). [527]\\n234. From van Leeuwen et al. (2018). Thanks to Anne Sibilsky for furnishing the illustration in Figure 16.3. [531]\\n235. From Boesch et al. (2019). Data kindly provided by Roger Mundry, who designed the clever analysis in the\\npaper. [537]\\n236. von Bertalanffy (1934). [537]\\n237. See Walker et al. (2006) and Leigh and Shea (1996). [541]\\n238. This exampleis based ona Stancase study by BobCarpenter. https://mc-stan.org/users/documentation/case-\\nstudies/lotka-volterra-predator-prey.html [541]\\n239. Hewitt (1921). Note that the lynx data and hare data come from different regions in most cases. While\\nthese data are often used to illustrate population dynamics, there is a deep literature suggesting they aren’t a\\ngreat example. A little searching will turn up a lot. [542]\\n240. Volterra (1926), Lotka (1925). [543]\\n241. Data are from Huffaker (1958). [551]\\nChapter 17\\n242. See Speed (1986) for extended comments like this, aimed at statisticians. You can find a copy of this essay\\nonline with a quick internet search. [553]\\n243. A related phenomenon in popular culture and in science is the Forer effect or Barnum effect. See Forer\\n(1949) and Meehl (1956). [553]\\n244. There have been a few attempts to model these mutual interactions. See McElreath and Smaldino (2015).\\n[553]\\n245. Horton (2015). [553]\\n246. Maybe better to say “especially those in the best journals.” See Ioannidis (2005) and also Ioannidis (2012)\\nfor a highly cited and debated argument. There is a lot of recent and better work in this area, including the Many\\n'},\n",
       " {'index': 589,\n",
       "  'number': 571,\n",
       "  'content': 'ENDNOTES\\n571\\nLabs Replication Projects for social psychology, which have both confirmed and rejected famous findings. [554]\\n247. A particularly infamous example of an un-replicable economic finding that had a big impact on policy is\\nReinhart and Rogoff (2010). Although apparently, if not actually, influential in national and international bud-\\nget debates, the finding was based on odd inclusion criteria and an Excel spreadsheet error. See Herndon et al.\\n(2014). Many other false findings result from no error at all, just misleading samples. The answer is not always\\nin the data, remember. But if you torture the data long enough, it will confess. [554]\\n248. Fontani et al. (2014). This is a fantastic book which catalogs and explains hundreds of false discoveries in\\nelemental chemistry and physics. [554]\\n249. Laudan (1981). To be fair, there are several ways to interpret the pessimistic induction. Newtonian mechan-\\nics, for example, is strictly wrong. But it’s an amazingly successful theory nevertheless. I made a similar point\\nabout the geocentric model of the solar system, back in Chapter 4. But there are plenty of less successful theories\\nthat have also turned out to be false, despite being held to be true for decades or generations. [554]\\n250. This is the standard view in history and philosophy of science. See for introduction Campbell (1985); Hull\\n(1988); Kitcher (2000); Popper (1963, 1996). [554]\\n251. See Sedlemeier and Gigerenzer (1989) and more recent publications on the same topic. [555]\\n252. See for examples relevant to the process of discovery: Gelman and Loken (2013, 2014); Simmons et al. (2011,\\n2013). [555]\\n253. See Fanelli (2012); Franco et al. (2014); Rosenthal (1979). This one has the best title of the genre: Ferguson\\nand Heene (2012). [555]\\n254. Ecologist Art Shapiro published his satirical “Laws of Field Ecology Research” in Bulletin of the Entomologi-\\ncal Society of Canada in the early 1980s. I can’t find the original citation, but a copy provided by Art reads: “Law\\n#4: Never state explicitly the limits on generalizing from your results. The referees will take you at your word\\nand recommend rejection.” Sadly that has always been my experience as well. [555]\\n255. Two excellent examples of this phenomenon occurred in 2014 and 2015. First, Lin et al. (2014) published an\\nanalysis of gene expression that was terribly confounded by batch effects. Basically, they ran a bad experiment.\\nYoav Gilad discovered this and released a reanalysis on Twitter, later published as Gilad and Mizrahi-Man (2015).\\nThe original authors continue to deny the results were in error, and the saga continues. The second involves a\\ncompetition held by Lior Pachtor on his blog: https://liorpachter.wordpress.com/2015/05/26/pachters-p-value-\\nprize/. I recommend reading the whole thing, including the comments, which is where the action is. [555]\\n256. Replication and meta-analysis obviously interact strongly with all the other forces. For a unique article ad-\\ndressing replication and meta-analysis for the incentives they provide in the quality of research, see O’Rourke\\nand Detsky (1989). [555]\\n'},\n",
       " {'index': 590, 'number': 572, 'content': ''},\n",
       " {'index': 591,\n",
       "  'number': 573,\n",
       "  'content': 'Bibliography\\nAkaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In\\nPetrov, B. N. and Csaki, F., editors, Second International Symposium on Information Theory, pages\\n267–281.\\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic\\nControl, 19(6):716–723.\\nAkaike, H. (1978). A Bayesian analysis of the minimum AIC procedure. Ann. Inst. Statist. Math.,\\n30:9–14.\\nAkaike, H. (1981a). Likelihood of a model and information criteria. Journal of Econometrics, 16:3–14.\\nAkaike, H. (1981b). This week’s citation classic. Current Contents Engineering, Technology, and Applied\\nSciences, 12:42.\\nAmrhein, V., Greenland, S., and McShane, B. (2019). Scientists rise up against statistical significance.\\nNature, 567(7748):305–307.\\nAngrist, J. D. and Krueger, A. B. (1991). Does compulsory school attendance affect schooling and\\nearnings? The Quarterly Journal of Economics, 106(4):979–1014.\\nAngrist, J. D. and Krueger, A. B. (1995). Split-sample instrumental variables estimates of the return\\nto schooling. Journal of Business & Economic Statistics, 13(2):225–235.\\nBaker, S. G. (1994). The multinomial-Poisson transformation. Journal of the Royal Statistical Society,\\nSeries D, 43(4):495–504.\\nBalzer, L. B. (2017). “All generalizations are dangerous, even this one”. Epidemiology, 28(4).\\nBeheim, B., Atkinson, Q., Bulbulia, J., Gervais, W. M., Gray, R., Henrich, J., Lang, M., Monroe, M. W.,\\nMuthukrishna, M., Norenzayan, A., and et al. (2019). Corrected analyses show that moralizing\\ngods precede complex societies but serious data concerns remain. psyarxiv.com/jwa2n.\\nBerger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis. Springer-Verlag, New York,\\n2nd edition.\\nBerger, J. O. and Berry, D. A. (1988). Statistical analysis and the illusion of objectivity. American\\nScientist, pages 159–165.\\nBerkson, J. (1946). Limitations of the application of fourfold table analysis to hospital data. Biometrics\\nBulletin, 2:27–53.\\nBetancourt, M. (2017). A conceptual introduction to Hamiltonian Monte Carlo. arXiv:1701.02434.\\nBetancourt, M. J. and Girolami, M. (2013).\\nHamiltonian Monte Carlo for hierarchical models.\\narXiv:1312.0906.\\nBickel, P. J., Hammel, E. A., and O’Connell, J. W. (1975). Sex bias in graduate admission: Data from\\nBerkeley. Science, 187(4175):398–404.\\nBinmore, K. (2009). Rational Decisions. Princeton University Press.\\nBlom, T., Bongers, S., and Mooij, J. M. (2018). Beyond structural causal models: Causal constraints\\nmodels.\\nBlomberg, S. P., Rathnayake, S. I., and Moreau, C. M. (2019). Beyond brownian motion and the\\nornstein-uhlenbeck process: Stochastic diffusion models for the evolution of quantitative charac-\\nters. The American Naturalist, 0(0):000–000.\\n573\\n'},\n",
       " {'index': 592,\n",
       "  'number': 574,\n",
       "  'content': '574\\nBibliography\\nBoesch, C., Bombjaková, D., Meier, A., and Mundry, R. (2019). Learning curves and teaching when\\nacquiring nut-cracking in humans and chimpanzees. Scientific Reports, 9(1):1515.\\nBolker, B. (2008). Ecological Models and Data in R. Princeton University Press.\\nBox, G. E. P. (1979). Robustness in the strategy of scientific model building. In Launer, R. and\\nWilkinson, G., editors, Robustness in Statistics. Academic Press, New York.\\nBox, G. E. P. (1980). Sampling and Bayes’ inference in scientific modelling and robustness. Journal of\\nthe Royal Statistical Society A, 143:383–430.\\nBox, G. E. P. and Tiao, G. C. (1973). Bayesian Inference in Statistical Analysis. Addison-Wesley Pub.\\nCo., Reading, Mass.\\nBrakenhoff, T. B., van Smeden, M., Visseren, F. L. J., and Groenwold, R. H. H. (2018). Random\\nmeasurement error: Why worry? An example of cardiovascular risk factors. PLOS ONE, 13:1–8.\\nBreen, R. (2018). Some methodological problems in the study of multigenerational mobility. Euro-\\npean Sociological Review, 34:603–611.\\nBreiman, L. (1968). Probability. Addison-Wesley Pub. Co.\\nBrooks, S., Gelman, A., Jones, G. L., and Meng, X., editors (2011). Handbook of Markov Chain Monte\\nCarlo. Handbooks of Modern Statistical Methods. Chapman & Hall/CRC.\\nBurnham, K. and Anderson, D. (2002).\\nModel Selection and Multimodel Inference: A Practical\\nInformation-Theoretic Approach. Springer-Verlag, 2nd edition.\\nBürkner, P. C. and Charpentier, E. (2018).\\nModeling monotonic effects of ordinal predictors in\\nbayesian regression models. doi:10.31234/osf.io/9qkhj.\\nBürkner, P. C. and Vuorre, M. (2018).\\nOrdinal regression models in psychology: A tutorial.\\ndoi:10.31234/osf.io/x8swp.\\nCampbell, D. T. (1985). Toward an epistemologically-relevant sociology of science. Science, Technol-\\nogy, & Human Values, 10(1):38–48.\\nCaniglia, E. C., Zash, R., Swanson, S. A., Wirth, K. E., Diseko, M., Mayondi, G., Lockman, S.,\\nMmalane, M., Makhema, J., Dryden-Peterson, S., Kponee-Shovein, K. Z., John, O., Murray, E. J.,\\nand Shapiro, R. L. (2019). Methodological challenges when studying distance to care as an expo-\\nsure in health research. American Journal of Epidemiology, 188(9):1674–1681.\\nCasella, G. and George, E. I. (1992).\\nExplaining the Gibbs sampler.\\nThe American Statistician,\\n46(3):167–174.\\nCaticha, A. and Griffin, A. (2007). Updating probabilities. In Mohammad-Djafari, A., editor, Bayesian\\nInference and Maximum Entropy Methods in Science and Engineering, volume 872 of AIP Conf. Proc.\\nCho, A. (2011). Superluminal neutrinos: Where does the time go? Science, 334(6060):1200–1201.\\nCinelli, C. and Hazlett, C. (2020). Making sense of sensitivity: extending omitted variable bias. Journal\\nof the Royal Statistical Society: Series B (Statistical Methodology), 82(1):39–67.\\nClark, J. S. (2012). The coherence problem with the unified neutral theory of biodiversity. Trends in\\nEcology and Evolution, 27:198–2002.\\nCohen, L. and Malloy, C. J. (2014). Friends in high places. American Economic Journal: Economic\\nPolicy, 6:63–91.\\nCollins, H. M. and Pinch, T. (1998). The Golem: What You Should Know about Science. Cambridge\\nUniversity Press, 2nd edition.\\nCooper, N., Thomas, G. H., Venditti, C., Meade, A., and Freckleton, R. P. (2016). A cautionary note\\non the use of ornstein uhlenbeck models in macroevolutionary studies. Biological Journal of the\\nLinnean Society, 118(1):64–77.\\nCox, R. T. (1946). Probability, frequency and reasonable expectation. American Journal of Physics,\\n14:1–10.\\nCushman, F., Young, L., and Hauser, M. (2006). The role of conscious reasoning and intuition in\\nmoral judgment: Testing three principles of harm. Psychological Science, 17(12):1082–1089.\\nDaston, L. J. and Galison, P. (2007). Objectivity. MIT Press, Cambridge, MA.\\nDawes, R. (1975). Graduate admission variables and future success. Science, 28:721–723.\\n'},\n",
       " {'index': 593,\n",
       "  'number': 575,\n",
       "  'content': 'Bibliography\\n575\\nDecety, J., Cowell, J., Lee, K., Mahasneh, R., Malcolm-Smith, S., Selcuk, B., and Zhou, X. (2015).\\nRetracted: The negative association between religiousness and children’s altruism across the world.\\nCurrent Biology, 25(22):2951 – 2955.\\nElias, P. (1958). Two famous papers. IRE Transactions: on Information Theory, 4:99.\\nFahrmeir, L., Kneib, T., Lang, S., and Marx, B. (2013). Regression. Springer.\\nFanelli, D. (2012). Negative results are disappearing from most disciplines and countries. Scientomet-\\nrics, 90(3):891–904.\\nFelsenstein, J. (1985). Phylogenies and the comparative method. The American Naturalist, 125:1–15.\\nFerguson, C. J. and Heene, M. (2012). A vast graveyard of undead theories: Publication bias and\\npsychological science’s aversion to the null. Perspectives on Psychological Science, 7(6):555–561.\\nFeynman, R. (1967). The Character of Physical Law. MIT Press.\\nFeyrer, J. and Sacerdote, B. (2009). Colonialism and modern income: Islands as natural experiments.\\nThe Review of Economics and Statistics, 91(2):245–262.\\nFienberg, S. E. (2006). When did Bayesian inference become “Bayesian”? Bayesian Analysis, 1(1):1–\\n40.\\nFisher, R. A. (1925). Statistical Methods for Research Workers. Oliver and Boyd, Edinburgh.\\nFisher, R. A. (1955). Statistical methods and scientific induction. Journal of the Royal Statistical Society\\nB, 17(1):69–78.\\nFisher, R. A. (1956). Statistical Methods and Scientific Inference. Hafner, New York, NY.\\nFontani, M., Costa, M., and Orna, M. V. (2014). The Lost Elements: The Periodic Table’s Shadow Side.\\nOxford University Press, Oxford.\\nForer, B. (1949). The fallacy of personal validation: A classroom demonstration of gullibility. Journal\\nof Abnormal and Social Psychology, 44:118–123.\\nFranco, A., Malhotra, N., and Simonovits, G. (2014). Publication bias in the social sciences: Unlock-\\ning the file drawer. Science, 345:1502–1505.\\nFrank, S. (2007). Dynamics of Cancer: Incidence, Inheritance, and Evolution. Princeton University\\nPress, Princeton, NJ.\\nFrank, S. A. (2009). The common patterns of nature. Journal of Evolutionary Biology, 22:1563–1585.\\nFrank, S. A. (2011). Measurement scale in maximum entropy models of species abundance. Journal\\nof Evolutionary Biology, 24:485–496.\\nFreckleton, R. P. (2002). On the misuse of residuals in ecology: regression of residuals vs. multiple\\nregression. Journal of Animal Ecology, 71:542–545.\\nFullerton, A. S. (2009). A conceptual framework for ordered logistic regression models. Sociological\\nMethods & Research, 38(2):306–347.\\nGalton, F. (1989). Kinship and correlation. Statistical Science, 4(2):81–86.\\nGao, Y., Kennedy, L., Simpson, D., and Gelman, A. (2019). Improving multilevel regression and\\npoststratification with structured priors. arXiv:1908.06716.\\nGelfand, A. E. (1996). Model determination using sampling-based methods. Markov Chain Monte\\nCarlo in Practice, pages 145–161.\\nGelfand, A. E., Sahu, S. K., and Carlin, B. P. (1995). Efficient parameterisations for normal linear\\nmixed models. Biometrika, (82):479–488.\\nGelman, A. (2005). Analysis of variance: Why it is more important than ever. The Annals of Statistics,\\n33(1):1–53.\\nGelman, A., Carlin, J. C., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013). Bayesian\\nData Analysis. Chapman & Hall/CRC, 3rd edition.\\nGelman, A. and Greenland, S. (2019). Are confidence intervals better termed ”uncertainty intervals”?\\nBMJ, 366:l5381.\\nGelman, A. and Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models.\\nCambridge University Press.\\nGelman, A., Hwang, J., and Vehtari, A. (2014). Understanding predictive information criteria for\\nbayesian models. Statistics and Computing, 24(6):997–1016.\\n'},\n",
       " {'index': 594,\n",
       "  'number': 576,\n",
       "  'content': '576\\nBibliography\\nGelman, A. and Imbens, G. (2019). Why high-order polynomials should not be used in regression\\ndiscontinuity designs. Journal of Business & Economic Statistics, 37(3):447–456.\\nGelman, A. and Little, T. (1997). Poststratification into many categories using hierarchical logistic\\nregression. Survey Methodology, 23:127‒135.\\nGelman, A. and Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a\\nproblem, even when there is no ‘fishing expedition’ or ‘p-hacking’ and the research hypothesis was\\nposited ahead of time. Technical report, Department of Statistics, Columbia University.\\nGelman, A. and Loken, E. (2014).\\nEthics and statistics: The AAA tranche of subprime science.\\nCHANCE, 27(1):51–56.\\nGelman, A. and Nolan, D. (2002). Teaching Statistics: A Bag of Tricks. Oxford University Press.\\nGelman, A. and Robert, C. P. (2013). “Not only defended but also applied”: The perceived absurdity\\nof Bayesian inference. The American Statistician, 67(1):1–5.\\nGelman, A. and Rubin, D. (1992). Inference from iterative simulation using multiple sequences.\\nStatistical Science, 7:457–511.\\nGelman, A. and Rubin, D. B. (1995). Avoiding model selection in Bayesian social research. Sociological\\nMethodology, 25:165–173.\\nGelman, A. and Stern, H. (2006). The difference between “significant” and “not significant” is not\\nitself statistically significant. The American Statistician, 60(4):328–331.\\nGeman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restora-\\ntion of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(6):721–741.\\nGigerenzer, G. and Hoffrage, U. (1995). How to improve Bayesian reasoning without instruction:\\nFrequency formats. Psychological Review, 102:684–704.\\nGigerenzer, G., Krauss, S., and Vitouch, O. (2004). The null ritual: What you always wanted to know\\nabout significance testing but were afraid to ask. In Kaplan, D., editor, The Sage handbook of quan-\\ntitative methodology for the social sciences, pages 391–408. Sage Publications, Inc., Thousand Oaks.\\nGigerenzer, G., Swijtink, Z., Porter, T., Daston, L., Beatty, J., and Kruger, L. (1990). The Empire of\\nChance: How Probability Changed Science and Everyday Life. Cambridge University Press.\\nGigerenzer, G., Todd, P., and The ABC Research Group (2000). Simple Heuristics That Make Us Smart.\\nOxford University Press, Oxford.\\nGilad, Y. and Mizrahi-Man, O. (2015). A reanalysis of mouse encode comparative gene expression\\ndata. F1000Research, 4(121).\\nGillespie, J. H. (1977). Sampling theory for alleles in a random environment. Nature, 266:443–445.\\nGlenn, N. (2009). Is the apparent u-shape of well-being over the life course a result of inappropriate\\nuse of control variables? a commentary on blanchflower and oswald. Social Science and Medicine,\\n69:481–485.\\nGneiting, T., Balabdaoui, F., and Raftery, A. E. (2007). Probabilistic forecasts, calibration and sharp-\\nness. Journal of the Royal Statistical Society B, 69:243–268.\\nGrafen, A. (1989). The phylogenetic regression. Philosophical Transactions of the Royal Society of\\nLondon. Series B, Biological Sciences, 326(1233):119–157.\\nGrafen, A. and Hails, R. (2002). Modern Statistics for the Life Sciences. Oxford University Press,\\nOxford.\\nGriffin, A. (2008). Maximum Entropy: The Universal Method for Inference. PhD thesis, University of\\nAlbany, State University of New York, Department of Physics.\\nGronau, Q. F. and Wagenmakers, E.-J. (2019). Limitations of Bayesian leave-one-out cross-validation\\nfor model selection. Computational Brain & Behavior, 2(1):1–11.\\nGrosberg, A. (1998). Entropy of a knot: Simple arguments about difficult problem. In Stasiak, A.,\\nKatrich, V., and Kauffman, L. H., editors, Ideal Knots, pages 129–142. World Scientific.\\nGrünwald, P. D. (2007). The Minimum Description Length Principle. MIT Press, Cambridge MA.\\nHacking, I. (1983). Representing and Intervening: Introductory Topics in the Philosophy of Natural\\nScience. Cambridge University Press, Cambridge.\\n'},\n",
       " {'index': 595,\n",
       "  'number': 577,\n",
       "  'content': 'Bibliography\\n577\\nHahn, M. W. and Bentley, R. A. (2003). Drift as a mechanism for cultural change: an example from\\nbaby names. Proceedings of the Royal Society B, 270:S120–S123.\\nHarte, J. (1988). Consider A Spherical Cow: A Course in Environmental Problem Solving. University\\nScience Books.\\nHarte, J. (2011). Maximum Entropy and Ecology: A Theory of Abundance, Distribution, and Energetics.\\nOxford Series in Ecology and Evolution. Oxford University Press, Oxford.\\nHastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning: Data Mining,\\nInference, and Prediction. Springer, 2nd edition.\\nHastings, W. (1970). Monte Carlo sampling methods using Markov chains and their applications.\\nBiometrika, 57(1):97–109.\\nHauer, E. (2004). The harm done by tests of significance. Accident Analysis & Prevention, 36:495–500.\\nHenrion, M. and Fischoff, B. (1986). Assessing uncertainty in physcial constants. American Journal\\nof Physics, 54:791–798.\\nHerndon, T., Ash, M., and Pollin, R. (2014). Does high public debt consistently stifle economic\\ngrowth? A critique of Reinhart and Rogoff. Cambridge Journal of Economics, 38(2):257–279.\\nHernán, M. A. and Cole, S. R. (2009). Invited Commentary: Causal diagrams and measurement bias.\\nAm. J. Epidemiol., 170(8):959–962.\\nHewitt, C. G. (1921). The Conservation of the Wild Life of Canada. Charles Scribner’s Sons.\\nHilbe, J. M. (2011). Negative Binomial Regression. Cambridge University Press, Cambridge, 2nd\\nedition.\\nHinde, K. and Milligan, L. M. (2011). Primate milk synthesis: Proximate mechanisms and ultimate\\nperspectives. Evolutionary Anthropology, 20:9–23.\\nHoffman, M. D. and Gelman, A. (2011). The No-U-Turn sampler: Adaptively setting path lengths in\\nhamiltonian monte carlo. arXiv:1111.4246.\\nHorton, R. (2015). What is medicine’s 5 sigma? The Lancet, 385(April 11):1380.\\nHowell, N. (2000). Demography of the Dobe !Kung. Aldine de Gruyter, New York.\\nHowell, N. (2010). Life Histories of the Dobe !Kung: Food, Fatness, and Well-being over the Life-span.\\nOrigins of Human Behavior and Culture. University of California Press.\\nHubbell, S. P. (2001). The Unified Neutral Theory of Biodiversity and Biogeography. Princeton Univer-\\nsity Press, Princeton.\\nHuffaker, C. B. (1958). Experimental studies on predation: Dispersion factor and predator-prey os-\\ncillations. Hilgardia, 27:795–835.\\nHull, D. L. (1988). Science as a Process: An Evolutionary Account of the Social and Conceptual Devel-\\nopment of Science. University of Chicago Press, Chicago, IL.\\nIoannidis, J. P. A. (2005). Why most published research findings are false. PLoS Medicine, 2(8):0696–\\n0701.\\nIoannidis, J. P. A. (2012). Why science is not necessarily self-correction. Perspectives on Psychological\\nScience, 7(6):645–654.\\nJaynes, E. T. (1976). Confidence intervals vs Bayesian intervals. In Harper, W. L. and Hooker, C. A.,\\neditors, Foundations of Probability Theory, Statistical Inference, and Statistical Theories of Science,\\npage 175.\\nJaynes, E. T. (1984). The intutive inadequancy of classical statistics. Epistemologia, 7:43–74.\\nJaynes, E. T. (1985). Highly informative priors. Bayesian Statistics, 2:329–360.\\nJaynes, E. T. (1986). Monkeys, kangaroos and N. In Justice, J. H., editor, Maximum-Entropy and\\nBayesian Methods in Applied Statistics, page 26. Cambridge University Press, Cambridge.\\nJaynes, E. T. (1988). The relation of Bayesian and maximum entropy methods. In Erickson, G. J.\\nand Smith, C. R., editors, Maximum Entropy and Bayesian Methods in Science and Engineering,\\nvolume 1, pages 25–29. Kluwer Academic Publishers.\\nJaynes, E. T. (2003). Probability Theory: The Logic of Science. Cambridge University Press.\\nJones, N. S. and Moriarty, J. (2013). Evolutionary inference for function-valued traits: Gaussian\\nprocess regression on phylogenies. J R Soc Interface, 10(78):20120616.\\n'},\n",
       " {'index': 596,\n",
       "  'number': 578,\n",
       "  'content': '578\\nBibliography\\nJung, K., Shavitt, S., Viswanathan, M., and Hilbe, J. M. (2014). Female hurricanes are deadlier than\\nmale hurricanes. Proceedings of the National Academy of Sciences USA, 111(24):8782–8787.\\nKadane, J. B. (2011). Principles of Uncertainty. Chapman & Hall/CRC.\\nKitcher, P. (2000). Reviving the sociology of science. Philosophy of Science, 67:S33–S44.\\nKitcher, P. (2011). Science in a Democratic Society. Prometheus Books, Amherst, New York.\\nKleibergen, F. and Zivot, E. (2003). Bayesian and classical approaches to instrumental variable re-\\ngression. Journal of Econometrics, 114(1):29 – 72.\\nKline, M. A. and Boyd, R. (2010). Population size predicts technological complexity in Oceania. Proc.\\nR. Soc. B, 277:2559–2564.\\nKoster, J. and McElreath, R. (2017). Multinomial analysis of behavior: statistical methods. Behavioral\\nEcology and Sociobiology, 71(9):138.\\nKoster, J. M. and Leckie, G. (2014). Food sharing networks in lowland Nicaragua: An application of\\nthe social relations model to count data. Social Networks, 38:100 – 110.\\nKruscke, J. K. (2011). Doing Bayesian Data Analysis. Academic Press, Burlington, MA.\\nKullback, S. (1959). Information Theory and Statistics. John Wiley and Sons, NY.\\nKullback, S. (1987). The Kullback-Leibler distance. The American Statistician, 41(4):340.\\nKullback, S. and Leibler, R. A. (1951). On information and sufficiency. Annals of Mathematical\\nStatistics, 22(1):79–86.\\nLambert, D. (1992). Zero-inflated Poisson regression, with an application to defects in manufactur-\\ning. Technometrics, 34:1–14.\\nLandis, M. J., Schraiber, J. G., and Liang, M. (2013). Phylogenetic analysis using Lévy processes:\\nfinding jumps in the evolution of continuous traits. Syst. Biol., 62(2):193–204.\\nLansing, J. S. and Cox, M. P. (2011). The domain of the replicators: Selection, neutrality, and cultural\\nevolution (with commentary). Current Anthropology, 52:105–125.\\nLaudan, L. (1981). A confutation of convergent realism. Philosophy of Science, 48(1):19–49.\\nLee, R. B. and DeVore, I., editors (1976). Kalahari Hunter-Gatherers: Studies of the !Kung San and\\nTheir Neighbors. Harvard University Press, Cambridge.\\nLeigh, S. R. and Shea, B. T. (1996). Ontogeny of body size variation in African apes. Am. J. Phys.\\nAnthropol., 99(1):43–65.\\nLevins, R. (1966). The strategy of model building in population biology. American Scientist, 54.\\nLewandowski, D., Kurowicka, D., and Joe, H. (2009). Generating random correlation matrices based\\non vines and extended onion method. Journal of Multivariate Analysis, 100:1989–2001.\\nLiddell, T. M. and Kruschke, J. K. (2018). Analyzing ordinal data with metric models: What could\\npossibly go wrong? Journal of Experimental Social Psychology, 79:328 – 348.\\nLightsey, J. D., Rommel, S. A., Costidis, A. M., and Pitchford, T. D. (2006). Methods used during gross\\nnecropsy to determine watercraft-related mortality in the Florida manatee (Trichechus manatus\\nlatirostris). Journal of Zoo and Wildlife Medicine, 37(3):262–275.\\nLin, S., Lin, Y., Nery, J. R., Urich, M. A., Breschi, A., Davis, C. A., Dobin, A., Zaleski, C., Beer,\\nM. A., Chapman, W. C., Gingeras, T. R., Ecker, J. R., and Snyder, M. P. (2014). Comparison of\\nthe transcriptional landscapes between human and mouse tissues. Proc. Natl. Acad. Sci. U.S.A.,\\n111(48):17224–17229.\\nLindley, D. V. (1971). Estimation of many parameters. In Godambe, V. P. and Sprott, D. A., editors,\\nFoundations of Statistical Inference. Holt, Rinehart and Winston, Toronto.\\nLoken, E. and Gelman, A. (2017).\\nMeasurement error and the replication crisis.\\nScience,\\n355(6325):584–585.\\nLotka, A. J. (1925). Principles of Physical Biology. Waverly, Baltimore.\\nLunn, D., Jackson, C., Best, N., Thomas, A., and Spiegelhalter, D. (2013). The BUGS Book. CRC Press.\\nMacKenzie, D., Nichols, J., Royle, J., Pollock, K., Bailey, L., and Hines, J. (2017). Occupancy Estimation\\nand Modeling: Inferring Patterns and Dynamics of Species Occurrence (2nd edition). Academic\\nPress.\\n'},\n",
       " {'index': 597,\n",
       "  'number': 579,\n",
       "  'content': 'Bibliography\\n579\\nMagnusson, M., Andersen, M., Jonasson, J., and Vehtari, A. (2019). Bayesian leave-one-out cross-\\nvalidation for large data. Proceedings of the 36th International Conference on Machine Learning,\\n97:4244–4253.\\nMangel, M. and Samaniego, F. (1984). Abraham Wald’s work on aircraft survivability. Journal of the\\nAmerican Statistical Association, 79:259–267.\\nMarin, J.-M. and Robert, C. (2007). Bayesian Core: A Practical Approach to Computational Bayesian\\nStatistics. Springer.\\nMcCullagh, P. (1980). Regression models for ordinal data. Journal of the Royal Statistical Society,\\nSeries B, 42:109–142.\\nMcCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models. Chapman & Hall/CRC, Boca\\nRaton, Florida, 2nd edition.\\nMcElreath, R. and Smaldino, P. (2015). Replication, communication, and the population dynamics\\nof scientific discovery. PLoS One, 10(8):e0136088. doi:10.1371/journal.pone.0136088.\\nMcGrayne, S. B. (2011). The Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code,\\nHunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy.\\nYale University Press.\\nMcHenry, H. M. and Coffing, K. (2000). Australopithecus to Homo: Transformations in body and\\nmind. Annual Review of Anthropology, 29:125–146.\\nMeagher, J. P., Damoulas, T., Jones, K. E., and Girolami, M. (2018). Phylogenetic gaussian processes\\nfor bat echolocation. In Statistical Data Science, chapter 7, pages 111–124.\\nMeehl, P. E. (1956). Wanted—a good cookbook. The American Psychologist, 11:263–272.\\nMeehl, P. E. (1967). Theory-testing in psychology and physics: A methodological paradox. Philosophy\\nof Science, 34:103–115.\\nMeehl, P. E. (1990). Why summaries of research on psychological theories are often uninterpretable.\\nPsychological Reports, 66:195–244.\\nMetropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., and Teller, E. (1953). Equations of state\\ncalculations by fast computing machines. Journal of Chemical Physics, 21(6):1087–1092.\\nMetropolis, N. and Ulam, S. (1949). The Monte Carlo method. Journal of the American Statistical\\nAssociation, 44(247):335–341.\\nMolenberghs, G., Fitzmaurice, G., Kenward, M. G., Tsiatis, A., and Verbeke, G. (2014). Handbook of\\nMissing Data Methodology. CRC Press.\\nMontgomery, J. M., Nyhan, B., and Torres, M. (2018). How conditioning on posttreatment variables\\ncan ruin your experiment and what to do about it. American Journal of Political Science, 62(3):760–\\n775.\\nMorison, S. E. (1942). Admiral of the Ocean Sea: A Life of Christopher Columbus. Little, Brown and\\nCompany, Boston.\\nMulkay, M. and Gilbert, G. N. (1981). Putting philosophy to work: Karl Popper’s influence on scien-\\ntific practice. Philosophy of the Social Sciences, 11:389–407.\\nMuñoz-Rodríguez, P., Carruthers, T., Wood, J. R. I., Williams, B. R. M., Weitemier, K., Kronmiller, B.,\\nEllis, D., Anglin, N. L., Longway, L., Harris, S. A., Rausher, M. D., Kelly, S., Liston, A., and Scotland,\\nR. W. (2018). Reconciling Conflicting Phylogenies in the Origin of Sweet Potato and Dispersal to\\nPolynesia. Current Biology, 28(8):1246–1256.\\nNeal, R. M. (1998). Regression and classification using Gaussian process priors. In Bernardo, J. M.,\\neditor, Bayesian Statistics, volume 6, pages 475–501. Oxford University Press.\\nNeal, R. M. (2003). Slice sampling. The Annals of Statistics, 31:706–767.\\nNeal, R. M. (2012). MCMC using Hamiltonian dynamics. arXiv:1206.1901. Published as Chapter 5\\nof the Handbook of Markov Chain Monte Carlo, 2011.\\nNelder, J. and Wedderburn, R. (1972). Generalized linear models. Journal of the Royal Statistical\\nSociety, Series A, 135:370–384.\\nNettle, D. (1998). Explaining global patterns of language diversity. Journal of Anthropological Archae-\\nology, 17:354–74.\\n'},\n",
       " {'index': 598,\n",
       "  'number': 580,\n",
       "  'content': '580\\nBibliography\\nNieuwenhuis, S., Forstmann, B. U., and Wagenmakers, E.-J. (2011). Erroneous analyses of interac-\\ntions in neuroscience: a problem of significance. Nature Neuroscience, 14(9):1105–1107.\\nNunn, N. and Puga, D. (2012). Ruggedness: The blessing of bad geography in Africa. Review of\\nEconomics and Statistics, 94:20–36.\\nNuzzo, R. (2014). Statistical errors. Nature, 506:150–152.\\nO’Hagan, A. (1979). On outlier rejection phenomena in bayes inference. Journal of the Royal Statistical\\nSociety: Series B (Methodological), 41(3):358–367.\\nOhta, T. and Gillespie, J. H. (1996). Development of neutral and nearly neutral theories. Theoretical\\nPopulation Biology, 49:128–142.\\nO’Rourke, K. and Detsky, A. S. (1989). Meta-analysis in medical research: Strong encouragement for\\nhigher quality in individual research efforts. Journal of Clinical Epidemiology, 42(10):1021–1024.\\nPapaspiliopoulos, O., Roberts, G. O., and Skold, M. (2007). A general framework for the parametriza-\\ntion of hierarchical models. Statistical Science, (22):59–73.\\nPearl, J. (1995). On the testability of causal models with latent and instrumental variables. In Pro-\\nceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI’95, page 435‒443,\\nSan Francisco, CA, USA. Morgan Kaufmann Publishers Inc.\\nPearl, J. (2000). Causality: Models of Reasoning and Inference. Cambridge University Press, Cam-\\nbridge.\\nPearl, J. (2011). Invited Commentary: Understanding Bias Amplification. American Journal of Epi-\\ndemiology, 174(11):1223–1227.\\nPearl, J. (2014). Understanding Simpson’s paradox. The American Statistician, 68:8–13.\\nPearl, J. (2016). Causal Inference in Statistics: A Primer. John Wiley and Sons.\\nPearl, J. and Bareinboim, E. (2014). External validity: From do-calculus to transportability across\\npopulations. Statist. Sci., 29(4):579–595.\\nPearl, J. and MacKenzie, D. (2018). The Book of Why: The New Science of Cause and Effect. Basic\\nBooks, New York.\\nPearson, K. (1911). The Grammar of Science. A. and C. Black, London.\\nPfanzagl, J. and Sheynin, O. (1996). Studies in the history of probability and statistics. Biometrika,\\n83:891–898.\\nPopper, K. (1963). Conjectures and Refutations: The Growth of Scientific Knowledge. Routledge, New\\nYork.\\nPopper, K. (1996). The Myth of the Framework: In Defence of Science and Rationality. Routledge.\\nProulx, S. R. and Adler, F. R. (2010). The standard of neutrality: still flapping in the breeze? Journal\\nof Evolutionary Biology, 23:1339–1350.\\nRao, C. R. (1997). Statistics and Truth: Putting Chance To Work. World Scientific Publishing.\\nRaymer, D. M. and Smith, D. E. (2007). Spontaneous knotting of an agitated string. Proceedings of\\nthe National Academy of Sciences, 104(42):16432–16437.\\nReilly, C. and Zeringue, A. (2004). Improved predictions of lynx trappings using a biologial model. In\\nGelman, A. and Meng, X., editors, Applied Bayesian Modeling and Causal Inference from Incomplete-\\nData Perspectives, pages 297–308. John Wiley and Sons.\\nReinhart, C. and Rogoff, K. (2010). Growth in a time of debt. American Economic Review, 100(2):573–\\n578.\\nRice, K. (2010). A decision-theoretic formulation of Fisher’s approach to testing. The American\\nStatistician, 64(4):345–349.\\nRiley, S. J., DeGloria, S. D., and Elliot, R. (1999). A terrain ruggedness index that quantifies topo-\\ngraphic heterogeneity. Intermountain Journal of Sciences, 5:23–27.\\nRobert, C. and Casella, G. (2011). A short history of Markov chain Monte Carlo: Subjective rec-\\nollections from incomplete data. In Brooks, S., Gelman, A., Jones, G., and Meng, X.-L., editors,\\nHandbook of Markov Chain Monte Carlo, chapter 2. CRC Press.\\nRobert, C. P. (2007). The Bayesian Choice: from decision-theoretic foundations to computational im-\\nplementation. Springer Texts in Statistics. Springer, 2nd edition.\\n'},\n",
       " {'index': 599,\n",
       "  'number': 581,\n",
       "  'content': 'Bibliography\\n581\\nRoberts, G. O. and Sahu, S. K. (1997). Updating schemes, correlation structure, blocking and param-\\neterisation for the Gibbs sampler. Journal of the Royal Statistical Society, Series B, (59):291–317.\\nRohrer, J. M. (2017). Thinking clearly about correlations and causation: Graphical causal models for\\nobservational data. Advances in Methods and Practices in Psychological Science, 1:27–42.\\nRommel, S. A., Costidis, A. M., Pitchford, T. D., Lightsey, J. D., Snyder, R. H., and Haubold, E. M.\\n(2007). Forensic methods for characterizing watercraft from watercraft-induced wounds on the\\nFlorida manatee (Trichechus manatus latirostris). Marine Mammal Science, 23(1):110–132.\\nRosenbaum, P. R. (1984). The consequences of adjustment for a concomitant variable that has been\\naffected by the treatment. Journal of the Royal Statistical Society A, 147(5):656–666.\\nRosenthal, R. (1979). The file drawer problem and tolerance for null results. Psychological Bulletin,\\n86(3):638–641.\\nRubin, D. (1974). Estimating causal effects of treatments in randomized and nonrandomized studies.\\nJournal of Educational Psychology, 66:688‒701.\\nRubin, D. (1987). Multiple Imputation for Nonresponse in Surveys. John Wiley & Sons, Inc.\\nRubin, D. B. (1976). Inference and missing data. Biometrika, 63:581–592.\\nRubin, D. B. (2005). Causal inference using potential outcomes: Design, modeling, decisions. Journal\\nof the American Statistical Association, 100(469):322–331.\\nRubin, D. B. and Little, R. J. A. (2002). Statistical analysis with missing data. Wiley, New York, 2nd\\nedition.\\nSankararaman, S., Patterson, N., Li, H., Pääbo, S., and Reich, D. (2012). The date of interbreeding\\nbetween Neandertals and modern humans. PLoS Genetics, 8(10):e1002947.\\nSavage, L. J. (1962). The Foundations of Statistical Inference. Methuen.\\nSavage, P. E., Whitehouse, H., François, P., Currie, T. E., Feeney, K., Cioni, E., Purcell, R., Ross, R. M.,\\nLarson, J., Baines, J., and et al. (2019). Reply to beheim et al.: Reanalyses confirm robustness of\\noriginal analyses. osf.io/preprints/socarxiv/xjryt.\\nSchwarz, G. E. (1978). Estimating the dimension of a model. Annals of Statistics, 6:461–464.\\nSedlemeier, P. and Gigerenzer, G. (1989). Do studies of statistical power have an effect on the power\\nof studies? Psychological Bulletin, 105(2):309–316.\\nSenn, S. (2003). A conversation with John Nelder. Statistical Science, 18:118–131.\\nShannon, C. E. (1948). A mathematical theory of communication. The Bell System Technical Journal,\\n27:379–423.\\nShannon, C. E. (1956). The bandwagon. IRE Transactions: on Information Theory, 2:3.\\nShariff, A. F., Willard, A. K., Muthukrishna, M., Kramer, S. R., and Henrich, J. (2016). What is the\\nassociation between religious affiliation and children’s altruism? Current Biology, 26(15):R699–\\nR700.\\nSilk, J. B., Brosnan, S. F., Vonk, J., Henrich, J., Povinelli, D. J., Richardson, A. S., Lambeth, S. P., Mas-\\ncaro, J., and Schapiro, S. J. (2005). Chimpanzees are indifferent to the welfare of unrelated group\\nmembers. Nature, 437:1357–1359.\\nSilver, N. (2012). The Signal and the Noise: Why So Many Predictions Fail—but Some Don’t. Penguin\\nPress, New York.\\nSimmons, J. P., Nelson, L. D., and Simonsohn, U. (2011). False-positive psychology: Undisclosed\\nflexibility in data collection and analysis allows presenting anything as significant. Psychological\\nScience, 22:1359–1366.\\nSimmons, J. P., Nelson, L. D., and Simonsohn, U. (2013). Life after p-hacking. SSRN Scholarly Paper\\nID 2205186, Social Science Research Network, Rochester, NY.\\nSimon, H. (1969). The Sciences of the Artificial. MIT Press, Cambridge, Mass.\\nSimpson, E. H. (1951). The interpretation of interaction in contingency tables. Journal of the Royal\\nStatistical Society, Series B, 13:238–241.\\nSkilling, J. and Knuth, K. H. (2019). The symmetrical foundation of measure, probability, and quan-\\ntum theories. Annalen der Physik, 531:1800057.\\n'},\n",
       " {'index': 600,\n",
       "  'number': 582,\n",
       "  'content': '582\\nBibliography\\nSmaldino, P. (2017). Models are stupid, and we need more of them. In Vallacher, R. R., Read, S. J.,\\nand Nowak, A., editors, Computational Social Psychology, chapter 14.\\nSober, E. (2008). Evidence and Evolution: The logic behind the science. Cambridge University Press,\\nCambridge.\\nSpeed, T. (1986). Questions, answers and statistics. In International Conference on Teaching Statistics\\n2. International Association for Statistical Education.\\nStein, C. (1955). Inadmissibility of the usual estimator for the mean of a multivatiate normal distri-\\nbution. In Proceedings of the Third Berkeley Symposium of Mathematical Statistics and Probability,\\nvolume 1, pages 197–206, Berkeley. University of California Press.\\nTaleb, N. N. (2007). The Black Swan: the Impact of the Highly Improbable. Random House, New York.\\nTheobald, D. L. (2010). A formal test of the theory of universal common ancestry. Nature, 465:219–\\n222.\\nThistlethwaite, D. and Campbell, D. (1960). Regression-discontinuity analysis: An alternative to the\\nex post facto experiment. Journal of Educational Psychology, 51:309‒317.\\nUhlenbeck, G. E. and Ornstein, L. S. (1930). On the theory of the Brownian motion. Phys. Rev.,\\n36:823–841.\\nUyeda, J. C., Zenil-Ferguson, R., and Pennell, M. W. (2018). Rethinking phylogenetic comparative\\nmethods. Systematic Biology, 67(6):1091–1109.\\nvan der Lee, R. and Ellemers, N. (2015). Gender contributes to personal research funding success in\\nthe netherlands. Proceedings of the National Academy of Sciences, 112(40):12349–12353.\\nVan Horn, K. S. (2003). Constructing a logic of plausible inference: A guide to Cox’s theorem. Inter-\\nnational Journal of Approximate Reasoning, 34:3–24.\\nvan Leeuwen, E. J. C., Cohen, E., Collier-Baker, E., Rapold, C. J., Schäfer, M., Schütte, S., and Haun,\\nD. B. M. (2018). The development of human social learning across seven societies. Nature Com-\\nmunications, 9(1):2076.\\nvan Smeden, M., Lash, T. L., and Groenwold, R. H. H. (2019). Five myths about measurement error\\nin epidemiologic research. doi:10.17605/OSF.IO/MSX8D.\\nVehtari, A., Gelman, A., and Gabry, J. (2016). Practical Bayesian model evaluation using leave-one-\\nout cross-validation and WAIC. Statistics and Computing, 27(5):1413‒1432.\\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., and Bürkner, P.-C. (2019a). Rank-normalization,\\nfolding, and localization: An improved Rb for assessing convergence of MCMC.\\nVehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2019b). Pareto smoothed importance\\nsampling.\\nVehtari, A., Simpson, D. P., Yao, Y., and Gelman, A. (2019c). Limitations of “limitations of Bayesian\\nleave-one-out cross-validation for model selection”. Computational Brain & Behavior, 2(1):22–27.\\nVenn, J. (1876). The Logic of Chance. Macmillan and Co, New York, 2nd edition.\\nVolterra, V. (1926). Fluctuations in the abundance of a species considered mathematically. Nature,\\n118(2972):558–560.\\nvon Bertalanffy, L. (1934). Untersuchungen über die Gesetzlichkeit des Wachstums. Wilhelm Roux’\\nArchiv für Entwicklungsmechanik der Organismen, 131(4):613–652.\\nVonesh, J. R. and Bolker, B. M. (2005). Compensatory larval responses shift trade-offs associated with\\npredator-induced hatching plasticity. Ecology, 86:1580–1591.\\nWald, A. (1939). Contributions to the theory of statistical estimation and testing hypotheses. Annals\\nof Mathematical Statistics, 10(4):299–326.\\nWald, A. (1943). A method of estimating plane vulnerability based on damage of survivors. Technical\\nreport, Statistical Research Group, Columbia University.\\nWald, A. (1950). Statistical Decision Functions. J. Wiley, New York.\\nWalker, R., Hill, K., Burger, O., and Hurtado, A. M. (2006). Life in the slow lane revisited: Ontoge-\\nnetic separation between chimpanzees and humans. American Journal of Physical Anthropology,\\n129(4):577–583.\\n'},\n",
       " {'index': 601,\n",
       "  'number': 583,\n",
       "  'content': 'Bibliography\\n583\\nWang, W., Rothschild, D., Goel, S., and Gelman, A. (2015).\\nForecasting elections with non-\\nrepresentative polls. International Journal of Forecasting, 31(3):980–991.\\nWatanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and Widely Applicable Infor-\\nmation Criterion in singular learning theory. Journal of Machine Learning Research, 11:3571–3594.\\nWatanabe, S. (2018a). Higher order equivalence of Bayes cross validation and WAIC. In Ay, N.,\\nGibilisco, P., and Matúš, F., editors, Information Geometry and Its Applications, pages 47–73, Cham.\\nSpringer International Publishing.\\nWatanabe, S. (2018b). Mathematical Theory of Bayesian Statistics. CRC Press.\\nWearing, D. (2005). Forever Today: A True Story of Lost Memory and Never-Ending Love. Doubleday.\\nWelsh, Jr., H. H. and Lind, A. (1995). Habitat correlates of the Del Norte salamander, Plethodon\\nelongatus (Caudata: Plethodontidae) in northwestern California. Journal of Herpetology, 29:198–\\n210.\\nWhitehouse, H., Francois, P., Savage, P. E., Currie, T. E., Feeney, K. C., Cioni, E., Purcell, R., Ross,\\nR. M., Larson, J., Baines, J., Ter Haar, B., Covey, A., and Turchin, P. (2019). Complex societies\\nprecede moralizing gods throughout world history. Nature, 568(7751):226–229.\\nWilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hy-\\npotheses. The Annals of Mathematical Statistics, 9:60–62.\\nWilliams, D. A. (1975). The analysis of binary responses from toxicological experiments involving\\nreproduction and teratogenicity. Biometrics, 31:949–952.\\nWilliams, D. A. (1982). Extra-binomial variation in logistic linear models. Journal of the Royal Sta-\\ntistical Society, Series C, 31(2):144–148.\\nWilliams, P. M. (1980).\\nBayesian conditionalisation and the principle of minimum information.\\nBritish Journal for the Philosophy of Science, 31:131–144.\\nWimsatt, W. (2002). Using false models to elaborate constraints on processes: Blending inheritance\\nin organic and cultural evolution. Philosophy of Science, 69(S3):S12–S24.\\nWittgenstein, L. (1953). Philosophische Untersuchungen. Wissenschaftliche Buchgesellschaft, Frank-\\nfurt 2001.\\nWolpert, D. and Macready, W. (1997). No free lunch theorems for optimization. IEEE Transactions\\non Evolutionary Computation, page 67.\\nWood, S. N. (2017). Generalized Additive Models: an introduction with R (2nd ed). CRC/Taylor and\\nFrancis.\\nWright, S. (1921). Correlation and causation. Agricultural Research, 20:557‒585.\\nYao, Y., Vehtari, A., Simpson, D., and Gelman, A. (2018). Using stacking to average Bayesian predic-\\ntive distributions (with discussion). Bayesian Analysis, 13(3):917‒1007.\\nZhang, Y. and Yang, Y. (2015). Cross-validation for selecting a model selection procedure. Journal of\\nEconometrics, 187:950112.\\n'},\n",
       " {'index': 602, 'number': 584, 'content': ''},\n",
       " {'index': 603,\n",
       "  'number': 585,\n",
       "  'content': 'Citation index\\nAkaike (1973), 564, 573\\nCox (1946), 559, 574\\nAkaike (1974), 564, 573\\nCushman et al. (2006), 568, 574\\nAkaike (1978), 563, 564, 573\\nDaston and Galison (2007), 558, 574\\nAkaike (1981a), 564, 573\\nDawes (1975), 562, 574\\nAkaike (1981b), 564, 573\\nDecety et al. (2015), 562, 574\\nAmrhein et al. (2019), 560, 573\\nElias (1958), 564, 575\\nAngrist and Krueger (1991), 569, 573\\nFahrmeir et al. (2013), 562, 575\\nAngrist and Krueger (1995), 569, 573\\nFanelli (2012), 571, 575\\nBaker (1994), 567, 573\\nFelsenstein (1985), 569, 575\\nBalzer (2017), 568, 573\\nFerguson and Heene (2012), 571, 575\\nBeheim et al. (2019), 570, 573\\nFeynman (1967), 560, 575\\nBerger and Berry (1988), 560, 573\\nFeyrer and Sacerdote (2009), 569, 575\\nBerger (1985), 565, 573\\nFienberg (2006), 559, 565, 575\\nBerkson (1946), 562, 573\\nFisher (1925), 559, 560, 575\\nBetancourt and Girolami (2013), 569, 573\\nFisher (1955), 558, 575\\nBetancourt (2017), 566, 573\\nFisher (1956), 558, 560, 575\\nBickel et al. (1975), 567, 573\\nFontani et al. (2014), 571, 575\\nBinmore (2009), 559, 573\\nForer (1949), 570, 575\\nBlom et al. (2018), 563, 573\\nFranco et al. (2014), 571, 575\\nBlomberg et al. (2019), 569, 573\\nFrank (2007), 567, 575\\nBoesch et al. (2019), 570, 573\\nFrank (2009), 561, 564, 575\\nBolker (2008), 567, 574\\nFrank (2011), 564, 575\\nBox and Tiao (1973), 560, 574\\nFreckleton (2002), 562, 575\\nBox (1979), 557, 574\\nFullerton (2009), 568, 575\\nBox (1980), 561, 574\\nGalton (1989), 562, 575\\nBrakenhoff et al. (2018), 569, 574\\nGao et al. (2019), 568, 575\\nBreen (2018), 563, 574\\nGelfand et al. (1995), 569, 575\\nBreiman (1968), 561, 574\\nGelfand (1996), 564, 575\\nBrooks et al. (2011), 566, 574\\nGelman and Greenland (2019), 560, 575\\nBurnham and Anderson (2002), 563, 564, 574\\nGelman and Hill (2007), 562, 575\\nBürkner and Charpentier (2018), 568, 574\\nGelman and Imbens (2019), 569, 575\\nBürkner and Vuorre (2018), 568, 574\\nGelman and Little (1997), 568, 576\\nCampbell (1985), 571, 574\\nGelman and Loken (2013), 571, 576\\nCaniglia et al. (2019), 569, 574\\nGelman and Loken (2014), 571, 576\\nCasella and George (1992), 566, 574\\nGelman and Nolan (2002), 560, 576\\nCaticha and Griffin (2007), 566, 567, 574\\nGelman and Robert (2013), 559, 560, 576\\nCho (2011), 558, 574\\nGelman and Rubin (1992), 566, 576\\nCinelli and Hazlett (2020), 569, 574\\nGelman and Rubin (1995), 564, 576\\nClark (2012), 557, 574\\nGelman and Stern (2006), 562, 576\\nCohen and Malloy (2014), 569, 574\\nGelman et al. (2013), 560, 564, 575\\nCollins and Pinch (1998), 557, 558, 574\\nGelman et al. (2014), 564, 575\\nCooper et al. (2016), 569, 574\\nGelman (2005), 568, 575\\n585\\n'},\n",
       " {'index': 604,\n",
       "  'number': 586,\n",
       "  'content': '586\\nCITATION INDEX\\nGeman and Geman (1984), 566, 576\\nKullback (1987), 564, 578\\nGigerenzer and Hoffrage (1995), 560, 576\\nLambert (1992), 568, 578\\nGigerenzer et al. (1990), 566, 576\\nLandis et al. (2013), 569, 578\\nGigerenzer et al. (2000), 559, 576\\nLansing and Cox (2011), 557, 558, 578\\nGigerenzer et al. (2004), 559, 576\\nLaudan (1981), 571, 578\\nGilad and Mizrahi-Man (2015), 571, 576\\nLee and DeVore (1976), 561, 578\\nGillespie (1977), 557, 576\\nLeigh and Shea (1996), 570, 578\\nGlenn (2009), 563, 576\\nLevins (1966), 557, 570, 578\\nGneiting et al. (2007), 563, 576\\nLewandowski et al. (2009), 569, 578\\nGrafen and Hails (2002), 565, 576\\nLiddell and Kruschke (2018), 568, 578\\nGrafen (1989), 569, 576\\nLightsey et al. (2006), 565, 578\\nGriffin (2008), 566, 567, 576\\nLin et al. (2014), 571, 578\\nGronau and Wagenmakers (2019), 564, 576\\nLindley (1971), 558, 578\\nGrosberg (1998), 566, 576\\nLoken and Gelman (2017), 569, 578\\nGrünwald (2007), 563, 565, 576\\nLotka (1925), 570, 578\\nHacking (1983), 557, 576\\nLunn et al. (2013), 564, 578\\nHahn and Bentley (2003), 557, 576\\nMacKenzie et al. (2017), 569, 578\\nHarte (1988), 570, 577\\nMagnusson et al. (2019), 564, 578\\nHarte (2011), 564, 577\\nMangel and Samaniego (1984), 565, 579\\nHastie et al. (2009), 563, 577\\nMarin and Robert (2007), 558, 579\\nHastings (1970), 566, 577\\nMcCullagh and Nelder (1989), 567, 579\\nHauer (2004), 561, 577\\nMcCullagh (1980), 568, 579\\nHenrion and Fischoff (1986), 561, 577\\nMcElreath and Smaldino (2015), 570, 579\\nHerndon et al. (2014), 571, 577\\nMcGrayne (2011), 559, 579\\nHernán and Cole (2009), 569, 577\\nMcHenry and Coffing (2000), 563, 579\\nHewitt (1921), 570, 577\\nMeagher et al. (2018), 569, 579\\nHilbe (2011), 568, 577\\nMeehl (1956), 570, 579\\nHinde and Milligan (2011), 562, 577\\nMeehl (1967), 557, 579\\nHoffman and Gelman (2011), 566, 577\\nMeehl (1990), 562, 579\\nHorton (2015), 570, 577\\nMetropolis and Ulam (1949), 566, 579\\nHowell (2000), 561, 577\\nMetropolis et al. (1953), 566, 579\\nHowell (2010), 561, 577\\nMolenberghs et al. (2014), 569, 579\\nHubbell (2001), 557, 577\\nMontgomery et al. (2018), 563, 579\\nHuffaker (1958), 570, 577\\nMorison (1942), 559, 579\\nHull (1988), 571, 577\\nMulkay and Gilbert (1981), 557, 558, 579\\nIoannidis (2005), 560, 570, 577\\nMuñoz-Rodríguez et al. (2018), 566, 579\\nIoannidis (2012), 570, 577\\nNeal (1998), 569, 579\\nJaynes (1976), 563, 577\\nNeal (2003), 568, 579\\nJaynes (1984), 558, 577\\nNeal (2012), 566, 579\\nJaynes (1985), 560, 561, 577\\nNelder and Wedderburn (1972), 567, 579\\nJaynes (1986), 561, 577\\nNettle (1998), 565, 579\\nJaynes (1988), 566, 567, 577\\nNieuwenhuis et al. (2011), 562, 579\\nJaynes (2003), 557, 559, 561, 563–567, 577\\nNunn and Puga (2012), 565, 580\\nJones and Moriarty (2013), 569, 577\\nNuzzo (2014), 567, 580\\nJung et al. (2014), 568, 577\\nO’Hagan (1979), 568, 580\\nKadane (2011), 560, 578\\nO’Rourke and Detsky (1989), 571, 580\\nKitcher (2000), 571, 578\\nOhta and Gillespie (1996), 557, 580\\nKitcher (2011), 558, 578\\nPapaspiliopoulos et al. (2007), 569, 580\\nKleibergen and Zivot (2003), 569, 578\\nPearl and Bareinboim (2014), 568, 580\\nKline and Boyd (2010), 567, 578\\nPearl and MacKenzie (2018), 559, 563, 580\\nKoster and Leckie (2014), 569, 578\\nPearl (1995), 569, 580\\nKoster and McElreath (2017), 567, 578\\nPearl (2000), 559, 563, 580\\nKruscke (2011), 566, 578\\nPearl (2011), 569, 580\\nKullback and Leibler (1951), 564, 578\\nPearl (2014), 562, 563, 567, 580\\nKullback (1959), 564, 578\\nPearl (2016), 563, 580\\n'},\n",
       " {'index': 605,\n",
       "  'number': 587,\n",
       "  'content': 'CITATION INDEX\\n587\\nPearson (1911), 559, 580\\nVonesh and Bolker (2005), 568, 582\\nPfanzagl and Sheynin (1996), 565, 580\\nWald (1939), 565, 582\\nPopper (1963), 571, 580\\nWald (1943), 565, 582\\nPopper (1996), 557, 571, 580\\nWald (1950), 565, 582\\nProulx and Adler (2010), 557, 580\\nWalker et al. (2006), 570, 582\\nRao (1997), 566, 580\\nWang et al. (2015), 559, 582\\nRaymer and Smith (2007), 566, 580\\nWatanabe (2010), 564, 583\\nReilly and Zeringue (2004), 562, 580\\nWatanabe (2018a), 564, 583\\nReinhart and Rogoff (2010), 571, 580\\nWatanabe (2018b), 564, 583\\nRice (2010), 561, 580\\nWearing (2005), 568, 583\\nRiley et al. (1999), 565, 580\\nWelsh and Lind (1995), 567, 583\\nRobert and Casella (2011), 566, 580\\nWhitehouse et al. (2019), 570, 583\\nRoberts and Sahu (1997), 569, 580\\nWilks (1938), 564, 583\\nRobert (2007), 559, 561, 565, 580\\nWilliams (1975), 567, 583\\nRohrer (2017), 563, 581\\nWilliams (1980), 566, 567, 583\\nRommel et al. (2007), 565, 581\\nWilliams (1982), 567, 583\\nWimsatt (2002), 570, 583\\nRosenbaum (1984), 562, 581\\nWittgenstein (1953), 557, 583\\nRosenthal (1979), 571, 581\\nWolpert and Macready (1997), 560, 583\\nRubin and Little (2002), 570, 581\\nWood (2017), 562, 583\\nRubin (1974), 559, 581\\nWright (1921), 559, 583\\nRubin (1976), 570, 581\\nYao et al. (2018), 565, 583\\nRubin (1987), 570, 581\\nZhang and Yang (2015), 564, 583\\nRubin (2005), 562, 581\\nvan Leeuwen et al. (2018), 570, 582\\nSankararaman et al. (2012), 559, 581\\nvan der Lee and Ellemers (2015), 367, 582\\nSavage et al. (2019), 570, 581\\nvon Bertalanffy (1934), 570, 582\\nSavage (1962), 559, 581\\nvan Smeden et al. (2019), 569, 582\\nSchwarz (1978), 564, 581\\nSedlemeier and Gigerenzer (1989), 571, 581\\nSenn (2003), 567, 581\\nShannon (1948), 563, 581\\nShannon (1956), 564, 581\\nShariff et al. (2016), 562, 581\\nSilk et al. (2005), 567, 581\\nSilver (2012), 559, 563, 581\\nSimmons et al. (2011), 567, 571, 581\\nSimmons et al. (2013), 571, 581\\nSimon (1969), 559, 581\\nSimpson (1951), 567, 581\\nSkilling and Knuth (2019), 559, 581\\nSmaldino (2017), 570, 581\\nSober (2008), 558, 582\\nSpeed (1986), 570, 582\\nStein (1955), 568, 582\\nTaleb (2007), 561, 582\\nTheobald (2010), 559, 582\\nThistlethwaite and Campbell (1960), 569, 582\\nUhlenbeck and Ornstein (1930), 569, 582\\nUyeda et al. (2018), 569, 582\\nVan Horn (2003), 559, 582\\nVehtari et al. (2016), 564, 582\\nVehtari et al. (2019a), 566, 582\\nVehtari et al. (2019b), 564, 582\\nVehtari et al. (2019c), 564, 582\\nVenn (1876), 558, 582\\nVolterra (1926), 570, 582\\n'},\n",
       " {'index': 606, 'number': 588, 'content': ''},\n",
       " {'index': 607,\n",
       "  'number': 589,\n",
       "  'content': 'Topic index\\nabsolute deer, 336\\ncausal analysis, 16\\nabsolute effects, 336\\ncausal inference, 124\\naggregated binomial regression, 325\\ncausal models, 16\\nAIC, 219\\ncausal salad, 17, 170\\nAkaike information criterion, 219\\ncentered parameterization, 421\\nape package, 478, 481\\ncentering, 100\\nautocorrelation, 272\\nCholesky decomposition, 453\\nautocorrelation, of samples, 287\\ncollider, 176, 185\\nautomatic differentiation, 286\\ncollider bias, 162, 176\\nautoregressive model, 542, 551\\nColombo, Cristoforo, 19\\naxis, 114\\nColumbus, Christopher, 19\\ncompatibility interval, 54\\nb-spline, 114\\ncomplete case analysis, 146, 499, 515\\nb-splines, 110\\ncomplete pooling, 408\\nbackdoor, 184\\nbackpropagation, 286\\ncomplete-case, 504\\nbasis function, 115\\ncomplete.cases, 146\\nBayes factor, 221\\nconcentration of measure, 268\\nBayes’ theorem, 36\\nconcomitant variable bias, 170\\nBayesian data analysis, 10\\nconditional independence, 311\\nBayesian imputation, 490\\nconditional independencies, 130, 151, 174, 187\\nBayesian information criterion, 193, 221\\nconditioning, 237\\nBayesian updating, 29\\nconfidence interval, 54\\nBayesianism, 12\\nconfounding, 183\\nBerkson’s paradox, 161\\nconjugate pairs, 267\\nBertrand’s box paradox, 489\\nconsistency, model, 221\\nbeta distribution, 393\\nconsistent, 221\\nbeta-binomial, 370\\nconstrasts, 331\\nbias amplifier, 456\\ncontinuous mixture, 370\\nbias-variance trade-off, 201\\ncontrast, 156, 158\\nbias-variance tradeoff, see also overfitting\\nconvergence, MCMC, 284\\nbinomial distribution, 307\\ncorrelation matrix, prior for, 442\\nbinomial regression, 323\\ncorrelation, among parameters, 100\\nbivariate normal distribution, 510\\ncorrelation, spurious, 129\\nbody mass, 148\\ncounterfactual, 140\\nBrownian motion, 481\\ncounterfactual predictions, 135\\nBuridan’s ass, 250\\ncredible interval, 54\\nburn-in, 288\\ncross entropy, 208\\ncalibration, 204\\ncross-classification, 447\\ncategorical, 359\\ncross-classified, 415\\ncategorical variable, 153\\ncross-classified multilevel model, 447\\ncategorical variables, 124\\ncross-validation, 13, 192, 217\\n589\\n'},\n",
       " {'index': 608,\n",
       "  'number': 590,\n",
       "  'content': '590\\nTOPIC INDEX\\ncross-validation, Pareto-smoothed importance\\ngamma distribution, 315\\nsampling, 217\\ngamma-Poisson, 356, 373, 476\\ncumulative link, 369, 380\\ngarden of forking data, 20\\nCurse of Tippecanoe, 234\\nGaussian process regression, 468\\nGaussian processes, 436\\nd-separation, 174\\ngeneralized additive models, 120\\nDAG, 17, 128\\ngeneralized linear madness, 525\\ndata block, 535\\ngeneralized linear model, 312, 313\\ndata compression, 201\\nand information criteria, 320\\ndata dredging, 234\\ngeneralized linear models, 300, 323\\ndata generating process, 171\\ngenerated quantities, 335, 519\\ndata sharing, 555\\ngeocentric model, 71\\ndata(Howell1), 87, 97, 153\\nGibbs sampling, 267\\ndata(milk), 144, 156\\nGPL2, 471\\ndata(WaffleDivorce), 126\\ngradient, 273\\ndbetabinom, 371\\ngraphical causal model, 17\\ndeer, absolute, 336\\ngraphical causal models, 16\\ndescendent, 185\\ngrid approximation, 40\\ndeviance, 210\\nDeviance Information Criterion, 219\\nHamilton, William Rowan, 272\\ndifferential equations, 543\\nHamiltonian Monte Carlo, 271\\nDirected acyclic graph, 151\\nheavy tails, 76\\ndirected acyclic graph, 17, 128\\nhidden Markov model, 521, 536\\nDirichlet distribution, 393\\nhierarchical model, 401\\ndiscrete measurement error, 516\\nhighest posterior density interval, 56\\ndispersion, 370\\nHistomancy, 314\\ndivergence, see also Kullback-Leibler divergence,\\nHybrid Monte Carlo, 272\\n207, 304, 306\\nhyperparameters, 403\\ndivergent transition, 278, 293\\nhyperpriors, 403\\ndivergent transitions, 290, 407, 416, 419, 420\\ndo-operator, 188\\nidentifiable, 528\\ndummy data, 62\\nidentification, 16\\nidentity matrix, 480\\neffective number of parameters, 220\\nimportance sampling, 217\\nentropy, cross, 208\\nimputation, 504\\nepicycle, 71\\nimputation, multiple, 511\\nexchangable, 419\\nimpute, 499\\nexchangeable, 81\\nincluded variable bias, 170\\nexclusion restriction, 455\\nindex variable, 155\\nexponential distribution, 118, 314\\nindicator variable, 154\\nexponential distribution, as prior, 407\\ninformation criteria, 13, 192, 217, 219\\nexponential family, 7, 75, 314\\ninformation criteria, multilevel models, 426\\nexposure, 357\\ninformation entropy, 28, 206, 300\\nextract.samples, 90\\ninformation theory, 76, 193, 204, 205\\nfactors, 153\\nKullback-Leibler divergence, 207\\nfalsification, 9\\ninstrumental variable, 455, 498\\nfat tails, 76\\ninstrumental variables, 437\\nfolk theorem of statistical computing, 293, 296\\ninstrumental variables, with dagitty, 459\\nfork, 184\\ninteraction, 124, 238\\nforward algorithm, 521\\ninteraction, continuous, 252\\nFourier series, 71\\ninverse problem, 531\\nfrequentist, 11\\ninverse-link function, 327\\nfront-door criterion, 460\\ninverse-logit, 317\\nGalileo, 11\\nKelvin, 323\\nGalton, Francis, 92\\nKline2, 471\\n'},\n",
       " {'index': 609,\n",
       "  'number': 591,\n",
       "  'content': 'TOPIC INDEX\\n591\\nknots, 115\\nmissing data, 490\\nmissing not at random, 503\\nlarge world, 19\\nmissing values, 146\\nleapfrog steps, 274\\nmissing values, discrete, 516\\nleave-one-out cross-validation, 217\\nmisspecified, 441\\nlikelihood, 27, 33, 316\\nmixed effects model, 401\\nlikelihood, average, 37\\nmixing, MCMC, 284\\nlikelihood, marginal, 37\\nmixture model, 376\\nlinear model, 92\\nmixture, continuous, 375\\nlinear model, generalized, 312\\nmodel averaging, 229\\nlinear regression, 71\\nmodel block, 535\\nlink, 104, 107\\nmodel checking, 63, 426\\nlink function, 313, 316\\nmodel comparison, 13, 226\\nLKJcorr probability density, 442\\nmodel selection, 225\\nlme4, 420\\nmoderation, 238\\nlog link, 318\\nmodus tollens, 7\\nlog scoring rule, 204\\nMRP, 430\\nlog-linear, 351\\nmulticollinearity, 163\\nlog-pointwise-predictive-density, 210, 218, 220\\nmultilevel model, 14, 400\\nlog_sum_exp, 222\\nmultilevel model, cross-classified, 447\\nlogarithmic scaling, 148\\nmultilevel model, non-centered parameterization,\\nlogistic, 317\\n453\\nlogistic regression, 325\\nmultilevel regression and post-stratification, 430\\nlogit link, 316, 319\\nmultinomial distribution, 359\\nLord Kelvin, 323\\nmultinomial logistic regression, 359\\nloss function, 59\\nmultinomial logit, 359\\nLotka-Volterra model, 543\\nmultinomial-Poisson transformation, 363\\nlppd, 220\\nmultinomial-Poisson transformation, derivation,\\n365\\nmain effects, 253\\nmultiple imputation, 511\\nMarkov chain Monte Carlo, 45, 263\\nmultiple regression, 123\\nMarkov equivalence, 134, 151, 153\\nmultivariate linear model, 458\\nmaxent, 207\\nmultivariate regression, 510\\nmaximum a posteriori, 58, 87\\nmaximum entropy, 7, 34, 76, 207, 300\\nn_eff, 287\\nmaximum entropy classifier, 359\\nnegative binomial, 356\\nmaximum entropy distribution, 303\\nnegative-binomial, 373\\nmaximum entropy, binomial distribution, 312\\nno pooling, 409\\nmaximum entropy, Gaussian, 306\\nno-U-turn sampler, 274\\nmaximum entropy, Wallis derivation, 303\\nnon-centered parameterization, 421, 447, 453\\nmaximum likelihood estimate, 44\\nnon-identifiability, 169\\nmaximum treedepth, 294\\nnull model, 6\\nMCMC, 51, 263\\nnumber of samples, effective, 287\\nMCMC, convergence, 284\\nNUTS, 274\\nMCMC, mixing, 284\\nMCMC, stationarity, 284\\nobservation error, 8\\nmcreplicate, 213\\noccupancy models, 499\\nmeasurement error, 7, 490\\nOckham’s razor, 191\\nmeasurement error, discrete, 516\\nomitted variable bias, 170, 320, 502\\nmediation, 129, 344\\nopen science, 554, 555\\nmeta-analysis, 555\\nordered categorical, 369\\nMetropolis algorithm, 45, 267\\nordered categories, 380\\nmilk energy, 144\\nordered predictor variables, 391\\nMinimum Description Length, 201\\nordinary differential equations, 543\\nmissing at random, 503\\nordinary least squares, 196\\nmissing completely at random, 503\\nOrnstein‒Uhlenbeck process, 482\\n'},\n",
       " {'index': 610,\n",
       "  'number': 592,\n",
       "  'content': '592\\nTOPIC INDEX\\noutlier, 230\\nrelative risk, 337\\noutliers, dropping, 232\\nrelative shark, 336\\nover-dispersion, 369, 370, 407, 476\\nreparameterize, 420\\noverfitting, 3, 13, 20, 192–194\\nrepeatability, 554\\nreplication, 554\\np-hacking, 97\\nresiduals, 135, 314\\np-values, misinterpretation, 12\\nresiduals, uncertainty in, 137\\nPagel’s lambda, 482\\nridge regression, 216\\npairs, 168\\nrlkjcorr, 442\\nparameter, 27, 34\\nrobust regression, 233, 261\\nparameters, 32\\nrugged, dataset, 242\\nparameters block, 535\\nPareto distribution, 217, 218\\nsampling distribution, 11, 63\\npartial pooling, 14, 409\\nSaturn, 11\\npatristic distance, 477\\nsensitivity analysis, 319, 461\\npeer review, 555\\nsharing, data, 555\\npercentile intervals, 55\\nshark, relative, 336\\nphylogenetic regression, 477, 478\\nshrinkage, 405, 495\\npipe, 185\\nsim, 108, 109\\npoint estimate, 58\\nsim.train.test, 213\\nPoisson distribution, 315, 346\\nsim_train_test, 212\\nPoisson regression, 323\\nsimplex, 394, 533\\npolynomial regression, 110\\nSimpson’s paradox, 183, 345\\npooling, 405\\nsimulation, 61\\npost-stratification, 430\\nsmall world, 19\\npost-treatment bias, 170\\nsocial network, 467\\nposterior distribution, 36\\nsocial relations model, 462\\nposterior predictive check, 135\\nsoftmax, 359\\nposterior predictive distribution, 65\\nspherical cow, 527\\nposterior probability, 27\\nspline, 114\\npower analysis, 61\\nspurious correlation, 129\\npre-registration, 555\\nStan, 263\\nprecision, as inverse variance, 76\\nstandard error, 44\\npredictor variable, 91\\nstandardize, 111\\nprequential, 225\\nStanisław Ulam, 264\\nprinciple of indifference, 26\\nstargazing, 193\\nprior, 34\\nstart values, 89\\nprior predictive, 82\\nstate space model, 521, 536, 543\\nprior predictive simulation, 95, 97\\nstationarity, MCMC, 284\\nprior probability, 27\\nstep size, 274\\npriors, 94\\nstochastic, 78\\nprocess models, 5\\nstochastic block model, 467\\nproportional odds, 336, 337\\nStudent’s t, 233\\nPSIS, 217\\nsubjective Bayesian, 35\\nPtolemy, 71\\nsubjective belief, 11\\nquadratic approximation, 42, 87\\ntails, heavy and thin, 76\\nquap, 42, 87\\ntest sample, 211\\ntestable implications, 130\\nrandom effects, 401\\nthin tails, 76\\nrandomization, 28\\nThomson, William, 323\\nrandomized controlled experiments, 16\\ntide prediction, 323\\nRDD, 461\\ntime series, 536, 541\\nregression discontinuity, 461, 513\\ntrace plot, 284, 288\\nregularizing prior, 192, 214, 404\\ntrace rank plot, 284\\nrelative effects, 336\\ntraining sample, 211\\n'},\n",
       " {'index': 611,\n",
       "  'number': 593,\n",
       "  'content': 'TOPIC INDEX\\n593\\ntrank plot, 284, 288\\ntransformed parameters, 335, 453\\ntransitivity, 467\\ntransportability, 431\\ntreedepth, 294\\ntriptych, 257\\ntwo-stage least squares, 460\\nU-turn, 274\\nulam, 280\\nunderfitting, 192, see also overfitting, 201\\nvariance-covariance, 90\\nvarying effects, 402, 435\\nvarying intercepts, 402, 405\\nvarying slopes, 436, 437, 441\\nwarmup, 274\\nWidely Applicable Information Criterion, 220\\nzero-augmented, 369\\nzero-inflated, 369, 376\\n'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dfa076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
